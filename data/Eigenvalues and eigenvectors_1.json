{
    "pages": [
        {
            "desc_links": [
                "/wiki/Square_matrix",
                "/wiki/Row_and_column_vectors",
                "/wiki/Matrix-vector_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Linear_algebra",
                "/wiki/Linear_map",
                "/wiki/Vector_space",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Zero_vector"
            ],
            "links": [
                "/wiki/Eigenvector_centrality",
                "/wiki/Google",
                "/wiki/PageRank",
                "/wiki/Adjacency_matrix",
                "/wiki/Stationary_distribution",
                "/wiki/Markov_chain",
                "/wiki/Spectral_clustering",
                "/wiki/Solid_mechanics",
                "/wiki/Stress_(mechanics)",
                "/wiki/Diagonal",
                "/wiki/Shear_(mathematics)",
                "/wiki/Mechanics",
                "/wiki/Inertia_Tensor",
                "/wiki/Principal_axis_(mechanics)",
                "/wiki/Rigid_body",
                "/wiki/Tensor",
                "/wiki/Inertia",
                "/wiki/Center_of_mass",
                "/wiki/Image_processing",
                "/wiki/Brightness",
                "/wiki/Pixel",
                "/wiki/Covariance_matrix",
                "/wiki/Eigenface",
                "/wiki/Principal_component_analysis",
                "/wiki/Linear_combination",
                "/wiki/Facial_recognition_system",
                "/wiki/Biometrics",
                "/wiki/Data_compression",
                "/wiki/Recognition_of_human_individuals",
                "/wiki/Finite_element_analysis",
                "/wiki/Quadratic_eigenvalue_problem#Methods_of_Solution",
                "/wiki/Quadratic_eigenvalue_problem",
                "/wiki/Degrees_of_freedom_(mechanics)",
                "/wiki/Data_mining",
                "/wiki/Data_set",
                "/wiki/Bioinformatics",
                "/wiki/Data_mining",
                "/wiki/Chemometrics",
                "/wiki/Psychometrics",
                "/wiki/Marketing",
                "/wiki/Psychometrics",
                "/wiki/Q_methodology",
                "/wiki/Statistical_significance",
                "/wiki/Hypothesis_testing",
                "/wiki/Scree%27s_test",
                "/wiki/Factor_analysis",
                "/wiki/Structural_equation_model",
                "/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices",
                "/wiki/Symmetric_matrix",
                "/wiki/Positive_semidefinite_matrix",
                "/wiki/Positive_semidefinite_matrix",
                "/wiki/Orthogonal_basis",
                "/wiki/Multivariate_statistics",
                "/wiki/Sample_variance",
                "/wiki/Covariance_matrix",
                "/wiki/Principal_components_analysis",
                "/wiki/Linear_relation",
                "/wiki/Covariance_matrix",
                "/wiki/Correlation_matrix",
                "/wiki/Sample_variance",
                "/wiki/Principal_components_analysis",
                "/wiki/Explained_variance",
                "/wiki/Orthogonal_basis",
                "/wiki/Geology",
                "/wiki/Glacial_till",
                "/wiki/Clasts",
                "/wiki/Quantum_mechanics",
                "/wiki/Atomic_physics",
                "/wiki/Molecular_physics",
                "/wiki/Hartree%E2%80%93Fock",
                "/wiki/Atomic_orbital",
                "/wiki/Molecular_orbital",
                "/wiki/Fock_operator",
                "/wiki/Ionization_potential",
                "/wiki/Koopmans%27_theorem",
                "/wiki/Iteration",
                "/wiki/Self-consistent_field",
                "/wiki/Quantum_chemistry",
                "/wiki/Orthogonal",
                "/wiki/Basis_set_(chemistry)",
                "/wiki/Generalized_eigenvalue_problem",
                "/wiki/Roothaan_equations",
                "/wiki/Squeeze_mapping",
                "/wiki/Linear_equation",
                "/wiki/Linear_system",
                "/wiki/QR_algorithm",
                "/wiki/Householder_transformation",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Hermitian_matrix",
                "/wiki/Sparse_matrix",
                "/wiki/Lanczos_algorithm",
                "/wiki/Iterative_method",
                "/wiki/Accuracy",
                "/wiki/Round-off_error",
                "/wiki/Wilkinson%27s_polynomial",
                "/wiki/Differential_equation",
                "/wiki/Difference_equation",
                "/wiki/Weight_(representation_theory)",
                "/wiki/Algebra_representation",
                "/wiki/Associative_algebra",
                "/wiki/Module_(mathematics)",
                "/wiki/Representation_theory",
                "/wiki/Functional_analysis",
                "/wiki/Spectrum_(functional_analysis)",
                "/wiki/Bounded_operator",
                "/wiki/Zero_vector",
                "/wiki/Invariant_subspace",
                "/wiki/Direct_sum",
                "/wiki/Linear_map",
                "/wiki/Vector_space",
                "/wiki/Field_(algebra)",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Eigenfunction",
                "/wiki/Exponential_function",
                "/wiki/Hilbert_space",
                "/wiki/Banach_space",
                "/wiki/Differential_operator",
                "/wiki/Function_space",
                "/wiki/Derivative",
                "/wiki/Differential_equation",
                "/wiki/Triangular_matrix",
                "/wiki/Diagonal_matrices",
                "/wiki/Permutation_matrix",
                "/wiki/Defective_matrix",
                "/wiki/Generalized_eigenvector",
                "/wiki/Jordan_normal_form",
                "/wiki/Jordan_normal_form",
                "/wiki/Generalized_eigenspace",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Matrix_similarity",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Matrix_similarity",
                "/wiki/Closure_(mathematics)",
                "/wiki/Distributive_property",
                "/wiki/Commutative_property",
                "/wiki/Kernel_(linear_algebra)",
                "/wiki/Union_(set_theory)",
                "/wiki/Linear_subspace",
                "/wiki/Set_(mathematics)",
                "/wiki/Multiple_roots_of_a_polynomial",
                "/wiki/Polynomial_division",
                "/wiki/Complex_conjugate",
                "/wiki/Intermediate_value_theorem",
                "/wiki/Real_matrix",
                "/wiki/Irrational_number",
                "/wiki/Rational_number",
                "/wiki/Algebraic_number",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Factorization",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/Polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Coefficient",
                "/wiki/Characteristic_polynomial",
                "/wiki/Determinant",
                "/wiki/Identity_matrix",
                "/wiki/Scalar_multiplication",
                "/wiki/Parallel_(geometry)",
                "/wiki/Collinearity",
                "/wiki/Richard_Edler_von_Mises",
                "/wiki/Power_method",
                "/wiki/QR_algorithm",
                "/wiki/John_G.F._Francis",
                "/wiki/Vera_Kublanovskaya",
                "/wiki/David_Hilbert",
                "/wiki/Integral_operator",
                "/wiki/German_language",
                "/wiki/Helmholtz",
                "/wiki/Joseph_Liouville",
                "/wiki/Sturm%E2%80%93Liouville_theory",
                "/wiki/Hermann_Schwarz",
                "/wiki/Laplace%27s_equation",
                "/wiki/Henri_Poincar%C3%A9",
                "/wiki/Poisson%27s_equation",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_equation",
                "/wiki/Separation_of_variables",
                "/wiki/Th%C3%A9orie_analytique_de_la_chaleur",
                "/wiki/Jacques_Charles_Fran%C3%A7ois_Sturm",
                "/wiki/Charles_Hermite",
                "/wiki/Hermitian_matrix",
                "/wiki/Francesco_Brioschi",
                "/wiki/Orthogonal_matrix",
                "/wiki/Unit_circle",
                "/wiki/Alfred_Clebsch",
                "/wiki/Skew-symmetric_matrix",
                "/wiki/Karl_Weierstrass",
                "/wiki/Stability_theory",
                "/wiki/Defective_matrix",
                "/wiki/Leonhard_Euler",
                "/wiki/Rigid_body",
                "/wiki/Principal_axis_(mechanics)",
                "/wiki/Lagrange",
                "/wiki/Augustin_Louis_Cauchy",
                "/wiki/Quadric_surface",
                "/wiki/Secular_equation",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Quadratic_form",
                "/wiki/Differential_equation",
                "/wiki/Matrix_decomposition",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Mona_Lisa",
                "/wiki/Shear_mapping",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/German_language",
                "/wiki/Principal_axis_(mechanics)",
                "/wiki/Rigid_body",
                "/wiki/Stability_theory",
                "/wiki/Vibration_analysis#eigenvalue_problem",
                "/wiki/Atomic_orbital",
                "/wiki/Eigenface",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Square_matrix",
                "/wiki/Row_and_column_vectors",
                "/wiki/Matrix-vector_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Linear_algebra",
                "/wiki/Linear_map",
                "/wiki/Vector_space",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Zero_vector"
            ],
            "text": "The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.leads to a so-called quadratic eigenvalue problem,orEigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed byPrincipal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree\u2013Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree\u2013Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.The following table presents some example transformations in the plane along with their 2\u00d72 matrices, eigenvalues, and eigenvectors.Some numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation.This matrix equation is equivalent to two linear equationsOnce the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrixEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961. [39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]In theory, the coefficients of the characteristic polynomial can be computed exactly, since they are sums of products of matrix elements; and there are algorithms that can find all the roots of a polynomial of arbitrary degree to any required accuracy.[39] However, this approach is not viable in practice because the coefficients would be contaminated by unavoidable round-off errors, and the roots of a polynomial can be an extremely sensitive function of the coefficients (as exemplified by Wilkinson's polynomial).[39]A similar procedure is used for solving a differential equation of the formThe solution of this equation for x in terms of t is found by using its characteristic equationThe simplest difference equations have the formThe representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation \u2013 an associative algebra acting on a module. The study of such actions is the field of representation theory.For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars \u03bb for which the operator (T \u2212 \u03bbI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.If \u03bb is an eigenvalue of T, then the operator (T \u2212 \u03bbI) is not one-to-one, and therefore its inverse (T \u2212 \u03bbI)\u22121 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T \u2212 \u03bbI) may not have an inverse even if \u03bb is not an eigenvalue.Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar \u03bb \u2208 K such that there exists a non-zero vector v \u2208 V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue \u03bb to be any vector that, given \u03bb, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]The geometric multiplicity \u03b3T(\u03bb) of an eigenvalue \u03bb is the dimension of the eigenspace associated with \u03bb, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, \u03b3T(\u03bb) \u2265 1 because every eigenvalue has at least one eigenvector.So, both u + v and \u03b1v are either zero or eigenvectors of T associated with \u03bb, namely (u+v,\u03b1v) \u2208 E, and E is closed under addition and scalar multiplication. The eigenspace E associated with \u03bb is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]for (x,y) \u2208 V and \u03b1 \u2208 K. Therefore, if u and v are eigenvectors of T associated with eigenvalue \u03bb, namely (u,v) \u2208 E, thenBy definition of a linear transformation,which is the union of the zero vector with the set of all eigenvectors associated with\u00a0\u03bb. E is called the eigenspace or characteristic space of T associated with\u00a0\u03bb.Given an eigenvalue \u03bb, consider the setThis equation is called the eigenvalue equation for T, and the scalar \u03bb is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while \u03bbv is the product of the scalar \u03bb with v.[33]We say that a non-zero vector v \u2208 V is an eigenvector of T if and only if there exists a scalar \u03bb \u2208 K such thatThe concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,The main eigenfunction article gives other examples.is the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for \u03bb = 0 the eigenfunction f(t) is a constant.This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential functionThe functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C\u221e of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equationOn the other hand, the geometric multiplicity of the eigenvalue 2 is only 1, because its eigenspace is spanned by just one vector [0 1 \u22121 1]T and is therefore 1-dimensional. Similarly, the geometric multiplicity of the eigenvalue 3 is 1 because its eigenspace is spanned by just one vector [0 0 0 1]T. The total geometric multiplicity \u03b3A is 2, which is the smallest it could be for a matrix with two distinct eigenvalues. Geometric multiplicities are defined in a later section.The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is \u03bcA = 4 = n, the order of the characteristic polynomial and the dimension of A.has a characteristic polynomial that is the product of its diagonal elements,As in the previous example, the lower triangular matrixrespectively, as well as scalar multiples of these vectors.These eigenvalues correspond to the eigenvectors,which has the roots \u03bb1 = 1, \u03bb2 = 2, and \u03bb3 = 3. These roots are the diagonal elements as well as the eigenvalues of\u00a0A.The characteristic polynomial of A isConsider the lower triangular matrix,A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix. As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.respectively, as well as scalar multiples of these vectors.Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,which has the roots \u03bb1 = 1, \u03bb2 = 2, and \u03bb3 = 3. These roots are the diagonal elements as well as the eigenvalues of\u00a0A.The characteristic polynomial of A isMatrices with entries only along the main diagonal are called diagonal matrices. The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrixandThenFor the complex conjugate pair of imaginary eigenvalues, note thatFor the real eigenvalue \u03bb1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,This matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1\u00a0\u2212\u00a0\u03bb3, whose roots areConsider the cyclic permutation matrixThe characteristic polynomial of A isConsider the matrixThus, the vectors v\u03bb=1 and v\u03bb=3 are eigenvectors of A associated with the eigenvalues \u03bb = 1 and \u03bb = 3, respectively.is an eigenvector of A corresponding to \u03bb = 3, as is any scalar multiple of this vector.Any non-zero vector with v1 = v2 solves this equation. Therefore,For \u03bb = 3, Equation (2) becomesis an eigenvector of A corresponding to \u03bb = 1, as is any scalar multiple of this vector.Any non-zero vector with v1 = \u2212v2 solves this equation. Therefore,For \u03bb = 1, Equation (2) becomes,Setting the characteristic polynomial equal to zero, it has roots at \u03bb = 1 and \u03bb = 3, which are the two eigenvalues of A.Taking the determinant to find characteristic polynomial of A,The figure on the right shows the effect of this transformation on point coordinates in the plane. The eigenvectors v of this transformation satisfy Equation (1), and the values of \u03bb for which the determinant of the matrix (A\u00a0\u2212\u00a0\u03bbI) equals zero are the eigenvalues.Consider the matrixA matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P\u22121AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix \u039b or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and \u039b represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as\u00a0\u039b.or by instead left multiplying both sides by Q\u22121,Because the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q\u22121,With this in mind, define a diagonal matrix \u039b where each diagonal element \u039bii is the eigenvalue associated with the ith column of Q. ThenSince each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, ..., vn with associated eigenvalues \u03bb1, \u03bb2, ..., \u03bbn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.where \u03ba is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and \u03ba is its associated eigenvalue. Taking the transpose of this equation,The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation isMany disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word \"eigenvector\" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),Let A be an arbitrary n by n matrix of complex numbers with eigenvalues \u03bb1, \u03bb2, ..., \u03bbn. Each eigenvalue appears \u03bcA(\u03bbi) times in this list, where \u03bcA(\u03bbi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If \u03b3A = n, thenSuppose A has d \u2264 n distinct eigenvalues \u03bb1, \u03bb2, ..., \u03bbd, where the geometric multiplicity of \u03bbi is \u03b3A(\u03bbi). The total geometric multiplicity of A,The condition that \u03b3A(\u03bb) \u2264 \u03bcA(\u03bb) can be proven by considering a particular eigenvalue \u03be of A and diagonalizing the first \u03b3A(\u03be) columns of A with respect to the eigenvectors of \u03be, described in a later section. The resulting similar matrix B is block upper triangular, with its top left block being the diagonal matrix \u03beI\u03b3A(\u03be). As a result, the characteristic polynomial of B will have a factor of (\u03be\u00a0\u2212\u00a0\u03bb)\u03b3A(\u03be). The other factors of the characteristic polynomial of B are not known, so the algebraic multiplicity of \u03be as an eigenvalue of B is no less than the geometric multiplicity of \u03be as an eigenvalue of A. The last element of the proof is the property that similar matrices have the same characteristic polynomial.Because of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.The dimension of the eigenspace E associated with \u03bb, or equivalently the maximum number of linearly independent eigenvectors associated with \u03bb, is referred to as the eigenvalue's geometric multiplicity \u03b3A(\u03bb). Because E is also the nullspace of (A \u2212 \u03bbI), the geometric multiplicity of \u03bb is the dimension of the nullspace of (A \u2212 \u03bbI), also called the nullity of (A \u2212 \u03bbI), which relates to the dimension and rank of (A \u2212 \u03bbI) asBecause the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) \u2208 E, then (u + v) \u2208 E or equivalently A(u + v) = \u03bb(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v \u2208 E and \u03b1 is a complex number, (\u03b1v) \u2208 E or equivalently A(\u03b1v) = \u03bb(\u03b1v). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and \u03b1v are not zero, they are also eigenvectors of A associated with \u03bb.On one hand, this set is precisely the kernel or nullspace of the matrix (A \u2212 \u03bbI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with \u03bb. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with \u03bb, and E equals the nullspace of (A \u2212 \u03bbI). E is called the eigenspace or characteristic space of A associated with \u03bb.[7][8] In general \u03bb is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of \u2102n.Given a particular eigenvalue \u03bb of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),If \u03bcA(\u03bbi) = 1, then \u03bbi is said to be a simple eigenvalue.[27] If \u03bcA(\u03bbi) equals the geometric multiplicity of \u03bbi, \u03b3A(\u03bbi), defined in the next section, then \u03bbi is said to be a semisimple eigenvalue.If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n asSuppose a matrix A has dimension n and d \u2264 n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,Let \u03bbi be an eigenvalue of an n by n matrix A. The algebraic multiplicity \u03bcA(\u03bbi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (\u03bb \u2212 \u03bbi)k divides evenly that polynomial.[8][26][27]The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.If the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.Setting the characteristic polynomial equal to zero, it has roots at \u03bb = 1 and \u03bb = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = \u03bbv. In this example, the eigenvectors are any non-zero scalar multiples ofTaking the determinant of (M \u2212 \u03bbI), the characteristic polynomial of M isAs a brief example, which is described in more detail in the examples section later, consider the matrixwhere each \u03bbi may be real but in general is a complex number. The numbers \u03bb1, \u03bb2, ... \u03bbn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.The fundamental theorem of algebra implies that the characteristic polynomial of an n by n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable \u03bb and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (\u22121)n\u03bbn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A.Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A \u2212 \u03bbI) is zero. Therefore, the eigenvalues of A are values of \u03bb that satisfy the equationwhere I is the n by n identity matrix.Equation (1) can be stated equivalently asthen v is an eigenvector of the linear transformation A and the scale factor \u03bb is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.If it occurs that v and w are scalar multiples, that is ifwhere, for each row,orNow consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,In this case \u03bb = \u22121/20.These vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar \u03bb such thatConsider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectorsEigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means \"own\", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was \"proper value\", but the more distinctive term \"eigenvalue\" is standard today.[19]In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm\u2013Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincar\u00e9 studied Poisson's equation a few years later.[16]Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Th\u00e9orie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caract\u00e9ristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:where the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplicationThe Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.referred to as the eigenvalue equation or eigenequation. In general, \u03bb may be any scalar. For example, \u03bb may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value \u03bb, called an eigenvalue. This condition can be written as the equationEigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for \"proper\", \"characteristic\".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.Geometrically an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed.[3]There is a correspondence between n by n square matrices and linear transformations from an n-dimensional vector space to itself. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equationwhere \u03bb is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v.In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. This condition can be written as the equation",
            "title": "Eigenvalues and eigenvectors",
            "url": "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors"
        },
        {
            "desc_links": [
                "/wiki/Google",
                "/wiki/PageRank",
                "/wiki/Katz_centrality",
                "/wiki/Graph_theory",
                "/wiki/Node_(networking)",
                "/wiki/Network_(mathematics)"
            ],
            "links": [
                "/wiki/Neuroscience",
                "/wiki/Neuron",
                "/wiki/Google",
                "/wiki/PageRank",
                "/wiki/Katz_centrality",
                "/wiki/Graph_theory",
                "/wiki/Node_(networking)",
                "/wiki/Network_(mathematics)"
            ],
            "text": "In neuroscience, the eigenvector centrality of a neuron in a neural network has been found to correlate with its relative firing rate.[3]Google's PageRank and the Katz centrality are variants of the eigenvector centrality.[1]In graph theory, eigenvector centrality (also called eigencentrality) is a measure of the influence of a node in a network. Relative scores are assigned to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. A high eigenvector score means that a node is connected to many nodes who themselves have high scores.",
            "title": "Eigenvector centrality",
            "url": "https://en.wikipedia.org/wiki/Eigenvector_centrality"
        },
        {
            "desc_links": [
                "/wiki/Alexa_Internet",
                "/wiki/YouTube",
                "/wiki/Blogger_(service)",
                "/wiki/Criticism_of_Google",
                "/wiki/Privacy_concerns_regarding_Google",
                "/wiki/Criticism_of_Google#Aggressive_tax_avoidance",
                "/wiki/Criticism_of_Google#Antitrust",
                "/wiki/Censorship_by_Google",
                "/wiki/Criticism_of_Google#Page_Rank",
                "/wiki/Mission_statement",
                "/wiki/Don%27t_be_evil",
                "/wiki/Google_Search",
                "/wiki/Google_Docs,_Sheets,_and_Slides",
                "/wiki/Gmail",
                "/wiki/Inbox_by_Gmail",
                "/wiki/Google_Calendar",
                "/wiki/Google_Drive",
                "/wiki/Google%2B",
                "/wiki/Google_Allo",
                "/wiki/Google_Duo",
                "/wiki/Hangouts",
                "/wiki/Google_Translate",
                "/wiki/Google_Maps",
                "/wiki/Waze",
                "/wiki/Google_Earth",
                "/wiki/Street_View",
                "/wiki/YouTube",
                "/wiki/Google_Keep",
                "/wiki/Google_Photos",
                "/wiki/Android_(operating_system)",
                "/wiki/Google_Chrome",
                "/wiki/Chrome_OS",
                "/wiki/Google_Nexus",
                "/wiki/Pixel_(smartphone)",
                "/wiki/Google_Home",
                "/wiki/Google_Wifi",
                "/wiki/Google_Daydream#Headset",
                "/wiki/Google_Fiber",
                "/wiki/Project_Fi",
                "/wiki/Larry_Page",
                "/wiki/Sergey_Brin",
                "/wiki/Stanford_University",
                "/wiki/California",
                "/wiki/Googleplex",
                "/wiki/Alphabet_Inc.",
                "/wiki/Sundar_Pichai",
                "/wiki/Chief_executive_officer"
            ],
            "links": [
                "/wiki/United_States_Department_of_Defense",
                "/wiki/Drone_strike",
                "/wiki/Sundar_Pichai",
                "/wiki/New_America_(organization)",
                "/wiki/European_Union_vs._Google",
                "/wiki/Google_memo",
                "/wiki/David_Brooks_(political_commentator)",
                "/wiki/PRISM_(surveillance_program)",
                "/wiki/Mass_surveillance",
                "/wiki/Mission_statement",
                "/wiki/Don%27t_be_evil",
                "/wiki/Criticism_of_Google",
                "/wiki/Google_tax_avoidance",
                "/wiki/Criticism_of_Google#Page_rank",
                "/wiki/Criticism_of_Google#Copyright_issues",
                "/wiki/Censorship_by_Google",
                "/wiki/Google_privacy",
                "/wiki/Intellectual_property",
                "/wiki/Data_collection",
                "/wiki/Internet_privacy",
                "/wiki/Energy_consumption",
                "/wiki/Monopoly",
                "/wiki/Restraint_of_trade",
                "/wiki/Anti-competitive_practices",
                "/wiki/Patent_infringement",
                "/wiki/Euro",
                "/wiki/European_Union",
                "/wiki/New_Scientist",
                "/wiki/High-Tech_Employee_Antitrust_Litigation",
                "/wiki/Lobbying_in_the_United_States",
                "/wiki/Jim_Inhofe",
                "/wiki/American_Legislative_Exchange_Council",
                "/wiki/Competitive_Enterprise_Institute",
                "/wiki/Libertarianism",
                "/wiki/Think_tank",
                "/wiki/Matt_Brittin",
                "/wiki/Public_Accounts_Committee_(United_Kingdom)",
                "/wiki/Tax_avoidance",
                "/wiki/List_of_the_largest_information_technology_companies",
                "/wiki/Ireland",
                "/wiki/Netherlands",
                "/wiki/Bermuda",
                "/wiki/Transfer_pricing",
                "/wiki/Mathematical_Sciences_Research_Institute",
                "/wiki/International_Mathematical_Olympiad",
                "/wiki/Legalize_Love",
                "/wiki/Gay_rights",
                "/wiki/Google.org",
                "/wiki/Climate_change",
                "/wiki/Plug-in_hybrid",
                "/wiki/Electric_vehicle",
                "/wiki/Larry_Brilliant",
                "/wiki/Anagram",
                "/wiki/Kayak",
                "/wiki/Pacific_Ocean",
                "/wiki/FIFA_World_Cup_2010",
                "/wiki/FIFA_World_Cup",
                "/wiki/FIFA",
                "/wiki/Easter_egg_(media)",
                "/wiki/Swedish_Chef",
                "/wiki/Pig_Latin",
                "/wiki/Leet",
                "/wiki/Elmer_Fudd",
                "/wiki/International_Talk_Like_a_Pirate_Day",
                "/wiki/Klingon_language",
                "/wiki/Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything",
                "/wiki/Douglas_Adams",
                "/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy",
                "/wiki/Topeka,_Kansas",
                "/wiki/Google_Fiber",
                "/wiki/Gmail_Motion",
                "/wiki/April_Fools%27_Day",
                "/wiki/Google%27s_hoaxes#2000",
                "/wiki/TiSP",
                "/wiki/Optical_fiber",
                "/wiki/Gmail_Paper",
                "/wiki/Homepage",
                "/wiki/Holiday",
                "/wiki/Doodle",
                "/wiki/Burning_Man",
                "/wiki/Larry_Page",
                "/wiki/Sergey_Brin",
                "/wiki/Intern",
                "/wiki/Dennis_Hwang",
                "/wiki/Bastille_Day",
                "/wiki/Hyderabad",
                "/wiki/London",
                "/wiki/Camden_London_Borough_Council",
                "/wiki/Sydney",
                "/wiki/Google_Maps",
                "/wiki/London",
                "/wiki/Android_(operating_system)",
                "/wiki/Solar_panel",
                "/wiki/Watt",
                "/wiki/Rooftop_photovoltaic_power_station",
                "/wiki/Goats",
                "/wiki/Carbon_footprint",
                "/wiki/Bob_Widlar",
                "/wiki/National_Semiconductor",
                "/wiki/Harper%27s_Magazine",
                "/wiki/Don%27t_be_evil",
                "/wiki/Energy-saving",
                "/wiki/Ann_Arbor,_Michigan",
                "/wiki/Carnegie_Mellon",
                "/wiki/Pittsburgh",
                "/wiki/Smartphone_applications",
                "/wiki/Atlanta,_Georgia",
                "/wiki/Austin,_Texas",
                "/wiki/Boulder,_Colorado",
                "/wiki/Cambridge,_Massachusetts",
                "/wiki/San_Francisco",
                "/wiki/California",
                "/wiki/Seattle,_Washington",
                "/wiki/Reston,_Virginia",
                "/wiki/Washington,_D.C.",
                "/wiki/111_Eighth_Avenue",
                "/wiki/Mountain_View,_California",
                "/wiki/Googleplex",
                "/wiki/Googolplex",
                "/wiki/Lava_lamp",
                "/wiki/Table_football",
                "/wiki/Pro_rata",
                "/wiki/Marissa_Mayer",
                "/wiki/Intel",
                "/wiki/Diane_Bryant",
                "/wiki/Google_Cloud_Platform",
                "/wiki/Sheryl_Sandberg",
                "/wiki/Tim_Armstrong_(executive)",
                "/wiki/AOL",
                "/wiki/Marissa_Mayer",
                "/wiki/Yahoo!",
                "/wiki/One-dollar_salary",
                "/wiki/Fortune_(magazine)",
                "/wiki/E_notation",
                "/wiki/Google_Labs",
                "/wiki/Google_Developers",
                "/wiki/Software_development",
                "/wiki/Google_APIs",
                "/wiki/Application_programming_interface",
                "/wiki/Google_Services",
                "/wiki/Google_Photos",
                "/wiki/Google_Keep",
                "/wiki/Google_Calendar",
                "/wiki/Artificial_intelligence",
                "/wiki/Machine_learning",
                "/wiki/Stock_photography",
                "/wiki/Android_Nougat",
                "/wiki/DeepDream",
                "/wiki/Convolutional_neural_network",
                "/wiki/Google_Alerts",
                "/wiki/Change_detection_and_notification",
                "/wiki/Search_engine",
                "/wiki/Google_Shopping_Express",
                "/wiki/Google_Wallet",
                "/wiki/Google_News",
                "/wiki/Agence_France_Presse",
                "/wiki/The_Verge",
                "/wiki/Pune",
                "/wiki/Project_Fi",
                "/wiki/Google_Fiber",
                "/wiki/Alphabet_Inc.",
                "/wiki/Recode",
                "/wiki/Motorola_Mobility",
                "/wiki/Google_Cardboard",
                "/wiki/Virtual_reality",
                "/wiki/Chromecast",
                "/wiki/Chromebook",
                "/wiki/Nexus_One",
                "/wiki/Pixel_(smartphone)",
                "/wiki/Google_Chrome",
                "/wiki/Chrome_OS",
                "/wiki/Android_(operating_system)",
                "/wiki/Mobile_operating_system",
                "/wiki/Android_Wear",
                "/wiki/Android_TV",
                "/wiki/Android_Auto",
                "/wiki/Internet_of_things",
                "/wiki/Android_Things",
                "/wiki/Gmail",
                "/wiki/Inbox_by_Gmail",
                "/wiki/Email",
                "/wiki/Google_Calendar",
                "/wiki/Google_Maps",
                "/wiki/Satellite_imagery",
                "/wiki/Google_Drive",
                "/wiki/File_hosting_service",
                "/wiki/Google_Docs,_Sheets_and_Slides",
                "/wiki/Google_Photos",
                "/wiki/Google_Keep",
                "/wiki/Note-taking",
                "/wiki/Google_Translate",
                "/wiki/YouTube",
                "/wiki/Google%2B",
                "/wiki/Google_Allo",
                "/wiki/Google_Duo",
                "/wiki/Google_for_Entrepreneurs",
                "/wiki/Business_incubator",
                "/wiki/Coworking",
                "/wiki/Berlin",
                "/wiki/London",
                "/wiki/Madrid",
                "/wiki/Seoul",
                "/wiki/S%C3%A3o_Paulo",
                "/wiki/Tel_Aviv",
                "/wiki/Warsaw",
                "/wiki/The_Wall_Street_Journal",
                "/wiki/Adobe_Systems",
                "/wiki/Oracle_Corporation",
                "/wiki/Salesforce.com",
                "/wiki/IBM",
                "/wiki/Google_Search_Appliance",
                "/wiki/Google_Mini",
                "/wiki/G_Suite",
                "/wiki/Gmail",
                "/wiki/Google_Drive",
                "/wiki/Google_Docs,_Sheets_and_Slides",
                "/wiki/Gmail",
                "/wiki/Google_Photos",
                "/wiki/Interstitial_ad",
                "/wiki/Bing_(search_engine)",
                "/wiki/Thumbnail",
                "/wiki/Google_Books",
                "/wiki/Authors_Guild",
                "/wiki/%C3%89ditions_du_Seuil",
                "/wiki/Amazon.com",
                "/wiki/The_New_York_Times",
                "/wiki/Search_engine_indexing",
                "/wiki/Web_cache",
                "/wiki/Field_v._Google",
                "/wiki/Nevada",
                "/wiki/2600:_The_Hacker_Quarterly",
                "/wiki/Google_Instant",
                "/wiki/ComScore",
                "/wiki/Google_Search",
                "/wiki/Market_share",
                "/wiki/Search_engine_indexing",
                "/wiki/Operator_(computer_programming)",
                "/wiki/Competition_law",
                "/wiki/United_States_Department_of_Justice",
                "/wiki/Click_fraud",
                "/wiki/Google_Analytics",
                "/wiki/AdWords",
                "/wiki/AdSense",
                "/wiki/AdSense_for_Mobile",
                "/wiki/DoubleClick",
                "/wiki/Limited_liability_company",
                "/wiki/Conglomerate_(company)",
                "/wiki/Alphabet_Inc.",
                "/wiki/Sundar_Pichai",
                "/wiki/Larry_Page",
                "/wiki/The_Washington_Post",
                "/wiki/National_Security_Agency",
                "/wiki/Muscular_(surveillance_program)",
                "/wiki/Singapore",
                "/wiki/Taiwan",
                "/wiki/Google_Data_Centers",
                "/wiki/HTC",
                "/wiki/Lenovo",
                "/wiki/DeepMind_Technologies",
                "/wiki/London",
                "/wiki/Recode",
                "/wiki/Waze",
                "/wiki/Google_Maps",
                "/wiki/Apple_Inc.",
                "/wiki/Microsoft",
                "/wiki/Android_(operating_system)",
                "/wiki/Flextronics",
                "/wiki/Arris_Group",
                "/wiki/Motorola_Mobility",
                "/wiki/People%27s_Republic_of_China",
                "/wiki/The_Globe_and_Mail",
                "/wiki/Nortel_Networks",
                "/wiki/Global_IP_Solutions",
                "/wiki/Teleconferencing",
                "/wiki/AdMob",
                "/wiki/Federal_Trade_Commission",
                "/wiki/Google_Energy",
                "/wiki/Renewable_energy",
                "/wiki/Wind_farm",
                "/wiki/North_Dakota",
                "/wiki/NextEra_Energy_Resources",
                "/wiki/Federal_Energy_Regulatory_Commission",
                "/wiki/GeoEye",
                "/wiki/Vandenberg_Air_Force_Base",
                "/wiki/Life_(magazine)",
                "/wiki/NORAD_Tracks_Santa",
                "/wiki/Santa_Claus",
                "/wiki/Christmas_Eve",
                "/wiki/AOL",
                "/wiki/Video_search",
                "/wiki/Fox_Sports_Digital_Media",
                "/wiki/News_Corporation",
                "/wiki/MySpace",
                "/wiki/NASA",
                "/wiki/Ames_Research_Center",
                "/wiki/DoubleClick",
                "/wiki/Microsoft",
                "/wiki/AT%26T",
                "/wiki/Urchin_(software)",
                "/wiki/Google_Analytics",
                "/wiki/Keyhole,_Inc",
                "/wiki/Eponym",
                "/wiki/Google_Earth",
                "/wiki/AdSense#History",
                "/wiki/AdSense",
                "/wiki/Google_Groups#Deja_News",
                "/wiki/Usenet",
                "/wiki/Google_Groups",
                "/wiki/Alexa_Internet",
                "/wiki/Terabyte",
                "/wiki/Interbrand",
                "/wiki/Apple_Inc.",
                "/wiki/Alliance_for_Affordable_Internet",
                "/wiki/Facebook",
                "/wiki/Intel",
                "/wiki/Microsoft",
                "/wiki/Sir_Tim_Berners-Lee",
                "/wiki/Doodle",
                "/wiki/Yahoo!_Search",
                "/wiki/Calico_(company)",
                "/wiki/Apple,_Inc.",
                "/wiki/Arthur_D._Levinson",
                "/wiki/The_Washington_Post",
                "/wiki/Petabyte",
                "/wiki/CNN",
                "/wiki/Silicon_Graphics",
                "/wiki/Mountain_View,_California",
                "/wiki/Googleplex",
                "/wiki/Googolplex",
                "/wiki/Googleplex",
                "/wiki/Clive_Wilkinson",
                "/wiki/Google_(verb)",
                "/wiki/Merriam-Webster",
                "/wiki/Oxford_English_Dictionary",
                "/wiki/Pop_culture",
                "/wiki/Buffy_the_Vampire_Slayer",
                "/wiki/Idealab",
                "/wiki/Bill_T._Gross",
                "/wiki/Yahoo!_Search_Marketing",
                "/wiki/Palo_Alto,_California",
                "/wiki/Silicon_Valley",
                "/wiki/Online_advertising",
                "/wiki/Mutual_fund",
                "/wiki/Class_C_share",
                "/wiki/Class_A_share",
                "/wiki/NASDAQ",
                "/wiki/Ticker_symbol",
                "/wiki/Frankfurt_Stock_Exchange",
                "/wiki/The_New_York_Times",
                "/wiki/Sexism",
                "/wiki/Ageism",
                "/wiki/High-Tech_Employee_Antitrust_Litigation",
                "/wiki/Silicon_Valley",
                "/wiki/Morgan_Stanley",
                "/wiki/Credit_Suisse",
                "/wiki/Market_capitalization",
                "/wiki/Yahoo!",
                "/wiki/Initial_public_offering",
                "/wiki/Eric_Schmidt",
                "/wiki/Excite",
                "/wiki/Vinod_Khosla",
                "/wiki/Venture_capital",
                "/wiki/Kleiner_Perkins_Caufield_%26_Byers",
                "/wiki/Sequoia_Capital",
                "/wiki/Andy_Bechtolsheim",
                "/wiki/Sun_Microsystems",
                "/wiki/Angel_investor",
                "/wiki/Amazon.com",
                "/wiki/Jeff_Bezos",
                "/wiki/David_Cheriton",
                "/wiki/Ram_Shriram",
                "/wiki/Susan_Wojcicki",
                "/wiki/Menlo_Park,_California",
                "/wiki/Craig_Silverstein",
                "/wiki/Googol",
                "/wiki/PageRank",
                "/wiki/Relevance_(information_retrieval)",
                "/wiki/Larry_Page",
                "/wiki/Sergey_Brin",
                "/wiki/Stanford_University",
                "/wiki/Stanford,_California",
                "/wiki/Alexa_Internet",
                "/wiki/YouTube",
                "/wiki/Blogger_(service)",
                "/wiki/Criticism_of_Google",
                "/wiki/Privacy_concerns_regarding_Google",
                "/wiki/Criticism_of_Google#Aggressive_tax_avoidance",
                "/wiki/Criticism_of_Google#Antitrust",
                "/wiki/Censorship_by_Google",
                "/wiki/Criticism_of_Google#Page_Rank",
                "/wiki/Mission_statement",
                "/wiki/Don%27t_be_evil",
                "/wiki/Google_Search",
                "/wiki/Google_Docs,_Sheets,_and_Slides",
                "/wiki/Gmail",
                "/wiki/Inbox_by_Gmail",
                "/wiki/Google_Calendar",
                "/wiki/Google_Drive",
                "/wiki/Google%2B",
                "/wiki/Google_Allo",
                "/wiki/Google_Duo",
                "/wiki/Hangouts",
                "/wiki/Google_Translate",
                "/wiki/Google_Maps",
                "/wiki/Waze",
                "/wiki/Google_Earth",
                "/wiki/Street_View",
                "/wiki/YouTube",
                "/wiki/Google_Keep",
                "/wiki/Google_Photos",
                "/wiki/Android_(operating_system)",
                "/wiki/Google_Chrome",
                "/wiki/Chrome_OS",
                "/wiki/Google_Nexus",
                "/wiki/Pixel_(smartphone)",
                "/wiki/Google_Home",
                "/wiki/Google_Wifi",
                "/wiki/Google_Daydream#Headset",
                "/wiki/Google_Fiber",
                "/wiki/Project_Fi",
                "/wiki/Larry_Page",
                "/wiki/Sergey_Brin",
                "/wiki/Stanford_University",
                "/wiki/California",
                "/wiki/Googleplex",
                "/wiki/Alphabet_Inc.",
                "/wiki/Sundar_Pichai",
                "/wiki/Chief_executive_officer"
            ],
            "text": "In 2017, David Elliot and Chris Gillespie argued before the Ninth Circuit of the United States Court of Appeals that \"google\" had suffered genericide. The controversy began in 2012 when Gillespie acquired 763 domain names containing the word \"google.\" Google promptly filed a complaint with the NAF. Elliot then filed a petition for cancelling the Google trademark. Ultimately, the court ruled in favor of Google because Elliot failed to show a preponderance of evidence showing the genericide of \"google.\"[369]Google was working with the United States Department of Defense on drone software called \"Project Maven\" that could be used to improve the accuracy of drone strikes.[367] Thousands of Google employees, including senior engineers, have signed a letter urging Google CEO Sundar Pichai to end a controversial contract with the Pentagon.[368]Reportedly, Google's influenced New America think tank to expel their Open Markets research group, after the group has criticized Google monopolistic power and supported the EU $2.7B fine of Google.[365][366]On August 8, 2017, Google fired employee James Damore after he distributed a memo throughout the company which argued that \"Google's ideological echo chamber\" and bias clouded their thinking about diversity and inclusion, and that it is also biological factors, not discrimination alone, that cause the average woman to be less interested than men in technical positions.[359] Google CEO Sundar Pichai accused Damore in violating company policy by \"advancing harmful gender stereotypes in our workplace\", and he was fired on the same day.[360][361][362] New York Times columnist David Brooks argued Pichai had mishandled the case, and called for his resignation.[363][364]Following media reports about PRISM, NSA's massive electronic surveillance program, in June 2013, several technology companies were identified as participants, including Google.[357] According to leaks of said program, Google joined the PRISM program in 2009.[358]Google's commitment to such robust idealism has been increasingly called into doubt due to a number of the firm's actions and behaviours which appear to contradict this.[355][356]Google's mission statement, from the outset, was \"to organize the world's information and make it universally accessible and useful\",[352] and its unofficial slogan is \"Don't be evil\".[353] In October 2015, a related motto was adopted in the Alphabet corporate code of conduct by the phrase: \"Do the right thing\".[354] The original motto was retained in the code of conduct of Google, now a subsidiary of Alphabet.[7]Google's market dominance has led to prominent media coverage, including criticism of the company over issues such as aggressive tax avoidance,[348] search neutrality, copyright, censorship of search results and content,[349] and privacy.[350][351] Other criticisms include alleged misuse and manipulation of search results, its use of others' intellectual property, concerns that its compilation of data may violate people's privacy, and the energy consumption of its servers, as well as concerns over traditional business issues such as monopoly, restraint of trade, anti-competitive practices, and patent infringement.On June 27, 2017, the company received a record fine of \u20ac2.42 billion from the European Union for \"promoting its own shopping comparison service at the top of search results.\"[346] Commenting on the penalty, New Scientist magazine said: \"The hefty sum \u2013 the largest ever doled out by the EU's competition regulators \u2013 will sting in the short term, but Google can handle it. Alphabet, Google\u2019s parent company, made a profit of $2.5 billion (\u20ac2.2 billion) in the first six weeks of 2017 alone. The real impact of the ruling is that Google must stop using its dominance as a search engine to give itself the edge in another market: online price comparisons.\" The company disputed the ruling.[347]Google has been involved in a number of lawsuits including the High-Tech Employee Antitrust Litigation which resulted in Google being one of four companies to pay a $415 million settlement to employees.[345]In 2013, Google ranked 5th in lobbying spending, up from 213th in 2003. In 2012, the company ranked 2nd in campaign donations of technology and Internet sections.[344]In November 2017, Google bought 536 megawatts of wind power. The purchase made the firm reach 100% renewable energy. The wind energy comes from two power plants in South Dakota, one in Iowa and one in Oklahoma.[343]In July 2013, it was reported that Google had hosted a fundraising event for Oklahoma Senator Jim Inhofe, who has called climate change a \"hoax\".[341] In 2014 Google cut ties with the American Legislative Exchange Council (ALEC) after pressure from the Sierra Club, major unions and Google's own scientists because of ALEC's stance on climate change and opposition to renewable energy.[342]In June 2013, The Washington Post reported that Google had donated $50,000 to the Competitive Enterprise Institute, a libertarian think tank that calls human carbon emissions a positive factor in the environment and argues that global warming is not a concern.[340]In 2007, Google launched a project centered on developing renewable energy, titled the \"Renewable Energy Cheaper than Coal (RE<C)\" project.[338] However, the project was cancelled in 2014, after engineers Ross Koningstein and David Fork understood, after years of study, that \"best-case scenario, which was based on our most optimistic forecasts for renewable energy, would still result in severe climate change\", writing that they \"came to the conclusion that even if Google and others had led the way toward a wholesale adoption of renewable energy, that switch would not have resulted in significant reductions in carbon dioxide emissions\".[339]Google disclosed in September 2011 that it \"continuously uses enough electricity to power 200,000 homes\", almost 260 million watts or about a quarter of the output of a nuclear power plant. Total carbon emissions for 2010 were just under 1.5 million metric tons, mostly due to fossil fuels that provide electricity for the data centers. Google said that 25 percent of its energy was supplied by renewable fuels in 2010. An average search uses only 0.3 watt-hours of electricity, so all global searches are only 12.5 million watts or 5% of the total electricity consumption by Google.[337]Since 2007, Google has aimed for carbon neutrality in regard to its operations.[336]Google Vice President Matt Brittin testified to the Public Accounts Committee of the UK House of Commons that his UK sales team made no sales and hence owed no sales taxes to the UK.[334] In January 2016, Google reached a settlement with the UK to pay \u00a3130m in back taxes plus higher taxes in future.[335]Following criticism of the amount of corporate taxes that Google paid in the United Kingdom, Chairman Eric Schmidt said, \"It's called capitalism. We are proudly capitalistic.\" During the same December 2012 interview, Schmidt confirmed that the company had no intention of paying more to the UK exchequer.[333]Google uses various tax avoidance strategies. Out of the five largest American technology companies, it pays the lowest taxes to the countries of origin of its revenues. Google between 2007 and 2010 saved $3.1 billion in taxes by shuttling non-U.S. profits through Ireland and the Netherlands and then to Bermuda. Such techniques lower its non-U.S. tax rate to 2.3 per cent, while normally the corporate tax rate in for instance the UK is 28 per cent.[331] This has reportedly sparked a French investigation into Google's transfer pricing practices.[332]In March 2007, in partnership with the Mathematical Sciences Research Institute (MSRI), Google hosted the first Julia Robinson Mathematics Festival at its headquarters in Mountain View.[327] In 2011, Google donated 1\u00a0million euros to International Mathematical Olympiad to support the next five annual International Mathematical Olympiads (2011\u20132015).[328][329] In July 2012, Google launched a \"Legalize Love\" campaign in support of gay rights.[330]In 2008, Google announced its \"project 10100\" which accepted ideas for how to help the community and then allowed Google users to vote on their favorites.[324] After two years of silence, during which many wondered what had happened to the program,[325] Google revealed the winners of the project, giving a total of ten million dollars to various ideas ranging from non-profit organizations that promote education to a website that intends to make all legal documents public and online.[326]In 2004, Google formed the not-for-profit philanthropic Google.org, with a start-up fund of $1\u00a0billion.[321] The mission of the organization is to create awareness about climate change, global public health, and global poverty. One of its first projects was to develop a viable plug-in hybrid electric vehicle that can attain 100 miles per gallon. Google hired Larry Brilliant as the program's executive director in 2004,[322] and the current director is Megan Smith.[323]When searching for the word \"anagram,\" meaning a rearrangement of letters from one word to form other valid words, Google's suggestion feature displays \"Did you mean: nag a ram?\"[319] In Google Maps, searching for directions between places separated by large bodies of water, such as Los Angeles and Tokyo, results in instructions to \"kayak across the Pacific Ocean.\" During FIFA World Cup 2010, search queries including \"World Cup\" and \"FIFA\" caused the \"Goooo...gle\" page indicator at the bottom of every result page to read \"Goooo...al!\" instead.[320]Google's services contain easter eggs, such as the Swedish Chef's \"Bork bork bork,\" Pig Latin, \"Hacker\" or leetspeak, Elmer Fudd, Pirate, and Klingon as language selections for its search engine.[316] The search engine calculator provides the Answer to the Ultimate Question of Life, the Universe, and Everything from Douglas Adams' The Hitchhiker's Guide to the Galaxy.[317] When searching the word \"recursion\", the spell-checker's result for the properly spelled word is exactly the same word, creating a recursive link.[318]In 2010, Google changed its company name to Topeka in honor of Topeka, Kansas, whose mayor changed the city's name to Google for a short amount of time in an attempt to sway Google's decision in its new Google Fiber Project.[313][314] In 2011, Google announced Gmail Motion, an interactive way of controlling Gmail and the computer with body movements via the user's webcam.[315]Google has a tradition of creating April Fools' Day jokes. On April 1, 2000, Google MentalPlex allegedly featured the use of mental power to search the web.[309] In 2007, Google announced a free Internet service called TiSP, or Toilet Internet Service Provider, where one obtained a connection by flushing one end of a fiber-optic cable down their toilet.[310] Also in 2007, Google's Gmail page displayed an announcement for Gmail Paper, allowing users to have email messages printed and shipped to them.[311] In 2008, Google announced Gmail Custom time where users could change the time that the email was sent.[312]Since 1998, Google has been designing special, temporary alternate logos to place on their homepage intended to celebrate holidays, events, achievements and people. The first Google Doodle was in honor of the Burning Man Festival of 1998.[306][307] The doodle was designed by Larry Page and Sergey Brin to notify users of their absence in case the servers crashed. Subsequent Google Doodles were designed by an outside contractor, until Larry and Sergey asked then-intern Dennis Hwang to design a logo for Bastille Day in 2000. From that point onward, Doodles have been organized and created by a team of employees termed \"Doodlers\".[308]In May 2015, Google announced its intention to create its own campus in Hyderabad, India. The new campus, reported to be the company's largest outside the United States, will accommodate 13,000 employees.[304][305]In November 2013, Google announced plans for a new London headquarter, a notable 1 million square foot office able to accommodate 4,500 employees. Recognized as one of the biggest ever commercial property acquisitions at the time of the deal's announcement in January,[301] Google submitted plans for the new headquarter to the Camden Council in June 2017. The new building, if approved, will feature a rooftop garden with a running track, giant moving blinds, a swimming pool, and a multi-use games area for sports.[302][303]Internationally, Google has over 78 offices in more than 50 countries.[298] It also has product research and development operations in cities around the world, namely Sydney (birthplace location of Google Maps)[299] and London (part of Android development).[300]In October 2006, the company announced plans to install thousands of solar panels to provide up to 1.6\u00a0megawatts of electricity, enough to satisfy approximately 30% of the campus' energy needs.[293] The system will be the largest solar power system constructed on a U.S. corporate campus and one of the largest on any corporate site in the world.[293] In addition, Google announced in 2009 that it was deploying herds of goats to keep grassland around the Googleplex short, helping to prevent the threat from seasonal bush fires while also reducing the carbon footprint of mowing the extensive grounds.[294][295] The idea of trimming lawns using goats originated from Bob Widlar, an engineer who worked for National Semiconductor.[296] In 2008, Google faced accusations in Harper's Magazine of being an \"energy glutton\". The company was accused of employing its \"Don't be evil\" motto and its public energy-saving campaigns to cover up or make up for the massive amounts of energy its servers require.[297]By late 2006, Google established a new headquarters for its AdWords division in Ann Arbor, Michigan.[289] In November 2006, Google opened offices on Carnegie Mellon's campus in Pittsburgh, focusing on shopping-related advertisement coding and smartphone applications and programs.[290][291] Other office locations in the U.S. include Atlanta, Georgia; Austin, Texas; Boulder, Colorado; Cambridge, Massachusetts; San Francisco, California; Seattle, Washington; Reston, Virginia, and Washington, D.C.[292]In 2006, Google moved into about 300,000 square feet (27,900\u00a0m2) of office space in New York City, at 111 Eighth Avenue in Manhattan. The office was designed and built specially for Google, and houses its largest advertising sales team, which has been instrumental in securing large partnerships.[284] The New York headquarters includes a game room, micro-kitchens, and a video game area.[285] In 2010, Google bought the building housing the headquarter, in a deal that valued the property at around $1.9 billion, the biggest for a single building in the United States that year.[286][287] In February 2012, Google moved additional employees to the New York City campus, with a total of around 2,750 employees.[288]Google's extensive amenities are not available to all of its workers. Temporary workers such as book scanners do not have access to shuttles, Google cafes, or other perks.[283]Google's headquarters in Mountain View, California is referred to as \"the Googleplex\", a play on words on the number googolplex and the headquarters itself being a complex of buildings. The lobby is decorated with a piano, lava lamps, old server clusters, and a projection of search queries on the wall. The hallways are full of exercise balls and bicycles. Many employees have access to the corporate recreation center. Recreational amenities are scattered throughout the campus and include a workout room with weights and rowing machines, locker rooms, washers and dryers, a massage room, assorted video games, table football, a baby grand piano, a billiard table, and ping pong. In addition to the recreation room, there are snack rooms stocked with various foods and drinks, with special emphasis placed on nutrition.[281] Free food is available to employees 24/7, with the offerings provided by paid vending machines prorated based on and favoring those of better nutritional value.[282]As a motivation technique, Google uses a policy often called Innovation Time Off, where Google engineers are encouraged to spend 20% of their work time on projects that interest them. Some of Google's services, such as Gmail, Google News, Orkut, and AdSense originated from these independent endeavors.[279] In a talk at Stanford University, Marissa Mayer, Google's Vice President of Search Products and User Experience until July 2012, showed that half of all new product launches in the second half of 2005 had originated from the Innovation Time Off.[280]In 2017 former Intel executive Diane Bryant became Chief Operating Officer of Google Cloud.[277]In March 2008, Sheryl Sandberg, then vice-president of global online sales and operations, began her position as chief operating officer of Facebook.[274][275] In 2009, early employee Tim Armstrong left to become CEO of AOL. In July 2012, Google's first female engineer, Marissa Mayer, left Google to become Yahoo!'s CEO.[276]After the company's IPO in 2004, founders Sergey Brin and Larry Page and CEO Eric Schmidt requested that their base salary be cut to $1. Subsequent offers by the company to increase their salaries were turned down, primarily because their main compensation continues to come from owning stock in Google. Before 2004, Schmidt made $250,000 per year, and Page and Brin each received an annual salary of $150,000.[273]Google's employees are hired based on a hierarchical system. Employees are split into six hierarchies based on experience and can range \"from entry-level data center workers at level one to managers and experienced engineers at level six.\"[272]As of the second quarter in 2015, Google has 57,100 employees.[269] Google has released that 30 percent of their employees are female, and 70 percent are male.[270] A March 2013 report detailed that it had 10,000 developers based in more than 40 offices.[271]On Fortune magazine's list of the best companies to work for, Google ranked first in 2007, 2008 and 2012[262][263][264] and fourth in 2009 and 2010.[265][266] Google was also nominated in 2010 to be the world's most attractive employer to graduating students in the Universum Communications talent attraction index.[267] Google's corporate philosophy includes principles such as \"you can make money without doing evil,\" \"you can be serious without a suit,\" and \"work should be challenging and the challenge should be fun.\"[268]In June 2017, Google launched \"We Wear Culture\", a searchable archive of 3,000 years of global fashion. The archive, a result of collaboration between Google and over 180 museums, schools, fashion institutes, and other organizations, also offers curated exhibits of specific fashion topics and their impact on society.[260][261]In March 2017, Google launched a new website, opensource.google.com, to publish its internal documentation for Google Open Source projects.[258][259]Google owns the top-level domain 1e100.net which is used for some servers within Google's network. The name is a reference to the scientific E notation representation for 1 googol, 1E100 = 1 \u00d7 10100.[257]Google Labs was a page created by Google to demonstrate and test new projects.Google Developers is Google's site for software development tools, APIs, and technical resources. The site contains documentation on using Google developer tools and APIs\u2014including discussion groups and blogs for developers using Google's developer products.Google APIs are a set of application programming interfaces (APIs) developed by Google which allow communication with Google Services and their integration to other services. Examples of these include Search, Gmail, Translate or Google Maps. Third-party apps can use these APIs to take advantage of or extend the functionality of the existing services.In May 2017, Google added \"Family Groups\" to several of its services. The feature, which lets users create a group consisting of their family members' individual Google accounts, lets users add their \"Family Group\" as a collaborator to shared albums in Google Photos, shared notes in Google Keep, and common events in Google Calendar. At announcement, the feature is limited to Australia, Brazil, Canada, France, Germany, Ireland, Italy, Japan, Mexico, New Zealand, Russia, Spain, United Kingdom and United States.[255][256]In April 2017, Google launched AutoDraw, a web-based tool using artificial intelligence and machine learning to recognize users' drawings and replace scribbles with related stock images that have been created by professional artists.[251][252][253] The tool is built using the same technology as QuickDraw, an experimental game from Google's Creative Lab where users were tasked with drawing objects that algorithms would recognize within 20 seconds.[254]Google introduced its Family Link service in March 2017, letting parents buy Android Nougat-based Android devices for kids under 13 years of age and create a Google account through the app, with the parents controlling the apps installed, monitor the time spent using the device, and setting a \"Bedtime\" feature that remotely locks the device.[248][249][250]In July 2015 Google released DeepDream, an image recognition software capable of creating psychedelic images using a convolutional neural network.[245][246][247]Google Alerts is a content change detection and notification service, offered by the search engine company Google. The service sends emails to the user when it finds new results\u2014such as web pages, newspaper articles, or blogs\u2014that match the user's search term.[242][243][244]In 2013, Google launched Google Shopping Express, a delivery service initially available only in San Francisco and Silicon Valley.[241]In May 2011, Google announced Google Wallet, a mobile application for wireless payments.[240]Google launched its Google News service in 2002, an automated service which summarizes news articles from various websites.[238] In March 2005, Agence France Presse (AFP) sued Google for copyright infringement in federal court in the District of Columbia, a case which Google settled for an undisclosed amount in a pact that included a license of the full text of AFP articles for use on Google News.[239]In September 2016, Google began its Google Station initiative, a project for public Wi-Fi at railway stations in India. Caesar Sengupta, VP for Google's next billion users, told The Verge that 15,000 people get online for the first time thanks to Google Station and that 3.5 million people use the service every month. The expansion meant that Google was looking for partners around the world to further develop the initiative, which promised \"high-quality, secure, easily accessible Wi-Fi\".[234] By December, Google Station had been deployed at 100 railway stations,[235] and in February, Google announced its intention to expand beyond railway stations, with a plan to bring citywide Wi-Fi to Pune.[236][237]In April 2015, Google announced Project Fi, a mobile virtual network operator, that combines Wi-Fi and cellular networks from different telecommunication providers in an effort to enable seamless connectivity and fast Internet signal.[231][232][233]In February 2010, Google announced the Google Fiber project, with experimental plans to build an ultra-high-speed broadband network for 50,000 to 500,000 customers in one or more American cities.[227][228] Following Google's corporate restructure to make Alphabet Inc. its parent company, Google Fiber was moved to Alphabet's Access division.[229][230]In April 2016, Recode reported that Google had hired Rick Osterloh, Motorola Mobility's former President, to head Google's new hardware division.[222] In October 2016, Osterloh stated that \"a lot of the innovation that we want to do now ends up requiring controlling the end-to-end user experience\",[216] and Google announced several hardware platforms:In June 2014, Google announced Google Cardboard, a simple cardboard viewer that lets user place their smartphone in a special front compartment to view virtual reality (VR) media.[220][221]In July 2013, Google introduced the Chromecast dongle, that allows users to stream content from their smartphones to televisions.[218][219]In 2011, the Chromebook was introduced, described as a \"new kind of computer\" running Chrome OS.[217]In January 2010, Google released Nexus One, the first Android phone under its own, \"Nexus\", brand.[214] It spawned a number of phones and tablets under the \"Nexus\" branding[215] until its eventual discontinuation in 2016, replaced by a new brand called, Pixel.[216]It also develops the Google Chrome web browser,[212] and Chrome OS, an operating system based on Chrome.[213]Google develops the Android mobile operating system,[207] as well as its smartwatch,[208] television,[209] car,[210] and Internet of things-enabled smart devices variations.[211]Google offers Gmail, and the newer variant Inbox,[195] for email,[196] Google Calendar for time-management and scheduling,[197] Google Maps for mapping, navigation and satellite imagery,[198] Google Drive for cloud storage of files,[199] Google Docs, Sheets and Slides for productivity,[199] Google Photos for photo storage and sharing,[200] Google Keep for note-taking,[201] Google Translate for language translation,[202] YouTube for video viewing and sharing,[203] and Google+, Allo, and Duo for social interaction.[204][205][206]On September 24, 2012,[193] Google launched Google for Entrepreneurs, a largely not-for-profit business incubator providing startups with co-working spaces known as Campuses, with assistance to startup founders that may include workshops, conferences, and mentorships.[194] Presently, there are 7 Campus locations in Berlin, London, Madrid, Seoul, S\u00e3o Paulo, Tel Aviv, and Warsaw.On March 15, 2016, Google announced the introduction of Google Analytics 360 Suite, \"a set of integrated data and marketing analytics products, designed specifically for the needs of enterprise-class marketers.\" Among other things, the suite is designed to help \"enterprise class marketers\" \"see the complete customer journey\", generate \"useful insights\", and \"deliver engaging experiences to the right people\".[191] Jack Marshall of The Wall Street Journal wrote that the suite competes with existing marketing cloud offerings by companies including Adobe, Oracle, Salesforce, and IBM.[192]Google Search Appliance was launched in February 2002, targeted toward providing search technology for larger organizations.[9] Google launched the Mini three years later, which was targeted at smaller organizations. Late in 2006, Google began to sell Custom Search Business Edition, providing customers with an advertising-free window into Google.com's index. The service was renamed Google Site Search in 2008.[188] Site Search customers were notified by email in late March 2017 that no new licenses for Site Search would be sold after April 1, 2017, but that customer and technical support would be provided for the duration of existing license agreements.[189][190]G Suite is a monthly subscription offering for organizations and businesses to get access to a collection of Google's services, including Gmail, Google Drive and Docs, Sheets, and Slides, with additional administrative tools, unique domain names, and 24/7 support.[187]In May 2017, Google enabled a new \"Personal\" tab in Google Search, letting users search for content in their Google accounts' various services, including email messages from Gmail and photos from Google Photos.[185][186]In August 2016, Google announced two major changes to its mobile search results. The first change removes the \"mobile-friendly\" label that highlighted easy to read pages from its mobile search results page. For the second change, the company, starting on January 10, 2017, will punish mobile pages that show intrusive interstitial advertisements when a user first opens a page. Such pages will also rank lower in Google search results.[184]The \"Hummingbird\" update to the Google search engine was announced in September 2013. The update was introduced over the month prior to the announcement and allows users ask the search engine a question in natural language rather than entering keywords into the search box.[183]On July 21, 2010, in response to Bing, Google updated its image search to display a streaming sequence of thumbnails that enlarge when pointed at. Although web searches still appear in a batch per page format, on July 23, 2010, dictionary definitions for certain English words began appearing above the linked results for web searches.[182]Google also hosts Google Books. The company began scanning books and uploading limited previews, and full books were allowed, into its new book search engine. The Authors Guild, a group that represents 8,000 U.S. authors, filed a class action suit in a New York City federal court against Google in 2005 over this service. Google replied that it is in compliance with all existing and historical applications of copyright laws regarding books.[178] Google eventually reached a revised settlement in 2009 to limit its scans to books from the U.S., the UK, Australia, and Canada.[179] Furthermore, the Paris Civil Court ruled against Google in late 2009, asking it to remove the works of La Martini\u00e8re (\u00c9ditions du Seuil) from its database.[180] In competition with Amazon.com, Google sells digital versions of new books.[181]Google Watch has criticized Google's PageRank algorithms, saying that they discriminate against new websites and favor established sites.[177]In 2003, The New York Times complained about Google's indexing, claiming that Google's caching of content on its site infringed its copyright for the content.[173] In both Field v. Google and Parker v. Google, the United States District Court of Nevada ruled in favor of Google.[174][175] The publication 2600: The Hacker Quarterly has compiled a list of words that google's new instant search feature will not search.[176]According to comScore market research from November 2009, Google Search is the dominant search engine in the United States market, with a market share of 65.6%.[171] Google indexes billions of web pages to allow users to search for the information they desire through the use of keywords and operators.[172]In February 2003, Google stopped showing the advertisements of Oceana, a non-profit organization protesting a major cruise ship's sewage treatment practices. Google cited its editorial policy at the time, stating \"Google does not accept advertising if the ad or site advocates against other individuals, groups, or organizations.\"[168] In June 2008, Google reached an advertising agreement with Yahoo!, which would have allowed Yahoo! to feature Google advertisements on its web pages. The alliance between the two companies was never completely realized because of antitrust concerns by the U.S. Department of Justice. As a result, Google pulled out of the deal in November 2008.[169][170]One of the criticisms of this program is the possibility of click fraud, which occurs when a person or automated script clicks on advertisements without being interested in the product, causing the advertiser to pay money to Google unduly. Industry reports in 2006 claimed that approximately 14 to 20\u00a0 percent of clicks were fraudulent or invalid.[167]Google Analytics allows website owners to track where and how people use their website, for example by examining click rates for all the links on a page.[164] Google advertisements can be placed on third-party websites in a two-part program. Google's AdWords allows advertisers to display their advertisements in the Google content network, through a cost-per-click scheme.[165] The sister service, Google AdSense, allows website owners to display these advertisements on their website and earn money every time ads are clicked.[166]In 2007, Google launched \"AdSense for Mobile\", taking advantage of the emerging mobile advertising market.[163]For the 2006 fiscal year, the company reported $10.492\u00a0billion in total advertising revenues and only $112\u00a0million in licensing and other revenues.[159] In 2011, 96% of Google's revenue was derived from its advertising programs.[160] In addition to its own algorithms for understanding search requests, Google uses technology from the company DoubleClick, to project user interest and target advertising to the search context and the user history.[161][162]On September 1, 2017, Google Inc. announced its plans of restructuring as a limited liability company, Google LLC, as a wholly owned subsidiary of XXVI Holdings Inc., which is formed as a subsidiary of Alphabet Inc. to hold the equity of its other subsidiaries, including Google LLC and other bets.[158]On August 10, 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet. Google became Alphabet's leading subsidiary, and will continue to be the umbrella company for Alphabet's Internet interests. Upon completion of the restructure, Sundar Pichai became CEO of Google, replacing Larry Page, who became CEO of Alphabet.[155][156][157]In December 2016, Google announced that starting in 2017, it will power all of its data centers, as well as all of its offices, from 100% renewable energy. The commitment will make Google \"the world's largest corporate buyer of renewable power, with commitments reaching 2.6 gigawatts (2,600 megawatts) of wind and solar energy\". Google also stated that it does not count that as its final goal; it says that \"since the wind doesn't blow 24 hours a day, we'll also broaden our purchases to a variety of energy sources that can enable renewable power, every hour of every day\". Additionally, the project will \"help support communities\" around the world, as the purchase commitments will \"result in infrastructure investments of more than $3.5 billion globally\", and will \"generate tens of millions of dollars per year in revenue to local property owners, and tens of millions more to local and national governments in tax revenue\".[152][153][154]An August 2011 report estimated that Google had about 900,000 servers in their data centers, based on energy usage. The report does state that \"Google never says how many servers are running in its data centers.\"[151]Google's most efficient data center runs at 95\u00a0\u00b0F (35\u00a0\u00b0C) using only fresh air cooling, requiring no electrically powered air conditioning; the servers run so hot that humans cannot go near them for extended periods.[150]In October 2013, The Washington Post reported that the U.S. National Security Agency intercepted communications between Google's data centers, as part of a program named MUSCULAR.[146][147] This wiretapping was made possible because Google did not encrypt data passed inside its own network.[148] Google began encrypting data sent between data centers in 2013.[149]In 2011, the company had announced plans to build three data centers at a cost of more than $200\u00a0million in Asia (Singapore, Hong Kong and Taiwan) and said they would be operational within two years.[143][144] In December 2013, Google announced that it had scrapped the plan to build a data center in Hong Kong.[145]As of 2016, Google owned and operated nine data centers across North and South America, two in Asia, and four in Europe.[142]On September 21, 2017, HTC announced a \"cooperation agreement\" in which it would sell non-exclusive rights to certain intellectual property, as well as smartphone talent, to Google for $1.1 billion.[139][140][141]On January 29, 2014, Google announced that it would divest Motorola Mobility to Lenovo for $2.91 billion, a fraction of the original $12.5 billion price paid by Google to acquire the company. Google retained all but 2000 of Motorola's patents and entered into cross-licensing deals.[138]On January 26, 2014, Google announced it had agreed to acquire DeepMind Technologies, a privately held artificial intelligence company from London. DeepMind describes itself as having the ability to combine the best techniques from machine learning and systems neuroscience to build general-purpose learning algorithms. DeepMind's first commercial applications were used in simulations, e-commerce and games. As of December 2013, it was reported that DeepMind had roughly 75 employees.[134] Technology news website Recode reported that the company was purchased for $400 million though it was not disclosed where the information came from. A Google spokesman would not comment of the price.[135][136] The purchase of DeepMind aids in Google's recent growth in the artificial intelligence and robotics community.[137]In June 2013, Google acquired Waze, a $966 million deal.[132] While Waze would remain an independent entity, its social features, such as its crowdsourced location platform, were reportedly valuable integrations between Waze and Google Maps, Google's own mapping service.[133]This purchase was made in part to help Google gain Motorola's considerable patent portfolio on mobile phones and wireless technologies, to help protect Google in its ongoing patent disputes with other companies,[125] mainly Apple and Microsoft,[123] and to allow it to continue to freely offer Android.[126] After the acquisition closed, Google began to restructure the Motorola business to fit Google's strategy. On August 13, 2012, Google announced plans to lay off 4000 Motorola Mobility employees.[127] On December 10, 2012, Google sold the manufacturing operations of Motorola Mobility to Flextronics for $75\u00a0million.[128] As a part of the agreement, Flextronics will manufacture undisclosed Android and other mobile devices.[129] On December 19, 2012, Google sold the Motorola Home business division of Motorola Mobility to Arris Group for $2.35\u00a0billion in a cash-and-stock transaction. As a part of this deal, Google acquired a 15.7% stake in Arris Group valued at $300\u00a0million.[130][131]On August 15, 2011, Google made its largest-ever acquisition to-date when it announced that it would acquire Motorola Mobility for $12.5\u00a0billion[121][122] subject to approval from regulators in the United States and Europe. In a post on Google's blog, Google Chief Executive and co-founder Larry Page revealed that the acquisition was a strategic move to strengthen Google's patent portfolio. The company's Android operating system has come under fire in an industry-wide patent battle, as Apple and Microsoft have sued Android device makers such as HTC, Samsung, and Motorola.[123] The merger was completed on May 22, 2012, after the approval of People's Republic of China.[124]On April 4, 2011, The Globe and Mail reported that Google bid $900\u00a0million for 6000 Nortel Networks patents.[120]Also in 2010, Google purchased Global IP Solutions, a Norway-based company that provides web-based teleconferencing and other related services. This acquisition enabled Google to add telephone-style services to its list of products.[116] On May 27, 2010, Google announced it had also closed the acquisition of the mobile ad network AdMob. This occurred days after the Federal Trade Commission closed its investigation into the purchase.[117] Google acquired the company for an undisclosed amount.[118] In July 2010, Google signed an agreement with an Iowa wind farm to buy 114 megawatts of energy for 20 years.[119]In 2010, Google Energy made its first investment in a renewable energy project, putting $38.8\u00a0million into two wind farms in North Dakota. The company announced the two locations will generate 169.5\u00a0megawatts of power, enough to supply 55,000 homes. The farms, which were developed by NextEra Energy Resources, will reduce fossil fuel use in the region and return profits. NextEra Energy Resources sold Google a twenty-percent stake in the project to get funding for its development.[112] In February 2010, the Federal Energy Regulatory Commission FERC granted Google an authorization to buy and sell energy at market rates.[113] The order specifically states that Google Energy\u2014a subsidiary of Google\u2014holds the rights \"for the sale of energy, capacity, and ancillary services at market-based rates\", but acknowledges that neither Google Energy nor its affiliates \"own or control any generation or transmission\" facilities.[114] The corporation exercised this authorization in September 2013 when it announced it would purchase all the electricity produced by the not-yet-built 240-megawatt Happy Hereford wind farm.[115]In 2008, Google developed a partnership with GeoEye to launch a satellite providing Google with high-resolution (0.41\u00a0m monochrome, 1.65\u00a0m color) imagery for Google Earth. The satellite was launched from Vandenberg Air Force Base on September 6, 2008.[109] Google also announced in 2008 that it was hosting an archive of Life Magazine's photographs.[110][111]In 2007, Google began sponsoring NORAD Tracks Santa, displacing the former sponsor AOL. NORAD Tracks Santa purports to follow Santa Claus' progress on Christmas Eve,[106] using Google Earth to \"track Santa\" in 3-D for the first time.[107][108]In 2005 Google partnered with AOL[104] to enhance each other's video search services. In 2006 Google and Fox Interactive Media of News Corporation entered into a $900\u00a0million agreement to provide search and advertising on the then-popular social networking site MySpace.[105]In addition to the many companies Google has purchased, the firm has partnered with other organizations for research, advertising, and other activities. In 2005, Google partnered with NASA Ames Research Center to build 1,000,000 square feet (93,000\u00a0m2) of offices.[103]On April 13, 2007, Google reached an agreement to acquire DoubleClick for $3.1\u00a0billion, transferring to Google valuable relationships that DoubleClick had with Web publishers and advertising agencies.[101] The deal was approved despite anti-trust concerns raised by competitors Microsoft and AT&T.[102]In October 2006, Google announced that it had acquired the video-sharing site YouTube for $1.65\u00a0billion in Google stock,[97][98] and the deal was finalized on November 13, 2006.[99][100]In 2005. Google acquired Urchin Software in April 2005, using their Urchin on Demand product (along with ideas from Adaptive Path's Measure Map) to create Google Analytics in 2006.In 2004, Google acquired Keyhole, Inc.[96] Keyhole's eponymous product was later renamed Google Earth.In April 2003, Google acquired Applied Semantics, a company specializing in making software applications for the online advertising space.[93][94] The AdSense contextual advertising technology developed by Applied Semantics was adopted into Google's advertising efforts.[95][92]In 2001, Google acquired Deja News, the operators of a large archive of materials from Usenet.[89][90] Google rebranded the archive as Google Groups, and by the end of the year, it had expanded the history back to 1981.[91][92]As of October 2016, Google operates 70 offices in more than 40 countries.[85] Alexa, a company that monitors commercial web traffic, lists Google.com as the most visited website in the world.[86] Several other Google services also figure in the top 100 most visited websites, including YouTube[87] and Blogger.[88]In September 2015, Google engineering manager Rachel Potvin revealed details about Google's software code at an engineering conference. She revealed that the entire Google codebase, which spans every single service it develops, consists of over 2 billion lines of code. All that code is stored in a code repository available to all 25,000 Google engineers, and the code is regularly copied and updated on 10 Google data centers. To keep control, Potvin said Google has built its own \"version control system\", called \"Piper\", and that \"when you start a new project, you have a wealth of libraries already available to you. Almost everything has already been done.\" Engineers can make a single code change and deploy it on all services at the same time. The only major exceptions are that the PageRank search results algorithm is stored separately with only specific employee access, and the code for the Android operating system and the Google Chrome browser are also stored separately, as they don't run on the Internet. The \"Piper\" system spans 85 TB of data. Google engineers make 25,000 changes to the code each day and on a weekly basis change approximately 15 million lines of code across 250,000 files. With that much code, automated bots have to help. Potvin reported, \"You need to make a concerted effort to maintain code health. And this is not just humans maintaining code health, but robots too.\u201d Bots aren't writing code, but generating a lot of the data and configuration files needed to run the company's software. \"Not only is the size of the repository increasing,\" Potvin explained, \"but the rate of change is also increasing. This is an exponential curve.\"[83][84]According to Interbrand's annual Best Global Brands report, Google has been the second most valuable brand in the world (behind Apple Inc.) in 2013,[79] 2014,[80] 2015,[81] and 2016, with a valuation of $133 billion.[82]The corporation's consolidated revenue for the third quarter of 2013 was reported in mid-October 2013 as $14.89 billion, a 12 percent increase compared to the previous quarter.[77] Google's Internet business was responsible for $10.8 billion of this total, with an increase in the number of users' clicks on advertisements.[78]The Alliance for Affordable Internet (A4AI) was launched in October 2013; Google is part of the coalition of public and private organizations that also includes Facebook, Intel, and Microsoft. Led by Sir Tim Berners-Lee, the A4AI seeks to make Internet access more affordable so that access is broadened in the developing world, where only 31% of people are online. Google will help to decrease Internet access prices so they fall below the UN Broadband Commission's worldwide target of 5% of monthly income.[76]Google celebrated its 15-year anniversary on September 27, 2013, and in 2016 it celebrated its 18th birthday with an animated Doodle shown on web browsers around the world.[72] although it has used other dates for its official birthday.[73] The reason for the choice of September 27 remains unclear, and a dispute with rival search engine Yahoo! Search in 2005 has been suggested as the cause.[74][75]Google announced the launch of a new company, called Calico, on September 19, 2013, to be led by Apple, Inc. chairman Arthur Levinson. In the official public statement, Page explained that the \"health and well-being\" company would focus on \"the challenge of ageing and associated diseases\".[71]The year 2012 was the first time that Google generated $50 billion in annual revenue, generating $38 billion the previous year. In January 2013, then-CEO Larry Page commented, \"We ended 2012 with a strong quarter ... Revenues were up 36% year-on-year, and 8% quarter-on-quarter. And we hit $50 billion in revenues for the first time last year \u2013 not a bad achievement in just a decade and a half.\"[70]In 2005, The Washington Post reported on a 700 percent increase in third-quarter profit for Google, largely thanks to large companies shifting their advertising strategies from newspapers, magazines, and television to the Internet.[64] In January 2008, all the data that passed through Google's MapReduce software component had an aggregated size of 20 petabytes per day.[65][66][67] In 2009, a CNN report about top political searches of 2009 noted that \"more than a billion searches\" are being typed into Google on a daily basis.[68] In May 2011, the number of monthly unique visitors to Google surpassed one billion for the first time, an 8.4\u00a0percent increase from May 2010 (931\u00a0million).[69]In 2001, Google received a patent for its PageRank mechanism.[58] The patent was officially assigned to Stanford University and lists Lawrence Page as the inventor. In 2003, after outgrowing two other locations, the company leased an office complex from Silicon Graphics, at 1600 Amphitheatre Parkway in Mountain View, California.[59] The complex became known as the Googleplex, a play on the word googolplex, the number one followed by a googol zeroes. The Googleplex interiors were designed by Clive Wilkinson Architects. Three years later, Google bought the property from SGI for $319\u00a0million.[60] By that time, the name \"Google\" had found its way into everyday language, causing the verb \"google\" to be added to the Merriam-Webster Collegiate Dictionary and the Oxford English Dictionary, denoted as: \"to use the Google search engine to obtain information on the Internet\".[61][62] The first use of \"Google\" as a verb in pop culture happened on the TV series Buffy the Vampire Slayer, in 2002.[63]This model of selling keyword advertising was first pioneered by Goto.com, an Idealab spin-off created by Bill Gross.[55][56] When the company changed names to Overture Services, it sued Google over alleged infringements of the company's pay-per-click and bidding patents. Overture Services would later be bought by Yahoo! and renamed Yahoo! Search Marketing. The case was then settled out of court; Google agreed to issue shares of common stock to Yahoo! in exchange for a perpetual license.[57]In March 1999, the company moved its offices to Palo Alto, California,[51] which is home to several prominent Silicon Valley technology start-ups.[52] The next year, Google began selling advertisements associated with search keywords against Page and Brin's initial opposition toward an advertising-funded search engine.[53][9] In order to maintain an uncluttered page design, advertisements were solely text-based.[54]The stock performed well after the IPO, with shares hitting $350 for the first time on October 31, 2007,[47] primarily because of strong sales and earnings in the online advertising market.[48] The surge in stock price was fueled mainly by individual investors, as opposed to large institutional investors and mutual funds.[48] GOOG shares split into GOOG class C shares and GOOGL class A shares.[49] The company is listed on the NASDAQ stock exchange under the ticker symbols GOOGL and GOOG, and on the Frankfurt Stock Exchange under the ticker symbol GGQ1. These ticker symbols now refer to Alphabet Inc., Google's holding company, since the fourth quarter of 2015.[50]There were concerns that Google's IPO would lead to changes in company culture. Reasons ranged from shareholder pressure for employee benefit reductions to the fact that many company executives would become instant paper millionaires.[37] As a reply to this concern, co-founders Brin and Page promised in a report to potential investors that the IPO would not change the company's culture.[38] In 2005, articles in The New York Times[39] and other sources began suggesting that Google had lost its anti-corporate, no evil philosophy.[40][41][42] In an effort to maintain the company's unique culture, Google designated a Chief Culture Officer, who also serves as the Director of Human Resources. The purpose of the Chief Culture Officer is to develop and maintain the culture and work on ways to keep true to the core values that the company was founded on: a flat organization with a collaborative environment.[43] Google has also faced allegations of sexism and ageism from former employees.[44][45] In 2013, a class action against several Silicon Valley companies, including Google, was filed for alleged \"no cold call\" agreements which restrained the recruitment of high-tech employees.[46]At IPO, the company offered 19,605,052 shares at a price of $85 per share.[30][31] Shares were sold in an online auction format using a system built by Morgan Stanley and Credit Suisse, underwriters for the deal.[32][33] The sale of $1.67 bn (billion) gave Google a market capitalization of more than $23bn.[34] By January 2014, its market capitalization had grown to $397bn.[35] The vast majority of the 271\u00a0million shares remained under the control of Google, and many Google employees became instant paper millionaires. Yahoo!, a competitor of Google, also benefitted because it owned 8.4\u00a0million shares of Google before the IPO took place.[36]Google's initial public offering (IPO) took place five years later, on August 19, 2004. At that time Larry Page, Sergey Brin, and Eric Schmidt agreed to work together at Google for 20 years, until the year 2024.[29]Early in 1999, Brin and Page decided they wanted to sell Google to Excite. They went to Excite CEO George Bell and offered to sell it to him for $1 million. He rejected the offer. Vinod Khosla, one of Excite's venture capitalists, talked the duo down to $750,000, but Bell still rejected it.[28]After some additional, small investments through the end of 1998 to early 1999,[26] a new $25 million round of funding was announced on June 7, 1999,[27] with major investors including the venture capital firms Kleiner Perkins Caufield & Byers and Sequoia Capital.[25]Google was initially funded by an August 1998 contribution of $100,000 from Andy Bechtolsheim, co-founder of Sun Microsystems; the money was given before Google was incorporated.[25] Google received money from three other angel investors in 1998: Amazon.com founder Jeff Bezos, Stanford University computer science professor David Cheriton, and entrepreneur Ram Shriram.[26]The domain name for Google was registered on September 15, 1997,[21] and the company was incorporated on September 4, 1998. It was based in the garage of a friend (Susan Wojcicki[9]) in Menlo Park, California. Craig Silverstein, a fellow PhD student at Stanford, was hired as the first employee.[9][22][23]Page and Brin originally nicknamed their new search engine \"BackRub\", because the system checked backlinks to estimate the importance of a site.[13][14][15] Eventually, they changed the name to Google; the name of the search engine originated from a misspelling of the word \"googol\",[16][17] the number 1 followed by 100 zeros, which was picked to signify that the search engine was intended to provide large quantities of information.[18] Originally, Google ran under Stanford University's website, with the domains google.stanford.edu[19] and z.stanford.edu.[20]While conventional search engines ranked results by counting how many times the search terms appeared on the page, the two theorized about a better system that analyzed the relationships among websites.[10] They called this new technology PageRank; it determined a website's relevance by the number of pages, and the importance of those pages that linked back to the original site.[11][12]Google began in January 1996 as a research project by Larry Page and Sergey Brin when they were both PhD students at Stanford University in Stanford, California.[9]Alexa, a company that monitors commercial web traffic, lists Google.com as the most visited website in the world. Several other Google services also figure in the top 100 most visited websites, including YouTube and Blogger. Google is the most valuable brand in the world as of 2017,[6] but has received significant criticism involving issues such as privacy concerns, tax avoidance, antitrust, censorship, and search neutrality. Google's mission statement, from the outset, was \"to organize the world's information and make it universally accessible and useful\", and its unofficial slogan was \"Don't be evil\". In October 2015, the motto was replaced in the Alphabet corporate code of conduct by the phrase \"Do the right thing\", while the original one was retained in the code of conduct of Google.[7]The company's rapid growth since incorporation has triggered a chain of products, acquisitions, and partnerships beyond Google's core search engine (Google Search). It offers services designed for work and productivity (Google Docs, Sheets, and Slides), email (Gmail/Inbox), scheduling and time management (Google Calendar), cloud storage (Google Drive), social networking (Google+), instant messaging and video chat (Google Allo/Duo/Hangouts), language translation (Google Translate), mapping and turn-by-turn navigation (Google Maps/Waze/Earth/Street View), video sharing (YouTube), note-taking (Google Keep), and photo organizing and editing (Google Photos). The company leads the development of the Android mobile operating system, the Google Chrome web browser, and Chrome OS, a lightweight operating system based on the Chrome browser. Google has moved increasingly into hardware; from 2010 to 2015, it partnered with major electronics manufacturers in the production of its Nexus devices, and in October 2016, it released multiple hardware products (including the Google Pixel smartphone, Home smart speaker, Wifi mesh wireless router, and Daydream View virtual reality headset). The new hardware chief, Rick Osterloh, stated: \"a lot of the innovation that we want to do now ends up requiring controlling the end-to-end user experience\". Google has also experimented with becoming an Internet carrier. In February 2010, it announced Google Fiber, a fiber-optic infrastructure that was installed in Kansas City; in April 2015, it launched Project Fi in the United States, combining Wi-Fi and cellular networks from different providers; and in 2016, it announced the Google Station initiative to make public Wi-Fi available around the world, with initial deployment in India.Google LLC[5] is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, search engine, cloud computing, software, and hardware. Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University, California. Together, they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An Initial public offering (IPO) took place on August 19, 2004, and Google moved to its new headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. Google, Alphabet's leading subsidiary, will continue to be the umbrella company for Alphabet's Internet interests. Upon completion of the restructure, Sundar Pichai was appointed CEO of Google, replacing Larry Page, who became the CEO of Alphabet.",
            "title": "Google",
            "url": "https://en.wikipedia.org/wiki/Google"
        },
        {
            "desc_links": [
                "/wiki/Algorithm",
                "/wiki/Google_Search",
                "/wiki/Websites",
                "/wiki/Search_engine",
                "/wiki/Larry_Page"
            ],
            "links": [
                "/wiki/SEO",
                "/wiki/Moz_(marketing_software)",
                "/wiki/Search_Engine_Optimization_Metrics",
                "/wiki/Google_Chrome",
                "/wiki/Spam_in_blogs#nofollow",
                "/wiki/Nofollow",
                "/wiki/Semantic_link",
                "/wiki/Blog",
                "/wiki/Spamdexing",
                "/wiki/Blogosphere",
                "/wiki/Scale-free_network",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Web_crawler",
                "/wiki/Lexical_semantics",
                "/wiki/Word_Sense_Disambiguation",
                "/wiki/Semantic_similarity",
                "/wiki/WordNet",
                "/wiki/Synsets",
                "/wiki/Institute_for_Scientific_Information",
                "/wiki/Impact_factor",
                "/wiki/Eigenfactor",
                "/wiki/SCImago",
                "/wiki/Swiftype",
                "/wiki/Twitter",
                "/wiki/Neuroscience",
                "/wiki/Neuron",
                "/wiki/Attention_economy",
                "/wiki/Citation",
                "/wiki/Sociometry",
                "/wiki/Humanities",
                "/wiki/Search_engine_optimization",
                "/wiki/Nofollow",
                "/wiki/HTML_attribute",
                "/wiki/Matt_Cutts",
                "/wiki/Game_the_system",
                "/wiki/HTTP_302",
                "/wiki/Meta_tag",
                "/wiki/Website_spoofing",
                "/wiki/Google_Directory",
                "/wiki/Google_Places",
                "/wiki/Search_engine_results_page",
                "/wiki/Search_engine_optimization",
                "/wiki/Google_Toolbar",
                "/wiki/Bipartite_graphs",
                "/wiki/Graph_(data_structure)",
                "/wiki/MATLAB",
                "/wiki/GNU_Octave",
                "/wiki/Power_iteration",
                "/wiki/Link_farm",
                "/wiki/Trade_secret",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia",
                "/wiki/Eigengap",
                "/wiki/Stochastic_matrix",
                "/wiki/Eigenvector_centrality",
                "/wiki/Network_theory",
                "/wiki/Eigenvector",
                "/wiki/Adjacency_matrix",
                "/wiki/Uniform_Resource_Locator",
                "/wiki/Markov_chain",
                "/wiki/Probability_distribution",
                "/wiki/Probability_distribution",
                "/wiki/RankDex",
                "/wiki/Robin_Li",
                "/wiki/Baidu",
                "/wiki/Citation_analysis",
                "/wiki/Eugene_Garfield",
                "/wiki/Hyper_Search",
                "/wiki/Massimo_Marchiori",
                "/wiki/Jon_Kleinberg",
                "/wiki/HITS_algorithm",
                "/wiki/Web_page",
                "/wiki/Software_patent",
                "/wiki/Stanford_University",
                "/wiki/United_States_dollar",
                "/wiki/Stanford_University",
                "/wiki/Larry_Page",
                "/wiki/Sergey_Brin",
                "/wiki/Rajeev_Motwani",
                "/wiki/Terry_Winograd",
                "/wiki/Google_search",
                "/wiki/Google_Inc.",
                "/wiki/Eigenvalue",
                "/wiki/Scientometrics",
                "/wiki/Thomas_Saaty",
                "/wiki/Analytic_Hierarchy_Process",
                "/wiki/Cognitive_model",
                "/wiki/HITS_algorithm",
                "/wiki/Jon_Kleinberg",
                "/wiki/Teoma",
                "/wiki/Ask.com",
                "/wiki/CLEVER_project",
                "/wiki/TrustRank",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Webgraph",
                "/wiki/Hyperlink",
                "/wiki/Cnn.com",
                "/wiki/Usa.gov",
                "/wiki/Recursion",
                "/wiki/Incoming_link",
                "/wiki/Algorithm",
                "/wiki/Google_Search",
                "/wiki/Websites",
                "/wiki/Search_engine",
                "/wiki/Larry_Page"
            ],
            "text": "On  April 15, 2016 Google has officially shut down their Google Toolbar PageRank Data to public. Google had declared their intention to remove the PageRank score from the Google toolbar several months earlier.[69] Google will still be using PageRank score when determining how to rank content in search results.[70]\nGoogle elaborated on the reasons for PageRank deprecation at Q&A #March and announced Links and Content as the Top Ranking Factors, RankBrain was announced as the #3 Ranking Factor in October 2015 so the Top 3 Factors are now confirmed officially by Google.[68]\nEven though \"Toolbar\" PageRank is less important for SEO purposes, the existence of back-links from more popular websites continues to push a webpage higher up in search rankings.[67] Moz, an online marketing and SEO firm, offers a system similar to PageRank under the name \"MozRank.\"\nThe visible page rank is updated very infrequently. It was last updated in November 2013. In October 2014 Matt Cutts announced that another visible pagerank update would not be coming.[66]\nPageRank was once available for the verified site maintainers through the Google Webmaster Tools interface. However, on October 15, 2009, a Google employee confirmed that the company had removed PageRank from its Webmaster Tools section, saying that \"We've been telling people for a long time that they shouldn't focus on PageRank so much. Many site owners seem to think it's the most important metric for them to track, which is simply not true.\"[65] In addition, The PageRank indicator is not available in Google's own Chrome browser.\nIn an effort to manually control the flow of PageRank among pages within a website, many webmasters practice what is known as PageRank Sculpting[63]\u2014which is the act of strategically placing the nofollow attribute on certain internal links of a website in order to funnel PageRank towards those pages the webmaster deemed most important. This tactic has been used since the inception of the nofollow attribute, but may no longer be effective since Google announced that blocking PageRank transfer with nofollow does not redirect that PageRank to other links.[64]\nAs an example, people could previously create many message-board posts with links to their website to artificially inflate their PageRank. With the nofollow value, message-board administrators can modify their code to automatically insert \"rel='nofollow'\" to all hyperlinks in posts, thus preventing PageRank from being affected by those particular posts. This method of avoidance, however, also has various drawbacks, such as reducing the link value of legitimate comments. (See: Spam in blogs#nofollow)\nIn early 2005, Google implemented a new value, \"nofollow\",[62] for the rel attribute of HTML link and anchor elements, so that website developers and bloggers can make links that Google will not consider for the purposes of PageRank\u2014they are links that no longer constitute a \"vote\" in the PageRank system. The nofollow relationship was added in an attempt to help combat spamdexing.\nPagerank has recently been used to quantify the scientific impact of researchers. The underlying citation and collaboration networks are used in conjunction with pagerank algorithm in order to come up with a ranking system for individual publications which propagates to individual authors. The new index known as pagerank-index (Pi) is demonstrated to be fairer compared to h-index in the context of many drawbacks exhibited by h-index.[61]\nIn 2005, in a pilot study in Pakistan, Structural Deep Democracy, SD2[59][60] was used for leadership selection in a sustainable agriculture group called Contact Youth. SD2 uses PageRank for the processing of the transitive proxy votes, with the additional constraints of mandating at least two initial proxies per voter, and all voters are proxy candidates. More complex variants can be built on top of SD2, such as adding specialist proxies and direct votes for specific issues, but SD2 as the underlying umbrella system, mandates that generalist proxies should always be used.\nFor the analysis of protein networks in biology PageRank is also a useful tool.[57]\n[58]\nIn any ecosystem, a modified version of PageRank may be used to determine species that are essential to the continuing health of the environment.[56]\nThe PageRank may also be used as a methodology to measure the apparent impact of a community like the Blogosphere on the overall Web itself. This approach uses therefore the PageRank to measure the distribution of attention in reflection of the Scale-free network paradigm.[citation needed]\nA Web crawler may use PageRank as one of a number of importance metrics it uses to determine which URL to visit during a crawl of the web. One of the early working papers\n[54] that were used in the creation of Google is Efficient crawling through URL ordering,[55]\nwhich discusses the use of a number of different importance metrics to determine how deeply, and how much of a site Google will crawl. PageRank is presented as one of a number of these importance metrics, though there are others listed such as the number of inbound and outbound links for a URL, and the distance from the root directory on a site to the URL.\nPageRank has been used to rank spaces or streets to predict how many people (pedestrians or vehicles) come to the individual spaces or streets.[49][50] In lexical semantics it has been used to perform Word Sense Disambiguation,[51] Semantic similarity,[52] and also to automatically rank WordNet synsets according to how strongly they possess a given semantic property, such as positivity or negativity.[53]\nA similar new use of PageRank is to rank academic doctoral programs based on their records of placing their graduates in faculty positions. In PageRank terms, academic departments link to each other by hiring their faculty from each other (and from themselves).[48]\nA version of PageRank has recently been proposed as a replacement for the traditional Institute for Scientific Information (ISI) impact factor,[47] and implemented at Eigenfactor as well as at SCImago. Instead of merely counting total citation to a journal, the \"importance\" of each citation is determined in a PageRank fashion.\nSwiftype's site search product builds a \"PageRank that\u2019s specific to individual websites\" by looking at each website's signals of importance and prioritizing content based on factors such as number of links from the home page.[46]\nPersonalized PageRank is used by Twitter to present users with other accounts they may wish to follow.[45]\nIn neuroscience, the PageRank of a neuron in a neural network has been found to correlate with its relative firing rate.[44]\nThe mathematics of PageRank are entirely general and apply to any graph or network in any domain. Thus, PageRank is now regularly used in bibliometrics, social and information network analysis, and for link prediction and recommendation. It's even used for systems analysis of road networks, as well as biology, chemistry, neuroscience, and physics.[43]\n[42] Matteo Pasquinelli reckons the basis for the belief that PageRank has a social component lies in the idea of attention economy. With attention economy, value is placed on products that receive a greater amount of human attention and the results at the top of the PageRank garner a larger amount of focus then those on subsequent pages. The outcomes with the higher PageRank will therefore enter the human consciousness to a larger extent. These ideas can influence decision-making and the actions of the viewer have a direct relation to the PageRank. They possess a higher potential to attract a user\u2019s attention as their location increases the attention economy attached to the site. With this location they can receive more traffic and their online marketplace will have more purchases. The PageRank of these sites allow them to be trusted and they are able to parlay this trust into increased business.\n[41] Katja Mayer views PageRank as a social network as it connects differing viewpoints and thoughts in a single place. People go to PageRank for information and are flooded with citations of other authors who also have an opinion on the topic. This creates a social aspect where everything can be discussed and collected to provoke thinking. There is a social relationship that exists between PageRank and the people who use it as it is constantly adapting and changing to the shifts in modern society. Viewing the relationship between PageRank and the individual through sociometry allows for an in-depth look at the connection that results.\n[40] Laura Granka discusses PageRank by describing how the pages are not simply ranked via popularity as they contain a reliability that gives them a trustworthy quality. This has led to a development of behavior that is directly linked to PageRank. PageRank is viewed as the definitive rank of products and businesses and thus, can manipulate thinking. The information that is available to individuals is what shapes thinking and ideology and PageRank is the device that displays this information. The results shown are the forum to which information is delivered to the public and these results have a societal impact as they will affect how a person thinks and acts.\nThe PageRank algorithm has major effects on society as it contains a social influence. As opposed to the scientific viewpoint of PageRank as an algorithm the humanities instead view it through a lens examining its social components. In these instances, it is dissected and reviewed not for its technological advancement in the field of search engines, but for its societal influences.\nA more intelligent surfer that probabilistically hops from page to page depending on the content of the pages and query terms the surfer that it is looking for.\nThis model is based on a query-dependent PageRank score of a page which as the name suggests is also a function of query. When given a multiple-term query, Q={q1,q2,\u2026}, the surfer selects a q according to some probability distribution, P(q) and uses that term to guide its behavior for a large number of steps. It then selects another term according to the distribution to determine its behavior, and so on. The resulting distribution over visited web pages is QD-PageRank.[39]\nFor search engine optimization purposes, some companies offer to sell high PageRank links to webmasters.[38] As links from higher-PR pages are believed to be more valuable, they tend to be more expensive. It can be an effective and viable marketing strategy to buy link advertisements on content pages of quality and relevant sites to drive traffic and increase a webmaster's link popularity. However, Google has publicly warned webmasters that if they are or were discovered to be selling links for the purpose of conferring PageRank and reputation, their links will be devalued (ignored in the calculation of other pages' PageRanks). The practice of buying and selling links is intensely debated across the Webmaster community. Google advises webmasters to use the nofollow HTML attribute value on sponsored links. According to Matt Cutts, Google is concerned about webmasters who try to game the system, and thereby reduce the quality and relevance of Google search results.[38]\nIn the past, the PageRank shown in the Toolbar was easily manipulated. Redirection from one page to another, either via a HTTP 302 response or a \"Refresh\" meta tag, caused the source page to acquire the PageRank of the destination page. Hence, a new page with PR 0 and no incoming links could have acquired PR 10 by redirecting to the Google home page. This spoofing technique was a known vulnerability. Spoofing can generally be detected by performing a Google search for a source URL; if the URL of an entirely different site is displayed in the results, the latter URL may represent the destination of a redirection.\nThe Google Directory PageRank was an 8-unit measurement. Unlike the Google Toolbar, which shows a numeric PageRank value upon mouseover of the green bar, the Google Directory only displayed the bar, never the numeric values. Google Directory was closed on July 20, 2011.[37]\nAfter the introduction of Google Places into the mainstream organic SERP, numerous other factors in addition to PageRank affect ranking a business in Local Business Results.[36]\nPositioning of a webpage on Google SERPs for a keyword depends on relevance and reputation, also known as authority and popularity. PageRank is Google\u2019s indication of its assessment of the reputation of a webpage: It is non-keyword specific. Google uses a combination of webpage and website authority to determine the overall authority of a webpage competing for a keyword.[34] The PageRank of the HomePage of a website is the best indication Google offers for website authority.[35]\nThe search engine results page (SERP) is the actual result returned by a search engine in response to a keyword query. The SERP consists of a list of links to web pages with associated text snippets. The SERP rank of a web page refers to the placement of the corresponding link on the SERP, where higher placement means higher SERP rank. The SERP rank of a web page is a function not only of its PageRank, but of a relatively large and continuously adjusted set of factors (over 200).[33] Search engine optimization (SEO) is aimed at influencing the SERP rank for a website or a set of web pages.\nThe Google Toolbar long had a PageRank feature which displayed a visited page's PageRank as a whole number between 0 and 10. The most popular websites displayed a PageRank of 10. The least showed a PageRank of 0. Google has not disclosed the specific method for determining a Toolbar PageRank value, which is to be considered only a rough indication of the value of a website. In March 2016 Google announced it would no longer support this feature, and the underlying API would soon cease to operate.[32]\nA generalization of PageRank for the case of ranking two\ninteracting groups of objects was described in\n[30]\nIn applications it may be necessary to model systems having\nobjects of two kinds where a weighted relation is defined on\nobject pairs. This leads to considering bipartite graphs. For such graphs two related\npositive or nonnegative irreducible matrices corresponding to\nvertex partition sets can be defined. One can compute rankings of\nobjects in both groups as eigenvectors corresponding to the\nmaximal positive eigenvalues of these matrices. Normed\neigenvectors exist and are unique by the Perron or\nPerron-Frobenius theorem.\nExample: consumers and products. the relation weight is the product\nconsumption rate.\nthat is, the PageRank of an undirected graph equals to the degree distribution vector if and only if the graph is regular, i.e., every vertex has the same degree.\nThe PageRank of an undirected graph  G is statistically close to the degree distribution of the graph G,[28] but they are generally not identical: If R is the PageRank vector defined above, and D is the degree distribution vector\nThis example takes 13 iterations to converge.\nExample of code calling the rank function defined above:\nPageRank MATLAB/Octave implementation\nwith\nNote that in Eq. (***) the matrix on the right-hand side in the parenthesis can be interpreted as\nuntil\nThe solution is given by\ni.e., when convergence is assumed.\ni.e.,\nor in matrix notation\nAt each time step, the computation, as detailed above, yields\nPageRank can be computed either iteratively or algebraically. The iterative method can be viewed as the\npower iteration method[26][27] or the power method. The basic mathematical operations performed are identical.\nSince December 2007, when it started actively penalizing sites selling paid text links, Google has combatted link farms and other schemes designed to artificially inflate PageRank. How Google identifies link farms and other PageRank manipulation tools is among Google's trade secrets.\nVarious strategies to manipulate PageRank have been employed in concerted efforts to improve search results rankings and monetize advertising links. These strategies have severely impacted the reliability of the PageRank concept,[citation needed] which purports to determine which documents are actually highly valued by the Web community.\nSeveral strategies have been proposed to accelerate the computation of PageRank.[25]\nOne main disadvantage of PageRank is that it favors older pages. A new page, even a very good one, will not have many links unless it is part of an existing site (a site being a densely connected set of pages, such as Wikipedia).\nBecause of the large eigengap of the modified adjacency matrix above,[24] the values of the PageRank eigenvector can be approximated to within a high degree of accuracy within only a few iterations.\ni.e. the elements of each column sum up to 1, so the matrix is a stochastic matrix (for more details see the computation section below). Thus this is a variant of the eigenvector centrality measure used commonly in network analysis.\nwhere R is the solution of the equation\nThe PageRank values are the entries of the dominant right eigenvector of the modified adjacency matrix rescaled so that each column adds up to one. This makes PageRank a particularly elegant metric: the eigenvector is\nSo, the equation is as follows:\nWhen calculating PageRank, pages with no outbound links are assumed to link out to all other pages in the collection. Their PageRank scores are therefore divided evenly among all other pages. In other words, to be fair with pages that are not sinks, these random transitions are added to all nodes in the Web, with a residual probability usually set to d = 0.85, estimated from the frequency that an average surfer uses his or her browser's bookmark feature.\nIf a page has no links to other pages, it becomes a sink and therefore terminates the random surfing process. If the random surfer arrives at a sink page, it picks another URL at random and continues surfing again.\nThe formula uses a model of a random surfer who gets bored after several clicks and switches to a random page. The PageRank value of a page reflects the chance that the random surfer will land on that page by clicking on a link. It can be understood as a Markov chain in which the states are pages, and the transitions, which are all equally probable, are the links between pages.\nGoogle recalculates PageRank scores each time it crawls the Web and rebuilds its index. As Google increases the number of documents in its collection, the initial approximation of PageRank decreases for all documents.\nPage and Brin confused the two formulas in their most popular paper \"The Anatomy of a Large-Scale Hypertextual Web Search Engine\", where they mistakenly claimed that the latter formula formed a probability distribution over web pages.[5]\nThe difference between them is that the PageRank values in the first formula sum to one, while in the second formula each PageRank is multiplied by N and the sum becomes N. A statement in Page and Brin's paper that \"the sum of all PageRanks is one\"[5] and claims by other Google employees[23] support the first variant of the formula above.\nSo any page's PageRank is derived in large part from the PageRanks of other pages. The damping factor adjusts the derived value downward. The original paper, however, gave the following formula, which has led to some confusion:\nThe damping factor is subtracted from 1 (and in some variations of the algorithm, the result is divided by the number of documents (N) in the collection) and this term is then added to the product of the damping factor and the sum of the incoming PageRank scores. That is,\nThe PageRank theory holds that an imaginary surfer who is randomly clicking on links will eventually stop clicking. The probability, at any step, that the person will continue is a damping factor d. Various studies have tested different damping factors, but it is generally assumed that the damping factor will be set around 0.85.[5]\ni.e. the PageRank value for a page u is dependent on the PageRank values for each page v contained in the set Bu (the set containing all pages linking to page u), divided by the number L(v) of links from page v.\nIn the general case, the PageRank value for any page u can be expressed as:\nIn other words, the PageRank conferred by an outbound link is equal to the document's own PageRank score divided by the number of outbound links L( ).\nSuppose instead that page B had a link to pages C and A, page C had a link to page A, and page D had links to all three pages. Thus, upon the first iteration, page B would transfer half of its existing value, or 0.125, to page A and the other half, or 0.125, to page C.  Page C would transfer all of its existing value, 0.25, to the only page it links to, A. Since D had three outbound links, it would transfer one third of its existing value, or approximately 0.083, to A.  At the completion of this iteration, page A will have a PageRank of approximately 0.458.\nIf the only links in the system were from pages B, C, and D to A, each link would transfer 0.25 PageRank to A upon the next iteration, for a total of 0.75.\nThe PageRank transferred from a given page to the targets of its outbound links upon the next iteration is divided equally among all outbound links.\nAssume a small universe of four web pages: A, B, C and D. Links from a page to itself are ignored. Multiple outbound links from one page to another page are treated as a single link. PageRank is initialized to the same value for all pages. In the original form of PageRank, the sum of PageRank over all pages was the total number of pages on the web at that time, so each page in this example would have an initial value of 1. However, later versions of PageRank, and the remainder of this section, assume a probability distribution between 0 and 1. Hence the initial value for each page in this example is 0.25.\nA probability is expressed as a numeric value between 0 and 1. A 0.5 probability is commonly expressed as a \"50% chance\" of something happening. Hence, a PageRank of 0.5 means there is a 50% chance that a person clicking on a random link will be directed to the document with the 0.5 PageRank.\nThe PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. The PageRank computations require several passes, called \"iterations\", through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value.\nA small search engine called \"RankDex\" from IDD Information Services designed by Robin Li was, since 1996, already exploring a similar strategy for site-scoring and page ranking.[18] The technology in RankDex was patented by 1999[19] and used later when Li founded Baidu in China.[20][21] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[22]\nPageRank was influenced by citation analysis, early developed by Eugene Garfield in the 1950s at the University of Pennsylvania, and by Hyper Search, developed by Massimo Marchiori at the University of Padua. In the same year PageRank was introduced (1998), Jon Kleinberg published his important work on HITS. Google's founders cite Garfield, Marchiori, and Kleinberg in their original papers.[5][17]\nThe name \"PageRank\" plays off of the name of developer Larry Page, as well as the concept of a web page.[14] The word is a trademark of Google, and the PageRank process has been patented (U.S. Patent 6,285,999). However, the patent is assigned to Stanford University and not to Google. Google has exclusive license rights on the patent from Stanford University. The university received 1.8 million shares of Google in exchange for use of the patent; the shares were sold in 2005 for $336 million.[15][16]\nPageRank was developed at Stanford University by Larry Page and  Sergey Brin in 1996 as part of a research project about a new kind of search engine.[11] Sergey Brin had the idea that information on the web could be ordered in a hierarchy by \"link popularity\": A page is ranked higher as there are more links to it.[12] It was co-authored by Rajeev Motwani and Terry Winograd. The first paper about the project, describing PageRank and the initial prototype of the Google search engine, was published in 1998:[5] shortly after, Page and Brin founded Google Inc., the company behind the Google search engine. While just one of many factors that determine the ranking of Google search results, PageRank continues to provide the basis for all of Google's web search tools.[13]\nThe eigenvalue problem was suggested in 1976 by Gabriel Pinski and Francis Narin, who worked on scientometrics ranking scientific journals,[7]\nin 1977 by Thomas Saaty in his concept of Analytic Hierarchy Process which weighted alternative choices,[8]\nand in 1995 by Bradley Love and Steven Sloman as a cognitive model for concepts, the centrality algorithm.[9][10]\nOther link-based ranking algorithms for Web pages include the HITS algorithm invented by Jon Kleinberg (used by Teoma and now Ask.com),the IBM CLEVER project, the TrustRank algorithm and the hummingbird algorithm.[citation needed]\nNumerous academic papers concerning PageRank have been published since Page and Brin's original paper.[5] In practice, the PageRank concept may be vulnerable to manipulation. Research has been conducted into identifying falsely influenced PageRank rankings. The goal is to find an effective means of ignoring links from documents with falsely influenced PageRank.[6]\nA PageRank results from a mathematical algorithm based on the webgraph, created by all World Wide Web pages as nodes and hyperlinks as edges, taking into consideration authority hubs such as cnn.com or usa.gov. The rank value indicates an importance of a particular page. A hyperlink to a page counts as a vote of support. The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it (\"incoming links\"). A page that is linked to by many pages with high PageRank receives a high rank itself.\n It is not the only algorithm used by Google to order search engine results, but it is the first algorithm that was used by the company, and it is the best-known.[3][4]\nPageRank (PR) is an algorithm used by Google Search to rank websites in their search engine results. PageRank was named after Larry Page,[1] one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google: ",
            "title": "PageRank",
            "url": "https://en.wikipedia.org/wiki/PageRank"
        },
        {
            "desc_links": [
                "/wiki/Incidence_matrix",
                "/wiki/Degree_matrix",
                "/wiki/Degree_(graph_theory)",
                "/wiki/Vertex_(graph_theory)",
                "/wiki/Simple_graph",
                "/wiki/(0,1)-matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Eigenvector",
                "/wiki/Spectral_graph_theory",
                "/wiki/Graph_theory",
                "/wiki/Computer_science",
                "/wiki/Square_matrix",
                "/wiki/Graph_(discrete_mathematics)"
            ],
            "links": [
                "/wiki/Weighted_graph",
                "/wiki/Text_file",
                "/wiki/Base64",
                "/wiki/Locality_of_reference",
                "/wiki/Sparse_graph",
                "/wiki/Data_structure",
                "/wiki/Graph_(abstract_data_type)",
                "/wiki/Adjacency_list",
                "/wiki/Matrix_multiplication",
                "/wiki/Path_(graph_theory)",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Connectivity_(graph_theory)",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Characteristic_polynomial",
                "/wiki/Determinant",
                "/wiki/Trace_(matrix)",
                "/wiki/Linear_operator",
                "/wiki/Isospectral",
                "/wiki/Graph_isomorphism",
                "/wiki/Permutation_matrix",
                "/wiki/Complete_graph",
                "/wiki/Empty_graph",
                "/wiki/Zero_matrix",
                "/wiki/Directed_graph#Indegree_and_outdegree",
                "/wiki/Distance_matrix",
                "/wiki/Seidel_adjacency_matrix",
                "/wiki/Strongly_regular_graph",
                "/wiki/Two-graph",
                "/wiki/Multigraph",
                "/wiki/Weighted_graph",
                "/wiki/Bipartite_graph",
                "/wiki/Bipartite_graph",
                "/wiki/Multigraph",
                "/wiki/Loop_(graph_theory)",
                "/wiki/Algebraic_graph_theory",
                "/wiki/Incidence_matrix",
                "/wiki/Degree_matrix",
                "/wiki/Degree_(graph_theory)",
                "/wiki/Vertex_(graph_theory)",
                "/wiki/Simple_graph",
                "/wiki/(0,1)-matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Eigenvector",
                "/wiki/Spectral_graph_theory",
                "/wiki/Graph_theory",
                "/wiki/Computer_science",
                "/wiki/Square_matrix",
                "/wiki/Graph_(discrete_mathematics)"
            ],
            "text": "Besides the space tradeoff, the different data structures also facilitate different operations. Finding all vertices adjacent to a given vertex in an adjacency list is as simple as reading the list, and takes time proportional to the number of neighbors. With an adjacency matrix, an entire row must instead be scanned, which takes a larger amount of time, proportional to the number of vertices in the whole graph. On the other hand, testing whether there is an edge between two given vertices can be determined at once with an adjacency matrix, while requiring time proportional to the minimum degree of the two vertices with the adjacency list.[8][11]\nAn alternative form of adjacency matrix (which, however, requires a larger amount of space) replaces the numbers in each element of the matrix with pointers to edge objects (when edges are present) or null pointers (when there is no edge).[11]\nIt is also possible to store edge weights directly in the elements of an adjacency matrix.[8]\nBecause each entry in the adjacency matrix requires only one bit, it can be represented in a very compact way, occupying only |V |2/8 bytes to represent a directed graph, or (by using a packed triangular format and only storing the lower triangular part of the matrix) approximately |V |2/16 bytes to represent an undirected graph. Although slightly more succinct representations are possible, this method gets close to the information-theoretic lower bound for the minimum number of bits needed to represent all n-vertex graphs.[9] For storing graphs in text files, fewer bits per byte can be used to ensure that all bytes are text characters, for instance by using a Base64 representation.[10]\nBesides avoiding wasted space, this compactness encourages locality of reference.\nHowever, for a large sparse graph, adjacency lists require less storage space, because they do not waste any space to represent edges that are not present.[8][11]\nThe adjacency matrix may be used as a data structure for the representation of graphs in computer programs for manipulating graphs. The main alternative data structure, also in use for this application, is the adjacency list.[7][8]\nIf A is the adjacency matrix of the directed or undirected graph G, then the matrix An (i.e., the matrix product of n copies of A) has an interesting interpretation: the element (i, j) gives the number of (directed or undirected) walks of length n from vertex i to vertex j. If n is the smallest nonnegative integer, such that for some i, j, the element (i, j) of An is positive, then n is the distance between vertex i and vertex j. This implies, for example, that the number of triangles in an undirected graph G is exactly the trace of A3 divided by 6. Note that the adjacency matrix can be used to determine whether or not the graph is connected.\nIn particular, A1 and A2 are similar and therefore have the same minimal polynomial, characteristic polynomial, eigenvalues, determinant and trace. These can therefore serve as isomorphism invariants of graphs. However, two graphs may possess the same set of eigenvalues but not be isomorphic.[6] Such linear operators are said to be isospectral.\nSuppose two directed or undirected graphs G1 and G2 with adjacency matrices A1 and A2 are given. G1 and G2 are isomorphic if and only if there exists a permutation matrix P such that\nThe adjacency matrix of a complete graph contains all ones except along the diagonal where there are only zeros. The adjacency matrix of an empty graph is a zero matrix.\nIn directed graphs, the in-degree of a vertex can be computed by summing the entries of the corresponding column, and the out-degree can be computed by summing the entries of the corresponding row.\nThe convention followed here (for undirected graphs) is that each edge adds 1 to the appropriate cell in the matrix, and each loop adds 2.[4] This allows the degree of a vertex to be easily found by taking the sum of the values in either its respective row or column in the adjacency matrix.\nThe distance matrix has in position (i, j) the distance between vertices vi and vj. The distance is the length of a shortest path connecting the vertices. Unless lengths of edges are explicitly provided, the length of a path is the number of edges in it. The distance matrix resembles a high power of the adjacency matrix, but instead of telling only whether or not two vertices are connected (i.e., the connection matrix, which contains boolean values), it gives the exact distance between them.\nAn (a, b, c)-adjacency matrix A of a simple graph has Ai,j = a if (i, j) is an edge, b if it is not, and c on the diagonal. The Seidel adjacency matrix is a (\u22121, 1, 0)-adjacency matrix. This matrix is used in studying strongly regular graphs and two-graphs.[3]\nIf G is a bipartite multigraph or weighted graph then the elements bi,j are taken to be the number of edges between the vertices or the weight of the edge (ui, vj), respectively.\nFormally, let G = (U, V, E) be a bipartite graph with parts U = {u1, \u2026, ur} and V = {v1, \u2026, vs}. The biadjacency matrix is the r \u00d7 s 0\u20131 matrix B in which bi,j = 1 if and only if (ui, vj) \u2208 E. \nwhere B is an r \u00d7 s matrix, and 0r,r and 0s,s represent the r \u00d7 r and s \u00d7 s zero matrices. In this case, the smaller matrix B uniquely represents the graph, and the remaining parts of A can be discarded as redundant. B is sometimes called the biadjacency matrix.\nThe adjacency matrix A of a bipartite graph whose two parts have r and s vertices can be written in the form \nThe same concept can be extended to multigraphs and graphs with loops by storing the number of edges between each two vertices in the corresponding matrix element, and by allowing nonzero diagonal elements. Loops may be counted either once (as a single edge) or twice (as two vertex-edge incidences), as long as a consistent convention is followed. Undirected graphs often use the latter convention of counting loops twice, whereas directed graphs typically use the former convention.\nFor a simple graph with vertex set V, the adjacency matrix is a square |V|\u00a0\u00d7\u00a0|V| matrix A such that its element Aij is one when there is an edge from vertex i to vertex j, and zero when there is no edge.[1] The diagonal elements of the matrix are all zero, since edges from a vertex to itself (loops) are not allowed in simple graphs. It is also sometimes useful in algebraic graph theory to replace the nonzero elements with algebraic variables.[2]\nThe adjacency matrix should be distinguished from the incidence matrix for a graph, a different matrix representation whose elements indicate whether vertex\u2013edge pairs are incident or not, and degree matrix which contains information about the degree of each vertex.\nIn the special case of a finite simple graph, the adjacency matrix is a (0,1)-matrix with zeros on its diagonal. If the graph is undirected, the adjacency matrix is symmetric. \nThe relationship between a graph and the eigenvalues and eigenvectors of its adjacency matrix is studied in spectral graph theory.\nIn graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph.\n",
            "title": "Adjacency matrix",
            "url": "https://en.wikipedia.org/wiki/Adjacency_matrix"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Eigenvector",
                "/wiki/Eigenvalue",
                "/wiki/Probability",
                "/wiki/Statistics",
                "/wiki/Stable_distribution"
            ],
            "text": "Crudely stated, all of the above are specific cases of a common general concept. A stationary distribution is a specific entity which is unchanged by the effect of some matrix or operator: it need not be unique. Thus stationary distributions are related to eigenvectors for which the eigenvalue is unity.In some fields of application, the term stable distribution is used for the equivalent of a stationary (marginal) distribution, although in probability and statistics the term has a rather different meaning: see stable distribution.Stationary distribution may refer to:",
            "title": "Stationary distribution",
            "url": "https://en.wikipedia.org/wiki/Stationary_distribution"
        },
        {
            "desc_links": [
                "/wiki/Statistical_modeling",
                "/wiki/Cruise_control",
                "/wiki/Motor_vehicles",
                "/wiki/Exchange_rates",
                "/wiki/Dam",
                "/wiki/PageRank",
                "/wiki/Google_search_engine",
                "/wiki/Markov_Chain_Monte_Carlo",
                "/wiki/Bayesian_statistics",
                "/wiki/Random_walk",
                "/wiki/Gambler%27s_ruin",
                "/wiki/Wiener_process",
                "/wiki/Brownian_motion",
                "/wiki/Poisson_process",
                "/wiki/State_space",
                "/wiki/Continuous_and_discrete_variables",
                "/wiki/Probability_theory",
                "/wiki/Russia",
                "/wiki/Andrey_Markov",
                "/wiki/Stochastic_process",
                "/wiki/Markov_property",
                "/wiki/Memorylessness",
                "/wiki/Conditional_probability",
                "/wiki/Independence_(probability_theory)",
                "/wiki/Stochastic_model",
                "/wiki/Sequence"
            ],
            "links": [
                "/wiki/Bioinformatics",
                "/wiki/Natural_language_generation",
                "/wiki/Parody_generator",
                "/wiki/Dissociated_press",
                "/wiki/Mark_V_Shaney",
                "/wiki/Bunt_(baseball)",
                "/wiki/Base_stealing",
                "/wiki/Astroturf",
                "/wiki/Phrase_(music)",
                "/wiki/Algorithmic_composition",
                "/wiki/Software",
                "/wiki/CSound",
                "/wiki/Max_(software)",
                "/wiki/SuperCollider",
                "/wiki/Probability_vector",
                "/wiki/MIDI",
                "/wiki/Hertz",
                "/wiki/Snakes_and_Ladders",
                "/wiki/Hi_Ho!_Cherry-O",
                "/wiki/Population_genetics",
                "/wiki/Genetic_drift",
                "/wiki/Diffusion_equation",
                "/wiki/Motoo_Kimura",
                "/wiki/Population_process",
                "/wiki/Leslie_matrix",
                "/wiki/Population_dynamics",
                "/wiki/Epithelial_cells",
                "/wiki/Ion_channel",
                "/wiki/Path-dependent",
                "/wiki/Karl_Marx",
                "/wiki/Das_Kapital",
                "/wiki/Economic_development",
                "/wiki/Capitalism",
                "/wiki/Middle_class",
                "/wiki/Political",
                "/wiki/Authoritarian",
                "/wiki/Democratic_regime",
                "/wiki/Credit_rating_agency",
                "/wiki/General_equilibrium",
                "/wiki/Wikipedia:Accuracy_dispute#Disputed_statement",
                "/wiki/Talk:Markov_chain#Dubious",
                "/wiki/James_D._Hamilton",
                "/wiki/Markov_Switching_Multifractal",
                "/wiki/Laurent_E._Calvet",
                "/wiki/Markov_chain_Monte_Carlo",
                "/wiki/Bayesian_inference",
                "/wiki/Posterior_distribution",
                "/wiki/M/M/1_queue",
                "/wiki/Poisson_process",
                "/wiki/Queueing_theory",
                "/wiki/Agner_Krarup_Erlang",
                "/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm",
                "/wiki/LZ77_and_LZ78",
                "/wiki/Hidden_Markov_model",
                "/wiki/Viterbi_algorithm",
                "/wiki/Bioinformatics",
                "/wiki/Claude_Shannon",
                "/wiki/A_Mathematical_Theory_of_Communication",
                "/wiki/Information_theory",
                "/wiki/Information_entropy",
                "/wiki/Data_compression",
                "/wiki/Entropy_encoding",
                "/wiki/Arithmetic_coding",
                "/wiki/State_estimation",
                "/wiki/Pattern_recognition",
                "/wiki/Reinforcement_learning",
                "/wiki/Hidden_Markov_models",
                "/wiki/Speech_recognition#Algorithms",
                "/wiki/Markov_blanket",
                "/wiki/Superlattice",
                "/wiki/Copolymer",
                "/wiki/Steric_effects",
                "/wiki/In_silico",
                "/wiki/Michaelis%E2%80%93Menten_kinetics",
                "/wiki/Lattice_QCD",
                "/wiki/Thermodynamics",
                "/wiki/Statistical_mechanics",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Unit_vector",
                "/wiki/Identity_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Stationary_probability_distribution",
                "/wiki/Ergodic",
                "/wiki/Jump_process",
                "/wiki/Conditional_probability",
                "/wiki/Kolmogorov%27s_criterion",
                "/wiki/Transition_rate_matrix",
                "/wiki/Semigroup",
                "/wiki/Matrix_exponential",
                "/wiki/First-order_differential_equation",
                "/wiki/Autoregressive_model",
                "/wiki/Time_series",
                "/wiki/Interacting_particle_system",
                "/wiki/Stochastic_cellular_automata",
                "/wiki/Markov_chain_Monte_Carlo",
                "/wiki/Harris_chain",
                "/wiki/Harris_chain",
                "/wiki/Harris_chain",
                "/wiki/Markov_chains_on_a_measurable_state_space",
                "/wiki/Bernoulli_scheme",
                "/wiki/Bernoulli_process",
                "/wiki/Markov_chain_Monte_Carlo",
                "/wiki/Kolmogorov%27s_criterion",
                "/wiki/Detailed_balance",
                "/wiki/Eigendecomposition",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Identity_matrix",
                "/wiki/Zero_matrix",
                "/wiki/Stochastic_matrix",
                "/wiki/Eigenvector",
                "/wiki/Right_stochastic_matrix",
                "/wiki/Finite_set",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Element_(mathematics)",
                "/wiki/Ergodic_theory",
                "/wiki/Absorbing_Markov_chain",
                "/wiki/If_and_only_if",
                "/wiki/Expected_value",
                "/wiki/Random_variable",
                "/wiki/Bipartite_graph",
                "/wiki/Greatest_common_divisor",
                "/wiki/Periodic_function",
                "/wiki/Directed_acyclic_graph",
                "/wiki/Equivalence_relation",
                "/wiki/Equivalence_classes",
                "/wiki/Indexed_family",
                "/wiki/Exponent",
                "/wiki/Marginal_distribution",
                "/wiki/Chapman%E2%80%93Kolmogorov_equation",
                "/wiki/Identity_matrix",
                "/wiki/Forward_equation",
                "/wiki/First-order_differential_equation",
                "/wiki/Little-o_notation",
                "/wiki/Transition_rate_matrix",
                "/wiki/Finite_state_machine",
                "/wiki/Independent_and_identically_distributed_random_variables",
                "/wiki/Stochastic_row_vector",
                "/wiki/State_diagram",
                "/wiki/State_transition",
                "/wiki/Bull_market",
                "/wiki/Bear_market",
                "/wiki/Countable_set",
                "/wiki/Random_variable",
                "/wiki/Markov_property",
                "/wiki/Poisson_point_process",
                "/wiki/Andrei_Kolmogorov",
                "/wiki/Norbert_Wiener",
                "/wiki/Sydney_Chapman_(mathematician)",
                "/wiki/Chapman%E2%80%93Kolmogorov_equation",
                "/wiki/William_Feller",
                "/wiki/Eugene_Dynkin",
                "/wiki/Finite_group",
                "/wiki/Paul_Ehrenfest",
                "/wiki/Tatyana_Ehrenfest",
                "/wiki/Francis_Galton",
                "/wiki/Henry_William_Watson",
                "/wiki/Ir%C3%A9n%C3%A9e-Jules_Bienaym%C3%A9",
                "/wiki/Maurice_Fr%C3%A9chet",
                "/wiki/Andrey_Markov",
                "/wiki/Weak_law_of_large_numbers",
                "/wiki/Eugene_Onegin",
                "/wiki/Alexander_Pushkin",
                "/wiki/Central_limit_theorem",
                "/wiki/Random_walk",
                "/wiki/Number_line",
                "/wiki/Integers",
                "/wiki/Natural_numbers",
                "/wiki/Conditional_probability_distribution",
                "/wiki/Stochastic_matrix",
                "/wiki/State_space",
                "/wiki/Countable",
                "/wiki/Markov_chain#Variations",
                "/wiki/Markov_model",
                "/wiki/State_space",
                "/wiki/Continuous_and_discrete_variables",
                "/wiki/Stochastic_process",
                "/wiki/Markov_property",
                "/wiki/Serial_dependence",
                "/wiki/Statistical_modeling",
                "/wiki/Cruise_control",
                "/wiki/Motor_vehicles",
                "/wiki/Exchange_rates",
                "/wiki/Dam",
                "/wiki/PageRank",
                "/wiki/Google_search_engine",
                "/wiki/Markov_Chain_Monte_Carlo",
                "/wiki/Bayesian_statistics",
                "/wiki/Random_walk",
                "/wiki/Gambler%27s_ruin",
                "/wiki/Wiener_process",
                "/wiki/Brownian_motion",
                "/wiki/Poisson_process",
                "/wiki/State_space",
                "/wiki/Continuous_and_discrete_variables",
                "/wiki/Probability_theory",
                "/wiki/Russia",
                "/wiki/Andrey_Markov",
                "/wiki/Stochastic_process",
                "/wiki/Markov_property",
                "/wiki/Memorylessness",
                "/wiki/Conditional_probability",
                "/wiki/Independence_(probability_theory)",
                "/wiki/Stochastic_model",
                "/wiki/Sequence"
            ],
            "text": "In the bioinformatics field, they can be used to simulate DNA sequences.[2][96]Markov processes can also be used to generate superficially real-looking text given a sample document.[2] Markov processes are used in a variety of recreational \"parody generator\" software (see dissociated press, Jeff Harrison,[93] Mark V Shaney[94][95])Markov chain models have been used in advanced baseball analysis since 1960, although their use is still rare. Each half-inning of a baseball game fits the Markov chain state when the number of runners and outs are considered. During any at-bat, there are 24 possible combinations of number of outs and position of the runners. Mark Pankin shows that Markov chain models can be used to evaluate runs created for both individual players as well as a team.[91] He also discusses various kinds of strategies and play conditions: how Markov chain models have been used to analyze statistics for game situations such as bunting and base stealing and differences when playing on grass vs. astroturf.[92]Usually musical systems need to enforce specific control constraints on the finite-length sequences they generate, but control constraints are not compatible with Markov models, since they induce long-range dependencies that violate the Markov hypothesis of limited memory. In order to overcome this limitation, a new approach has been proposed.[90]Markov chains can be used structurally, as in Xenakis's Analogique A and B.[88] Markov chains are also used in systems which use a Markov model to react interactively to music input.[89]A second-order Markov chain can be introduced by considering the current state and also the previous state, as indicated in the second table. Higher, nth-order chains tend to \"group\" particular notes together, while 'breaking off' into other patterns and sequences occasionally. These higher-order chains tend to generate results with a sense of phrasal structure, rather than the 'aimless wandering' produced by a first-order system.[87]Markov chains are employed in algorithmic music composition, particularly in software such as CSound, Max and SuperCollider. In a first-order chain, the states of the system become note or pitch values, and a probability vector for each note is constructed, completing a transition probability matrix (see below). An algorithm is constructed to produce output note values based on the transition matrix weightings, which could be MIDI note values, frequency (Hz), or any other desirable metric.[86]Markov chains can be used to model many games of chance. The children's games Snakes and Ladders and \"Hi Ho! Cherry-O\", for example, are represented exactly by Markov chains. At each turn, the player starts in a given state (on a given square) and from there has fixed odds of moving to certain other states (squares).Markov chains have been used in population genetics in order to describe the change in gene frequencies in small populations affected by genetic drift, for example in diffusion equation method described by Motoo Kimura.[2][85]Markov chains are also used in simulations of brain function, such as the simulation of the mammalian neocortex.[84]Markov chains also have many applications in biological modelling, particularly population processes, which are useful in modelling processes that are (at least) analogous to biological populations. The Leslie matrix, is one such example used to describe the population dynamics of many species, though some of its entries are not probabilities (they may be greater than 1). Another example is the modeling of cell shape in dividing sheets of epithelial cells.[83] Yet another example is the state of ion channels in cell membranes.Markov chains are generally used in describing path-dependent arguments, where current structural configurations condition future outcomes. An example is the reformulation of the idea, originally due to Karl Marx's Das Kapital, tying economic development to the rise of capitalism. In current research, it is common to use a Markov chain to model how once a country reaches a specific level of economic development, the configuration of structural factors, such as size of the middle class, the ratio of urban to rural residence, the rate of political mobilization, etc., will generate a higher probability of transitioning from authoritarian to democratic regime.[82]Credit rating agencies produce annual tables of the transition probabilities for bonds of different credit ratings.[81]Dynamic macroeconomics heavily uses Markov chains. An example is using Markov chains to exogenously model prices of equity (stock) in a general equilibrium setting.[80]Markov chains are used in finance and economics to model a variety of different phenomena, including asset prices and market crashes. The first financial model to use a Markov chain was from Prasad et al. in 1974.[dubious \u2013 discuss][76] Another was the regime-switching model of James D. Hamilton (1989), in which a Markov chain is used to model switches between periods high and low GDP growth (or alternatively, economic expansions and recessions).[77] A more recent example is the Markov Switching Multifractal model of Laurent E. Calvet and Adlai J. Fisher, which builds upon the convenience of earlier regime-switching models.[78][79] It uses an arbitrarily large Markov chain to drive the level of volatility of asset returns.Markov chain methods have also become very important for generating sequences of random numbers to accurately reflect very complicated desired probability distributions, via a process called Markov chain Monte Carlo (MCMC). In recent years this has revolutionized the practicability of Bayesian inference methods, allowing a wide range of posterior distributions to be simulated and their parameters found numerically.Markov models have also been used to analyze web navigation behavior of users. A user's web link transition on a particular website can be modeled using first- or second-order Markov models and can be used to make predictions regarding future navigation and to personalize the web page for an individual user.Numerous queueing models use continuous-time Markov chains. For example, an M/M/1 queue is a CTMC on the non-negative integers where upward transitions from i to i\u00a0+\u00a01 occur at rate \u03bb according to a Poisson process and describe job arrivals, while transitions from i to i\u00a0\u2013\u00a01 (for i\u00a0>\u00a01) occur at rate \u03bc (job service times are exponentially distributed) and describe completed services (departures) from the queue.Markov chains are the basis for the analytical treatment of queues (queueing theory). Agner Krarup Erlang initiated the subject in 1917.[72] This makes them critical for optimizing the performance of telecommunications networks, where messages must often compete for limited resources (such as bandwidth).[73]The LZMA lossless data compression algorithm combines Markov chains with Lempel-Ziv compression to achieve very high compression ratios.Markov chains are also the basis for hidden Markov models, which are an important tool in such diverse fields as telephone networks (which use the Viterbi algorithm for error correction), speech recognition and bioinformatics (such as in rearrangements detection[71]).Markov chains are used throughout information processing. Claude Shannon's famous 1948 paper A Mathematical Theory of Communication, which in a single step created the field of information theory, opens by introducing the concept of entropy through Markov modeling of the English language. Such idealized models can capture many of the statistical regularities of systems. Even without describing the full structure of the system perfectly, such signal models can make possible very effective data compression through entropy encoding techniques such as arithmetic coding. They also allow effective state estimation and pattern recognition. Markov chains also play an important role in reinforcement learning.Hidden Markov models are the basis for most modern automatic speech recognition systems.Several theorists have proposed the idea of the Markov chain statistical test (MCST), a method of conjoining Markov chains to form a \"Markov blanket\", arranging these chains in several recursive layers (\"wafering\") and producing more efficient test sets\u2014samples\u2014as a replacement for exhaustive testing. MCSTs also have uses in temporal state-based networks; Chilukuri et al.'s paper entitled \"Temporal Uncertainty Reasoning Networks for Evidence Fusion with Applications to Object Detection and Tracking\" (ScienceDirect) gives a background and case study for applying MCSTs to a wider range of applications.Similarly, it has been suggested that the crystallization and growth of some epitaxial superlattice oxide materials can be accurately described by Markov chains.[70]Also, the growth (and composition) of copolymers may be modeled using Markov chains. Based on the reactivity ratios of the monomers that make up the growing polymer chain, the chain's composition may be calculated (e.g., whether monomers tend to add in alternating fashion or in long runs of the same monomer). Due to steric effects, second-order Markov effects may also play a role in the growth of some polymer chains.An algorithm based on a Markov chain was also used to focus the fragment-based growth of chemicals in silico towards a desired class of compounds such as drugs or natural products.[69] As a molecule is grown, a fragment is selected from the nascent molecule as the \"current\" state. It is not aware of its past (i.e., it is not aware of what is already bonded to it). It then transitions to the next state when a fragment is attached to it. The transition probabilities are trained on databases of authentic classes of compounds.The classical model of enzyme activity, Michaelis\u2013Menten kinetics, can be viewed as a Markov chain, where at each time step the reaction proceeds in some direction. While Michaelis-Menten is fairly straightforward, far more complicated reaction networks can also be modeled with Markov chains.Markov chains and continuous-time Markov processes are useful in chemistry when physical systems closely approximate the Markov property. For example, imagine a large number n of molecules in solution in state A, each of which can undergo a chemical reaction to state B with a certain average rate. Perhaps the molecule is an enzyme, and the states refer to how it is folded. The state of any single enzyme follows a Markov chain, and since the molecules are essentially independent of each other, the number of molecules in state A or B at a time is n times the probability a given molecule is in that state.Markov chains are used in lattice QCD simulations.[68]The paths, in the path integral formulation of quantum mechanics, are Markov chains.[67]Markovian systems appear extensively in thermodynamics and statistical mechanics, whenever probabilities are used to represent unknown or unmodelled details of the system, if it can be assumed that the dynamics are time-invariant, and that no relevant history need be considered which is not already included in the state description.[66][citation needed]Research has reported the application and usefulness of Markov chains in a wide range of topics such as physics, chemistry, medicine, music, game theory and sports.Another discrete-time process that may be derived from a continuous-time Markov chain is a \u03b4-skeleton\u2014the (discrete-time) Markov chain formed by observing X(t) at intervals of \u03b4 units of time. The random variables X(0),\u00a0X(\u03b4),\u00a0X(2\u03b4),\u00a0... give the sequence of states visited by the \u03b4-skeleton.Note that S may be periodic, even if Q is not. Once \u03c0 is found, it must be normalized to a unit vector.where I is the identity matrix and diag(Q) is the diagonal matrix formed by selecting the main diagonal from the matrix Q and setting all other elements to zero.From this, S may be written asOne method of finding the stationary probability distribution, \u03c0, of an ergodic continuous-time Markov chain, Q, is by first finding its embedded Markov chain (EMC). Strictly speaking, the EMC is a regular discrete-time Markov chain, sometimes referred to as a jump process. Each element of the one-step transition probability matrix of the EMC, S, is denoted by sij, and represents the conditional probability of transitioning from state i into state j. These conditional probabilities may be found byA chain is said to be reversible if the reversed process is the same as the forward process. Kolmogorov's criterion states that the necessary and sufficient condition for a process to be reversible is that the product of transition rates around a closed loop must be the same in both directions.The hitting time is the time, starting in a given set of states until the chain arrives in a given state or set of states. The distribution of such a time period has a phase type distribution. The simplest such distribution is that of a single exponentially distributed transition.The image to the right describes a discrete-time Markov chain with state-space {1,2,3,4,5,6,7,8,9}. The player controls Pac-Man through a maze, eating pac-dots. Meanwhile, he is being hunted by ghosts. For convenience, the maze shall be a small 3x3-grid and the monsters move randomly in horizontal and vertical directions. A secret passageway between states 2 and 8 can be used in both directions. Entries with probability zero are removed in the following transition matrix:The stationary distribution of this chain can be found by solving \u03c0\u00a0Q\u00a0=\u00a00 subject to the constraint that elements must sum to 1 to obtainThe image to the right describes a continuous-time Markov chain with state-space {Bull market, Bear market, Stagnant market} and transition rate matrixwith the additional constraint thatObserve that each row has the same distribution as this does not depend on starting state. The row vector \u03c0 may be found by solving[65]as t\u00a0\u2192\u00a0\u221e the distribution tends toThe stationary distribution for an irreducible recurrent CTMC is the probability distribution to which the process converges for large values of t. Observe that for the two-state process considered earlier with P(t) given byis used.However, direct solutions are complicated to compute for larger matrices. The fact that Q is the generator for a semigroup of matricesThe above relation for forward matrix can be solved explicitly in this case to giveIn a simple case such as a CTMC on the state space {1,2}. The general Q matrix for such a process is the following 2\u00a0\u00d7\u00a02 matrix with \u03b1,\u03b2\u00a0>\u00a00where the prime denotes differentiation with respect to t. The solution to this equation is given by a matrix exponentialWrite P(t) for the matrix with entries pij = P(Xt\u00a0=\u00a0j\u00a0|\u00a0X0\u00a0=\u00a0i). Then the matrix P(t) satisfies the forward equation, a first-order differential equationAn example of a non-Markovian process with a Markovian representation is an autoregressive time series of order greater than one.[64]If Y has the Markov property, then it is a Markovian representation of X.In some cases, apparently non-Markovian processes may still have Markovian representations, constructed by expanding the concept of the 'current' and 'future' states. For example, let X be a non-Markovian process. Then define a process Y, such that each state of Y represents a time-interval of states of X. Mathematically, this takes the form:Considering a collection of Markov chains whose evolution takes in account the state of other Markov chains, is related to the notion of locally interacting Markov chains. This corresponds to the situation when the state space has a (Cartesian-) product form. See interacting particle system and stochastic cellular automata (probabilistic cellular automata). See for instance Interaction of Markov Processes[62] or[63]The use of Markov chains in Markov chain Monte Carlo methods covers cases where the process follows a continuous state space.Then we could collapse the sets into an auxiliary point \u03b1, and a recurrent Harris chain can be modified to contain \u03b1. Lastly, the collection of Harris chains is a comfortable level of generality, which is broad enough to contain a large number of interesting examples, yet restrictive enough to allow for a rich theory.Many results for Markov chains with finite state space can be generalized to chains with uncountable state space through Harris chains. The main idea is to see if there is a point in the state space that the chain hits with probability one. Generally, it is not true for continuous state space, however, we can define sets A and B along with a positive number \u03b5 and a probability measure \u03c1, such thatFor an overview of Markov chains on a general state space, see the article Markov chains on a measurable state space.A Bernoulli scheme is a special case of a Markov chain where the transition probability matrix has identical rows, which means that the next state is even independent of the current state (in addition to being independent of the past states). A Bernoulli scheme with only two possible states is known as a Bernoulli process.For example, consider the following Markov chain:Reversible Markov chains are common in Markov chain Monte Carlo (MCMC) approaches because the detailed balance equation for a desired distribution \u03c0 necessarily implies that the Markov chain has been constructed so that \u03c0 is a steady-state distribution. Even with time-inhomogeneous Markov chains, where multiple transition matrices are used, if each such transition matrix exhibits detailed balance with the desired \u03c0 distribution, this necessarily implies that \u03c0 is a steady-state distribution of the Markov chain.Kolmogorov's criterion gives a necessary and sufficient condition for a Markov chain to be reversible directly from the transition matrix probabilities. The criterion requires that the products of probabilities around every closed loop are the same in both directions around the loop.The left- and right-hand sides of this last equation are identical except for a reversing of the time indices n and\u00a0n\u00a0+\u00a01.If the Markov chain begins in the steady-state distribution, i.e., if Pr(X0\u00a0=\u00a0i)\u00a0=\u00a0\u03c0i, then Pr(Xn\u00a0=\u00a0i)\u00a0=\u00a0\u03c0i for all n and the detailed balance equation can be written asAs n was arbitrary, this reasoning holds for any n, and therefore for reversible Markov chains \u03c0 is always a steady-state distribution of Pr(Xn+1\u00a0=\u00a0j\u00a0|\u00a0Xn\u00a0=\u00a0i) for every\u00a0n.which essentially states that the total amount of money person j receives (including from himself) during the time-step equals the amount of money he pays others, which equals all the money he initially had because it was assumed that all money is spent (i.e. pji sums to 1 over i). The assumption is a technical one, because the money not really used is simply thought of as being paid from person j to himself (i.e. pjj is not necessarily zero).The single time-step from n to n\u00a0+\u00a01 can be thought of as each person i having \u03c0i dollars initially and paying each person j a fraction pij of it. The detailed balance condition states that upon each payment, the other person pays exactly the same amount of money back.[60] Clearly the total amount of money \u03c0 each person has remains the same after the time-step, since every dollar spent is balanced by a corresponding dollar received. This can be shown more formally by the equalitythe detailed balance equation can be written more compactly asConsidering a fixed arbitrary time n and using the shorthandfor all times n and all states i and j. This condition is known as the detailed balance condition (some books call it the local balance equation).A Markov chain is said to be reversible if there is a probability distribution \u03c0 over its states such thatIf we multiply x with P from right and continue this operation with the results, in the end we get the stationary distribution \u03c0. In other words, \u03c0 = ui \u2190 xPP...P = xPk as k \u2192 \u221e. That meansLet the eigenvalues be enumerated such that:Let U be the matrix of eigenvectors (each normalized to having an L2 norm equal to 1) where each column is a left eigenvector of P and let \u03a3 be the diagonal matrix of left eigenvalues of P, i.e. \u03a3 = diag(\u03bb1,\u03bb2,\u03bb3,...,\u03bbn). Then by eigendecompositionOne thing to notice is that if P has an element Pi,i on its main diagonal that is equal to 1 and the ith row or column is otherwise filled with 0's, then that row or column will remain unchanged in all of the subsequent powers Pk. Hence, the ith row or column of Q will have the 1 and the 0's in the same positions as in P.Here is one method for doing so: first, define the function f(A) to return the matrix A with its right-most column replaced with all 1's. If [f(P \u2212 In)]\u22121 exists then[57][citation needed]where In is the identity matrix of size n, and 0n,n is the zero matrix of size n\u00d7n. Multiplying together stochastic matrices always yields another stochastic matrix, so Q must be a stochastic matrix (see the definition above). It is sometimes sufficient to use the matrix equation above and the fact that Q is a stochastic matrix to solve for Q. Including the fact that the sum of each the rows in P is 1, there are n+1 equations for determining n unknowns, so it is computationally easier if on the one hand one selects one row in Q and substitute each of its elements by one, and on the other one substitute the corresponding element (the one in the same column) in the vector 0, and next left-multiply this latter vector by the inverse of transformed former matrix to find Q.Subtracting Q from both sides and factoring then yieldsIt is always true thatNote that this example illustrates a periodic Markov chain.If the Markov chain is irreducible and aperiodic, then there is a unique stationary distribution \u03c0. Additionally, in this case Pk converges to a rank-one matrix in which each row is the stationary distribution \u03c0, that is,If the Markov chain is time-homogeneous, then the transition matrix P is the same after each step, so the k-step transition probability can be computed as the k-th power of the transition matrix, Pk.By comparing this definition with that of an eigenvector we see that the two concepts are related and thatA stationary distribution \u03c0 is a (row) vector, whose entries are non-negative and sum to 1, is unchanged by the operation of transition matrix P on it and so is defined bySince each row of P sums to one and all elements are non-negative, P is a right stochastic matrix.If the state space is finite, the transition probability distribution can be represented by a matrix, called the transition matrix, with the (i, j)th element of P equal todoes exist for every integer\u00a0r.does not exist, although the limitIf a state i is periodic with period k\u00a0>\u00a01 then the limitand for any other state i, let fij be the probability that the chain ever visits state j if it starts at\u00a0i,A Markov chain with more than one state and just one out-going transition per state is either not irreducible or not aperiodic, hence cannot be ergodic.It can be shown that a finite state irreducible Markov chain is ergodic if it has an aperiodic state. More generally, a Markov chain is ergodic if there is a number N such that any state can be reached from any other state in any number of steps greater than or equal to a number N. In case of a fully connected transition matrix, where all transitions have a non-zero probability, this condition is fulfilled with\u00a0N\u00a0=\u00a01.A state i is said to be ergodic if it is aperiodic and positive recurrent. In other words, a state i is ergodic if it is recurrent, has a period of 1, and has finite mean recurrence time. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic.If every state can reach an absorbing state, then the Markov chain is an absorbing Markov chain.A state i is called absorbing if it is impossible to leave this state. Therefore, the state i is absorbing if and only ifIt can be shown that a state i is recurrent if and only if the expected number of visits to this state is infinite, i.e.,State i is positive recurrent (or non-null persistent) if Mi is finite; otherwise, state i is null recurrent (or null persistent).Even if the hitting time is finite with probability 1, it need not have a finite expectation. The mean recurrence time at state i is the expected return time Mi:State i is recurrent (or persistent) if it is not transient. Recurrent states are guaranteed (with probability 1) to have a finite hitting time. Recurrence and transience are class properties, that is, they either hold or do not hold equally for all members of a communicating class.is the probability that we return to state i for the first time after n steps. Therefore, state i is transient ifThe numberA state i is said to be transient if, given that we start in state i, there is a non-zero probability that we will never return to i. Formally, let the random variable Ti be the first return time to state i (the \"hitting time\"):Every state of a bipartite graph has an even period.If k = 1, then the state is said to be aperiodic. Otherwise (k\u00a0>\u00a01), the state is said to be periodic with period\u00a0k. A Markov chain is aperiodic if every state is aperiodic. An irreducible Markov chain only needs one aperiodic state to imply all states are aperiodic.(where \"gcd\" is the greatest common divisor) provided that this set is not empty. Otherwise the period is not defined. Note that even though a state has period k, it may not be possible to reach the state in k steps. For example, suppose it is possible to return to the state in {6,\u00a08,\u00a010,\u00a012,\u00a0...} time steps; k would be 2, even though 2 does not appear in this list.A state i has period k if any return to state i must occur in multiples of k time steps. Formally, the period of a state is defined asA Markov chain is said to be irreducible if its state space is a single communicating class; in other words, if it is possible to get to any state from any state.A state i is said to be essential or final if for all j such that i\u00a0\u2192\u00a0j it is also true that j\u00a0\u2192\u00a0i. A state i is inessential if it is not essential.[55] A state is final if and only if its communicating class is closed.A communicating class is closed if the probability of leaving the class is zero, namely if i is in C but j is not, then j is not accessible from\u00a0i. The set of communicating classes forms a directed, acyclic graph by inheriting the arrows from the original state space. A communicating class is closed if and only if it has no outgoing arrows in this graph.A state i is said to communicate with state j (written i\u00a0\u2194\u00a0j) if both i\u00a0\u2192\u00a0j and j\u00a0\u2192\u00a0i. A communicating class is a maximal set of states C such that every pair of states in C communicates with each other. Communication is an equivalence relation, and communicating classes are the equivalence classes of this relation.This integer is allowed to be different for each pair of states, hence the subscripts in nij. Allowing n to be zero means that every state is accessible from itself by definition. The accessibility relation is reflexive and transitive, but not necessarily symmetric.A state j is said to be accessible from a state i (written i\u00a0\u2192\u00a0j) if a system started in state i has a non-zero probability of transitioning into state j at some point. Formally, state j is accessible from state i if there exists an integer nij\u00a0\u2265\u00a00 such thatA Markov chain is said to be irreducible if it is possible to get to any state from any state. The following explains this definition more formally.Note: The superscript (n) is an index and not an exponent.The marginal distribution Pr(Xn\u00a0=\u00a0x) is the distribution over states at time n. The initial distribution is Pr(X0\u00a0=\u00a0x). The evolution of the process through one time step is described bywhere S is the state space of the Markov chain.The n-step transition probabilities satisfy the Chapman\u2013Kolmogorov equation, that for any k such that 0\u00a0<\u00a0k\u00a0<\u00a0n,andFor a time-homogeneous Markov chain:and the single-step transition isThe probability of going from state i to state j in n time steps iswith initial condition P(0) is the identity matrix.where pij is the solution of the forward equation (a first-order differential equation)For any value n = 0, 1, 2, 3, ... and times indexed up to this value of n: t0, t1, t2, ... and all states recorded at these times i0, i1, i2, i3, ... it holds thatDefine a discrete-time Markov chain Yn to describe the nth jump of the process and variables S1, S2, S3, ... to describe holding times in each of the states where Si follows the exponential distribution with rate parameter \u2212qYiYi.using little-o notation. The qij can be seen as measuring how quickly the transition from i to j happensLet Xt be the random variable describing the state of the process at time t, and assume that the process is in a state i at time t. Then Xt\u00a0+\u00a0h is independent of previous values (Xs\u00a0:\u00a0s\u2264\u00a0t) and as h \u2192 0 uniformly in t for all jThere are three equivalent definitions of the process.[54]A continuous-time Markov chain (Xt)t\u00a0\u2265\u00a00 is defined by a finite or countable state space S, a transition rate matrix Q with dimensions equal to that of the state space and initial probability distribution defined on the state space. For i\u00a0\u2260\u00a0j, the elements qij are non-negative and describe the rate of the process transitions from state i to state j. The elements qii are chosen such that each row of the transition rate matrix sums to zero.A finite state machine can be used as a representation of a Markov chain. Assuming a sequence of independent and identically distributed input signals (for example, symbols from a binary alphabet chosen by coin tosses), if the machine is in state y at time n, then the probability that it moves to state x at time n\u00a0+\u00a01 depends only on the current state.A thorough development and many examples can be found in the on-line monograph Meyn & Tweedie 2005.[53]Using the transition matrix it is possible to calculate, for example, the long-term fraction of weeks during which the market is stagnant, or the average number of weeks it will take to go from a stagnant to a bull market. Using the transition probabilities, the steady-state probabilities indicate that 62.5% of weeks will be in a bull market, 31.25% of weeks will be in a bear market and 6.25% of weeks will be stagnant, since:In particular, if at time n the system is in state 2\u00a0(bear), then at time n\u00a0+\u00a03 the distribution isThe distribution over states can be written as a stochastic row vector x with the relation x(n\u00a0+\u00a01)\u00a0=\u00a0x(n)P. So if at time n the system is in state x(n), then three time periods later, at time n\u00a0+\u00a03 the distribution isA state diagram for a simple example is shown in the figure on the right, using a directed graph to picture the state transitions. The states represent whether a hypothetical stock market is exhibiting a bull market, bear market, or stagnant market trend during a given week. According to the figure, a bull week is followed by another bull week 90% of the time, a bear week 7.5% of the time, and a stagnant week the other 2.5% of the time. Labelling the state space {1\u00a0=\u00a0bull, 2\u00a0=\u00a0bear, 3\u00a0=\u00a0stagnant} the transition matrix for this example iswhere m is finite, is a process satisfyingThe possible values of Xi form a countable set S called the state space of the chain.A discrete-time Markov chain is a sequence of random variables X1, X2, X3, ... with the Markov property, namely that the probability of moving to the next state depends only on the present state and not on the previous statesThe process described here is an approximation of a Poisson point process - Poisson processes are also Markov processes.The process described here is a Markov chain on a countable state space that follows a random walk.Andrei Kolmogorov developed in a 1931 paper a large part of the early theory of continuous-time Markov processes.[46][47] Kolmogorov was partly inspired by Louis Bachelier's 1900 work on fluctuations in the stock market as well as Norbert Wiener's work on Einstein's model of Brownian movement.[46][48] He introduced and studied a particular set of Markov processes known as diffusion processes, where he derived a set of differential equations describing the processes.[46][49] Independent of Kolmogorov's work, Sydney Chapman derived in a 1928 paper an equation, now called the Chapman\u2013Kolmogorov equation, in a less mathematically rigorous way than Kolmogorov, while studying Brownian movement.[50] The differential equations are now called the Kolmogorov equations[51] or the Kolmogorov\u2013Chapman equations.[52] Other mathematicians who contributed significantly to the foundations of Markov processes include William Feller, starting in 1930s, and then later Eugene Dynkin, starting in the 1950s.[47]In 1912 Poincar\u00e9 studied Markov chains on finite groups with an aim to study card shuffling. Other early uses of Markov chains include a diffusion model, introduced by Paul and Tatyana Ehrenfest in 1907, and a branching process, introduced by Francis Galton and Henry William Watson in 1873, preceding the work of Markov.[12][13] After the work of Galton and Watson, it was later revealed that their branching process had been independently discovered and studied around three decades earlier by Ir\u00e9n\u00e9e-Jules Bienaym\u00e9.[44] Starting in 1928, Maurice Fr\u00e9chet became interested in Markov chains, eventually resulting in him publishing in 1938 a detailed study on Markov chains.[12][45]Andrey Markov studied Markov chains in the early 20th century.[2] Markov was interested in studying an extension of independent random sequences, motivated by a disagreement with Pavel Nekrasov who claimed independence was necessary for the weak law of large numbers to hold.[2][43] In his first paper on Markov chains, published in 1906, Markov showed that under certain conditions the average outcomes of the Markov chain would converge to a fixed vector of values, so proving a weak law of large numbers without the independence assumption,[2][12][13][14] which had been commonly regarded as a requirement for such mathematical laws to hold.[14] Markov later used Markov chains to study the distribution of vowels in Eugene Onegin, written by Alexander Pushkin, and proved a central limit theorem for such chains.[2][12]A series of independent events (for example, a series of coin flips) satisfies the formal definition of a Markov chain. However, the theory is usually applied only when the probability distribution of the next step depends non-trivially on the current state.This creature's eating habits can be modeled with a Markov chain since its choice tomorrow depends solely on what it ate today, not what it ate yesterday or any other time in the past. One statistical property that could be calculated is the expected percentage, over a long period, of the days on which the creature will eat grapes.Another example is the dietary habits of a creature who eats only grapes, cheese, or lettuce, and whose dietary habits conform to the following rules:A famous Markov chain is the so-called \"drunkard's walk\", a random walk on the number line where, at each step, the position may change by +1 or \u22121 with equal probability. From any position there are two possible transitions, to the next or previous integer. The transition probabilities depend only on the current position, not on the manner in which the position was reached. For example, the transition probabilities from 5 to 4 and 5 to 6 are both 0.5, and all other transition probabilities from 5 are 0. These probabilities are independent of whether the system was previously in 4 or 6.Since the system changes randomly, it is generally impossible to predict with certainty the state of a Markov chain at a given point in the future.[2] However, the statistical properties of the system's future can be predicted.[2] In many applications, it is these statistical properties that are important.A discrete-time random process involves a system which is in a certain state at each step, with the state changing randomly between steps.[2] The steps are often thought of as moments in time, but they can equally well refer to physical distance or any other discrete measurement.[42] Formally, the steps are the integers or natural numbers, and the random process is a mapping of these to states. The Markov property states that the conditional probability distribution for the system at the next step (and in fact at all future steps) depends only on the current state of the system, and not additionally on the state of the system at previous steps.The changes of state of the system are called transitions.[40] The probabilities associated with various state changes are called transition probabilities.[40] The process is characterized by a state space, a transition matrix describing the probabilities of particular transitions, and an initial state (or initial distribution) across the state space.[41] By convention, we assume all possible states and transitions have been included in the definition of the process, so there is always a next state, and the process does not terminate.[2]While the time parameter is usually discrete, the state space of a Markov chain does not have any generally agreed-on restrictions: the term may refer to a process on an arbitrary state space.[39] However, many applications of Markov chains employ finite or countably infinite state spaces, which have a more straightforward statistical analysis. Besides time-index and state-space parameters, there are many other variations, extensions and generalizations (see Variations). For simplicity, most of this article concentrates on the discrete-time, discrete state-space case, unless mentioned otherwise.Note that there is no definitive agreement in the literature on the use of some of the terms that signify special cases of Markov processes. Usually the term \"Markov chain\" is reserved for a process with a discrete set of times, i.e. a discrete-time Markov chain (DTMC),[2][35][35] but a few authors use the term \"Markov process\" to refer to a continuous-time Markov chain (CTMC) without explicit mention.[36][37][38] In addition, there are other extensions of Markov processes that are referred to as such but do not necessarily fall within any of these four categories (see Markov model). Moreover, the time index need not necessarily be real-valued; like with the state space, there are conceivable processes that move through index sets with other mathematical constructs. Notice that the general state space continuous-time Markov chain is general to such a degree that it has no designated term.The system's state space and time parameter index need to be specified. The following table gives an overview of the different instances of Markov processes for different levels of state space generality and for discrete time v. continuous time:A Markov chain is a stochastic process with the Markov property.[2] The term \"Markov chain\" refers to the sequence of random variables such a process moves through, with the Markov property defining serial dependence only between adjacent periods (as in a \"chain\").[2] It can thus be used for describing systems that follow a chain of linked events, where what happens next depends only on the current state of the system.The adjective Markovian is used to describe something that is related to a Markov process.[34]Markov chains have many applications as statistical models of real-world processes,[26][27][28] such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, exchange rates of currencies, storage systems such as dams, and population growths of certain animal species.[29] The algorithm known as PageRank, which was originally proposed for the internet search engine Google, is based on a Markov process.[30][31] Furthermore, Markov processes are the basis for general stochastic simulation methods known as Gibbs sampling and Markov Chain Monte Carlo, are used for simulating random objects with specific probability distributions, and have found extensive application in Bayesian statistics.[29][32][33]Markov studied Markov processes in the early 20th century, publishing his first paper on the topic in 1906.[7][12][13][14] Random walks on integers and the gambler's ruin problem are examples of Markov processes.[15][16][17] Some variations of these processes were studied hundreds of years earlier in the context of independent variables.[18][19] Two important examples of Markov processes are the Wiener process, also known as the Brownian motion process, and the Poisson process,[20] which are considered the most important and central stochastic processes in the theory of stochastic processes,[21][22][23] and were discovered repeatedly and independently, both before and after 1906, in various settings.[24][25] These two processes are Markov processes in continuous time, while random walks on the integers and the gambler's ruin problem are examples of Markov processes in discrete time.[16][17]A Markov chain is a type of Markov process that has either discrete state space or discrete index set (often representing time), but the precise definition of a Markov chain varies.[6] For example, it is common to define a Markov chain as a Markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time),[7][8][9][10][11] but it is also common to define a Markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space).[6]In probability theory and related fields, a Markov process, named after the Russian mathematician Andrey Markov, is a stochastic process that satisfies the Markov property[2][3][4][5] (sometimes characterized as \"memorylessness\"). Roughly speaking, a process satisfies the Markov property if one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process's full history, hence independently from such history; i.e., conditional on the present state of the system, its future and past states are independent.A Markov chain is \"a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.\"[1]",
            "title": "Markov chain",
            "url": "https://en.wikipedia.org/wiki/Markov_chain"
        },
        {
            "desc_links": [
                "/wiki/Segmentation-based_object_categorization",
                "/wiki/Multivariate_statistics",
                "/wiki/Cluster_analysis",
                "/wiki/Spectrum_of_a_matrix",
                "/wiki/Eigenvalues",
                "/wiki/Similarity_matrix",
                "/wiki/Dimensionality_reduction"
            ],
            "links": [
                "/wiki/Conductance_(graph)",
                "/wiki/Scikit-learn",
                "/wiki/Apache_Spark#MLlib_Machine_Learning_Library\\MLlib",
                "/wiki/Power_iteration",
                "/wiki/R_(programming_language)",
                "/wiki/Nonlinear_dimensionality_reduction",
                "/wiki/Laplacian_matrix",
                "/wiki/Ill-conditioned",
                "/wiki/Preconditioner#Preconditioning_for_eigenvalue_problems",
                "/wiki/LOBPCG",
                "/wiki/Community_structure",
                "/wiki/Laplacian_matrix#Symmetric_normalized_Laplacian",
                "/wiki/Laplacian_matrix",
                "/wiki/Segmentation-based_object_categorization",
                "/wiki/Multivariate_statistics",
                "/wiki/Cluster_analysis",
                "/wiki/Spectrum_of_a_matrix",
                "/wiki/Eigenvalues",
                "/wiki/Similarity_matrix",
                "/wiki/Dimensionality_reduction"
            ],
            "text": "Ravi Kannan, Santosh Vempala and Adrian Vetta[11] proposed a bicriteria measure to define the quality of a given clustering. They said that a clustering was an (\u03b1, \u03b5)-clustering if the conductance of each cluster(in the clustering) was at least \u03b1 and the weight of the inter-cluster edges was at most \u03b5 fraction of the total weight of all the edges in the graph. They also look at two approximation algorithms in the same paper.This problem can be recast as,such thatFree software to implement spectral clustering is available in large open source projects like Scikit-learn,[6] MLlib for pseudo-eigenvector clustering using the power iteration method,[7] and R.[8]Spectral clustering is closely related to nonlinear dimensionality reduction, and dimension reduction techniques such as locally-linear embedding can be used to reduce errors from noise or outliers.[5]For large-sized graphs, the second eigenvalue of the (normalized) graph Laplacian matrix is often ill-conditioned, leading to slow convergence of iterative eigenvalue solvers. Preconditioning is a key technology accelerating the convergence, e.g., in the matrix-free LOBPCG method. Spectral clustering has been successfully applied on large graphs by first identifying their community structure, and then clustering communities.[4]rather than the symmetric normalized Laplacian matrix.Another possibility is to use the Laplacian matrix defined asSpectral clustering is well known to relate to partitioning of a mass-spring system, where each mass is associated with a data point and each spring stiffness corresponds to a weight of an edge describing a similarity of the two related data points. Specifically, the classical reference [1] explains that the eigenvalue problem describing transversal vibration modes of a mass-spring system is exactly the same as the eigenvalue problem for the graph Laplacian used for the spectral clustering. The masses that are tightly connected by the springs in the mass-spring system evidently move together from the equilibrium position in low-frequency vibration modes, so that the components of the eigenvectors corresponding to the smallest eigenvalues of the graph Laplacian can be used for meaningful clustering of the masses.In application to image segmentation, spectral clustering is known as segmentation-based object categorization.In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.",
            "title": "Spectral clustering",
            "url": "https://en.wikipedia.org/wiki/Spectral_clustering"
        },
        {
            "desc_links": [
                "/wiki/Civil_engineering",
                "/wiki/Aerospace_engineering",
                "/wiki/Nuclear_engineering",
                "/wiki/Biomedical_engineering",
                "/wiki/Mechanical_engineering",
                "/wiki/Geology",
                "/wiki/Physics",
                "/wiki/Materials_science",
                "/wiki/Anatomy",
                "/wiki/Dental_prosthesis",
                "/wiki/Surgical_implant",
                "/wiki/Euler-Bernoulli_beam_equation",
                "/wiki/Tensor",
                "/wiki/Continuum_mechanics",
                "/wiki/Solid",
                "/wiki/Deformation_(mechanics)",
                "/wiki/Force",
                "/wiki/Temperature",
                "/wiki/Phase_(chemistry)"
            ],
            "links": [
                "/wiki/Linear",
                "/wiki/Non-linear",
                "/wiki/Deformation_(mechanics)",
                "/wiki/Modulus_of_elasticity",
                "/wiki/Rheology",
                "/wiki/Fluid_mechanics",
                "/wiki/Civil_engineering",
                "/wiki/Aerospace_engineering",
                "/wiki/Nuclear_engineering",
                "/wiki/Biomedical_engineering",
                "/wiki/Mechanical_engineering",
                "/wiki/Geology",
                "/wiki/Physics",
                "/wiki/Materials_science",
                "/wiki/Anatomy",
                "/wiki/Dental_prosthesis",
                "/wiki/Surgical_implant",
                "/wiki/Euler-Bernoulli_beam_equation",
                "/wiki/Tensor",
                "/wiki/Continuum_mechanics",
                "/wiki/Solid",
                "/wiki/Deformation_(mechanics)",
                "/wiki/Force",
                "/wiki/Temperature",
                "/wiki/Phase_(chemistry)"
            ],
            "text": "There are four basic models that describe how a solid responds to an applied stress:It is most common for analysts in solid mechanics to use linear material models, due to ease of computation. However, real materials often exhibit non-linear behavior. As new materials are used and old ones are pushed to their limits, non-linear material models are becoming more common.A material has a rest shape and its shape departs away from the rest shape due to stress. The amount of departure from rest shape is called deformation, the proportion of deformation to original size is called strain. If the applied stress is sufficiently low (or the imposed strain is small enough), almost all solid materials behave in such a way that the strain is directly proportional to the stress; the coefficient of the proportion is called the modulus of elasticity. This region of deformation is known as the linearly elastic region.As shown in the following table, solid mechanics inhabits a central place within continuum mechanics. The field of rheology presents an overlap between solid and fluid mechanics.Solid mechanics is fundamental for civil, aerospace, nuclear, biomedical and mechanical engineering, for geology, and for many branches of physics such as materials science.[1] It has specific applications in many other areas, such as understanding the anatomy of living beings, and the design of dental prostheses and surgical implants. One of the most common practical applications of solid mechanics is the Euler-Bernoulli beam equation. Solid mechanics extensively uses tensors to describe stresses, strains, and the relationship between them.Solid mechanics is the branch of continuum mechanics that studies the behavior of solid materials, especially their motion and deformation under the action of forces, temperature changes, phase changes, and other external or internal agents.",
            "title": "Solid mechanics",
            "url": "https://en.wikipedia.org/wiki/Solid_mechanics"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Finite_deformation_tensor",
                "/wiki/Index_notation",
                "/wiki/Two-point_tensor",
                "/wiki/Engineering_stress",
                "/wiki/Orthonormal_basis",
                "/wiki/Finite_deformation_tensor",
                "/wiki/Cauchy_stress_tensor",
                "/wiki/Stress_measures",
                "/wiki/Stress_measures#Biot_stress",
                "/wiki/Stress_measures#Kirchhoff_stress",
                "/wiki/Finite_element_method",
                "/wiki/Finite_difference_method",
                "/wiki/Boundary_element_method",
                "/wiki/Linear_elasticity",
                "/wiki/Hooke%E2%80%99s_law",
                "/wiki/Elasticity_(physics)",
                "/wiki/Theory_of_elasticity",
                "/wiki/Infinitesimal_strain_theory",
                "/wiki/Plasticity_(physics)",
                "/wiki/Fracture",
                "/wiki/Phase_transition",
                "/wiki/Euler%27s_laws",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Linear_momentum",
                "/wiki/Angular_momentum",
                "/wiki/Euler-Cauchy_stress_principle",
                "/wiki/Partial_differential_equations",
                "/wiki/Strain_tensor",
                "/wiki/Boundary-value_problem",
                "/wiki/Constitutive_equations",
                "/wiki/Body_force",
                "/wiki/Bearing_(mechanical)",
                "/wiki/Static_equilibrium",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Stress_analysis",
                "/wiki/Applied_physics",
                "/wiki/Engineering",
                "/wiki/Tunnel",
                "/wiki/Dam",
                "/wiki/Geology",
                "/wiki/Plate_tectonics",
                "/wiki/Volcano",
                "/wiki/Avalanche",
                "/wiki/Biology",
                "/wiki/Anatomy",
                "/wiki/Stress_field",
                "/wiki/Shear_stress",
                "/wiki/Viscosity",
                "/wiki/Ductile",
                "/wiki/Brittle",
                "/wiki/Non-Newtonian_fluid",
                "/wiki/Infinitesimal_strain_theory",
                "/wiki/Finite_strain_theory",
                "/wiki/Piola%E2%80%93Kirchoff_stress_tensor",
                "/wiki/Stress_measures",
                "/wiki/Stress_measures",
                "/wiki/Curvature",
                "/wiki/Radius_of_curvature_(mathematics)",
                "/wiki/Infinitesimal",
                "/wiki/Mohr%27s_circle",
                "/wiki/Rotational_symmetry",
                "/wiki/Cylindrical_symmetry",
                "/wiki/Cylinder_stress",
                "/wiki/Z-tube",
                "/wiki/Axle",
                "/wiki/I-beam",
                "/wiki/Pipe_(fluid_conveyance)",
                "/wiki/Pressure_vessel",
                "/wiki/Linear_elasticity",
                "/wiki/Strength_of_materials",
                "/wiki/Plasticity_(physics)",
                "/wiki/Fracture",
                "/wiki/Cavitation",
                "/wiki/Crystal_structure",
                "/wiki/Chemistry",
                "/wiki/Birefringence",
                "/wiki/Polarizability",
                "/wiki/Permeability_(earth_sciences)",
                "/wiki/Deformation_(mechanics)",
                "/wiki/Spring_(device)",
                "/wiki/Gas",
                "/wiki/Plasma_(physics)",
                "/wiki/Temperature",
                "/wiki/Phase_(chemistry)",
                "/wiki/Impulse_(physics)",
                "/wiki/Piecewise",
                "/wiki/Continuous_function",
                "/wiki/Pressure",
                "/wiki/Pascal_(unit)",
                "/wiki/Newton_(force)",
                "/wiki/Square_metre",
                "/wiki/International_System_of_Units",
                "/wiki/Pound-force",
                "/wiki/Square_inch",
                "/wiki/Imperial_units",
                "/wiki/Inner_product",
                "/wiki/Compression_(physical)",
                "/wiki/Tension_(physics)",
                "/wiki/Shear_stress",
                "/wiki/Fluid",
                "/wiki/Hydrostatic_pressure",
                "/wiki/Solid",
                "/wiki/Fluid_dynamics",
                "/wiki/Liquid",
                "/wiki/Tensor",
                "/wiki/Cauchy_stress_tensor",
                "/wiki/Linear_map",
                "/wiki/Surface_normal",
                "/wiki/Cartesian_coordinates",
                "/wiki/Symmetric_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Homogeneous",
                "/wiki/Tensor_field",
                "/wiki/Macroscopic",
                "/wiki/Quantum_mechanics",
                "/wiki/Metal",
                "/wiki/Fiber",
                "/wiki/Wood",
                "/wiki/Torque",
                "/wiki/Energy",
                "/wiki/Laminar_flow",
                "/wiki/Galileo_Galilei",
                "/wiki/Experimental_method",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/Cartesian_coordinates",
                "/wiki/Analytic_geometry",
                "/wiki/Isaac_Newton",
                "/wiki/Newton%27s_laws",
                "/wiki/Calculus",
                "/wiki/Augustin-Louis_Cauchy",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Capital_(architecture)",
                "/wiki/Arch",
                "/wiki/Cupola",
                "/wiki/Truss",
                "/wiki/Flying_buttress",
                "/wiki/Gothic_architecture",
                "/wiki/Composite_bow",
                "/wiki/Glass_blowing",
                "/wiki/Engineering",
                "/wiki/Truss",
                "/wiki/Cross_section_(geometry)",
                "/wiki/Strain_rate_tensor",
                "/wiki/Linear_elasticity",
                "/wiki/Strength_of_materials",
                "/wiki/Plasticity_(physics)",
                "/wiki/Fracture",
                "/wiki/Cavitation",
                "/wiki/Crystal_structure",
                "/wiki/Chemistry",
                "/wiki/Prestressed_concrete",
                "/wiki/Tempered_glass",
                "/wiki/Newton%27s_laws_of_motion#Newton's_second_law",
                "/wiki/Thermal_expansion",
                "/wiki/Chemistry",
                "/wiki/Electromagnetic_field",
                "/wiki/Piezoelectricity",
                "/wiki/Magnetostriction",
                "/wiki/Gravity",
                "/wiki/Contact_force",
                "/wiki/Friction",
                "/wiki/Deformation_(mechanics)",
                "/wiki/Spring_(device)",
                "/wiki/Gas",
                "/wiki/Continuum_mechanics",
                "/wiki/Physical_quantity",
                "/wiki/Force",
                "/wiki/Particle",
                "/wiki/Deformation_(mechanics)#Strain",
                "/wiki/Solid",
                "/wiki/Weight",
                "/wiki/Liquid",
                "/wiki/Pressure",
                "/wiki/Pressure",
                "/wiki/Reaction_force",
                "/wiki/Intermolecular_force",
                "/wiki/Statistical_mechanics",
                "/wiki/Molecule"
            ],
            "text": "The 2nd Piola\u2013Kirchhoff stress tensor is energy conjugate to the Green\u2013Lagrange finite strain tensor.If the material rotates without a change in stress state (rigid rotation), the components of the 2nd Piola\u2013Kirchhoff stress tensor remain constant, irrespective of material orientation.This tensor, a one-point tensor, is symmetric.In index notation with respect to an orthonormal basis,The 1st Piola\u2013Kirchhoff stress is energy conjugate to the deformation gradient.If the material rotates without a change in stress state (rigid rotation), the components of the 1st Piola\u2013Kirchhoff stress tensor will vary with material orientation.Because it relates different coordinate systems, the 1st Piola\u2013Kirchhoff stress is a two-point tensor. In general, it is not symmetric. The 1st Piola\u2013Kirchhoff stress is the 3D generalization of the 1D concept of engineering stress.In terms of components with respect to an orthonormal basis, the first Piola\u2013Kirchhoff stress is given byIn the case of finite deformations, the Piola\u2013Kirchhoff stress tensors express the stress relative to the reference configuration. This is in contrast to the Cauchy stress tensor which expresses the stress relative to the present configuration. For infinitesimal deformations and rotations, the Cauchy and Piola\u2013Kirchhoff tensors are identical.Other useful stress measures include the first and second Piola\u2013Kirchhoff stress tensors, the Biot stress tensor, and the Kirchhoff stress tensor.Still, for two- or three-dimensional cases one must solve a partial differential equation problem. Analytical or closed-form solutions to the differential equations can be obtained when the geometry, constitutive relations, and boundary conditions are simple enough. Otherwise one must generally resort to numerical approximations such as the finite element method, the finite difference method, and the boundary element method.Stress analysis is simplified when the physical dimensions and the distribution of loads allow the structure to be treated as one- or two-dimensional. In the analysis of trusses, for example, the stress field may be assumed to be uniform and uniaxial over each member. Then the differential equations reduce to a finite set of equations (usually linear) with finitely many unknowns. In other contexts one may be able to reduce the three-dimensional problem to a two-dimensional one, and/or replace the general stress and strain tensors by simpler models like uniaxial tension/compression, simple shear, etc.However, engineered structures are usually designed so that the maximum expected stresses are well within the range of linear elasticity (the generalization of Hooke\u2019s law for continuous media); that is, the deformations caused by internal stresses are linearly related to them. In this case the differential equations that define the stress tensor are linear, and the problem becomes much easier. For one thing, the stress at any point will be a linear function of the loads, too. For small enough stresses, even non-linear systems can usually be assumed to be linear.Stress analysis for elastic structures is based on the theory of elasticity and infinitesimal strain theory. When the applied loads cause permanent deformation, one must use more complicated constitutive equations, that can account for the physical processes involved (plastic flow, fracture, phase change, etc.).The basic stress analysis problem can be formulated by Euler's equations of motion for continuous bodies (which are consequences of Newton's laws for conservation of linear momentum and angular momentum) and the Euler-Cauchy stress principle, together with the appropriate constitutive equations. Thus one obtains a system of partial differential equations involving the stress tensor field and the strain tensor field, as unknown functions to be determined. The external body forces appear as the independent (\"right-hand side\") term in the differential equations, while the concentrated forces appear as boundary conditions. The basic stress analysis problem is therefore a boundary-value problem.Stress analysis may be carried out experimentally, by applying loads to the actual artifact or to scale model, and measuring the resulting stresses, by any of several available methods. This approach is often used for safety certification and monitoring. However, most stress analysis is done by mathematical methods, especially during design.In stress analysis one normally disregards the physical causes of the forces or the precise nature of the materials. Instead, one assumes that the stresses are related to deformation (and, in non-static problems, to the rate of deformation) of the material by known constitutive equations.[11]The typical problem in stress analysis is to determine these internal stresses, given the external forces that are acting on the system. The latter may be body forces (such as gravity or magnetic attraction), that act throughout the volume of a material;[10]:p.42\u201381 or concentrated loads (such as friction between an axle and a bearing, or the weight of a train wheel on a rail), that are imagined to act over a two-dimensional area, or along a line, or at single point.Stress analysis is generally concerned with objects and structures that can be assumed to be in macroscopic static equilibrium. By Newton's laws of motion, any external forces are being applied to such a system must be balanced by internal reaction forces,[9]:p.97 which are almost always surface contact forces between adjacent particles \u2014 that is, as stress.[5] Since every particle needs to be in equilibrium, this reaction stress will generally propagate from particle, creating a stress distribution throughout the body.Stress analysis is a branch of applied physics that covers the determination of the internal distribution of internal forces in solid objects. It is an essential tool in engineering for the study and design of structures such as tunnels, dams, mechanical parts, and structural frames, under prescribed or expected loads. It is also important in many other disciplines; for example, in geology, to study phenomena like plate tectonics, vulcanism and avalanches; and in biology, to understand the anatomy of living beings.Solids, liquids, and gases have stress fields. Static fluids support normal stress but will flow under shear stress. Moving viscous fluids can support shear stress (dynamic pressure). Solids can support both shear and normal stress, with ductile materials failing under shear and brittle materials failing under normal stress. All materials have temperature dependent variations in stress-related properties, and non-Newtonian materials have rate-dependent variations.The Cauchy stress tensor is used for stress analysis of material bodies experiencing small deformations where the differences in stress distribution in most cases can be neglected. For large deformations, also called finite deformations, other measures of stress, such as the first and second Piola\u2013Kirchhoff stress tensors, the Biot stress tensor, and the Kirchhoff stress tensor, are required.The analysis of stress can be considerably simplified also for thin bars, beams or wires of uniform (or smoothly varying) composition and cross-section that are subjected to moderate bending and twisting. For those bodies, one may consider only cross-sections that are perpendicular to the bar's axis, and redefine a \"particle\" as being a piece of wire with infinitesimal length between two such cross sections. The ordinary stress is then reduced to a scalar (tension or compression of the bar), but one must take into account also a bending stress (that tries to change the bar's curvature, in some direction perpendicular to the axis) and a torsional stress (that tries to twist or un-twist it about its axis).In that view, one redefines a \"particle\" as being an infinitesimal patch of the plate's surface, so that the boundary between adjacent particles becomes an infinitesimal line element; both are implicitly extended in the third dimension, normal to (straight through) the plate. \"Stress\" is then redefined as being a measure of the internal forces between two adjacent \"particles\" across their common line element, divided by the length of that line. Some components of the stress tensor can be ignored, but since particles are not infinitesimal in the third dimension one can no longer ignore the torque that a particle applies on its neighbors. That torque is modeled as a bending stress that tends to change the curvature of the plate. However, these simplifications may not hold at welds, at sharp bends and creases (where the radius of curvature is comparable to the thickness of the plate).Man-made objects are often made from stock plates of various materials by operations that do not change their essentially two-dimensional character, like cutting, drilling, gentle bending and welding along the edges. The description of stress in such bodies can be simplified by modeling those parts as two-dimensional surfaces rather than three-dimensional bodies.In general, stress is not uniformly distributed over a material body, and may vary with time. Therefore, the stress tensor must be defined for each point and each moment, by considering an infinitesimal particle of the medium surrounding that point, and taking the average stresses in that particle as being the stresses at the point.The Cauchy stress tensor obeys the tensor transformation law under a change in the system of coordinates. A graphical representation of this transformation law is the Mohr's circle of stress distribution.Combined stresses cannot be described by a single vector. Even if the material is stressed in the same way throughout the volume of the body, the stress across any imaginary surface will depend on the orientation of that surface, in a non-trivial way.Parts with rotational symmetry, such as wheels, axles, pipes, and pillars, are very common in engineering. Often the stress patterns that occur in such parts have rotational or even cylindrical symmetry. The analysis of such cylinder stresses can take advantage of the symmetry to reduce the dimension of the domain and/or of the stress tensor.In these situations, the stress across any imaginary internal surface turns out to be equal in magnitude and always directed perpendicularly to the surface independently of the surface's orientation. This type of stress may be called isotropic normal or just isotropic; if it is compressive, it is called hydrostatic pressure or just pressure. Gases by definition cannot withstand tensile stresses, but some liquids may withstand surprisingly large amounts of isotropic tensile stress under some circumstances. see Z-tube.Another simple type of stress occurs when the material body is under equal compression or tension in all directions. This is the case, for example, in a portion of liquid or gas at rest, whether enclosed in some container or as part of a larger mass of fluid; or inside a cube of elastic material that is being pressed or pulled on all six faces by equal perpendicular forces \u2014 provided, in both cases, that the material is homogeneous, without built-in stress, and that the effect of gravity and other external forces can be neglected.As in the case of an axially loaded bar, in practice the shear stress may not be uniformly distributed over the layer; so, as before, the ratio F/A will only be an average (\"nominal\", \"engineering\") stress. However, that average is often sufficient for practical purposes.[8]:p.292 Shear stress is observed also when a cylindrical bar such as a shaft is subjected to opposite torques at its ends. In that case, the shear stress on each cross-section is parallel to the cross-section, but oriented tangentially relative to the axis, and increases with distance from the axis. Significant shear stress occurs in the middle plate (the \"web\") of I-beams under bending loads, due to the web constraining the end plates (\"flanges\").However, unlike normal stress, this simple shear stress is directed parallel to the cross-section considered, rather than perpendicular to it.[7] For any plane S that is perpendicular to the layer, the net internal force across S, and hence the stress, will be zero.Normal stress occurs in many other situations besides axial tension and compression. If an elastic bar with uniform and symmetric cross-section is bent in one of its planes of symmetry, the resulting bending stress will still be normal (perpendicular to the cross-section), but will vary over the cross section: the outer part will be under tensile stress, while the inner part will be compressed. Another variant of normal stress is the hoop stress that occurs on the walls of a cylindrical pipe or vessel filled with pressurized fluid.On the other hand, if one imagines the bar being cut along its length, parallel to the axis, there will be no force (hence no stress) between the two halves across the cut.In some situations, the stress within a body may adequately be described by a single number, or by a single vector (a number and a direction). Three such simple stress situations, that are often encountered in engineering design, are the uniaxial normal stress, the simple shear stress, and the isotropic normal stress.[7]The relation between stress and its effects and causes, including deformation and rate of change of deformation, can be quite complicated (although a linear approximation may be adequate in practice if the quantities are small enough). Stress that exceeds certain strength limits of the material will result in permanent deformation (such as plastic flow, fracture, cavitation) or even change its crystal structure and chemical composition.Conversely, stress is usually correlated with various effects on the material, possibly including changes in physical properties like birefringence, polarization, and permeability. The imposition of stress by an external agent usually creates some strain (deformation) in the material, even if it is too small to be detected. In a solid material, such strain will in turn generate an internal elastic stress, analogous to the reaction force of a stretched spring, tending to restore the material to its original undeformed state. Fluid materials (liquids, gases and plasmas) by definition can only oppose deformations that would change their volume. However, if the deformation is changing with time, even in fluids there will usually be some viscous stress, opposing that change.Stress in a material body may be due to multiple physical causes, including external influences and internal physical processes. Some of these agents (like gravity, changes in temperature and phase, and electromagnetic fields) act on the bulk of the material, varying continuously with position and time. Other agents (like external loads and friction, ambient pressure, and contact forces) may create stresses and forces that are concentrated on certain surfaces, lines, or points; and possibly also on very short time intervals (as in the impulses due to collisions). In general, the stress distribution in the body is expressed as a piecewise continuous function of space and time.The dimension of stress is that of pressure, and therefore its coordinates are commonly measured in the same units as pressure: namely, pascals (Pa, that is, newtons per square metre) in the International System, or pounds per square inch (psi) in the Imperial system. Because mechanical stresses easily exceed a million Pascals, MPa, which stands for megapascal, is a common unit of stress.If the normal unit vector n of the surface (pointing from Q towards P) is assumed fixed, the normal component can be expressed by a single number, the dot product T \u00b7 n. This number will be positive if P is \"pulling\" on Q (tensile stress), and negative if P is \"pushing\" against Q (compressive stress) The shear component is then the vector T \u2212 (T \u00b7 n)n.In general, the stress T that a particle P applies on another particle Q across a surface S can have any direction relative to S. The vector T may be regarded as the sum of two components: the normal stress (compression or tension) perpendicular to the surface, and the shear stress that is parallel to the surface.Quantitatively, the stress is expressed by the Cauchy traction vector T defined as the traction force F between adjacent parts of the material across an imaginary separating surface S, divided by the area of S.[5]:p.41\u201350 In a fluid at rest the force is perpendicular to the surface, and is the familiar pressure. In a solid, or in a flow of viscous liquid, the force F may not be perpendicular to S; hence the stress across a surface must be regarded a vector quantity, not a scalar. Moreover, the direction and magnitude generally depend on the orientation of S. Thus the stress state of the material must be described by a tensor, called the (Cauchy) stress tensor; which is a linear function that relates the normal vector n of a surface S to the stress T across S. With respect to any chosen coordinate system, the Cauchy stress tensor can be represented as a symmetric matrix of 3\u00d73 real numbers. Even within a homogeneous body, the stress tensor may vary from place to place, and may change over time; therefore, the stress within a material is, in general, a time-varying tensor field.Following the basic premises of continuum mechanics, stress is a macroscopic concept. Namely, the particles considered in its definition and analysis should be just small enough to be treated as homogeneous in composition and state, but still large enough to ignore quantum effects and the detailed motions of molecules. Thus, the force between two particles is actually the average of a very large number of atomic forces between their molecules; and physical quantities like mass, velocity, and forces that act through the bulk of three-dimensional bodies, like gravity, are assumed to be smoothly distributed over them.[4]:p.90\u2013106 Depending on the context, one may also assume that the particles are large enough to allow the averaging out of other microscopic features, like the grains of a metal rod or the fibers of a piece of wood.Stress is defined as the force across a \"small\" boundary per unit area of that boundary, for all orientations of the boundary.[3] Being derived from a fundamental physical quantity (force) and a purely geometrical quantity (area), stress is also a fundamental quantity, like velocity, torque or energy, that can be quantified and analyzed without explicit consideration of the nature of the material or of its physical causes.The understanding of stress in liquids started with Newton, who provided a differential formula for friction forces (shear stress) in parallel laminar flow.Ancient and medieval architects did develop some geometrical methods and simple formulas to compute the proper sizes of pillars and beams, but the scientific understanding of stress became possible only after the necessary tools were invented in the 17th and 18th centuries: Galileo Galilei's rigorous experimental method, Ren\u00e9 Descartes's coordinates and analytic geometry, and Newton's laws of motion and equilibrium and calculus of infinitesimals.[2] With those tools, Augustin-Louis Cauchy was able to give the first rigorous and general mathematical model for stress in a homogeneous medium.[citation needed] Cauchy observed that the force across an imaginary surface was a linear function of its normal vector; and, moreover, that it must be a symmetric function (with zero total momentum).[citation needed]Over several millennia, architects and builders, in particular, learned how to put together carefully shaped wood beams and stone blocks to withstand, transmit, and distribute stress in the most effective manner, with ingenious devices such as the capitals, arches, cupolas, trusses and the flying buttresses of Gothic cathedrals.Since ancient times humans have been consciously aware of stress inside materials. Until the 17th century, the understanding of stress was largely intuitive and empirical; and yet it resulted in some surprisingly sophisticated technology, like the composite bow and glass blowing.[1]In some branches of engineering, the term stress is occasionally used in a looser sense as a synonym of \"internal force\". For example, in the analysis of trusses, it may refer to the total traction or compression force acting on a beam, rather than the force divided by the area of its cross-section.The relation between mechanical stress, deformation, and the rate of change of deformation can be quite complicated, although a linear approximation may be adequate in practice if the quantities are small enough. Stress that exceeds certain strength limits of the material will result in permanent deformation (such as plastic flow, fracture, cavitation) or even change its crystal structure and chemical composition.Significant stress may exist even when deformation is negligible or non-existent (a common assumption when modeling the flow of water). Stress may exist in the absence of external forces; such built-in stress is important, for example, in prestressed concrete and tempered glass. Stress may also be imposed on a material without the application of net forces, for example by changes in temperature or chemical composition, or by external electromagnetic fields (as in piezoelectric and magnetostrictive materials).Strain inside a material may arise by various mechanisms, such as stress as applied by external forces to the bulk material (like gravity) or to its surface (like contact forces, external pressure, or friction). Any strain (deformation) of a solid material generates an internal elastic stress, analogous to the reaction force of a spring, that tends to restore the material to its original non-deformed state. In liquids and gases, only deformations that change the volume generate persistent elastic stress. However, if the deformation is gradually changing with time, even in fluids there will usually be some viscous stress, opposing that change. Elastic and viscous stresses are usually combined under the name mechanical stress.In continuum mechanics, stress is a physical quantity that expresses the internal forces that neighboring particles of a continuous material exert on each other, while strain is the measure of the deformation of the material. For example, when a solid vertical bar is supporting a weight, each particle in the bar pushes on the particles immediately below it. When a liquid is in a closed container under pressure, each particle gets pushed against by all the surrounding particles. The container walls and the pressure-inducing surface (such as a piston) push against them in (Newtonian) reaction. These macroscopic forces are actually the net result of a very large number of intermolecular forces and collisions between the particles in those molecules. Stress is frequently represented by a lowercase Greek letter sigma (\u03c3).",
            "title": "Stress (mechanics)",
            "url": "https://en.wikipedia.org/wiki/Stress_(mechanics)"
        },
        {
            "desc_links": [
                "/wiki/Matrix_algebra",
                "/wiki/Matrix_(math)",
                "/wiki/Geometry",
                "/wiki/Line_segment",
                "/wiki/Vertex_(geometry)",
                "/wiki/Polygon",
                "/wiki/Polyhedron",
                "/wiki/Edge_(geometry)",
                "/wiki/Ancient_Greek",
                "/wiki/Strabo",
                "/wiki/Euclid",
                "/wiki/Rhombus",
                "/wiki/Cuboid"
            ],
            "links": [
                "/wiki/Equivalence_class",
                "/wiki/Euler_characteristic",
                "/wiki/Vector_field",
                "/wiki/Circle",
                "/wiki/Betti_number",
                "/wiki/Torus",
                "/wiki/Lefschetz_fixed_point_theorem",
                "/wiki/Subset",
                "/wiki/Cartesian_product",
                "/wiki/Graph_of_a_relation",
                "/wiki/Equality_(mathematics)",
                "/wiki/Relation_(mathematics)",
                "/wiki/Graph_of_a_function",
                "/wiki/Identity_function",
                "/wiki/Fixed_point_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Diagonal_matrix",
                "/wiki/Cuboid",
                "/wiki/Triangle",
                "/wiki/Tetrahedron",
                "/wiki/Polyhedron",
                "/wiki/Solid_object",
                "/wiki/Three-dimensional_space",
                "/wiki/Two-dimensional_space",
                "/wiki/Face_(geometry)",
                "/wiki/Face_diagonal",
                "/wiki/Space_diagonal",
                "/wiki/Heptagon",
                "/wiki/Hexagon",
                "/wiki/Triangle",
                "/wiki/OEIS",
                "/wiki/Convex_polygon",
                "/wiki/Concurrent_lines",
                "/wiki/Polygon",
                "/wiki/Line_segment",
                "/wiki/Quadrilateral",
                "/wiki/Convex_polygon",
                "/wiki/Re-entrant_polygon",
                "/wiki/Association_football",
                "/wiki/Diagonal_(football)",
                "/wiki/Diagonal_lashing",
                "/wiki/Diagonal_pliers",
                "/wiki/Engineering",
                "/wiki/Scaffolding",
                "/wiki/Matrix_algebra",
                "/wiki/Matrix_(math)",
                "/wiki/Geometry",
                "/wiki/Line_segment",
                "/wiki/Vertex_(geometry)",
                "/wiki/Polygon",
                "/wiki/Polyhedron",
                "/wiki/Edge_(geometry)",
                "/wiki/Ancient_Greek",
                "/wiki/Strabo",
                "/wiki/Euclid",
                "/wiki/Rhombus",
                "/wiki/Cuboid"
            ],
            "text": "In geometric studies, the idea of intersecting the diagonal with itself is common, not directly, but by perturbing it within an equivalence class. This is related at a deep level with the Euler characteristic and the zeros of vector fields. For example, the circle S1 has Betti numbers 1, 1, 0, 0, 0, and therefore Euler characteristic 0. A geometric way of expressing this is to look at the diagonal on the two-torus S1xS1 and observe that it can move off itself by the small motion (\u03b8, \u03b8) to (\u03b8, \u03b8 + \u03b5). In general, the intersection number of the graph of a function with the diagonal may be computed using homology via the Lefschetz fixed point theorem; the self-intersection of the diagonal is the special case of the identity function.By analogy, the subset of the Cartesian product X\u00d7X of any set X with itself, consisting of all pairs (x,x), is called the diagonal, and is the graph of the equality relation on X or equivalently the graph of the identity function from X to x. This plays an important part in geometry; for example, the fixed points of a mapping F from X to itself may be obtained by intersecting the graph of F with the diagonal.The top-right to bottom-left diagonal is sometimes described as the minor diagonal or antidiagonal. The off-diagonal entries are those not on the main diagonal. A diagonal matrix is one whose off-diagonal entries are all zero.[12][13]A cuboid has two diagonals on each of the six faces and four space diagonals.Just as a triangle has no diagonals, so also a tetrahedron (with four triangular faces) has no face diagonals and no space diagonals.A polyhedron (a solid object in three-dimensional space, bounded by two-dimensional faces) may have two different types of diagonals: face diagonals on the various faces, connecting non-adjacent vertices on the same face; and space diagonals, entirely in the interior of the polyhedron (except for the endpoints on the vertices).In any regular n-gon with n even, the long diagonals all intersect each other at the polygon's center.A regular heptagon has 14 diagonals. The seven shorter ones equal each other, and the seven longer ones equal each other. The reciprocal of the side equals the sum of the reciprocals of a short and a long diagonal.A regular hexagon has nine diagonals: the six shorter ones are equal to each other in length; the three longer ones are equal to each other in length and intersect each other at the center of the hexagon. The ratio of a long diagonal to a side is 2.A triangle has no diagonals.This is OEIS sequence A006522.[6]For n-gons with n=3, 4, ... the number of regions is[5]In a convex polygon, if no three diagonals are concurrent at a single point in the interior, the number of regions that the diagonals divide the interior into is given byAs applied to a polygon, a diagonal is a line segment joining any two non-consecutive vertices. Therefore, a quadrilateral has two diagonals, joining opposite pairs of vertices. For any convex polygon, all the diagonals are inside the polygon, but for re-entrant polygons, some diagonals are outside of the polygon.In association football, the diagonal system of control is the method referees and assistant referees use to position themselves in one of the four quadrants of the pitch.A diagonal lashing is a type of lashing used to bind spars or poles together applied so that the lashings cross over the poles at an angle.Diagonal pliers are wire-cutting pliers defined by the cutting edges of the jaws intersects the joint rivet at an angle or \"on a diagonal\", hence the name.In engineering, a diagonal brace is a beam used to brace a rectangular structure (such as scaffolding) to withstand strong forces pushing into it; although called a diagonal, due to practical considerations diagonal braces are often not connected to the corners of the rectangle.There are also other, non-mathematical uses.In matrix algebra, a diagonal of a square matrix is a set of entries extending from one corner to the farthest corner.In geometry, a diagonal is a line segment joining two vertices of a polygon or polyhedron, when those vertices are not on the same edge. Informally, any sloping line is called diagonal. The word \"diagonal\" derives from the ancient Greek \u03b4\u03b9\u03b1\u03b3\u03ce\u03bd\u03b9\u03bf\u03c2 diagonios,[1] \"from angle to angle\" (from \u03b4\u03b9\u03ac- dia-, \"through\", \"across\" and \u03b3\u03c9\u03bd\u03af\u03b1 gonia, \"angle\", related to gony \"knee\"); it was used by both Strabo[2] and Euclid[3] to refer to a line connecting two vertices of a rhombus or cuboid,[4] and later adopted into Latin as diagonus (\"slanting line\").",
            "title": "Diagonal",
            "url": "https://en.wikipedia.org/wiki/Diagonal"
        },
        {
            "desc_links": [
                "/wiki/Cartesian_geometry",
                "/wiki/Hyperplane",
                "/wiki/Linear_transformation",
                "/wiki/Measure_(mathematics)",
                "/wiki/Three-dimensional_geometry",
                "/wiki/Laminar_flow",
                "/wiki/Rotation_(geometry)",
                "/wiki/Angle",
                "/wiki/Straight_angle",
                "/wiki/Line_segment",
                "/wiki/Parallelogram",
                "/wiki/Circle",
                "/wiki/Ellipse",
                "/wiki/Area",
                "/wiki/Collinear",
                "/wiki/Italic_font",
                "/wiki/Latin_alphabet",
                "/wiki/Cartesian_coordinates",
                "/wiki/Plane_geometry",
                "/wiki/Linear_map",
                "/wiki/Signed_distance_function",
                "/wiki/Straight_line",
                "/wiki/Parallel_(geometry)"
            ],
            "links": [
                "/wiki/Oblique_type",
                "/wiki/Digital_image",
                "/wiki/Pixel",
                "/wiki/Pythagorean_theorem",
                "/wiki/Geometric_mean_theorem#Based_on_shear_mappings",
                "/wiki/William_Kingdon_Clifford",
                "/wiki/Block_matrix",
                "/wiki/Direct_sum_of_vector_spaces",
                "/wiki/Vector_space",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Matrix_product",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Three-dimensional_geometry",
                "/wiki/Laminar_flow",
                "/wiki/Rotation_(geometry)",
                "/wiki/Angle",
                "/wiki/Straight_angle",
                "/wiki/Line_segment",
                "/wiki/Parallelogram",
                "/wiki/Circle",
                "/wiki/Ellipse",
                "/wiki/Area",
                "/wiki/Collinear",
                "/wiki/Italic_font",
                "/wiki/Latin_alphabet",
                "/wiki/Plane_geometry",
                "/wiki/Linear_map",
                "/wiki/Signed_distance_function",
                "/wiki/Straight_line",
                "/wiki/Parallel_(geometry)"
            ],
            "text": "The oblique type can be thought of as normal text under a shear.An algorithm due to Alan W. Paeth uses a sequence of three shear mappings (horizontal, vertical, then horizontal again) to rotate a digital image by an arbitrary angle. The algorithm is very simple to implement, and very efficient, since each step processes only one column or one row of pixels at a time.[4]The area-preserving property of a shear mapping can be used for results involving area. For instance, the Pythagorean theorem has been illustrated with shear mapping[3] as well as the related geometric mean theorem.The following applications of shear mapping were noted by William Kingdon Clifford:where M is a linear mapping from W\u2032 into W. Therefore in block matrix terms L can be represented ascorrespondingly, the typical shear fixing W is L whereTo be more precise, if V is the direct sum of W and W\u2032, and we write vectors asFor a vector space V and subspace W, a shear fixing W translates all vectors parallel to W.If the coordinates of a point are written as a column vector (a 2\u00d71 matrix), the shear mapping can be written as multiplication by a 2\u00d72 matrix:The same definition is used in three-dimensional geometry, except that the distance is measured from a fixed plane. A three-dimensional shearing transformation preserves the volume of solid figures, but changes areas of plane figures (except those that are parallel to the displacement). This transformation is used to describe laminar flow of a fluid between plates, one moving in a plane above and parallel to the first.Shear mappings must not be confused with rotations. Applying a shear map to a set of points of the plane will change all angles between them (except straight angles), and the length of any line segment that is not parallel to the direction of displacement. Therefore it will usually distort the shape of a geometric figure, for example turning squares into non-square parallelograms, and circles into ellipses. However a shearing does preserve the area of geometric figures and the alignment and relative distances of collinear points. A shear mapping is the main difference between the upright and slanted (or italic) styles of letters.In plane geometry, a shear mapping is a linear map that displaces each point in fixed direction, by an amount proportional to its signed distance from a line that is parallel to that direction.[1] This type of mapping is also called shear transformation, transvection, or just shearing.",
            "title": "Shear mapping",
            "url": "https://en.wikipedia.org/wiki/Shear_(mathematics)"
        },
        {
            "desc_links": [
                "/wiki/Greek_language",
                "/wiki/Science",
                "/wiki/Physical_body",
                "/wiki/Force",
                "/wiki/Displacement_(vector)",
                "/wiki/Ancient_Greece",
                "/wiki/Aristotle",
                "/wiki/Archimedes",
                "/wiki/History_of_classical_mechanics",
                "/wiki/Timeline_of_classical_mechanics",
                "/wiki/Early_modern_period",
                "/wiki/Galileo",
                "/wiki/Johannes_Kepler",
                "/wiki/Isaac_Newton",
                "/wiki/Classical_mechanics",
                "/wiki/Classical_physics"
            ],
            "links": [
                "/wiki/Quantum_mechanics",
                "/wiki/Field_theory_(physics)",
                "/wiki/Classical_field_theory",
                "/wiki/Quantum_field_theory",
                "/wiki/Electromagnetism",
                "/wiki/Gravitational",
                "/wiki/Wave_function",
                "/wiki/Orbit",
                "/wiki/Rotation",
                "/wiki/Atomic_nucleus",
                "/wiki/Elasticity_(physics)",
                "/wiki/Fluid",
                "/wiki/Degrees_of_freedom_(physics_and_chemistry)",
                "/wiki/Physical_body",
                "/wiki/Projectiles",
                "/wiki/Spacecraft",
                "/wiki/Star",
                "/wiki/Mechanical_engineering",
                "/wiki/Solids",
                "/wiki/Fluids",
                "/wiki/Gases",
                "/wiki/Liquids",
                "/wiki/General_relativity",
                "/wiki/Albert_Einstein",
                "/wiki/Quantum_mechanics",
                "/wiki/Galileo_Galilei",
                "/wiki/Isaac_Newton",
                "/wiki/Two_New_Sciences",
                "/wiki/Philosophi%C3%A6_Naturalis_Principia_Mathematica",
                "/wiki/Calculus",
                "/wiki/Newtonian_mechanics",
                "/wiki/Hibat_Allah_Abu%27l-Barakat_al-Baghdaadi",
                "/wiki/John_Philoponus",
                "/wiki/Projectile_motion",
                "/wiki/Theory_of_impetus",
                "/wiki/Jean_Buridan",
                "/wiki/Inertia",
                "/wiki/Velocity",
                "/wiki/Acceleration",
                "/wiki/Momentum",
                "/wiki/Oxford_Calculators",
                "/wiki/Thomas_Bradwardine",
                "/wiki/Aristotelian_mechanics",
                "/wiki/Hipparchus",
                "/wiki/Theory_of_everything",
                "/wiki/Einstein",
                "/wiki/General_relativity",
                "/wiki/Special_relativity",
                "/wiki/Theory_of_relativity",
                "/wiki/Isaac_Newton",
                "/wiki/Galileo",
                "/wiki/Speed_of_light",
                "/wiki/Classical_mechanics",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Relativistic_mechanics",
                "/wiki/Lorentz_transformations",
                "/wiki/Hendrik_Lorentz",
                "/wiki/Lorentz_factor",
                "/wiki/Galileo",
                "/wiki/Isaac_Newton",
                "/wiki/Albert_Einstein",
                "/wiki/Special_relativity",
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum",
                "/wiki/Correspondence_principle",
                "/wiki/Classical_mechanics",
                "/wiki/Quantum_mechanics",
                "/wiki/Isaac_Newton",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Philosophi%C3%A6_Naturalis_Principia_Mathematica",
                "/wiki/Exact_science",
                "/wiki/Mathematics",
                "/wiki/Experiment",
                "/wiki/Greek_language",
                "/wiki/Science",
                "/wiki/Physical_body",
                "/wiki/Force",
                "/wiki/Displacement_(vector)",
                "/wiki/Ancient_Greece",
                "/wiki/Aristotle",
                "/wiki/Archimedes",
                "/wiki/History_of_classical_mechanics",
                "/wiki/Timeline_of_classical_mechanics",
                "/wiki/Early_modern_period",
                "/wiki/Galileo",
                "/wiki/Johannes_Kepler",
                "/wiki/Isaac_Newton",
                "/wiki/Classical_mechanics",
                "/wiki/Classical_physics"
            ],
            "text": "The following are categorized as being part of quantum mechanics:The following are described as forming classical mechanics:Note that there is also the \"theory of fields\" which constitutes a separate discipline in physics, formally treated as distinct from mechanics, whether classical fields or quantum fields. But in actual practice, subjects belonging to mechanics and fields are closely interwoven. Thus, for instance, forces that act on particles are frequently derived from fields (electromagnetic or gravitational), and particles generate fields by acting as sources. In fact, in quantum mechanics, particles themselves are fields, as described theoretically by the wave function.The following are two lists of various subjects that are studied in mechanics.For instance, the motion of a spacecraft, regarding its orbit and attitude (rotation), is described by the relativistic theory of classical mechanics, while the analogous movements of an atomic nucleus are described by quantum mechanics.Otherwise, bodies may be semi-rigid, i.e. elastic, or non-rigid, i.e. fluid. These subjects have both classical and quantum divisions of study.Other distinctions between the various sub-disciplines of mechanics, concern the nature of the bodies being described. Particles are bodies with little (known) internal structure, treated as mathematical points in classical mechanics. Rigid bodies have size and shape, but retain a simplicity close to that of the particle, adding just a few so-called degrees of freedom, such as orientation in space.The often-used term body needs to stand for a wide assortment of objects, including particles, projectiles, spacecraft, stars, parts of machinery, parts of solids, parts of fluids (gases and liquids), etc.Two main modern developments in mechanics are general relativity of Einstein, and quantum mechanics, both developed in the 20th century based in part on earlier 19th-century ideas. The development in the modern continuum mechanics, particularly in the areas of elasticity, plasticity, fluid dynamics, electrodynamics and thermodynamics of deformable media, started in the second half of the 20th century.There is some dispute over priority of various ideas: Newton's Principia is certainly the seminal work and has been tremendously influential, and the systematic mathematics therein did not and could not have been stated earlier because calculus had not been developed. However, many of the ideas, particularly as pertain to inertia (impetus) and falling bodies had been developed and stated by earlier researchers, both the then-recent Galileo and the less-known medieval predecessors. Precise credit is at times difficult or contentious because scientific language and standards of proof changed, so whether medieval statements are equivalent to modern statements or sufficient proof, or instead similar to modern statements and hypotheses is often debatable.Two central figures in the early modern age are Galileo Galilei and Isaac Newton. Galileo's final statement of his mechanics, particularly of falling bodies, is his Two New Sciences (1638). Newton's 1687 Philosophi\u00e6 Naturalis Principia Mathematica provided a detailed mathematical account of mechanics, using the newly developed mathematics of calculus and providing the basis of Newtonian mechanics.[5]On the question of a body subject to a constant (uniform) force, the 12th-century Jewish-Arab Nathanel (Iraqi, of Baghdad) stated that constant force imparts constant acceleration, while the main properties are uniformly accelerated motion (as of falling bodies) was worked out by the 14th-century Oxford Calculators.In the Middle Ages, Aristotle's theories were criticized and modified by a number of figures, beginning with John Philoponus in the 6th century. A central problem was that of projectile motion, which was discussed by Hipparchus and Philoponus. This led to the development of the theory of impetus by 14th-century French priest Jean Buridan, which developed into the modern theories of inertia, velocity, acceleration and momentum. This work and others was developed in 14th-century England by the Oxford Calculators such as Thomas Bradwardine, who studied and formulated various laws regarding falling bodies.The main theory of mechanics in antiquity was Aristotelian mechanics.[4] A later developer in this tradition is Hipparchus.[5]Relativistic corrections are also needed for quantum mechanics, although general relativity has not been integrated. The two theories remain incompatible, a hurdle which must be overcome in developing a theory of everything.In analogy to the distinction between quantum and classical mechanics, Einstein's general and special theories of relativity have expanded the scope of Newton and Galileo's formulation of mechanics. The differences between relativistic and Newtonian mechanics become significant and even dominant as the velocity of a massive body approaches the speed of light. For instance, in Newtonian mechanics, Newton's laws of motion specify that F = ma, whereas in Relativistic mechanics and Lorentz transformations, which were first discovered by Hendrik Lorentz, F = \u03b3ma (where \u03b3 is the Lorentz factor, which is almost equal to 1 for low speeds).Often cited as the father of modern science, Galileo brought together the ideas of other great thinkers of his time and began to analyze motion in terms of distance traveled from some starting position and the time that it took. He showed that the speed of falling objects increases steadily during the time of their fall. This acceleration is the same for heavy objects as for light ones, provided air friction (air resistance) is discounted. The English mathematician and physicist Isaac Newton improved this analysis by defining force and mass and relating these to acceleration. For objects traveling at speeds close to the speed of light, Newton\u2019s laws were superseded by Albert Einstein\u2019s theory of relativity. For atomic and subatomic particles, Newton\u2019s laws were superseded by quantum theory. For everyday phenomena, however, Newton\u2019s three laws of motion remain the cornerstone of dynamics, which is the study of what causes motion.Quantum mechanics is of a bigger scope, as it encompasses classical mechanics as a sub-discipline which applies under certain restricted circumstances. According to the correspondence principle, there is no contradiction or conflict between the two subjects, each simply pertains to specific situations. The correspondence principle states that the behavior of systems described by quantum theories reproduces classical physics in the limit of large quantum numbers. Quantum mechanics has superseded classical mechanics at the foundation level and is indispensable for the explanation and prediction of processes at the molecular, atomic, and sub-atomic level. However, for macroscopic processes classical mechanics is able to solve problems which are unmanageably difficult in quantum mechanics and hence remains useful and well used. Modern descriptions of such behavior begin with a careful definition of such quantities as displacement (distance moved), time, velocity, acceleration, mass, and force. Until about 400 years ago, however, motion was explained from a very different point of view. For example, following the ideas of Greek philosopher and scientist Aristotle, scientists reasoned that a cannonball falls down because its natural position is in the Earth; the sun, the moon, and the stars travel in circles around the earth because it is the nature of heavenly objects to travel in perfect circles.Historically, classical mechanics came first, while quantum mechanics is a comparatively recent invention. Classical mechanics originated with Isaac Newton's laws of motion in Philosophi\u00e6 Naturalis Principia Mathematica; Quantum Mechanics was discovered in the early 20th century. Both are commonly held to constitute the most certain knowledge that exists about physical nature. Classical mechanics has especially often been viewed as a model for other so-called exact sciences. Essential in this respect is the extensive use of mathematics in theories, as well as the decisive role played by experiment in generating and testing them.Mechanics (Greek \u03bc\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ae) is that area of science which is concerned with the behaviour of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment. The scientific discipline has its origins in Ancient Greece with the writings of Aristotle and Archimedes[1][2][3] (see History of classical mechanics and Timeline of classical mechanics). During the early modern period, scientists such as Galileo, Kepler, and Newton, laid the foundation for what is now known as classical mechanics. It is a branch of classical physics that deals with particles that are either at rest or are moving with velocities significantly less than the speed of light. It can also be defined as a branch of science which deals with the motion of and forces on objects.",
            "title": "Mechanics",
            "url": "https://en.wikipedia.org/wiki/Mechanics"
        },
        {
            "desc_links": [
                "/wiki/Diagonal_matrix",
                "/wiki/Rigid_body",
                "/wiki/Tensor",
                "/wiki/Torque",
                "/wiki/Angular_acceleration",
                "/wiki/Intensive_and_extensive_properties"
            ],
            "links": [
                "/wiki/Ellipsoid",
                "/wiki/Poinsot%27s_ellipsoid",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Sylvester%27s_law_of_inertia",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Unit_vector",
                "/wiki/Jacobi_identity",
                "/wiki/Cross_product",
                "/wiki/Kinematics",
                "/wiki/Centre_of_mass",
                "/wiki/Centre_of_mass",
                "/wiki/Cross_product#Conversion_to_matrix_multiplication",
                "/wiki/Centre_of_mass",
                "/wiki/Kinematics",
                "/wiki/Resultant_force",
                "/wiki/Moment_(physics)",
                "/wiki/Centre_of_mass",
                "/wiki/Mechanical_system",
                "/wiki/List_of_moments_of_inertia",
                "/wiki/Parallel_axis_theorem",
                "/wiki/Second_moment_of_area",
                "/wiki/Beam_(structure)",
                "/wiki/Multiple_integral",
                "/wiki/Kater%27s_pendulum",
                "/wiki/Gravimeter",
                "/wiki/Angular_momentum",
                "/wiki/Angular_velocity",
                "/wiki/Radius_of_gyration",
                "/wiki/Simple_pendulum",
                "/wiki/Rotation_around_a_fixed_axis",
                "/wiki/Torque",
                "/wiki/Angular_acceleration",
                "/wiki/Figure_skating_spins",
                "/wiki/Diving",
                "/wiki/Diving#Dive_positions",
                "/wiki/Angular_momentum",
                "/wiki/Angular_velocity",
                "/wiki/Angular_momentum",
                "/wiki/Kinetic_energy",
                "/wiki/Rigid_body_dynamics",
                "/wiki/Christiaan_Huygens",
                "/wiki/Compound_pendulum",
                "/wiki/Leonhard_Euler",
                "/wiki/Euler%27s_laws#Euler's_second_law",
                "/wiki/Torque",
                "/wiki/Angular_momentum",
                "/wiki/Angular_acceleration",
                "/wiki/Angular_velocity",
                "/wiki/SI",
                "/wiki/Imperial_units",
                "/wiki/United_States_customary_units",
                "/wiki/Diagonal_matrix"
            ],
            "text": "Thus, the magnitude of a point x in the direction n on the inertia ellipsoid isLet a point x on this ellipsoid be defined in terms of its magnitude and direction, x = |x|n, where n is a unit vector. Then the relationship presented above, between the inertia matrix and the scalar moment of inertia In around an axis in the direction n, yieldsto see that the semi-principal diameters of this ellipsoid are given bydefines an ellipsoid in the body frame. Write this equation in the form,orThe moment of inertia matrix in body-frame coordinates is a quadratic form that defines a surface in the body called Poinsot's ellipsoid.[28] Let \u039b be the inertia matrix relative to the centre of mass aligned with the principal axes, then the surfaceFor bodies with constant density an axis of rotational symmetry is a principal axis.The columns of the rotation matrix Q define the directions of the principal axes of the body, and the constants I1, I2, and I3 are called the principal moments of inertia. This result was first shown by J. J. Sylvester (1852), and is a form of Sylvester's law of inertia.[26][27]whereMeasured in the body frame the inertia matrix is a constant real symmetric matrix. A real symmetric matrix has the eigendecomposition into the product of a rotation matrix Q and a diagonal matrix \u039b, given byNotice that A changes as the body moves, while IB\nC remains constant.where vectors y in the body fixed coordinate frame have coordinates x in the inertial frame. Then, the inertia matrix of the body measured in the inertial frame is given byLet the body frame inertia matrix relative to the centre of mass be denoted IB\nC, and define the orientation of the body frame relative to the inertial frame by the rotation matrix A, such that,The use of the inertia matrix in Newton's second law assumes its components are computed relative to axes parallel to the inertial frame and not relative to a body-fixed reference frame.[6][23] This means that as the body moves the components of the inertia matrix change with time. In contrast, the components of the inertia matrix measured in a body-fixed frame are constant.It is common in rigid body mechanics to use notation that explicitly identifies the x, y, and z axes, such as Ixx and Ixy, for the components of the inertia tensor.The components of tensors of degree two can be assembled into a matrix. For the inertia tensor this matrix is given by,and can be interpreted as the moment of inertia around the x-axis when the object rotates around the y-axis.where the dot product is taken with the corresponding elements in the component tensors. A product of inertia term such as I12 is obtained by the computationThe inertia tensor can be used in the same way as the inertia matrix to compute the scalar moment of inertia about an arbitrary axis in the direction n,where r defines the coordinates of a point in the body and \u03c1(r) is the mass density at that point. The integral is taken over the volume V of the body. The inertia tensor is symmetric because Iij =\u00a0Iji.The inertia tensor for a continuous body is given byIn this case, the components of the inertia tensor are given bywhere E is the identity tensorFor a rigid system of particles Pk, k = 1, \u2026, N each of mass mk with position coordinates rk\u00a0=\u00a0(xk, yk, zk), the inertia tensor is given byThis tensor is of degree two because the component tensors are each constructed from two basis vectors. In this form the inertia tensor is also called the inertia binor.where ei, i\u00a0=\u00a01,\u00a02,\u00a03 are the three orthogonal unit vectors defining the inertial frame in which the body moves. Using this basis the inertia tensor is given byThis shows that the inertia matrix can be used to calculate the moment of inertia of a body around any specified rotation axis in the body.where IR is the moment of inertia matrix of the system relative to the reference point R.Thus, the moment of inertia around the line L through R in the direction k\u0302 is obtained from the calculationwhere the dot and the cross products have been interchanged. Exchanging products, and simplifying by noting that \u0394ri and k\u0302 are orthogonal:The simplification of this equation uses the triple scalar product identityThe magnitude squared of the perpendicular vector isnoting that k\u0302 is a unit vector.To relate this scalar moment of inertia to the inertia matrix of the body, introduce the skew-symmetric matrix [k\u0302] such that [k\u0302]y = k\u0302 \u00d7 y, then we have the identitywhere E is the identity matrix, so as to avoid confusion with the inertia matrix, and k\u0302\u2009k\u0302T is the outer product matrix formed from the unit vector k\u0302 along the line\u00a0L.This is derived as follows. Let a rigid assembly of N particles, Pi, i = 1, \u2026, N, have coordinates ri. Choose R as a reference point and compute the moment of inertia around a line L defined by the unit vector k\u0302 through the reference point R, L(t) = R + tk\u0302. The perpendicular vector from this line to the particle Pi is obtained from \u0394ri by removing the component that projects onto k\u0302.where IR is the moment of inertia matrix of the system relative to the reference point R, and [\u0394ri] is the skew symmetric matrix obtained from the vector \u0394ri = ri \u2212 R.The scalar moment of inertia, IL, of a body about a specified axis whose direction is specified by the unit vector k\u0302 and passes through the body at a point R is as follows:[6]Note on the minus sign: By using the skew symmetric matrix of position vectors relative to the reference point, the inertia matrix of each particle has the form \u2212m[r]2, which is similar to the mr2 that appears in planar movement. However, to make this to work out correctly a minus sign is needed. This minus sign can be absorbed into the term m[r]T[r], if desired, by using the skew-symmetry property of [r].where d is the vector from the centre of mass C to the reference point R.The result is the parallel axis theorem,The first term is the inertia matrix IC relative to the centre of mass. The second and third terms are zero by definition of the centre of mass C. And the last term is the total mass of the system multiplied by the square of the skew-symmetric matrix [d] constructed from d.Distribute over the cross product to obtainwhere d is the vector from the centre of mass C to the reference point R. Use this equation to compute the inertia matrix,Let C be the centre of mass of the rigid system, thenConsider the inertia matrix IR obtained for a rigid system of particles measured relative to a reference point R, given byThe inertia matrix of a body depends on the choice of the reference point. There is a useful relationship between the inertia matrix relative to the centre of mass C and the inertia matrix relative to another point R. This relationship is called the parallel axis theorem.[3][6]where IC is the inertia matrix relative to the centre of mass.Thus, the resultant torque on the rigid system of particles is given byobtained from the Jacobi identity for the triple cross product as shown in the proof below:The calculation uses the identityUse the centre of mass C as the reference point, and introduce the skew-symmetric matrix [\u0394ri] = [ri \u2212 C] to represent the cross product (ri \u2212 C) \u00d7, to obtainwhere ai is the acceleration of the particle Pi. The kinematics of a rigid body yields the formula for the acceleration of the particle Pi in terms of the position R and acceleration Ar of the reference point, as well as the angular velocity vector \u03c9 and angular acceleration vector \u03b1 of the rigid system as,The inertia matrix appears in the application of Newton's second law to a rigid assembly of particles. The resultant torque on this system is,[3][6]where IC is the inertia matrix relative to the centre of mass and M is the total mass.Thus, the kinetic energy of the rigid system of particles is given byThe second term in this equation is zero because C is the centre of mass. Introduce the skew-symmetric matrix [\u0394ri] so the kinetic energy becomesThis equation expands to yield three termswhere \u0394ri = ri\u00a0\u2212\u00a0C is the position vector of a particle relative to the centre of mass.The kinetic energy of a rigid system of particles can be formulated in terms of the centre of mass and a matrix of mass moments of inertia of the system. Let the system of particles Pi, i = 1, \u2026,n be located at the coordinates ri with velocities vi, then the kinetic energy is[3][6]is the symmetric inertia matrix of the rigid system of particles measured relative to the centre of mass C.where IC defined byThen, the skew-symmetric matrix [\u0394ri] obtained from the relative position vector \u0394ri = ri \u2212 C, can be used to define,where the terms containing VR (= C) sum to zero by the definition of centre of mass.The inertia matrix is constructed by considering the angular momentum, with the reference point R of the body chosen to be the centre of mass C:[3][6]Note that the cross product can be equivalently written as matrix multiplication by combining the first operand and the operator into a, skew-symmetric, matrix, [b], constructed from the components of b = (bx, by, bz):where \u03c9 is the angular velocity of the system, and VR is the velocity of R.and the (absolute) velocities areLet the system of particles Pi, i = 1, \u2026, n be located at the coordinates ri with velocities vi relative to a fixed reference frame. For a (possibly moving) reference point R, the relative positions areThe scalar moments of inertia appear as elements in a matrix when a system of particles is assembled into a rigid body that moves in three-dimensional space. This inertia matrix appears in the calculation of the angular momentum, kinetic energy and resultant torque of the rigid system of particles.[3][4][5][6][25]Use the centre of mass C as the reference point and define the moment of inertia relative to the centre of mass IC, then the equation for the resultant torque simplifies to[18]:1029where \u00eai \u00d7 \u00eai = 0, and \u00eai \u00d7 t\u0302i = k\u0302 is the unit vector perpendicular to the plane for all of the particles Pi.This yields the resultant torque on the system asFor systems that are constrained to planar movement, the angular velocity and angular acceleration vectors are directed along k\u0302 perpendicular to the plane of movement, which simplifies this acceleration equation. In this case, the acceleration vectors can be simplified by introducing the unit vectors \u00eai from the reference point R to a point ri and the unit vectors t\u0302i = k\u0302 \u00d7 \u00eai , soThe kinematics of a rigid body yields the formula for the acceleration of the particle Pi in terms of the position R and acceleration A of the reference particle as well as the angular velocity vector \u03c9 and angular acceleration vector \u03b1 of the rigid system of particles as,where ri denotes the trajectory of each particle.Newton's laws for a rigid system of N particles, Pi, i = 1, \u2026 N, can be written in terms of a resultant force and torque at a reference point R, to yield[14][17]The moment of inertia IC is the polar moment of inertia of the body.Let the reference point be the centre of mass C of the system so the second term becomes zero, and introduce the moment of inertia IC so the kinetic energy is given by[18]:1084The kinetic energy of a rigid system of particles moving in the plane is given by[14][17]For a given amount of angular momentum, a decrease in the moment of inertia results in an increase in the angular velocity. Figure skaters can change their moment of inertia by pulling in their arms. Thus, the angular velocity achieved by a skater with outstretched arms results in a greater angular velocity when the arms are pulled in, because of the reduced moment of inertia. A figure skater is not, however, a rigid body.The moment of inertia IC about an axis perpendicular to the movement of the rigid system and through the centre of mass is known as the polar moment of inertia. Specifically, it is the second moment of mass with respect to the orthogonal distance from an axis (or pole).then the equation for angular momentum simplifies to[18]:1028and define the moment of inertia relative to the centre of mass IC asUse the centre of mass C as the reference point soThe angular momentum vector for the planar movement of a rigid system of particles is given by[14][17]Note on the cross product: When a body moves parallel to a ground plane, the trajectories of all the points in the body lie in planes parallel to this ground plane. This means that any rotation that the body undergoes must be around an axis perpendicular to this plane. Planar movement is often presented as projected onto this ground plane so that the axis of rotation appears as a point. In this case, the angular velocity and angular acceleration of the body are scalars and the fact that they are vectors along the rotation axis is ignored. This is usually preferred for introductions to the topic. But in the case of moment of inertia, the combination of mass and geometry benefits from the geometric properties of the cross product. For this reason, in this section on planar movement the angular velocity and accelerations of the body are vectors perpendicular to the ground plane, and the cross product operations are the same as used for the study of spatial rigid body movement.This defines the relative position vector and the velocity vector for the rigid system of the particles moving in a plane.For planar movement the angular velocity vector is directed along the unit vector k which is perpendicular to the plane of movement. Introduce the unit vectors ei from the reference point R to a point ri , and the unit vector t\u0302i = k\u0302 \u00d7 \u00eai sowhere \u03c9 is the angular velocity of the system and V is the velocity of R.If a system of n particles, Pi, i = 1, \u2026, n, are assembled into a rigid body, then the momentum of the system can be written in terms of positions relative to a reference point R, and absolute velocities viIf a mechanical system is constrained to move parallel to a fixed plane, then the rotation of a body in the system occurs around an axis k\u0302 perpendicular to this plane. In this case, the moment of inertia of the mass in this system is a scalar known as the polar moment of inertia. The definition of the polar moment of inertia can be obtained by considering momentum, kinetic energy and Newton's laws for the planar movement of a rigid system of particles.[14][17][23][24]where m = 4/3\u03c0R3\u03c1 is the mass of the sphere.Therefore, the moment of inertia of the ball is the sum of the moments of inertia of the discs along the z-axis,then the radius r of the disc at the cross-section z along the z-axis isAs one more example, consider the moment of inertia of a solid sphere of constant density about an axis through its centre of mass. This is determined by summing the moments of inertia of the thin discs that form the sphere. If the surface of the ball is defined by the equation[18]:1301A list of moments of inertia formulas for standard body shapes provides a way to obtain the moment of inertia of a complex body as an assembly of simpler shaped bodies. The parallel axis theorem is used to shift the reference point of the individual bodies to the reference point of the assembly.The moment of inertia of a compound pendulum constructed from a thin disc mounted at the end of a thin rod that oscillates around a pivot at the other end of the rod, begins with the calculation of the moment of inertia of the thin rod and thin disc about their respective centres of mass.[18]Note on second moment of area: The moment of inertia of a body moving in a plane and the second moment of area of a beam's cross-section are often confused. The moment of inertia of body with the shape of the cross-section is the second moment of this area about the z-axis perpendicular to the cross-section, weighted by its density. This is also called the polar moment of the area, and is the sum of the second moments about the x- and y-axes.[22] The stresses in a beam are calculated using the second moment of the cross-sectional area around either the x-axis or y-axis depending on the load.Here, the function \u03c1 gives the mass density at each point (x, y, z), r is a vector perpendicular to the axis of rotation and extending from a point on the rotation axis to a point (x, y, z) in the solid, and the integration is evaluated over the volume\u00a0V of the body\u00a0Q. The moment of inertia of a flat surface is similar with the mass density being replaced by its areal mass density with the integral evaluated over its area.Another expression replaces the summation with an integral,The moment of inertia of a continuous body rotating about a specified axis is calculated in the same way, except with infinitely many point particles. Thus the limits of summation are removed, and the sum is written as follows:Thus, moment of inertia is a physical property that combines the mass and distribution of the particles around the rotation axis. Notice that rotation about different axes of the same body yield different moments of inertia.This shows that the moment of inertia of the body is the sum of each of the mr2 terms, that isConsider the kinetic energy of an assembly of N masses mi that lie at the distances ri from the pivot point P, which is the nearest point on the axis of rotation. It is the sum of the kinetic energy of the individual masses,[17]:516\u2013517[18]:1084\u20131085 [18]:1296\u20131300The moment of inertia about an axis of a body is calculated by summing mr2 for every particle in the body, where r is the perpendicular distance to the specified axis. To see how moment of inertia arises in the study of the movement of an extended body, it is convenient to consider a rigid assembly of point masses. (This equation can be used for axes that are not principal axes provided that it is understood that this does not fully describe the moment of inertia.[21])The moment of inertia of a complex system such as a vehicle or airplane around its vertical axis can be measured by suspending the system from three points to form a trifilar pendulum. A trifilar pendulum is a platform supported by three wires designed to oscillate in torsion around its vertical centroidal axis.[19] The period of oscillation of the trifilar pendulum yields the moment of inertia of the system.[20]Notice that the distance to the center of oscillation of the seconds pendulum must be adjusted to accommodate different values for the local acceleration of gravity. Kater's pendulum is a compound pendulum that uses this property to measure the local acceleration of gravity, and is called a gravimeter.orThis shows that the quantity I = mr2 is how mass combines with the shape of a body to define rotational inertia. The moment of inertia of an arbitrarily shaped body is the sum of the values mr2 for all of the elements of mass in the body.Similarly, the kinetic energy of the pendulum mass is defined by the velocity of the pendulum around the pivot to yieldusing a similar derivation the previous equation.The quantity I = mr2 also appears in the angular momentum of a simple pendulum, which is calculated from the velocity v = \u03c9 \u00d7 r of the pendulum mass around the pivot, where \u03c9 is the angular velocity of the mass about the pivot point. This angular momentum is given byMoment of inertia can be measured using a simple pendulum, because it is the resistance to the rotation caused by gravity. Mathematically, the moment of inertia of the pendulum is the ratio of the torque due to gravity about the pivot of a pendulum to its angular acceleration about that pivot point. For a simple pendulum this is found to be the product of the mass of the particle m with the square of its distance r to the pivot, that iswhere k is known as the radius of gyration.In general, given an object of mass m, an effective radius k can be defined for an axis through its center of mass, with such a value that its moment of inertia isThis simple formula generalizes to define moment of inertia for an arbitrarily shaped body as the sum of all the elemental point masses dm each multiplied by the square of its perpendicular distance r to an axis k\u0302.Thus, moment of inertia depends on both the mass m of a body and its geometry, or shape, as defined by the distance r to the axis of rotation.For a simple pendulum, this definition yields a formula for the moment of inertia I in terms of the mass m of the pendulum and its distance r from the pivot point as,If the shape of the body does not change, then its moment of inertia appears in Newton's law of motion as the ratio of an applied torque \u03c4 on a body to the angular acceleration \u03b1 around a principal axis, that isIf the angular momentum of a system is constant, then as the moment of inertia gets smaller, the angular velocity must increase. This occurs when spinning figure skaters pull in their outstretched arms or divers curl their bodies into a tuck position during a dive, to spin faster.[7][8][9][10][11][12][13]Moment of inertia I is defined as the ratio of the net angular momentum L of a system to its angular velocity \u03c9 around a principal axis,[7][8] that isThe moment of inertia of a rotating flywheel is used in a machine to resist variations in applied torque to smooth its rotational output. The moment of inertia of an airplane about its longitudinal, horizontal and vertical axis determines how steering forces on the control surfaces of its wings, elevators and tail affect the plane in roll, pitch and yaw.Moment of inertia also appears in momentum, kinetic energy, and in Newton's laws of motion for a rigid body as a physical parameter that combines its shape and mass. There is an interesting difference in the way moment of inertia appears in planar and spatial movement. Planar movement has a single scalar that defines the moment of inertia, while for spatial movement the same calculations yield a 3\u00a0\u00d7\u00a03 matrix of moments of inertia, called the inertia matrix or inertia tensor.[5][6]The natural frequency of oscillation of a compound pendulum is obtained from the ratio of the torque imposed by gravity on the mass of the pendulum to the resistance to acceleration defined by the moment of inertia. Comparison of this natural frequency to that of a simple pendulum consisting of a single point of mass provides a mathematical formulation for moment of inertia of an extended body.[3][4]In 1673 Christiaan Huygens introduced this parameter in his study of the oscillation of a body hanging from a pivot, known as a compound pendulum.[1] The term moment of inertia was introduced by Leonhard Euler in his book Theoria motus corporum solidorum seu rigidorum in 1765,[1][2] and it is incorporated into Euler's second law.When a body is rotating, or free to rotate, around an axis, a torque must be applied to change its angular momentum. The amount of torque needed to cause any given angular acceleration (the rate of change in angular velocity) is proportional to the moment of inertia of the body. Moment of inertia may be expressed in units of kilogram metre squared (kg\u00b7m2) in SI units and pound-foot-second squared (lb\u00b7ft\u00b7s2) in imperial or US units.For bodies constrained to rotate in a plane, it is sufficient to consider their moment of inertia about an axis perpendicular to the plane. For bodies free to rotate in three dimensions, their moments can be described by a symmetric 3\u00a0\u00d7\u00a03 matrix; each body has a set of mutually perpendicular principal axes for which this matrix is diagonal and torques around the axes act independently of each other.",
            "title": "Moment of inertia",
            "url": "https://en.wikipedia.org/wiki/Inertia_Tensor"
        },
        {
            "desc_links": [
                "/wiki/Diagonal_matrix",
                "/wiki/Rigid_body",
                "/wiki/Tensor",
                "/wiki/Torque",
                "/wiki/Angular_acceleration",
                "/wiki/Intensive_and_extensive_properties"
            ],
            "links": [
                "/wiki/Ellipsoid",
                "/wiki/Poinsot%27s_ellipsoid",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Sylvester%27s_law_of_inertia",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Unit_vector",
                "/wiki/Jacobi_identity",
                "/wiki/Cross_product",
                "/wiki/Kinematics",
                "/wiki/Centre_of_mass",
                "/wiki/Centre_of_mass",
                "/wiki/Cross_product#Conversion_to_matrix_multiplication",
                "/wiki/Centre_of_mass",
                "/wiki/Kinematics",
                "/wiki/Resultant_force",
                "/wiki/Moment_(physics)",
                "/wiki/Centre_of_mass",
                "/wiki/Mechanical_system",
                "/wiki/List_of_moments_of_inertia",
                "/wiki/Parallel_axis_theorem",
                "/wiki/Second_moment_of_area",
                "/wiki/Beam_(structure)",
                "/wiki/Multiple_integral",
                "/wiki/Kater%27s_pendulum",
                "/wiki/Gravimeter",
                "/wiki/Angular_momentum",
                "/wiki/Angular_velocity",
                "/wiki/Radius_of_gyration",
                "/wiki/Simple_pendulum",
                "/wiki/Rotation_around_a_fixed_axis",
                "/wiki/Torque",
                "/wiki/Angular_acceleration",
                "/wiki/Figure_skating_spins",
                "/wiki/Diving",
                "/wiki/Diving#Dive_positions",
                "/wiki/Angular_momentum",
                "/wiki/Angular_velocity",
                "/wiki/Angular_momentum",
                "/wiki/Kinetic_energy",
                "/wiki/Rigid_body_dynamics",
                "/wiki/Christiaan_Huygens",
                "/wiki/Compound_pendulum",
                "/wiki/Leonhard_Euler",
                "/wiki/Euler%27s_laws#Euler's_second_law",
                "/wiki/Torque",
                "/wiki/Angular_momentum",
                "/wiki/Angular_acceleration",
                "/wiki/Angular_velocity",
                "/wiki/SI",
                "/wiki/Imperial_units",
                "/wiki/United_States_customary_units",
                "/wiki/Diagonal_matrix"
            ],
            "text": "Thus, the magnitude of a point x in the direction n on the inertia ellipsoid isLet a point x on this ellipsoid be defined in terms of its magnitude and direction, x = |x|n, where n is a unit vector. Then the relationship presented above, between the inertia matrix and the scalar moment of inertia In around an axis in the direction n, yieldsto see that the semi-principal diameters of this ellipsoid are given bydefines an ellipsoid in the body frame. Write this equation in the form,orThe moment of inertia matrix in body-frame coordinates is a quadratic form that defines a surface in the body called Poinsot's ellipsoid.[28] Let \u039b be the inertia matrix relative to the centre of mass aligned with the principal axes, then the surfaceFor bodies with constant density an axis of rotational symmetry is a principal axis.The columns of the rotation matrix Q define the directions of the principal axes of the body, and the constants I1, I2, and I3 are called the principal moments of inertia. This result was first shown by J. J. Sylvester (1852), and is a form of Sylvester's law of inertia.[26][27]whereMeasured in the body frame the inertia matrix is a constant real symmetric matrix. A real symmetric matrix has the eigendecomposition into the product of a rotation matrix Q and a diagonal matrix \u039b, given byNotice that A changes as the body moves, while IB\nC remains constant.where vectors y in the body fixed coordinate frame have coordinates x in the inertial frame. Then, the inertia matrix of the body measured in the inertial frame is given byLet the body frame inertia matrix relative to the centre of mass be denoted IB\nC, and define the orientation of the body frame relative to the inertial frame by the rotation matrix A, such that,The use of the inertia matrix in Newton's second law assumes its components are computed relative to axes parallel to the inertial frame and not relative to a body-fixed reference frame.[6][23] This means that as the body moves the components of the inertia matrix change with time. In contrast, the components of the inertia matrix measured in a body-fixed frame are constant.It is common in rigid body mechanics to use notation that explicitly identifies the x, y, and z axes, such as Ixx and Ixy, for the components of the inertia tensor.The components of tensors of degree two can be assembled into a matrix. For the inertia tensor this matrix is given by,and can be interpreted as the moment of inertia around the x-axis when the object rotates around the y-axis.where the dot product is taken with the corresponding elements in the component tensors. A product of inertia term such as I12 is obtained by the computationThe inertia tensor can be used in the same way as the inertia matrix to compute the scalar moment of inertia about an arbitrary axis in the direction n,where r defines the coordinates of a point in the body and \u03c1(r) is the mass density at that point. The integral is taken over the volume V of the body. The inertia tensor is symmetric because Iij =\u00a0Iji.The inertia tensor for a continuous body is given byIn this case, the components of the inertia tensor are given bywhere E is the identity tensorFor a rigid system of particles Pk, k = 1, \u2026, N each of mass mk with position coordinates rk\u00a0=\u00a0(xk, yk, zk), the inertia tensor is given byThis tensor is of degree two because the component tensors are each constructed from two basis vectors. In this form the inertia tensor is also called the inertia binor.where ei, i\u00a0=\u00a01,\u00a02,\u00a03 are the three orthogonal unit vectors defining the inertial frame in which the body moves. Using this basis the inertia tensor is given byThis shows that the inertia matrix can be used to calculate the moment of inertia of a body around any specified rotation axis in the body.where IR is the moment of inertia matrix of the system relative to the reference point R.Thus, the moment of inertia around the line L through R in the direction k\u0302 is obtained from the calculationwhere the dot and the cross products have been interchanged. Exchanging products, and simplifying by noting that \u0394ri and k\u0302 are orthogonal:The simplification of this equation uses the triple scalar product identityThe magnitude squared of the perpendicular vector isnoting that k\u0302 is a unit vector.To relate this scalar moment of inertia to the inertia matrix of the body, introduce the skew-symmetric matrix [k\u0302] such that [k\u0302]y = k\u0302 \u00d7 y, then we have the identitywhere E is the identity matrix, so as to avoid confusion with the inertia matrix, and k\u0302\u2009k\u0302T is the outer product matrix formed from the unit vector k\u0302 along the line\u00a0L.This is derived as follows. Let a rigid assembly of N particles, Pi, i = 1, \u2026, N, have coordinates ri. Choose R as a reference point and compute the moment of inertia around a line L defined by the unit vector k\u0302 through the reference point R, L(t) = R + tk\u0302. The perpendicular vector from this line to the particle Pi is obtained from \u0394ri by removing the component that projects onto k\u0302.where IR is the moment of inertia matrix of the system relative to the reference point R, and [\u0394ri] is the skew symmetric matrix obtained from the vector \u0394ri = ri \u2212 R.The scalar moment of inertia, IL, of a body about a specified axis whose direction is specified by the unit vector k\u0302 and passes through the body at a point R is as follows:[6]Note on the minus sign: By using the skew symmetric matrix of position vectors relative to the reference point, the inertia matrix of each particle has the form \u2212m[r]2, which is similar to the mr2 that appears in planar movement. However, to make this to work out correctly a minus sign is needed. This minus sign can be absorbed into the term m[r]T[r], if desired, by using the skew-symmetry property of [r].where d is the vector from the centre of mass C to the reference point R.The result is the parallel axis theorem,The first term is the inertia matrix IC relative to the centre of mass. The second and third terms are zero by definition of the centre of mass C. And the last term is the total mass of the system multiplied by the square of the skew-symmetric matrix [d] constructed from d.Distribute over the cross product to obtainwhere d is the vector from the centre of mass C to the reference point R. Use this equation to compute the inertia matrix,Let C be the centre of mass of the rigid system, thenConsider the inertia matrix IR obtained for a rigid system of particles measured relative to a reference point R, given byThe inertia matrix of a body depends on the choice of the reference point. There is a useful relationship between the inertia matrix relative to the centre of mass C and the inertia matrix relative to another point R. This relationship is called the parallel axis theorem.[3][6]where IC is the inertia matrix relative to the centre of mass.Thus, the resultant torque on the rigid system of particles is given byobtained from the Jacobi identity for the triple cross product as shown in the proof below:The calculation uses the identityUse the centre of mass C as the reference point, and introduce the skew-symmetric matrix [\u0394ri] = [ri \u2212 C] to represent the cross product (ri \u2212 C) \u00d7, to obtainwhere ai is the acceleration of the particle Pi. The kinematics of a rigid body yields the formula for the acceleration of the particle Pi in terms of the position R and acceleration Ar of the reference point, as well as the angular velocity vector \u03c9 and angular acceleration vector \u03b1 of the rigid system as,The inertia matrix appears in the application of Newton's second law to a rigid assembly of particles. The resultant torque on this system is,[3][6]where IC is the inertia matrix relative to the centre of mass and M is the total mass.Thus, the kinetic energy of the rigid system of particles is given byThe second term in this equation is zero because C is the centre of mass. Introduce the skew-symmetric matrix [\u0394ri] so the kinetic energy becomesThis equation expands to yield three termswhere \u0394ri = ri\u00a0\u2212\u00a0C is the position vector of a particle relative to the centre of mass.The kinetic energy of a rigid system of particles can be formulated in terms of the centre of mass and a matrix of mass moments of inertia of the system. Let the system of particles Pi, i = 1, \u2026,n be located at the coordinates ri with velocities vi, then the kinetic energy is[3][6]is the symmetric inertia matrix of the rigid system of particles measured relative to the centre of mass C.where IC defined byThen, the skew-symmetric matrix [\u0394ri] obtained from the relative position vector \u0394ri = ri \u2212 C, can be used to define,where the terms containing VR (= C) sum to zero by the definition of centre of mass.The inertia matrix is constructed by considering the angular momentum, with the reference point R of the body chosen to be the centre of mass C:[3][6]Note that the cross product can be equivalently written as matrix multiplication by combining the first operand and the operator into a, skew-symmetric, matrix, [b], constructed from the components of b = (bx, by, bz):where \u03c9 is the angular velocity of the system, and VR is the velocity of R.and the (absolute) velocities areLet the system of particles Pi, i = 1, \u2026, n be located at the coordinates ri with velocities vi relative to a fixed reference frame. For a (possibly moving) reference point R, the relative positions areThe scalar moments of inertia appear as elements in a matrix when a system of particles is assembled into a rigid body that moves in three-dimensional space. This inertia matrix appears in the calculation of the angular momentum, kinetic energy and resultant torque of the rigid system of particles.[3][4][5][6][25]Use the centre of mass C as the reference point and define the moment of inertia relative to the centre of mass IC, then the equation for the resultant torque simplifies to[18]:1029where \u00eai \u00d7 \u00eai = 0, and \u00eai \u00d7 t\u0302i = k\u0302 is the unit vector perpendicular to the plane for all of the particles Pi.This yields the resultant torque on the system asFor systems that are constrained to planar movement, the angular velocity and angular acceleration vectors are directed along k\u0302 perpendicular to the plane of movement, which simplifies this acceleration equation. In this case, the acceleration vectors can be simplified by introducing the unit vectors \u00eai from the reference point R to a point ri and the unit vectors t\u0302i = k\u0302 \u00d7 \u00eai , soThe kinematics of a rigid body yields the formula for the acceleration of the particle Pi in terms of the position R and acceleration A of the reference particle as well as the angular velocity vector \u03c9 and angular acceleration vector \u03b1 of the rigid system of particles as,where ri denotes the trajectory of each particle.Newton's laws for a rigid system of N particles, Pi, i = 1, \u2026 N, can be written in terms of a resultant force and torque at a reference point R, to yield[14][17]The moment of inertia IC is the polar moment of inertia of the body.Let the reference point be the centre of mass C of the system so the second term becomes zero, and introduce the moment of inertia IC so the kinetic energy is given by[18]:1084The kinetic energy of a rigid system of particles moving in the plane is given by[14][17]For a given amount of angular momentum, a decrease in the moment of inertia results in an increase in the angular velocity. Figure skaters can change their moment of inertia by pulling in their arms. Thus, the angular velocity achieved by a skater with outstretched arms results in a greater angular velocity when the arms are pulled in, because of the reduced moment of inertia. A figure skater is not, however, a rigid body.The moment of inertia IC about an axis perpendicular to the movement of the rigid system and through the centre of mass is known as the polar moment of inertia. Specifically, it is the second moment of mass with respect to the orthogonal distance from an axis (or pole).then the equation for angular momentum simplifies to[18]:1028and define the moment of inertia relative to the centre of mass IC asUse the centre of mass C as the reference point soThe angular momentum vector for the planar movement of a rigid system of particles is given by[14][17]Note on the cross product: When a body moves parallel to a ground plane, the trajectories of all the points in the body lie in planes parallel to this ground plane. This means that any rotation that the body undergoes must be around an axis perpendicular to this plane. Planar movement is often presented as projected onto this ground plane so that the axis of rotation appears as a point. In this case, the angular velocity and angular acceleration of the body are scalars and the fact that they are vectors along the rotation axis is ignored. This is usually preferred for introductions to the topic. But in the case of moment of inertia, the combination of mass and geometry benefits from the geometric properties of the cross product. For this reason, in this section on planar movement the angular velocity and accelerations of the body are vectors perpendicular to the ground plane, and the cross product operations are the same as used for the study of spatial rigid body movement.This defines the relative position vector and the velocity vector for the rigid system of the particles moving in a plane.For planar movement the angular velocity vector is directed along the unit vector k which is perpendicular to the plane of movement. Introduce the unit vectors ei from the reference point R to a point ri , and the unit vector t\u0302i = k\u0302 \u00d7 \u00eai sowhere \u03c9 is the angular velocity of the system and V is the velocity of R.If a system of n particles, Pi, i = 1, \u2026, n, are assembled into a rigid body, then the momentum of the system can be written in terms of positions relative to a reference point R, and absolute velocities viIf a mechanical system is constrained to move parallel to a fixed plane, then the rotation of a body in the system occurs around an axis k\u0302 perpendicular to this plane. In this case, the moment of inertia of the mass in this system is a scalar known as the polar moment of inertia. The definition of the polar moment of inertia can be obtained by considering momentum, kinetic energy and Newton's laws for the planar movement of a rigid system of particles.[14][17][23][24]where m = 4/3\u03c0R3\u03c1 is the mass of the sphere.Therefore, the moment of inertia of the ball is the sum of the moments of inertia of the discs along the z-axis,then the radius r of the disc at the cross-section z along the z-axis isAs one more example, consider the moment of inertia of a solid sphere of constant density about an axis through its centre of mass. This is determined by summing the moments of inertia of the thin discs that form the sphere. If the surface of the ball is defined by the equation[18]:1301A list of moments of inertia formulas for standard body shapes provides a way to obtain the moment of inertia of a complex body as an assembly of simpler shaped bodies. The parallel axis theorem is used to shift the reference point of the individual bodies to the reference point of the assembly.The moment of inertia of a compound pendulum constructed from a thin disc mounted at the end of a thin rod that oscillates around a pivot at the other end of the rod, begins with the calculation of the moment of inertia of the thin rod and thin disc about their respective centres of mass.[18]Note on second moment of area: The moment of inertia of a body moving in a plane and the second moment of area of a beam's cross-section are often confused. The moment of inertia of body with the shape of the cross-section is the second moment of this area about the z-axis perpendicular to the cross-section, weighted by its density. This is also called the polar moment of the area, and is the sum of the second moments about the x- and y-axes.[22] The stresses in a beam are calculated using the second moment of the cross-sectional area around either the x-axis or y-axis depending on the load.Here, the function \u03c1 gives the mass density at each point (x, y, z), r is a vector perpendicular to the axis of rotation and extending from a point on the rotation axis to a point (x, y, z) in the solid, and the integration is evaluated over the volume\u00a0V of the body\u00a0Q. The moment of inertia of a flat surface is similar with the mass density being replaced by its areal mass density with the integral evaluated over its area.Another expression replaces the summation with an integral,The moment of inertia of a continuous body rotating about a specified axis is calculated in the same way, except with infinitely many point particles. Thus the limits of summation are removed, and the sum is written as follows:Thus, moment of inertia is a physical property that combines the mass and distribution of the particles around the rotation axis. Notice that rotation about different axes of the same body yield different moments of inertia.This shows that the moment of inertia of the body is the sum of each of the mr2 terms, that isConsider the kinetic energy of an assembly of N masses mi that lie at the distances ri from the pivot point P, which is the nearest point on the axis of rotation. It is the sum of the kinetic energy of the individual masses,[17]:516\u2013517[18]:1084\u20131085 [18]:1296\u20131300The moment of inertia about an axis of a body is calculated by summing mr2 for every particle in the body, where r is the perpendicular distance to the specified axis. To see how moment of inertia arises in the study of the movement of an extended body, it is convenient to consider a rigid assembly of point masses. (This equation can be used for axes that are not principal axes provided that it is understood that this does not fully describe the moment of inertia.[21])The moment of inertia of a complex system such as a vehicle or airplane around its vertical axis can be measured by suspending the system from three points to form a trifilar pendulum. A trifilar pendulum is a platform supported by three wires designed to oscillate in torsion around its vertical centroidal axis.[19] The period of oscillation of the trifilar pendulum yields the moment of inertia of the system.[20]Notice that the distance to the center of oscillation of the seconds pendulum must be adjusted to accommodate different values for the local acceleration of gravity. Kater's pendulum is a compound pendulum that uses this property to measure the local acceleration of gravity, and is called a gravimeter.orThis shows that the quantity I = mr2 is how mass combines with the shape of a body to define rotational inertia. The moment of inertia of an arbitrarily shaped body is the sum of the values mr2 for all of the elements of mass in the body.Similarly, the kinetic energy of the pendulum mass is defined by the velocity of the pendulum around the pivot to yieldusing a similar derivation the previous equation.The quantity I = mr2 also appears in the angular momentum of a simple pendulum, which is calculated from the velocity v = \u03c9 \u00d7 r of the pendulum mass around the pivot, where \u03c9 is the angular velocity of the mass about the pivot point. This angular momentum is given byMoment of inertia can be measured using a simple pendulum, because it is the resistance to the rotation caused by gravity. Mathematically, the moment of inertia of the pendulum is the ratio of the torque due to gravity about the pivot of a pendulum to its angular acceleration about that pivot point. For a simple pendulum this is found to be the product of the mass of the particle m with the square of its distance r to the pivot, that iswhere k is known as the radius of gyration.In general, given an object of mass m, an effective radius k can be defined for an axis through its center of mass, with such a value that its moment of inertia isThis simple formula generalizes to define moment of inertia for an arbitrarily shaped body as the sum of all the elemental point masses dm each multiplied by the square of its perpendicular distance r to an axis k\u0302.Thus, moment of inertia depends on both the mass m of a body and its geometry, or shape, as defined by the distance r to the axis of rotation.For a simple pendulum, this definition yields a formula for the moment of inertia I in terms of the mass m of the pendulum and its distance r from the pivot point as,If the shape of the body does not change, then its moment of inertia appears in Newton's law of motion as the ratio of an applied torque \u03c4 on a body to the angular acceleration \u03b1 around a principal axis, that isIf the angular momentum of a system is constant, then as the moment of inertia gets smaller, the angular velocity must increase. This occurs when spinning figure skaters pull in their outstretched arms or divers curl their bodies into a tuck position during a dive, to spin faster.[7][8][9][10][11][12][13]Moment of inertia I is defined as the ratio of the net angular momentum L of a system to its angular velocity \u03c9 around a principal axis,[7][8] that isThe moment of inertia of a rotating flywheel is used in a machine to resist variations in applied torque to smooth its rotational output. The moment of inertia of an airplane about its longitudinal, horizontal and vertical axis determines how steering forces on the control surfaces of its wings, elevators and tail affect the plane in roll, pitch and yaw.Moment of inertia also appears in momentum, kinetic energy, and in Newton's laws of motion for a rigid body as a physical parameter that combines its shape and mass. There is an interesting difference in the way moment of inertia appears in planar and spatial movement. Planar movement has a single scalar that defines the moment of inertia, while for spatial movement the same calculations yield a 3\u00a0\u00d7\u00a03 matrix of moments of inertia, called the inertia matrix or inertia tensor.[5][6]The natural frequency of oscillation of a compound pendulum is obtained from the ratio of the torque imposed by gravity on the mass of the pendulum to the resistance to acceleration defined by the moment of inertia. Comparison of this natural frequency to that of a simple pendulum consisting of a single point of mass provides a mathematical formulation for moment of inertia of an extended body.[3][4]In 1673 Christiaan Huygens introduced this parameter in his study of the oscillation of a body hanging from a pivot, known as a compound pendulum.[1] The term moment of inertia was introduced by Leonhard Euler in his book Theoria motus corporum solidorum seu rigidorum in 1765,[1][2] and it is incorporated into Euler's second law.When a body is rotating, or free to rotate, around an axis, a torque must be applied to change its angular momentum. The amount of torque needed to cause any given angular acceleration (the rate of change in angular velocity) is proportional to the moment of inertia of the body. Moment of inertia may be expressed in units of kilogram metre squared (kg\u00b7m2) in SI units and pound-foot-second squared (lb\u00b7ft\u00b7s2) in imperial or US units.For bodies constrained to rotate in a plane, it is sufficient to consider their moment of inertia about an axis perpendicular to the plane. For bodies free to rotate in three dimensions, their moments can be described by a symmetric 3\u00a0\u00d7\u00a03 matrix; each body has a set of mutually perpendicular principal axes for which this matrix is diagonal and torques around the axes act independently of each other.",
            "title": "Moment of inertia",
            "url": "https://en.wikipedia.org/wiki/Principal_axis_(mechanics)"
        },
        {
            "desc_links": [
                "/wiki/Special_relativity",
                "/wiki/Speed_of_light",
                "/wiki/Quantum_mechanics",
                "/wiki/Molecules",
                "/wiki/Rotational_spectroscopy#Classification_of_molecules_based_on_rotational_behavior",
                "/wiki/Physics",
                "/wiki/Physical_body",
                "/wiki/Deformation_(engineering)",
                "/wiki/Distance",
                "/wiki/Point_(geometry)",
                "/wiki/Force"
            ],
            "links": [
                "/wiki/Configuration_space_(physics)",
                "/wiki/Manifold",
                "/wiki/Rotation_group_SO(3)",
                "/wiki/Euclidean_group#Direct_and_indirect_isometries",
                "/wiki/Euclidean_group#Direct_and_indirect_isometries",
                "/wiki/Euclidean_group",
                "/wiki/Translation_(geometry)",
                "/wiki/Rotation",
                "/wiki/Through_and_through",
                "/wiki/Equality_(objects)",
                "/wiki/Proper_rotation",
                "/wiki/Chirality_(mathematics)",
                "/wiki/Mirror_image",
                "/wiki/Symmetry",
                "/wiki/Symmetry_group",
                "/wiki/Point_groups_in_three_dimensions",
                "/wiki/Vehicle",
                "/wiki/Winding_number",
                "/wiki/Polygon#Angles",
                "/wiki/Coordinate_system",
                "/wiki/Time_derivative",
                "/wiki/Time_derivative",
                "/wiki/Angular_velocity",
                "/wiki/Vector_(geometry)",
                "/wiki/Angular_speed",
                "/wiki/Axis_of_rotation",
                "/wiki/Euler%27s_rotation_theorem",
                "/wiki/Angular_velocity",
                "/wiki/Axis_of_rotation",
                "/wiki/Time_derivative",
                "/wiki/Derivative",
                "/wiki/Velocity",
                "/wiki/Vector_(geometry)",
                "/wiki/Time_derivative",
                "/wiki/Velocity",
                "/wiki/Motion_(physics)",
                "/wiki/Axis_of_rotation",
                "/wiki/Velocity",
                "/wiki/Angular_velocity",
                "/wiki/Frame_of_reference",
                "/wiki/Translation_(physics)",
                "/wiki/Rotation",
                "/wiki/Coordinate_system#Examples",
                "/wiki/Position_vector",
                "/wiki/Space#Classical_mechanics",
                "/wiki/Coordinate_system",
                "/wiki/Center_of_mass",
                "/wiki/Centroid",
                "/wiki/Coordinate_system",
                "/wiki/Kinematics",
                "/wiki/Dynamics_(mechanics)",
                "/wiki/Velocity",
                "/wiki/Acceleration",
                "/wiki/Momentum",
                "/wiki/Impulse_(physics)",
                "/wiki/Kinetic_energy",
                "/wiki/Coordinate_system#Examples",
                "/wiki/Collinear",
                "/wiki/Time-invariant",
                "/wiki/Special_relativity",
                "/wiki/Speed_of_light",
                "/wiki/Quantum_mechanics",
                "/wiki/Molecules",
                "/wiki/Rotational_spectroscopy#Classification_of_molecules_based_on_rotational_behavior",
                "/wiki/Physics",
                "/wiki/Physical_body",
                "/wiki/Deformation_(engineering)",
                "/wiki/Distance",
                "/wiki/Point_(geometry)",
                "/wiki/Force"
            ],
            "text": "The configuration space of a rigid body with one point fixed (i.e., a body with zero translational motion) is given by the underlying manifold of the rotation group SO(3). The configuration space of a nonfixed (with non-zero translational motion) rigid body is E+(3), the subgroup of direct isometries of the Euclidean group in three dimensions (combinations of translations and rotations).A sheet with a through and through image is achiral. We can distinguish again two cases:For a (rigid) rectangular transparent sheet, inversion symmetry corresponds to having on one side an image without rotational symmetry and on the other side an image such that what shines through is the image at the top side, upside down. We can distinguish two cases:Two rigid bodies are said to be different (not copies) if there is no proper rotation from one to the other. A rigid body is called chiral if its mirror image is different in that sense, i.e., if it has either no symmetry or its symmetry group contains only proper rotations. In the opposite case an object is called achiral: the mirror image is a copy, not a different object. Such an object may have a symmetry plane, but not necessarily: there may also be a plane of reflection with respect to which the image of the object is a rotated version. The latter applies for S2n, of which the case n = 1 is inversion symmetry.When the center of mass is used as reference point:However, depending on the application, a convenient choice may be:Any point that is rigidly connected to the body can be used as reference point (origin of coordinate system L) to describe the linear motion of the body (the linear position, velocity and acceleration vectors depend on the choice).Vehicles, walking people, etc., usually rotate according to changes in the direction of the velocity: they move forward with respect to their own orientation. Then, if the body follows a closed orbit in a plane, the angular velocity integrated over a time interval in which the orbit is completed once, is an integer times 360\u00b0. This integer is the winding number with respect to the origin of the velocity. Compare the amount of rotation associated with the vertices of a polygon.In 2D, the angular velocity is a scalar, and matrix A(t) simply represents a rotation in the xy-plane by an angle which is the integral of the angular velocity over time.whereIf C is the origin of a local coordinate system L, attached to the body,where Q is the point fixed in B that instantaneously coincident with R at the instant of interest.[7] This equation is often combined with Acceleration of two points fixed on a rigid body.The acceleration in reference frame N of the point R moving in body B while B is moving in frame N is given bywhere Q is the point fixed in B that is instantaneously coincident with R at the instant of interest.[7] This relation is often combined with the relation for the Velocity of two points fixed on a rigid body.If the point R is moving in rigid body B while B moves in reference frame N, then the velocity of R in N isBy differentiating the equation for the Velocity of two points fixed on a rigid body in N with respect to time, the acceleration in reference frame N of a point Q fixed on a rigid body B can be expressed asThe acceleration of point P in reference frame N is defined as the time derivative in N of its velocity:[5]where O is any arbitrary point fixed in reference frame N, and the N to the left of the d/dt operator indicates that the derivative is taken in reference frame N. The result is independent of the selection of O so long as O is fixed in N.The velocity of point P in reference frame N is defined as the time derivative in N of the position vector from O to P:[5]For any set of three points P, Q, and R, the position vector from P to R is the sum of the position vector from P to Q and the position vector from Q to R:In this case, rigid bodies and reference frames are indistinguishable and completely interchangeable.The angular velocity of a rigid body B in a reference frame N is equal to the sum of the angular velocity of a rigid body D in N and the angular velocity of B with respect to D:[4]Angular velocity is a vector quantity that describes the angular speed at which the orientation of the rigid body is changing and the instantaneous axis about which it is rotating (the existence of this instantaneous axis is guaranteed by the Euler's rotation theorem). All points on a rigid body experience the same angular velocity at all times. During purely rotational motion, all points on the body change position except for those lying on the instantaneous axis of rotation. The relationship between orientation and angular velocity is not directly analogous to the relationship between position and velocity. Angular velocity is not the time rate of change of orientation, because there is no such concept as an orientation vector that can be differentiated to obtain the angular velocity.The linear velocity of a rigid body is a vector quantity, equal to the time rate of change of its linear position. Thus, it is the velocity of a reference point fixed to the body. During purely translational motion (motion with no rotation), all points on a rigid body move with the same velocity. However, when motion involves rotation, the instantaneous velocity of any two points on the body will generally not be the same. Two points of a rotating body will have the same instantaneous velocity only if they happen to lie on an axis parallel to the instantaneous axis of rotation.Velocity (also called linear velocity) and angular velocity are measured with respect to a frame of reference.In general, when a rigid body moves, both its position and orientation vary with time. In the kinematic sense, these changes are referred to as translation and rotation, respectively. Indeed, the position of a rigid body can be viewed as a hypothetic translation and rotation (roto-translation) of the body starting from a hypothetic reference position (not necessarily coinciding with a position actually taken by the body during its motion).The linear position can be represented by a vector with its tail at an arbitrary reference point in space (the origin of a chosen coordinate system) and its tip at an arbitrary point of interest on the rigid body, typically coinciding with its center of mass or centroid. This reference point may define the origin of a coordinate system fixed to the body.Thus, the position of a rigid body has two components: linear and angular, respectively.[2] The same is true for other kinematic and kinetic quantities describing the motion of a rigid body, such as linear and angular velocity, acceleration, momentum, impulse, and kinetic energy.[3]The position of a rigid body is the position of all the particles of which it is composed. To simplify the description of this position, we exploit the property that the body is rigid, namely that all its particles maintain the same distance relative to each other. If the body is rigid, it is sufficient to describe the position of at least three non-collinear particles. This makes it possible to reconstruct the position of all the other particles, provided that their time-invariant position relative to the three selected particles is known. However, typically a different, mathematically more convenient, but equivalent approach is used. The position of the whole body is represented by:In the study of special relativity, a perfectly rigid body does not exist; and objects can only be assumed to be rigid if they are not moving near the speed of light. In quantum mechanics a rigid body is usually thought of as a collection of point masses. For instance, in quantum mechanics molecules (consisting of the point masses: electrons and nuclei) are often seen as rigid bodies (see classification of molecules as rigid rotors).In physics, a rigid body is a solid body in which deformation is zero or so small it can be neglected. The distance between any two given points on a rigid body remains constant in time regardless of external forces exerted on it. A rigid body is usually considered as a continuous distribution of mass.",
            "title": "Rigid body",
            "url": "https://en.wikipedia.org/wiki/Rigid_body"
        },
        {
            "desc_links": [
                "/wiki/Tullio_Levi-Civita",
                "/wiki/Gregorio_Ricci-Curbastro",
                "/wiki/Bernhard_Riemann",
                "/wiki/Elwin_Bruno_Christoffel",
                "/wiki/Absolute_differential_calculus",
                "/wiki/Differential_geometry",
                "/wiki/Manifold",
                "/wiki/Riemann_curvature_tensor",
                "/wiki/Stress_(mechanics)",
                "/wiki/Elasticity_(physics)",
                "/wiki/Fluid_mechanics",
                "/wiki/General_relativity",
                "/wiki/Tensor_field",
                "/wiki/Covariant_transformation",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Array_data_structure#Multidimensional_arrays",
                "/wiki/Tensor_algebra",
                "/wiki/Mathematics",
                "/wiki/Geometry",
                "/wiki/Linear_relation",
                "/wiki/Geometric_vector",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Dot_product",
                "/wiki/Cross_product",
                "/wiki/Linear_map",
                "/wiki/Geometric_vector",
                "/wiki/Cauchy_stress_tensor"
            ],
            "links": [
                "/wiki/Algebraic_topology",
                "/wiki/K%C3%BCnneth_theorem",
                "/wiki/Abstract_algebra",
                "/wiki/Homological_algebra",
                "/wiki/Representation_theory",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Category_theory",
                "/wiki/Monoidal_category",
                "/wiki/Continuum_mechanics",
                "/wiki/Differential_geometry",
                "/wiki/Quadratic_form",
                "/wiki/Metric_tensor",
                "/wiki/Riemann_curvature_tensor",
                "/wiki/Exterior_algebra",
                "/wiki/Hermann_Grassmann",
                "/wiki/Differential_form",
                "/wiki/%C3%89lie_Cartan",
                "/wiki/Albert_Einstein",
                "/wiki/General_relativity",
                "/wiki/Marcel_Grossmann",
                "/wiki/Gregorio_Ricci-Curbastro",
                "/wiki/Tullio_Levi-Civita",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/Algebraic_form",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Woldemar_Voigt",
                "/wiki/Spin_representation",
                "/wiki/Tensor_representation",
                "/wiki/Classical_groups",
                "/wiki/Orthonormal_basis",
                "/wiki/Simply_connected",
                "/wiki/Orientation_entanglement",
                "/wiki/Plate_trick",
                "/wiki/Spinor",
                "/wiki/Functor",
                "/wiki/Local_diffeomorphism",
                "/wiki/Jet_(mathematics)",
                "/wiki/Natural_bundle",
                "/wiki/Rational_representation",
                "/wiki/Semisimple",
                "/wiki/Current_density",
                "/wiki/Electromagnetism",
                "/wiki/Determinant",
                "/wiki/Change_of_variables_formula",
                "/wiki/Scalar_density",
                "/wiki/Scalar_field",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Density_on_a_manifold",
                "/wiki/Scalar_(physics)",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Tensor_product_of_Hilbert_spaces",
                "/wiki/Hilbert_space",
                "/wiki/Nonlinear_system",
                "/wiki/Algebraic_dual",
                "/wiki/Banach_space",
                "/wiki/Continuous_dual",
                "/wiki/Banach_manifold",
                "/wiki/Fr%C3%A9chet_manifold",
                "/wiki/Tensor_product",
                "/wiki/Tensor_product_of_modules",
                "/wiki/Module_over_a_ring",
                "/wiki/Nonlinear_optics",
                "/wiki/Polarization_density#Relation_between_P_and_E_in_various_materials",
                "/wiki/Electric_field",
                "/wiki/Taylor_series",
                "/wiki/Computer_vision",
                "/wiki/Trifocal_tensor",
                "/wiki/Fundamental_matrix_(computer_vision)",
                "/wiki/Volume_form",
                "/wiki/Type_of_a_tensor",
                "/wiki/Linear_elasticity",
                "/wiki/Continuum_mechanics",
                "/wiki/Solid_body",
                "/wiki/Fluid",
                "/wiki/Stress_(mechanics)",
                "/wiki/Strain_tensor",
                "/wiki/Elasticity_tensor",
                "/wiki/Nondegenerate_bilinear_form",
                "/wiki/Metric_tensor",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Tensor_product",
                "/wiki/Scalar_multiplication",
                "/wiki/Component-free_treatment_of_tensors",
                "/wiki/Tensor_product#Tensor_product_of_vector_spaces",
                "/wiki/Abstract_index_notation",
                "/wiki/Indeterminate_(variable)",
                "/wiki/Penrose_graphical_notation",
                "/wiki/Einstein_summation_convention",
                "/wiki/Summation_sign",
                "/wiki/Ricci_calculus",
                "/wiki/Inner_product",
                "/wiki/Outer_product",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Summation",
                "/wiki/Symmetric_tensor",
                "/wiki/Antisymmetric_tensor",
                "/wiki/Partial_derivative",
                "/wiki/Covariant_derivative",
                "/wiki/Bilinear_form",
                "/wiki/Inner_product",
                "/wiki/Coordinate_basis",
                "/wiki/Tangent_space",
                "/wiki/Partial_derivative",
                "/wiki/Tensor_field",
                "/wiki/Naturally_isomorphic",
                "/wiki/Vector_bundle",
                "/wiki/Coherent_sheaves",
                "/wiki/Topological_tensor_product",
                "/wiki/Tensor_product_of_Hilbert_spaces",
                "/wiki/Symmetric_monoidal_category",
                "/wiki/Tensor_product_of_modules",
                "/wiki/Bijection",
                "/wiki/Tensor_product",
                "/wiki/Universal_property",
                "/wiki/Double_dual",
                "/wiki/Dual_space",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Differential_geometry",
                "/wiki/Tangent_space",
                "/wiki/Multilinear_map",
                "/wiki/Einstein_summation_convention",
                "/wiki/Matrix_inverse",
                "/wiki/Vector_space",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Basis_(linear_algebra)#Ordered_bases_and_coordinates",
                "/wiki/Linear_operator",
                "/wiki/Subscript_and_superscript",
                "/wiki/Raising_and_lowering_indices",
                "/wiki/Array_data_structure#Dimension",
                "/wiki/Tensor_rank",
                "/wiki/Tullio_Levi-Civita",
                "/wiki/Gregorio_Ricci-Curbastro",
                "/wiki/Bernhard_Riemann",
                "/wiki/Elwin_Bruno_Christoffel",
                "/wiki/Absolute_differential_calculus",
                "/wiki/Differential_geometry",
                "/wiki/Manifold",
                "/wiki/Riemann_curvature_tensor",
                "/wiki/Stress_(mechanics)",
                "/wiki/Elasticity_(physics)",
                "/wiki/Fluid_mechanics",
                "/wiki/General_relativity",
                "/wiki/Tensor_field",
                "/wiki/Covariant_transformation",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Array_data_structure#Multidimensional_arrays",
                "/wiki/Tensor_algebra",
                "/wiki/Mathematics",
                "/wiki/Geometry",
                "/wiki/Linear_relation",
                "/wiki/Geometric_vector",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Dot_product",
                "/wiki/Cross_product",
                "/wiki/Linear_map",
                "/wiki/Geometric_vector",
                "/wiki/Cauchy_stress_tensor"
            ],
            "text": "From about the 1920s onwards, it was realised that tensors play a basic role in algebraic topology (for example in the K\u00fcnneth theorem).[36] Correspondingly there are types of tensors at work in many branches of abstract algebra, particularly in homological algebra and representation theory. Multilinear algebra can be developed in greater generality than for scalars coming from a field. For example, scalars can come from a ring. But the theory is then less geometric and computations more technical and less algorithmic.[37] Tensors are generalized within category theory by means of the concept of monoidal category, from the 1960s.[38]Tensors were also found to be useful in other fields such as continuum mechanics. Some well-known examples of tensors in differential geometry are quadratic forms such as metric tensors, and the Riemann curvature tensor. The exterior algebra of Hermann Grassmann, from the middle of the nineteenth century, is itself a tensor theory, and highly geometric, but it was some time before it was seen, with the theory of differential forms, as naturally unified with tensor calculus. The work of \u00c9lie Cartan made differential forms one of the basic kinds of tensors used in mathematics.In the 20th century, the subject came to be known as tensor analysis, and achieved broader acceptance with the introduction of Einstein's theory of general relativity, around 1915. General relativity is formulated completely in the language of tensors. Einstein had learned about them, with great difficulty, from the geometer Marcel Grossmann.[34] Levi-Civita then initiated a correspondence with Einstein to correct mistakes Einstein had made in his use of tensor analysis. The correspondence lasted 1915\u201317, and was characterized by mutual respect:Tensor calculus was developed around 1890 by Gregorio Ricci-Curbastro under the title absolute differential calculus, and originally presented by Ricci in 1892.[32] It was made accessible to many mathematicians by the publication of Ricci and Tullio Levi-Civita's 1900 classic text M\u00e9thodes de calcul diff\u00e9rentiel absolu et leurs applications (Methods of absolute differential calculus and their applications).[33]The concepts of later tensor analysis arose from the work of Carl Friedrich Gauss in differential geometry, and the formulation was much influenced by the theory of algebraic forms and invariants developed during the middle of the nineteenth century.[29] The word \"tensor\" itself was introduced in 1846 by William Rowan Hamilton[30] to describe something different from what is now meant by a tensor.[Note 3] The contemporary usage was introduced by Woldemar Voigt in 1898.[31]Succinctly, spinors are elements of the spin representation of the rotation group, while tensors are elements of its tensor representations. Other classical groups have tensor representations, and so also tensors that are compatible with the group, but all non-compact classical groups have infinite-dimensional unitary representations as well.When changing from one orthonormal basis (called a frame) to another by a rotation, the components of a tensor transform by that same rotation. This transformation does not depend on the path taken through the space of frames. However, the space of frames is not simply connected (see orientation entanglement and plate trick): there are continuous paths in the space of frames with the same beginning and ending configurations that are not deformable one into the other. It is possible to attach an additional discrete invariant to each frame that incorporates this path dependence, and which turns out (locally) to have values of \u00b11.[26] A spinor is an object that transforms like a tensor under rotations in the frame, apart from a possible sign that is determined by the value of this discrete invariant.[27][28]The transformation law for a tensor behaves as a functor on the category of admissible coordinate systems, under general linear transformations (or, other transformations within some class, such as local diffeomorphisms.) This makes a tensor a special case of a geometrical object, in the technical sense that it is a function of the coordinate system transforming functorially under coordinate changes.[23] Examples of objects obeying more general kinds of transformation laws are jets and, more generally still, natural bundles.[24][25]Under an affine transformation of the coordinates, a tensor transforms by the linear part of the transformation itself (or its inverse) on each index. These come from the rational representations of the general linear group. But this is not quite the most general linear transformation law that such an object may have: tensor densities are non-rational, but are still semisimple representations. A further class of transformations come from the logarithmic representation of the general linear group, a reducible but not semisimple representation,[22] consisting of an (x,y) \u2208 R2 with the transformation lawHere w is called the weight. In general, any tensor multiplied by a power of this function or its absolute value is called a tensor density, or a weighted tensor.[20][21] An example of a tensor density is the current density of electromagnetism.A tensor density transforms like a tensor under a coordinate change, except that it in addition picks up a factor of the absolute value of the determinant of the coordinate transition:[19]More generally, if the Cartesian coordinates xyz undergo a linear transformation, then the numerical value of the density \u03c1 must change by a factor of the reciprocal of the absolute value of the determinant of the coordinate transformation, so that the integral remains invariant, by the change of variables formula for integration. Such a quantity that scales by the reciprocal of the absolute value of the determinant of the coordinate transition map is called a scalar density. To model a non-constant density, \u03c1 is a function of the variables xyz (a scalar field), and under a curvilinear change of coordinates, it transforms by the reciprocal of the Jacobian of the coordinate change. For more on the intrinsic meaning, see Density on a manifold.where the Cartesian coordinates xyz are measured in m. If the units of length are changed into cm, then the numerical values of the coordinate functions must be rescaled by a factor of 100:Suppose that a homogeneous medium fills R3, so that the density of the medium is described by a single scalar value \u03c1 in kg m\u22123. The mass, in kg, of a region \u03a9 is obtained by multiplying \u03c1 by the volume of the region \u03a9, or equivalently integrating the constant \u03c1 over the region:The notion of a tensor can be generalized in a variety of ways to infinite dimensions. One, for instance, is via the tensor product of Hilbert spaces.[16] Another way of generalizing the idea of tensor, common in nonlinear analysis, is via the multilinear maps definition where instead of using finite-dimensional vector spaces and their algebraic duals, one uses infinite-dimensional Banach spaces and their continuous dual.[17] Tensors thus live naturally on Banach manifolds[18] and Fr\u00e9chet manifolds.The vector spaces of a tensor product need not be the same, and sometimes the elements of such a more general tensor product are called \"tensors\". For example, an element of the tensor product space V \u2297 W is a second-order \"tensor\" in this more general sense,[14] and an order-d tensor may likewise be defined as an element of a tensor product of d different vector spaces.[15] A type (n, m) tensor, in the sense defined previously, is also a tensor of order n + m in this more general sense. The concept of tensor product can be extended to arbitrary modules over a ring.The field of nonlinear optics studies the changes to material polarization density under extreme electric fields. The polarization waves generated are related to the generating electric fields through the nonlinear susceptibility tensor. If the polarization P is not linearly proportional to the electric field E, the medium is termed nonlinear. To a good approximation (for sufficiently weak fields, assuming no permanent dipole moments are present), P is given by a Taylor series in E whose coefficients are the nonlinear susceptibilities:The concept of a tensor of order two is often conflated with that of a matrix. Tensors of higher order do however capture ideas important in science and engineering, as has been shown successively in numerous areas as they develop. This happens, for instance, in the field of computer vision, with the trifocal tensor generalizing the fundamental matrix.Common applications includeIf a particular surface element inside the material is singled out, the material on one side of the surface will apply a force on the other side. In general, this force will not be orthogonal to the surface, but it will depend on the orientation of the surface in a linear manner. This is described by a tensor of type (2, 0), in linear elasticity, or more precisely by a tensor field of type (2, 0), since the stresses may vary from point to point.Important examples are provided by continuum mechanics. The stresses inside a solid body or fluid are described by a tensor field. The stress tensor and strain tensor are both second-order tensor fields, and are related in a general linear elastic material by a fourth-order elasticity tensor fields. In detail, the tensor quantifying stress in a 3-dimensional solid object has components that can be conveniently represented as a 3\u2009\u00d7\u20093 array. The three faces of a cube-shaped infinitesimal volume segment of the solid are each subject to some given force. The force's vector components are also three in number. Thus, 3\u2009\u00d7\u20093, or 9 components are required to describe the stress at this cube-shaped infinitesimal segment. Within the bounds of this solid is a whole mass of varying stress quantities, each requiring 9 quantities to describe. Thus, a second-order tensor is needed.Conversely, the inverse operation can be defined, and is called raising an index. This is equivalent to a similar contraction on the product with a (2, 0)-tensor. This inverse metric tensor has components that are the matrix inverse of those of the metric tensor.When a vector space is equipped with a nondegenerate bilinear form (or metric tensor as it is often called in this context), operations can be defined that convert a contravariant (upper) index into a covariant (lower) index and vice versa. A metric tensor is a (symmetric) (0, 2)-tensor; it is thus possible to contract an upper index of a tensor with one of the lower indices of the metric tensor in the product. This produces a new tensor with the same index structure as the previous tensor, but with lower index generally shown in the same position of the contracted upper index. This operation is quite graphically known as lowering an index.(yet again assuming the summation convention).The contraction of T on the first and last slots is then the vectorcan be written as a linear combinationThe contraction can also be understood using the definition of a tensor as an element of a tensor product of copies of the space V with the space V\u2217 by first decomposing the tensor into a linear combination of simple tensors, and then applying a factor from V\u2217 to a factor from V. For example, a tensorThe contraction is often used in conjunction with the tensor product to contract an index from each tensor.Where the summation is again implied. When the (1, 1)-tensor is interpreted as a linear map, this operation is known as the trace.If S is of type (l, k) and T is of type (n, m), then the tensor product S \u2297 T has type (l + n, k + m).which again produces a map that is linear in all its arguments. On components, the effect is to multiply the components of the two input tensors pairwise, i.e.The tensor product takes two tensors, S and T, and produces a new tensor, S \u2297 T, whose order is the sum of the orders of the original tensors. When described as multilinear maps, the tensor product simply multiplies the two tensors, i.e.There are several operations on tensors that again produce a tensor. The linear nature of tensor implies that two tensors of the same type may be added together, and that tensors may be multiplied by a scalar with results analogous to the scaling of a vector. On components, these operations are simply performed component-wise. These operations do not change the type of the tensor; but there are also operations that produce a tensor of different type.A component-free treatment of tensors uses notation that emphasises that tensors do not rely on any basis, and is defined in terms of the tensor product of vector spaces.The abstract index notation is a way to write tensors such that the indices are no longer thought of as numerical, but rather are indeterminates. This notation captures the expressiveness of indices and the basis-independence of index-free notation.Penrose graphical notation is a diagrammatic notation which replaces the symbols for tensors with shapes, and their indices by lines and curves. It is independent of basis elements, and requires no symbols for the indices.The Einstein summation convention dispenses with writing summation signs, leaving the summation implicit. Any repeated index symbol is summed over: if the index i is used twice in a given term of a tensor expression, it means that the term is to be summed for all i. Several distinct pairs of indices may be summed this way.Ricci calculus is the modern formalism and notation for tensor indices: indicating inner and outer products, covariance and contravariance, summations of tensor components, symmetry and antisymmetry, and partial and covariant derivatives.There are several notational systems that are used to describe tensors and perform calculations involving them.Raising an index on an (n, m)-tensor produces an (n + 1, m \u2212 1)-tensor; this corresponds to moving diagonally down and to the left on the table. Symmetrically, lowering an index corresponds to moving diagonally up and to the right on the table. Contraction of an upper with a lower index of an (n, m)-tensor produces an (n \u2212 1, m \u2212 1)-tensor; this corresponds to moving diagonally up and to the left on the table.This table shows important examples of tensors on vector spaces and tensor fields on manifolds. The tensors are classified according to their type (n, m), where n is the number of contravariant indices, m is the number of covariant indices, and n + m gives the total order of the tensor. For example, a bilinear form is the same thing as a (0, 2)-tensor; an inner product is an example of a (0, 2)-tensor, but not all (0, 2)-tensors are inner products. In the (0, M)-entry of the table, M denotes the dimensionality of the underlying vector space or manifold because for each dimension of the space, a separate index is needed to select that dimension to get a maximally covariant antisymmetric tensor.defining a coordinate transformation,[2]In this context, a coordinate basis is often chosen for the tangent vector space. The transformation law may then be expressed in terms of partial derivatives of the coordinate functions,In many applications, especially in differential geometry and physics, it is natural to consider a tensor with components that are functions of the point in a space. This was the setting of Ricci's original work. In modern mathematical terminology such an object is called a tensor field, often referred to simply as a tensor.[2]This discussion of tensors so far assumes finite dimensionality of the spaces involved, where the spaces of tensors obtained by each of these constructions are naturally isomorphic.[Note 2] Constructions of spaces of tensors based on the tensor product and multilinear mappings can be generalized, essentially without modification, to vector bundles or coherent sheaves.[9] For infinite-dimensional vector spaces, inequivalent topologies lead to inequivalent notions of tensor, and these various isomorphisms may or may not hold depending on what exactly is meant by a tensor (see topological tensor product). In some applications, it is the tensor product of Hilbert spaces that is intended, whose properties are the most similar to the finite-dimensional case. A more modern view is that it is the tensors' structure as a symmetric monoidal category that encodes their most important properties, rather than the specific models of those categories.[10]Tensor products can be defined in great generality\u00a0\u2013 for example, involving arbitrary modules over a ring. In principle, one could define a \"tensor\" simply to be an element of any tensor product. However, the mathematics literature usually reserves the term tensor for an element of a tensor product of any number of copies of a single vector space V and its dual, as above.Using the properties of the tensor product, it can be shown that these components satisfy the transformation law for a type (p, q) tensor. Moreover, the universal property of the tensor product gives a 1-to-1 correspondence between tensors defined in this way and tensors defined as multilinear maps.A basis vi of V and basis wj of W naturally induce a basis vi \u2297 wj of the tensor product V \u2297 W. The components of a tensor T are the coefficients of the tensor with respect to the basis obtained from a basis {ei} for V and its dual basis {\u03b5j}, i.e.For some mathematical applications, a more abstract approach is sometimes useful. This can be achieved by defining tensors in terms of elements of tensor products of vector spaces, which in turn are defined through a universal property. A type (p, q) tensor is defined in this context as an element of the tensor product of vector spaces,[7][8]In viewing a tensor as a multilinear map, it is conventional to identify the vector space V with the space of linear functionals on the dual of V, the double dual V\u2217\u2217. There is always a natural linear map from V to its double dual, given by evaluating a linear form in V\u2217 against a vector in V. This linear mapping is an isomorphism in finite dimensions, and it is often then expedient to identify V with its double dual.a (p + q)-dimensional array of components can be obtained. A different choice of basis will yield different components. But, because T is linear in all of its arguments, the components satisfy the tensor transformation law used in the multilinear array definition. The multidimensional array of components of T thus form a tensor according to that definition. Moreover, such an array can be realized as the components of some multilinear map T. This motivates viewing multilinear maps as the intrinsic objects underlying tensors.By applying a multilinear map T of type (p, q) to a basis {ej} for V and a canonical cobasis {\u03b5i} for V\u2217,where V\u2217 is the corresponding dual space of covectors, which is linear in each of its arguments. The above assumes V is a vector space over the real numbers, R. More generally, V can be taken over an arbitrary field of numbers, F (e.g. the complex numbers) with a one-dimensional vector space over F replacing R as the codomain of the multilinear maps.A downside to the definition of a tensor using the multidimensional array approach is that it is not apparent from the definition that the defined object is indeed basis independent, as is expected from an intrinsically geometric object. Although it is possible to show that transformation laws indeed ensure independence from the basis, sometimes a more intrinsic definition is preferred. One approach that is common in differential geometry is to define tensors relative to a fixed (finite-dimensional) vector space V, which is usually taken to be a particular vector space of some geometrical significance like the tangent space to a manifold.[6] In this approach, a type (p, q) tensor T is defined as a multilinear map,The definition of a tensor as a multidimensional array satisfying a transformation law traces back to the work of Ricci.[2]This discussion motivates the following formal definition:[3][4]Here the primed indices denote components in the new coordinates, and the unprimed indices denote the components in the old coordinates. Such a tensor is said to be of order or type (p, q). The terms \"order\", \"type\", \"rank\", \"valence\", and \"degree\" are all sometimes used for the same concept. Here, the term \"order\" or \"total order\" will be used for the total dimension of the array (or its generalisation in other definitions), p + q in the preceding example, and the term \"type\" for the pair giving the number of contravariant and covariant indices. A tensor of type (p, q) is also called a (p, q)-tensor for short.The transformation law for an order p + q tensor with p contravariant indices and q covariant indices is thus given as,Combinations of covariant and contraviant components with the same index allow us to express geometric invariants. For example, the fact that a vector is the same object in different coordinate systems can be captured by the following equations, using the formulas defined above:This is called a covariant transformation law, because the covector transforms by the same matrix as the change of basis matrix. The components of a more general tensor transform by some combination of covariant and contravariant transformations, with one transformation law for each index. If the transformation matrix of an index is the inverse matrix of the basis transformation, then the index is called contravariant and is conventionally denoted with an upper index (superscript). If the transformation matrix of an index is the basis transformation itself, then the index is called covariant and is denoted with a lower index (subscript).where the hat denotes the components in the new basis. This is called a contravariant transformation law, because the vector transforms by the inverse of the change of basis. In contrast, the components, wi, of a covector (or row vector), w transform with the matrix R itself,Here R ji are the entries of the change of basis matrix, and in the rightmost expression the summation sign was suppressed: this is the Einstein summation convention, which will be used throughout this article.[Note 1] The components vi of a column vector v transform with the inverse of the matrix R,Just as a vector in an n-dimensional space is represented by a one-dimensional array of length n with respect to a given basis, any tensor with respect to a basis is represented by a multidimensional array. For example, a linear operator is represented in a basis as a two-dimensional square n \u00d7 n array. The numbers in the multidimensional array are known as the scalar components of the tensor or simply its components. They are denoted by indices giving their position in the array, as subscripts and superscripts, following the symbolic name of the tensor. For example, the components of an order 2 tensor T could be denoted Tij\u202f, where i and j are indices running from 1 to n, or also by T\u2009j\ni. Whether an index is displayed as a superscript or subscript depends on the transformation properties of the tensor, described below. Thus while Tij and T\u2009j\ni can both be expressed as n by n matrices, and are numerically related via index juggling, the difference in their transformation laws indicates it would be improper to add them together. The total number of indices required to identify each component uniquely is equal to the dimension of the array, and is called the order, degree or rank of the tensor. However, the term \"rank\" generally has another meaning in the context of matrices and tensors.Although seemingly different, the various approaches to defining tensors describe the same geometric concept using different languages and at different levels of abstraction.Tensors were conceived in 1900 by Tullio Levi-Civita and Gregorio Ricci-Curbastro, who continued the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others, as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.[2]Tensors are important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as stress, elasticity, fluid mechanics, and general relativity. In applications, it is common to study situations in which a different tensor can occur at each point of an object; for example the stress within an object may vary from one location to another. This leads to the concept of a tensor field. In some areas, tensor fields are so ubiquitous that they are simply called \"tensors\".Because they express a relationship between vectors, tensors themselves must be independent of a particular choice of basis. The basis independence of a tensor then takes the form of a covariant and/or contravariant transformation law that relates the array computed in one basis to that computed in another one. The precise form of the transformation law determines the type (or valence) of the tensor. The tensor type is a pair of natural numbers (n, m), where n is the number of contravariant indices and m is the number of covariant indices. The total order of a tensor is the sum of these two numbers.Given a reference basis of vectors, a tensor can be represented as an organized multidimensional array of numerical values. The order (also degree or rank) of a tensor is the dimensionality of the array needed to represent it, or equivalently, the number of indices needed to label a component of that array. For example, a linear map is represented by a matrix (a 2-dimensional array) in a basis, and therefore is a 2nd-order tensor. A vector is represented as a 1-dimensional array in a basis, and is a 1st-order tensor. Scalars are single numbers and are thus 0th-order tensors. The collection of tensors on a vector space forms a tensor algebra.In mathematics, tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors. Elementary examples of such relations include the dot product, the cross product, and linear maps. Geometric vectors, often used in physics and engineering applications, and scalars themselves are also tensors.[1] A more sophisticated example is the Cauchy stress tensor T, which takes a direction v as input and produces the stress T(v) on the surface normal to this vector for output, thus expressing a relationship between these two vectors, shown in the figure (right).",
            "title": "Tensor",
            "url": "https://en.wikipedia.org/wiki/Tensor"
        },
        {
            "desc_links": [
                "/wiki/Friction",
                "/wiki/Drag_(physics)",
                "/wiki/Gravitation",
                "/wiki/Aristotle",
                "/wiki/Momentum",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Velocity",
                "/wiki/Mass",
                "/wiki/Physical_system",
                "/wiki/Isaac_Newton",
                "/wiki/Philosophi%C3%A6_Naturalis_Principia_Mathematica",
                "/wiki/Classical_physics",
                "/wiki/Force",
                "/wiki/Physical_body",
                "/wiki/Motion_(physics)",
                "/wiki/Speed",
                "/wiki/Relative_direction"
            ],
            "links": [
                "/wiki/Moment_of_inertia",
                "/wiki/Rotation",
                "/wiki/Angular_momentum",
                "/wiki/Torque",
                "/wiki/Gyroscope",
                "/wiki/Conservation_of_energy",
                "/wiki/Momentum#Conservation",
                "/wiki/Non-inertial_reference_frame",
                "/wiki/Fictitious_force",
                "/wiki/Coriolis_effect",
                "/wiki/Inertial_frame",
                "/wiki/Galilean_transformation",
                "/wiki/Inertial_frame",
                "/wiki/Einstein",
                "/wiki/General_theory_of_relativity",
                "/wiki/Space-time_continuum",
                "/wiki/Dennis_Sciama",
                "/wiki/Gravity",
                "/wiki/Tidal_force",
                "/wiki/Mass#Inertial_mass",
                "/wiki/Theory_of_relativity",
                "/wiki/Physicists",
                "/wiki/Mathematicians",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Accuracy_dispute#Disputed_statement",
                "/wiki/Talk:Inertia#Dubious",
                "/wiki/Geodesic_deviation",
                "/wiki/Albert_Einstein",
                "/wiki/Special_relativity",
                "/wiki/Inertial_reference_frames",
                "/wiki/Mass",
                "/wiki/Energy",
                "/wiki/Distance",
                "/wiki/Principle_of_relativity",
                "/wiki/General_theory_of_relativity",
                "/wiki/Space-time",
                "/wiki/Johannes_Kepler",
                "/wiki/Epitome_Astronomiae_Copernicanae",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Euclidean_vector",
                "/wiki/Physics",
                "/wiki/Classical_mechanics",
                "/wiki/Isaac_Newton",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Philosophiae_Naturalis_Principia_Mathematica",
                "/wiki/Isaac_Beeckman",
                "/wiki/Einstein",
                "/wiki/Special_Relativity",
                "/wiki/Horizontal_and_vertical",
                "/wiki/Gravity",
                "/wiki/Friction",
                "/wiki/Nicolaus_Copernicus",
                "/wiki/Galileo",
                "/wiki/Copernican_model",
                "/wiki/Giambattista_Benedetti",
                "/wiki/Albert_of_Saxony_(philosopher)",
                "/wiki/Oxford_Calculators",
                "/wiki/Nicole_Oresme",
                "/wiki/Jean_Buridan",
                "/wiki/Peripatetic_school",
                "/wiki/Millennia",
                "/wiki/Lucretius",
                "/wiki/Epicurus",
                "/wiki/John_Philoponus",
                "/wiki/Averroes",
                "/wiki/Scholasticism",
                "/wiki/Islamic_Golden_Age",
                "/wiki/Renaissance",
                "/wiki/Western_philosophy",
                "/wiki/Aristotle",
                "/wiki/Friction",
                "/wiki/Drag_(physics)",
                "/wiki/Gravitation",
                "/wiki/Aristotle",
                "/wiki/Momentum",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Velocity",
                "/wiki/Mass",
                "/wiki/Physical_system",
                "/wiki/Isaac_Newton",
                "/wiki/Philosophi%C3%A6_Naturalis_Principia_Mathematica",
                "/wiki/Classical_physics",
                "/wiki/Force",
                "/wiki/Physical_body",
                "/wiki/Motion_(physics)",
                "/wiki/Speed",
                "/wiki/Relative_direction"
            ],
            "text": "A gas or liquid in a container will also resist changes in rotational rate.Another form of inertia is rotational inertia (\u2192 moment of inertia), the property that a rotating rigid body maintains its state of uniform rotational motion. Its angular momentum is unchanged, unless an external torque is applied; this is also called conservation of angular momentum. Rotational inertia depends on the object remaining structurally intact as a rigid body, and also has practical consequences. For example, a gyroscope uses the property that it resists any change in the axis of rotation.In summary, the principle of inertia is intimately linked with the principles of conservation of energy and conservation of momentum.However, in reference frames which are experiencing acceleration (non-inertial reference frames), objects appear to be affected by fictitious forces. For example, if the railway carriage were accelerating, the ball would not fall vertically within the carriage but would appear to an observer to be deflected because the carriage and the ball would not be traveling at the same speed while the ball was falling. Other examples of fictitious forces occur in rotating frames such as the earth. For example, a missile at the North Pole could be aimed directly at a location and fired southwards. An observer would see it apparently deflected away from its target by a force (the Coriolis force), but in reality, the southerly target has moved because earth has rotated while the missile is in flight. Because the earth is rotating, a useful inertial frame of reference is defined by the stars, which only move imperceptibly during most observations. Newton's first law of motion is known as the principle of inertia.In an inertial frame, all the observers in uniform (non-accelerating) motion will observe the same laws of physics. However, observers in another inertial frame can make a simple, and intuitively obvious, transformation (the Galilean transformation), to convert their observations. Thus, an observer from outside the moving train could deduce that the dropped ball within the carriage fell vertically downwards.In a location such as a steadily moving railway carriage, a dropped ball (as seen by an observer in the carriage) would behave as it would if it were dropped in a stationary carriage. The ball would simply descend vertically. It is possible to ignore the motion of the carriage by defining it as an inertial frame. In a moving but non-accelerating frame, the ball behaves normally because the train and its contents continue to move at a constant velocity. Before being dropped, the ball was traveling with the train at the same speed, and the ball's inertia ensured that it continued to move in the same speed and direction as the train, even while dropping. Note that, here, it is inertia which ensured that, not its mass.At any non-zero speed, relativistic mass always exceeds gravitational mass. If the mass is made to travel close to the speed of light, its \"inertial mass\" (relativistic) as observed from a stationary frame would be very great while its gravitational mass would remain at its rest value, but the gravitational effect of the extra energy would exactly balance the measured increase in inertial mass.No physical difference has been found between gravitational and inertial mass in a given inertial frame. In experimental measurements, the two always agree within the margin of error for the experiment. Einstein used the fact that gravitational and inertial mass were equal to begin his general theory of relativity, in which he postulated that gravitational mass was the same as inertial mass, and that the acceleration of gravity is a result of a \"valley\" or slope in the space-time continuum that masses \"fell down\". Dennis Sciama later showed that the reaction force produced by the combined gravity of all matter in the universe upon an accelerating object is mathematically equal to the object's inertia,[24] but this would only be a workable physical explanation if, by some mechanism, the gravitational effects operated instantaneously.At high speeds, and especially near the speed of light, inertial mass can be determined by measuring the magnetic field strength and the curvature of the path of an electrically-charged mass such as an electron.Inertial mass is found by applying a known net force to an unknown mass, measuring the resulting acceleration, and applying Newton's Second Law, m = F/a. This gives an accurate value for mass, limited only by the accuracy of the measurements. When astronauts need to be measured in the weightlessness of free fall, they actually find their inertial mass in a special chair called a body mass measurement device (BMMD).Gravitational mass is measured by comparing the force of gravity of an unknown mass to the force of gravity of a known mass. This is typically done with some sort of balance. Equal masses will match on a balance because the gravitational field applies to them equally, producing identical weight. This assumption breaks down near supermassive objects such as black holes and neutron stars due to tidal effects. It also breaks down in weightless environments, because no matter what objects are compared, it will yield a balanced reading. There is no measurable difference between gravitational mass and inertial mass. The gravitational mass is defined by the quantity of gravitational field material a mass possesses, including its energy. The \"inertial mass\" (relativistic mass) is a function of the acceleration a mass has undergone and its resultant speed. A mass that has been accelerated to speeds close to the speed of light has its \"relativistic mass\" increased, and that is why the magnetic field strength in particle accelerators must be increased to force the mass's path to curve. In practice, \"inertial mass\" is normally taken to be \"invariant mass\" and so is identical to gravitational mass without the energy component.This meaning of a body's inertia therefore is altered from the popular meaning as \"a tendency to maintain momentum\" to a description of the measure of how difficult it is to change the velocity of a body, but it is consistent with the fact that motion in one reference frame can disappear in another, so it is the change in velocity that is important.Here, F is force, m is inertial mass, and a is acceleration.But mass, as related to the \"inertia\" of a body, can also be defined by the formula:The factor m is referred to as inertial mass.This was clear at the beginning of the 20th century, before the advent of the theory of relativity. Mass, m, denoted something like an amount of substance or quantity of matter. At the same time, mass was the quantitative measure of inertia of a body.Physicists and mathematicians appear to be less inclined to use the popular concept of inertia as \"a tendency to maintain momentum\" and instead favor the mathematically useful definition of inertia as the measure of a body's resistance to changes in velocity or simply a body's inertial mass.Another profound conclusion of the theory of special relativity\u2014perhaps the most well known\u2014was that energy and mass are not separate things but are, in fact, interchangeable. But this new relationship also carried with it new implications for the concept of inertia. The logical conclusion of special relativity was that if mass exhibits the principle of inertia, then inertia must also apply to energy. This theory, and subsequent experiments confirming some of its conclusions, have also served to radically expand the meaning of inertia to apply more widely and to include inertia of energy.[citation needed]When dealing with very large scales, the traditional Newtonian idea of \"inertia\" does not actually apply and cannot necessarily be relied upon. Luckily, for sufficiently small regions of spacetime, the special theory can be used and inertia still means the same (and works the same) as in the classical model.[dubious \u2013 discuss]As a result of this redefinition, Einstein also redefined the concept of \"inertia\" in terms of geodesic deviation instead, with some subtle but significant additional implications. The result of this is that, according to general relativity, inertia is the gravitational coupling between matter and spacetime.Albert Einstein's theory of special relativity, as proposed in his 1905 paper entitled \"On the Electrodynamics of Moving Bodies\" was built on the understanding of inertia and inertial reference frames developed by Galileo and Newton. While this revolutionary theory did significantly change the meaning of many Newtonian concepts such as mass, energy, and distance, Einstein's concept of inertia remained unchanged from Newton's original meaning (in fact, the entire theory was based on Newton's definition of inertia). However, this resulted in a limitation inherent in special relativity: the principle of relativity could only apply to reference frames that were inertial in nature (meaning when no acceleration was present). In an attempt to address this limitation, Einstein proceeded to develop his general theory of relativity (\"The Foundation of the General Theory of Relativity,\" 1916), which ultimately provided a unified theory for both inertial and noninertial (accelerated) reference frames. However, in order to accomplish this, in general relativity, Einstein found it necessary to redefine several fundamental concepts (such as gravity) in terms of a new concept of \"curvature\" of space-time, instead of the more traditional system of forces understood by Newton.[23]Nevertheless, despite defining the concept so elegantly in his laws of motion, even Newton did not actually use the term \"inertia\" to refer to his First Law. In fact, Newton originally viewed the phenomenon he described in his First Law of Motion as being caused by \"innate forces\" inherent in matter, which resisted any acceleration. Given this perspective, and borrowing from Kepler, Newton attributed the term \"inertia\" to mean \"the innate force possessed by an object which resists changes in motion\"; thus, Newton defined \"inertia\" to mean the cause of the phenomenon, rather than the phenomenon itself. However, Newton's original ideas of \"innate resistive force\" were ultimately problematic for a variety of reasons, and thus most physicists no longer think in these terms. As no alternate mechanism has been readily accepted, and it is now generally accepted that there may not be one which we can know, the term \"inertia\" has come to mean simply the phenomenon itself, rather than any inherent mechanism. Thus, ultimately, \"inertia\" in modern classical physics has come to be a name for the same phenomenon described by Newton's First Law of Motion, and the two concepts are now considered to be equivalent.The term \"inertia\" was first introduced by Johannes Kepler in his Epitome Astronomiae Copernicanae[22] (published in three parts from 1617\u20131621); however, the meaning of Kepler's term (which he derived from the Latin word for \"idleness\" or \"laziness\") was not quite the same as its modern interpretation. Kepler defined inertia only in terms of a resistance to movement, once again based on the presumption that rest was a natural state which did not need explanation. It was not until the later work of Galileo and Newton unified rest and motion in one principle that the term \"inertia\" could be applied to these concepts as it is today.[citation needed]Note that \"velocity\" in this context is defined as a vector, thus Newton's \"constant velocity\" implies both constant speed and constant direction (and also includes the case of zero speed, or no motion). Since initial publication, Newton's Laws of Motion (and by inclusion, this first law) have come to form the basis for the branch of physics known as classical mechanics.[21]Concepts of inertia in Galileo's writings would later come to be refined, modified and codified by Isaac Newton as the first of his Laws of Motion (first published in Newton's work, Philosophiae Naturalis Principia Mathematica, in 1687):The first physicist to completely break away from the Aristotelian model of motion was Isaac Beeckman in 1614.[20]It is also worth noting that Galileo later (in 1632) concluded that based on this initial premise of inertia, it is impossible to tell the difference between a moving object and a stationary one without some outside reference to compare it against.[19] This observation ultimately came to be the basis for Einstein to develop the theory of Special Relativity.Galileo writes that \"all external impediments removed, a heavy body on a spherical surface concentric with the earth will maintain itself in that state in which it has been; if placed in movement towards the west (for example), it will maintain itself in that movement.\"[14] This notion which is termed \"circular inertia\" or \"horizontal circular inertia\" by historians of science, is a precursor to, but distinct from, Newton's notion of rectilinear inertia.[15][16] For Galileo, a motion is \"horizontal\" if it does not carry the moving body towards or away from the centre of the earth, and for him, \"a ship, for instance, having once received some impetus through the tranquil sea, would move continually around our globe without ever stopping.\"[17][18]The principle of inertia states it is the tendency of an object to resist a change in motion. According to Newton, an object will stay at rest or stay in motion (i.e. \"maintain its velocity\") unless acted on by a net external force, whether it results from gravity, friction, contact, or some other force. The Aristotelian division of motion into mundane and celestial became increasingly problematic in the face of the conclusions of Nicolaus Copernicus in the 16th century, who argued that the earth (and everything on it) was in fact never \"at rest\", but was actually in constant motion around the sun.[12] Galileo, in his further development of the Copernican model, recognized these problems with the then-accepted nature of motion and, at least partially as a result, included a restatement of Aristotle's description of motion in a void as a basic physical principle:Benedetti cites the motion of a rock in a sling as an example of the inherent linear motion of objects, forced into circular motion.Shortly before Galileo's theory of inertia, Giambattista Benedetti modified the growing theory of impetus to involve linear motion alone:Buridan's thought was followed up by his pupil Albert of Saxony (1316\u20131390) and the Oxford Calculators, who performed various experiments that further undermined the classical, Aristotelian view. Their work in turn was elaborated by Nicole Oresme who pioneered the practice of demonstrating laws of motion in the form of graphs.In the 14th century, Jean Buridan rejected the notion that a motion-generating property, which he named impetus, dissipated spontaneously. Buridan's position was that a moving object would be arrested by the resistance of the air and the weight of the body which would oppose its impetus.[10] Buridan also maintained that impetus increased with speed; thus, his initial idea of impetus was similar in many ways to the modern concept of momentum. Despite the obvious similarities to more modern ideas of inertia, Buridan saw his theory as only a modification to Aristotle's basic philosophy, maintaining many other peripatetic views, including the belief that there was still a fundamental difference between an object in motion and an object at rest. Buridan also believed that impetus could be not only linear, but also circular in nature, causing objects (such as celestial bodies) to move in a circle.Despite its general acceptance, Aristotle's concept of motion was disputed on several occasions by notable philosophers over nearly two millennia. For example, Lucretius (following, presumably, Epicurus) stated that the \"default state\" of matter was motion, not stasis.[6] In the 6th century, John Philoponus criticized the inconsistency between Aristotle's discussion of projectiles, where the medium keeps projectiles going, and his discussion of the void, where the medium would hinder a body's motion. Philoponus proposed that motion was not maintained by the action of a surrounding medium, but by some property imparted to the object when it was set in motion. Although this was not the modern concept of inertia, for there was still the need for a power to keep a body in motion, it proved a fundamental step in that direction.[7][8][9] This view was strongly opposed by Averroes and by many scholastic philosophers who supported Aristotle. However, this view did not go unchallenged in the Islamic world, where Philoponus did have several supporters who further developed his ideas.Prior to the Renaissance, the most generally accepted theory of motion in Western philosophy was based on Aristotle who around about 335 BC to 322 BC said that, in the absence of an external motive power, all objects (on Earth) would come to rest and that moving objects only continue to move so long as there is a power inducing them to do so. Aristotle explained the continued motion of projectiles, which are separated from their projector, by the action of the surrounding medium, which continues to move the projectile in some way.[4] Aristotle concluded that such violent motion in a void was impossible.[5]On the surface of the Earth, inertia is often masked by the effects of friction and air resistance, both of which tend to decrease the speed of moving objects (commonly to the point of rest), and gravity. This misled the philosopher Aristotle to believe that objects would move only as long as force was applied to them:[2][3]In common usage, the term \"inertia\" may refer to an object's \"amount of resistance to change in velocity\" (which is quantified by its mass), or sometimes to its momentum, depending on the context. The term \"inertia\" is more properly understood as shorthand for \"the principle of inertia\" as described by Newton in his First Law of Motion: an object not subject to any net external force moves at a constant velocity. Thus, an object will continue moving at its current velocity until some force causes its speed or direction to change.Inertia comes from the Latin word, iners, meaning idle, sluggish. Inertia is one of the primary manifestations of mass, which is a quantitative property of physical systems. Isaac Newton defined inertia as his first law in his Philosophi\u00e6 Naturalis Principia Mathematica, which states: [1]Inertia is also defined as the tendency of objects to keep moving in a straight line at a constant velocity. The principle of inertia is one of the fundamental principles in classical physics that are still used to describe the motion of objects and how they are affected by the applied forces on them.Inertia is the resistance of any physical object to any change in its state of motion. This includes changes to the object's speed, direction, or state of rest.",
            "title": "Inertia",
            "url": "https://en.wikipedia.org/wiki/Inertia"
        },
        {
            "desc_links": [
                "/wiki/Mechanics",
                "/wiki/Momentum",
                "/wiki/Angular_momentum",
                "/wiki/Rigid_body_dynamics",
                "/wiki/Orbital_mechanics",
                "/wiki/Point_mass",
                "/wiki/Center_of_mass_frame",
                "/wiki/Inertial_frame",
                "/wiki/Rigid_body",
                "/wiki/Centroid",
                "/wiki/Horseshoe",
                "/wiki/Planets",
                "/wiki/Solar_System",
                "/wiki/Physics",
                "/wiki/Mass",
                "/wiki/Weight_function",
                "/wiki/Position_(vector)",
                "/wiki/Mechanics"
            ],
            "links": [
                "/wiki/Static_equilibrium",
                "/wiki/Physical_law",
                "/wiki/Summation",
                "/wiki/Torque",
                "/wiki/Relative_motion",
                "/wiki/Axis_of_rotation",
                "/wiki/High_jump",
                "/wiki/Fosbury_Flop",
                "/wiki/Orbit",
                "/wiki/Natural_satellite",
                "/wiki/Planet",
                "/wiki/Star",
                "/wiki/Earth",
                "/wiki/Sun",
                "/wiki/Pluto#Charon",
                "/wiki/Helicopter",
                "/wiki/Hover_(helicopter)",
                "/wiki/Rotorhead",
                "/wiki/Helicopter_flight_controls#Cyclic",
                "/wiki/Aircraft",
                "/wiki/Elevator_(aircraft)",
                "/wiki/Stall_(flight)",
                "/wiki/Humvee",
                "/wiki/Rollover",
                "/wiki/Sports_car",
                "/wiki/Car_handling",
                "/wiki/Planimeter",
                "/wiki/Centroid",
                "/wiki/Displacement_(ship)",
                "/wiki/Center_of_buoyancy",
                "/wiki/Plumb_line",
                "/wiki/Conservation_of_momentum",
                "/wiki/Newton%27s_Third_Law",
                "/wiki/Resultant_force",
                "/wiki/Resultant_force",
                "/wiki/Resultant_force",
                "/wiki/Periodic_boundary_conditions",
                "/wiki/Molecular_dynamics",
                "/wiki/Density",
                "/wiki/Centroid",
                "/wiki/Newton%27s_second_law",
                "/wiki/Euler%27s_laws#Euler's_first_law",
                "/wiki/Pappus_of_Alexandria",
                "/wiki/Guido_Ubaldi",
                "/wiki/Francesco_Maurolico",
                "/wiki/Federico_Commandino",
                "/wiki/Simon_Stevin",
                "/wiki/Luca_Valerio",
                "/wiki/Jean-Charles_de_la_Faille",
                "/wiki/Paul_Guldin",
                "/wiki/John_Wallis",
                "/wiki/Louis_Carr%C3%A9_(mathematician)",
                "/wiki/Pierre_Varignon",
                "/wiki/Alexis_Clairaut",
                "/wiki/Archimedes",
                "/wiki/Torque",
                "/wiki/Lever",
                "/wiki/Mechanics",
                "/wiki/Momentum",
                "/wiki/Angular_momentum",
                "/wiki/Rigid_body_dynamics",
                "/wiki/Orbital_mechanics",
                "/wiki/Point_mass",
                "/wiki/Center_of_mass_frame",
                "/wiki/Inertial_frame",
                "/wiki/Rigid_body",
                "/wiki/Centroid",
                "/wiki/Horseshoe",
                "/wiki/Planets",
                "/wiki/Solar_System",
                "/wiki/Physics",
                "/wiki/Mass",
                "/wiki/Weight_function",
                "/wiki/Position_(vector)",
                "/wiki/Mechanics"
            ],
            "text": "In kinesiology and biomechanics, the center of mass is an important parameter that assists people in understanding their human locomotion. Typically, a human\u2019s center of mass is detected with one of two methods: The reaction board method is a static analysis that involves the person lying down on that instrument, and use of their static equilibrium equation to find their center of mass; the segmentation method relies on a mathematical solution based on the physical principle that the summation of the torques of individual body sections, relative to a specified axis, must equal the torque of the whole system that constitutes the body, measured relative to the same axis.[23]When high jumpers perform a \"Fosbury Flop\", they bend their respective bodies in such a way that they clear the bar while their respective centers of mass do not necessarily do so.[22] Because it is the height of the center of gravity (rather than of the highest part of the body) that constrains the minimum energy investment for \"clearing\" the bar, \"snaking over\" the bar can reduce the energy expended in propelling the body upward.The center of mass plays an important role in astronomy and astrophysics, where it is commonly referred to as the barycenter. The barycenter is the point between two objects where they balance each other; it is the center of mass where two or more celestial bodies orbit each other. When a moon orbits a planet, or a planet orbits a star, both bodies are actually orbiting around a point that lies away from the center of the primary (larger) body.[21] For example, the Moon does not orbit the exact center of the Earth, but a point on a line between the center of the Earth and the Moon, approximately 1,710\u00a0km (1,062\u00a0miles) below the surface of the Earth, where their respective masses balance. This is the point about which the Earth and Moon orbit as they travel around the Sun. If the masses are more similar, e.g., Pluto and Charon, the barycenter will fall outside both bodies.For helicopters in hover, the center of mass is always directly below the rotorhead. In forward flight, the center of mass will move forward to balance the negative pitch torque produced by applying cyclic control to propel the helicopter forward; consequently a cruising helicopter flies \"nose-down\" in level flight.[20]The center of mass is an important point on an aircraft, which significantly affects the stability of the aircraft. To ensure the aircraft is stable enough to be safe to fly, the center of mass must fall within specified limits. If the center of mass is ahead of the forward limit, the aircraft will be less maneuverable, possibly to the point of being unable to rotate for takeoff or flare for landing.[18] If the center of mass is behind the aft limit, the aircraft will be more maneuverable, but also less stable, and possibly so unstable that it is impossible to fly. The moment arm of the elevator will also be reduced, which makes it more difficult to recover from a stalled condition.[19]The characteristic low profile of the U. S. military Humvee was designed in part to allow it tilt farther than taller vehicles, without a rollover, because its low center of mass would stay over the space bounded the four wheels even at angles far from the horizontal.Engineers try to design a sports car so that its center of mass is lowered to make the car handle better, that is maintaining traction while executing relatively sharp turns.The three-dimensional coordinates of the center of mass are determined by performing this experiment twice with the object positioned so that these forces are measured for two different horizontal planes through the object. The center of mass will be the intersection of the two lines L1 and L2 obtained from the two experiments.The center of mass lies on the vertical line L, given byThis equation yields the coordinates of the center of mass R* in the horizontal plane as,orA direct development of the planimeter known as an integraph, or integerometer, can be used to establish the position of the centroid or center of mass of an irregular two-dimensional shape. This method can be applied to a shape with an irregular, smooth or complex boundary where other methods are too difficult. It was regularly used by ship builders to compare with the required displacement and center of buoyancy of a ship, and ensure it would not capsize.[16][17]The shape of an object might already be mathematically determined, but it may be too complex to use a known formula. In this case, one can subdivide the complex shape into simpler, more elementary shapes, whose centers of mass are easy to find. If the total mass and center of mass can be determined for each area, then the center of mass of the whole is the weighted average of the centers.[14] This method can even work for objects with holes, which can be accounted for as negative masses.[15]An experimental method for locating the center of mass is to suspend the object from two locations and to drop plumb lines from the suspension points. The intersection of the two lines is the center of mass.[13]The center of mass of a body with an axis of symmetry and constant density must lie on this axis. Thus, the center of mass of a circular cylinder of constant density has its center of mass on the axis of the cylinder. In the same way, the center of mass of a spherically symmetric body of constant density is at the center of the sphere. In general, for any symmetry of a body, its center of mass will be a fixed point of that symmetry.[12]The experimental determination of the center of mass of a body uses gravity forces on the body and relies on the fact that in the parallel gravity field near the surface of the earth the center of mass is the same as the center of gravity.The Law of Conservation of Momentum predicts that for any system not subjected to external forces the momentum of the system will remain constant, which means the center of mass will move with constant velocity. This applies for all systems with classical internal forces, including magnetic fields, electric fields, chemical reactions, and so on. More formally, this is true for any internal forces that cancel in accordance with Newton's Third Law.[11]where m is the total mass of all the particles, p is the linear momentum, and L is the angular momentumIf R is chosen as the center of mass these equations simplify toandThe total linear and angular momentum vectors relative to the reference point R areThe linear and angular momentum of a collection of particles can be simplified by measuring the position and velocity of the particles relative to the center of mass. Let the system of particles Pi, i=1,...,n of masses mi be located at the coordinates ri with velocities vi. Select a reference point R and compute the relative position and velocity vectors,By selecting the center of gravity as the reference point for a rigid body, the gravity forces will not cause the body to rotate, which means the weight of the body can be considered to be concentrated at the center of mass.which means the resultant torque T=0. Because the resultant torque is zero the body will move as though it is a particle with its mass concentrated at the center of mass.If the reference point R is chosen so that it is the center of mass, thenandwhere dm is the mass at the point r, g is the acceleration of gravity, and k is a unit vector defining the vertical direction. Choose a reference point R in the volume and compute the resultant force and torque at this point,In physics the benefits of using the center of mass to model a mass distribution can be seen by considering the resultant of the gravity forces on a continuous body. Consider a body Q of volume V with density \u03c1(r) at each point r in the volume. In a parallel gravity field the force f at each point r is given by,In the study of the dynamics of aircraft, vehicles and vessels, forces and moments need to be resolved relative to the mass center. That is true independent of whether gravity itself is a consideration. Referring to the mass-center as the center-of-gravity is something of a colloquialism, but it is in common usage and when gravity gradient effects are negligible, center-of-gravity and mass-center are the same and are used interchangeably.It is useful to note that the mass-center is a fixed property for a given rigid body (e.g. with no slosh or articulation), whereas the center-of-gravity may, in addition, depend upon its orientation in a non-uniform gravitational field. In the latter case, the center-of-gravity will always be located somewhat closer to the main attractive body as compared to the mass-center, and thus will change its position in the body of interest as its orientation is changed.A body's center of gravity is the point around which the resultant torque due to gravity forces vanishes. Where a gravity field can be considered to be uniform, the mass-center and the center-of-gravity will be the same. However, for satellites in orbit around a planet, in the absence of other torques being applied to a satellite, the slight variation (gradient) in gravitational field between closer-to (stronger) and further-from (weaker) the planet can lead to a torque that will tend to align the satellite such that its long axis is vertical. In such a case, it is important to make the distinction between the center-of-gravity and the mass-center. Any horizontal offset between the two will result in an applied torque.where M is the sum of the masses of all of the particles.For particles in a system with periodic boundary conditions two particles can be neighbours even though they are on opposite sides of the system. This occurs often in molecular dynamics simulations, for example, in which clusters form at random locations and sometimes neighbouring atoms cross the periodic boundary. When a cluster straddles the periodic boundary, a naive calculation of the center of mass will be incorrect. A generalized method for calculating the center of mass for periodic systems is to treat each coordinate, x and y and/or z, as if it were on a circle instead of a line.[10] The calculation takes every particle's x coordinate and maps it to an angle,Let the percentage of the total mass divided between these two particles vary from 100% P1 and 0% P2 through 50% P1 and 50% P2 to 0% P1 and 100% P2, then the center of mass R moves along the line from P1 to P2. The percentages of mass at each point can be viewed as projective coordinates of the point R on this line, and are termed barycentric coordinates. Another way of interpreting the process here is the mechanical balancing of moments about an arbitrary point. The numerator gives the total moment that is then balanced by an equivalent total force at the center of mass. This can be generalized to three points and four points to define projective coordinates in the plane, and in space, respectively.The coordinates R of the center of mass of a two-particle system, P1 and P2, with masses m1 and m2 is given byIf a continuous mass distribution has uniform density, which means \u03c1 is constant, then the center of mass is the same as the centroid of the volume.[9]where M is the total mass in the volume.Solve this equation for the coordinates R to obtainIf the mass distribution is continuous with the density \u03c1(r) within a solid Q, then the integral of the weighted position coordinates of the points in this volume relative to the center of mass R over the volume V is zero, that iswhere M is the sum of the masses of all of the particles.Solving this equation for R yields the formulaIn the case of a system of particles Pi, i = 1,\u2009\u2026,\u2009n\u2009, each with mass mi that are located in space with coordinates ri, i = 1,\u2009\u2026,\u2009n\u2009, the coordinates R of the center of mass satisfy the conditionThe center of mass is the unique point at the center of a distribution of mass in space that has the property that the weighted position vectors relative to this point sum to zero. In analogy to statistics, the center of mass is the mean location of a distribution of mass in space.Newton's second law is reformulated with respect to the center of mass in Euler's first law.[8]Later mathematicians who developed the theory of the center of mass include Pappus of Alexandria, Guido Ubaldi, Francesco Maurolico,[2] Federico Commandino,[3] Simon Stevin,[4] Luca Valerio,[5] Jean-Charles de la Faille, Paul Guldin,[6] John Wallis, Louis Carr\u00e9, Pierre Varignon, and Alexis Clairaut.[7]The concept of \"center of mass\" in the form of the center of gravity was first introduced by the ancient Greek physicist, mathematician, and engineer Archimedes of Syracuse. He worked with simplified assumptions about gravity that amount to a uniform field, thus arriving at the mathematical properties of what we now call the center of mass. Archimedes showed that the torque exerted on a lever by weights resting at various points along the lever is the same as what it would be if all of the weights were moved to a single point\u2014their center of mass. In work on floating bodies he demonstrated that the orientation of a floating object is the one that makes its center of mass as low as possible. He developed mathematical techniques for finding the centers of mass of objects of uniform density of various well-defined shapes.[1]The center of mass is a useful reference point for calculations in mechanics that involve masses distributed in space, such as the linear and angular momentum of planetary bodies and rigid body dynamics. In orbital mechanics, the equations of motion of planets are formulated as point masses located at the centers of mass. The center of mass frame is an inertial frame in which the center of mass of a system is at rest with respect to the origin of the coordinate system.In the case of a single rigid body, the center of mass is fixed in relation to the body, and if the body has uniform density, it will be located at the centroid. The center of mass may be located outside the physical body, as is sometimes the case for hollow or open-shaped objects, such as a horseshoe. In the case of a distribution of separate bodies, such as the planets of the Solar System, the center of mass may not correspond to the position of any individual member of the system.In physics, the center of mass of a distribution of mass in space is the unique point where the weighted relative position of the distributed mass sums to zero, or the point where if a force is applied it moves in the direction of the force without rotating. The distribution of mass is balanced around the center of mass and the average of the weighted position coordinates of the distributed mass defines its coordinates. Calculations in mechanics are often simplified when formulated with respect to the center of mass. It is a hypothetical point where entire mass of an object may be assumed to be concentrated to visualise its motion. In other words, the center of mass is the particle equivalent of a given object for application of Newton's laws of motion.",
            "title": "Center of mass",
            "url": "https://en.wikipedia.org/wiki/Center_of_mass"
        },
        {
            "desc_links": [
                "/wiki/Algorithm",
                "/wiki/Image_processing",
                "/wiki/Digital_image",
                "/wiki/Digital_signal_processing",
                "/wiki/Analog_image_processing",
                "/wiki/Multidimensional_systems"
            ],
            "links": [
                "/wiki/Westworld_(film)",
                "/wiki/Pixellate",
                "/wiki/Image_sensor",
                "/wiki/Color_correction",
                "/wiki/Image_file_formats",
                "/wiki/Affine_transformations",
                "/wiki/Space_Foundation",
                "/wiki/Jet_Propulsion_Laboratory",
                "/wiki/Massachusetts_Institute_of_Technology",
                "/wiki/Bell_Laboratories",
                "/wiki/University_of_Maryland,_College_Park",
                "/wiki/Satellite_imagery",
                "/wiki/Wirephoto",
                "/wiki/Medical_physics",
                "/wiki/Videophone",
                "/wiki/Character_recognition",
                "/wiki/Television_standards_conversion",
                "/wiki/Algorithm",
                "/wiki/Image_processing",
                "/wiki/Digital_image",
                "/wiki/Digital_signal_processing",
                "/wiki/Analog_image_processing",
                "/wiki/Multidimensional_systems"
            ],
            "text": "Westworld (1973) was the first feature film to use the digital image processing to pixellate photography to simulate an android's point of view.[5]Digital cameras generally include specialized digital image processing hardware \u2013 either dedicated chips or added circuitry on other chips \u2013 to convert the raw data from their image sensor into a color-corrected image in a standard image file formatAffine transformations enable basic image transformations including scale, rotate, translate, mirror and sheer as is shown in the following examples show:[4]MATLAB example for spatial domain highpass filtering.Notice that the highpass filter shows extra edges when zero padded compared to the repeated edge padding.Images are typically padded before being transformed to the Fourier space, the highpass filtered images below illustrate the consequences of different padding techniques:Digital filters are used to blur and sharpen digital images. Filtering can be performed in the spatial domain by convolution with specifically designed kernels (filter array), or in the frequency (Fourier) domain by masking specific frequency regions. The following examples show both methods: [3]Some techniques which are used in digital image processing include:In particular, digital image processing is the only practical technology for:Digital image processing allows the use of much more complex algorithms, and hence, can offer both more sophisticated performance at simple tasks, and the implementation of methods which would be impossible by analog means.Digital image processing technology for medical applications was inducted into the Space Foundation Space Technology Hall of Fame in 1994.[2]Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s at the Jet Propulsion Laboratory, Massachusetts Institute of Technology, Bell Laboratories, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement.[1] The cost of processing was fairly high, however, with the computing equipment of that era. That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available. Images then could be processed in real time, for some dedicated problems such as television standards conversion. As general-purpose computers became faster, they started to take over the role of dedicated hardware for all but the most specialized and computer-intensive operations. With the fast computers and signal processors available in the 2000s, digital image processing has become the most common form of image processing and generally, is used because it is not only the most versatile method, but also the cheapest.Digital image processing is the use of computer algorithms to perform image processing on digital images. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and signal distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems.",
            "title": "Digital image processing",
            "url": "https://en.wikipedia.org/wiki/Image_processing"
        },
        {
            "desc_links": [
                "/wiki/Darkness",
                "/wiki/Star",
                "/wiki/Apparent_magnitude",
                "/wiki/Absolute_magnitude",
                "/wiki/HSL_color_space",
                "/wiki/Hue",
                "/wiki/Saturation_(color_theory)",
                "/wiki/RGB_color_space",
                "/wiki/Arithmetic_mean",
                "/wiki/White%27s_illusion",
                "/wiki/Beorht",
                "/wiki/Common_Germanic",
                "/wiki/Proto-Indo-European_language",
                "/wiki/Photometry_(optics)",
                "/wiki/Luminance",
                "/wiki/Radiometric",
                "/wiki/Radiance",
                "/wiki/Federal_Standard_1037C",
                "/wiki/Visual_perception",
                "/wiki/Light",
                "/wiki/Luminance",
                "/wiki/Color_appearance_model#Color_appearance_parameters",
                "/wiki/Color_appearance_model",
                "/wiki/Lightness"
            ],
            "links": [
                "/wiki/Timbre",
                "/wiki/Spectral_centroid",
                "/wiki/Federal_Trade_Commission",
                "/wiki/Electric_light",
                "/wiki/Luminous_flux",
                "/wiki/Darkness",
                "/wiki/Star",
                "/wiki/Apparent_magnitude",
                "/wiki/Absolute_magnitude",
                "/wiki/HSL_color_space",
                "/wiki/Hue",
                "/wiki/Saturation_(color_theory)",
                "/wiki/RGB_color_space",
                "/wiki/Arithmetic_mean",
                "/wiki/White%27s_illusion",
                "/wiki/Beorht",
                "/wiki/Common_Germanic",
                "/wiki/Proto-Indo-European_language",
                "/wiki/Photometry_(optics)",
                "/wiki/Luminance",
                "/wiki/Radiometric",
                "/wiki/Radiance",
                "/wiki/Federal_Standard_1037C",
                "/wiki/Visual_perception",
                "/wiki/Light",
                "/wiki/Luminance",
                "/wiki/Color_appearance_model#Color_appearance_parameters",
                "/wiki/Color_appearance_model",
                "/wiki/Lightness"
            ],
            "text": "The term \"brightness\" is also used in discussions of sound timbres, in a rough analogy with visual brightness. Timbre researchers consider brightness to be one of the perceptually strongest distinctions between sounds,[6] and formalize it acoustically as an indication of the amount of high-frequency content in a sound, using a measure such as the spectral centroid.The United States Federal Trade Commission (FTC) has assigned an unconventional meaning to brightness when applied to lamps. When appearing on light bulb packages, brightness means luminous flux, while in other contexts it means luminance.[5] Luminous flux is the total amount of light coming from a source, such as a lighting device. Luminance, the original meaning of brightness, is the amount of light per solid angle coming from an area, such as the sky. The table below shows the standard ways of indicating the amount of light.Brightness is, at least in some respects, the antonym of darkness.With regard to stars, brightness is quantified as apparent magnitude and absolute magnitude.Brightness is also a color coordinate in HSL color space\u00a0: hue, saturation, and lightness, meaning here brightness.In the RGB color space, brightness can be thought of as the arithmetic mean \u03bc of the red, green, and blue color coordinates (although some of the three components make the light seem brighter than others, which, again, may be compensated by some display systems automatically):[4]A given target luminance can elicit different perceptions of brightness in different contexts; see, for example, White's illusion.Terminology The adjective bright derives from an Old English beorht with the same meaning via metathesis giving Middle English briht. The word is from a Common Germanic *berhtaz, ultimately from a PIE root with a closely related meaning, *bhereg- \"white, bright\". \"Brightness\" was formerly used as a synonym for the photometric term luminance and (incorrectly) for the radiometric term radiance. As defined by the US Federal Glossary of Telecommunication Terms (FS-1037C), \"brightness\" should now be used only for non-quantitative references to physiological sensations and perceptions of light.[3]Brightness is an attribute of visual perception in which a source appears to be radiating or reflecting light.[1] In other words, brightness is the perception elicited by the luminance of a visual target. It is not necessarily proportional to luminance. This is a subjective attribute/property of an object being observed and one of the color appearance parameters of color appearance models. Brightness refers to an absolute term and should not be confused with Lightness.[2]",
            "title": "Brightness",
            "url": "https://en.wikipedia.org/wiki/Brightness"
        },
        {
            "desc_links": [
                "/wiki/Portmanteau",
                "/wiki/Voxel",
                "/wiki/Texel_(graphics)",
                "/wiki/Camera_sensor",
                "/wiki/Chroma_subsampling",
                "/wiki/Bayer_filter",
                "/wiki/Sampling_(signal_processing)",
                "/wiki/Intensity_(physics)",
                "/wiki/RGB_color_model",
                "/wiki/CMYK_color_model",
                "/wiki/Digital_imaging",
                "/wiki/Raster_graphics",
                "/wiki/All_points_addressable",
                "/wiki/Display_device"
            ],
            "links": [
                "/wiki/Micro_Four_Thirds_System",
                "/wiki/DxO_Labs",
                "/wiki/Perceptual_MegaPixel",
                "/wiki/Sigma_35mm_f/1.4_DG_HSM_lens",
                "/wiki/Nikon_D800",
                "/wiki/Charge-coupled_device",
                "/wiki/CMOS",
                "/wiki/Bayer_filter",
                "/wiki/Demosaicing",
                "/wiki/Image_sensor",
                "/wiki/Digital_camera",
                "/wiki/Computer_display",
                "/wiki/Sample_(graphics)",
                "/wiki/Subpixel_rendering",
                "/wiki/Pixel_geometry",
                "/wiki/Cathode_Ray_Tube",
                "/wiki/Image_sensor",
                "/wiki/Bayer_filter",
                "/wiki/Channel_(digital_image)",
                "/wiki/LCD",
                "/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions",
                "/wiki/Highcolor",
                "/wiki/Opacity_(optics)",
                "/wiki/Radian",
                "/wiki/Focal_length",
                "/wiki/F-number",
                "/wiki/Arcsecond#Symbols_and_abbreviations",
                "/wiki/GUI",
                "/wiki/Video_card",
                "/wiki/LCD",
                "/wiki/Native_resolution",
                "/wiki/Triad_(monitors)",
                "/wiki/Cathode_ray_tube",
                "/wiki/Regular_grid",
                "/wiki/Convolution_kernel",
                "/wiki/JPEG",
                "/wiki/Bijection",
                "/wiki/Bitmap",
                "/wiki/Raster_graphics",
                "/wiki/Raster_scan",
                "/wiki/Halftone",
                "/wiki/Video_Graphics_Array",
                "/wiki/Dots_per_inch",
                "/wiki/Pixels_per_inch",
                "/wiki/Image_resolution",
                "/wiki/CMYK",
                "/wiki/Digital_camera",
                "/wiki/Pixilation",
                "/wiki/Pixie",
                "/wiki/Norman_McLaren",
                "/wiki/Grant_Munro_(filmmaker)",
                "/wiki/IBM_Personal_Computer",
                "/wiki/Paul_Nipkow",
                "/wiki/Wireless_World",
                "/wiki/Data_element",
                "/wiki/Variety_(magazine)",
                "/wiki/Frederic_C._Billingsley",
                "/wiki/Jet_Propulsion_Laboratory",
                "/wiki/Space_probe",
                "/wiki/Palo_Alto,_California",
                "/wiki/Portmanteau",
                "/wiki/Voxel",
                "/wiki/Texel_(graphics)",
                "/wiki/Camera_sensor",
                "/wiki/Chroma_subsampling",
                "/wiki/Bayer_filter",
                "/wiki/Sampling_(signal_processing)",
                "/wiki/Intensity_(physics)",
                "/wiki/RGB_color_model",
                "/wiki/CMYK_color_model",
                "/wiki/Digital_imaging",
                "/wiki/Raster_graphics",
                "/wiki/All_points_addressable",
                "/wiki/Display_device"
            ],
            "text": "One new method to add Megapixels has been introduced in a Micro Four Thirds System camera which only uses 16MP sensor, but can produce 64MP RAW (40MP JPEG) by making two exposures, shifting the sensor by a half pixel between them. Using a tripod to take level multi-shots within an instance, the multiple 16MP images are then generated into a unified 64MP image.[23]DxO Labs invented the Perceptual MegaPixel (P-MPix) to measure the sharpness that a camera produces when paired to a particular lens \u2013 as opposed to the MP a manufacturer states for a camera product which is based only on the camera's sensor. The new P-MPix claims to be a more accurate and relevant value for photographers to consider when weighing-up camera sharpness.[21] As of mid-2013, the Sigma 35mm f/1.4 DG HSM lens mounted on a Nikon D800 has the highest measured P-MPix. However, with a value of 23 MP, it still wipes-off more than one-third of the D800's 36.3 MP sensor.[22]Digital cameras use photosensitive electronics, either charge-coupled device (CCD) or complementary metal\u2013oxide\u2013semiconductor (CMOS) image sensors, consisting of a large number of single sensor elements, each of which records a measured intensity level. In most digital cameras, the sensor array is covered with a patterned color filter mosaic having red, green, and blue regions in the Bayer filter arrangement, so that each sensor element can record the intensity of a single primary color of light. The camera interpolates the color information of neighboring sensor elements, through a process called demosaicing, to create the final image. These sensor elements are often called \"pixels\", even though they only record 1 channel (only red, or green, or blue) of the final color image. Thus, two of the three color channels for each sensor must be interpolated and a so-called N-megapixel camera that produces an N-megapixel image provides only one-third of the information that an image of the same size could get from a scanner. Thus, certain color contrasts may look fuzzier than others, depending on the allocation of the primary colors (green has twice as many elements as red or blue in the Bayer arrangement).A megapixel (MP) is a million pixels; the term is used not only for the number of pixels in an image, but also to express the number of image sensor elements of digital cameras or the number of display elements of digital displays. For example, a camera that makes a 2048\u00d71536 pixel image (3,145,728 finished image pixels) typically uses a few extra rows and columns of sensor elements and is commonly said to have \"3.2 megapixels\" or \"3.4 megapixels\", depending on whether the number reported is the \"effective\" or the \"total\" pixel count.[20]The concept of subpixels is related to samples.This latter approach, referred to as subpixel rendering, uses knowledge of pixel geometry to manipulate the three colored subpixels separately, producing an increase in the apparent resolution of color displays. While CRT displays use red-green-blue-masked phosphor areas, dictated by a mesh grid called the shadow mask, it would require a difficult calibration step to be aligned with the displayed pixel raster, and so CRTs do not currently use subpixel rendering.For systems with subpixels, two different approaches can be taken:Most digital camera image sensors use single-color sensor regions, for example using the Bayer filter pattern, and in the camera industry these are known as pixels just like in the display industry, not subpixels.Many display and image-acquisition systems are, for various reasons, not capable of displaying or sensing the different color channels at the same site. Therefore, the pixel grid is divided into single-color regions that contribute to the displayed or sensed color when viewed at a distance. In some displays, such as LCD, LED, and plasma displays, these single-color regions are separately addressable elements, which have come to be known as subpixels.[19] For example, LCDs typically divide each pixel vertically into three subpixels. When the square pixel is divided into three subpixels, each subpixel is necessarily rectangular. In display industry terminology, subpixels are often referred to as pixels,[by whom?] as they are the basic addressable elements in a viewpoint of hardware, and hence pixel circuits rather than subpixel circuits is used.For color depths of 15 or more bits per pixel, the depth is normally the sum of the bits allocated to each of the red, green, and blue components. Highcolor, usually meaning 16 bpp, normally has five bits for red and blue each, and six bits for green, as the human eye is more sensitive to errors in green than in the other two primary colors. For applications involving transparency, the 16 bits may be divided into five bits each of red, green, and blue, with one bit left for transparency. A 24-bit depth allows 8 bits per component. On some systems, 32-bit depth is available: this means that each 24-bit pixel has an extra 8 bits to describe its opacity (for purposes of combining with another image)....The number of distinct colors that can be represented by a pixel depends on the number of bits per pixel (bpp). A 1 bpp image uses 1-bit for each pixel, so each pixel can be either on or off. Each additional bit doubles the number of colors available, so a 2 bpp image can have 4 colors, and a 3 bpp image can have 8 colors:The pixel scale used in astronomy is the angular distance between two objects on the sky that fall one pixel apart on the detector (CCD or infrared chip). The scale s measured in radians is the ratio of the pixel spacing p and focal length f of the preceding optics, s=p/f. (The focal length is the product of the focal ratio by the diameter of the associated lens or mirror.) Because p is usually expressed in units of arcseconds per pixel, because 1 radian equals 180/\u03c0*3600\u2248206,265 arcseconds, and because diameters are often given in millimeters and pixel sizes in micrometers which yields another factor of 1,000, the formula is often quoted as s=206p/f.Computers can use pixels to display an image, often an abstract image that represents a GUI. The resolution of this image is called the display resolution and is determined by the video card of the computer. LCD monitors also use pixels to display an image, and have a native resolution. Each pixel is made up of triads, with the number of these triads determining the native resolution. On some CRT monitors, the beam sweep rate may be fixed, resulting in a fixed native resolution. Most CRT monitors do not have a fixed beam sweep rate, meaning they do not have a native resolution at all - instead they have a set of resolutions that are equally well supported. To produce the sharpest images possible on an LCD, the user must ensure the display resolution of the computer matches the native resolution of the monitor.For example:For convenience, pixels are normally arranged in a regular two-dimensional grid. By using this arrangement, many common operations can be implemented by uniformly applying the same operation to each pixel independently. Other arrangements of pixels are possible, with some sampling patterns even changing the shape (or kernel) of each pixel across the image. For this reason, care must be taken when acquiring an image on one device and displaying it on another, or when converting image data from one pixel format to another.The pixels, or color samples, that form a digitized image (such as a JPEG file used on a web page) may or may not be in one-to-one correspondence with screen pixels, depending on how a computer displays an image. In computing, an image composed of pixels is known as a bitmapped image or a raster image. The word raster originates from television scanning patterns, and has been widely used to describe similar halftone printing and storage techniques.The more pixels used to represent an image, the closer the result can resemble the original. The number of pixels in an image is sometimes called the resolution, though resolution has a more specific definition. Pixel counts can be expressed as a single number, as in a \"three-megapixel\" digital camera, which has a nominal three million pixels, or as a pair of numbers, as in a \"640 by 480 display\", which has 640 pixels from side to side and 480 from top to bottom (as in a VGA display), and therefore has a total number of 640\u00d7480 = 307,200 pixels or 0.3 megapixels.The measures dots per inch (dpi) and pixels per inch (ppi) are sometimes used interchangeably, but have distinct meanings, especially for printer devices, where dpi is a measure of the printer's density of dot (e.g. ink droplet) placement.[14] For example, a high-quality photographic image may be printed with 600 ppi on a 1200 dpi inkjet printer.[15] Even higher dpi numbers, such as the 4800 dpi quoted by printer manufacturers since 2002, do not mean much in terms of achievable resolution.[16]A pixel is generally thought of as the smallest single component of a digital image. However, the definition is highly context-sensitive. For example, there can be \"printed pixels\" in a page, or pixels carried by electronic signals, or represented by digital values, or pixels on a display device, or pixels in a digital camera (photosensor elements). This list is not exhaustive and, depending on context, synonyms include pel, sample, byte, bit, dot, and spot. Pixels can be used as a unit of measure such as: 2400 pixels per inch, 640 pixels per line, or spaced 10 pixels apart.Pixilation, spelled with a second i, is an unrelated filmmaking technique that dates to the beginnings of cinema, in which live actors are posed frame by frame and photographed to create stop-motion animation. An archaic British word meaning \"possession by spirits (pixies)\", the term has been used to describe the animation process since the early 1950s; various animators, including Norman McLaren and Grant Munro, are credited with popularizing it.[13]Pixels, abbreviated as 'px,' are also a unit of measurement commonly used in graphic and web design, equivalent to roughly 1\u204496 inch (0.26\u00a0mm). This measurement is used to make sure a given element will display as the same size no matter what screen resolution views it.[12]Some authors explain pixel as picture cell, as early as 1972.[10] In graphics and in image and video processing, pel is often used instead of pixel.[11] For example, IBM used it in their Technical Reference for the original PC.The concept of a \"picture element\" dates to the earliest days of television, for example as \"Bildpunkt\" (the German word for pixel, literally 'picture point') in the 1888 German patent of Paul Nipkow. According to various etymologies, the earliest publication of the term picture element itself was in Wireless World magazine in 1927,[8] though it had been used earlier in various U.S. patents filed as early as 1911.[9]The word is a combination of pix, for picture, and element. The word pix appeared in Variety magazine headlines in 1932, as an abbreviation for the word pictures, in reference to movies.[7] By 1938, \"pix\" was being used in reference to still pictures by photojournalists.[6]The word \"pixel\" was first published in 1965 by Frederic C. Billingsley of JPL, to describe the picture elements of video images from space probes to the Moon and Mars.[5] Billingsley had learned the word from Keith E. McFarland, at the Link Division of General Precision in Palo Alto, who in turn said he did not know where it originated. McFarland said simply it was \"in use at the time\" (circa 1963).[6]The word pixel is a portmanteau of pix (from \"pictures\", shortened to \"pics\") and el (for \"element\"); similar formations with 'el' include the words voxel[4] and texel.[4]In some contexts (such as descriptions of camera sensors), pixel refers to a single scalar element of a multi-component representation (more precisely called a photosite in the camera sensor context, although the neologism sensel is sometimes used to describe the elements of a digital camera's sensor),[3] while in yet other contexts it may refer to the set of component intensities for a spatial position. Drawing a distinction between pixels, photosites, and samples may reduce confusion when describing color systems that use chroma subsampling or cameras that use Bayer filter to produce color components via upsampling.Each pixel is a sample of an original image; more samples typically provide more accurate representations of the original. The intensity of each pixel is variable. In color imaging systems, a color is typically represented by three or four component intensities such as red, green, and blue, or cyan, magenta, yellow, and black.In digital imaging, a pixel, pel,[1] dots, or picture element[2] is a physical point in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.",
            "title": "Pixel",
            "url": "https://en.wikipedia.org/wiki/Pixel"
        },
        {
            "desc_links": [
                "/wiki/Symmetric_matrix",
                "/wiki/Positive_semi-definite_matrix",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Covariance",
                "/wiki/Random_vector",
                "/wiki/Random_variable",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Joint_probability_distribution"
            ],
            "links": [
                "/wiki/Financial_economics",
                "/wiki/Modern_portfolio_theory",
                "/wiki/Mutual_fund_separation_theorem",
                "/wiki/Capital_asset_pricing_model",
                "/wiki/Normative_economics",
                "/wiki/Positive_economics",
                "/wiki/Diversification_(finance)",
                "/wiki/Transformation_matrix",
                "/wiki/Whitening_transformation",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Rayleigh_quotient",
                "/wiki/Principal_component_analysis",
                "/wiki/Karhunen-Lo%C3%A8ve_transform",
                "/wiki/Variance#Generalizations",
                "/wiki/Complex_number",
                "/wiki/Complex_conjugation",
                "/wiki/Multivariate_normal_distribution",
                "/wiki/Elliptical_distribution",
                "/wiki/Probability_density_function",
                "/wiki/Normal_equations",
                "/wiki/Ordinary_least_squares",
                "/wiki/Regression_analysis",
                "/wiki/Schur_complement",
                "/wiki/Conditional_variance",
                "/wiki/Conditional_mean",
                "/wiki/Cross-covariance",
                "/wiki/Off-diagonal_element",
                "/wiki/Variance",
                "/wiki/Expected_value",
                "/wiki/Random_variable",
                "/wiki/Variance",
                "/wiki/Covariance",
                "/wiki/Column_vector",
                "/wiki/Symmetric_matrix",
                "/wiki/Positive_semi-definite_matrix",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Covariance",
                "/wiki/Random_vector",
                "/wiki/Random_variable",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Joint_probability_distribution"
            ],
            "text": "The covariance matrix plays a key role in financial economics, especially in portfolio theory and its mutual fund separation theorem and in the capital asset pricing model. The matrix of covariances among various assets' returns is used to determine, under certain assumptions, the relative amounts of different assets that investors should (in a normative analysis) or are predicted to (in a positive analysis) choose to hold in a context of diversification.The covariance matrix is a useful tool in many different areas. From it a transformation matrix can be derived, called a whitening transformation, that allows one to completely decorrelate the data[citation needed] or, from a different point of view, to find an optimal basis for representing the data in a compact way[citation needed] (see Rayleigh quotient for a formal proof and additional properties of covariance matrices). This is called principal component analysis (PCA) and the Karhunen-Lo\u00e8ve transform (KL-transform).These empirical sample correlation matrices are the most straightforward and most often used estimators for the correlation matrices, but other estimators also exist, including regularised or shrinkage estimators, which may have better properties.or, if the column means were known a-priori,The variance of a complex scalar-valued random variable with expected value \u03bc is conventionally defined using complex conjugation:If a vector of n possibly correlated random variables is jointly normally distributed, or more generally elliptically distributed, then its probability density function can be expressed in terms of the covariance matrix.[5]The matrix of regression coefficients may often be given in transpose form, \u03a3XX\u22121\u03a3XY, suitable for post-multiplying a row vector of explanatory variables xT rather than pre-multiplying a column vector x. In this form they correspond to the coefficients obtained by inverting the matrix of the normal equations of ordinary least squares (OLS).The matrix \u03a3YX\u03a3XX\u22121 is known as the matrix of regression coefficients, while in linear algebra \u03a3Y|X is the Schur complement of \u03a3XX in \u03a3X,Yand conditional variancedefined by conditional meanBy comparison, the notation for the cross-covariance between two vectors isEach element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself, which always equals 1. Each off-diagonal element is between 1 and \u20131 inclusive.This form can be seen as a generalization of the scalar-valued variance to higher dimensions. Recall that for a scalar-valued random variable XThe definition above is equivalent to the matrix equalityis the expected value of the i th entry in the vector X. In other words,where the operator E denotes the expected (mean) value of its argument, andare random variables, each with finite variance, then the covariance matrix \u03a3 is the matrix whose (i,\u00a0j) entry is the covarianceIf the entries in the column vectorThroughout this article, boldfaced unsubscripted X and Y are used to refer to random vectors, and unboldfaced subscripted Xi and Yi are used to refer to random scalars.Because the covariance of the ith random variable with itself is simply that random variable's variance, each element on the principal diagonal of the covariance matrix is the variance of one of the random variables. Because the covariance of the ith random variable with the jth one is the same thing as the covariance of the jth random variable with the ith one, every covariance matrix is symmetric. In addition, every covariance matrix is positive semi-definite.Intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. As an example, the variation in a collection of random points in two-dimensional space cannot be characterized fully by a single number, nor would the variances in the x and y directions contain all of the necessary information; a 2\u00d72 matrix would be necessary to fully characterize the two-dimensional variation.In probability theory and statistics, a covariance matrix (also known as dispersion matrix or variance\u2013covariance matrix) is a matrix whose element in the i, j position is the covariance between the i\u00a0th and j\u00a0th elements of a random vector. A random vector is a random variable with multiple dimensions. Each element of the vector is a scalar random variable. Each element has either a finite number of observed empirical values or a finite or infinite number of potential values. The potential values are specified by a theoretical joint probability distribution.",
            "title": "Covariance matrix",
            "url": "https://en.wikipedia.org/wiki/Covariance_matrix"
        },
        {
            "desc_links": [
                "/wiki/Eigenvector",
                "/wiki/Computer_vision",
                "/wiki/Facial_recognition_system",
                "/wiki/Facial_recognition_system",
                "/wiki/Alex_Pentland",
                "/wiki/Covariance_matrix",
                "/wiki/Probability_distribution",
                "/wiki/Dimension",
                "/wiki/Vector_space"
            ],
            "links": [
                "/wiki/Active_appearance_model",
                "/wiki/Active_shape_model",
                "/wiki/Fisherface",
                "/wiki/Linear_discriminant_analysis",
                "/wiki/Euclidean_distance",
                "/wiki/Singular_value_decomposition",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Handwriting_recognition",
                "/wiki/Lip_reading",
                "/wiki/Speaker_recognition",
                "/wiki/Sign_language",
                "/wiki/Gestures",
                "/wiki/Medical_imaging",
                "/wiki/Symmetry",
                "/wiki/Principal_component_analysis",
                "/wiki/Statistical_analysis",
                "/wiki/Digital_photograph",
                "/wiki/Eigenvectors",
                "/wiki/Covariance_matrix",
                "/wiki/Principal_component_analysis",
                "/wiki/Eigenvector",
                "/wiki/Computer_vision",
                "/wiki/Facial_recognition_system",
                "/wiki/Facial_recognition_system",
                "/wiki/Alex_Pentland",
                "/wiki/Covariance_matrix",
                "/wiki/Probability_distribution",
                "/wiki/Dimension",
                "/wiki/Vector_space"
            ],
            "text": "To cope with illumination distraction in practice, the eigenface method usually discards the first three eigenfaces from the dataset. Since illumination is usually the cause behind the largest variations in face images, the first three eigenfaces will mainly capture the information of 3-dimensional lighting changes, which has little contribution to face recognition. By discarding those three eigenfaces, there will be a decent amount of boost in accuracy of face recognition, but other methods such as Fisherface and Linear space still have the advantage.However, the deficiencies of the eigenface method are also obvious:Eigenface provides an easy and cheap way to realize face recognition in that:Many modern approaches still use principal component analysis as a means of dimension reduction or to form basis images for different modes of variation.A further alternative to eigenfaces and fisherfaces is the active appearance model. This approach uses an active shape model to describe the outline of a face. By collecting many face outlines, principal component analysis can be used to form a basis set of models which, encapsulate the variation of different faces.Various extensions have been made to the eigenface method such eigenfeatures. This method combines facial metrics (measuring distance between facial features) with the eigenface representation. Another method similar to the eigenface technique is 'fisherfaces' which uses linear discriminant analysis.[9] This method for facial recognition is less sensitive to variation in lighting and pose of the face than using eigenfaces. Fisherface utilises labelled data to retain more of the class specific information during the dimension reduction stage.The weights of each gallery image only convey information describing that image, not that subject. An image of one subject under frontal lighting may have very different weights to those of the same subject under strong left lighting. This limits the application of such a system. Experiments in the original Eigenface paper presented the following results: an average of 96% with light variation, 85% with orientation variation, and 64% with size variation. (Turk & Pentland 1991, p.\u00a0590)Intuitively, recognition process with eigenface method is to project query images into the face-space spanned by eigenfaces we have calculated and in that face-space find the closest match to a face class.To recognise faces, gallery images, those seen by the system, are saved as collections of weights describing the contribution each eigenface has to that image. When a new face is presented to the system for classification, its own weights are found by projecting the image onto the collection of eigenfaces. This provides a set of weights describing the probe face. These weights are then classified against all weights in the gallery set to find the closest match. A nearest neighbour method is a simple approach for finding the Euclidean distance between two vectors, where the minimum can be classified as the closest subject.(Turk & Pentland 1991, p.\u00a0590)Facial recognition was the source of motivation behind the creation of eigenfaces. For this use, eigenfaces have advantages over other techniques available, such as the system's speed and efficiency. As eigenface is primarily a dimension reduction method, a system can represent many subjects with a relatively small set of data. As a face recognition system it is also fairly invariant to large reductions in image sizing, however it begins to fail considerably when the variation between the seen images and probe image is large.Using SVD on data matrix X, we don\u2019t need to calculate the actual covariance matrix to get eigenfaces.Thus we can see easily that:Let the singular value decomposition (SVD) of X be:Meaning that, if ui is an eigenvector of TTT, then vi\u00a0=\u00a0Tui is an eigenvector of S. If we have a training set of 300 images of 100\u00a0\u00d7\u00a0100 pixels, the matrix TTT is a 300\u00a0\u00d7\u00a0300 matrix, which is much more manageable than the 10,000\u00a0\u00d7 10,000 covariance matrix. Notice however that the resulting vectors vi are not normalised; if normalisation is required it should be applied as an extra step.then we notice that by pre-multiplying both sides of the equation with T, we obtainHowever TTT is a large matrix, and if instead we take the eigenvalue decomposition ofLet T be the matrix of preprocessed training examples, where each column contains one mean-subtracted image. The covariance matrix can then be computed as S = TTT and the eigenvector decomposition of S is given byPerforming PCA directly on the covariance matrix of the images is often computationally infeasible. If small images are used, say 100\u00a0\u00d7\u00a0100 pixels, each image is a point in a 10,000-dimensional space and the covariance matrix S is a matrix of 10,000\u00a0\u00d7 10,000\u00a0= 108 elements. However the rank of the covariance matrix is limited by the number of training examples: if there are N training examples, there will be at most N\u00a0\u2212\u00a01 eigenvectors with non-zero eigenvalues. If the number of training examples is smaller than the dimensionality of the images, the principal components can be computed more easily as follows.Note that although the covariance matrix S generates many eigenfaces, only a fraction of those are needed to represent the majority of the faces. For example, to represent 95% of the total variation of all face images, only the first 43 eigenfaces are needed. To calculate this result, implement the following code:Here is an example of calculating eigenfaces with Extended Yale Face Database B. To evade computational and storage bottleneck, the face images are sampled down by a factor 4x4=16.These eigenfaces can now be used to represent both existing and new faces: we can project a new (mean-subtracted) image on the eigenfaces and thereby record how that new face differs from the mean face. The eigenvalues associated with each eigenface represent how much the images in the training set vary from the mean image in that direction. We lose information by projecting the image on a subset of the eigenvectors, but we minimize this loss by keeping those eigenfaces with the largest eigenvalues. For instance, if we are working with a 100 x 100 image, then we will obtain 10,000 eigenvectors. In practical applications, most faces can typically be identified using a projection on between 100 and 150 eigenfaces, so that most of the 10,000 eigenvectors can be discarded.To create a set of eigenfaces, one must:The technique used in creating eigenfaces and using them for recognition is also used outside of face recognition. This technique is also used for Handwriting recognition, lip reading, voice recognition, sign language/hand gestures interpretation and medical imaging analysis. Therefore, some do not use the term eigenface, but prefer to use 'eigenimage'.The eigenfaces that are created will appear as light and dark areas that are arranged in a specific pattern. This pattern is how different features of a face are singled out to be evaluated and scored. There will be a pattern to evaluate symmetry, if there is any style of facial hair, where the hairline is, or evaluate the size of the nose or mouth. Other eigenfaces have patterns that are less simple to identify, and the image of the eigenface may look very little like a face.A set of eigenfaces can be generated by performing a mathematical process called principal component analysis (PCA) on a large set of images depicting different human faces. Informally, eigenfaces can be considered a set of \"standardized face ingredients\", derived from statistical analysis of many pictures of faces. Any human face can be considered to be a combination of these standard faces. For example, one's face might be composed of the average face plus 10% from eigenface 1, 55% from eigenface 2, and even -3% from eigenface 3. Remarkably, it does not take many eigenfaces combined together to achieve a fair approximation of most faces. Also, because a person's face is not recorded by a digital photograph, but instead as just a list of values (one value for each eigenface in the database used), much less space is taken for each person's face.Once established, the eigenface method was expanded to include methods of preprocessing to improve accuracy.[4] Multiple manifold approaches were also used to build sets of eigenfaces for different subjects[5][6] and different features, such as the eyes.[7]In 1991 M. Turk and A. Pentland expanded these results and presented the Eigenface method of face recognition.[3] In addition to designing a system for automated face recognition using eigenfaces, they showed a way of calculating the eigenvectors of a covariance matrix in such a way as to make it possible for computers at that time to perform eigen-decomposition on a large number of face images. Face images usually occupy a high-dimensional space and conventional principal component analysis was intractable on such data sets. Turk and Pentland's paper demonstrated ways to extract the eigenvectors based on matrices sized by the number of images rather than the number of pixels.The Eigenface approach began with a search for a low-dimensional representation of face images. Sirovich and Kirby (1987) showed that principal component analysis could be used on a collection of face images to form a set of basis features. These basis images, known as Eigenpictures, could be linearly combined to reconstruct images in the original training set. If the training set consists of M images, principal component analysis could form a basis set of N images, where N < M. The reconstruction error is reduced by increasing the number of eigenpictures, however the number needed is always chosen less than M. For example, if you need to generate a number of N eigenfaces for a training set of M face images, you can say that each face image can be made up of \"proportions\" of all this K \"features\" or eigenfaces\u00a0: Face image1 = (23% of E1) + (2% of E2) + (51% of E3) + ... + (1% En).Eigenfaces is the name given to a set of eigenvectors when they are used in the computer vision problem of human face recognition.[1] The approach of using eigenfaces for recognition was developed by Sirovich and Kirby (1987) and used by Matthew Turk and Alex Pentland in face classification.[2] The eigenvectors are derived from the covariance matrix of the probability distribution over the high-dimensional vector space of face images. The eigenfaces themselves form a basis set of all images used to construct the covariance matrix. This produces dimension reduction by allowing the smaller set of basis images to represent the original training images. Classification can be achieved by comparing how faces are represented by the basis set.",
            "title": "Eigenface",
            "url": "https://en.wikipedia.org/wiki/Eigenface"
        },
        {
            "desc_links": [
                "/wiki/Canonical_correlation",
                "/wiki/Orthogonal_coordinate_system",
                "/wiki/Factor_analysis",
                "/wiki/Eigenvectors",
                "/wiki/Dimension_(metadata)",
                "/wiki/Exploratory_data_analysis",
                "/wiki/Predictive_modeling",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Covariance",
                "/wiki/Correlation",
                "/wiki/Singular_value_decomposition",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Z-score",
                "/wiki/Karl_Pearson",
                "/wiki/Principal_axis_theorem",
                "/wiki/Harold_Hotelling",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Signal_processing",
                "/wiki/Harold_Hotelling",
                "/wiki/Proper_orthogonal_decomposition",
                "/wiki/Singular_value_decomposition",
                "/wiki/Eigendecomposition",
                "/wiki/Factor_analysis",
                "/wiki/Eckart%E2%80%93Young_theorem",
                "/wiki/Psychometrics",
                "/wiki/Empirical_orthogonal_functions",
                "/wiki/Spectral_theorem",
                "/wiki/Mode_shape",
                "/wiki/Orthogonal_transformation",
                "/wiki/Correlation_and_dependence",
                "/wiki/Variance",
                "/wiki/Orthogonal",
                "/wiki/Orthogonal_basis_set"
            ],
            "links": [
                "/wiki/Independent_component_analysis",
                "/wiki/Sparse_PCA",
                "/wiki/Robust_principal_component_analysis",
                "/wiki/Outlier",
                "/wiki/Data_mining",
                "/wiki/Correlation_clustering",
                "/wiki/Tucker_decomposition",
                "/wiki/PARAFAC",
                "/wiki/Multilinear_subspace_learning",
                "/wiki/Multilinear_principal_component_analysis",
                "/wiki/Nonlinear_dimensionality_reduction",
                "/wiki/Curve",
                "/wiki/Manifold",
                "/wiki/Approximation",
                "/wiki/Projection_(mathematics)",
                "/wiki/Elastic_map",
                "/wiki/Principal_geodesic_analysis",
                "/wiki/Kernel_PCA",
                "/wiki/Non-negative_matrix_factorization",
                "/wiki/K-means_clustering",
                "/wiki/Factor_analysis",
                "/wiki/Correspondence_analysis",
                "/wiki/Jean-Paul_Benz%C3%A9cri",
                "/wiki/Contingency_tables",
                "/wiki/Chi-squared_statistic",
                "/wiki/Detrended_correspondence_analysis",
                "/wiki/Canonical_correspondence_analysis",
                "/wiki/Multiple_correspondence_analysis",
                "/wiki/Order_parameters",
                "/wiki/Phase_transitions",
                "/wiki/Spike_sorting",
                "/wiki/Electrophysiology#Extracellular_recording",
                "/wiki/Cluster_analysis",
                "/wiki/Neuroscience",
                "/wiki/Neuron",
                "/wiki/Action_potential",
                "/wiki/Spike-triggered_covariance",
                "/wiki/White_noise",
                "/wiki/Electric_current",
                "/wiki/Covariance_matrix",
                "/wiki/Eigenvectors_and_eigenvalues",
                "/wiki/Vector_space",
                "/wiki/Stock",
                "/wiki/Asset_allocation",
                "/wiki/Stock_selection_criterion",
                "/wiki/Quantitative_finance",
                "/wiki/Risk_management",
                "/wiki/Interest_rate_derivative",
                "/wiki/Swap_(finance)",
                "/wiki/Ludovic_Lebart",
                "/wiki/Algorithm",
                "/wiki/Partial_least_squares",
                "/wiki/Genomics",
                "/wiki/Metabolomics",
                "/wiki/Non-linear_iterative_partial_least_squares",
                "/wiki/Gram%E2%80%93Schmidt",
                "/wiki/Lanczos_algorithm",
                "/wiki/Power_iteration",
                "/wiki/High_dimensional_data",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Non-negative_matrix_factorization",
                "/wiki/Principal_component_analysis#Relation_between_PCA_and_Non-negative_Matrix_Factorization",
                "/wiki/Diagonal",
                "/wiki/Regression_analysis",
                "/wiki/Pattern_recognition",
                "/wiki/Linear_discriminant_analysis",
                "/wiki/Autoencoder",
                "/wiki/Artificial_neural_network",
                "/wiki/Orthogonal",
                "/wiki/Minimum_mean_square_error",
                "/wiki/Sample_variance",
                "/wiki/Euclidean_space",
                "/wiki/Eigenvalue",
                "/wiki/Principle_Component_Analysis#PCA_and_information_theory",
                "/wiki/Dimensionality_reduction",
                "/wiki/Discrete_cosine_transform",
                "/wiki/Nonlinear_dimensionality_reduction",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Frobenius_norm",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Polar_decomposition",
                "/wiki/Diagonal_matrix",
                "/wiki/Singular_value_decomposition",
                "/wiki/Signal-to-noise_ratio",
                "/wiki/Regression_analysis",
                "/wiki/Explanatory_variable",
                "/wiki/Overfitting",
                "/wiki/Principal_component_regression",
                "/wiki/Dimensionality_reduction",
                "/wiki/Cluster_analysis",
                "/wiki/Covariance_matrix",
                "/wiki/Whitening_transformation",
                "/wiki/Rayleigh_quotient",
                "/wiki/Positive_semidefinite_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Eigenvector",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Empirical_mean",
                "/wiki/Orthogonal_transformation",
                "/wiki/Linear_transformation",
                "/wiki/Coordinate_system",
                "/wiki/Covariance_matrix",
                "/wiki/Ellipsoid",
                "/wiki/Canonical_correlation",
                "/wiki/Orthogonal_coordinate_system",
                "/wiki/Factor_analysis",
                "/wiki/Eigenvectors",
                "/wiki/Dimension_(metadata)",
                "/wiki/Exploratory_data_analysis",
                "/wiki/Predictive_modeling",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Covariance",
                "/wiki/Correlation",
                "/wiki/Singular_value_decomposition",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Z-score",
                "/wiki/Karl_Pearson",
                "/wiki/Principal_axis_theorem",
                "/wiki/Harold_Hotelling",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Signal_processing",
                "/wiki/Harold_Hotelling",
                "/wiki/Proper_orthogonal_decomposition",
                "/wiki/Singular_value_decomposition",
                "/wiki/Eigendecomposition",
                "/wiki/Factor_analysis",
                "/wiki/Eckart%E2%80%93Young_theorem",
                "/wiki/Psychometrics",
                "/wiki/Empirical_orthogonal_functions",
                "/wiki/Spectral_theorem",
                "/wiki/Mode_shape"
            ],
            "text": "then the decomposition is unique up to multiplication by a scalar.[60]Independent component analysis (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.A particular disadvantage of PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables.Robust principal component analysis (RPCA) via decomposition in low-rank and sparse matrices is a modification of the widely used statistical procedure principal component analysis (PCA) which works well with respect to grossly corrupted observations.[56][57][58][59]While PCA finds the mathematically optimal method (as in minimizing the squared error), it is sensitive to outliers in the data that produce large errors PCA tries to avoid. It therefore is common practice to remove outliers before computing PCA. However, in some contexts, outliers can be difficult to identify. For example, in data mining algorithms like correlation clustering, the assignment of points to clusters and outliers is not known beforehand. A recently proposed generalization of PCA[55] based on a weighted PCA increases robustness by assigning different weights to data objects based on their estimated relevancy.N-way principal component analysis may be performed with models such as Tucker decomposition, PARAFAC, multiple factor analysis, co-inertia analysis, STATIS, and DISTATIS.In multilinear subspace learning,[54] PCA is generalized to multilinear PCA (MPCA) that extracts features directly from tensor representations. MPCA is solved by performing PCA in each mode of the tensor iteratively. MPCA has been applied to face recognition, gait recognition, etc. MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.Most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in PCA or K-means. Pearson's original idea was to take a straight line (or plane) which will be \"the best fit\" to a set of data points. Principal curves and manifolds[53] give the natural geometric framework for PCA generalization and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold for data approximation, and by encoding using standard geometric projection onto the manifold, as it is illustrated by Fig. See also the elastic map algorithm and principal geodesic analysis. Another popular generalization is kernel PCA, which corresponds to PCA performed in a reproducing kernel Hilbert space associated with a positive definite kernel.Non-negative matrix factorization (NMF) is a dimension reduction method where only non-negative elements in the matrices are used, which is therefore a promising method in astronomy,[17][18][19] in the sense that astrophysical signals are non-negative. The PCA components are orthogonal to each other, while the NMF components are all non-negative and therefore constructs a non-orthogonal basis.It was asserted in [46][47] that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by the principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. However, that PCA is a useful relaxation of k-means clustering was not a new result (see, for example,[48]), and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.[49]Factor analysis is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Different from PCA, factor analysis is a correlation-focused approach seeking to reproduce the inter-correlations among variables, in which the factors \"represent the common variance of variables, excluding unique variance\".[44] In terms of the correlation matrix, this corresponds with focusing on explaining the off-diagonal terms (i.e. shared co-variance), while PCA focuses on explaining the terms that sit on the diagonal. However, as a side result, when trying to reproduce the on-diagonal terms, PCA also tends to fit relatively well the off-diagonal correlations.[45] Results given by PCA and factor analysis are very similar in most situations, but this is not always the case, and there are some problems where the results are significantly different. Factor analysis is generally used when the research purpose is detecting data structure (i.e., latent constructs or factors) or causal modeling.Principal component analysis creates variables that are linear combinations of the original variables. The new variables have the property that the variables are all orthogonal. The PCA transformation can be helpful as a pre-processing step before clustering. PCA is a variance-focused approach seeking to reproduce the total variable variance, in which components reflect both common and unique variance of the variable. PCA is generally preferred for purposes of data reduction (i.e., translating variable space into optimal factor space) but not when the goal is to detect the latent construct or factors.Correspondence analysis (CA) was developed by Jean-Paul Benz\u00e9cri[41] and is conceptually similar to PCA, but scales the data (which should be non-negative) so that rows and columns are treated equivalently. It is traditionally applied to contingency tables. CA decomposes the chi-squared statistic associated to this table into orthogonal factors.[42] Because CA is a descriptive technique, it can be applied to tables for which the chi-squared statistic is appropriate or not. Several variants of CA are available including detrended correspondence analysis and canonical correspondence analysis. One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data.[43]PCA as a dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles. It has been used in determining collective variables, i.e. order parameters, during phase transitions in the brain.[40]In neuroscience, PCA is also used to discern the identity of a neuron from the shape of its action potential. Spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron. In spike sorting, one first uses PCA to reduce the dimensionality of the space of action potential waveforms, and then performs clustering analysis to associate specific action potentials with individual neurons.A variant of principal components analysis is used in neuroscience to identify the specific properties of a stimulus that increase a neuron's probability of generating an action potential.[39] This technique is known as spike-triggered covariance analysis. In a typical application an experimenter presents a white noise process as a stimulus (usually either as a sensory input to a test subject, or as a current injected directly into the neuron) and records a train of action potentials, or spikes, produced by the neuron as a result. Presumably, certain features of the stimulus make the neuron more likely to spike. In order to extract these features, the experimenter calculates the covariance matrix of the spike-triggered ensemble, the set of all stimuli (defined and discretized over a finite time window, typically on the order of 100 ms) that immediately preceded a spike. The eigenvectors of the difference between the spike-triggered covariance matrix and the covariance matrix of the prior stimulus ensemble (the set of all stimuli, defined over the same length time window) then indicate the directions in the space of stimuli along which the variance of the spike-triggered ensemble differed the most from that of the prior stimulus ensemble. Specifically, the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike-triggered ensemble showed the largest positive change compared to the variance of the prior. Since these were the directions in which varying the stimulus led to a spike, they are often good approximations of the sought after relevant stimulus features.PCA has also been applied to share portfolios in a similar fashion.[36] One application is to reduce portfolio risk, where allocation strategies are applied to the \"principal portfolios\" instead of the underlying stocks.[37] A second is to enhance portfolio return, using the principal components to select stocks with upside potential.[38]In quantitative finance, principal component analysis can be directly applied to the risk management of interest rate derivatives portfolios.[35] Trading multiple swap instruments which are usually a function of 30-500 other market quotable swap instruments is sought to be reduced to usually 3 or 4 principal components, representing the path of interest rates on a macro basis. Converting risks to be represented as those to factor loadings (or multipliers) provides assessments and understanding beyond that available to simply collectively viewing risks to individual 30-500 buckets.These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, L\u00ea & Pag\u00e8s 2009 and Pag\u00e8s 2013. Few software offer this option in an \"automatic\" way. This is the case of SPAD that historically, following the work of Ludovic Lebart, was the first to propose this option, and the R package FactoMineR.In PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable species. For this, the following results are produced.In an \"online\" or \"streaming\" situation with data arriving piece by piece rather than being stored in a single batch, it is useful to make an estimate of the PCA projection that can be updated sequentially. This can be done efficiently, but requires different algorithms.[34]However, for large data matrices, or matrices that have a high degree of column collinearity, NIPALS suffers from loss of orthogonality due to machine precision limitations accumulated in each iteration step.[32] A Gram\u2013Schmidt (GS) re-orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality.[33]Non-linear iterative partial least squares (NIPALS) is an algorithm for computing the first few components in a principal component or partial least squares analysis. For very-high-dimensional datasets, such as those generated in the *omics sciences (e.g., genomics, metabolomics) it is usually only necessary to compute the first few PCs. The non-linear iterative partial least squares (NIPALS) algorithm calculates t1 and w1T from X. The outer product, t1w1T can then be subtracted from X leaving the residual matrix E1. This can be then used to calculate subsequent PCs.[31] This results in a dramatic reduction in computational time since calculation of the covariance matrix is avoided.One way to compute the eigenvalue that corresponds with each principal component is to measure the difference in mean-squared-distance between the rows and the centroid, before and after subtracting out the principal component. The eigenvalue that corresponds with the component that was removed is equal to this difference.Subsequent principal components can be computed by subtracting component r from X (see Gram\u2013Schmidt) and then repeating this algorithm to find the next principal component. However this simple approach is not numerically stable if more than a small number of principal components are required, because imprecisions in the calculations will additively affect the estimates of subsequent principal components. More advanced methods build on this basic idea, as with the closely related Lanczos algorithm.This algorithm is simply an efficient way of calculating XTX r, normalizing, and placing the result back in r (power iteration). It avoids the np2 operations of calculating the covariance matrix. r will typically get close to the first principal component of X within a small number of iterations, c. (The magnitude of s will be larger after each iteration. Convergence can be detected when it increases by an amount too small for the precision of the machine.)In practical implementations especially with high dimensional data (large p), the covariance method is rarely used because it is not efficient. One way to compute the first principal component efficiently[30] is shown in the following pseudo-code, for a data matrix X with zero mean, without ever computing its covariance matrix.This is very constructive, as cov(X) is guaranteed to be a non-negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix.Let X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data.[25] Hence we proceed by centering the data as follows:The goal is to transform a given data set X of dimension p to an alternative data set Y of smaller dimension L. Equivalently, we are seeking to find the matrix Y, where Y is the Karhunen\u2013Lo\u00e8ve transform (KLT) of matrix X:The following is a detailed description of PCA using the covariance method (see also here) as opposed to the correlation method.[24]Under the assumption thatDimensionality reduction loses information, in general. PCA-based dimensionality reduction tends to minimize that information loss, under certain signal and noise models.The other limitation is the mean-removal process before constructing the covariance matrix for PCA. In fields such as astronomy, all the signals are non-negative, and the mean-removal process will force the mean of some astrophysical exposures to be zero, which consequently creates unphysical negative fluxes,[15] and forward modeling has to be performed to recover the true magnitude of the signals.[16] As an alternative method, non-negative matrix factorization focusing only on the non-negative elements in the matrices, which is well-suited for astrophysical observations.[17][18][19] See more at Relation between PCA and Non-negative Matrix Factorization.The applicability of PCA is limited by certain assumptions[14] made in its derivation.As noted above, the results of PCA depend on the scaling of the variables. A scale-invariant form of PCA has been developed.[13]Before we look at its usage, we first look at diagonal elements,The statistical implication of this property is that the last few PCs are not simply unstructured left-overs after removing the important PCs. Because these last PCs have variances as small as possible they are useful in their own right. They can help to detect unsuspected near-constant linear relationships between the elements of x, and they may also be useful in regression, in selecting a subset of variables from x, and in outlier detection.Some properties of PCA include:[12]PCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability.[10] However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes.[11] The linear discriminant analysis is an alternative which is optimized for class separability.An autoencoder neural network with a linear hidden layer is similar to PCA. Upon convergence, the weight vectors of the K neurons in the hidden layer will form a basis for the space spanned by the first K principal components. Unlike PCA, this technique will not necessarily produce orthogonal vectors.Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation). Also see the article by Kromrey & Foster-Johnson (1998) on \"Mean-centering in Moderated Regression: Much Ado About Nothing\".Mean subtraction (a.k.a. \"mean centering\") is necessary for performing PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.[9]PCA is sensitive to the scaling of the variables. If we have just two variables and they have the same sample variance and are positively correlated, then the PCA will entail a rotation by 45\u00b0 and the \"loadings\" for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Note that Pearson's original paper was entitled \"On Lines and Planes of Closest Fit to Systems of Points in Space\" \u2013 \"in space\" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.Given a set of points in Euclidean space, the first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. The second principal component corresponds to the same concept after all correlation with the first principal component has been subtracted from the points. The singular values (in \u03a3) are the square roots of the eigenvalues of the matrix XTX. Each eigenvalue is proportional to the portion of the \"variance\" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see below). PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest \"variance\" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example and when applicable, to the discrete cosine transform, and in particular to the DCT-II which is simply known as the \"DCT\". Nonlinear dimensionality reduction techniques tend to be more computationally demanding than PCA.The truncation of a matrix M or T using a truncated singular value decomposition in this way produces a truncated matrix that is the nearest possible matrix of rank L to the original matrix, in the sense of the difference between the two having the smallest possible Frobenius norm, a result known as the Eckart\u2013Young theorem [1936].As with the eigen-decomposition, a truncated n \u00d7 L score matrix TL can be obtained by considering only the first L largest singular values and their singular vectors:Efficient algorithms exist to calculate the SVD of X without having to form the matrix XTX, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix[citation needed], unless only a handful of components are required.so each column of T is given by one of the left singular vectors of X multiplied by the corresponding singular value. This form is also the polar decomposition of T.Using the singular value decomposition the score matrix T can be writtenIn terms of this factorization, the matrix XTX can be writtenHere \u03a3 is an n-by-p rectangular diagonal matrix of positive numbers \u03c3(k), called the singular values of X; U is an n-by-n matrix, the columns of which are orthogonal unit vectors of length n called the left singular vectors of X; and W is a p-by-p whose columns are orthogonal unit vectors of length p and called the right singular vectors of X.The principal components transformation can also be associated with another matrix factorization, the singular value decomposition (SVD) of X,Dimensionality reduction may also be appropriate when the variables in a dataset are noisy. If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of T will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix W, which can be thought of as a high-dimensional rotation of the co-ordinate axes). However, with more of the total variance concentrated in the first few principal components compared to the same noise variance, the proportionate effect of the noise is less\u2014the first few components achieve a higher signal-to-noise ratio. PCA thus can have the effect of concentrating much of the signal into the first few principal components, which can usefully be captured by dimensionality reduction; while the later principal components may be dominated by noise, and so disposed of without great loss.Similarly, in regression analysis, the larger the number of explanatory variables allowed, the greater is the chance of overfitting the model, producing conclusions that fail to generalise to other datasets. One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called principal component regression.Such dimensionality reduction can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible. For example, selecting L\u00a0=\u00a02 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains clusters these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable.The transformation T = X W maps a data vector x(i) from an original space of p variables to a new space of p variables which are uncorrelated over the dataset. However, not all the principal components need to be kept. Keeping only the first L principal components, produced by using only the first L loading vectors, gives the truncated transformation(\u03bb(k) being equal to the sum of the squares over the dataset associated with each component k: \u03bb(k) = \u03a3i tk2(i) = \u03a3i (x(i) \u22c5 w(k))2)where \u039b is the diagonal matrix of eigenvalues \u03bb(k) of XTXThe empirical covariance matrix between the principal components becomesIn matrix form, the empirical covariance matrix for the original variables can be writtenAnother way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.where the eigenvalue property of w(k) has been used to move from line 2 to line 3. However eigenvectors w(j) and w(k) corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value). The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset.The sample covariance Q between two of the different principal components over the dataset is given by:XTX itself can be recognised as proportional to the empirical sample covariance matrix of the dataset X.where W is a p-by-p matrix whose columns are the eigenvectors of XTX. The transpose of W is sometimes called the whitening or sphering transformation.The full principal components decomposition of X can therefore be given asThe kth principal component of a data vector x(i) can therefore be given as a score tk(i) = x(i) \u22c5 w(k) in the transformed co-ordinates, or as the corresponding vector in the space of the original variables, {x(i) \u22c5 w(k)} w(k), where w(k) is the kth eigenvector of XTX.It turns out that this gives the remaining eigenvectors of XTX, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the loading vectors are eigenvectors of XTX.and then finding the loading vector which extracts the maximum variance from this new data matrixThe kth component can be found by subtracting the first k\u00a0\u2212\u00a01 principal components from X:With w(1) found, the first principal component of a data vector x(i) can then be given as a score t1(i) = x(i) \u22c5 w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables, {x(i) \u22c5 w(1)} w(1).The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector.Since w(1) has been defined to be a unit vector, it equivalently also satisfiesEquivalently, writing this in matrix form givesIn order to maximize variance, the first loading vector w(1) thus has to satisfyConsider a data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor).PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.[3]This procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data, and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis . The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.PCA can be thought of as fitting an n-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.[6][7]PCA is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.PCA is mostly used as a tool in exploratory data analysis and for making predictive models. It's often used to visualize genetic distance and relatedness between populations. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute.[4] The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score).[5]PCA was invented in 1901 by Karl Pearson,[1] as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s.[2] Depending on the field of application, it is also named the discrete Karhunen\u2013Lo\u00e8ve transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch.\u00a07 of Jolliffe's Principal Component Analysis[3]), Eckart\u2013Young theorem (Harman, 1960), or Schmidt\u2013Mirsky theorem in psychometrics, empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.",
            "title": "Principal component analysis",
            "url": "https://en.wikipedia.org/wiki/Principal_component_analysis"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Linear combination",
            "url": "https://en.wikipedia.org/wiki/Linear_combination"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/National_Institute_of_Informatics",
                "/wiki/Emotion_recognition",
                "/wiki/Facebook",
                "/wiki/Facebook",
                "/wiki/DeepFace",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/National_Telecommunications_and_Information_Administration",
                "/wiki/National_Telecommunications_and_Information_Administration",
                "/wiki/Electronic_Frontier_Foundation",
                "/wiki/ACLU",
                "/wiki/Privacy",
                "/wiki/Surveillance_society",
                "/wiki/Logan_International_Airport",
                "/wiki/Police",
                "/wiki/Tampa,_Florida",
                "/wiki/Florida",
                "/wiki/London_Borough_of_Newham",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Authentication",
                "/wiki/Android_Ice_Cream_Sandwich",
                "/wiki/Smartphone",
                "/wiki/Lock_screen",
                "/wiki/Microsoft",
                "/wiki/Xbox_360",
                "/wiki/Kinect",
                "/wiki/Windows_10",
                "/wiki/IPhone_X",
                "/wiki/Face_ID",
                "/wiki/Super_Bowl_XXXV",
                "/wiki/Tampa_Bay,_Florida",
                "/wiki/September_11_attacks",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Xinjiang",
                "/wiki/Ars_Technica",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Federal_Bureau_of_Investigation",
                "/wiki/Next_Generation_Identification",
                "/wiki/Death_of_Freddie_Gray",
                "/wiki/U.S._Department_of_State",
                "/wiki/Australian_Border_Force",
                "/wiki/SmartGate",
                "/wiki/Biometric_passport",
                "/wiki/Ottawa_International_Airport",
                "/wiki/Tocumen_International_Airport",
                "/wiki/Apple_Inc.",
                "/wiki/Face_ID",
                "/wiki/Central_processing_unit",
                "/wiki/DeepFace",
                "/wiki/Deep_learning"
            ],
            "text": "Another method to protect from facial recognition systems are specific haircuts and make-up patterns that prevent the used algorithms to detect a face.[89]In January 2013 Japanese researchers from the National Institute of Informatics created 'privacy visor' glasses that use nearly infrared light to make the face underneath it unrecognizable to face recognition software.[82] The latest version uses a titanium frame, light-reflective material and a mask which uses angles and patterns to disrupt facial recognition technology through both absorbing and bouncing back light sources.[83][84][85][86] In December 2016 a form of anti-CCTV and facial recognition sunglasses called 'reflectacles' were invented by a custom-spectacle-craftsman based in Chicago named Scott Urban.[87] They reflect infrared and, optionally, visible light which makes the users face a white blur to cameras.[88]Facial recognition systems have been used for emotion recognition[78][79] In 2016 Facebook acquired emotion detection startup FacioMetrics.[80][81]The lack of regulations holding facial recognition technology companies to requirements of racial biased testing can be a significant flaw in the adoption of use in law enforcement. CyberExtruder, a company that markets itself to law enforcement said that they had not performed testing or research on bias in their software. CyberExtruder did note that some skin colours are more difficult for the software to recognize with current limitations of the technology. \u201cJust as individuals with very dark skin are hard to identify with high significance via facial recognition, individuals with very pale skin are the same,\u201d said Blake Senftner, a senior software engineer at CyberExtruder.[77]It is believed that with such large margins of error in this technology, both legal advocates and facial recognition software companies say that the technology should only supply a portion of the case \u2013 not evidence that can lead to an arrest of an individual. [77]The software, which has taken an expanding role among law enforcement agencies in the US over the last several years, has been mired in controversy because of its effect on people of color. Experts fear that the new technology may actually be hurting the communities the police claims they are trying to protect.[76] It is considered an imperfect biometric, and in a study conducted by Georgetown University researcher Clare Garvie, she concluded that \"there\u2019s no consensus in the scientific community that it provides a positive identification of somebody.\u201d[77]Facial recognition technology has been proven to work less accurately on the people of colour.[74] One study by Joy Buolamwini (MIT Media Lab) and Timnit Gebru (Microsoft Research) found that the error rate for gender recognition for women of color within three commercial facial recognition systems ranged from 23.8% to 36%, whereas for lighter-skinned men it was between 0.0 and 1.6%. Overall accuracy rates for identifying men (91.9%) were higher than for women (79.4%), and none of the systems accommodated a non-binary understanding of gender.[75]All over the world, law enforcement agencies have begun using facial recognition software to aid in the identifying of criminals. For example, the Chinese police force were able to identify twenty-five wanted suspects using facial recognition equipment at the Qingdao International Beer Festival, one of which had been on the run for 10 years.[72] The equipment works by recording a 15 second video clip and taking multiple snapshots of the subject. That data is compared and analyzed with images from the police department\u2019s database and within 20 minutes, the subject can be identified with a 98.1% accuracy[73]In December 2017, Facebook rolled out a new feature that notifies a user when someone uploads a photo that includes what Facebook thinks is their face, even if they are not tagged. Facebook has attempted to frame the new functionality in a positive light, amidst prior backlashes. [70] Facebook\u2019s head of privacy, Rob Sherman, addressed this new feature as one that gives people more control over their photos online. \u201cWe\u2019ve thought about this as a really empowering feature,\u201d he says. \u201cThere may be photos that exist that you don\u2019t know about.\u201d [71]Social media web sites such as Facebook have very large numbers of photographs of people, annotated with names. This represents a database which may be abused by governments for face recognition purposes.[67] Facebook's DeepFace has become the subject of several class action lawsuits under the Biometric Information Privacy Act, with claims alleging that Facebook is collecting and storing face recognition data of its users without obtaining informed consent, in direct violation of the Biometric Information Privacy Act.[68] The most recent case was dismissed in January 2016 because the court lacked jurisdiction.[69] Therefore, it is still unclear if the Biometric Information Privacy Act will be effective in protecting biometric data privacy rights.The largest concern with the development of biometric technology, and more specifically facial recognition, has to do with privacy. The rise in facial recognition technologies have led people to be concerned that large companies, such as Google or Apple, or even Government agencies will be using it for mass surveillance of the public. Regardless of whether or not they have committed a crime, in general people do not wish to have their every action watched or track. People tend to believe that, since we live in a free society, we should be able to go out in public without the fear of being identified and surveilled. People worry that with the rising prevalence of facial recognition, they will begin to lose their anonymity.[citation needed]In July 2015, the United States Government Accountability Office conducted a Report to the Ranking Member, Subcommittee on Privacy, Technology and the Law, Committee on the Judiciary, U.S. Senate. The report discussed facial recognition technology's commercial uses, privacy issues, and the applicable federal law. It states that previously, issues concerning facial recognition technology were discussed and represent the need for updated federal privacy laws that continually match the degree and impact of advanced technologies. Also, that some industry, government, and privacy organizations are in the process of developing, or have developed, \"voluntary privacy guidelines\". These guidelines vary between the groups, but overall aim to gain consent and inform citizens of the intended use of facial recognition technology. This helps counteract the privacy issues that arise when citizens are unaware where their personal, privacy data gets put to use as the report indicates as a prevalent issue.[60]In 2014, the National Telecommunications and Information Association (NTIA) began a multi-stakeholder process to engage privacy advocates and industry representatives to establish guidelines regarding the use of face recognition technology by private companies.[64] In June 2015, privacy advocates left the bargaining table over what they felt was an impasse based on the industry representatives being unwilling to agree to consent requirements for the collection of face recognition data.[65] The NTIA and industry representatives continued without the privacy representatives, and draft rules are expected to be presented in the spring of 2016.[66]In July 2012, a hearing was held before the Subcommittee on Privacy, Technology and the Law of the Committee on the Judiciary, United States Senate, to address issues surrounding what face recognition technology means for privacy and civil liberties.[63]Face recognition was used in Russia to harass women allegedly involved in online pornography.[61] In Russia there is an app 'FindFace' which can identify faces with about 70% accuracy using the social media app called VK. This app would not be possible in other countries which do not use VK as their social media platform photos are not stored the same way as with VK.[62]Face recognition can be used not just to identify an individual, but also to unearth other personal data associated with an individual \u2013 such as other photos featuring the individual, blog posts, social networking profiles, Internet behavior, travel patterns, etc. \u2013 all through facial features alone.[59] Concerns have been raised over who would have access to the knowledge of one's whereabouts and people with them at any given time.[60] Moreover, individuals have limited ability to avoid or thwart face recognition tracking unless they hide their faces. This fundamentally changes the dynamic of day-to-day privacy by enabling any marketer, government agency, or random stranger to secretly collect the identities and associated personal information of any individual captured by the face recognition system.[59] Consumers may not understand or be aware of what their data is being used for, which denies them the ability to consent to how their personal information gets shared.[60]Civil rights right organizations and privacy campaigners such as the Electronic Frontier Foundation[56] and the ACLU[57] express concern that privacy is being compromised by the use of surveillance technologies. Some fear that it could lead to a \u201ctotal surveillance society,\u201d with the government and other authorities having the ability to know the whereabouts and activities of all citizens around the clock. This knowledge has been, is being, and could continue to be deployed to prevent the lawful exercise of rights of citizens to criticize those in office, specific government policies or corporate practices. Many centralized power structures with such surveillance capabilities have abused their privileged access to maintain control of the political and economic apparatus, and to curtail populist reforms.[58]Systems are often advertised as having accuracy near 100%, this is misleading as the studies often uses much smaller sample sizes than would be necessary for large scale applications. Because facial recognition is not completely accurate, it creates a list of potential matches. A human operator must then look through these potential matches and studies show the operators pick the correct match out of the list only about half the time. This causes the issue of targeting the wrong suspect.[29][55]In 2014, Facebook stated that in a standardized two-option facial recognition test, its online system scored 97.25% accuracy, compared to the human benchmark of 97.5%.[54]A system at Boston's Logan Airport was shut down in 2003 after failing to make any matches during a two-year test period.[53]An experiment in 2002 by the local police department in Tampa, Florida, had similarly disappointing results.[36]Critics of the technology complain that the London Borough of Newham scheme has, as of 2004[update], never recognized a single criminal, despite several criminals in the system's database living in the Borough and the system having been running for several years. \"Not once, as far as the police know, has Newham's automatic face recognition system spotted a live target.\"[36][51] This information seems to conflict with claims that the system was credited with a 34% reduction in crime (hence why it was rolled out to Birmingham also).[52] However it can be explained by the notion that when the public is regularly told that they are under constant video surveillance with advanced face recognition technology, this fear alone can reduce the crime rate, whether the face recognition system technically works or does not. This has been the basis for several other face recognition based security systems, where the technology itself does not work particularly well but the user's perception of the technology does.Data privacy is the main concern when it comes to storing biometrics data in companies. Data stores about face or biometrics can be accessed by third party if not stored properly or hacked. In the Techworld, Parris adds (2017), \u201cHackers will already be looking to replicate people's faces to trick facial recognition systems, but the technology has proved harder to hack than fingerprint or voice recognition technology in the past.\u201d Facial recognition is used as added security in different websites, phone application and payment method, but question on researcher\u2019s mind is,Is facial recognition method is safe on itself?There is also inconstancy in the datasets used by researchers. Researchers may use anywhere from several subjects to scores of subjects, and a few hundred images to thousands of images. It is important for researchers to make available the datasets they used to each other, or have at least a standard dataset.[50]Face recognition is less effective if facial expressions vary. A big smile can render the system less effective. For instance: Canada, in 2009, allowed only neutral facial expressions in passport photos.[49]Ralph Gross, a researcher at the Carnegie Mellon Robotics Institute in 2008, describes one obstacle related to the viewing angle of the face: \"Face recognition has been getting pretty good at full frontal faces and 20 degrees off, but as soon as you go towards profile, there've been problems.\"[4] Besides the pose variations, low-resolution face images are also very hard to recognize. This is one of the main obstacles of face recognition in surveillance systems.[48]However, as compared to other biometric techniques, face recognition may not be most reliable and efficient. Quality measures are very important in facial recognition systems as large degrees of variations are possible in face images. Factors such as illumination, expression, pose and noise during face capture can affect the performance of facial recognition systems.[47] Among all biometric systems, facial recognition has the highest false acceptance and rejection rates,[47] thus questions have been raised on the effectiveness of face recognition software in cases of railway and airport security.[citation needed]One key advantage of a facial recognition system that it is able to person mass identification as it does not require the cooperation of the test subject to work. Properly designed systems installed in airports, multiplexes, and other public places can identify individuals among the crowd, without passers-by even being aware of the system. [47]Face recognition systems have also been used by photo management software to identify the subjects of photographs, enabling features such as searching images by person, as well as suggesting photos to be shared with a specific contact if their presence were detected in a photo.[45][46]Face recognition has been leveraged as a form of biometric authentication for various computing platforms and devices;[9] Android 4.0 \"Ice Cream Sandwich\" added facial recognition using a smartphone's front camera as a means of unlocking devices,[40][41] while Microsoft introduced face recognition login to its Xbox 360 video game console through its Kinect accessory,[42] as well as Windows 10 via its \"Windows Hello\" platform (which requires an infrared-illuminated camera).[43] Apple's iPhone X smartphone introduced facial recognition to the product line with its \"Face ID\" platform, which uses an infrared illumination system.[44]In the 2000 Mexican presidential election, the Mexican government employed face recognition software to prevent voter fraud. Some individuals had been registering to vote under several different names, in an attempt to place multiple votes. By comparing new face images to those already in the voter database, authorities were able to reduce duplicate registrations.[37] Similar technologies are being used in the United States to prevent people from obtaining fake identification cards and driver\u2019s licenses.[38][39]At Super Bowl XXXV in January 2001, police in Tampa Bay, Florida used Viisage face recognition software to search for potential criminals and terrorists in attendance at the event. 19 people with minor criminal records were potentially identified.[35][36]In addition to being used for security systems, authorities have found a number of other applications for face recognition systems. While earlier post-9/11 deployments were well publicized trials, more recent deployments are rarely written about due to their covert nature.[citation needed]As of late 2017, China has deployed facial recognition technology in Xinjiang. Reporters visiting the region found surveillance cameras installed every hundred meters or so in several cities, as well as facial recognition checkpoints at areas like gas stations, shopping centers, and mosque entrances.[33][34]In May 2017, a man was arrested using an automatic facial recognition (AFR) system mounted on a van operated by the South Wales Police. Ars Technica reported that \"this appears to be the first time [AFR] has led to an arrest\".[32]In 2017, Time & Attendance company ClockedIn released facial recognition as a form of attendance tracking for businesses and organisations looking to have a more automated system of keeping track of hours worked as well as for security and health and safety control.[citation needed]The FBI has also instituted its Next Generation Identification program to include face recognition, as well as more traditional biometrics like fingerprints and iris scans, which can pull from both criminal and civil databases.[31]In recent years Maryland has used face recognition by comparing people's faces to their driver's license photos. The system drew controversy when it was used in Baltimore to arrest unruly protesters after the death of Freddie Gray in police custody.[30] Many other states are using or developing a similar system however some states have laws prohibiting its use.The U.S. Department of State operates one of the largest face recognition systems in the world with a database of 117 million American adults, with photos typically drawn from driver's license photos.[28] Although it is still far from completion, it is being put to use in certain cities to give clues as to who was in the photo. The FBI uses the photos as an investigative tool not for positive identification.[29]The Australian Border Force and New Zealand Customs Services have set up an automated border processing system called SmartGate that uses face recognition, which compares the face of the traveller with the data in the e-passport microchip.[25] Major Canadian airports will be using a new facial recognition program as part of the Primary Inspection Kiosk program that will compare people's faces to their passports. This program will first come to Ottawa International Airport in early 2017 and to other airports in 2018.[26] The Tocumen International Airport in Panama operates an airport-wide surveillance system using hundreds of live face recognition cameras to identify wanted individuals passing through the airport.[27]It also works in the dark. This is done by using a \"Flood Illuminator\", which is a dedicated infrared flash that throws out invisible infrared light onto the user's face to properly read the 30,000 facial points.[24]The technology learns from changes in a user's appearance, and therefore works with hats, scarves, glasses and many sunglasses, beard and makeup.[23]Apple introduced Face ID on the flagship iPhone X as a biometric authentication successor to the Touch ID, a fingerprint based system. Face ID has a facial recognition sensor that consists of two parts: a \"Romeo\" module that projects more than 30,000 infrared dots onto the user's face, and a \"Juliet\" module that reads the pattern.[21] The pattern is sent to a local \"Secure Enclave\" in the device's central processing unit (CPU) to confirm a match with the phone owner's face.[22] The facial pattern is not accessible by Apple. The system will not work with eyes closed, in an effort to prevent unauthorized access.[22]DeepFace is a deep learning facial recognition system created by a research group at Facebook. It identifies human faces in digital images. It employs a nine-layer neural net with over 120 million connection weights, and was trained on four million images uploaded by Facebook users.[18][19] The system is said to be 97% accurate, compared to 85% for the FBI's Next Generation Identification system.[20] One of the creators of the software, Yaniv Taigman, came to Facebook via their 2007 acquisition of Face.com.",
            "title": "Facial recognition system",
            "url": "https://en.wikipedia.org/wiki/Facial_recognition_system"
        },
        {
            "desc_links": [
                "/wiki/Driver%27s_license",
                "/wiki/Passport",
                "/wiki/Password",
                "/wiki/Personal_identification_number",
                "/wiki/Fingerprint",
                "/wiki/Facial_recognition_system",
                "/wiki/DNA",
                "/wiki/Palm_print",
                "/wiki/Hand_geometry",
                "/wiki/Iris_recognition",
                "/wiki/Retinal_scan",
                "/wiki/Keystroke_dynamics",
                "/wiki/Gait_analysis",
                "/wiki/Speaker_recognition",
                "/wiki/Access_control",
                "/wiki/Surveillance"
            ],
            "links": [
                "/wiki/National_identification_number",
                "/wiki/Aadhaar",
                "/wiki/Biometric",
                "/wiki/Fingerprint",
                "/wiki/Iris_scan",
                "/wiki/Demographic_data",
                "/wiki/Mobile_telephone_numbering_in_India",
                "/wiki/Biometric_voter_registration",
                "/wiki/International_Institute_for_Democracy_and_Electoral_Assistance",
                "/wiki/Armenia",
                "/wiki/Angola",
                "/wiki/Bangladesh",
                "/wiki/Bhutan",
                "/wiki/Bolivia",
                "/wiki/Brazil",
                "/wiki/Burkina_Faso",
                "/wiki/Cambodia",
                "/wiki/Cameroon",
                "/wiki/Chad",
                "/wiki/Colombia",
                "/wiki/Comoros",
                "/wiki/Democratic_Republic_of_the_Congo",
                "/wiki/Costa_Rica",
                "/wiki/Ivory_Coast",
                "/wiki/Dominican_Republic",
                "/wiki/Fiji",
                "/wiki/Gambia",
                "/wiki/Ghana",
                "/wiki/Guatemala",
                "/wiki/India",
                "/wiki/Iraq",
                "/wiki/Kenya",
                "/wiki/Lesotho",
                "/wiki/Liberia",
                "/wiki/Malawi",
                "/wiki/Mali",
                "/wiki/Mauritania",
                "/wiki/Mexico",
                "/wiki/Morocco",
                "/wiki/Mozambique",
                "/wiki/Namibia",
                "/wiki/Nepal",
                "/wiki/Nicaragua",
                "/wiki/Nigeria",
                "/wiki/Panama",
                "/wiki/Peru",
                "/wiki/Philippines",
                "/wiki/Senegal",
                "/wiki/Sierra_Leone",
                "/wiki/Solomon_Islands",
                "/wiki/Somaliland",
                "/wiki/Swaziland",
                "/wiki/Tanzania",
                "/wiki/Uganda",
                "/wiki/Uruguay",
                "/wiki/Venezuela",
                "/wiki/Yemen",
                "/wiki/Zambia",
                "/wiki/Zimbabwe",
                "/wiki/Australia",
                "/wiki/Brazil",
                "/wiki/Canada",
                "/wiki/Cyprus",
                "/wiki/Greece",
                "/wiki/China",
                "/wiki/Gambia",
                "/wiki/Germany",
                "/wiki/India",
                "/wiki/Iraq",
                "/wiki/Israel",
                "/wiki/Italy",
                "/wiki/Malaysia",
                "/wiki/Netherlands",
                "/wiki/New_Zealand",
                "/wiki/Nigeria",
                "/wiki/Norway",
                "/wiki/Pakistan",
                "/wiki/South_Africa",
                "/wiki/Saudi_Arabia",
                "/wiki/Tanzania",
                "/wiki/Ukraine",
                "/wiki/United_Arab_Emirates",
                "/wiki/United_Kingdom",
                "/wiki/United_States",
                "/wiki/Venezuela",
                "/wiki/Intelligence_assessment",
                "/wiki/Soft_biometrics",
                "/wiki/Mercedes-Benz_S-Class",
                "/wiki/Nayef_Al-Rodhan",
                "/wiki/Surveillance",
                "/wiki/Simone_Browne_(sociologist)",
                "/wiki/Research_and_development",
                "/wiki/Mongoloid",
                "/wiki/Commodification",
                "/wiki/Giorgio_Agamben",
                "/wiki/Discipline",
                "/wiki/Biopower",
                "/wiki/Botnet",
                "/wiki/John_Michael_McConnell",
                "/wiki/United_States_Navy",
                "/wiki/United_States_Director_of_National_Intelligence",
                "/wiki/Booz_Allen_Hamilton",
                "/wiki/Telerobotics",
                "/wiki/Electroencephalography",
                "/wiki/Electrocardiography",
                "/wiki/University_of_Kent",
                "/wiki/Hamming_distance",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Artifact_(error)",
                "/wiki/Normalization_(image_processing)",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Database",
                "/wiki/Passwords",
                "/wiki/Authentication",
                "/wiki/Smart_card",
                "/wiki/Personal_identification_number",
                "/wiki/Driver%27s_license",
                "/wiki/Passport",
                "/wiki/Password",
                "/wiki/Personal_identification_number",
                "/wiki/Fingerprint",
                "/wiki/Facial_recognition_system",
                "/wiki/DNA",
                "/wiki/Palm_print",
                "/wiki/Hand_geometry",
                "/wiki/Iris_recognition",
                "/wiki/Retinal_scan",
                "/wiki/Keystroke_dynamics",
                "/wiki/Gait_analysis",
                "/wiki/Speaker_recognition",
                "/wiki/Access_control",
                "/wiki/Surveillance"
            ],
            "text": "About 550 million residents have been enrolled and assigned 480 million Aadhaar national identification numbers as of 7 November 2013. [64] It aims to cover the entire population of 1.2 billion in a few years.[65] However it is being challenged by critics over privacy concerns and possible transformation of the state into a surveillance state, or into a banana republic.[66][67]India's national ID program called Aadhaar is the largest biometric database in the world. It is a biometrics-based digital identity assigned for a person's lifetime, verifiable[63] online instantly in the public domain, at any time, from anywhere, in a paperless way. It is designed to enable government agencies to deliver a retail public service, securely based on biometric data (fingerprint, iris scan and face photo), along with demographic data (name, age, gender, address, parent/spouse name, mobile phone number) of a person. The data is transmitted in encrypted form over the internet for authentication, aiming to free it from the limitations of physical presence of a person at a given place.There are also numerous countries applying biometrics for voter registration and similar electoral purposes. According to the International IDEA\u2019s ICTs in Elections Database,[59] some of the countries using (2017) Biometric Voter Registration (BVR) are Armenia, Angola, Bangladesh, Bhutan, Bolivia, Brazil, Burkina Faso, Cambodia, Cameroon, Chad, Colombia, Comoros, Congo (Democratic Republic of), Costa Rica, Ivory Coast, Dominican Republic, Fiji, Gambia, Ghana, Guatemala, India, Iraq, Kenya, Lesotho, Liberia, Malawi, Mali, Mauritania, Mexico, Morocco, Mozambique, Namibia, Nepal, Nicaragua, Nigeria, Panama, Peru, The Philippines, Senegal, Sierra Leone, Solomon Islands, Somaliland, Swaziland, Tanzania, Uganda, Uruguay, Venezuela, Yemen, Zambia, and Zimbabwe.[60][61][62]Among low to middle income countries, roughly 1.2 billion people have already received identification through a biometric identification program.[58]Countries using biometrics include Australia, Brazil, Canada, Cyprus, Greece, China, Gambia, Germany, India, Iraq, Israel, Italy, Malaysia, Netherlands, New Zealand, Nigeria, Norway, Pakistan, South Africa, Saudi Arabia, Tanzania,[57] Ukraine, United Arab Emirates, United Kingdom, United States and Venezuela.Certain members of the civilian community are worried about how biometric data is used but full disclosure may not be forthcoming. In particular, the Unclassified Report of the United States' Defense Science Board Task Force on Defense Biometrics states that it is wise to protect, and sometimes even to disguise, the true and total extent of national capabilities in areas related directly to the conduct of security-related activities.[56] This also potentially applies to Biometrics. It goes on to say that this is a classic feature of intelligence and military operations. In short, the goal is to preserve the security of 'sources and methods'.According to an article written in 2009 by S. Magnuson in the National Defense Magazine entitled \"Defense Department Under Pressure to Share Biometric Data\" the United States has bilateral agreements with other nations aimed at sharing biometric data.[55] To quote that article:In testimony before the US House Appropriations Committee, Subcommittee on Homeland Security on \"biometric identification\" in 2009, Kathleen Kraninger and Robert A Mocny[54] commented on international cooperation and collaboration with respect to biometric data, as follows:Many countries, including the United States, are planning to share biometric data with other nations.Soft biometrics traits are physical, behavioral or adhered human characteristics that have been derived from the way human beings normally distinguish their peers (e.g. height, gender, hair color). They are used to complement the identity information provided by the primary biometric identifiers. Although soft biometric characteristics lack the distinctiveness and permanence to recognize an individual uniquely and reliably, and can be easily faked, they provide some evidence about the users identity that could be beneficial. In other words, despite the fact they are unable to individualize a subject, they are effective in distinguishing between people. Combinations of personal attributes like gender, race, eye color, height and other visible identification marks can be used to improve the performance of traditional biometric systems.[51] Most soft biometrics can be easily collected and are actually collected during enrollment. Two main ethical issues are raised by soft biometrics.[52] First, some of soft biometric traits are strongly cultural based; e.g., skin colors for determining ethnicity risk to support racist approaches, biometric sex recognition at the best recognizes gender from tertiary sexual characters, being unable to determine genetic and chromosomal sexes; soft biometrics for aging recognition are often deeply influenced by ageist stereotypes, etc. Second, soft biometrics have strong potential for categorizing and profiling people, so risking of supporting processes of stigmatization and exclusion.[53]Several methods for generating new exclusive biometrics have been proposed. The first fingerprint-based cancelable biometric system was designed and developed by Tulyakov et al.[47] Essentially, cancelable biometrics perform a distortion of the biometric image or features before matching. The variability in the distortion parameters provides the cancelable nature of the scheme. Some of the proposed techniques operate using their own recognition engines, such as Teoh et al.[48] and Savvides et al.,[49] whereas other methods, such as Dabbah et al.,[50] take the advantage of the advancement of the well-established biometric research for their recognition front-end to conduct recognition. Although this increases the restrictions on the protection system, it makes the cancellable templates more accessible for available biometric technologies\"Cancelable biometrics refers to the intentional and systematically repeatable distortion of biometric features in order to protect sensitive user-specific data. If a cancelable feature is compromised, the distortion characteristics are changed, and the same biometrics is mapped to a new template, which is used subsequently. Cancelable biometrics is one of the major categories for biometric template protection purpose besides biometric cryptosystem.\"[45] In biometric cryptosystem, \"the error-correcting coding techniques are employed to handle intraclass variations.\"[46] This ensures a high level of security but has limitations such as specific input format of only small intraclass variations.Cancelable biometrics is a way in which to incorporate protection and the replacement features into biometrics to create a more secure system. It was first proposed by Ratha et al.[44]One advantage of passwords over biometrics is that they can be re-issued. If a token or a password is lost or stolen, it can be cancelled and replaced by a newer version. This is not naturally available in biometrics. If someone's face is compromised from a database, they cannot cancel or reissue it. If the electronic biometric identifier is stolen, it is nearly impossible to change a biometric feature. This renders the person\u2019s biometric feature questionable for future use in authentication, such as the case with the hacking of security-clearance-related background information from the Office of Personnel Management (OPM) in the United States.When thieves cannot get access to secure properties, there is a chance that the thieves will stalk and assault the property owner to gain access. If the item is secured with a biometric device, the damage to the owner could be irreversible, and potentially cost more than the secured property. For example, in 2005, Malaysian car thieves cut off the finger of a Mercedes-Benz S-Class owner when attempting to steal the car.[43]There are three categories of privacy concerns:[42]It is possible that data obtained during biometric enrollment may be used in ways for which the enrolled individual has not consented. For example, most biometric features could disclose physiological and/or pathological medical conditions (e.g., some fingerprint patterns are related to chromosomal diseases, iris patterns could reveal genetic sex, hand vein patterns could reveal vascular diseases, most behavioral biometrics could reveal neurological diseases, etc.).[40] Moreover, second generation biometrics, notably behavioral and electro-physiologic biometrics (e.g., based on electrocardiography, electroencephalography, electromyography), could be also used for emotion detection.[41]The biometrics of intent poses further risks. In his paper in Harvard International Review, Prof Nayef Al-Rodhan cautions about the high risks of miscalculations, wrongful accusations and infringements of civil liberties. Critics in the US have also signalled a conflict with the 4th Amendment.Other scholars[36] have emphasized, however, that the globalized world is confronted with a huge mass of people with weak or absent civil identities. Most developing countries have weak and unreliable documents and the poorer people in these countries do not have even those unreliable documents.[37] Without certified personal identities, there is no certainty of right, no civil liberty.[38] One can claim her rights, including the right to refuse to be identified, only if she is an identifiable subject, if she has a public identity. In such a sense, biometrics could play a pivotal role in supporting and promoting respect for human dignity and fundamental rights.[39]In Dark Matters: On the Surveillance of Blackness, surveillance scholar Simone Browne formulates a similar critique as Agamben, citing a recent study[33] relating to biometrics R&D that found that the gender classification system being researched \"is inclined to classify Africans as males and Mongoloids as females.\"[33] Consequently, Browne argues that the conception of an objective biometric technology is difficult if such systems are subjectively designed, and are vulnerable to cause errors as described in the study above. The stark expansion of biometric technologies in both the public and private sector magnifies this concern. The increasing commodification of biometrics by the private sector adds to this danger of loss of human value. Indeed, corporations value the biometric characteristics more than the individuals value them.[34] Browne goes on to suggest that modern society should incorporate a \"biometric consciousness\" that \"entails informed public debate around these technologies and their application, and accountability by the state and the private sector, where the ownership of and access to one's own body data and other intellectual property that is generated from one's body data must be understood as a right.\"[35]In a well-known case,[31] Italian philosopher Giorgio Agamben refused to enter the United States in protest at the United States Visitor and Immigrant Status Indicator (US-VISIT) program\u2019s requirement for visitors to be fingerprinted and photographed. Agamben argued that gathering of biometric data is a form of bio-political tattooing, akin to the tattooing of Jews during the Holocaust. According to Agamben, biometrics turn the human persona into a bare body. Agamben refers to the two words used by Ancient Greeks for indicating \"life\", zoe, which is the life common to animals and humans, just life; and bios, which is life in the human context, with meanings and purposes. Agamben envisages the reduction to bare bodies for the whole humanity.[32] For him, a new bio-political relationship between citizens and the state is turning citizens into pure biological life (zoe) depriving them from their humanity (bios); and biometrics would herald this new world.Biometrics have been considered also instrumental to the development of state authority[27] (to put it in Foucauldian terms, of discipline and biopower[28]). By turning the human subject into a collection of biometric parameters, biometrics would dehumanize the person,[29] infringe bodily integrity, and, ultimately, offend human dignity.[30]Recently, another approach to biometric security was developed, this method scans the entire body of prospects to guarantee a better identification of this prospect. This method is not globally accepted because it is very complex and prospects are concerned about their privacy.A basic premise in the above proposal is that the person that has uniquely authenticated themselves using biometrics with the computer is in fact also the agent performing potentially malicious actions from that computer. However, if control of the computer has been subverted, for example in which the computer is part of a botnet controlled by a hacker, then knowledge of the identity of the user at the terminal does not materially improve network security or aid law enforcement activities.[26]John Michael (Mike) McConnell, a former vice admiral in the United States Navy, a former Director of U.S. National Intelligence, and Senior Vice President of Booz Allen Hamilton promoted the development of a future capability to require biometric authentication to access certain public networks in his keynote speech[25] at the 2009 Biometric Consortium Conference.An operator signature is a biometric mode where the manner in which a person using a device or complex system is recorded as a verification template.[24] One potential use for this type of biometric signature is to distinguish among remote users of telerobotic surgery systems that utilize public networks for communication.[24]On the portability side of biometric products, more and more vendors are embracing significantly miniaturized biometric authentication systems (BAS) thereby driving elaborate cost savings, especially for large-scale deployments.In recent times, biometrics based on brain (electroencephalogram) and heart (electrocardiogram) signals have emerged.[22][23] The research group at University of Kent led by Ramaswamy Palaniappan has shown that people have certain distinct brain and heart patterns that are specific for each individual. The advantage of such 'futuristic' technology is that it is more fraud resistant compared to conventional biometrics like fingerprints. However, such technology is generally more cumbersome and still has issues such as lower accuracy and poor reproducibility over time. This new generation of biometrical systems is called biometrics of intent and it aims to scan intent. The technology will analyze physiological features such as eye movement, body temperature, breathing etc. and predict dangerous behaviour or hostile intent before it materializes into action.Adaptive biometric systems aim to auto-update the templates or model to the intra-class variation of the operational data.[21] The two-fold advantages of these systems are solving the problem of limited training data and tracking the temporal variations of the input data through adaptation. Recently, adaptive biometrics have received a significant attention from the research community. This research direction is expected to gain momentum because of their key promulgated advantages. First, with an adaptive biometric system, one no longer needs to collect a large number of biometric samples during the enrollment process. Second, it is no longer necessary to re-enroll or retrain the system from scratch in order to cope with the changing environment. This convenience can significantly reduce the cost of maintaining a biometric system. Despite these advantages, there are several open issues involved with these systems. For mis-classification error (false acceptance) by the biometric system, cause adaptation using impostor sample. However, continuous research efforts are directed to resolve the open issues associated to the field of adaptive biometrics. More information about adaptive biometric systems can be found in the critical review by Rattani et al.An early cataloging of fingerprints dates back to 1891 when Juan Vucetich started a collection of fingerprints of criminals in Argentina.[16] Josh Ellenbogen and Nitzan Lebovic argued that Biometrics is originated in the identificatory systems of criminal activity developed by Alphonse Bertillon (1853\u20131914) and developed by Francis Galton's theory of fingerprints and physiognomy.[17] According to Lebovic, Galton's work \"led to the application of mathematical models to fingerprints, phrenology, and facial characteristics\", as part of \"absolute identification\" and \"a key to both inclusion and exclusion\" of populations.[18] Accordingly, \"the biometric system is the absolute political weapon of our era\" and a form of \"soft control\".[19] The theoretician David Lyon showed that during the past two decades biometric systems have penetrated the civilian market, and blurred the lines between governmental forms of control and private corporate control.[20] Kelly A. Gates identified 9/11 as the turning point for the cultural language of our present: \"in the language of cultural studies, the aftermath of 9/11 was a moment of articulation, where objects or events that have no necessary connection come together and a new discourse formation is established: automated facial recognition as a homeland security technology.\" Kelly A. Gates, Our Biometric Future: Facial Recognition Technology and the Culture of Surveillance (New York, 2011), p.\u00a0100.The following are used as performance metrics for biometric systems:[15]Spoof attacks consist in submitting fake biometric traits to biometric systems, and are a major threat that can curtail their security. Multi-modal biometric systems are commonly believed to be intrinsically more robust to spoof attacks, but recent studies[14] have shown that they can be evaded by spoofing even a single biometric trait.Multimodal biometric systems can fuse these unimodal systems sequentially, simultaneously, a combination thereof, or in series, which refer to sequential, parallel, hierarchical and serial integration modes, respectively. Fusion of the biometrics information can occur at different stages of a recognition system. In case of feature level fusion, the data itself or the features extracted from multiple biometrics are fused. Matching-score level fusion consolidates the scores generated by multiple classifiers pertaining to different modalities. Finally, in case of decision level fusion the final results of multiple classifiers are combined via techniques such as majority voting. Feature level fusion is believed to be more effective than the other levels of fusion because the feature set contains richer information about the input biometric data than the matching score or the output decision of a classifier. Therefore, fusion at the feature level is expected to provide better recognition results.[10]Multimodal biometric systems use multiple sensors or biometrics to overcome the limitations of unimodal biometric systems.[10] For instance iris recognition systems can be compromised by aging irises[11] and finger scanning systems by worn-out or cut fingerprints. While unimodal biometric systems are limited by the integrity of their identifier, it is unlikely that several unimodal systems will suffer from identical limitations. Multimodal biometric systems can obtain sets of information from the same marker (i.e., multiple images of an iris, or scans of the same finger) or information from different biometrics (requiring fingerprint scans and, using voice recognition, a spoken pass-code).[12][13]During the enrollment phase, the template is simply stored somewhere (on a card or within a database or both). During the matching phase, the obtained template is passed to a matcher that compares it with other existing templates, estimating the distance between them using any algorithm (e.g. Hamming distance). The matching program will analyze the template with the input. This will then be output for any specified use or purpose (e.g. entrance in a restricted area)[citation needed]. Selection of biometrics in any practical application depending upon the characteristic measurements and user requirements.[9] In selecting a particular biometric, factors to consider include, performance, social acceptability, ease of circumvention and/or spoofing, robustness, population coverage, size of equipment needed and identity theft deterrence. Selection of a biometric based on user requirements considers sensor and device availability, computational time and reliability, cost, sensor size and power consumption.The first time an individual uses a biometric system is called enrollment. During the enrollment, biometric information from an individual is captured and stored. In subsequent uses, biometric information is detected and compared with the information stored at the time of enrollment. Note that it is crucial that storage and retrieval of such systems themselves be secure if the biometric system is to be robust. The first block (sensor) is the interface between the real world and the system; it has to acquire all the necessary data. Most of the times it is an image acquisition system, but it can change according to the characteristics desired. The second block performs all the necessary pre-processing: it has to remove artifacts from the sensor, to enhance the input (e.g. removing background noise), to use some kind of normalization, etc. In the third block necessary features are extracted. This step is an important step as the correct features need to be extracted in the optimal way. A vector of numbers or an image with particular properties is used to create a template. A template is a synthesis of the relevant characteristics extracted from the source. Elements of the biometric measurement that are not used in the comparison algorithm are discarded in the template to reduce the filesize and to protect the identity of the enrollee[citation needed].Second, in identification mode the system performs a one-to-many comparison against a biometric database in an attempt to establish the identity of an unknown individual. The system will succeed in identifying the individual if the comparison of the biometric sample to a template in the database falls within a previously set threshold. Identification mode can be used either for 'positive recognition' (so that the user does not have to provide any information about the template to be used) or for 'negative recognition' of the person \"where the system establishes whether the person is who she (implicitly or explicitly) denies to be\".[4] The latter function can only be achieved through biometrics since other methods of personal recognition such as passwords, PINs or keys are ineffective.The block diagram illustrates the two basic modes of a biometric system.[4] First, in verification (or authentication) mode the system performs a one-to-one comparison of a captured biometric with a specific template stored in a biometric database in order to verify the individual is the person they claim to be. Three steps are involved in the verification of a person.[9] In the first step, reference models for all the users are generated and stored in the model database. In the second step, some samples are matched with reference models to generate the genuine and impostor scores and calculate the threshold. Third step is the testing step. This process may use a smart card, username or ID number (e.g. PIN) to indicate which template should be used for comparison.[note 3] 'Positive recognition' is a common use of the verification mode, \"where the aim is to prevent multiple people from using the same identity\".[4]Proper biometric use is very application dependent. Certain biometrics will be better than others based on the required levels of convenience and security.[8] No single biometric will meet all the requirements of every possible application.[7]Many different aspects of human physiology, chemistry or behavior can be used for biometric authentication. The selection of a particular biometric for use in a specific application involves a weighting of several factors. Jain et al. (1999)[7] identified seven such factors to be used when assessing the suitability of any trait for use in biometric authentication.More traditional means of access control include token-based identification systems, such as a driver's license or passport, and knowledge-based identification systems, such as a password or personal identification number.[3] Since biometric identifiers are unique to individuals, they are more reliable in verifying identity than token and knowledge-based methods; however, the collection of biometric identifiers raises privacy concerns about the ultimate use of this information.[3][6]Biometric identifiers are the distinctive, measurable characteristics used to label and describe individuals.[3] Biometric identifiers are often categorized as physiological versus behavioral characteristics.[4] Physiological characteristics are related to the shape of the body. Examples include, but are not limited to fingerprint, palm veins, face recognition, DNA, palm print, hand geometry, iris recognition, retina and odour/scent. Behavioral characteristics are related to the pattern of behavior of a person, including but not limited to typing rhythm, gait, and voice.[note 2] Some researchers have coined the term behaviometrics to describe the latter class of biometrics.[5]Biometrics is the technical term for body measurements and calculations. It refers to metrics related to human characteristics. Biometrics authentication (or realistic authentication)[note 1] is used in computer science as a form of identification and access control.[1][2] It is also used to identify individuals in groups that are under surveillance.",
            "title": "Biometrics",
            "url": "https://en.wikipedia.org/wiki/Biometrics"
        },
        {
            "desc_links": [
                "/wiki/Computational_resource",
                "/wiki/Space%E2%80%93time_tradeoff",
                "/wiki/Video_compression",
                "/wiki/Electronic_hardware",
                "/wiki/Lossy_data_compression",
                "/wiki/Data_file",
                "/wiki/Data_transmission",
                "/wiki/Channel_coding",
                "/wiki/Line_coding",
                "/wiki/Signal_processing",
                "/wiki/Information",
                "/wiki/Bit",
                "/wiki/Lossy_compression",
                "/wiki/Lossless_compression",
                "/wiki/Redundancy_(information_theory)"
            ],
            "links": [
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Exabytes",
                "/wiki/Shannon_information",
                "/wiki/Final_Fantasy_XII",
                "/wiki/Compression_of_Genomic_Re-Sequencing_Data",
                "/wiki/International_HapMap_Project",
                "/wiki/ITU-T",
                "/wiki/International_Organization_for_Standardization",
                "/wiki/Discrete_cosine_transform",
                "/wiki/N._Ahmed",
                "/wiki/K._R._Rao",
                "/wiki/Fractal_compression",
                "/wiki/Matching_pursuit",
                "/wiki/Discrete_wavelet_transform",
                "/wiki/Wavelet_compression",
                "/wiki/HDV",
                "/wiki/Video_file_format",
                "/wiki/DV",
                "/wiki/Video_compression_picture_types",
                "/wiki/MPEG-2",
                "/wiki/Image_compression",
                "/wiki/Redundancy_(information_theory)",
                "/wiki/JPEG",
                "/wiki/Uncompressed_video",
                "/wiki/Pixel",
                "/wiki/Macroblock",
                "/wiki/Video_codec",
                "/wiki/Residual_frame",
                "/wiki/Variable_bitrate",
                "/wiki/Lossy_compression",
                "/wiki/Uncompressed_video",
                "/wiki/Uncompressed_video#Storage_and_Data_Rates_for_Uncompressed_Video",
                "/wiki/List_of_codecs#Lossless_video_compression",
                "/wiki/MPEG-4",
                "/wiki/Trade-off",
                "/wiki/Video_quality",
                "/wiki/Compression_artifact",
                "/wiki/List_of_codecs#Video_codecs",
                "/wiki/Video_codec",
                "/wiki/Image_compression",
                "/wiki/Motion_compensation",
                "/wiki/Broadcast_automation",
                "/wiki/University_of_Buenos_Aires",
                "/wiki/IBM_PC",
                "/wiki/Audicom",
                "/wiki/A-law_algorithm",
                "/wiki/%CE%9C-law_algorithm",
                "/wiki/Speech_encoding",
                "/wiki/Latency_(engineering)",
                "/wiki/Linear_predictive_coding",
                "/wiki/Quantization_(signal_processing)",
                "/wiki/Modified_discrete_cosine_transform",
                "/wiki/Time_domain",
                "/wiki/Frequency_domain",
                "/wiki/Absolute_threshold_of_hearing",
                "/wiki/Simultaneous_masking",
                "/wiki/Temporal_masking",
                "/wiki/Equal-loudness_contour",
                "/wiki/Psychoacoustic_model",
                "/wiki/Audio_quality",
                "/wiki/Digital_generation_loss",
                "/wiki/MP3",
                "/wiki/Psychoacoustics",
                "/wiki/Auditory_system",
                "/wiki/Internet",
                "/wiki/Audio_file_format",
                "/wiki/MPEG-4_SLS",
                "/wiki/WavPack",
                "/wiki/OptimFROG_DualStream",
                "/wiki/Shorten_(file_format)",
                "/wiki/Free_Lossless_Audio_Codec",
                "/wiki/Apple_Lossless",
                "/wiki/MPEG-4_ALS",
                "/wiki/Windows_Media_Audio_9_Lossless",
                "/wiki/Monkey%27s_Audio",
                "/wiki/TTA_(codec)",
                "/wiki/WavPack",
                "/wiki/List_of_codecs#Lossless_data_compression",
                "/wiki/Audio_editing",
                "/wiki/Vorbis",
                "/wiki/MP3",
                "/wiki/Waveform",
                "/wiki/FLAC",
                "/wiki/Shorten_(file_format)",
                "/wiki/TTA_(codec)",
                "/wiki/Linear_prediction",
                "/wiki/Convolution",
                "/wiki/White_noise",
                "/wiki/Decorrelation",
                "/wiki/Compact_disc",
                "/wiki/High_fidelity",
                "/wiki/MP3",
                "/wiki/Bit_rate",
                "/wiki/Redundancy_(information_theory)",
                "/wiki/Coding_theory",
                "/wiki/Pattern_recognition",
                "/wiki/Linear_prediction",
                "/wiki/Dynamic_range_compression",
                "/wiki/Bandwidth_(computing)",
                "/wiki/List_of_codecs#Audio",
                "/wiki/Software",
                "/wiki/Codec",
                "/wiki/Psychoacoustics",
                "/wiki/Data_differencing",
                "/wiki/Entropy_(information_theory)",
                "/wiki/Relative_entropy",
                "/wiki/Feature_space_vector",
                "/wiki/Machine_learning",
                "/wiki/Posterior_probabilities",
                "/wiki/Arithmetic_coding",
                "/wiki/Information_theory",
                "/wiki/Algorithmic_information_theory",
                "/wiki/Rate%E2%80%93distortion_theory",
                "/wiki/Claude_Shannon",
                "/wiki/Coding_theory",
                "/wiki/Statistical_inference",
                "/wiki/Audio_compression_(data)",
                "/wiki/Psychoacoustics",
                "/wiki/Audio_signal_processing",
                "/wiki/Speech_coding",
                "/wiki/Audio_coding_format",
                "/wiki/Internet_telephony",
                "/wiki/Image_compression",
                "/wiki/Digital_camera",
                "/wiki/DVD",
                "/wiki/MPEG-2",
                "/wiki/Video_coding_format",
                "/wiki/Video_compression",
                "/wiki/Lossy_data_compression",
                "/wiki/Lossless_data_compression",
                "/wiki/Luminance",
                "/wiki/JPEG",
                "/wiki/Image_compression",
                "/wiki/Trade-off",
                "/wiki/Psychoacoustics",
                "/wiki/Probabilistic_model",
                "/wiki/Arithmetic_coding",
                "/wiki/Finite-state_machine",
                "/wiki/Probability_distribution",
                "/wiki/JPEG",
                "/wiki/H.263",
                "/wiki/H.264/MPEG-4_AVC",
                "/wiki/HEVC",
                "/wiki/Grammar-based_codes",
                "/wiki/Sequitur_algorithm",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Probabilistic_algorithm",
                "/wiki/Prediction_by_partial_matching",
                "/wiki/Burrows%E2%80%93Wheeler_transform",
                "/wiki/Lempel%E2%80%93Ziv",
                "/wiki/DEFLATE_(algorithm)",
                "/wiki/PKZIP",
                "/wiki/Gzip",
                "/wiki/Portable_Network_Graphics",
                "/wiki/Terry_Welch",
                "/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch",
                "/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch",
                "/wiki/Graphics_Interchange_Format",
                "/wiki/Huffman_coding",
                "/wiki/Brotli",
                "/wiki/LZX_(algorithm)",
                "/wiki/Cabinet_(file_format)",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Lossless_data_compression",
                "/wiki/Algorithm",
                "/wiki/Redundancy_(information_theory)",
                "/wiki/Self-information",
                "/wiki/Run-length_encoding",
                "/wiki/Computational_resource",
                "/wiki/Space%E2%80%93time_tradeoff",
                "/wiki/Video_compression",
                "/wiki/Electronic_hardware",
                "/wiki/Lossy_data_compression",
                "/wiki/Data_file",
                "/wiki/Data_transmission",
                "/wiki/Channel_coding",
                "/wiki/Line_coding",
                "/wiki/Signal_processing",
                "/wiki/Information",
                "/wiki/Bit",
                "/wiki/Lossy_compression",
                "/wiki/Lossless_compression",
                "/wiki/Redundancy_(information_theory)"
            ],
            "text": "It is estimated that the total amount of data that is stored on the world's storage devices could be further compressed with existing compression algorithms by a remaining average factor of 4.5:1.[citation needed] It is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits in 2007, but when the corresponding content is optimally compressed, this only represents 295 exabytes of Shannon information.[36]In order to emulate CD-based consoles such as the PlayStation 2, data compression is desirable to reduce huge amounts of disk space used by ISOs. For example, Final Fantasy XII (USA) is normally 2.9 gigabytes. With proper compression, it is reduced to around 90% of that size.[35]Genetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset.[32] Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-fold\u2014allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes).[33][34]The following table is a partial history of international video compression standards.Today, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) apply a discrete cosine transform (DCT) for spatial redundancy reduction. The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974.[31] Other methods, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT) have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.[29]It is possible to build a computer-based video editor that spots problems caused when I frames are edited out while other frames need them. This has allowed newer formats like HDV to be used for editing. However, this process demands a lot more computing power than editing intraframe compressed video with the same picture quality.Because interframe compression copies data from one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Some video formats, such as DV, compress each frame independently using intraframe compression. Making 'cuts' in intraframe-compressed video is almost as easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesn't want. Another difference between intraframe and interframe compression is that, with intraframe systems, each frame uses a similar amount of data. In most interframe systems, certain frames (such as \"I frames\" in MPEG-2) aren't allowed to copy data from other frames, so they require much more data than other frames nearby.[22]The most powerful used method works by comparing each frame in the video with the previous one. If the frame contains areas where nothing has moved, the system simply issues a short command that copies that part of the previous frame, bit-for-bit, into the next one. If sections of the frame move in a simple manner, the compressor emits a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Interframe compression works well for programs that will simply be played back by the viewer, but can cause problems if the video sequence needs to be edited.[30]One of the most powerful techniques for compressing video is interframe compression. Interframe compression uses one or more earlier or later frames in a sequence to compress the current frame, while intraframe compression uses only the current frame, effectively being image compression.[29]Video data may be represented as a series of still image frames. The sequence of frames contains spatial and temporal redundancy that video compression algorithms attempt to eliminate or code in a smaller size. Similarities can be encoded by only storing differences between frames, or by using perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression.[10] Some of these methods are inherently lossy while others may preserve all relevant information from the original, uncompressed video.Some video compression schemes typically operate on square-shaped groups of neighboring pixels, often called macroblocks. These pixel groups or blocks of pixels are compared from one frame to the next, and the video compression codec sends only the differences within those blocks. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate.The majority of video compression algorithms use lossy compression. Uncompressed video requires a very high data rate. Although lossless video compression codecs perform at a compression factor of 5-12, a typical MPEG-4 lossy compression video has a compression factor between 20 and 200.[28] As in all lossy compression, there is a trade-off between video quality, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts.Video compression uses modern coding techniques to reduce redundancy in video data. Most video compression algorithms and codecs combine spatial image compression and temporal motion compensation. Video compression is a practical implementation of source coding in information theory. In practice, most video codecs also use audio compression techniques in parallel to compress the separate, but combined data streams as one package.[27]The world's first commercial broadcast automation audio compression system was developed by Oscar Bonello, an engineering professor at the University of Buenos Aires.[25] In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967,[26] he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies.A literature compendium for a large variety of audio coding systems was published in the IEEE Journal on Selected Areas in Communications (JSAC), February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding.[24] Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee.Perhaps the earliest algorithms used in speech encoding (and audio data compression in general) were the A-law algorithm and the \u00b5-law algorithm.This relation illustrates the compromise between high resolution (a large number of analog intervals) and high compression (small integers generated). This application of quantization is used by several speech compression methods. This is accomplished, in general, by some combination of two approaches:If the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals.Speech encoding is an important category of audio data compression. The perceptual models used to estimate what a human ear can hear are generally somewhat different from those used for music. The range of frequencies needed to convey the sounds of a human voice are normally far narrower than that needed for music, and the sound is normally less complex. As a result, speech can be encoded at high quality using a relatively low bit rate.In contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analysed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms (46 ms for two-way communication)).Latency results from the methods used to encode and decode the data. Some codecs will analyze a longer segment of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. (Often codecs create segments called a \"frame\" to create discrete data segments for encoding and decoding.) The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality.Lossy formats are often used for the distribution of streaming audio or interactive applications (such as the coding of speech for digital transmission in cell phone networks). In such applications, the data must be decompressed as the data flows, rather than after the entire data stream has been transmitted. Not all audio codecs can be used for streaming applications, and for such applications a codec designed to stream data effectively will usually be chosen.[22]Other types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. These coders use a model of the sound's generator (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may be thought of as a basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coder's quantization noise into the spectrum of the target signal, partially masking it.[22]To determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain. Once transformed, typically into the frequency domain, component frequencies can be allocated bits according to how audible they are. Audibility of spectral components calculated using the absolute threshold of hearing and the principles of simultaneous masking\u2014the phenomenon wherein a signal is masked by another signal separated by frequency\u2014and, in some cases, temporal masking\u2014where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models.[23]Due to the nature of lossy algorithms, audio quality suffers when a file is decompressed and recompressed (digital generation loss). This makes lossy compression unsuitable for storing the intermediate results in professional audio engineering applications, such as sound editing and multitrack recording. However, they are very popular with end users (particularly MP3) as a megabyte can store about a minute's worth of music at adequate quality.The innovation of lossy audio compression was to use psychoacoustics to recognize that not all data in an audio stream can be perceived by the human auditory system. Most lossy compression reduces perceptual redundancy by first identifying perceptually irrelevant sounds, that is, sounds that are very hard to hear. Typical examples include high frequencies or sounds that occur at the same time as louder sounds. Those sounds are coded with decreased accuracy or not at all.Lossy audio compression is used in a wide range of applications. In addition to the direct applications (MP3 players or computers), digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression (data of 5 percent to 20 percent of the original stream, rather than 50 percent to 60 percent), by discarding less-critical data.[22]Other formats are associated with a distinct system, such as:Some audio formats feature a combination of a lossy format and a lossless correction; this allows stripping the correction to easily obtain a lossy file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream.A number of lossless audio compression formats exist. Shorten was an early lossless format. Newer ones include Free Lossless Audio Codec (FLAC), Apple's Apple Lossless (ALAC), MPEG-4 ALS, Microsoft's Windows Media Audio 9 Lossless (WMA Lossless), Monkey's Audio, TTA, and WavPack. See list of lossless codecs for a complete listing.When audio files are to be processed, either by further compression or for editing, it is desirable to work from an unchanged original (uncompressed or losslessly compressed). Processing of a lossily compressed file for some purpose usually produces a final result inferior to the creation of the same compressed file from an uncompressed original. In addition to sound editing or mixing, lossless audio compression is often used for archival storage, or as master copies.Lossless audio compression produces a representation of digital data that decompress to an exact digital duplicate of the original audio stream, unlike playback from lossy compression techniques such as Vorbis and MP3. Compression ratios are around 50\u201360% of original size,[21] which is similar to those for generic lossless data compression. Lossless compression is unable to attain high compression ratios due to the complexity of waveforms and the rapid changes in sound forms. Codecs like FLAC, Shorten, and TTA use linear prediction to estimate the spectrum of the signal. Many of these algorithms use convolution with the filter [-1 1] to slightly whiten or flatten the spectrum, thereby allowing traditional lossless compression to work more efficiently. The process is reversed upon decompression.The acceptable trade-off between loss of audio quality and transmission or storage size depends upon the application. For example, one 640MB compact disc (CD) holds approximately one hour of uncompressed high fidelity music, less than 2 hours of music compressed losslessly, or 7 hours of music compressed in the MP3 format at a medium bit rate. A digital sound recorder can typically store around 200 hours of clearly intelligible speech in 640MB.[20]In both lossy and lossless compression, information redundancy is reduced, using methods such as coding, pattern recognition, and linear prediction to reduce the amount of information used to represent the uncompressed data.Audio data compression, not to be confused with dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. Lossy audio compression algorithms provide higher compression at the cost of fidelity and are used in numerous audio applications. These algorithms almost all rely on psychoacoustics to eliminate or reduce fidelity of less audible sounds, thereby reducing the space required to store or transmit them.[2]When one wishes to emphasize the connection, one may use the term differential compression to refer to data differencing.Data compression can be viewed as a special case of data differencing:[18][19] Data differencing consists of producing a difference given a source and a target, with patching producing a target given a source and a difference, while data compression consists of producing a compressed file given a target, and decompression consists of producing a target given only a compressed file. Thus, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a \"difference from nothing.\" This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data.However a new, alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space \u2135, such that C(.) maps an input string x, corresponds to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM. [17]There is a close connection between machine learning and compression: a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution) while an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence.\"[14][15][16]The theoretical background of compression is provided by information theory (which is closely related to algorithmic information theory) for lossless compression and rate\u2013distortion theory for lossy compression. These areas of study were essentially created by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Coding theory is also related to this. The idea of data compression is also deeply connected with statistical inference.[13]In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding, or voice coding, is sometimes distinguished as a separate discipline from audio compression. Different audio and speech compression standards are listed under audio coding formats. Voice compression is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players.[9]Lossy image compression can be used in digital cameras, to increase storage capacities with minimal degradation of picture quality. Similarly, DVDs use the lossy MPEG-2 video coding format for video compression.Lossy data compression is the converse of lossless data compression. In the late 1980s, digital images became more common, and standards for compressing them emerged. In the early 1990s, lossy compression methods began to be widely used.[8] In these schemes, some loss of information is acceptable. Dropping nonessential detail from the data source can save storage space. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information.[12] There is a corresponding trade-off between preserving information and reducing size. A number of popular compression formats exploit these perceptual differences, including those used in music files, images, and video.In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was its use as an optional (but not widely used) feature of the JPEG image coding standard.[10] It has since been applied in various other designs including H.263, H.264/MPEG-4 AVC and HEVC for video coding.[11]The class of grammar-based codes are gaining popularity because they can compress highly repetitive input extremely effectively, for instance, a biological data collection of the same or closely related species, a huge versioned document collection, internet archival, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Sequitur and Re-Pair are practical grammar compression algorithms for which software is publicly available.[citation needed]The best modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows\u2013Wheeler transform can also be viewed as an indirect form of statistical modelling.[9]The Lempel\u2013Ziv (LZ) compression methods are among the most popular algorithms for lossless storage.[7] DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. DEFLATE is used in PKZIP, Gzip, and PNG. In the mid-1980s, following work by Terry Welch, the LZW (Lempel\u2013Ziv\u2013Welch) algorithm rapidly became the method of choice for most general-purpose compression systems. LZW is used in GIF images, programs such as PKZIP, and hardware devices such as modems.[8] LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded (e.g. SHRI, LZX). Current LZ-based coding schemes that perform well are Brotli and LZX. LZX is used in Microsoft's CAB format.[citation needed]Lossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of color that do not change over several pixels; instead of coding \"red pixel, red pixel, ...\" the data may be encoded as \"279 red pixels\". This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy.Compression is useful because it reduces resources required to store and transmit data. Computational resources are consumed in the compression process and, usually, in the reversal of the process (decompression). Data compression is subject to a space\u2013time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.[5][6]The process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding; encoding done at the source of the data before it is stored or transmitted.[4] Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal.In signal processing, data compression, source coding,[1] or bit-rate reduction involves encoding information using fewer bits than the original representation.[2] Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information.[3]",
            "title": "Data compression",
            "url": "https://en.wikipedia.org/wiki/Data_compression"
        },
        {
            "desc_links": [
                "/wiki/Driver%27s_license",
                "/wiki/Passport",
                "/wiki/Password",
                "/wiki/Personal_identification_number",
                "/wiki/Fingerprint",
                "/wiki/Facial_recognition_system",
                "/wiki/DNA",
                "/wiki/Palm_print",
                "/wiki/Hand_geometry",
                "/wiki/Iris_recognition",
                "/wiki/Retinal_scan",
                "/wiki/Keystroke_dynamics",
                "/wiki/Gait_analysis",
                "/wiki/Speaker_recognition",
                "/wiki/Access_control",
                "/wiki/Surveillance"
            ],
            "links": [
                "/wiki/National_identification_number",
                "/wiki/Aadhaar",
                "/wiki/Biometric",
                "/wiki/Fingerprint",
                "/wiki/Iris_scan",
                "/wiki/Demographic_data",
                "/wiki/Mobile_telephone_numbering_in_India",
                "/wiki/Biometric_voter_registration",
                "/wiki/International_Institute_for_Democracy_and_Electoral_Assistance",
                "/wiki/Armenia",
                "/wiki/Angola",
                "/wiki/Bangladesh",
                "/wiki/Bhutan",
                "/wiki/Bolivia",
                "/wiki/Brazil",
                "/wiki/Burkina_Faso",
                "/wiki/Cambodia",
                "/wiki/Cameroon",
                "/wiki/Chad",
                "/wiki/Colombia",
                "/wiki/Comoros",
                "/wiki/Democratic_Republic_of_the_Congo",
                "/wiki/Costa_Rica",
                "/wiki/Ivory_Coast",
                "/wiki/Dominican_Republic",
                "/wiki/Fiji",
                "/wiki/Gambia",
                "/wiki/Ghana",
                "/wiki/Guatemala",
                "/wiki/India",
                "/wiki/Iraq",
                "/wiki/Kenya",
                "/wiki/Lesotho",
                "/wiki/Liberia",
                "/wiki/Malawi",
                "/wiki/Mali",
                "/wiki/Mauritania",
                "/wiki/Mexico",
                "/wiki/Morocco",
                "/wiki/Mozambique",
                "/wiki/Namibia",
                "/wiki/Nepal",
                "/wiki/Nicaragua",
                "/wiki/Nigeria",
                "/wiki/Panama",
                "/wiki/Peru",
                "/wiki/Philippines",
                "/wiki/Senegal",
                "/wiki/Sierra_Leone",
                "/wiki/Solomon_Islands",
                "/wiki/Somaliland",
                "/wiki/Swaziland",
                "/wiki/Tanzania",
                "/wiki/Uganda",
                "/wiki/Uruguay",
                "/wiki/Venezuela",
                "/wiki/Yemen",
                "/wiki/Zambia",
                "/wiki/Zimbabwe",
                "/wiki/Australia",
                "/wiki/Brazil",
                "/wiki/Canada",
                "/wiki/Cyprus",
                "/wiki/Greece",
                "/wiki/China",
                "/wiki/Gambia",
                "/wiki/Germany",
                "/wiki/India",
                "/wiki/Iraq",
                "/wiki/Israel",
                "/wiki/Italy",
                "/wiki/Malaysia",
                "/wiki/Netherlands",
                "/wiki/New_Zealand",
                "/wiki/Nigeria",
                "/wiki/Norway",
                "/wiki/Pakistan",
                "/wiki/South_Africa",
                "/wiki/Saudi_Arabia",
                "/wiki/Tanzania",
                "/wiki/Ukraine",
                "/wiki/United_Arab_Emirates",
                "/wiki/United_Kingdom",
                "/wiki/United_States",
                "/wiki/Venezuela",
                "/wiki/Intelligence_assessment",
                "/wiki/Soft_biometrics",
                "/wiki/Mercedes-Benz_S-Class",
                "/wiki/Nayef_Al-Rodhan",
                "/wiki/Surveillance",
                "/wiki/Simone_Browne_(sociologist)",
                "/wiki/Research_and_development",
                "/wiki/Mongoloid",
                "/wiki/Commodification",
                "/wiki/Giorgio_Agamben",
                "/wiki/Discipline",
                "/wiki/Biopower",
                "/wiki/Botnet",
                "/wiki/John_Michael_McConnell",
                "/wiki/United_States_Navy",
                "/wiki/United_States_Director_of_National_Intelligence",
                "/wiki/Booz_Allen_Hamilton",
                "/wiki/Telerobotics",
                "/wiki/Electroencephalography",
                "/wiki/Electrocardiography",
                "/wiki/University_of_Kent",
                "/wiki/Hamming_distance",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Artifact_(error)",
                "/wiki/Normalization_(image_processing)",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Database",
                "/wiki/Passwords",
                "/wiki/Authentication",
                "/wiki/Smart_card",
                "/wiki/Personal_identification_number",
                "/wiki/Driver%27s_license",
                "/wiki/Passport",
                "/wiki/Password",
                "/wiki/Personal_identification_number",
                "/wiki/Fingerprint",
                "/wiki/Facial_recognition_system",
                "/wiki/DNA",
                "/wiki/Palm_print",
                "/wiki/Hand_geometry",
                "/wiki/Iris_recognition",
                "/wiki/Retinal_scan",
                "/wiki/Keystroke_dynamics",
                "/wiki/Gait_analysis",
                "/wiki/Speaker_recognition",
                "/wiki/Access_control",
                "/wiki/Surveillance"
            ],
            "text": "About 550 million residents have been enrolled and assigned 480 million Aadhaar national identification numbers as of 7 November 2013. [64] It aims to cover the entire population of 1.2 billion in a few years.[65] However it is being challenged by critics over privacy concerns and possible transformation of the state into a surveillance state, or into a banana republic.[66][67]India's national ID program called Aadhaar is the largest biometric database in the world. It is a biometrics-based digital identity assigned for a person's lifetime, verifiable[63] online instantly in the public domain, at any time, from anywhere, in a paperless way. It is designed to enable government agencies to deliver a retail public service, securely based on biometric data (fingerprint, iris scan and face photo), along with demographic data (name, age, gender, address, parent/spouse name, mobile phone number) of a person. The data is transmitted in encrypted form over the internet for authentication, aiming to free it from the limitations of physical presence of a person at a given place.There are also numerous countries applying biometrics for voter registration and similar electoral purposes. According to the International IDEA\u2019s ICTs in Elections Database,[59] some of the countries using (2017) Biometric Voter Registration (BVR) are Armenia, Angola, Bangladesh, Bhutan, Bolivia, Brazil, Burkina Faso, Cambodia, Cameroon, Chad, Colombia, Comoros, Congo (Democratic Republic of), Costa Rica, Ivory Coast, Dominican Republic, Fiji, Gambia, Ghana, Guatemala, India, Iraq, Kenya, Lesotho, Liberia, Malawi, Mali, Mauritania, Mexico, Morocco, Mozambique, Namibia, Nepal, Nicaragua, Nigeria, Panama, Peru, The Philippines, Senegal, Sierra Leone, Solomon Islands, Somaliland, Swaziland, Tanzania, Uganda, Uruguay, Venezuela, Yemen, Zambia, and Zimbabwe.[60][61][62]Among low to middle income countries, roughly 1.2 billion people have already received identification through a biometric identification program.[58]Countries using biometrics include Australia, Brazil, Canada, Cyprus, Greece, China, Gambia, Germany, India, Iraq, Israel, Italy, Malaysia, Netherlands, New Zealand, Nigeria, Norway, Pakistan, South Africa, Saudi Arabia, Tanzania,[57] Ukraine, United Arab Emirates, United Kingdom, United States and Venezuela.Certain members of the civilian community are worried about how biometric data is used but full disclosure may not be forthcoming. In particular, the Unclassified Report of the United States' Defense Science Board Task Force on Defense Biometrics states that it is wise to protect, and sometimes even to disguise, the true and total extent of national capabilities in areas related directly to the conduct of security-related activities.[56] This also potentially applies to Biometrics. It goes on to say that this is a classic feature of intelligence and military operations. In short, the goal is to preserve the security of 'sources and methods'.According to an article written in 2009 by S. Magnuson in the National Defense Magazine entitled \"Defense Department Under Pressure to Share Biometric Data\" the United States has bilateral agreements with other nations aimed at sharing biometric data.[55] To quote that article:In testimony before the US House Appropriations Committee, Subcommittee on Homeland Security on \"biometric identification\" in 2009, Kathleen Kraninger and Robert A Mocny[54] commented on international cooperation and collaboration with respect to biometric data, as follows:Many countries, including the United States, are planning to share biometric data with other nations.Soft biometrics traits are physical, behavioral or adhered human characteristics that have been derived from the way human beings normally distinguish their peers (e.g. height, gender, hair color). They are used to complement the identity information provided by the primary biometric identifiers. Although soft biometric characteristics lack the distinctiveness and permanence to recognize an individual uniquely and reliably, and can be easily faked, they provide some evidence about the users identity that could be beneficial. In other words, despite the fact they are unable to individualize a subject, they are effective in distinguishing between people. Combinations of personal attributes like gender, race, eye color, height and other visible identification marks can be used to improve the performance of traditional biometric systems.[51] Most soft biometrics can be easily collected and are actually collected during enrollment. Two main ethical issues are raised by soft biometrics.[52] First, some of soft biometric traits are strongly cultural based; e.g., skin colors for determining ethnicity risk to support racist approaches, biometric sex recognition at the best recognizes gender from tertiary sexual characters, being unable to determine genetic and chromosomal sexes; soft biometrics for aging recognition are often deeply influenced by ageist stereotypes, etc. Second, soft biometrics have strong potential for categorizing and profiling people, so risking of supporting processes of stigmatization and exclusion.[53]Several methods for generating new exclusive biometrics have been proposed. The first fingerprint-based cancelable biometric system was designed and developed by Tulyakov et al.[47] Essentially, cancelable biometrics perform a distortion of the biometric image or features before matching. The variability in the distortion parameters provides the cancelable nature of the scheme. Some of the proposed techniques operate using their own recognition engines, such as Teoh et al.[48] and Savvides et al.,[49] whereas other methods, such as Dabbah et al.,[50] take the advantage of the advancement of the well-established biometric research for their recognition front-end to conduct recognition. Although this increases the restrictions on the protection system, it makes the cancellable templates more accessible for available biometric technologies\"Cancelable biometrics refers to the intentional and systematically repeatable distortion of biometric features in order to protect sensitive user-specific data. If a cancelable feature is compromised, the distortion characteristics are changed, and the same biometrics is mapped to a new template, which is used subsequently. Cancelable biometrics is one of the major categories for biometric template protection purpose besides biometric cryptosystem.\"[45] In biometric cryptosystem, \"the error-correcting coding techniques are employed to handle intraclass variations.\"[46] This ensures a high level of security but has limitations such as specific input format of only small intraclass variations.Cancelable biometrics is a way in which to incorporate protection and the replacement features into biometrics to create a more secure system. It was first proposed by Ratha et al.[44]One advantage of passwords over biometrics is that they can be re-issued. If a token or a password is lost or stolen, it can be cancelled and replaced by a newer version. This is not naturally available in biometrics. If someone's face is compromised from a database, they cannot cancel or reissue it. If the electronic biometric identifier is stolen, it is nearly impossible to change a biometric feature. This renders the person\u2019s biometric feature questionable for future use in authentication, such as the case with the hacking of security-clearance-related background information from the Office of Personnel Management (OPM) in the United States.When thieves cannot get access to secure properties, there is a chance that the thieves will stalk and assault the property owner to gain access. If the item is secured with a biometric device, the damage to the owner could be irreversible, and potentially cost more than the secured property. For example, in 2005, Malaysian car thieves cut off the finger of a Mercedes-Benz S-Class owner when attempting to steal the car.[43]There are three categories of privacy concerns:[42]It is possible that data obtained during biometric enrollment may be used in ways for which the enrolled individual has not consented. For example, most biometric features could disclose physiological and/or pathological medical conditions (e.g., some fingerprint patterns are related to chromosomal diseases, iris patterns could reveal genetic sex, hand vein patterns could reveal vascular diseases, most behavioral biometrics could reveal neurological diseases, etc.).[40] Moreover, second generation biometrics, notably behavioral and electro-physiologic biometrics (e.g., based on electrocardiography, electroencephalography, electromyography), could be also used for emotion detection.[41]The biometrics of intent poses further risks. In his paper in Harvard International Review, Prof Nayef Al-Rodhan cautions about the high risks of miscalculations, wrongful accusations and infringements of civil liberties. Critics in the US have also signalled a conflict with the 4th Amendment.Other scholars[36] have emphasized, however, that the globalized world is confronted with a huge mass of people with weak or absent civil identities. Most developing countries have weak and unreliable documents and the poorer people in these countries do not have even those unreliable documents.[37] Without certified personal identities, there is no certainty of right, no civil liberty.[38] One can claim her rights, including the right to refuse to be identified, only if she is an identifiable subject, if she has a public identity. In such a sense, biometrics could play a pivotal role in supporting and promoting respect for human dignity and fundamental rights.[39]In Dark Matters: On the Surveillance of Blackness, surveillance scholar Simone Browne formulates a similar critique as Agamben, citing a recent study[33] relating to biometrics R&D that found that the gender classification system being researched \"is inclined to classify Africans as males and Mongoloids as females.\"[33] Consequently, Browne argues that the conception of an objective biometric technology is difficult if such systems are subjectively designed, and are vulnerable to cause errors as described in the study above. The stark expansion of biometric technologies in both the public and private sector magnifies this concern. The increasing commodification of biometrics by the private sector adds to this danger of loss of human value. Indeed, corporations value the biometric characteristics more than the individuals value them.[34] Browne goes on to suggest that modern society should incorporate a \"biometric consciousness\" that \"entails informed public debate around these technologies and their application, and accountability by the state and the private sector, where the ownership of and access to one's own body data and other intellectual property that is generated from one's body data must be understood as a right.\"[35]In a well-known case,[31] Italian philosopher Giorgio Agamben refused to enter the United States in protest at the United States Visitor and Immigrant Status Indicator (US-VISIT) program\u2019s requirement for visitors to be fingerprinted and photographed. Agamben argued that gathering of biometric data is a form of bio-political tattooing, akin to the tattooing of Jews during the Holocaust. According to Agamben, biometrics turn the human persona into a bare body. Agamben refers to the two words used by Ancient Greeks for indicating \"life\", zoe, which is the life common to animals and humans, just life; and bios, which is life in the human context, with meanings and purposes. Agamben envisages the reduction to bare bodies for the whole humanity.[32] For him, a new bio-political relationship between citizens and the state is turning citizens into pure biological life (zoe) depriving them from their humanity (bios); and biometrics would herald this new world.Biometrics have been considered also instrumental to the development of state authority[27] (to put it in Foucauldian terms, of discipline and biopower[28]). By turning the human subject into a collection of biometric parameters, biometrics would dehumanize the person,[29] infringe bodily integrity, and, ultimately, offend human dignity.[30]Recently, another approach to biometric security was developed, this method scans the entire body of prospects to guarantee a better identification of this prospect. This method is not globally accepted because it is very complex and prospects are concerned about their privacy.A basic premise in the above proposal is that the person that has uniquely authenticated themselves using biometrics with the computer is in fact also the agent performing potentially malicious actions from that computer. However, if control of the computer has been subverted, for example in which the computer is part of a botnet controlled by a hacker, then knowledge of the identity of the user at the terminal does not materially improve network security or aid law enforcement activities.[26]John Michael (Mike) McConnell, a former vice admiral in the United States Navy, a former Director of U.S. National Intelligence, and Senior Vice President of Booz Allen Hamilton promoted the development of a future capability to require biometric authentication to access certain public networks in his keynote speech[25] at the 2009 Biometric Consortium Conference.An operator signature is a biometric mode where the manner in which a person using a device or complex system is recorded as a verification template.[24] One potential use for this type of biometric signature is to distinguish among remote users of telerobotic surgery systems that utilize public networks for communication.[24]On the portability side of biometric products, more and more vendors are embracing significantly miniaturized biometric authentication systems (BAS) thereby driving elaborate cost savings, especially for large-scale deployments.In recent times, biometrics based on brain (electroencephalogram) and heart (electrocardiogram) signals have emerged.[22][23] The research group at University of Kent led by Ramaswamy Palaniappan has shown that people have certain distinct brain and heart patterns that are specific for each individual. The advantage of such 'futuristic' technology is that it is more fraud resistant compared to conventional biometrics like fingerprints. However, such technology is generally more cumbersome and still has issues such as lower accuracy and poor reproducibility over time. This new generation of biometrical systems is called biometrics of intent and it aims to scan intent. The technology will analyze physiological features such as eye movement, body temperature, breathing etc. and predict dangerous behaviour or hostile intent before it materializes into action.Adaptive biometric systems aim to auto-update the templates or model to the intra-class variation of the operational data.[21] The two-fold advantages of these systems are solving the problem of limited training data and tracking the temporal variations of the input data through adaptation. Recently, adaptive biometrics have received a significant attention from the research community. This research direction is expected to gain momentum because of their key promulgated advantages. First, with an adaptive biometric system, one no longer needs to collect a large number of biometric samples during the enrollment process. Second, it is no longer necessary to re-enroll or retrain the system from scratch in order to cope with the changing environment. This convenience can significantly reduce the cost of maintaining a biometric system. Despite these advantages, there are several open issues involved with these systems. For mis-classification error (false acceptance) by the biometric system, cause adaptation using impostor sample. However, continuous research efforts are directed to resolve the open issues associated to the field of adaptive biometrics. More information about adaptive biometric systems can be found in the critical review by Rattani et al.An early cataloging of fingerprints dates back to 1891 when Juan Vucetich started a collection of fingerprints of criminals in Argentina.[16] Josh Ellenbogen and Nitzan Lebovic argued that Biometrics is originated in the identificatory systems of criminal activity developed by Alphonse Bertillon (1853\u20131914) and developed by Francis Galton's theory of fingerprints and physiognomy.[17] According to Lebovic, Galton's work \"led to the application of mathematical models to fingerprints, phrenology, and facial characteristics\", as part of \"absolute identification\" and \"a key to both inclusion and exclusion\" of populations.[18] Accordingly, \"the biometric system is the absolute political weapon of our era\" and a form of \"soft control\".[19] The theoretician David Lyon showed that during the past two decades biometric systems have penetrated the civilian market, and blurred the lines between governmental forms of control and private corporate control.[20] Kelly A. Gates identified 9/11 as the turning point for the cultural language of our present: \"in the language of cultural studies, the aftermath of 9/11 was a moment of articulation, where objects or events that have no necessary connection come together and a new discourse formation is established: automated facial recognition as a homeland security technology.\" Kelly A. Gates, Our Biometric Future: Facial Recognition Technology and the Culture of Surveillance (New York, 2011), p.\u00a0100.The following are used as performance metrics for biometric systems:[15]Spoof attacks consist in submitting fake biometric traits to biometric systems, and are a major threat that can curtail their security. Multi-modal biometric systems are commonly believed to be intrinsically more robust to spoof attacks, but recent studies[14] have shown that they can be evaded by spoofing even a single biometric trait.Multimodal biometric systems can fuse these unimodal systems sequentially, simultaneously, a combination thereof, or in series, which refer to sequential, parallel, hierarchical and serial integration modes, respectively. Fusion of the biometrics information can occur at different stages of a recognition system. In case of feature level fusion, the data itself or the features extracted from multiple biometrics are fused. Matching-score level fusion consolidates the scores generated by multiple classifiers pertaining to different modalities. Finally, in case of decision level fusion the final results of multiple classifiers are combined via techniques such as majority voting. Feature level fusion is believed to be more effective than the other levels of fusion because the feature set contains richer information about the input biometric data than the matching score or the output decision of a classifier. Therefore, fusion at the feature level is expected to provide better recognition results.[10]Multimodal biometric systems use multiple sensors or biometrics to overcome the limitations of unimodal biometric systems.[10] For instance iris recognition systems can be compromised by aging irises[11] and finger scanning systems by worn-out or cut fingerprints. While unimodal biometric systems are limited by the integrity of their identifier, it is unlikely that several unimodal systems will suffer from identical limitations. Multimodal biometric systems can obtain sets of information from the same marker (i.e., multiple images of an iris, or scans of the same finger) or information from different biometrics (requiring fingerprint scans and, using voice recognition, a spoken pass-code).[12][13]During the enrollment phase, the template is simply stored somewhere (on a card or within a database or both). During the matching phase, the obtained template is passed to a matcher that compares it with other existing templates, estimating the distance between them using any algorithm (e.g. Hamming distance). The matching program will analyze the template with the input. This will then be output for any specified use or purpose (e.g. entrance in a restricted area)[citation needed]. Selection of biometrics in any practical application depending upon the characteristic measurements and user requirements.[9] In selecting a particular biometric, factors to consider include, performance, social acceptability, ease of circumvention and/or spoofing, robustness, population coverage, size of equipment needed and identity theft deterrence. Selection of a biometric based on user requirements considers sensor and device availability, computational time and reliability, cost, sensor size and power consumption.The first time an individual uses a biometric system is called enrollment. During the enrollment, biometric information from an individual is captured and stored. In subsequent uses, biometric information is detected and compared with the information stored at the time of enrollment. Note that it is crucial that storage and retrieval of such systems themselves be secure if the biometric system is to be robust. The first block (sensor) is the interface between the real world and the system; it has to acquire all the necessary data. Most of the times it is an image acquisition system, but it can change according to the characteristics desired. The second block performs all the necessary pre-processing: it has to remove artifacts from the sensor, to enhance the input (e.g. removing background noise), to use some kind of normalization, etc. In the third block necessary features are extracted. This step is an important step as the correct features need to be extracted in the optimal way. A vector of numbers or an image with particular properties is used to create a template. A template is a synthesis of the relevant characteristics extracted from the source. Elements of the biometric measurement that are not used in the comparison algorithm are discarded in the template to reduce the filesize and to protect the identity of the enrollee[citation needed].Second, in identification mode the system performs a one-to-many comparison against a biometric database in an attempt to establish the identity of an unknown individual. The system will succeed in identifying the individual if the comparison of the biometric sample to a template in the database falls within a previously set threshold. Identification mode can be used either for 'positive recognition' (so that the user does not have to provide any information about the template to be used) or for 'negative recognition' of the person \"where the system establishes whether the person is who she (implicitly or explicitly) denies to be\".[4] The latter function can only be achieved through biometrics since other methods of personal recognition such as passwords, PINs or keys are ineffective.The block diagram illustrates the two basic modes of a biometric system.[4] First, in verification (or authentication) mode the system performs a one-to-one comparison of a captured biometric with a specific template stored in a biometric database in order to verify the individual is the person they claim to be. Three steps are involved in the verification of a person.[9] In the first step, reference models for all the users are generated and stored in the model database. In the second step, some samples are matched with reference models to generate the genuine and impostor scores and calculate the threshold. Third step is the testing step. This process may use a smart card, username or ID number (e.g. PIN) to indicate which template should be used for comparison.[note 3] 'Positive recognition' is a common use of the verification mode, \"where the aim is to prevent multiple people from using the same identity\".[4]Proper biometric use is very application dependent. Certain biometrics will be better than others based on the required levels of convenience and security.[8] No single biometric will meet all the requirements of every possible application.[7]Many different aspects of human physiology, chemistry or behavior can be used for biometric authentication. The selection of a particular biometric for use in a specific application involves a weighting of several factors. Jain et al. (1999)[7] identified seven such factors to be used when assessing the suitability of any trait for use in biometric authentication.More traditional means of access control include token-based identification systems, such as a driver's license or passport, and knowledge-based identification systems, such as a password or personal identification number.[3] Since biometric identifiers are unique to individuals, they are more reliable in verifying identity than token and knowledge-based methods; however, the collection of biometric identifiers raises privacy concerns about the ultimate use of this information.[3][6]Biometric identifiers are the distinctive, measurable characteristics used to label and describe individuals.[3] Biometric identifiers are often categorized as physiological versus behavioral characteristics.[4] Physiological characteristics are related to the shape of the body. Examples include, but are not limited to fingerprint, palm veins, face recognition, DNA, palm print, hand geometry, iris recognition, retina and odour/scent. Behavioral characteristics are related to the pattern of behavior of a person, including but not limited to typing rhythm, gait, and voice.[note 2] Some researchers have coined the term behaviometrics to describe the latter class of biometrics.[5]Biometrics is the technical term for body measurements and calculations. It refers to metrics related to human characteristics. Biometrics authentication (or realistic authentication)[note 1] is used in computer science as a form of identification and access control.[1][2] It is also used to identify individuals in groups that are under surveillance.",
            "title": "Biometrics",
            "url": "https://en.wikipedia.org/wiki/Recognition_of_human_individuals"
        },
        {
            "desc_links": [
                "/wiki/Numerical_analysis",
                "/wiki/Mathematical_physics",
                "/wiki/Structural_analysis",
                "/wiki/Heat_transfer",
                "/wiki/Fluid_flow",
                "/wiki/Electromagnetic_potential",
                "/wiki/Closed-form_expression",
                "/wiki/Boundary_value_problem",
                "/wiki/Partial_differential_equations",
                "/wiki/Algebraic_equation",
                "/wiki/Variational_methods",
                "/wiki/Calculus_of_variations"
            ],
            "links": [
                "/wiki/Computational_fluid_dynamics",
                "/wiki/Finite_volume_method",
                "/wiki/Finite_difference_method",
                "/wiki/Gradient_discretisation_method",
                "/wiki/Loubignac_iteration",
                "/wiki/Extended_finite_element_method",
                "/wiki/Hp-FEM",
                "/wiki/Partition_of_unity",
                "/wiki/Discrete_element_method",
                "/wiki/Hp-FEM",
                "/wiki/Spectral_element_method",
                "/wiki/Spectral_method",
                "/wiki/Hp-FEM",
                "/wiki/Spectral_element_method",
                "/wiki/Weak_formulation",
                "/wiki/Computer",
                "/wiki/Dirichlet_problem",
                "/wiki/Calculus",
                "/wiki/Linear_algebra",
                "/wiki/Superconvergence",
                "/wiki/P-FEM",
                "/wiki/Hp-FEM",
                "/wiki/Extended_finite_element_method",
                "/wiki/Isogeometric_analysis",
                "/wiki/Galerkin_method",
                "/wiki/Calculus_of_variations",
                "/wiki/John_Argyris",
                "/wiki/University_of_Stuttgart",
                "/wiki/Ray_W._Clough",
                "/wiki/University_of_California,_Berkeley",
                "/wiki/Olgierd_Zienkiewicz",
                "/wiki/Ernest_Hinton",
                "/wiki/Bruce_Irons_(engineer)",
                "/wiki/Swansea_University",
                "/wiki/Philippe_G._Ciarlet",
                "/wiki/Pierre-and-Marie-Curie_University",
                "/wiki/Cornell_University",
                "/wiki/NASTRAN",
                "/wiki/DNV_GL",
                "/wiki/SESAM_(FEM)",
                "/wiki/Gilbert_Strang",
                "/wiki/George_Fix",
                "/wiki/Numerical_analysis",
                "/wiki/Engineering",
                "/wiki/Electromagnetism",
                "/wiki/Heat_transfer",
                "/wiki/Fluid_dynamics",
                "/wiki/Lattice_(group)",
                "/wiki/Second_order_equation",
                "/wiki/Elliptic_equation",
                "/wiki/Torsion_(mechanics)",
                "/wiki/Cylinder_(geometry)",
                "/wiki/John_William_Strutt,_3rd_Baron_Rayleigh",
                "/wiki/Walther_Ritz",
                "/wiki/Boris_Galerkin",
                "/wiki/Elasticity_(physics)",
                "/wiki/Structural_analysis",
                "/wiki/Civil_engineering",
                "/wiki/Aeronautical_engineering",
                "/wiki/Alexander_Hrennikoff",
                "/wiki/Richard_Courant",
                "/wiki/Ioannis_Argyris",
                "/wiki/Feng_Kang",
                "/wiki/Partial_differential_equation",
                "/wiki/Polygon_mesh",
                "/wiki/Discretization",
                "/wiki/Numerical_weather_prediction",
                "/wiki/Tropical_cyclone",
                "/wiki/Eddy_(fluid_dynamics)",
                "/wiki/Engineering",
                "/wiki/Engineering_analysis",
                "/wiki/Mesh_generation",
                "/wiki/Complex_system",
                "/wiki/Software",
                "/wiki/Physics",
                "/wiki/Euler-Bernoulli_beam_equation",
                "/wiki/Heat_equation",
                "/wiki/Navier-Stokes_equations",
                "/wiki/Integral_equation",
                "/wiki/Transformation_matrix",
                "/wiki/Coordinate_system",
                "/wiki/Coordinates",
                "/wiki/Linear",
                "/wiki/Numerical_linear_algebra",
                "/wiki/Euler%27s_method",
                "/wiki/Runge-Kutta",
                "/wiki/Partial_differential_equation",
                "/wiki/Galerkin_method",
                "/wiki/Inner_product",
                "/wiki/Weight_function",
                "/wiki/Polynomial",
                "/wiki/Initial_value",
                "/wiki/Numerical_analysis",
                "/wiki/Mathematical_physics",
                "/wiki/Structural_analysis",
                "/wiki/Heat_transfer",
                "/wiki/Fluid_flow",
                "/wiki/Electromagnetic_potential",
                "/wiki/Closed-form_expression",
                "/wiki/Boundary_value_problem",
                "/wiki/Partial_differential_equations",
                "/wiki/Algebraic_equation",
                "/wiki/Variational_methods",
                "/wiki/Calculus_of_variations"
            ],
            "text": "FEA has also been proposed to use in stochastic modelling for numerically solving probability models.[21][22]This powerful design tool has significantly improved both the standard of engineering designs and the methodology of the design process in many industrial applications.[19] The introduction of FEM has substantially decreased the time to take products from concept to the production line.[19] It is primarily through improved initial prototype designs using FEM that testing and development have been accelerated.[20] In summary, benefits of FEM include increased accuracy, enhanced design and better insight into critical design parameters, virtual prototyping, fewer hardware prototypes, a faster and less expensive design cycle, increased productivity, and increased revenue.[19]FEM allows detailed visualization of where structures bend or twist, and indicates the distribution of stresses and displacements. FEM software provides a wide range of simulation options for controlling the complexity of both modeling and analysis of a system. Similarly, the desired level of accuracy required and associated computational time requirements can be managed simultaneously to address most engineering applications. FEM allows entire designs to be constructed, refined, and optimized before the design is manufactured.A variety of specializations under the umbrella of the mechanical engineering discipline (such as aeronautical, biomechanical, and automotive industries) commonly use integrated FEM in design and development of their products. Several modern FEM packages include specific components such as thermal, electromagnetic, fluid, and structural working environments. In a structural simulation, FEM helps tremendously in producing stiffness and strength visualizations and also in minimizing weight, materials, and costs.[17]Generally, FEM is the method of choice in all types of analysis in structural mechanics (i.e. solving for deformation and stresses in solid bodies or dynamics of structures) while computational fluid dynamics (CFD) tends to use FDM or other methods like finite volume method (FVM). CFD problems usually require discretization of the problem into a large number of cells/gridpoints (millions and more), therefore cost of the solution favors simpler, lower order approximation within each cell. This is especially true for 'external flow' problems, like air flow around the car or airplane, or weather simulation.The finite difference method (FDM) is an alternative way of approximating solutions of PDEs. The differences between FEM and FDM are:Some types of finite element methods (conforming, nonconforming, mixed finite element methods) are particular cases of the gradient discretisation method (GDM). Hence the convergence properties of the GDM, which are established for a series of problems (linear and non linear elliptic problems, linear, nonlinear and degenerate parabolic problems), hold as well for these particular finite element methods.Loubignac iteration is an iterative method in finite element methods.Spectral element methods combine the geometric flexibility of finite elements and the acute accuracy of spectral methods. Spectral methods are the approximate solution of weak form partial equations that are based on high-order Lagragian interpolants and used only with certain quadrature rules.[15]The S-FEM, Smoothed Finite Element Methods, are a particular class of numerical simulation algorithms for the simulation of physical phenomena. It was developed by combining meshfree methods with the finite element method.formulations and procedures, and the boundary element discretization. However, unlike the boundary element method, no fundamental differential solution is required.in the area of numerical analysis of fracture mechanics problems. It is a semi-analytical fundamental-solutionless method which combines the advantages of both the finnite elementThe introduction of the novice scaled boundary finite element method (SBFEM) came from Song and Wolf (1997)*. The SBFEM has been one of the most profitable contributionsScaled boundary finite element method (SBFEM)XFEM has also been implemented in codes like Altair Radioss, ASTER, Morfeo and Abaqus. It is increasingly being adopted by other commercial finite element software, with a few plugins and actual core implementations available (ANSYS, SAMCEF, OOFELIE, etc.).Several research codes implement this technique to various degrees: 1. GetFEM++ 2. xfem++ 3. openxfem++The extended finite element method (XFEM) is a numerical technique based on the generalized finite element method (GFEM) and the partition of unity method (PUM). It extends the classical finite element method by enriching the solution space for solutions to differential equations with discontinuous functions. Extended finite element methods enrich the approximation space so that it is able to naturally reproduce the challenging feature associated with the problem of interest: the discontinuity, singularity, boundary layer, etc. It was shown that for some problems, such an embedding of the problem's feature into the approximation space can significantly improve convergence rates and accuracy. Moreover, treating problems with discontinuities with XFEMs suppresses the need to mesh and remesh the discontinuity surfaces, thus alleviating the computational costs and projection errors associated with conventional finite element methods, at the cost of restricting the discontinuities to mesh edges.The hpk-FEM combines adaptively, elements with variable size h, polynomial degree of the local approximations p and global differentiability of the local approximations (k-1) in order to achieve best convergence rates.The hp-FEM combines adaptively, elements with variable size h and polynomial degree p in order to achieve exceptionally fast, exponential convergence rates.[14]The mixed finite element method is a type of finite element method in which extra independent variables are introduced as nodal variables during the discretization of a partial differential equation problem.The generalized finite element method (GFEM) uses local spaces consisting of functions, not necessarily polynomials, that reflect the available information on the unknown solution and thus ensure good local approximation. Then a partition of unity is used to \u201cbond\u201d these spaces together to form the approximating subspace. The effectiveness of GFEM has been shown when applied to problems with domains having complicated boundaries, problems with micro-scales, and problems with boundary layers.[13]The Applied Element Method, or AEM combines features of both FEM and Discrete element method, or (DEM).If instead of making h smaller, one increases the degree of the polynomials used in the basis function, one has a p-method. If one combines these two refinement types, one obtains an hp-method (hp-FEM). In the hp-FEM, the polynomial degrees can vary from element to element. High order methods with large uniform p are called spectral finite element methods (SFEM). These are not to be confused with spectral methods.In general, the finite element method is characterized by the following process.then we may rephrase (4) asandbe matrices whose entries areandare both zero.andandThe primary advantage of this choice of basis is that the inner productsMore advanced implementations (adaptive finite element methods) utilize a method to assess the quality of the results (based on error estimation theory) and modify the mesh during the solution aiming to achieve approximate solution within some bounds from the 'exact' solution of the continuum problem. Mesh adaptivity may utilize various techniques, the most popular are:Examples of methods that use higher degree piecewise polynomial basis functions are the hp-FEM and spectral FEM.Depending on the author, the word \"element\" in \"finite element method\" refers either to the triangles in the domain, the piecewise linear basis function, or both. So for instance, an author interested in curved domains might replace the triangles with curved primitives, and so might describe the elements as being curvilinear. On the other hand, some authors replace \"piecewise linear\" by \"piecewise quadratic\" or even \"piecewise polynomial\". The author might then say \"higher order element\" instead of \"higher degree polynomial\". Finite element method is not restricted to triangles (or tetrahedra in 3-d, or higher order simplexes in multidimensional spaces), but can be defined on quadrilateral subdomains (hexahedra, prisms, or pyramids in 3-d, and so on). Higher order shapes (curvilinear elements) can be defined with polynomial and even non-polynomial shapes (e.g. ellipse or circle).with a finite-dimensional version:P1 and P2 are ready to be discretized which leads to a common sub-problem (3). The basic idea is to replace the infinite-dimensional linear problem:The first step is to convert P1 and P2 into their equivalent weak formulations.After this second step, we have concrete formulae for a large but finite-dimensional linear problem whose solution will approximately solve the original BVP. This finite-dimensional problem is then implemented on a computer.Our explanation will proceed in two steps, which mirror two essential steps one must take to solve a boundary value problem (BVP) using the FEM.P2 is a two-dimensional problem (Dirichlet problem)P1 is a one-dimensional problemWe will demonstrate the finite element method using two sample problems from which the general method can be extrapolated. It is assumed that the reader is familiar with calculus and linear algebra.Postprocessing procedures are designed for the extraction of the data of interest from a finite element solution. In order to meet the requirements of solution verification, postprocessors need to provide for a posteriori error estimation in terms of the quantities of interest. When the errors of approximation are larger than what is considered acceptable then the discretization has to be changed either by an automated adaptive process or by action of the analyst. There are some very efficient postprocessors that provide for the realization of superconvergence.There are various numerical solution algorithms that can be classified into two broad categories; direct and iterative solvers. These algorithms are designed to exploit the sparsity of matrices that depend on the choices of variational formulation and discretization strategy.A discretization strategy is understood to mean a clearly defined set of procedures that cover (a) the creation of finite element meshes, (b) the definition of basis function on reference elements (also called shape functions) and (c) the mapping of reference elements onto the elements of the mesh. Examples of discretization strategies are the h-version, p-version, hp-version, x-FEM, isogeometric analysis, etc. Each discretization strategy has certain advantages and disadvantages. A reasonable criterion in selecting a discretization strategy is to realize nearly optimal performance for the broadest set of mathematical models in a particular model class.Examples of variational formulation are the Galerkin method, the discontinuous Galerkin method, mixed methods, etc.A finite element method is characterized by a variational formulation, a discretization strategy, one or more solution algorithms and post-processing procedures.Finite element methods are numerical methods for approximating the solutions of mathematical problems that are usually formulated so as to precisely state an idea of some aspect of physical reality.The finite element method obtained its real impetus in the 1960s and 1970s by the developments of J. H. Argyris with co-workers at the University of Stuttgart, R. W. Clough with co-workers at UC Berkeley, O. C. Zienkiewicz with co-workers Ernest Hinton, Bruce Irons[7] and others at the University of Swansea, Philippe G. Ciarlet at the University of Paris 6 and Richard Gallagher with co-workers at Cornell University. Further impetus was provided in these years by available open source finite element software programs. NASA sponsored the original version of NASTRAN, and UC Berkeley made the finite element program SAP IV[8] widely available. In Norway the ship classification society Det Norske Veritas (now DNV GL) developed Sesam in 1969 for use in analysis of ships.[9] A rigorous mathematical basis to the finite element method was provided in 1973 with the publication by Strang and Fix.[10] The method has since been generalized for the numerical modeling of physical systems in a wide variety of engineering disciplines, e.g., electromagnetism, heat transfer, and fluid dynamics.[11][12]Hrennikoff's work discretizes the domain by using a lattice analogy, while Courant's approach divides the domain into finite triangular subregions to solve second order elliptic partial differential equations (PDEs) that arise from the problem of torsion of a cylinder. Courant's contribution was evolutionary, drawing on a large body of earlier results for PDEs developed by Rayleigh, Ritz, and Galerkin.While it is difficult to quote a date of the invention of the finite element method, the method originated from the need to solve complex elasticity and structural analysis problems in civil and aeronautical engineering. Its development can be traced back to the work by A. Hrennikoff[4] and R. Courant[5] in the early 1940s. Another pioneer was Ioannis Argyris. In the USSR, the introduction of the practical application of the method is usually connected with name of Leonard Oganesyan.[6] In China, in the later 1950s and early 1960s, based on the computations of dam constructions, K. Feng proposed a systematic numerical method for solving partial differential equations. The method was called the finite difference method based on variation principle, which was another independent invention of the finite element method. Although the approaches used by these pioneers are different, they share one essential characteristic: mesh discretization of a continuous domain into a set of discrete sub-domains, usually called elements.FEA is a good choice for analyzing problems over complicated domains (like cars and oil pipelines), when the domain changes (as during a solid state reaction with a moving boundary), when the desired precision varies over the entire domain, or when the solution lacks smoothness. FEA simulations provide a valuable resource as they remove multiple instances of creation and testing of hard prototypes for various high fidelity situations.[3] For instance, in a frontal crash simulation it is possible to increase prediction accuracy in \"important\" areas like the front of the car and reduce it in its rear (thus reducing cost of the simulation). Another example would be in numerical weather prediction, where it is more important to have accurate predictions over developing highly nonlinear phenomena (such as tropical cyclones in the atmosphere, or eddies in the ocean) rather than relatively calm areas.FEM is best understood from its practical application, known as finite element analysis (FEA). FEA as applied in engineering is a computational tool for performing engineering analysis. It includes the use of mesh generation techniques for dividing a complex problem into small elements, as well as the use of software program coded with FEM algorithm. In applying FEA, the complex problem is usually a physical system with the underlying physics such as the Euler-Bernoulli beam equation, the heat equation, or the Navier-Stokes equations expressed in either PDE or integral equations, while the divided small elements of the complex problem represent different areas in the physical system.In step (2) above, a global system of equations is generated from the element equations through a transformation of coordinates from the subdomains' local nodes to the domain's global nodes. This spatial transformation includes appropriate orientation adjustments as applied in relation to the reference coordinate system. The process is often carried out by FEM software using coordinate data generated from the subdomains.These equation sets are the element equations. They are linear if the underlying PDE is linear, and vice versa. Algebraic equation sets that arise in the steady state problems are solved using numerical linear algebra methods, while ordinary differential equation sets that arise in the transient problems are solved by numerical integration using standard techniques such as Euler's method or the Runge-Kutta method.In the first step above, the element equations are simple equations that locally approximate the original complex equations to be studied, where the original equations are often partial differential equations (PDE). To explain the approximation in this process, FEM is commonly introduced as a special case of Galerkin method. The process, in mathematical language, is to construct an integral of the inner product of the residual and the weight functions and set the integral to zero. In simple terms, it is a procedure that minimizes the error of approximation by fitting trial functions into the PDE. The residual is the error caused by the trial functions, and the weight functions are polynomial approximation functions that project the residual. The process eliminates all the spatial derivatives from the PDE, thus approximating the PDE locally withA typical work out of the method involves (1) dividing the domain of the problem into a collection of subdomains, with each subdomain represented by a set of element equations to the original problem, followed by (2) systematically recombining all sets of element equations into a global system of equations for the final calculation. The global system of equations has known solution techniques, and can be calculated from the initial values of the original problem to obtain a numerical answer.The subdivision of a whole domain into simpler parts has several advantages:[2]The finite element method (FEM) or finite element analysis (FEA), is a numerical method for solving problems of engineering and mathematical physics. Typical problem areas of interest include structural analysis, heat transfer, fluid flow, mass transport, and electromagnetic potential. The analytical solution of these problems generally require the solution to boundary value problems for partial differential equations. The finite element method formulation of the problem results in a system of algebraic equations. The method yields approximate values of the unknowns at discrete number of points over the domain.[1] To solve the problem, it subdivides a large problem into smaller, simpler parts that are called finite elements. The simple equations that model these finite elements are then assembled into a larger system of equations that models the entire problem. FEM then uses variational methods from the calculus of variations to approximate a solution by minimizing an associated error function.",
            "title": "Finite element method",
            "url": "https://en.wikipedia.org/wiki/Finite_element_analysis"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Quadratic eigenvalue problem",
            "url": "https://en.wikipedia.org/wiki/Quadratic_eigenvalue_problem#Methods_of_Solution"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Quadratic eigenvalue problem",
            "url": "https://en.wikipedia.org/wiki/Quadratic_eigenvalue_problem"
        },
        {
            "desc_links": [
                "/wiki/Exact_constraint",
                "/wiki/Orientation_(geometry)",
                "/wiki/Translation_(physics)",
                "/wiki/Rotation",
                "/wiki/Drifting_(motorsports)",
                "/wiki/Classical_mechanics",
                "/wiki/Mechanical_system",
                "/wiki/Mechanical_engineering",
                "/wiki/Aerospace_engineering",
                "/wiki/Robotics",
                "/wiki/Structural_engineering"
            ],
            "links": [
                "/wiki/Electrical_engineering",
                "/wiki/Phased_array",
                "/wiki/Antenna_(radio)",
                "/wiki/Beamforming",
                "/wiki/Radar",
                "/wiki/Kinematic_chain",
                "/wiki/Joint",
                "/wiki/Robotics",
                "/wiki/Biomechanics",
                "/wiki/Satellites",
                "/wiki/Mechanism_(engineering)",
                "/wiki/Linkage_(mechanical)",
                "/wiki/Four-bar_linkage",
                "/wiki/Linkage_(mechanical)",
                "/wiki/Euler_angles",
                "/wiki/Rigid_body",
                "/wiki/Affine_transformation",
                "/wiki/SO(n)",
                "/wiki/Exact_constraint",
                "/wiki/Orientation_(geometry)",
                "/wiki/Translation_(physics)",
                "/wiki/Rotation",
                "/wiki/Drifting_(motorsports)",
                "/wiki/Classical_mechanics",
                "/wiki/Mechanical_system",
                "/wiki/Mechanical_engineering",
                "/wiki/Aerospace_engineering",
                "/wiki/Robotics",
                "/wiki/Structural_engineering"
            ],
            "text": "In electrical engineering degrees of freedom is often used to describe the number of directions in which a phased array antenna can form either beams or nulls. It is equal to one less than the number of elements contained in the array, as one element is used as a reference against which either constructive or destructive interference may be applied using each of the remaining antenna elements. Radar practice and communication link practice, with beam steering being more prevalent for radar applications and null steering being more prevalent for interference suppression in communication links.A summary of formulas and methods for computing the degrees-of-freedom in mechanical systems has been given by Pennestri, Cavacece, and Vita.[5]In mobile robotics, a car-like robot can reach any position and orientation in 2-D space, so it needs 3 DOFs to describe its pose, but at any point, you can move it only by a forward motion and a steering angle. So it has two control DOFs and three representational DOFs; i.e. it is non-holonomic. A fixed-wing aircraft, with 3\u20134 control DOFs (forward motion, roll, pitch, and to a limited extent, yaw) in a 3-D space, is also non-holonomic, as it cannot move directly up/down or left/right.A specific type of linkage is the open kinematic chain, where a set of rigid links are connected at joints; a joint may provide one DOF (hinge/sliding), or two (cylindrical). Such chains occur commonly in robotics, biomechanics, and for satellites and other space structures. A human arm is considered to have seven DOFs. A shoulder gives pitch, yaw, and roll, an elbow allows for pitch, and a wrist allows for pitch,yaw and roll . Only 3 of those movements would be necessary to move the hand to any point in space, but people would lack the ability to grasp things from different angles or directions. A robot (or object) that has mechanisms to control all 6 physical DOF is said to be holonomic. An object with fewer controllable DOFs than total DOFs is said to be non-holonomic, and an object with more controllable DOFs than total DOFs (such as the human arm) is said to be redundant. Although keep in mind that it is not redundant in the human arm because the two DOFs; wrist and shoulder, that represent the same movement; roll, supply each other since they can't do a full 360. The degree of freedom are like different movements that can be made.A system with several bodies would have a combined DOF that is the sum of the DOFs of the bodies, less the internal constraints they may have on relative motion. A mechanism or linkage containing a number of connected rigid bodies may have more than the degrees of freedom for a single rigid body. Here the term degrees of freedom is used to describe the number of parameters needed to specify the spatial pose of a linkage.An example of a planar simple closed chain is the planar four-bar linkage, which is a four-bar loop with four one degree-of-freedom joints and therefore has mobility\u00a0M\u00a0=\u00a01.and the special cases becomeIn this case, the mobility formula is given byIt is common practice to design the linkage system so that the movement of all of the bodies are constrained to lie on parallel planes, to form what is known as a planar linkage. It is also possible to construct the linkage system so that all of the bodies move on concentric spheres, forming a spherical linkage. In both cases, the degrees of freedom of the links in each system is now three rather than six, and the constraints imposed by joints are now c\u00a0=\u00a03\u00a0\u2212\u00a0f.An example of a simple closed chain is the RSSR spatial four-bar linkage. The sum of the freedom of these joints is eight, so the mobility of the linkage is two, where one of the degrees of freedom is the rotation of the coupler around the line joining the two S joints.An example of a simple open chain is a serial robot manipulator. These robotic systems are constructed from a series of links connected by six one degree-of-freedom revolute or prismatic joints, so the system has six degrees of freedom.For a simple closed chain, n moving links are connected end-to-end by n\u00a0+\u00a01 joints such that the two ends are connected to the ground link forming a loop. In this case, we have N\u00a0=\u00a0j and the mobility of the chain isThere are two important special cases: (i) a simple open chain, and (ii) a simple closed chain. A single open chain consists of n moving links connected end to end by n joints, with one end connected to a ground link. Thus, in this case N\u00a0=\u00a0j\u00a0+\u00a01 and the mobility of the chain isRecall that N includes the fixed link.The result is that the mobility of a system formed from n moving links and j joints each with freedom fi, i\u00a0=\u00a01,\u00a0...,\u00a0j, is given byJoints that connect bodies in this system remove degrees of freedom and reduce mobility. Specifically, hinges and sliders each impose five constraints and therefore remove five degrees of freedom. It is convenient to define the number of constraints c that a joint imposes in terms of the joint's freedom f, where c\u00a0=\u00a06\u00a0\u2212\u00a0f. In the case of a hinge or slider, which are one degree of freedom joints, have f\u00a0=\u00a01 and therefore c\u00a0=\u00a06\u00a0\u2212\u00a01\u00a0=\u00a05.because the fixed body has zero degrees of freedom relative to itself.Consider a system of n rigid bodies moving in space has 6n degrees of freedom measured relative to a fixed frame. In order to count the degrees of freedom of this system, include the fixed body in the count of bodies, so that mobility is independent of the choice of the body that forms the fixed frame. Then the degree-of-freedom of the unconstrained system of N\u00a0=\u00a0n\u00a0+\u00a01 isThe mobility formula counts the number of parameters that define the configuration of a set of rigid bodies that are constrained by joints connecting these bodies.[3][4]The trajectory of an airplane in flight has three degrees of freedom and its attitude along the trajectory has three degrees of freedom, for a total of six degrees of freedom.See also Euler anglesThe motion of a ship at sea has the six degrees of freedom of a rigid body, and is described as:[2]The degree of freedom of a system can be viewed as the minimum number of coordinates required to specify a configuration. Applying this definition, we have:A non-rigid or deformable body may be thought of as a collection of many minute particles (infinite number of DOFs), this is often approximated by a finite DOF system. When motion involving large displacements is the main objective of study (e.g. for analyzing the motion of satellites), a deformable body may be approximated as a rigid body (or even a particle) in order to simplify the analysis.The position of an n-dimensional rigid body is defined by the rigid transformation, [T]\u00a0=\u00a0[A,\u00a0d], where d is an n-dimensional translation and A is an n\u00a0\u00d7\u00a0n rotation matrix, which has n translational degrees of freedom and n(n\u00a0\u2212\u00a01)/2 rotational degrees of freedom. The number of rotational degrees of freedom comes from the dimension of the rotation group\u00a0SO(n).The exact constraint mechanical design method manages the degrees of freedom to neither underconstrain nor overconstrain a device.[1]The position and orientation of a rigid body in space is defined by three components of translation and three components of rotation, which means that it has six degrees of freedom.An automobile with highly stiff suspension can be considered to be a rigid body traveling on a plane (a flat, two-dimensional space). This body has three independent degrees of freedom consisting of two components of translation and one angle of rotation. Skidding or drifting is a good example of an automobile's three independent degrees of freedom.The position of a single railcar (engine) moving along a track has one degree of freedom because the position of the car is defined by the distance along the track. A train of rigid cars connected by hinges to an engine still has only one degree of freedom because the positions of the cars behind the engine are constrained by the shape of the track.In physics, the degree of freedom (DOF) of a mechanical system is the number of independent parameters that define its configuration. It is the number of parameters that determine the state of a physical system and is important to the analysis of systems of bodies in mechanical engineering, aeronautical engineering, robotics, and structural engineering..",
            "title": "Degrees of freedom (mechanics)",
            "url": "https://en.wikipedia.org/wiki/Degrees_of_freedom_(mechanics)"
        },
        {
            "desc_links": [
                "/wiki/Data_dredging",
                "/wiki/Cluster_analysis",
                "/wiki/Anomaly_detection",
                "/wiki/Association_rule_mining",
                "/wiki/Sequential_pattern_mining",
                "/wiki/Spatial_index",
                "/wiki/Predictive_analytics",
                "/wiki/Decision_support_system",
                "/wiki/Misnomer",
                "/wiki/Buzzword",
                "/wiki/Information_processing",
                "/wiki/Data_collection",
                "/wiki/Information_extraction",
                "/wiki/Data_warehouse",
                "/wiki/Data_analysis",
                "/wiki/Decision_support_system",
                "/wiki/Business_intelligence",
                "/wiki/Data_analysis",
                "/wiki/Analytics",
                "/wiki/Data_set",
                "/wiki/Machine_learning",
                "/wiki/Statistics",
                "/wiki/Database_system",
                "/wiki/Interdisciplinary",
                "/wiki/Computer_science",
                "/wiki/Data_management",
                "/wiki/Data_pre-processing",
                "/wiki/Statistical_model",
                "/wiki/Statistical_inference",
                "/wiki/Computational_complexity_theory",
                "/wiki/Data_visualization",
                "/wiki/Online_algorithm"
            ],
            "links": [
                "/wiki/Fair_use",
                "/wiki/Google_Book_Search_Settlement_Agreement",
                "/wiki/Database_Directive",
                "/wiki/Web_mining",
                "/wiki/Database_Directive",
                "/wiki/Hargreaves_review",
                "/wiki/Limitations_and_exceptions_to_copyright",
                "/wiki/Copyright_Directive",
                "/wiki/European_Commission",
                "/wiki/Open_access",
                "/wiki/Family_Educational_Rights_and_Privacy_Act",
                "/wiki/US_Congress",
                "/wiki/Health_Insurance_Portability_and_Accountability_Act",
                "/wiki/International_Safe_Harbor_Privacy_Principles",
                "/wiki/Edward_Snowden",
                "/wiki/Global_surveillance_disclosure",
                "/wiki/National_Security_Agency",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Personally_identifiable_information",
                "/wiki/Aggregate_function",
                "/wiki/Total_Information_Awareness",
                "/wiki/ADVISE",
                "/wiki/Examples_of_data_mining",
                "/wiki/Predictive_analytics",
                "/wiki/Predictive_Model_Markup_Language",
                "/wiki/XML",
                "/wiki/Subspace_clustering",
                "/wiki/Cross_Industry_Standard_Process_for_Data_Mining",
                "/wiki/Java_Data_Mining",
                "/wiki/List_of_computer_science_conferences#Data_Management",
                "/wiki/SIGMOD",
                "/wiki/International_Conference_on_Very_Large_Data_Bases",
                "/wiki/Association_for_Computing_Machinery",
                "/wiki/SIGKDD",
                "/wiki/Academic_journal",
                "/wiki/Overfitting",
                "/wiki/Test_set",
                "/wiki/Training_set",
                "/wiki/Receiver_operating_characteristic",
                "/wiki/Reproducibility",
                "/wiki/Statistical_hypothesis_testing",
                "/wiki/Machine_learning",
                "/wiki/Overfitting",
                "/wiki/Data_mart",
                "/wiki/Data_warehouse",
                "/wiki/Multivariate_statistics",
                "/wiki/Statistical_noise",
                "/wiki/Missing_data",
                "/wiki/SEMMA",
                "/wiki/Cross_Industry_Standard_Process_for_Data_Mining",
                "/wiki/Data",
                "/wiki/Bayes%27_theorem",
                "/wiki/Regression_analysis",
                "/wiki/Data_set",
                "/wiki/Neural_networks",
                "/wiki/Cluster_analysis",
                "/wiki/Genetic_algorithms",
                "/wiki/Decision_tree_learning",
                "/wiki/Decision_rules",
                "/wiki/Support_vector_machines",
                "/wiki/Applied_statistics",
                "/wiki/Database_management",
                "/wiki/AAAI",
                "/wiki/Usama_Fayyad",
                "/wiki/Data_Mining_and_Knowledge_Discovery",
                "/wiki/Gregory_I._Piatetsky-Shapiro",
                "/wiki/Artificial_intelligence",
                "/wiki/Machine_learning",
                "/wiki/Michael_Lovell",
                "/wiki/Review_of_Economic_Studies",
                "/wiki/Data_dredging",
                "/wiki/Cluster_analysis",
                "/wiki/Anomaly_detection",
                "/wiki/Association_rule_mining",
                "/wiki/Sequential_pattern_mining",
                "/wiki/Spatial_index",
                "/wiki/Predictive_analytics",
                "/wiki/Decision_support_system",
                "/wiki/Misnomer",
                "/wiki/Buzzword",
                "/wiki/Information_processing",
                "/wiki/Data_collection",
                "/wiki/Information_extraction",
                "/wiki/Data_warehouse",
                "/wiki/Data_analysis",
                "/wiki/Decision_support_system",
                "/wiki/Business_intelligence",
                "/wiki/Data_analysis",
                "/wiki/Analytics",
                "/wiki/Data_set",
                "/wiki/Machine_learning",
                "/wiki/Statistics",
                "/wiki/Database_system",
                "/wiki/Interdisciplinary",
                "/wiki/Computer_science",
                "/wiki/Data_management",
                "/wiki/Data_pre-processing",
                "/wiki/Statistical_model",
                "/wiki/Statistical_inference",
                "/wiki/Computational_complexity_theory",
                "/wiki/Data_visualization",
                "/wiki/Online_algorithm"
            ],
            "text": "Several researchers and organizations have conducted reviews of data mining tools and surveys of data miners. These identify some of the strengths and weaknesses of the software packages. They also provide an overview of the behaviors, preferences and views of data miners. Some of these reports include:The following applications are available under proprietary licenses.The following applications are available under free/open source licenses. Public access to application source code is also available.By contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that content mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one being text and data mining.[39]Due to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. Where a database is pure data in Europe there is likely to be no copyright, but database rights may exist so data mining becomes subject to regulations by the Database Directive. On the recommendation of the Hargreaves review this led to the UK government to amend its copyright law in 2014[36] to allow content mining as a limitation and exception. Only the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[37] The focus on the solution to this legal issue being licences and not limitations and exceptions led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[38]U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. Use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.\"[35] This underscores the necessity for data anonymity in data aggregation and mining practices.Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.-E.U. Safe Harbor Principles currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement have failed.[citation needed]The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial, emotional, or bodily harm to the indicated individual. In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies.[34]Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[29] However, even \"de-identified\"/\"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[33]It is recommended that an individual is made aware of the following before data are collected:[29]Data mining requires data preparation which can uncover information or patterns which may compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[29] This is not data mining per se, but a result of the preparation of data before \u2013 and for the purposes of \u2013 the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[30][31][32]The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[26] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[27][28]While the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[25]Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.For exchanging the extracted models \u2013 in particular for use in predictive analytics\u00a0\u2013 the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[24]There have been some efforts to define standards for the data mining process, for example the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006, but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data BasesComputer science conferences on data mining include:The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[20][21] Since 1989 this ACM SIG has hosted an annual international conference and published its proceedings,[22] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".[23]If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.Data mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening.[19]Data mining involves six common classes of tasks:[5]Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[15] The only other data mining standard named in these polls was SEMMA. However, 3\u20134 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[16][17] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[18]or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.It exists, however, in many variations on this theme, such as the Cross Industry Standard Process for Data Mining (CRISP-DM) which defines six phases:The knowledge discovery in databases (KDD) process is commonly defined with the stages:The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology has dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns[14] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever larger data sets.In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDDD Newsletter SIGKDD Explorations.[13] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase \"database mining\"\u2122, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[11] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[12] Currently, the terms data mining and knowledge discovery are used interchangeably.In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies 1983. Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative).[10]The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.The term is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics \u2013 or, when referring to actual methods, artificial intelligence and machine learning \u2013 are more appropriate.Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] It is an essential process where intelligent methods are applied to extract data patterns.[1][2] It is an interdisciplinary subfield of computer science.[1][3][4] The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.[1] Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5]",
            "title": "Data mining",
            "url": "https://en.wikipedia.org/wiki/Data_mining"
        },
        {
            "desc_links": [
                "/wiki/Open_data",
                "/wiki/Data",
                "/wiki/Table_(database)",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Column_(database)",
                "/wiki/Row_(database)",
                "/wiki/Space_probe",
                "/wiki/Data_processing",
                "/wiki/Big_data"
            ],
            "links": [
                "/wiki/Statistical",
                "/wiki/Statistics",
                "/wiki/Sampling_(statistics)",
                "/wiki/Statistical_population",
                "/wiki/Algorithms",
                "/wiki/Software",
                "/wiki/SPSS",
                "/wiki/Imputation_(statistics)",
                "/wiki/Real_number",
                "/wiki/Integer",
                "/wiki/Nominal_data",
                "/wiki/Number",
                "/wiki/Level_of_measurement",
                "/wiki/Missing_values",
                "/wiki/Statistical_measure",
                "/wiki/Standard_deviation",
                "/wiki/Kurtosis",
                "/wiki/Open_data",
                "/wiki/Data",
                "/wiki/Table_(database)",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Column_(database)",
                "/wiki/Row_(database)",
                "/wiki/Space_probe",
                "/wiki/Data_processing",
                "/wiki/Big_data"
            ],
            "text": "Several classic data sets have been used extensively in the statistical literature:In statistics, data sets usually come from actual observations obtained by sampling a statistical population, and each row corresponds to the observations on one element of that population. Data sets may further be generated by algorithms for the purpose of testing certain kinds of software. Some modern statistical analysis software such as SPSS still present their data in the classical data set fashion. If data is missing or suspicious an imputation method may be used to complete a data set.[6]The values may be numbers, such as real numbers or integers, for example representing a person's height in centimeters, but may also be nominal data (i.e., not consisting of numerical values), for example representing a person's ethnicity. More generally, values may be of any of the kinds described as a level of measurement. For each variable, the values are normally all of the same kind. However, there may also be missing values, which must be indicated in some way.Several characteristics define a data set's structure and properties. These include the number and types of the attributes or variables, and various statistical measures applicable to them, such as standard deviation and kurtosis.[5]In the open data discipline, data set is the unit to measure the information released in a public open data repository. The European Open Data portal aggregates more than half a million data sets.[2] In this field other definitions have been proposed [3] but currently there is not an official one. Some other issues (real-time data sources,[4] non-relational data sets, etc.) increases the difficulty to reach a consensus about it.A data set (or dataset) is a collection of data. Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question. The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set. Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows. The term data set may also be used more loosely, to refer to the data in a collection of closely related tables, corresponding to a particular experiment or event. An example of this type is the data sets collected by space agencies performing experiments with instruments aboard space probes. Data sets that are so large that traditional data processing applications are inadequate to deal with them are known as big data.[1]",
            "title": "Data set",
            "url": "https://en.wikipedia.org/wiki/Data_set"
        },
        {
            "desc_links": [
                "/wiki/Umbrella_term",
                "/wiki/Computer_programming",
                "/wiki/Genomics",
                "/wiki/Gene",
                "/wiki/Nucleotide",
                "/wiki/Single-nucleotide_polymorphism",
                "/wiki/Nucleic_acid",
                "/wiki/Protein",
                "/wiki/Proteomics",
                "/wiki/Help:IPA/English",
                "/wiki/File:En-us-bioinformatics.ogg",
                "/wiki/Interdisciplinary",
                "/wiki/Software_tool",
                "/wiki/Biology",
                "/wiki/Computer_science",
                "/wiki/Biology",
                "/wiki/Mathematics",
                "/wiki/Statistics",
                "/wiki/Engineering",
                "/wiki/In_silico"
            ],
            "links": [
                "/wiki/Intelligent_Systems_for_Molecular_Biology",
                "/wiki/European_Conference_on_Computational_Biology",
                "/wiki/Research_in_Computational_Molecular_Biology",
                "/wiki/Massive_open_online_course",
                "/wiki/Coursera",
                "/wiki/University_of_California,_San_Diego",
                "/wiki/Johns_Hopkins_University",
                "/wiki/EdX",
                "/wiki/Harvard_University",
                "/wiki/Rosalind_(education_platform)",
                "/wiki/Swiss_Institute_of_Bioinformatics",
                "/wiki/Canadian_Bioinformatics_Workshops",
                "/wiki/Creative_Commons",
                "/wiki/Raspberry_Pi",
                "/wiki/BioCompute_Object",
                "/wiki/Food_and_Drug_Administration",
                "/wiki/National_Institutes_of_Health",
                "/wiki/Human_Variome_Project",
                "/wiki/European_Federation_for_Medical_Informatics",
                "/wiki/Stanford_University",
                "/wiki/New_York_Genome_Center",
                "/wiki/George_Washington_University",
                "/wiki/Galaxy_(computational_biology)",
                "/wiki/Kepler_scientific_workflow_system",
                "/wiki/Apache_Taverna",
                "/wiki/UGENE",
                "/wiki/Anduril_(workflow_engine)",
                "/wiki/High-performance_Integrated_Virtual_Environment",
                "/wiki/Bioinformatics_workflow_management_systems",
                "/wiki/Workflow_management_system",
                "/wiki/European_Bioinformatics_Institute",
                "/wiki/Sequence_alignment_software",
                "/wiki/Multiple_sequence_alignment",
                "/wiki/Bioinformatics#Sequence_analysis",
                "/wiki/Service-orientation",
                "/wiki/Bioinformatics_workflow_management_systems",
                "/wiki/SOAP",
                "/wiki/REST",
                "/wiki/List_of_open-source_bioinformatics_software",
                "/wiki/Bioconductor",
                "/wiki/BioPerl",
                "/wiki/Biopython",
                "/wiki/BioJava",
                "/wiki/BioJS",
                "/wiki/BioRuby",
                "/wiki/Bioclipse",
                "/wiki/EMBOSS",
                "/wiki/.NET_Bio",
                "/wiki/Orange_(software)",
                "/wiki/Apache_Taverna",
                "/wiki/UGENE",
                "/wiki/GenoCAD",
                "/wiki/Open_Bioinformatics_Foundation",
                "/wiki/Bioinformatics_Open_Source_Conference",
                "/wiki/Free_and_open-source_software",
                "/wiki/Algorithm",
                "/wiki/In_silico",
                "/wiki/Open_code",
                "/wiki/Plug-in_(computing)",
                "/wiki/De_facto",
                "/wiki/Software",
                "/wiki/List_of_bioinformatics_companies",
                "/wiki/OBO_Foundry",
                "/wiki/Gene_ontology",
                "/wiki/Biodiversity",
                "/wiki/Taxonomic_database",
                "/wiki/Microbiome",
                "/wiki/Phylogenetics",
                "/wiki/Niche_modelling",
                "/wiki/Species_richness",
                "/wiki/DNA_barcoding",
                "/wiki/Speciesism",
                "/wiki/Flow_cytometry",
                "/wiki/Medical_imaging",
                "/wiki/Accuracy",
                "/wiki/Objectivity_(science)",
                "/wiki/Diagnostics",
                "/wiki/Statistics",
                "/wiki/Computational_linguistics",
                "/wiki/Algorithm",
                "/wiki/Interactome",
                "/wiki/X-ray_crystallography",
                "/wiki/Protein_nuclear_magnetic_resonance_spectroscopy",
                "/wiki/Protein%E2%80%93protein_interaction",
                "/wiki/Protein%E2%80%93protein_docking",
                "/wiki/Computer_simulation",
                "/wiki/Cell_(biology)",
                "/wiki/Metabolic_network",
                "/wiki/Enzyme",
                "/wiki/Metabolism",
                "/wiki/Signal_transduction",
                "/wiki/Gene_regulatory_network",
                "/wiki/Artificial_life",
                "/wiki/Biological_network",
                "/wiki/Metabolic_network",
                "/wiki/Interactome",
                "/wiki/Leghemoglobin",
                "/wiki/Homology_(biology)",
                "/wiki/Amino_acid",
                "/wiki/Primary_structure",
                "/wiki/Bovine_spongiform_encephalopathy",
                "/wiki/Mad_Cow_Disease",
                "/wiki/Prion",
                "/wiki/Secondary_structure",
                "/wiki/Tertiary_structure",
                "/wiki/Quaternary_structure",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Chromosome_conformation_capture",
                "/wiki/Hi-C_(experiment)",
                "/wiki/ChIA-PET",
                "/wiki/Nuclear_organization",
                "/wiki/Topologically_Associating_Domain",
                "/wiki/Cell_nucleus",
                "/wiki/Regulation_of_gene_expression",
                "/wiki/RNA_splicing",
                "/wiki/Mitochondrion",
                "/wiki/Cellular_respiration",
                "/wiki/Metabolism",
                "/wiki/Protein_function_prediction",
                "/wiki/Protein_subcellular_localization_prediction",
                "/wiki/Organelle",
                "/wiki/Cancer",
                "/wiki/Gene_ontology",
                "/wiki/Biological_database",
                "/wiki/Microarray",
                "/wiki/Cell_cycle",
                "/wiki/Cluster_analysis",
                "/wiki/Regulatory_elements",
                "/wiki/K-means_clustering",
                "/wiki/Self-organizing_map",
                "/wiki/Hierarchical_clustering",
                "/wiki/Consensus_clustering",
                "/wiki/Sequence_motif",
                "/wiki/Enhancer_(genetics)",
                "/wiki/Chromosome_conformation_capture",
                "/wiki/Regulation_of_gene_expression",
                "/wiki/Hormone",
                "/wiki/Protein",
                "/wiki/Protein_microarray",
                "/wiki/Mass_spectrometry",
                "/wiki/Proteomics",
                "/wiki/Immunohistochemistry",
                "/wiki/Tissue_microarray",
                "/wiki/Gene_expression",
                "/wiki/Messenger_RNA",
                "/wiki/DNA_microarray",
                "/wiki/Expressed_sequence_tag",
                "/wiki/Serial_analysis_of_gene_expression",
                "/wiki/Massively_parallel_signature_sequencing",
                "/wiki/RNA-Seq",
                "/wiki/Signal_(information_theory)",
                "/wiki/Noise",
                "/wiki/Epithelial",
                "/wiki/Lesion",
                "/wiki/Exome",
                "/wiki/Cancer",
                "/wiki/Point_mutation",
                "/wiki/Gene",
                "/wiki/Human_genome",
                "/wiki/Germline",
                "/wiki/Oligonucleotide",
                "/wiki/Comparative_genomic_hybridization",
                "/wiki/Single-nucleotide_polymorphism",
                "/wiki/Terabyte",
                "/wiki/Noise",
                "/wiki/Hidden_Markov_model",
                "/wiki/Copy_number_variation",
                "/wiki/Diabetes_mellitus",
                "/wiki/Infertility",
                "/wiki/Breast_cancer",
                "/wiki/Alzheimer%27s_Disease",
                "/wiki/Homology_(biology)",
                "/wiki/Genes",
                "/wiki/Homology_(biology)#Orthology",
                "/wiki/Endosymbiosis",
                "/wiki/Heuristics",
                "/wiki/Approximation_algorithms",
                "/wiki/Markov_chain_Monte_Carlo",
                "/wiki/Bayesian_analysis",
                "/wiki/Computer_science",
                "/wiki/Genetic_algorithm",
                "/wiki/Evolutionary_tree",
                "/wiki/Evolutionary_biology",
                "/wiki/Species",
                "/wiki/Informatics_(academic_field)",
                "/wiki/ENCODE",
                "/wiki/The_Institute_for_Genomic_Research",
                "/wiki/Haemophilus_influenzae",
                "/wiki/Owen_White",
                "/wiki/GeneMark",
                "/wiki/Haemophilus_influenzae",
                "/wiki/Genomics",
                "/wiki/Genome_project#Genome_annotation",
                "/wiki/DNA_sequencing",
                "/wiki/Shotgun_sequencing",
                "/wiki/The_Institute_for_Genomic_Research",
                "/wiki/Haemophilus_influenzae",
                "/wiki/Human_genome",
                "/wiki/DNA_sequencing",
                "/wiki/Algorithm",
                "/wiki/Base_calling",
                "/wiki/Phi_X_174",
                "/wiki/Sequencing",
                "/wiki/DNA_sequence",
                "/wiki/Protein",
                "/wiki/Species",
                "/wiki/Molecular_systematics",
                "/wiki/Phylogenetic_tree",
                "/wiki/Computer_program",
                "/wiki/BLAST",
                "/wiki/Nucleotide",
                "/wiki/Sequence_alignment",
                "/wiki/Algorithm",
                "/wiki/Graph_theory",
                "/wiki/Artificial_intelligence",
                "/wiki/Soft_computing",
                "/wiki/Data_mining",
                "/wiki/Image_processing",
                "/wiki/Computer_simulation",
                "/wiki/Discrete_mathematics",
                "/wiki/Control_theory",
                "/wiki/System_theory",
                "/wiki/Information_theory",
                "/wiki/Statistics",
                "/wiki/Biological_computation",
                "/wiki/Computational_biology",
                "/wiki/Bioengineering",
                "/wiki/Biology",
                "/wiki/Computer",
                "/wiki/Human_Genome_Project",
                "/wiki/DNA",
                "/wiki/Pattern_recognition",
                "/wiki/Data_mining",
                "/wiki/Machine_learning",
                "/wiki/Biological_Data_Visualization",
                "/wiki/Sequence_alignment",
                "/wiki/Gene_finding",
                "/wiki/Genome_assembly",
                "/wiki/Drug_design",
                "/wiki/Drug_discovery",
                "/wiki/Protein_structural_alignment",
                "/wiki/Protein_structure_prediction",
                "/wiki/Gene_expression",
                "/wiki/Protein%E2%80%93protein_interactions",
                "/wiki/Genome-wide_association_studies",
                "/wiki/Evolution",
                "/wiki/Cellular_model",
                "/wiki/Amino_acid_sequence",
                "/wiki/Protein_domain",
                "/wiki/Protein_structure",
                "/wiki/Computational_biology",
                "/wiki/Protein_sequences",
                "/wiki/Frederick_Sanger",
                "/wiki/Insulin",
                "/wiki/Margaret_Oakley_Dayhoff",
                "/wiki/David_Lipman",
                "/wiki/National_Center_for_Biotechnology_Information",
                "/wiki/Elvin_A._Kabat",
                "/wiki/Paulien_Hogeweg",
                "/wiki/Biophysics",
                "/wiki/Biochemistry",
                "/wiki/Molecular_biology",
                "/wiki/Image_processing",
                "/wiki/Signal_processing",
                "/wiki/Mutation",
                "/wiki/Text_mining",
                "/wiki/Ontology_(information_science)",
                "/wiki/Umbrella_term",
                "/wiki/Computer_programming",
                "/wiki/Genomics",
                "/wiki/Gene",
                "/wiki/Nucleotide",
                "/wiki/Single-nucleotide_polymorphism",
                "/wiki/Nucleic_acid",
                "/wiki/Protein",
                "/wiki/Proteomics"
            ],
            "text": "There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera's Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX's Data Analysis for Life Sciences XSeries (Harvard). University of Southern California offers a Masters In Translational Bioinformatics focusing on biomedical applications.Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273\u03c0 project or 4273pi project[48] also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils.[49][50] 4273\u03c0 is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273\u03c0 operating system.[51][52]In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as a both a \u201cstandard trial use\u201d document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.[46][47]It was decided that the BioCompute paradigm would be in the form of digital \u2018lab notebooks\u2019 which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while it furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.[45]In 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics.[43] Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm.[44] These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.A bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed toBasic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis).[42] The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the WikiOpener extension. This system allows the database to be accessed and updated by all experts in the field.[41]The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation[39] have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.[40]Many free and open-source software tools have existed and continued to grow since the 1980s.[39] The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.Databases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.The OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.Biological ontologies are directed acyclic graphs of controlled vocabularies. They are designed to capture biological concepts and descriptions in a way that can be easily categorised and analysed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:The area of research draws from statistics and computational linguistics.The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:Other interactions encountered in the field include Protein\u2013ligand (including drug) and protein\u2013peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein\u2013protein interactions only based on these 3D shapes, without performing protein\u2013protein interaction experiments. A variety of methods have been developed to tackle the protein\u2013protein docking problem, though it seems that there is still much work to be done in this field.Systems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.Network analysis seeks to understand the relationships within biological networks such as metabolic or protein\u2013protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.[38]One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably.Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy \u2013 a.k.a. Mad Cow Disease \u2013 prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of secondary, tertiary and quaternary structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.[citation needed]Data from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.[37]The localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellualr location databases, and prediction tools.[35][36]Microscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.Several approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, cellular compartment, has been devised to capture subcellular localization in many biological databases.Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.For example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.Regulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.[34]The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as \"Whole Transcriptome Shotgun Sequencing\" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies.[33] Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.With the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.[32]Two important principles can be used in the analysis of cancer genomes bioinformatically pertaining to the identification of mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second cancer contains driver mutations which need to be distinguished from passengers.[31]In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes,[24] infertility,[25] breast cancer[26] or Alzheimer's Disease.[27] Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases.[28] Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits.[29] Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.[30]Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.[23]Many of these studies are based on the homology detection and protein families computation.[22]The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.[21] Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.Future work endeavours to reconstruct the now more complex tree of life.Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).The first description of a comprehensive genome annotation system was published in 1995 [19] by the team at The Institute for Genomic Research that performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae.[19] Owen White designed and built a software system to identify the genes encoding all proteins, transfer RNAs, ribosomal RNAs (and other sites) and to make initial functional assignments. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.Most DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae)[19] generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.Before sequences can be analyzed they have to be obtained. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.Since the Phage \u03a6-X174 was sequenced in 1977,[17] the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides.[18] These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself.Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence[16], soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.Bioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein\u2013protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures.[15] The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the \"mother and father of bioinformatics.\"[11] Dayhoff compiled one of the first protein sequence databases, initially published as books[12] and pioneered methods of sequence alignment and molecular evolution.[13] Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.[14]Historically, the term bioinformatics did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems.[8][9][10] This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).[8]Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA,[2] RNA,[2][3] proteins[4] as well as biomolecular interactions.[5][6][7]Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis \"pipelines\" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences, called proteomics.[1]",
            "title": "Bioinformatics",
            "url": "https://en.wikipedia.org/wiki/Bioinformatics"
        },
        {
            "desc_links": [
                "/wiki/Data_dredging",
                "/wiki/Cluster_analysis",
                "/wiki/Anomaly_detection",
                "/wiki/Association_rule_mining",
                "/wiki/Sequential_pattern_mining",
                "/wiki/Spatial_index",
                "/wiki/Predictive_analytics",
                "/wiki/Decision_support_system",
                "/wiki/Misnomer",
                "/wiki/Buzzword",
                "/wiki/Information_processing",
                "/wiki/Data_collection",
                "/wiki/Information_extraction",
                "/wiki/Data_warehouse",
                "/wiki/Data_analysis",
                "/wiki/Decision_support_system",
                "/wiki/Business_intelligence",
                "/wiki/Data_analysis",
                "/wiki/Analytics",
                "/wiki/Data_set",
                "/wiki/Machine_learning",
                "/wiki/Statistics",
                "/wiki/Database_system",
                "/wiki/Interdisciplinary",
                "/wiki/Computer_science",
                "/wiki/Data_management",
                "/wiki/Data_pre-processing",
                "/wiki/Statistical_model",
                "/wiki/Statistical_inference",
                "/wiki/Computational_complexity_theory",
                "/wiki/Data_visualization",
                "/wiki/Online_algorithm"
            ],
            "links": [
                "/wiki/Fair_use",
                "/wiki/Google_Book_Search_Settlement_Agreement",
                "/wiki/Database_Directive",
                "/wiki/Web_mining",
                "/wiki/Database_Directive",
                "/wiki/Hargreaves_review",
                "/wiki/Limitations_and_exceptions_to_copyright",
                "/wiki/Copyright_Directive",
                "/wiki/European_Commission",
                "/wiki/Open_access",
                "/wiki/Family_Educational_Rights_and_Privacy_Act",
                "/wiki/US_Congress",
                "/wiki/Health_Insurance_Portability_and_Accountability_Act",
                "/wiki/International_Safe_Harbor_Privacy_Principles",
                "/wiki/Edward_Snowden",
                "/wiki/Global_surveillance_disclosure",
                "/wiki/National_Security_Agency",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Personally_identifiable_information",
                "/wiki/Aggregate_function",
                "/wiki/Total_Information_Awareness",
                "/wiki/ADVISE",
                "/wiki/Examples_of_data_mining",
                "/wiki/Predictive_analytics",
                "/wiki/Predictive_Model_Markup_Language",
                "/wiki/XML",
                "/wiki/Subspace_clustering",
                "/wiki/Cross_Industry_Standard_Process_for_Data_Mining",
                "/wiki/Java_Data_Mining",
                "/wiki/List_of_computer_science_conferences#Data_Management",
                "/wiki/SIGMOD",
                "/wiki/International_Conference_on_Very_Large_Data_Bases",
                "/wiki/Association_for_Computing_Machinery",
                "/wiki/SIGKDD",
                "/wiki/Academic_journal",
                "/wiki/Overfitting",
                "/wiki/Test_set",
                "/wiki/Training_set",
                "/wiki/Receiver_operating_characteristic",
                "/wiki/Reproducibility",
                "/wiki/Statistical_hypothesis_testing",
                "/wiki/Machine_learning",
                "/wiki/Overfitting",
                "/wiki/Data_mart",
                "/wiki/Data_warehouse",
                "/wiki/Multivariate_statistics",
                "/wiki/Statistical_noise",
                "/wiki/Missing_data",
                "/wiki/SEMMA",
                "/wiki/Cross_Industry_Standard_Process_for_Data_Mining",
                "/wiki/Data",
                "/wiki/Bayes%27_theorem",
                "/wiki/Regression_analysis",
                "/wiki/Data_set",
                "/wiki/Neural_networks",
                "/wiki/Cluster_analysis",
                "/wiki/Genetic_algorithms",
                "/wiki/Decision_tree_learning",
                "/wiki/Decision_rules",
                "/wiki/Support_vector_machines",
                "/wiki/Applied_statistics",
                "/wiki/Database_management",
                "/wiki/AAAI",
                "/wiki/Usama_Fayyad",
                "/wiki/Data_Mining_and_Knowledge_Discovery",
                "/wiki/Gregory_I._Piatetsky-Shapiro",
                "/wiki/Artificial_intelligence",
                "/wiki/Machine_learning",
                "/wiki/Michael_Lovell",
                "/wiki/Review_of_Economic_Studies",
                "/wiki/Data_dredging",
                "/wiki/Cluster_analysis",
                "/wiki/Anomaly_detection",
                "/wiki/Association_rule_mining",
                "/wiki/Sequential_pattern_mining",
                "/wiki/Spatial_index",
                "/wiki/Predictive_analytics",
                "/wiki/Decision_support_system",
                "/wiki/Misnomer",
                "/wiki/Buzzword",
                "/wiki/Information_processing",
                "/wiki/Data_collection",
                "/wiki/Information_extraction",
                "/wiki/Data_warehouse",
                "/wiki/Data_analysis",
                "/wiki/Decision_support_system",
                "/wiki/Business_intelligence",
                "/wiki/Data_analysis",
                "/wiki/Analytics",
                "/wiki/Data_set",
                "/wiki/Machine_learning",
                "/wiki/Statistics",
                "/wiki/Database_system",
                "/wiki/Interdisciplinary",
                "/wiki/Computer_science",
                "/wiki/Data_management",
                "/wiki/Data_pre-processing",
                "/wiki/Statistical_model",
                "/wiki/Statistical_inference",
                "/wiki/Computational_complexity_theory",
                "/wiki/Data_visualization",
                "/wiki/Online_algorithm"
            ],
            "text": "Several researchers and organizations have conducted reviews of data mining tools and surveys of data miners. These identify some of the strengths and weaknesses of the software packages. They also provide an overview of the behaviors, preferences and views of data miners. Some of these reports include:The following applications are available under proprietary licenses.The following applications are available under free/open source licenses. Public access to application source code is also available.By contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that content mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one being text and data mining.[39]Due to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. Where a database is pure data in Europe there is likely to be no copyright, but database rights may exist so data mining becomes subject to regulations by the Database Directive. On the recommendation of the Hargreaves review this led to the UK government to amend its copyright law in 2014[36] to allow content mining as a limitation and exception. Only the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[37] The focus on the solution to this legal issue being licences and not limitations and exceptions led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[38]U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. Use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.\"[35] This underscores the necessity for data anonymity in data aggregation and mining practices.Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.-E.U. Safe Harbor Principles currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement have failed.[citation needed]The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial, emotional, or bodily harm to the indicated individual. In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies.[34]Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[29] However, even \"de-identified\"/\"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[33]It is recommended that an individual is made aware of the following before data are collected:[29]Data mining requires data preparation which can uncover information or patterns which may compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[29] This is not data mining per se, but a result of the preparation of data before \u2013 and for the purposes of \u2013 the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[30][31][32]The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[26] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[27][28]While the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[25]Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.For exchanging the extracted models \u2013 in particular for use in predictive analytics\u00a0\u2013 the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[24]There have been some efforts to define standards for the data mining process, for example the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006, but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.Data mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data BasesComputer science conferences on data mining include:The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[20][21] Since 1989 this ACM SIG has hosted an annual international conference and published its proceedings,[22] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".[23]If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.Data mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening.[19]Data mining involves six common classes of tasks:[5]Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[15] The only other data mining standard named in these polls was SEMMA. However, 3\u20134 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[16][17] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[18]or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.It exists, however, in many variations on this theme, such as the Cross Industry Standard Process for Data Mining (CRISP-DM) which defines six phases:The knowledge discovery in databases (KDD) process is commonly defined with the stages:The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology has dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns[14] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever larger data sets.In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDDD Newsletter SIGKDD Explorations.[13] The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase \"database mining\"\u2122, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[11] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[12] Currently, the terms data mining and knowledge discovery are used interchangeably.In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies 1983. Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative).[10]The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.The term is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[8] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[9] Often the more general terms (large scale) data analysis and analytics \u2013 or, when referring to actual methods, artificial intelligence and machine learning \u2013 are more appropriate.Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] It is an essential process where intelligent methods are applied to extract data patterns.[1][2] It is an interdisciplinary subfield of computer science.[1][3][4] The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.[1] Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5]",
            "title": "Data mining",
            "url": "https://en.wikipedia.org/wiki/Data_mining"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Chemometrics",
            "url": "https://en.wikipedia.org/wiki/Chemometrics"
        },
        {
            "desc_links": [
                "/wiki/Psychologists",
                "/wiki/Human_resources",
                "/wiki/Learning_and_development",
                "/wiki/Measurement",
                "/wiki/Trait_theory",
                "/wiki/Educational_measurement",
                "/wiki/Questionnaire",
                "/wiki/Test_(student_assessment)",
                "/wiki/Personality_tests",
                "/wiki/Item_response_theory",
                "/wiki/Intraclass_correlation",
                "/wiki/Psychological",
                "/wiki/Measurement",
                "/wiki/National_Council_on_Measurement_in_Education"
            ],
            "links": [
                "/wiki/Artificial_intelligence",
                "/wiki/Comparative_psychology",
                "/wiki/Evolutionary_psychology",
                "/wiki/Evaluation",
                "/wiki/Educational_evaluation",
                "/wiki/Joint_Committee_on_Standards_for_Educational_Evaluation",
                "/wiki/Standards_for_Educational_and_Psychological_Testing",
                "/wiki/Psychological_testing",
                "/wiki/Professional_certification",
                "/wiki/Test_(student_assessment)",
                "/wiki/Program_evaluation",
                "/wiki/Validity_(statistics)",
                "/wiki/Reliability_(statistics)",
                "/wiki/Quality_(business)",
                "/wiki/Standards_organization",
                "/wiki/Bias",
                "/wiki/Item_response_theory",
                "/wiki/Latent_trait",
                "/wiki/Concurrent_validity",
                "/wiki/Predictive_validity",
                "/wiki/Construct_validity",
                "/wiki/Content_validity",
                "/wiki/Job_analysis",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Spearman%E2%80%93Brown_prediction_formula",
                "/wiki/Cronbach%27s_%CE%B1",
                "/wiki/Mean",
                "/wiki/Intra-class_correlation",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Reliability_(psychometric)",
                "/wiki/Test_validity",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Factor_analysis",
                "/wiki/Multidimensional_scaling",
                "/wiki/Data_clustering",
                "/wiki/Structural_equation_modeling",
                "/wiki/Path_analysis_(statistics)",
                "/wiki/Covariance_matrix",
                "/wiki/Classical_test_theory",
                "/wiki/Item_response_theory",
                "/wiki/Rasch_model",
                "/wiki/Personality_test",
                "/wiki/Minnesota_Multiphasic_Personality_Inventory",
                "/wiki/Big_five_personality_traits",
                "/wiki/Personality_and_Preference_Inventory",
                "/wiki/Myers-Briggs_Type_Indicator",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Likert_scale",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Literacy",
                "/wiki/Mathematics",
                "/wiki/Classical_test_theory",
                "/wiki/Item_Response_Theory",
                "/wiki/Rasch_model",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Intelligence_(trait)",
                "/wiki/Stanford-Binet_IQ_test",
                "/wiki/Alfred_Binet",
                "/wiki/General_intelligence_factor",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Rasch_model",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Covariance_matrix",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Stanley_Smith_Stevens",
                "/wiki/Levels_of_measurement",
                "/wiki/Karl_Pearson",
                "/wiki/Carl_Brigham",
                "/wiki/L._L._Thurstone",
                "/wiki/Anne_Anastasi",
                "/wiki/Georg_Rasch",
                "/wiki/Eugene_Galanter",
                "/wiki/Johnson_O%27Connor",
                "/wiki/Frederic_M._Lord",
                "/wiki/Ledyard_R_Tucker",
                "/wiki/Arthur_Jensen",
                "/wiki/David_Andrich",
                "/wiki/Attitude_(psychology)",
                "/wiki/Belief",
                "/wiki/Academic_achievement",
                "/wiki/Physical_sciences",
                "/wiki/Activism",
                "/wiki/L._L._Thurstone",
                "/wiki/Law_of_comparative_judgment",
                "/wiki/Ernst_Heinrich_Weber",
                "/wiki/Gustav_Fechner",
                "/wiki/Factor_analysis",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Leopold_Szondi",
                "/wiki/Psychophysics",
                "/wiki/Intelligence_(trait)",
                "/wiki/Anthropometric",
                "/wiki/The_Origin_of_Species",
                "/wiki/Species",
                "/wiki/Charles_Darwin",
                "/wiki/Sir_Francis_Galton",
                "/wiki/James_McKeen_Cattell",
                "/wiki/J.E._Herbart",
                "/wiki/E.H._Weber",
                "/wiki/Gustav_Fechner",
                "/wiki/Wundt",
                "/wiki/Experimental_psychology",
                "/wiki/Psychologists",
                "/wiki/Human_resources",
                "/wiki/Learning_and_development",
                "/wiki/Measurement",
                "/wiki/Trait_theory",
                "/wiki/Educational_measurement",
                "/wiki/Questionnaire",
                "/wiki/Test_(student_assessment)",
                "/wiki/Personality_tests",
                "/wiki/Item_response_theory",
                "/wiki/Intraclass_correlation",
                "/wiki/Psychological",
                "/wiki/Measurement",
                "/wiki/National_Council_on_Measurement_in_Education"
            ],
            "text": "The evaluation of abilities, traits and learning evolution of machines has been mostly unrelated to the case of humans and non-human animals, with specific approaches in the area of artificial intelligence. A more integrated approach, under the name of universal psychometrics, has also been proposed.[30]Psychometrics addresses human abilities, attitudes, traits and educational evolution. Notably, the study of behavior, mental processes and abilities of non-human animals is usually addressed by comparative psychology, or with a continuum between non-human animals and the rest of animals by evolutionary psychology. Nonetheless there are some advocators for a more gradual transition between the approach taken for humans and the approach taken for (non-human) animals.[26][27][28] [29]Each publication presents and elaborates a set of standards for use in a variety of educational settings. The standards provide guidelines for designing, implementing, assessing and improving the identified form of evaluation.[25] Each of the standards has been placed in one of four fundamental categories to promote educational evaluations that are proper, useful, feasible, and accurate. In these sets of standards, validity and reliability considerations are covered under the accuracy topic. For example, the student accuracy standards help ensure that student evaluations will provide sound, accurate, and credible information about student learning and performance.In the field of evaluation, and in particular educational evaluation, the Joint Committee on Standards for Educational Evaluation[21] has published three sets of standards for evaluations. The Personnel Evaluation Standards[22] was published in 1988, The Program Evaluation Standards (2nd edition)[23] was published in 1994, and The Student Evaluation Standards[24] was published in 2003.In 2014, the American Educational Research Association (AERA), American Psychological Association (APA), and National Council on Measurement in Education (NCME) published a revision of the Standards for Educational and Psychological Testing,[20] which describes standards for test development, evaluation, and use. The Standards cover essential topics in testing including validity, reliability/errors of measurement, and fairness in testing. The book also establishes standards related to testing operations including test design and development, scores, scales, norms, score linking, cut scores, test administration, scoring, reporting, score interpretation, test documentation, and rights and responsibilities of test takers and test users. Finally, the Standards cover topics related to testing applications, including psychological testing and assessment, workplace testing and credentialing, educational testing and assessment, and testing in program evaluation and public policy.The considerations of validity and reliability typically are viewed as essential elements for determining the quality of any test. However, professional and practitioner associations frequently have placed these concerns within broader contexts when developing standards and making overall judgments about the quality of any test as a whole within a given context. A consideration of concern in many applied research settings is whether or not the metric of a given psychological inventory is meaningful or arbitrary.[19]Many psychometricians are also concerned with finding and eliminating test bias from their psychological tests. Test bias is a form of systematic (i.e., non-random) error which leads to examinees from one demographic group having an unwarranted advantage over examinees from another demographic group.[15] According to leading experts, test bias may cause differences in average scores across demographic groups, but differences in group scores are not sufficient evidence that test bias is actually present because the test could be measuring real differences among groups.[16][17] Psychometricians use sophisticated scientific methods to search for test bias and eliminate it. Research shows that it is usually impossible for people reading a test item to accurately determine whether it is biased or not.[18]Item response theory models the relationship between latent traits and responses to test items. Among other advantages, IRT provides a basis for obtaining an estimate of the location of a test-taker on a given latent trait as well as the standard error of measurement of that location. For example, a university student's knowledge of history can be deduced from his or her score on a university test and then be compared reliably with a high school student's knowledge deduced from a less difficult test. Scores derived by classical test theory do not have this characteristic, and assessment of actual ability (rather than ability relative to other test-takers) must be assessed by comparing scores to those of a \"norm group\" randomly selected from the population. In fact, all measures derived from classical test theory are dependent on the sample tested, while, in principle, those derived from item response theory are not.There are a number of different forms of validity. Criterion-related validity can be assessed by correlating a measure with a criterion measure theoretically expected to be related. When the criterion measure is collected at the same time as the measure being validated the goal is to establish concurrent validity; when the criterion is collected later the goal is to establish predictive validity. A measure has construct validity if it is related to measures of other constructs as required by theory. Content validity is a demonstration that the items of a test do an adequate job of covering the domain being measured. In a personnel selection example, test content is based on a defined statement or set of statements of knowledge, skill, ability, or other characteristics obtained from a job analysis.Internal consistency, which addresses the homogeneity of a single test form, may be assessed by correlating performance on two halves of a test, which is termed split-half reliability; the value of this Pearson product-moment correlation coefficient for two half-tests is adjusted with the Spearman\u2013Brown prediction formula to correspond to the correlation between two full-length tests.[14] Perhaps the most commonly used index of reliability is Cronbach's \u03b1, which is equivalent to the mean of all possible split-half coefficients. Other approaches include the intra-class correlation, which is the ratio of variance of measurements of a given target to the variance of all targets.Both reliability and validity can be assessed statistically. Consistency over repeated measures of the same test can be assessed with the Pearson correlation coefficient, and is often called test-retest reliability.[14] Similarly, the equivalence of different versions of the same measure can be indexed by a Pearson correlation, and is called equivalent forms reliability or a similar term.[14]Key concepts in classical test theory are reliability and validity. A reliable measure is one that measures a construct consistently across time, individuals, and situations. A valid measure is one that measures what it is intended to measure. Reliability is necessary, but not sufficient, for validity.One of the main deficiencies in various factor analyses is a lack of consensus in cutting points for determining the number of latent factors. A usual procedure is to stop factoring when eigenvalues drop below one because the original sphere shrinks. The lack of the cutting points concerns other multivariate methods, also.[citation needed]Psychometricians have also developed methods for working with large matrices of correlations and covariances. Techniques in this general tradition include: factor analysis,[11] a method of determining the underlying dimensions of data; multidimensional scaling,[12] a method for finding a simple representation for data with a large number of latent dimensions; and data clustering, an approach to finding objects that are like each other. All these multivariate descriptive methods try to distill large amounts of data into simpler structures. More recently, structural equation modeling[13] and path analysis represent more sophisticated approaches to working with large covariance matrices. These methods allow statistically sophisticated models to be fitted to data and tested to determine if they are adequate fits.Psychometricians have developed a number of different measurement theories. These include classical test theory (CTT) and item response theory (IRT).[8][9] An approach which seems mathematically to be similar to IRT but also quite distinctive, in terms of its origins and features, is represented by the Rasch model for measurement. The development of the Rasch model, and the broader class of models to which it belongs, was explicitly founded on requirements of measurement in the physical sciences.[10]Another major focus in psychometrics has been on personality testing. There have been a range of theoretical approaches to conceptualizing and measuring personality. Some of the better known instruments include the Minnesota Multiphasic Personality Inventory, the Five-Factor Model (or \"Big 5\") and tools such as Personality and Preference Inventory and the Myers-Briggs Type Indicator. Attitudes have also been studied extensively using psychometric approaches.[citation needed] A common method in the measurement of attitudes is the use of the Likert scale. An alternative method involves the application of unfolding measurement models, the most general being the Hyperbolic Cosine Model (Andrich & Luo, 1993).[7]Psychometrics is applied widely[citation needed] in educational assessment to measure abilities in domains such as reading, writing, and mathematics. The main approaches in applying tests in these domains have been classical test theory and the more recent Item Response Theory and Rasch measurement models. These latter approaches permit joint scaling of persons and assessment items, which provides a basis for mapping of developmental continua by allowing descriptions of the skills displayed at various points along a continuum.[citation needed]The first[citation needed]psychometric instruments were designed to measure the concept of intelligence.[6] One historical approach involved the Stanford-Binet IQ test, developed originally by the French psychologist Alfred Binet. Intelligence tests are useful tools for various purposes. An alternative conception of intelligence is that cognitive capacities within individuals are a manifestation of a general component, or general intelligence factor, as well as cognitive capacity specific to a given domain.[citation needed]On the other hand, when measurement models such as the Rasch model are employed, numbers are not assigned based on a rule. Instead, in keeping with Reese's statement above, specific criteria for measurement are stated, and the goal is to construct procedures or operations that provide data that meet the relevant criteria. Measurements are estimated based on the models, and tests are conducted to ascertain whether the relevant criteria have been met.[citation needed]These divergent responses are reflected in alternative approaches to measurement. For example, methods based on covariance matrices are typically employed on the premise that numbers, such as raw scores derived from assessments, are measurements. Such approaches implicitly entail Stevens's definition of measurement, which requires only that numbers are assigned according to some rule. The main research task, then, is generally considered to be the discovery of associations between scores, and of factors posited to underlie such associations.[citation needed]Indeed, Stevens's definition of measurement was put forward in response to the British Ferguson Committee, whose chair, A. Ferguson, was a physicist. The committee was appointed in 1932 by the British Association for the Advancement of Science to investigate the possibility of quantitatively estimating sensory events. Although its chair and other members were physicists, the committee also included several psychologists. The committee's report highlighted the importance of the definition of measurement. While Stevens's response was to propose a new definition, which has had considerable influence in the field, this was by no means the only response to the report. Another, notably different, response was to accept the classical definition, as reflected in the following statement:The definition of measurement in the social sciences has a long history. A currently widespread definition, proposed by Stanley Smith Stevens (1946), is that measurement is \"the assignment of numerals to objects or events according to some rule.\" This definition was introduced in the paper in which Stevens proposed four levels of measurement. Although widely adopted, this definition differs in important respects from the more classical definition of measurement adopted in the physical sciences, namely that scientific measurement entails \"the estimation or discovery of the ratio of some magnitude of a quantitative attribute to a unit of the same attribute\" (p.\u00a0358)[5]Figures who made significant contributions to psychometrics include Karl Pearson, Henry F. Kaiser, Carl Brigham, L. L. Thurstone, Anne Anastasi, Georg Rasch, Eugene Galanter, Johnson O'Connor, Frederic M. Lord, Ledyard R Tucker, Arthur Jensen, and David Andrich.More recently, psychometric theory has been applied in the measurement of personality, attitudes, and beliefs, and academic achievement. Measurement of these unobservable phenomena is difficult, and much of the research and accumulated science in this discipline has been developed in an attempt to properly define and quantify such phenomena. Critics, including practitioners in the physical sciences and social activists, have argued that such definition and quantification is impossibly difficult, and that such measurements are often misused, such as with psychometric personality tests used in employment procedures:The psychometrician L. L. Thurstone, founder and first president of the Psychometric Society in 1936, developed and applied a theoretical approach to measurement referred to as the law of comparative judgment, an approach that has close connections to the psychophysical theory of Ernst Heinrich Weber and Gustav Fechner. In addition, Spearman and Thurstone both made important contributions to the theory and application of factor analysis, a statistical method developed and used extensively in psychometrics.[citation needed] In the late 1950s, Leopold Szondi made an historical and epistemological assessment of the impact of statistical thinking onto psychology during previous few decades: \"in the last decades, the specifically psychological thinking has been almost completely suppressed and removed, and replaced by a statistical thinking. Precisely here we see the cancer of testology and testomania of today.\"[3]E.H. Weber built upon Herbart's work and tried to prove the existence of a psychological threshold, saying that a minimum stimulus was necessary to activate a sensory system. After Weber, G.T. Fechner expanded upon the knowledge he gleaned from Herbart and Weber, to devise the law that the strength of a sensation grows as the logarithm of the stimulus intensity. A follower of Weber and Fechner, Wilhelm Wundt is credited with founding the science of psychology. It is Wundt's influence that paved the way for others to develop psychological testing.[2]The origin of psychometrics also has connections to the related field of psychophysics. Around the same time that Darwin, Galton, and Cattell were making their discoveries, Herbart was also interested in \"unlocking the mysteries of human consciousness\" through the scientific method. (Kaplan & Saccuzzo, 2010) Herbart was responsible for creating mathematical models of the mind, which were influential in educational practices in years to come.Galton wrote a book entitled \"Hereditary Genius\" about different characteristics that people possess and how those characteristics make them more \"fit\" than others. Today these differences, such as sensory and motor functioning (reaction time, visual acuity, and physical strength) are important domains of scientific psychology. Much of the early theoretical and applied work in psychometrics was undertaken in an attempt to measure intelligence. Galton, often referred to as \"the father of psychometrics,\" devised and included mental tests among his anthropometric measures. James McKeen Cattell, who is considered a pioneer of psychometrics went on to extend Galton's work. Cattell also coined the term mental test, and is responsible for the research and knowledge which ultimately led to the development of modern tests. (Kaplan & Saccuzzo, 2010)Charles Darwin was the inspiration behind Sir Francis Galton who led to the creation of psychometrics. In 1859, Darwin published his book \"The Origin of Species\", which pertained to individual differences in animals. This book discussed how individual members in a species differ and how they possess characteristics that are more adaptive and successful or less adaptive and less successful. Those who are adaptive and successful are the ones that survive and give way to the next generation, who would be just as or more adaptive and successful. This idea, studied previously in animals, led to Galton's interest and study of human beings and how they differ one from another, and more importantly, how to measure those differences.Psychological testing has come from two streams of thought: the first, from Darwin, Galton, and Cattell on the measurement of individual differences, and the second, from Herbart, Weber, Fechner, and Wundt and their psychophysical measurements of a similar construct. The second set of individuals and their research is what has led to the development of experimental psychology, and standardized testing.[2]Practitioners are described as psychometricians. Psychometricians usually possess a specific qualification, and most are psychologists with advanced graduate training. In addition to traditional academic institutions, many psychometricians work for the government or in human resources departments. Others specialize as learning and development professionals.The field is concerned with the objective measurement of skills and knowledge, abilities, attitudes, personality traits, and educational achievement. Some psychometric researchers focus on the construction and validation of assessment instruments such as questionnaires, tests, raters' judgments, and personality tests. Others focus on research relating to measurement theory (e.g., item response theory; intraclass correlation).Psychometrics is a field of study concerned with the theory and technique of psychological measurement. As defined by the National Council on Measurement in Education (NCME), psychometrics refers to psychological measurement. Generally, it refers to the field in psychology and education that is devoted to testing, measurement, assessment, and related activities.[1]",
            "title": "Psychometrics",
            "url": "https://en.wikipedia.org/wiki/Psychometrics"
        },
        {
            "desc_links": [
                "/wiki/Management",
                "/wiki/Exchange_(economics)",
                "/wiki/Relationship_marketing",
                "/wiki/Customer",
                "/wiki/Business_administration"
            ],
            "links": [
                "/wiki/Innovation",
                "/wiki/Thomas_Edison",
                "/wiki/Product_lifecycle",
                "/wiki/Strategic_business_unit",
                "/wiki/Conglomerate_(company)",
                "/wiki/Advertising",
                "/wiki/Public_relations",
                "/wiki/Marketing_communications",
                "/wiki/Positioning_(marketing)",
                "/wiki/Market_segmentation",
                "/wiki/Quantitative_research",
                "/wiki/Qualitative_research",
                "/wiki/Statistical_hypothesis_testing",
                "/wiki/Chi-squared_test",
                "/wiki/Linear_regression",
                "/wiki/Correlation_coefficient",
                "/wiki/Frequency_distribution",
                "/wiki/Poisson_distribution",
                "/wiki/Binomial_distribution",
                "/wiki/PESTLE_analysis",
                "/wiki/Macroeconomics",
                "/wiki/Inflation",
                "/wiki/Unemployment",
                "/wiki/Social_Trends",
                "/wiki/Services_marketing",
                "/wiki/E._Jerome_McCarthy",
                "/wiki/Target_market",
                "/wiki/Neil_Borden",
                "/wiki/American_Marketing_Association",
                "/wiki/Harvard_Business_School",
                "/wiki/Marketing_mix",
                "/wiki/Marketing_mix",
                "/wiki/Marketing_plan",
                "/wiki/Value_(marketing)",
                "/wiki/Triple_bottom_line",
                "/wiki/Green_marketing",
                "/wiki/Economies_of_scale",
                "/wiki/Economies_of_scope",
                "/wiki/Market_segmentation",
                "/wiki/Product_design",
                "/wiki/Art_director",
                "/wiki/Brand_management",
                "/wiki/Advertising",
                "/wiki/Copywriting",
                "/wiki/Advertising",
                "/wiki/Distribution_(business)",
                "/wiki/Sales",
                "/wiki/Social_sciences",
                "/wiki/Psychology",
                "/wiki/Sociology",
                "/wiki/Mathematics",
                "/wiki/Economics",
                "/wiki/Anthropology",
                "/wiki/Neuroscience",
                "/wiki/Wikipedia:Verifiability",
                "/wiki/Wikipedia:Verifiability",
                "/wiki/Chartered_Institute_of_Marketing",
                "/wiki/Value-based_marketing",
                "/wiki/Shareholder_value",
                "/wiki/Philip_Kotler",
                "/wiki/American_Marketing_Association",
                "/wiki/Sales_process_engineering",
                "/wiki/Management",
                "/wiki/Exchange_(economics)",
                "/wiki/Relationship_marketing",
                "/wiki/Customer",
                "/wiki/Business_administration"
            ],
            "text": "Marketing is also used to promote business' products and is a great way to promote the business.In a product innovation approach, the company pursues product innovation, then tries to develop a market for the product. Product innovation drives the process and marketing research is conducted primarily to ensure that profitable market segment(s) exist for the innovation. The rationale is that customers may not know what options will be available to them in the future so we should not expect them to tell us what they will buy in the future. However, marketers can aggressively over-pursue product innovation and try to overcapitalize on a niche. When pursuing a product innovation approach, marketers must ensure that they have a varied and multi-tiered approach to product innovation. It is claimed that if Thomas Edison depended on marketing research he would have produced larger candles rather than inventing light bulbs. Many firms, such as research and development focused companies, successfully focus on product innovation. Many purists doubt whether this is really a form of marketing orientation at all, because of the ex post status of consumer research. Some even question whether it is marketing.The SIVA Model provides a demand/customer centric version alternative to the well-known 4Ps supply side model (product, price, place, promotion) of marketing management.A formal approach to this customer-focused marketing is known as SIVA[59] (Solution, Information, Value, Access). This system is basically the four Ps renamed and reworded to provide a customer focus.In the consumer-driven approach, consumer wants are the drivers of all strategic marketing decisions. No strategy is pursued until it passes the test of consumer research. Every aspect of a market offering, including the nature of the product itself, is driven by the needs of potential consumers. The starting point is always the consumer. The rationale for this approach is that there is no point spending R&D funds developing products that people will not buy. History attests to many products that were commercial failures in spite of being technological breakthroughs.[58]Many companies today have a customer focus (or market orientation). This implies that the company focuses its activities and products on consumer demands. Generally there are three ways of doing this: the customer-driven approach, the sense of identifying market changes and the product innovation approach.Demand for a good begins to taper off, and the firm may opt to discontinue manufacture of the product. This is so, if revenue for the product comes from efficiency savings in production, over actual sales of a good/service. However, if a product services a niche market, or is complementary to another product, it may continue manufacture of the product, despite a low level of sales/revenue being accrued.A product's sales start to level off, and an increasing number of entrants to a market produce price falls for the product. Firms may use sales promotions to raise sales.The product's sales/revenue is increasing, which may stimulate more marketing communications to sustain sales. More entrants enter into the market, to reap the apparent high profits that the industry is producing.In this stage, a product is launched onto the market. To stimulate growth of sales/revenue, use of advertising may be high, in order to heighten awareness of the product in question.The product life cycle (PLC) is a tool used by marketing managers to gauge the progress of a product, especially relating to sales or revenue accrued over time. The PLC is based on a few key assumptions, including:To use the example of the sports goods industry again, the marketing department would draw up marketing plans, strategies and communications to help the SBU achieve its marketing aims.The functional level relates to departments within the SBUs, such as marketing, finance, HR, production, etc. The functional level would adopt the SBU's strategy and determine how to accomplish the SBU's own objectives in its market.A strategic business unit (SBU) is a subsidiary within a firm, which participates within a given market/industry. The SBU would embrace the corporate strategy, and attune it to its own particular industry. For instance, an SBU may partake in the sports goods industry. It thus would ascertain how it would attain additional sales of sports goods, in order to satisfy the overall business strategy.As an example, if one pictures a group of companies (or a conglomerate), top management may state that sales for the group should increase by 25% over a ten-year period.Corporate marketing objectives are typically broad-based in nature, and pertain to the general vision of the firm in the short, medium or long-term.As stated previously, the senior management of a firm would formulate a general business strategy for a firm. However, this general business strategy would be interpreted and implemented in different contexts throughout the firm.Within the overall strategic marketing plan, the stages of the process are listed as thus:Generally speaking, an organisation's marketing planning process is derived from its overall business strategy. Thus, when top management are devising the firm's strategic direction/mission, the intended marketing activities are incorporated into this plan.The area of marketing planning involves forging a plan for a firm's marketing activities. A marketing plan can also pertain to a specific product, as well as to an organisation's overall marketing strategy.Marketing communications mix is used to reach, engage, provoke audience-centered conversations. It consists of 5 tools, which are 1)Advertising, 2)Sales & Promotion, 3)Public Relations, 4)Direct Marketing and 5)Personal Selling. The types of messages that are enhanced can be 1)Informational, 2)Emotional, 3)User-generated, or/and 4)Brand content. The last main component of MC mix is Media, which corresponds to the channel used to send the message. Media is divided into 3 categories, and these are media by 1)Form, 2)Source and 3)Functionality.Advertising occurs when a firm directly pays a media channel to publicize its product. Common examples of this include TV and radio adverts, billboards, branding, sponsorship, etc.Publicity involves attaining space in media, without having to pay directly for such coverage. As an example, an organization may have the launch of a new product covered by a newspaper or TV news segment. This benefits the firm in question since it is making consumers aware of its product, without necessarily paying a newspaper or television station to cover the event.PR can span:Public relations (or PR, as an acronym) is the use of media tools by a firm in order to promote goodwill from an organization to a target market segment, or other consumers of a firm's good/service. PR stems from the fact that a firm cannot seek to antagonize or inflame its market base, due to incurring a lessened demand for its good/service. Organizations undertake PR in order to assure consumers, and to forestall negative perceptions towards it.An example is coupons or a sale. People are given an incentive to buy, but this does not build customer loyalty or encourage future repeat buys. A major drawback of sales promotion is that it is easily copied by competition. It cannot be used as a sustainable source of differentiation.Short-term incentives to encourage buying of products:Oral presentation given by a salesperson who approaches individuals or a group of potential customers:Marketing communications encompass four distinct subsets, which are:Marketing communications is an audience-centered activity designed to engage audiences and promote responses. It is defined by actions a firm takes to communicate with end-users, consumers, and external parties.A firm often performs this by producing a perceptual map, which denotes similar products produced in the same industry according to how consumers perceive their price and quality. From a product's placing on the map, a firm would tailor its marketing communications to suit meld with the product's perception among consumers, and its position among competitors' offering.Positioning concerns how to position a product in the minds of consumers and inform what attributes differentiate it from the competitor's products.The next step in the targeting process is the level of differentiation involved in a segment serving. Three modes of differentiation exist, which are commonly applied by firms. These are:The DAMP acronym (meaning Discernable, Accessible, Measurable and Profitable) are used as criteria to gauge the viability of a target market. The elements of DAMP are:Once a segment has been identified, a firm must ascertain whether the segment is beneficial for them to service.Four commonly used criteria are used for segmentation, which include:Segmentation involves the initial splitting up of consumers into persons of like needs/wants/tastes.The steps of segmentation are Segment, Target, Position (abbreviated STP).Moreover, with more diversity in the tastes of modern consumers, firms are noting the benefit of servicing a multiplicity of new markets.A firm only possesses a certain amount of resources. Accordingly, it must make choices (and appreciate the related costs) in servicing specific groups of consumers.Market segmentation is conducted for two main purposes, including:Market segmentation consists of taking the total heterogeneous market for a product and dividing it into several sub-markets or segments, each of which tends to be homogeneous in all significant aspects.[57]Marketing research spans a number of stages,[56] including:Marketing researchers use statistical methods (such as quantitative research, qualitative research, hypothesis tests, Chi-square tests, linear regression, correlation coefficients, frequency distributions, Poisson and binomial distributions, etc.) to interpret their findings and convert data into information.[55]A distinction should be made between marketing research and market research. Market research pertains to research in a given market. As an example, a firm may conduct research in a target market, after selecting a suitable market segment. In contrast, marketing research relates to all research conducted within marketing. Market research is a subset of marketing research.Marketing research is a systematic process of analyzing data which involves conducting research to support marketing activities, and the statistical interpretation of data into information. This information is then used by managers to plan marketing activities, gauge the nature of a firm's marketing environment and to attain information from suppliers.A firms internal environment consists of factors inside of the actual company. These are factors controlled by the firm and they affect the relationship that a firm has with its customers. These include factors such as:By contrast to the macro-environment, an organization holds a greater degree of control over these factors.A firm's micro-environment typically spans:A firm's micro-environment comprises factors pertinent to the firm itself, or stakeholders closely connected with the firm or company.A firm's marketing macro-environment consists of a variety of external factors that manifest on a large (or macro) scale. These are typically economic, social, political or technological phenomena. A common method of assessing a firm's macro-environment is via a PESTLE (Political, Economic, Social, Technological, Legal, Ecological) analysis. Within a PESTLE analysis, a firm would analyze national political issues, culture and climate, key macroeconomic conditions, health and indicators (such as economic growth, inflation, unemployment, etc.), social trends/attitudes, and the nature of technology's impact on its society and the business processes within the society.The term \"marketing environment\" relates to all of the factors (whether internal, external, direct or indirect) that affect a firm's marketing decision-making/planning. A firm's marketing environment consists of three main areas, which are:To overcome the deficiencies of the 4 P model, some authors have suggested extensions or modifications to the original model. Extensions of the four P's include \"people\", \"process\", and \"physical evidence\" and are often applied in the case of services marketing[47] Other extensions have been found necessary in retail marketing, industrial marketing and internet marketing:Other important criticisms include that the marketing mix lacks a strategic framework and is therefore unfit to be a planning instrument, particularly when uncontrollable, external elements are an important aspect of the marketing environment. [46]From a model-building perspective, the 4 Ps has attracted a number of criticisms. Well-designed models should exhibit clearly defined categories that are mutually exclusive, with no overlap. Yet, the 4 Ps model has extensive overlapping problems. Some of the Ps are only defined in vague terms. Several authors stress the hybrid nature of the fourth P, mentioning the presence of two important dimensions, \"communication\" (general and informative communications such as public relations and corporate communications) and \"promotion\" (persuasive communications such as advertising and direct selling). Certain marketing activities, such as personal selling, may be classified as either promotion or as part of the place (i.e. distribution) element. [45] Some pricing tactics such as promotional pricing can be classified as price variables or promotional variables and therefore also exhibit some overlap.Morgan, in Riding the Waves of Change (Jossey-Bass, 1988), suggests that one of the greatest limitations of the 4 Ps approach \"is that it unconsciously emphasizes the inside\u2013out view (looking from the company outwards), whereas the essence of marketing should be the outside\u2013in approach\". An inside-out approach is the traditional planning approach where the organisation identifies its desired goals and objectives which are often based around what has always been done. Marketing's task then becomes one of \"selling\" the organisation's products and messages to the \"outside\" or external stakeholders.[43] In contrast, an outside-in approach first seeks to understand the needs and wants of the consumer. [44]The traditional marketing mix refers to four broad levels of marketing decision, namely: product, price, promotion, and place.[41][42]The \"marketing mix\" gained widespread acceptance with the publication, in 1960, of E. Jerome McCarthy's text, Basic Marketing: A Managerial Approach which outlined the ingredients in the mix as the memorable 4 Ps, namely product, price, place and promotion. [39] The marketing mix is based upon four controllable variables that a company manages in its effort to satisfy the corporation's objectives as well as the needs and wants of a target market.[35] Once there is understanding of the target market's interests, marketers develop tactics, using the 4Ps, to encourage buyers to purchase product. The successful use of the model is predicated upon the degree to which the target market's needs and wants have been understood, and the extent to which marketers have developed and correctly deployed the tactics. Today, the marketing mix or marketing program is understood to refer to the \"set of marketing tools that the firm uses to pursue its marketing objectives in the target market\".[40]Inspired by the idea of marketers as mixers of ingredients, Neil Borden one of Culliton's colleagues at Harvard, coined the phrase the marketing mix and used it wherever possible. According to Borden's own account, he used the term, 'marketing mix' consistently from the late 1940s. [36] For instance, he is on record as having used the term, 'marketing mix,' in his presidential address given to the American Marketing Association in 1953. [37] In the mid-1960s, Borden published a retrospective article detailing the early history of the marketing mix in which he claims that he was inspired by Culliton's idea of 'mixers', and credits himself with coining the term, 'marketing mix'.[38] Borden's continued and consistent use of the phrase, \"marketing mix,\" contributed to the process of popularising the concept throughout the 1940s and 50s.During the 1940s, the discipline of marketing was in transition. Interest in the functional school of thought, which was primarily concerned with mapping the functions of marketing was waning while the managerial school of thought, which focussed on the problems and challenges confronting marketers was gaining ground. [32] The concept of marketers as \"mixers of ingredients,\" was first introduced by James Culliton, a Professor at Harvard Business School. [33] At this time theorists began to develop checklists of the elements that made up the marketing mix, however, there was little agreement as to what should be included in the list. Many scholars and practitioners relied on lengthy classifications of factors that needed to be considered to understand consumer responses.[34] Neil Borden developed a complicated model in the late 1940s, based upon at least twelve different factors.[35]The four Ps, often referred to as the marketing mix or the marketing program,[31] represent the basic tools which marketers can use to bring their products or services to market. They are the foundation of managerial marketing and the marketing plan typically devotes a section to each of these Ps.A number of scholars and practitioners have argued that marketers have a greater social responsibility than simply satisfying customers and providing them with superior value. Instead, marketing activities should strive to benefit society's overall well-being. Marketing organisations that have embraced the societal marketing concept typically identify key stakeholder groups such as employees, customers, and local communities. They should consider the impact of their activities on all stakeholders. Companies that adopt a societal marketing perspective typically practice triple bottom line reporting whereby they publish social impact and environmental impact reports alongside financial performance reports. Sustainable marketing or green marketing is an extension of societal marketing. [30]The marketing orientation often has three prime facets, which are:The marketing orientation is perhaps the most common orientation used in contemporary marketing. It is a customer-centric approach that involves a firm basing its marketing program around products that suit new consumer tastes. Firms adopting a marketing orientation typically engage in extensive market research to gauge consumer desires, use R&D to develop a product attuned to the revealed information, and then utilize promotion techniques to ensure consumers are aware of the product's existence and the benefits it can deliver. [28] Scales designed to measure a firm's overall market orientation have been developed and found to be relatively robust in a variety of contexts. [29]A firm focusing on a production orientation specializes in producing as much as possible of a given product or service in order to achieve economies of scale or economies of scope. A production orientation may be deployed when a high demand for a product or service exists, coupled with certainty that consumer tastes and preferences remain relatively constant (similar to the sales orientation). The so-called production era is thought to have dominated marketing practice from the 1860s to the 1930s, but other theorists argue that evidence of the production orientation can still be found in some companies or industries. Specifically Kotler and Armstrong note that the production philosophy is \"one of the oldest philosophies that guides sellers... [and] is still useful in some situations.\" [27]A 2011 meta analyses[26] has found that the factors with the greatest impact on sales performance are a salesperson's sales related knowledge (knowledge of market segments, sales presentation skills, conflict resolution, and products), degree of adaptiveness (changing behaviour based on the aforementioned knowledge), role clarity (salesperson's role is to expressly to sell), cognitive aptitude (intelligence) and work engagement (motivation and interest in a sales role).A firm using a sales orientation focuses primarily on the selling/promotion of the firm's existing products, rather than determining new or unmet consumer needs or desires. Consequently, this entails simply selling existing products, using promotion and direct sales techniques to attain the highest sales possible.[23] The sales orientation \"is typically practised with unsought goods.\" [24] One study found that industrial companies are more likely to hold a sales orientation than consumer goods companies. [25] The approach may also suit scenarios in which a firm holds dead stock, or otherwise sells a product that is in high demand, with little likelihood of changes in consumer tastes diminishing demand.A firm employing a product orientation is mainly concerned with the quality of its own product. A product orientation is based on the assumption that, all things being equal, consumers will purchase products of a superior quality. The approach is most effective when the firm has deep insights into customers and their needs and desires derived from research or intuition and understands consumers' quality expectations and reservation prices. For example, Sony Walkman and Apple iPod were innovative product designs that addressed consumers' unmet needs. Although the product orientation has largely been supplanted by the marketing orientation, firms practising a product orientation can still be found in haute couture and in arts marketing. [22]A marketing orientation has been defined as a \"philosophy of business management.\" [18] or \"a corporate state of mind\" [19] or as an \"organisation[al] culture\" [20] Although scholars continue to debate the precise nature of specific orientations that inform marketing practice, the most commonly cited orientations are as follows: [21]Marketing research, conducted for the purpose of new product development or product improvement, is often concerned with identifying the consumer's unmet needs. [13] Customer needs are central to market segmentation which is concerned with dividing markets into distinct groups of buyers on the basis of \"distinct needs, characteristics, or behaviors who might require separate products or marketing mixes.\" [14] Needs-based segmentation (also known as benefit segmentation) \"places the customers' desires at the forefront of how a company designs and markets products or services.\" [15] Although needs-based segmentation is difficult to do in practice, has been proved to be one of the most effective ways to segment a market. [16] In addition, a great deal of advertising and promotion is designed to show how a given product's benefits meet the customer's needs, wants or expectations in a unique way.[17]Given the centrality of customer needs and wants in marketing, a rich understanding of these concepts is essential:[12]The 'marketing concept' proposes that in order to satisfy the organizational objectives, an organization should anticipate the needs and wants of consumers and satisfy these more effectively than competitors. This concept originated from Adam Smith's book\u00a0The Wealth of Nations,\u00a0but would not become widely used until nearly 200 years later.[11] Marketing and Marketing Concepts are directly related.The process of marketing is that of bringing a product to market in which includes these steps: broad market research; market targeting and market segmentation; determining distribution, pricing and promotion strategies; developing a communications strategy; budgeting; and visioning long-term market development goals.[10] Many parts of the marketing process (e.g. product design, art director, brand management, advertising, copywriting etc.) involve use of the creative arts.Marketing practice tended to be seen as a creative industry in the past, which included advertising, distribution and selling. However, because the academic study of marketing makes extensive use of social sciences, psychology, sociology, mathematics, economics, anthropology and neuroscience, the profession is now widely recognized as a science,[8][not in citation given]allowing numerous universities to offer Master-of-Science (MSc) programs.[9][not in citation given]The Chartered Institute of Marketing defines marketing as \"the management process responsible for identifying, anticipating and satisfying customer requirements profitably.\"[6] A similar concept is the value-based marketing which states the role of marketing to contribute to increasing shareholder value.[7] In this context, marketing can be defined as \"the management process that seeks to maximise returns to shareholders by developing relationships with valued customers and creating a competitive advantage.\"[7]Philip Kotler defines marketing as\u00a0:-marketing is about Satisfying needs and wants through an exchange process.Marketing is defined by the American Marketing Association as \"the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offerings that have value for customers, clients, partners, and society at large.\"[4] The term developed from the original meaning which referred literally to going to market with goods for sale. From a sales process engineering perspective, marketing is \"a set of processes that are interconnected and interdependent with other functions\" of a business aimed at achieving customer interest and satisfaction.[5]Marketing is the study and management of exchange relationships.[1][2] Marketing is used to create, keep and satisfy the customer. With the customer as the focus of its activities, it can be concluded that Marketing is one of the premier components of Business Management - the other being Innovation.[3]",
            "title": "Marketing",
            "url": "https://en.wikipedia.org/wiki/Marketing"
        },
        {
            "desc_links": [
                "/wiki/Psychologists",
                "/wiki/Human_resources",
                "/wiki/Learning_and_development",
                "/wiki/Measurement",
                "/wiki/Trait_theory",
                "/wiki/Educational_measurement",
                "/wiki/Questionnaire",
                "/wiki/Test_(student_assessment)",
                "/wiki/Personality_tests",
                "/wiki/Item_response_theory",
                "/wiki/Intraclass_correlation",
                "/wiki/Psychological",
                "/wiki/Measurement",
                "/wiki/National_Council_on_Measurement_in_Education"
            ],
            "links": [
                "/wiki/Artificial_intelligence",
                "/wiki/Comparative_psychology",
                "/wiki/Evolutionary_psychology",
                "/wiki/Evaluation",
                "/wiki/Educational_evaluation",
                "/wiki/Joint_Committee_on_Standards_for_Educational_Evaluation",
                "/wiki/Standards_for_Educational_and_Psychological_Testing",
                "/wiki/Psychological_testing",
                "/wiki/Professional_certification",
                "/wiki/Test_(student_assessment)",
                "/wiki/Program_evaluation",
                "/wiki/Validity_(statistics)",
                "/wiki/Reliability_(statistics)",
                "/wiki/Quality_(business)",
                "/wiki/Standards_organization",
                "/wiki/Bias",
                "/wiki/Item_response_theory",
                "/wiki/Latent_trait",
                "/wiki/Concurrent_validity",
                "/wiki/Predictive_validity",
                "/wiki/Construct_validity",
                "/wiki/Content_validity",
                "/wiki/Job_analysis",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Spearman%E2%80%93Brown_prediction_formula",
                "/wiki/Cronbach%27s_%CE%B1",
                "/wiki/Mean",
                "/wiki/Intra-class_correlation",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Reliability_(psychometric)",
                "/wiki/Test_validity",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Factor_analysis",
                "/wiki/Multidimensional_scaling",
                "/wiki/Data_clustering",
                "/wiki/Structural_equation_modeling",
                "/wiki/Path_analysis_(statistics)",
                "/wiki/Covariance_matrix",
                "/wiki/Classical_test_theory",
                "/wiki/Item_response_theory",
                "/wiki/Rasch_model",
                "/wiki/Personality_test",
                "/wiki/Minnesota_Multiphasic_Personality_Inventory",
                "/wiki/Big_five_personality_traits",
                "/wiki/Personality_and_Preference_Inventory",
                "/wiki/Myers-Briggs_Type_Indicator",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Likert_scale",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Literacy",
                "/wiki/Mathematics",
                "/wiki/Classical_test_theory",
                "/wiki/Item_Response_Theory",
                "/wiki/Rasch_model",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Intelligence_(trait)",
                "/wiki/Stanford-Binet_IQ_test",
                "/wiki/Alfred_Binet",
                "/wiki/General_intelligence_factor",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Rasch_model",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Covariance_matrix",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Stanley_Smith_Stevens",
                "/wiki/Levels_of_measurement",
                "/wiki/Karl_Pearson",
                "/wiki/Carl_Brigham",
                "/wiki/L._L._Thurstone",
                "/wiki/Anne_Anastasi",
                "/wiki/Georg_Rasch",
                "/wiki/Eugene_Galanter",
                "/wiki/Johnson_O%27Connor",
                "/wiki/Frederic_M._Lord",
                "/wiki/Ledyard_R_Tucker",
                "/wiki/Arthur_Jensen",
                "/wiki/David_Andrich",
                "/wiki/Attitude_(psychology)",
                "/wiki/Belief",
                "/wiki/Academic_achievement",
                "/wiki/Physical_sciences",
                "/wiki/Activism",
                "/wiki/L._L._Thurstone",
                "/wiki/Law_of_comparative_judgment",
                "/wiki/Ernst_Heinrich_Weber",
                "/wiki/Gustav_Fechner",
                "/wiki/Factor_analysis",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Leopold_Szondi",
                "/wiki/Psychophysics",
                "/wiki/Intelligence_(trait)",
                "/wiki/Anthropometric",
                "/wiki/The_Origin_of_Species",
                "/wiki/Species",
                "/wiki/Charles_Darwin",
                "/wiki/Sir_Francis_Galton",
                "/wiki/James_McKeen_Cattell",
                "/wiki/J.E._Herbart",
                "/wiki/E.H._Weber",
                "/wiki/Gustav_Fechner",
                "/wiki/Wundt",
                "/wiki/Experimental_psychology",
                "/wiki/Psychologists",
                "/wiki/Human_resources",
                "/wiki/Learning_and_development",
                "/wiki/Measurement",
                "/wiki/Trait_theory",
                "/wiki/Educational_measurement",
                "/wiki/Questionnaire",
                "/wiki/Test_(student_assessment)",
                "/wiki/Personality_tests",
                "/wiki/Item_response_theory",
                "/wiki/Intraclass_correlation",
                "/wiki/Psychological",
                "/wiki/Measurement",
                "/wiki/National_Council_on_Measurement_in_Education"
            ],
            "text": "The evaluation of abilities, traits and learning evolution of machines has been mostly unrelated to the case of humans and non-human animals, with specific approaches in the area of artificial intelligence. A more integrated approach, under the name of universal psychometrics, has also been proposed.[30]Psychometrics addresses human abilities, attitudes, traits and educational evolution. Notably, the study of behavior, mental processes and abilities of non-human animals is usually addressed by comparative psychology, or with a continuum between non-human animals and the rest of animals by evolutionary psychology. Nonetheless there are some advocators for a more gradual transition between the approach taken for humans and the approach taken for (non-human) animals.[26][27][28] [29]Each publication presents and elaborates a set of standards for use in a variety of educational settings. The standards provide guidelines for designing, implementing, assessing and improving the identified form of evaluation.[25] Each of the standards has been placed in one of four fundamental categories to promote educational evaluations that are proper, useful, feasible, and accurate. In these sets of standards, validity and reliability considerations are covered under the accuracy topic. For example, the student accuracy standards help ensure that student evaluations will provide sound, accurate, and credible information about student learning and performance.In the field of evaluation, and in particular educational evaluation, the Joint Committee on Standards for Educational Evaluation[21] has published three sets of standards for evaluations. The Personnel Evaluation Standards[22] was published in 1988, The Program Evaluation Standards (2nd edition)[23] was published in 1994, and The Student Evaluation Standards[24] was published in 2003.In 2014, the American Educational Research Association (AERA), American Psychological Association (APA), and National Council on Measurement in Education (NCME) published a revision of the Standards for Educational and Psychological Testing,[20] which describes standards for test development, evaluation, and use. The Standards cover essential topics in testing including validity, reliability/errors of measurement, and fairness in testing. The book also establishes standards related to testing operations including test design and development, scores, scales, norms, score linking, cut scores, test administration, scoring, reporting, score interpretation, test documentation, and rights and responsibilities of test takers and test users. Finally, the Standards cover topics related to testing applications, including psychological testing and assessment, workplace testing and credentialing, educational testing and assessment, and testing in program evaluation and public policy.The considerations of validity and reliability typically are viewed as essential elements for determining the quality of any test. However, professional and practitioner associations frequently have placed these concerns within broader contexts when developing standards and making overall judgments about the quality of any test as a whole within a given context. A consideration of concern in many applied research settings is whether or not the metric of a given psychological inventory is meaningful or arbitrary.[19]Many psychometricians are also concerned with finding and eliminating test bias from their psychological tests. Test bias is a form of systematic (i.e., non-random) error which leads to examinees from one demographic group having an unwarranted advantage over examinees from another demographic group.[15] According to leading experts, test bias may cause differences in average scores across demographic groups, but differences in group scores are not sufficient evidence that test bias is actually present because the test could be measuring real differences among groups.[16][17] Psychometricians use sophisticated scientific methods to search for test bias and eliminate it. Research shows that it is usually impossible for people reading a test item to accurately determine whether it is biased or not.[18]Item response theory models the relationship between latent traits and responses to test items. Among other advantages, IRT provides a basis for obtaining an estimate of the location of a test-taker on a given latent trait as well as the standard error of measurement of that location. For example, a university student's knowledge of history can be deduced from his or her score on a university test and then be compared reliably with a high school student's knowledge deduced from a less difficult test. Scores derived by classical test theory do not have this characteristic, and assessment of actual ability (rather than ability relative to other test-takers) must be assessed by comparing scores to those of a \"norm group\" randomly selected from the population. In fact, all measures derived from classical test theory are dependent on the sample tested, while, in principle, those derived from item response theory are not.There are a number of different forms of validity. Criterion-related validity can be assessed by correlating a measure with a criterion measure theoretically expected to be related. When the criterion measure is collected at the same time as the measure being validated the goal is to establish concurrent validity; when the criterion is collected later the goal is to establish predictive validity. A measure has construct validity if it is related to measures of other constructs as required by theory. Content validity is a demonstration that the items of a test do an adequate job of covering the domain being measured. In a personnel selection example, test content is based on a defined statement or set of statements of knowledge, skill, ability, or other characteristics obtained from a job analysis.Internal consistency, which addresses the homogeneity of a single test form, may be assessed by correlating performance on two halves of a test, which is termed split-half reliability; the value of this Pearson product-moment correlation coefficient for two half-tests is adjusted with the Spearman\u2013Brown prediction formula to correspond to the correlation between two full-length tests.[14] Perhaps the most commonly used index of reliability is Cronbach's \u03b1, which is equivalent to the mean of all possible split-half coefficients. Other approaches include the intra-class correlation, which is the ratio of variance of measurements of a given target to the variance of all targets.Both reliability and validity can be assessed statistically. Consistency over repeated measures of the same test can be assessed with the Pearson correlation coefficient, and is often called test-retest reliability.[14] Similarly, the equivalence of different versions of the same measure can be indexed by a Pearson correlation, and is called equivalent forms reliability or a similar term.[14]Key concepts in classical test theory are reliability and validity. A reliable measure is one that measures a construct consistently across time, individuals, and situations. A valid measure is one that measures what it is intended to measure. Reliability is necessary, but not sufficient, for validity.One of the main deficiencies in various factor analyses is a lack of consensus in cutting points for determining the number of latent factors. A usual procedure is to stop factoring when eigenvalues drop below one because the original sphere shrinks. The lack of the cutting points concerns other multivariate methods, also.[citation needed]Psychometricians have also developed methods for working with large matrices of correlations and covariances. Techniques in this general tradition include: factor analysis,[11] a method of determining the underlying dimensions of data; multidimensional scaling,[12] a method for finding a simple representation for data with a large number of latent dimensions; and data clustering, an approach to finding objects that are like each other. All these multivariate descriptive methods try to distill large amounts of data into simpler structures. More recently, structural equation modeling[13] and path analysis represent more sophisticated approaches to working with large covariance matrices. These methods allow statistically sophisticated models to be fitted to data and tested to determine if they are adequate fits.Psychometricians have developed a number of different measurement theories. These include classical test theory (CTT) and item response theory (IRT).[8][9] An approach which seems mathematically to be similar to IRT but also quite distinctive, in terms of its origins and features, is represented by the Rasch model for measurement. The development of the Rasch model, and the broader class of models to which it belongs, was explicitly founded on requirements of measurement in the physical sciences.[10]Another major focus in psychometrics has been on personality testing. There have been a range of theoretical approaches to conceptualizing and measuring personality. Some of the better known instruments include the Minnesota Multiphasic Personality Inventory, the Five-Factor Model (or \"Big 5\") and tools such as Personality and Preference Inventory and the Myers-Briggs Type Indicator. Attitudes have also been studied extensively using psychometric approaches.[citation needed] A common method in the measurement of attitudes is the use of the Likert scale. An alternative method involves the application of unfolding measurement models, the most general being the Hyperbolic Cosine Model (Andrich & Luo, 1993).[7]Psychometrics is applied widely[citation needed] in educational assessment to measure abilities in domains such as reading, writing, and mathematics. The main approaches in applying tests in these domains have been classical test theory and the more recent Item Response Theory and Rasch measurement models. These latter approaches permit joint scaling of persons and assessment items, which provides a basis for mapping of developmental continua by allowing descriptions of the skills displayed at various points along a continuum.[citation needed]The first[citation needed]psychometric instruments were designed to measure the concept of intelligence.[6] One historical approach involved the Stanford-Binet IQ test, developed originally by the French psychologist Alfred Binet. Intelligence tests are useful tools for various purposes. An alternative conception of intelligence is that cognitive capacities within individuals are a manifestation of a general component, or general intelligence factor, as well as cognitive capacity specific to a given domain.[citation needed]On the other hand, when measurement models such as the Rasch model are employed, numbers are not assigned based on a rule. Instead, in keeping with Reese's statement above, specific criteria for measurement are stated, and the goal is to construct procedures or operations that provide data that meet the relevant criteria. Measurements are estimated based on the models, and tests are conducted to ascertain whether the relevant criteria have been met.[citation needed]These divergent responses are reflected in alternative approaches to measurement. For example, methods based on covariance matrices are typically employed on the premise that numbers, such as raw scores derived from assessments, are measurements. Such approaches implicitly entail Stevens's definition of measurement, which requires only that numbers are assigned according to some rule. The main research task, then, is generally considered to be the discovery of associations between scores, and of factors posited to underlie such associations.[citation needed]Indeed, Stevens's definition of measurement was put forward in response to the British Ferguson Committee, whose chair, A. Ferguson, was a physicist. The committee was appointed in 1932 by the British Association for the Advancement of Science to investigate the possibility of quantitatively estimating sensory events. Although its chair and other members were physicists, the committee also included several psychologists. The committee's report highlighted the importance of the definition of measurement. While Stevens's response was to propose a new definition, which has had considerable influence in the field, this was by no means the only response to the report. Another, notably different, response was to accept the classical definition, as reflected in the following statement:The definition of measurement in the social sciences has a long history. A currently widespread definition, proposed by Stanley Smith Stevens (1946), is that measurement is \"the assignment of numerals to objects or events according to some rule.\" This definition was introduced in the paper in which Stevens proposed four levels of measurement. Although widely adopted, this definition differs in important respects from the more classical definition of measurement adopted in the physical sciences, namely that scientific measurement entails \"the estimation or discovery of the ratio of some magnitude of a quantitative attribute to a unit of the same attribute\" (p.\u00a0358)[5]Figures who made significant contributions to psychometrics include Karl Pearson, Henry F. Kaiser, Carl Brigham, L. L. Thurstone, Anne Anastasi, Georg Rasch, Eugene Galanter, Johnson O'Connor, Frederic M. Lord, Ledyard R Tucker, Arthur Jensen, and David Andrich.More recently, psychometric theory has been applied in the measurement of personality, attitudes, and beliefs, and academic achievement. Measurement of these unobservable phenomena is difficult, and much of the research and accumulated science in this discipline has been developed in an attempt to properly define and quantify such phenomena. Critics, including practitioners in the physical sciences and social activists, have argued that such definition and quantification is impossibly difficult, and that such measurements are often misused, such as with psychometric personality tests used in employment procedures:The psychometrician L. L. Thurstone, founder and first president of the Psychometric Society in 1936, developed and applied a theoretical approach to measurement referred to as the law of comparative judgment, an approach that has close connections to the psychophysical theory of Ernst Heinrich Weber and Gustav Fechner. In addition, Spearman and Thurstone both made important contributions to the theory and application of factor analysis, a statistical method developed and used extensively in psychometrics.[citation needed] In the late 1950s, Leopold Szondi made an historical and epistemological assessment of the impact of statistical thinking onto psychology during previous few decades: \"in the last decades, the specifically psychological thinking has been almost completely suppressed and removed, and replaced by a statistical thinking. Precisely here we see the cancer of testology and testomania of today.\"[3]E.H. Weber built upon Herbart's work and tried to prove the existence of a psychological threshold, saying that a minimum stimulus was necessary to activate a sensory system. After Weber, G.T. Fechner expanded upon the knowledge he gleaned from Herbart and Weber, to devise the law that the strength of a sensation grows as the logarithm of the stimulus intensity. A follower of Weber and Fechner, Wilhelm Wundt is credited with founding the science of psychology. It is Wundt's influence that paved the way for others to develop psychological testing.[2]The origin of psychometrics also has connections to the related field of psychophysics. Around the same time that Darwin, Galton, and Cattell were making their discoveries, Herbart was also interested in \"unlocking the mysteries of human consciousness\" through the scientific method. (Kaplan & Saccuzzo, 2010) Herbart was responsible for creating mathematical models of the mind, which were influential in educational practices in years to come.Galton wrote a book entitled \"Hereditary Genius\" about different characteristics that people possess and how those characteristics make them more \"fit\" than others. Today these differences, such as sensory and motor functioning (reaction time, visual acuity, and physical strength) are important domains of scientific psychology. Much of the early theoretical and applied work in psychometrics was undertaken in an attempt to measure intelligence. Galton, often referred to as \"the father of psychometrics,\" devised and included mental tests among his anthropometric measures. James McKeen Cattell, who is considered a pioneer of psychometrics went on to extend Galton's work. Cattell also coined the term mental test, and is responsible for the research and knowledge which ultimately led to the development of modern tests. (Kaplan & Saccuzzo, 2010)Charles Darwin was the inspiration behind Sir Francis Galton who led to the creation of psychometrics. In 1859, Darwin published his book \"The Origin of Species\", which pertained to individual differences in animals. This book discussed how individual members in a species differ and how they possess characteristics that are more adaptive and successful or less adaptive and less successful. Those who are adaptive and successful are the ones that survive and give way to the next generation, who would be just as or more adaptive and successful. This idea, studied previously in animals, led to Galton's interest and study of human beings and how they differ one from another, and more importantly, how to measure those differences.Psychological testing has come from two streams of thought: the first, from Darwin, Galton, and Cattell on the measurement of individual differences, and the second, from Herbart, Weber, Fechner, and Wundt and their psychophysical measurements of a similar construct. The second set of individuals and their research is what has led to the development of experimental psychology, and standardized testing.[2]Practitioners are described as psychometricians. Psychometricians usually possess a specific qualification, and most are psychologists with advanced graduate training. In addition to traditional academic institutions, many psychometricians work for the government or in human resources departments. Others specialize as learning and development professionals.The field is concerned with the objective measurement of skills and knowledge, abilities, attitudes, personality traits, and educational achievement. Some psychometric researchers focus on the construction and validation of assessment instruments such as questionnaires, tests, raters' judgments, and personality tests. Others focus on research relating to measurement theory (e.g., item response theory; intraclass correlation).Psychometrics is a field of study concerned with the theory and technique of psychological measurement. As defined by the National Council on Measurement in Education (NCME), psychometrics refers to psychological measurement. Generally, it refers to the field in psychology and education that is devoted to testing, measurement, assessment, and related activities.[1]",
            "title": "Psychometrics",
            "url": "https://en.wikipedia.org/wiki/Psychometrics"
        },
        {
            "desc_links": [
                "/wiki/Psychology",
                "/wiki/Social_sciences",
                "/wiki/William_Stephenson_(psychologist)"
            ],
            "links": [
                "/wiki/Nursing",
                "/wiki/Veterinary_medicine",
                "/wiki/Public_health",
                "/wiki/Transportation",
                "/wiki/Education",
                "/wiki/Rural_sociology",
                "/wiki/Hydrology",
                "/wiki/Mobile_phone",
                "/wiki/Subjectivity",
                "/wiki/UC_Riverside",
                "/wiki/Cultural_Consensus_Theory",
                "/wiki/Consensus_based_assessment",
                "/wiki/Karl_Popper",
                "/wiki/Factor_analysis",
                "/wiki/Correlations",
                "/wiki/Variable_(mathematics)",
                "/wiki/Psychology",
                "/wiki/Social_sciences",
                "/wiki/William_Stephenson_(psychologist)"
            ],
            "text": "Some information on validation of the method is available.[7][8]Q-methodology has been used as a research tool in a wide variety of disciplines including nursing, veterinary medicine, public health, transportation, education, rural sociology, hydrology and mobile communication.[1][2][3][4][5] The methodology is particularly useful when researchers wish to understand and describe the variety of subjective viewpoints on an issue.[6]One Q-sort should produce two sets of data. The first is the physical distribution of sorted objects. The second is either an ongoing 'think-out-loud' narrative or a discussion that immediately follows the sorting exercise. The purpose of these narratives were, in the first instance, to elicit discussion of the reasons for particular placements. While the relevance of this qualitative data is often suppressed in current uses of Q-methodology, the modes of reasoning behind placement of an item can be more analytically relevant than the absolute placement of cards.The \"Q sort\" data collection procedure is traditionally done using a paper template and the sample of statements or other stimuli printed on individual cards. However, there are also computer software applications for conducting online Q sorts. For example, consulting firm Davis Brand Capital has created a proprietary online product, nQue, that they use to conduct online Q sorts that mimic the analog, paper-based sorting procedure. However, the web-based software application that uses a drag-and-drop, graphical user interface to assist researchers is not available for commercial sale. UC Riverside's Riverside Situational Q-sort (RSQ), a newly developed tool by the university, purports to measure the psychological properties of situations. Their International Situations Project is using the tool to explore the psychologically salient aspects of situations and how those aspects may differ across cultures with this university-developed web-based application. To date there has been no study of differences in sorts produced by use of computer based vs. physical sorting.An alternative method that determines the similarity among subjects somewhat like Q methodology, as well as the cultural \"truth\" of the statements used in the test, is Cultural Consensus Theory.In studies of intelligence, Q factor analysis can generate Consensus based assessment (CBA) scores as direct measures. Alternatively, the unit of measurement of a person in this context is his factor loading for a Q-sort he or she performs. Factors represent norms with respect to schemata. The individual who gains the highest factor loading on an Operant factor is the person most able to conceive the norm for the factor. What the norm means is a matter, always, for conjecture and refutation (Popper). It may be indicative of the wisest solution, or the most responsible, the most important, or an optimized-balanced solution. These are all untested hypotheses that require future study.One salient difference between Q and other social science research methodologies, such as surveys, is that it typically uses many fewer subjects. This can be a strength, as Q is sometimes used with a single subject, and it makes research far less expensive. In such cases, a person will rank the same set of statements under different conditions of instruction. For example, someone might be given a set of statements about personality traits and then asked to rank them according to how well they describe herself, her ideal self, her father, her mother, etc. Working with a single individual is particularly relevant in the study of how an individual's rankings change over time and this was the first use of Q-methodology. As Q-methodology works with a small non-representative sample, conclusions are limited to those who participated in the study.The sample of statements for a Q sort is drawn from and claimed to be representative of a \"concourse\"\u2014the sum of all things people say or think about the issue being investigated. Commonly Q methodologists use a structured sampling approach in order to try and represent the full breadth of the concourse.The data for Q factor analysis come from a series of \"Q sorts\" performed by one or more subjects. A Q sort is a ranking of variables\u2014typically presented as statements printed on small cards\u2014according to some \"condition of instruction.\" For example, in a Q study of people's views of a celebrity, a subject might be given statements like \"He is a deeply religious man\" and \"He is a liar,\" and asked to sort them from \"most like how I think about this celebrity\" to \"least like how I think about this celebrity.\" The use of ranking, rather than asking subjects to rate their agreement with statements individually, is meant to capture the idea that people think about ideas in relation to other ideas, rather than in isolation.The name \"Q\" comes from the form of factor analysis that is used to analyze the data. Normal factor analysis, called \"R method,\" involves finding correlations between variables (say, height and age) across a sample of subjects. Q, on the other hand, looks for correlations between subjects across a sample of variables. Q factor analysis reduces the many individual viewpoints of the subjects down to a few \"factors,\" which are claimed to represent shared ways of thinking. It is sometimes said that Q factor analysis is R factor analysis with the data table turned sideways. While helpful as a heuristic for understanding Q, this explanation may be misleading, as most Q methodologists argue that for mathematical reasons no one data matrix would be suitable for analysis with both Q and R.Q Methodology is a research method used in psychology and in social sciences to study people's \"subjectivity\"\u2014that is, their viewpoint. Q was developed by psychologist William Stephenson. It has been used both in clinical settings for assessing a patient's progress over time (intra-rater comparison), as well as in research settings to examine how people think about a topic (inter-rater comparisons).",
            "title": "Q methodology",
            "url": "https://en.wikipedia.org/wiki/Q_methodology"
        },
        {
            "desc_links": [
                "/wiki/Clinical_significance",
                "/wiki/Experiment",
                "/wiki/Observational_study",
                "/wiki/Sample_(statistics)",
                "/wiki/Statistical_population",
                "/wiki/Sampling_error",
                "/wiki/Statistical_hypothesis_testing",
                "/wiki/Null_hypothesis",
                "/wiki/Significance_level",
                "/wiki/P-value"
            ],
            "links": [
                "/wiki/American_Statistical_Association",
                "/wiki/Data_dredging",
                "/wiki/Bayesian_statistics",
                "/wiki/Effect_size",
                "/wiki/Cohen%27s_d",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Coefficient_of_determination",
                "/wiki/Genome-wide_association_study",
                "/wiki/Particle_physics",
                "/wiki/Manufacturing",
                "/wiki/Standard_deviation",
                "/wiki/Normal_distribution",
                "/wiki/Higgs_boson",
                "/wiki/Research_question",
                "/wiki/Alternative_hypothesis",
                "/wiki/Statistical_power",
                "/wiki/Conditional_probability",
                "/wiki/Type_I_error#Type_I_error",
                "/wiki/Sampling_distribution",
                "/wiki/One-tailed_test",
                "/wiki/Two-tailed_test",
                "/wiki/Type_I_error#Type_I_error",
                "/wiki/Null_hypothesis",
                "/wiki/Confidence_level",
                "/wiki/Ronald_Fisher",
                "/wiki/Jerzy_Neyman",
                "/wiki/Egon_Pearson",
                "/wiki/Clinical_significance",
                "/wiki/Experiment",
                "/wiki/Observational_study",
                "/wiki/Sample_(statistics)",
                "/wiki/Statistical_population",
                "/wiki/Sampling_error",
                "/wiki/Statistical_hypothesis_testing",
                "/wiki/Null_hypothesis",
                "/wiki/Significance_level",
                "/wiki/P-value"
            ],
            "text": "In 2016, the American Statistical Association (ASA) published a statement on p-values, saying that \"the widespread use of 'statistical significance' (generally interpreted as 'p\u22640.05') as a license for making a claim of a scientific finding (or implied truth) leads to considerable distortion of the scientific process\".[47] In 2017, a group of 72 authors proposed to enhance reproducibility by changing the p-value threshold for statistical significance from 0.05 to 0.005.[49] Other researchers responded that imposing a more stringent significance threshold would aggravate problems such as data dredging; alternative propositions are thus to select and justify flexible p-value thresholds before collecting data,[50] or to interpret p-values as continuous indices, thereby discarding thresholds and statistical significance.[51]Other editors, commenting on this ban have noted: \"Banning the reporting of p-values, as Basic and Applied Social Psychology recently did, is not going to solve the problem because it is merely treating a symptom of the problem. There is nothing wrong with hypothesis testing and p-values per se as long as authors, reviewers, and action editors use them correctly.\" [46] Using Bayesian statistics can improve confidence levels but also requires making additional assumptions,[47] and may not necessarily improve practice regarding statistical testing.[48]Starting in the 2010s, some journals began questioning whether significance testing, and particularly using a threshold of \u03b1=5%, was being relied on too heavily as the primary measure of validity of a hypothesis.[42] Some journals encouraged authors to do more detailed analysis than just a statistical significance test. In social psychology, the Journal of Basic and Applied Social Psychology banned the use of significance testing altogether from papers it published,[43] requiring authors to use other measures to evaluate hypotheses and impact.[44][45]A statistically significant result may not be easy to reproduce.[38] In particular, some statistically significant results will in fact be false positives. Each failed attempt to reproduce a result increases the likelihood that the result was a false positive.[41]Effect size is a measure of a study's practical significance.[39] A statistically significant result may have a weak effect. To gauge the research significance of their result, researchers are encouraged to always report an effect size along with p-values. An effect size measure quantifies the strength of an effect, such as the distance between two means in units of standard deviation (cf. Cohen's d), the correlation coefficient between two variables or its square, and other measures.[40]Researchers focusing solely on whether their results are statistically significant might report findings that are not substantive[36] and not replicable.[37][38] There is also a difference between statistical significance and practical significance. A study that is found to be statistically significant may not necessarily be practically significant.[39]In other fields of scientific research such as genome-wide association studies significance levels as low as 6992500000000000000\u26605\u00d710\u22128 are not uncommon,[34][35] because the number of tests performed is extremely large.In specific fields such as particle physics and manufacturing, statistical significance is often expressed in multiples of the standard deviation or sigma (\u03c3) of a normal distribution, with significance thresholds set at a much stricter level (e.g. 5\u03c3).[31][32] For instance, the certainty of the Higgs boson particle's existence was based on the 5\u03c3 criterion, which corresponds to a p-value of about 1 in 3.5 million.[32][33]The use of a one-tailed test is dependent on whether the research question or alternative hypothesis specifies a direction such as whether a group of objects is heavier or the performance of students on an assessment is better.[3] A two-tailed test may still be used but it will be less powerful than a one-tailed test because the rejection region for a one-tailed test is concentrated on one end of the null distribution and is twice the size (5% vs. 2.5%) of each rejection region for a two-tailed test. As a result, the null hypothesis can be rejected with a less extreme result if a one-tailed test was used.[30] The one-tailed test is only more powerful than a two-tailed test if the specified direction of the alternative hypothesis is correct. If it is wrong, however, then the one-tailed test has no power.For example, when \u03b1 is set to 5%, the conditional probability of a type I error, given that the null hypothesis is true, is 5%,[27] and a statistically significant result is one where the observed p-value is less than 5%.[28] When drawing data from a sample, this means that the rejection region comprises 5% of the sampling distribution.[29] These 5% can be allocated to one side of the sampling distribution, as in a one-tailed test, or partitioned to both sides of the distribution as in a two-tailed test, with each tail (or rejection region) containing 2.5% of the distribution.To determine whether a result is statistically significant, a researcher calculates a p-value, which is the probability of observing an effect of the same magnitude or more extreme given that the null hypothesis is true.[11] The null hypothesis is rejected if the p-value is less than a predetermined level, \u03b1. \u03b1 is called the significance level, and is the probability of rejecting the null hypothesis given that it is true (a type I error). It is usually set at or below 5%.Statistical significance plays a pivotal role in statistical hypothesis testing. It is used to determine whether the null hypothesis should be rejected or retained. The null hypothesis is the default assumption that nothing happened or changed.[26] For the null hypothesis to be rejected, an observed result has to be statistically significant, i.e. the observed p-value is less than the pre-specified significance level.Sometimes researchers talk about the confidence level \u03b3 = (1 \u2212 \u03b1) instead. This is the probability of not rejecting the null hypothesis given that it is true.[23][24] Confidence levels and confidence intervals were introduced by Neyman in 1937.[25]The significance level \u03b1 is the threshold for p below which the experimenter assumes the null hypothesis is false, and something else is going on. This means \u03b1 is also the probability of mistakenly rejecting the null hypothesis, if the null hypothesis is true.[4]Despite his initial suggestion of 0.05 as a significance level, Fisher did not intend this cutoff value to be fixed. In his 1956 publication Statistical methods and scientific inference, he recommended that significance levels be set according to specific circumstances.[21]In 1925, Ronald Fisher advanced the idea of statistical hypothesis testing, which he called \"tests of significance\", in his publication Statistical Methods for Research Workers.[18][19][20] Fisher suggested a probability of one in twenty (0.05) as a convenient cutoff level to reject the null hypothesis.[21] In a 1933 paper, Jerzy Neyman and Egon Pearson called this cutoff the significance level, which they named \u03b1. They recommended that \u03b1 be set ahead of time, prior to any data collection.[21][22]The term significance does not imply importance here, and the term statistical significance is not the same as research, theoretical, or practical significance.[1][2][17] For example, the term clinical significance refers to the practical importance of a treatment effect.The significance level for a study is chosen before data collection, and typically set to 5%[12] or much lower, depending on the field of study.[13] In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone.[14][15] But if the p-value of an observed effect is less than the significance level, an investigator may conclude that the effect reflects the characteristics of the whole population,[1] thereby rejecting the null hypothesis.[16] This technique for testing the significance of results was developed in the early 20th century.In statistical hypothesis testing,[1][2] a result has statistical significance when it is very unlikely to have occurred given the null hypothesis.[3] More precisely, a study's defined significance level, \u03b1, is the probability of the study rejecting the null hypothesis, given that it were true;[4] and the p-value of a result, p, is the probability of obtaining a result at least as extreme, given that the null hypothesis were true. The result is statistically significant, by the standards of the study, when p\u00a0<\u00a0\u03b1.[5][6][7][8][9][10][11]",
            "title": "Statistical significance",
            "url": "https://en.wikipedia.org/wiki/Statistical_significance"
        },
        {
            "desc_links": [
                "/wiki/Exploratory_data_analysis",
                "/wiki/Statistical_model",
                "/wiki/Model_selection",
                "/wiki/Akaike_information_criterion",
                "/wiki/Bayes_factor",
                "/wiki/Hypothesis",
                "/wiki/Observable_variable",
                "/wiki/Statistical_model",
                "/wiki/Random_variable",
                "/wiki/Statistical_inference",
                "/wiki/Alternative_hypothesis",
                "/wiki/Statistically_significant",
                "/wiki/Null_hypothesis",
                "/wiki/Alternative_hypothesis",
                "/wiki/Type_I_and_type_II_errors"
            ],
            "links": [
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Bible_Analyzer",
                "/wiki/Correlation_does_not_imply_causation",
                "/wiki/Design_of_experiments",
                "/wiki/Inferential_statistics",
                "/wiki/David_Hume",
                "/wiki/Probability#Interpretations",
                "/wiki/Philosophy_of_science",
                "/wiki/Objectivity_(science)",
                "/wiki/Probability",
                "/wiki/Hypothesis",
                "/wiki/Ronald_Fisher",
                "/wiki/Neyman%E2%80%93Pearson_lemma",
                "/wiki/Bayes%27_Theorem",
                "/wiki/Subjectivity",
                "/wiki/Prior_probability",
                "/wiki/P-value",
                "/wiki/Bayesian_inference",
                "/wiki/Parameter_estimation",
                "/wiki/Priors",
                "/wiki/Student%27s_t-test",
                "/wiki/Bayes_factors",
                "/wiki/Meta-analysis",
                "/wiki/Confidence_interval",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Sample_size_determination",
                "/wiki/Statistical_process_control",
                "/wiki/Detection_theory",
                "/wiki/Decision_theory",
                "/wiki/Game_theory",
                "/wiki/John_Tukey",
                "/wiki/Neyman%E2%80%93Pearson_lemma",
                "/wiki/Likelihood-ratio_test",
                "/wiki/Philosophic_burden_of_proof#Proving_a_negative",
                "/wiki/Falsifiability",
                "/wiki/Karl_Pearson",
                "/wiki/Contingency_table",
                "/wiki/Statistical_independence",
                "/wiki/Principle_of_indifference",
                "/wiki/Ronald_Fisher",
                "/wiki/Karl_Pearson",
                "/wiki/Chi_squared_test",
                "/wiki/Walter_Frank_Raphael_Weldon",
                "/wiki/Pierre_Laplace",
                "/wiki/Paul_Meehl",
                "/wiki/Epistemological",
                "/wiki/Have_one%27s_cake_and_eat_it_too",
                "/wiki/Test_statistic",
                "/wiki/Null_hypothesis",
                "/wiki/Objectivity_(science)",
                "/wiki/Strawman",
                "/wiki/Detection_theory",
                "/wiki/Neyman%E2%80%93Pearson_lemma",
                "/wiki/Fiducial_inference",
                "/wiki/Pierre-Simon_Laplace",
                "/wiki/Karl_Pearson",
                "/wiki/P-value",
                "/wiki/Pearson%27s_chi-squared_test",
                "/wiki/William_Sealy_Gosset",
                "/wiki/Student%27s_t-distribution",
                "/wiki/Ronald_Fisher",
                "/wiki/Null_hypothesis",
                "/wiki/Analysis_of_variance",
                "/wiki/Statistical_significance",
                "/wiki/Jerzy_Neyman",
                "/wiki/Egon_Pearson",
                "/wiki/Principle_of_indifference",
                "/wiki/Statistical_significance",
                "/wiki/Sample_(statistics)",
                "/wiki/Geiger_counter",
                "/wiki/Poisson_distribution",
                "/wiki/Radioactive_decay",
                "/wiki/False_positive",
                "/wiki/Clairvoyance",
                "/wiki/Suit_(cards)",
                "/wiki/Error_of_the_first_kind",
                "/wiki/Error_of_the_second_kind",
                "/wiki/Trial_(law)",
                "/wiki/Muriel_Bristol",
                "/wiki/Publication_bias",
                "/wiki/Multiple_testing",
                "/wiki/Data_mining",
                "/wiki/Family_wise_error_rate",
                "/wiki/False_discovery_rate",
                "/wiki/How_to_Lie_with_Statistics",
                "/wiki/Forecasting",
                "/wiki/Effect_size",
                "/wiki/Scientific_method",
                "/wiki/Statistical_inference",
                "/wiki/Bigfoot",
                "/wiki/Proof_by_contradiction",
                "/wiki/Ronald_Fisher",
                "/wiki/Design_of_experiments",
                "/wiki/Fallacy",
                "/wiki/Argument_from_ignorance",
                "/wiki/Statistical_power",
                "/wiki/Bayesian_statistics",
                "/wiki/Posterior_probability",
                "/wiki/Bayesian_decision_theory",
                "/wiki/Decision_theory",
                "/wiki/Optimal_decision",
                "/wiki/Statistical_power",
                "/wiki/Sample_size_determination",
                "/wiki/Frequentist_inference",
                "/wiki/Bayesian_inference",
                "/wiki/Null_hypothesis",
                "/wiki/Decision_theory",
                "/wiki/Alternative_hypothesis",
                "/wiki/Exploratory_data_analysis",
                "/wiki/Statistical_model",
                "/wiki/Model_selection",
                "/wiki/Akaike_information_criterion",
                "/wiki/Bayes_factor",
                "/wiki/Hypothesis",
                "/wiki/Observable_variable",
                "/wiki/Statistical_model",
                "/wiki/Random_variable",
                "/wiki/Statistical_inference",
                "/wiki/Alternative_hypothesis",
                "/wiki/Statistically_significant",
                "/wiki/Null_hypothesis",
                "/wiki/Alternative_hypothesis",
                "/wiki/Type_I_and_type_II_errors"
            ],
            "text": "An academic study states that the cookbook method of teaching introductory statistics leaves no time for history, philosophy or controversy. Hypothesis testing has been taught as received unified method. Surveys showed that graduates of the class were filled with philosophical misconceptions (on all aspects of statistical inference) that persisted among instructors.[77] While the problem was addressed more than a decade ago,[78] and calls for educational reform continue,[79] students still graduate from statistics classes holding fundamental misconceptions about hypothesis testing.[80] Ideas for improving the teaching of hypothesis testing include encouraging students to search for statistical errors in published papers, teaching the history of statistics and emphasizing the controversy in a generally dry subject.[81]Statistics is increasingly being taught in schools with hypothesis testing being one of the elements taught.[73][74] Many conclusions reported in the popular press (political opinion polls to medical studies) are based on statistics. Some writers have stated that statistical analysis of this kind allows for thinking clearly about problems involving mass data, as well as the effective reporting of trends and inferences from said data, but caution that writers for a broad public should have a solid understanding of the field in order to use the terms and concepts correctly.[75][76][citation needed][75][76][citation needed] An introductory college statistics class places much emphasis on hypothesis testing \u2013 perhaps half of the course. Such fields as literature and divinity now include findings based on statistical analysis (see the Bible Analyzer). An introductory statistics class teaches hypothesis testing as a cookbook process. Hypothesis testing is also taught at the postgraduate level. Statisticians learn how to create good statistical test procedures (like z, Student's t, F and chi-squared). Statistical hypothesis testing is considered a mature area within statistics,[66] but a limited amount of development continues.Many of the philosophical criticisms of hypothesis testing are discussed by statisticians in other contexts, particularly correlation does not imply causation and the design of experiments. Hypothesis testing is of continuing interest to philosophers.[31][72]Fisher and Neyman opposed the subjectivity of probability. Their views contributed to the objective definitions. The core of their historical disagreement was philosophical.Hypothesis testing and philosophy intersect. Inferential statistics, which includes hypothesis testing, is applied probability. Both probability and its application are intertwined with philosophy. Philosopher David Hume wrote, \"All knowledge degenerates into probability.\" Competing practical definitions of probability reflect philosophical differences. The most common application of hypothesis testing is in the scientific interpretation of experimental data, which is naturally studied by the philosophy of science.Advocates of a Bayesian approach sometimes claim that the goal of a researcher is most often to objectively assess the probability that a hypothesis is true based on the data they have collected.[69][70] Neither Fisher's significance testing, nor Neyman\u2013Pearson hypothesis testing can provide this information, and do not claim to. The probability a hypothesis is true can only be derived from use of Bayes' Theorem, which was unsatisfactory to both the Fisher and Neyman\u2013Pearson camps due to the explicit use of subjectivity in the form of the prior probability.[27][71] Fisher's strategy is to sidestep this with the p-value (an objective index based on the data alone) followed by inductive inference, while Neyman\u2013Pearson devised their approach of inductive behaviour.Bayesian inference is one proposed alternative to significance testing. (Nickerson cited 10 sources suggesting it, including Rozeboom (1960)).[59] For example, Bayesian parameter estimation can provide rich information about the data from which researchers can draw inferences, while using uncertain priors that exert only minimal influence on the results when enough data is available. Psychologist John K. Kruschke has suggested Bayesian estimation as an alternative for the t-test.[67] Alternatively two competing models/hypothesis can be compared using Bayes factors.[68] Bayesian methods could be criticized for requiring information that is seldom available in the cases where significance testing is most heavily used. Neither the prior probabilities nor the probability distribution of the test statistic under the alternative hypothesis are often available in the social sciences.[59]On one \"alternative\" there is no disagreement: Fisher himself said,[19] \"In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.\" Cohen, an influential critic of significance testing, concurred,[58] \"... don't look for a magic alternative to NHST [null hypothesis significance testing] ... It doesn't exist.\" \"... given the problems of statistical induction, we must finally rely, as have the older sciences, on replication.\" The \"alternative\" to significance testing is repeated testing. The easiest way to decrease statistical uncertainty is by obtaining more data, whether by increased sample size or by repeated tests. Nickerson claimed to have never seen the publication of a literally replicated experiment in psychology.[59] An indirect approach to replication is meta-analysis.The numerous criticisms of significance testing do not lead to a single alternative. A unifying position of critics is that statistics should not lead to a conclusion or a decision but to a probability or to an estimated value with a confidence interval rather than to an accept-reject decision regarding a particular hypothesis. It is unlikely that the controversy surrounding significance testing will be resolved in the near future. Its supposed flaws and unpopularity do not eliminate the need for an objective and transparent means of reaching conclusions regarding studies that produce statistical results. Critics have not unified around an alternative. Other forms of reporting confidence or uncertainty could probably grow in popularity. One strong critic of significance testing suggested a list of reporting alternatives:[65] effect sizes for importance, prediction intervals for confidence, replications and extensions for replicability, meta-analyses for generality. None of these suggested alternatives produces a conclusion/decision. Lehmann said that hypothesis testing theory can be presented in terms of conclusions/decisions, probabilities, or confidence intervals. \"The distinction between the ... approaches is largely one of reporting and interpretation.\"[66]Controversy over significance testing, and its effects on publication bias in particular, has produced several results. The American Psychological Association has strengthened its statistical reporting requirements after review,[61] medical journal publishers have recognized the obligation to publish some results that are not statistically significant to combat publication bias[62] and a journal (Journal of Articles in Support of the Null Hypothesis) has been created to publish such results exclusively.[63] Textbooks have added some cautions[64] and increased coverage of the tools necessary to estimate the size of the sample required to produce significant results. Major organizations have not abandoned use of significance tests although some have discussed doing so.[61]Critics and supporters are largely in factual agreement regarding the characteristics of null hypothesis significance testing (NHST): While it can provide critical information, it is inadequate as the sole tool for statistical analysis. Successfully rejecting the null hypothesis may offer no support for the research hypothesis. The continuing controversy concerns the selection of the best statistical practices for the near-term future given the (often poor) existing practices. Critics would prefer to ban NHST completely, forcing a complete departure from those practices, while supporters suggest a less absolute change.[citation needed]Criticism of statistical hypothesis testing fills volumes[46][47][48][49][50][51] citing 300\u2013400 primary references[citation needed]. Much of the criticism can be summarized by the following issues:Fisher thought that hypothesis testing was a useful strategy for performing industrial quality control, however, he strongly disagreed that hypothesis testing could be useful for scientists.[26] Hypothesis testing provides a means of finding test statistics used in significance testing.[29] The concept of power is useful in explaining the consequences of adjusting the significance level and is heavily used in sample size determination. The two methods remain philosophically distinct.[31] They usually (but not always) produce the same mathematical answer. The preferred answer is context dependent.[29] While the existing merger of Fisher and Neyman\u2013Pearson theories has been heavily criticized, modifying the merger to achieve Bayesian goals has been considered.[45]The terminology is inconsistent. Hypothesis testing can mean any mixture of two formulations that both changed with time. Any discussion of significance testing vs hypothesis testing is doubly vulnerable to confusion.The dispute over formulations is unresolved. Science primarily uses Fisher's (slightly modified) formulation as taught in introductory statistics. Statisticians study Neyman\u2013Pearson theory in graduate school. Mathematicians are proud of uniting the formulations. Philosophers consider them separately. Learned opinions deem the formulations variously competitive (Fisher vs Neyman), incompatible[23] or complementary.[29] The dispute has become more complex since Bayesian inference has achieved respectability.Fisher's significance testing has proven a popular flexible statistical tool in application with little mathematical growth potential. Neyman\u2013Pearson hypothesis testing is claimed as a pillar of mathematical statistics,[44] creating a new paradigm for the field. It also stimulated new applications in statistical process control, detection theory, decision theory and game theory. Both formulations have been successful, but the successes have been of a different character.The two forms of hypothesis testing are based on different problem formulations. The original test is analogous to a true/false question; the Neyman\u2013Pearson test is more like multiple choice. In the view of Tukey[43] the former produces a conclusion on the basis of only strong evidence while the latter produces a decision on the basis of available evidence. While the two tests seem quite different both mathematically and philosophically, later developments lead to the opposite claim. Consider many tiny radioactive sources. The hypotheses become 0,1,2,3... grains of radioactive sand. There is little distinction between none or some radiation (Fisher) and 0 grains of radioactive sand versus all of the alternatives (Neyman\u2013Pearson). The major Neyman\u2013Pearson paper of 1933[27] also considered composite hypotheses (ones whose distribution includes an unknown parameter). An example proved the optimality of the (Student's) t-test, \"there can be no better test for the hypothesis under consideration\" (p 321). Neyman\u2013Pearson theory was proving the optimality of Fisherian methods from its inception.Neyman\u2013Pearson theory can accommodate both prior probabilities and the costs of actions resulting from decisions.[42] The former allows each test to consider the results of earlier tests (unlike Fisher's significance tests). The latter allows the consideration of economic issues (for example) as well as probabilities. A likelihood ratio remains a good criterion for selecting among hypotheses.An example of Neyman\u2013Pearson hypothesis testing can be made by a change to the radioactive suitcase example. If the \"suitcase\" is actually a shielded container for the transportation of radioactive material, then a test might be used to select among three hypotheses: no radioactive source present, one present, two (all) present. The test could be required for safety, with actions required in each case. The Neyman\u2013Pearson lemma of hypothesis testing says that a good criterion for the selection of hypotheses is the ratio of their probabilities (a likelihood ratio). A simple method of solution is to select the hypothesis with the highest probability for the Geiger counts observed. The typical result matches intuition: few counts imply no source, many counts imply two sources and intermediate counts imply one source. Notice also that usually there are problems for proving a negative. Null hypotheses should be at least falsifiable.1904: Karl Pearson develops the concept of \"contingency\" in order to determine whether outcomes are independent of a given categorical factor. Here the null hypothesis is by default that two things are unrelated (e.g. scar formation and death rates from smallpox).[40] The null hypothesis in this case is no longer predicted by theory or conventional wisdom, but is instead the principle of indifference that lead Fisher and others to dismiss the use of \"inverse probabilities\".[41]1900: Karl Pearson develops the chi squared test to determine \"whether a given form of frequency curve will effectively describe the samples drawn from a given population.\" Thus the null hypothesis is that a population is described by some distribution predicted by theory. He uses as an example the numbers of five and sixes in the Weldon dice throw data.[39]1778: Pierre Laplace compares the birthrates of boys and girls in multiple European cities. He states: \"it is natural to conclude that these possibilities are very nearly in the same ratio\". Thus Laplace's null hypothesis that the birthrates of boys and girls should be equal given \"conventional wisdom\".[24]Paul Meehl has argued that the epistemological importance of the choice of null hypothesis has gone largely unacknowledged. When the null hypothesis is predicted by theory, a more precise experiment will be a more severe test of the underlying theory. When the null hypothesis defaults to \"no difference\" or \"no effect\", a more precise experiment is a less severe test of the theory that motivated performing the experiment.[38] An examination of the origins of the latter practice may therefore be useful:A comparison between Fisherian, frequentist (Neyman\u2013Pearson)Sometime around 1940,[34] in an apparent effort to provide researchers with a \"non-controversial\"[36] way to have their cake and eat it too, the authors of statistical text books began anonymously combining these two strategies by using the p-value in place of the test statistic (or data) to test against the Neyman\u2013Pearson \"significance level\".[34] Thus, researchers were encouraged to infer the strength of their data against some null hypothesis using p-values, while also thinking they are retaining the post-data collection objectivity provided by hypothesis testing. It then became customary for the null hypothesis, which was originally some realistic research hypothesis, to be used almost solely as a strawman \"nil\" hypothesis (one where a treatment has no effect, regardless of the context).[37]The modern version of hypothesis testing is a hybrid of the two approaches that resulted from confusion by writers of statistical textbooks (as predicted by Fisher) beginning in the 1940s.[34] (But signal detection, for example, still uses the Neyman/Pearson formulation.) Great conceptual differences and many caveats in addition to those mentioned above were ignored. Neyman and Pearson provided the stronger terminology, the more rigorous mathematics and the more consistent philosophy, but the subject taught today in introductory statistics has more similarities with Fisher's method than theirs.[35] This history explains the inconsistent terminology (example: the null hypothesis is never accepted, but there is a region of acceptance).Events intervened: Neyman accepted a position in the western hemisphere, breaking his partnership with Pearson and separating disputants (who had occupied the same building) by much of the planetary diameter. World War II provided an intermission in the debate. The dispute between Fisher and Neyman terminated (unresolved after 27 years) with Fisher's death in 1962. Neyman wrote a well-regarded eulogy.[32] Some of Neyman's later publications reported p-values and significance levels.[33]The dispute between Fisher and Neyman\u2013Pearson was waged on philosophical grounds, characterized by a philosopher as a dispute over the proper role of models in statistical inference.[31]Fisher and Neyman/Pearson clashed bitterly. Neyman/Pearson considered their formulation to be an improved generalization of significance testing.(The defining paper[27] was abstract. Mathematicians have generalized and refined the theory for decades.[29]) Fisher thought that it was not applicable to scientific research because often, during the course of the experiment, it is discovered that the initial assumptions about the null hypothesis are questionable due to unexpected sources of error. He believed that the use of rigid reject/accept decisions based on models formulated before data is collected was incompatible with this common scenario faced by scientists and attempts to apply this method to scientific research would lead to mass confusion.[30]Neyman & Pearson considered a different problem (which they called \"hypothesis testing\"). They initially considered two simple hypotheses (both with frequency distributions). They calculated two probabilities and typically selected the hypothesis associated with the higher probability (the hypothesis more likely to have generated the sample). Their method always selected a hypothesis. It also allowed the calculation of both types of error probabilities.The p-value was devised as an informal, but objective, index meant to help a researcher determine (based on other knowledge) whether to modify future experiments or strengthen one's faith in the null hypothesis.[26] Hypothesis testing (and Type I/II errors) was devised by Neyman and Pearson as a more objective alternative to Fisher's p-value, also meant to determine researcher behaviour, but without requiring any inductive inference by the researcher.[27][28]Fisher popularized the \"significance test\". He required a null-hypothesis (corresponding to a population frequency distribution) and a sample. His (now familiar) calculations determined whether to reject the null-hypothesis or not. Significance testing did not utilize an alternative hypothesis so there was no concept of a Type II error.Fisher was an agricultural statistician who emphasized rigorous experimental design and methods to extract a result from few samples assuming Gaussian distributions. Neyman (who teamed with the younger Pearson) emphasized mathematical rigor and methods to obtain more results from many samples and a wider range of distributions. Modern hypothesis testing is an inconsistent hybrid of the Fisher vs Neyman/Pearson formulation, methods and terminology developed in the early 20th century. While hypothesis testing was popularized early in the 20th century, evidence of its use can be found much earlier. In the 1770s Laplace considered the statistics of almost half a million births. The statistics showed an excess of boys compared to girls.[24] He concluded by calculation of a p-value that the excess was a real, but unexplained, effect.[25]Significance testing is largely the product of Karl Pearson (p-value, Pearson's chi-squared test), William Sealy Gosset (Student's t-distribution), and Ronald Fisher (\"null hypothesis\", analysis of variance, \"significance test\"), while hypothesis testing was developed by Jerzy Neyman and Egon Pearson (son of Karl). Ronald Fisher began his life in statistics as a Bayesian (Zabell 1992), but Fisher soon grew disenchanted with the subjectivity involved (namely use of the principle of indifference when determining prior probabilities), and sought to provide a more \"objective\" approach to inductive inference.[23]A statistical hypothesis test compares a test statistic (z or t for examples) to a threshold. The test statistic (the formula found in the table below) is based on optimality. For a fixed level of Type I error rate, use of these statistics minimizes Type II error rates (equivalent to maximizing power). The following terms describe tests in terms of such optimality:The following definitions are mainly based on the exposition in the book by Lehmann and Romano:[5]The test described here is more fully the null-hypothesis statistical significance test. The null hypothesis represents what we would believe by default, before seeing any evidence. Statistical significance is a possible finding of the test, declared when the observed sample is unlikely to have occurred by chance if the null hypothesis were true. The name of the test describes its formulation and its possible outcome. One characteristic of the test is its crisp decision: to reject or not reject the null hypothesis. A calculated value is compared to a threshold, which is determined from the tolerable risk of error.To slightly formalize intuition: radioactivity is suspected if the Geiger-count with the suitcase is among or exceeds the greatest (5% or 1%) of the Geiger-counts made with ambient radiation alone. This makes no assumptions about the distribution of counts. Many ambient radiation observations are required to obtain good probability estimates for rare events.The test does not directly assert the presence of radioactive material. A successful test asserts that the claim of no radioactive material present is unlikely given the reading (and therefore ...). The double negative (disproving the null hypothesis) of the method is confusing, but using a counter-example to disprove is standard mathematical practice. The attraction of the method is its practicality. We know (from experience) the expected range of counts with only ambient radioactivity present, so we can say that a measurement is unusually large. Statistics just formalizes the intuitive by using numbers instead of adjectives. We probably do not know the characteristics of the radioactive suitcases; We just assume that they produce larger readings.As an example, consider determining whether a suitcase contains some radioactive material. Placed under a Geiger counter, it produces 10 counts per minute. The null hypothesis is that no radioactive material is in the suitcase and that all measured counts are due to ambient radioactivity typical of the surrounding air and harmless objects. We can then calculate how likely it is that we would observe 10 counts per minute if the null hypothesis were true. If the null hypothesis predicts (say) on average 9 counts per minute, then according to the Poisson distribution typical for radioactive decay there is about 41% chance of recording 10 or more counts. Thus we can say that the suitcase is compatible with the null hypothesis (this does not guarantee that there is no radioactive material, just that we don't have enough evidence to suggest there is). On the other hand, if the null hypothesis predicts 3 counts per minute (for which the Poisson distribution predicts only 0.1% chance of recording 10 or more counts) then the suitcase is not compatible with the null hypothesis, and there are likely other factors responsible to produce the measurements.Before the test is actually performed, the maximum acceptable probability of a Type I error (\u03b1) is determined. Typically, values in the range of 1% to 5% are selected. (If the maximum acceptable error rate is zero, an infinite number of correct guesses is required.) Depending on this Type 1 error rate, the critical value c is calculated. For example, if we select an error rate of 1%, c is calculated thus:Thus, c = 10 yields a much greater probability of false positive.Being less critical, with c=10, gives:and hence, very small. The probability of a false positive is the probability of randomly guessing correctly all 25 times.When the test subject correctly predicts all 25 cards, we will consider him clairvoyant, and reject the null hypothesis. Thus also with 24 or 23 hits. With only 5 or 6 hits, on the other hand, there is no cause to consider him so. But what about 12 hits, or 17 hits? What is the critical number, c, of hits, at which point we consider the subject to be clairvoyant? How do we determine the critical value c? It is obvious that with the choice c=25 (i.e. we only accept clairvoyance when all cards are predicted correctly) we're more critical than with c=10. In the first case almost no test subjects will be recognized to be clairvoyant, in the second case, a certain number will pass the test. In practice, one decides how critical one will be. That is, one decides how often one accepts an error of the first kind \u2013 a false positive, or Type I error. With c = 25 the probability of such an error is:andIf the null hypothesis is valid, the only thing the test person can do is guess. For every card, the probability (relative frequency) of any single suit appearing is 1/4. If the alternative is valid, the test subject will predict the suit correctly with probability greater than 1/4. We will call the probability of guessing correctly p. The hypotheses, then, are:As we try to find evidence of his clairvoyance, for the time being the null hypothesis is that the person is not clairvoyant.[22] The alternative is: the person is (more or less) clairvoyant.A person (the subject) is tested for clairvoyance. He is shown the reverse of a randomly chosen playing card 25 times and asked which of the four suits it belongs to. The number of hits, or correct answers, is called X.The statement also relies on the inference that the sampling was random. If someone had been picking through the bag to find white beans, then it would explain why the handful had so many white beans, and also explain why the number of white beans in the bag was depleted (although the bag is probably intended to be assumed much larger than one's hand).A simple generalization of the example considers a mixed bag of beans and a handful that contain either very few or very many white beans. The generalization considers both extremes. It requires more calculations and more comparisons to arrive at a formal answer, but the core philosophy is unchanged; If the composition of the handful is greatly different from that of the bag, then the sample probably originated from another bag. The original example is termed a one-sided or a one-tailed test while the generalization is termed a two-sided or two-tailed test.The beans in the bag are the population. The handful are the sample. The null hypothesis is that the sample originated from the population. The criterion for rejecting the null-hypothesis is the \"obvious\" difference in appearance (an informal difference in the mean). The interesting result is that consideration of a real population and a real sample produced an imaginary bag. The philosopher was considering logic rather than probability. To be a real statistical hypothesis test, this example requires the formalities of a probability calculation and a comparison of that probability to a standard.The following example was produced by a philosopher describing scientific methods generations before hypothesis testing was formalized and popularized.[21]A criminal trial can be regarded as either or both of two decision processes: guilty vs not guilty or evidence vs a threshold (\"beyond a reasonable doubt\"). In one view, the defendant is judged; in the other view the performance of the prosecution (which bears the burden of proof) is judged. A hypothesis test can be regarded as either a judgment of a hypothesis or as a judgment of evidence.The hypothesis of innocence is only rejected when an error is very unlikely, because one doesn't want to convict an innocent defendant. Such an error is called error of the first kind (i.e., the conviction of an innocent person), and the occurrence of this error is controlled to be rare. As a consequence of this asymmetric behaviour, an error of the second kind (acquitting a person who committed the crime), is more common.A statistical test procedure is comparable to a criminal trial; a defendant is considered not guilty as long as his or her guilt is not proven. The prosecutor tries to prove the guilt of the defendant. Only when there is enough evidence for the prosecution is the defendant convicted.In a famous example of hypothesis testing, known as the Lady tasting tea,[19] Dr. Muriel Bristol, a female colleague of Fisher claimed to be able to tell whether the tea or the milk was added first to a cup. Fisher proposed to give her eight cups, four of each variety, in random order. One could then ask what the probability was for her getting the number she got correct, but just by chance. The null hypothesis was that the Lady had no such ability. The test statistic was a simple count of the number of successes in selecting the 4 cups. The critical region was the single case of 4 successes of 4 possible based on a conventional probability criterion (<\u00a05%; 1 of 70 \u2248\u00a01.4%). Fisher asserted that no alternative hypothesis was (ever) required. The lady correctly identified every cup,[20] which would be considered a statistically significant result.Those making critical decisions based on the results of a hypothesis test are prudent to look at the details rather than the conclusion alone. In the physical sciences most results are fully accepted only when independently confirmed. The general advice concerning statistics is, \"Figures never lie, but liars figure\" (anonymous).Hypothesis testing acts as a filter of statistical conclusions; only those results meeting a probability threshold are publishable. Economics also acts as a publication filter; only those results favorable to the author and funding source may be submitted for publication. The impact of filtering on publication is termed publication bias. A related problem is that of multiple testing (sometimes linked to data mining), in which a variety of tests for a variety of possible effects are applied to a single data set and only those yielding a significant result are reported. These are often dealt with by using multiplicity correction procedures that control the family wise error rate (FWER) or the false discovery rate (FDR).The book How to Lie with Statistics[16][17] is the most popular book on statistics ever published.[18] It does not much consider hypothesis testing, but its cautions are applicable, including: Many claims are made on the basis of samples too small to convince. If a report does not mention sample size, be doubtful.A statistical analysis of misleading data produces misleading conclusions. The issue of data quality can be more subtle. In forecasting for example, there is no agreement on a measure of forecast accuracy. In the absence of a consensus measurement, no decision based on measurements will be without controversy.The conclusion of the test is only as solid as the sample upon which it is based. The design of the experiment is critical. A number of unexpected effects have been observed including:The successful hypothesis test is associated with a probability and a type-I error rate. The conclusion might be wrong.\"If the government required statistical procedures to carry warning labels like those on drugs, most inference methods would have long labels indeed.\"[15] This caution applies to hypothesis tests and alternatives to them.Significance testing has been the favored statistical tool in some experimental social sciences (over 90% of articles in the Journal of Applied Psychology during the early 1990s).[14] Other fields have favored the estimation of parameters (e.g., effect size). Significance testing is used as a substitute for the traditional comparison of predicted value and experimental result at the core of the scientific method. When theory is only capable of predicting the sign of a relationship, a directional (one-sided) hypothesis test can be configured so that only a statistically significant result supports theory. This form of theory appraisal is the most heavily criticized application of hypothesis testing.Statistical hypothesis testing plays an important role in the whole of statistics and in statistical inference. For example, Lehmann (1992) in a review of the fundamental paper by Neyman and Pearson (1933) says: \"Nevertheless, despite their shortcomings, the new paradigm formulated in the 1933 paper, and the many developments carried out within its framework continue to play a central role in both the theory and practice of statistics and can be expected to do so in the foreseeable future\".Real world applications of hypothesis testing include:[13]Statistics are helpful in analyzing most collections of data. This is equally true of hypothesis testing which can justify conclusions even when no scientific theory exists. In the Lady tasting tea example, it was \"obvious\" that no difference existed between (milk poured into tea) and (tea poured into milk). The data contradicted the \"obvious\".\"The probability of rejecting the null hypothesis is a function of five factors: whether the test is one- or two tailed, the level of significance, the standard deviation, the amount of deviation from the null hypothesis, and the number of observations.\"[12] These factors are a source of criticism; factors under the control of the experimenter/analyst give the results an appearance of subjectivity.Whether rejection of the null hypothesis truly justifies acceptance of the research hypothesis depends on the structure of the hypotheses. Rejecting the hypothesis that a large paw print originated from a bear does not immediately prove the existence of Bigfoot. Hypothesis testing emphasizes the rejection, which is based on a probability, rather than the acceptance, which requires extra steps of logic.In the Lady tasting tea example (below), Fisher required the Lady to properly categorize all of the cups of tea to justify the conclusion that the result was unlikely to result from chance. His test revealed that if the lady was effectively guessing at random (the null hypothesis), there was a 1.4% chance that the observed results (perfectly ordered tea) would occur.Some people find it helpful to think of the hypothesis testing framework as analogous to a mathematical proof by contradiction.[11]If the p-value is not less than the chosen significance threshold (equivalently, if the observed test statistic is outside the critical region), then the evidence is insufficient to support a conclusion. (This is similar to a \"not guilty\" verdict.) The researcher typically gives extra consideration to those cases where the p-value is close to the significance level.If the p-value is less than the chosen significance threshold (equivalently, if the observed test statistic is in the critical region), then we say the null hypothesis is rejected at the chosen level of significance. Rejection of the null hypothesis is a conclusion. This is like a \"guilty\" verdict in a criminal trial: the evidence is sufficient to reject innocence, thus proving guilt. We might accept the alternative hypothesis (and the research hypothesis).The p-value is the probability that a given result (or a more significant result) would occur under the null hypothesis. For example, say that a fair coin is tested for fairness (the null hypothesis). At a significance level of 0.05, the fair coin would be expected to (incorrectly) reject the null hypothesis in about 1 out of every 20 tests. The p-value does not provide the probability that either hypothesis is correct (a common source of confusion).[10]The phrase \"test of significance\" was coined by statistician Ronald Fisher.[9]It is particularly critical that appropriate sample sizes be estimated before conducting the experiment.The processes described here are perfectly adequate for computation. They seriously neglect the design of experiments considerations.[7][8]It is important to note the difference between accepting the null hypothesis and simply failing to reject it. The \"fail to reject\" terminology highlights the fact that the null hypothesis is assumed to be true from the start of the test; if there is a lack of evidence against it, it simply continues to be assumed true. The phrase \"accept the null hypothesis\" may suggest it has been proved simply because it has not been disproved, a logical fallacy known as the argument from ignorance. Unless a test with particularly high power is used, the idea of \"accepting\" the null hypothesis may be dangerous. Nonetheless the terminology is prevalent throughout statistics, where the meaning actually intended is well understood.The former report is adequate, the latter gives a more detailed explanation of the data and the reason why the suitcase is being checked.The difference in the two processes applied to the Radioactive suitcase example (below):The latter process relied on extensive tables or on computational support not always available. The explicit calculation of a probability is useful for reporting. The calculations are now trivially performed with appropriate software.The two processes are equivalent.[6] The former process was advantageous in the past when only tables of test statistics at common probability thresholds were available. It allowed a decision to be made without the calculation of a probability. It was adequate for classwork and for operational use, but it was deficient for reporting results.An alternative process is commonly used:In the statistics literature, statistical hypothesis testing plays a fundamental role.[5] The usual line of reasoning is as follows:One na\u00efve Bayesian approach to hypothesis testing is to base decisions on the posterior probability,[3][4] but this fails when comparing point and continuous hypotheses. Other approaches to decision making, such as Bayesian decision theory, attempt to balance the consequences of incorrect decisions across all possibilities, rather than concentrating on a single null hypothesis. A number of other approaches to reaching a decision based on data are available via decision theory and optimal decisions, some of which have desirable properties. Hypothesis testing, though, is a dominant approach to data analysis in many fields of science. Extensions to the theory of hypothesis testing include the study of the power of tests, i.e. the probability of correctly rejecting the null hypothesis given that it is false. Such considerations can be used for the purpose of sample size determination prior to the collection of data.Statistical hypothesis testing is a key technique of both frequentist inference and Bayesian inference, although the two types of inference have notable differences. Statistical hypothesis tests define a procedure that controls (fixes) the probability of incorrectly deciding that a default position (null hypothesis) is incorrect. The procedure is based on how likely it would be for a set of observations to occur if the null hypothesis were true. Note that this probability of making an incorrect decision is not the probability that the null hypothesis is true, nor whether any specific alternative hypothesis is true. This contrasts with other possible techniques of decision theory in which the null and alternative hypothesis are treated on a more equal basis.Confirmatory data analysis can be contrasted with exploratory data analysis, which may not have pre-specified hypotheses.An alternative framework for statistical hypothesis testing is to specify a set of statistical models, one for each candidate hypothesis, and then use model selection techniques to choose the most appropriate model.[2] The most common selection techniques are based on either Akaike information criterion or Bayes factor.A statistical hypothesis, sometimes called confirmatory data analysis, is a hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables.[1] A statistical hypothesis test is a method of statistical inference. Commonly, two statistical data sets are compared, or a data set obtained by sampling is compared against a synthetic data set from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis that proposes no relationship between two data sets. The comparison is deemed statistically significant if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probability\u2014the significance level. Hypothesis tests are used in determining what outcomes of a study would lead to a rejection of the null hypothesis for a pre-specified level of significance. The process of distinguishing between the null hypothesis and the alternative hypothesis is aided by identifying two conceptual types of errors (type 1 & type 2), and by specifying parametric limits on e.g. how much type 1 error will be permitted.",
            "title": "Statistical hypothesis testing",
            "url": "https://en.wikipedia.org/wiki/Hypothesis_testing"
        },
        {
            "desc_links": [
                "/wiki/Principal_component_analysis",
                "/wiki/Exploratory_factor_analysis",
                "/wiki/Statistics",
                "/wiki/Variance",
                "/wiki/Variable_(mathematics)",
                "/wiki/Latent_variable",
                "/wiki/Linear_combination",
                "/wiki/Errors_and_residuals_in_statistics",
                "/wiki/Psychometrics",
                "/wiki/Personality",
                "/wiki/Marketing",
                "/wiki/Product_management",
                "/wiki/Operations_research",
                "/wiki/Finance"
            ],
            "links": [
                "/wiki/Oligonucleotide",
                "/wiki/DNA_microarrays",
                "/wiki/Affymetrix",
                "/wiki/RNA",
                "/wiki/Geochemistry",
                "/wiki/Geochemistry",
                "/wiki/Hydrochemistry",
                "/wiki/Astrophysics",
                "/wiki/Cosmology",
                "/wiki/Ecology",
                "/wiki/Molecular_biology",
                "/wiki/Biochemistry",
                "/wiki/R_(programming_language)",
                "/wiki/SPSS",
                "/wiki/SAS_System",
                "/wiki/Stata",
                "/wiki/STATISTICA",
                "/wiki/Random_error",
                "/wiki/Exploratory_factor_analysis",
                "/wiki/Principal_component_analysis",
                "/wiki/Psychometrics",
                "/wiki/Crystallized_intelligence",
                "/wiki/Raymond_Cattell",
                "/wiki/Fluid_and_crystallized_intelligence",
                "/wiki/16_Personality_Factors",
                "/wiki/Psychometrics",
                "/wiki/Charles_Spearman",
                "/wiki/Intelligence_(trait)",
                "/wiki/General_intelligence_factor",
                "/wiki/Dataset",
                "/wiki/Interpretability",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Varimax_rotation",
                "/wiki/Orthogonal",
                "/wiki/Occam%27s_razor",
                "/wiki/Eigenvalue,_eigenvector_and_eigenspace",
                "/wiki/Y-axis",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/SPSS",
                "/wiki/Statistical_software",
                "/wiki/Confidence_interval",
                "/wiki/John_L._Horn",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/R_(programming_language)",
                "/wiki/Anton_Formann",
                "/wiki/Sample_size",
                "/wiki/Item_response_theory#The_item_response_function",
                "/wiki/Correlation_coefficient",
                "/wiki/Angle#Types_of_angles",
                "/wiki/Coefficient",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Eigenvalue",
                "/wiki/Correlation_matrix",
                "/wiki/Multiple_regression",
                "/wiki/Principal_component_analysis",
                "/wiki/Confirmatory_factor_analysis",
                "/wiki/Structural_equation_modeling",
                "/wiki/Least-squares_estimation",
                "/wiki/Exploratory_factor_analysis",
                "/wiki/Skew_coordinates",
                "/wiki/Generalized_minimal_residual_method",
                "/wiki/Variance",
                "/wiki/Uncorrelated",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Errors_and_residuals_in_statistics",
                "/wiki/Intelligence_(trait)",
                "/wiki/Evidence",
                "/wiki/Population_(statistics)",
                "/wiki/Constant_(mathematics)",
                "/wiki/Astronomy",
                "/wiki/Principal_component_analysis",
                "/wiki/Exploratory_factor_analysis",
                "/wiki/Statistics",
                "/wiki/Variance",
                "/wiki/Variable_(mathematics)",
                "/wiki/Latent_variable",
                "/wiki/Linear_combination",
                "/wiki/Errors_and_residuals_in_statistics",
                "/wiki/Psychometrics",
                "/wiki/Personality",
                "/wiki/Marketing",
                "/wiki/Product_management",
                "/wiki/Operations_research",
                "/wiki/Finance"
            ],
            "text": "Factor analysis has been implemented in several statistical analysis programs since the 1980s:Factor analysis can be used for summarizing high-density oligonucleotide DNA microarrays data at probe level for Affymetrix GeneChips. In this case, the latent variable corresponds to the RNA concentration in a sample.[33]In geochemistry, different factors can correspond to different mineral associations, and thus to mineralisation.[32]In groundwater quality management, it is important to relate the spatial distribution of different chemical parameters to different possible sources, which have different chemical signatures. For example, a sulfide mine is likely to be associated with high levels of acidity, dissolved sulfates and transition metals. These signatures can be identified as factors through R-mode factor analysis, and the location of possible sources can be suggested by contouring the factor scores.[31]Factor analysis has also been widely used in physical sciences such as geochemistry, hydrochemistry,[30] astrophysics and cosmology, as well as biological sciences, such as ecology, molecular biology and biochemistry.The analysis will isolate the underlying factors that explain the data using a matrix of associations.[29] Factor analysis is an interdependence technique. The complete set of interdependent relationships is examined. There is no specification of dependent variables, independent variables, or causality. Factor analysis assumes that all the rating data on different attributes can be reduced down to a few important dimensions. This reduction is possible because some attributes may be related to each other. The rating given to any one attribute is partially the result of the influence of other attributes. The statistical algorithm deconstructs the rating (called a raw score) into its various components, and reconstructs the partial scores into underlying factor scores. The degree of correlation between the initial raw score and the final factor score is called a factor loading.The data collection stage is usually done by marketing research professionals. Survey questions ask the respondent to rate a product sample or descriptions of product concepts on a range of attributes. Anywhere from five to twenty attributes are chosen. They could include things like: ease of use, weight, accuracy, durability, colourfulness, price, or size. The attributes chosen will vary depending on the product being studied. The same question is asked about all the products in the study. The data for multiple products is coded and input into a statistical program such as R, SPSS, SAS, Stata, STATISTICA, JMP, and SYSTAT.The basic steps are:The differences between principal components analysis and factor analysis are further illustrated by Suhr (2009):[25]For this reason, Brown (2009) recommends using factor analysis when theoretical ideas about relationships between variables exist, whereas PCA should be used if the goal of the researcher is to explore patterns in their data.Factor analysis takes into account the random error that is inherent in measurement, whereas PCA fails to do so. This point is exemplified by Brown (2009),[28] who indicated that, in respect to the correlation matrices involved in the calculations:Fabrigar et al. (1999)[24] address a number of reasons used to suggest that principal components analysis is not equivalent to factor analysis:While exploratory factor analysis and principal component analysis are treated as synonymous techniques in some fields of statistics, this has been criticised (e.g. Fabrigar et al., 1999;[24] Suhr, 2009[25]). In factor analysis, the researcher makes the assumption that an underlying causal model exists, whereas PCA is simply a variable reduction technique.[26] Researchers have argued that the distinctions between the two techniques may mean that there are objective benefits for preferring one over the other based on the analytic goal. If the factor model is incorrectly formulated or the assumptions are not met, then factor analysis will give erroneous results. Factor analysis has been used successfully where adequate understanding of the system permits good initial model formulations. Principal component analysis employs a mathematical transformation to the original data with no assumptions about the form of the covariance matrix. The aim of PCA is to determine a few linear combinations of the original variables that can be used to summarize the data set without losing much information.[27]Factor analysis in psychology is most often associated with intelligence research. However, it also has been used to find factors in a broad range of domains such as personality, attitudes, beliefs, etc. It is linked to psychometrics, as it can assess the validity of an instrument by finding if the instrument indeed measures the postulated factors.Factor analysis is used to identify \"factors\" that explain a variety of results on different tests. For example, intelligence research found that people who get a high score on a test of verbal ability are also good on other tests that require verbal abilities. Researchers explained this by using factor analysis to isolate one factor, often called crystallized intelligence or verbal intelligence, which represents the degree to which someone is able to solve problems involving verbal skills.Raymond Cattell expanded on Spearman's idea of a two-factor theory of intelligence after performing his own tests and factor analysis. He used a multi-factor theory to explain intelligence. Cattell's theory addressed alternate factors in intellectual development, including motivation and psychology. Cattell also developed several mathematical methods for adjusting psychometric graphs, such as his \"scree\" test and similarity coefficients. His research led to the development of his theory of fluid and crystallized intelligence, as well as his 16 Personality Factors theory of personality. Cattell was a strong advocate of factor analysis and psychometrics. He believed that all theory should be derived from research, which supports the continued use of empirical observation and objective testing to study human intelligence.Charles Spearman pioneered the use of factor analysis in the field of psychology and is sometimes credited with the invention of factor analysis. He discovered that school children's scores on a wide variety of seemingly unrelated subjects were positively correlated, which led him to postulate that a general mental ability, or g, underlies and shapes human cognitive performance. His postulate now enjoys broad support in the field of intelligence research, where it is known as the g theory.Promax rotation is an alternative non-orthogonal (oblique) rotation method which is computationally faster than the direct oblimin method and therefore is sometimes used for very large datasets.Direct oblimin rotation is the standard method when one wishes a non-orthogonal (oblique) solution \u2013 that is, one in which the factors are allowed to be correlated. This will result in higher eigenvalues but diminished interpretability of the factors. See below.[clarification needed]Equimax rotation is a compromise between varimax and quartimax criteria.Quartimax rotation is an orthogonal alternative which minimizes the number of factors needed to explain each variable. This type of rotation often generates a general factor on which most variables are loaded to a high or medium degree. Such a factor structure is usually not helpful to the research purpose.Varimax rotation is an orthogonal rotation of the factor axes to maximize the variance of the squared loadings of a factor (column) on all the variables (rows) in a factor matrix, which has the effect of differentiating the original variables by extracted factor. Each factor will tend to have either large or small loadings of any particular variable. A varimax solution yields results which make it as easy as possible to identify each variable with a single factor. This is the most common rotation option. However, the orthogonality (i.e., independence) of factors is often an unrealistic assumption. Oblique rotations are inclusive of orthogonal rotation, and for that reason, oblique rotations are a preferred method.[21]The unrotated output maximizes variance accounted for by the first and subsequent factors, and forces the factors to be orthogonal. This data-compression comes at the cost of having most items load on the early factors, and usually, of having many items load substantially on more than one factor. Rotation serves to make the output more understandable, by seeking so-called \"Simple Structure\": A pattern of loadings where each item loads strongly on only one of the factors, and much more weakly on the other factors. Rotations can be orthogonal or oblique (allowing the factors to correlate).Variance explained criteria: Some researchers simply use the rule of keeping enough factors to account for 90% (sometimes 80%) of the variation. Where the researcher's goal emphasizes parsimony (explaining variance with as few factors as possible), the criterion could be as low as 50%.Scree plot: The Cattell scree test plots the components as the X axis and the corresponding eigenvalues as the Y-axis. As one moves to the right, toward later components, the eigenvalues drop. When the drop ceases and the curve makes an elbow toward less steep decline, Cattell's scree test says to drop all further components after the one starting the elbow. This rule is sometimes criticised for being amenable to researcher-controlled \"fudging\". That is, as picking the \"elbow\" can be subjective because the curve has multiple elbows or is a smooth curve, the researcher may be tempted to set the cut-off at the number of factors desired by their research agenda.[citation needed]Kaiser criterion: The Kaiser rule is to drop all components with eigenvalues under 1.0 \u2013 this being the eigenvalue equal to the information accounted for by an average single item. The Kaiser criterion is the default in SPSS and most statistical software but is not recommended when used as the sole cut-off criterion for estimating the number of factors as it tends to over-extract factors.[19] A variation of this method has been created where a researcher calculates confidence intervals for each eigenvalue and retains only factors which have the entire confidence interval greater than 1.0.[15][20]\nVelicer\u2019s (1976) MAP test[14] \u201cinvolves a complete principal components analysis followed by the examination of a series of matrices of partial correlations\u201d (p.\u00a0397). The squared correlation for Step \u201c0\u201d (see Figure 4) is the average squared off-diagonal correlation for the unpartialed correlation matrix. On Step 1, the first principal component and its associated items are partialed out. Thereafter, the average squared off-diagonal correlation for the subsequent correlation matrix is then computed for Step 1. On Step 2, the first two principal components are partialed out and the resultant average squared off-diagonal correlation is again computed. The computations are carried out for k minus one step (k representing the total number of variables in the matrix). Thereafter, all of the average squared correlations for each step are lined up and the step number in the analyses that resulted in the lowest average squared partial correlation determines the number of components or factors to retain.[14] By this method, components are maintained as long as the variance in the correlation matrix represents systematic variance, as opposed to residual or error variance. Although methodologically akin to principal components analysis, the MAP technique has been shown to perform quite well in determining the number of factors to retain in multiple simulation studies.[15][16][17] This procedure is made available through SPSS's user interface. See Courtney (2013)[18] for guidance.Horn's parallel analysis (PA): A Monte-Carlo based simulation method that compares the observed eigenvalues with those obtained from uncorrelated normal variables. A factor or component is retained if the associated eigenvalue is bigger than the 95th percentile of the distribution of eigenvalues derived from the random data. PA is one of the most recommended rules for determining the number of components to retain,[citation needed] but many programs fail to include this option (a notable exception being R ).[12] However, Formann provided both theoretical and empirical evidence that its application might not be appropriate in many cases since its performance is considerably influenced by sample size, item discrimination, and type of correlation coefficient.[13]Researchers wish to avoid such subjective or arbitrary criteria for factor retention as \"it made sense to me\". A number of objective methods have been developed to solve this problem, allowing users to determine an appropriate range of solutions to investigate. Methods may not agree. For instance, the parallel analysis may suggest 5 factors while Velicer's MAP suggests 6, so the researcher may request both 5 and 6-factor solutions and discuss each in terms of their relation to external data and theory.Factor scores (also called component scores in PCA): are the scores of each case (row) on each factor (column). To compute the factor score for a given case for a given factor, one takes the case's standardized score on each variable, multiplies by the corresponding loadings of the variable for the given factor, and sums these products. Computing factor scores allows one to look for factor outliers. Also, factor scores may be used as variables in subsequent modeling. (Explained from PCA not from Factor Analysis perspective).Extraction sums of squared loadings: Initial eigenvalues and eigenvalues after extraction (listed by SPSS as \"Extraction Sums of Squared Loadings\") are the same for PCA extraction, but for other extraction methods, eigenvalues after extraction will be lower than their initial counterparts. SPSS also prints \"Rotation Sums of Squared Loadings\" and even for PCA, these eigenvalues will differ from initial and extraction eigenvalues, though their total will be the same.Eigenvalues/characteristic roots: The eigenvalue for a given factor measures the variance in all the variables which is accounted for by that factor. The ratio of eigenvalues is the ratio of explanatory importance of the factors with respect to the variables. If a factor has a low eigenvalue, then it is contributing little to the explanation of variances in the variables and may be ignored as redundant with more important factors. Eigenvalues measure the amount of variation in the total sample accounted for by each factor.Uniqueness of a variable: That is, uniqueness is the variability of a variable minus its communality.Spurious solutions: If the communality exceeds 1.0, there is a spurious solution, which may reflect too small a sample or the researcher has too many or too few factors.Communality: The sum of the squared factor loadings for all factors for a given variable (row) is the variance in that variable accounted for by all the factors, and this is called the communality. The communality measures the percent of variance in a given variable explained by all the factors jointly and may be interpreted as the reliability of the indicator.In oblique rotation, one gets both a pattern matrix and a structure matrix. The structure matrix is simply the factor loading matrix as in orthogonal rotation, representing the variance in a measured variable explained by a factor on both a unique and common contributions basis. The pattern matrix, in contrast, contains coefficients which just represent unique contributions. The more factors, the lower the pattern coefficients as a rule since there will be more common contributions to variance explained. For oblique rotation, the researcher looks at both the structure and pattern coefficients when attributing a label to a factor. Principles of oblique rotation can be derived from both cross entropy and its dual entropy.[11]Interpreting factor loadings: By one rule of thumb in confirmatory factor analysis, loadings should be .7 or higher to confirm that independent variables identified a priori are represented by a particular factor, on the rationale that the .7 level corresponds to about half of the variance in the indicator being explained by the factor. However, the .7 standard is a high one and real-life data may well not meet this criterion, which is why some researchers, particularly for exploratory purposes, will use a lower level such as .4 for the central factor and .25 for other factors. In any event, factor loadings must be interpreted in the light of theory, not by arbitrary cutoff levels.Factor loadings: Commonality is the square of standardized outer loading of an item. Analogous to Pearson's r, the squared factor loading is the percent of variance in that indicator variable explained by the factor. To get the percent of variance in all the variables accounted for by each factor, add the sum of the squared factor loadings for that factor (column) and divide by the number of variables. (Note the number of variables equals the sum of their variances as the variance of a standardized variable is 1.) This is the same as dividing the factor's eigenvalue by the number of variables.Factor regression model is a combinatorial model of factor model and regression model; or alternatively, it can be viewed as the hybrid factor model,[10] whose factors are partially known.Alpha factoring is based on maximizing the reliability of factors, assuming variables are randomly sampled from a universe of variables. All other methods assume cases to be sampled and variables fixed.Image factoring is based on the correlation matrix of predicted variables rather than actual variables, where each variable is predicted from the others using multiple regression.Common factor analysis, also called principal factor analysis (PFA) or principal axis factoring (PAF), seeks the least number of factors which can account for the common variance (correlation) of a set of variables.Canonical factor analysis, also called Rao's canonical factoring, is a different method of computing the same model as PCA, which uses the principal axis method. Canonical factor analysis seeks factors which have the highest canonical correlation with the observed variables. Canonical factor analysis is unaffected by arbitrary rescaling of the data.Principal component analysis (PCA) is a widely used method for factor extraction, which is the first phase of EFA.[9] Factor weights are computed to extract the maximum possible variance, with successive factoring continuing until there is no further meaningful variance left.[9] The factor model must then be rotated for analysis.[9]Confirmatory factor analysis (CFA) is a more complex approach that tests the hypothesis that the items are associated with specific factors.[9] CFA uses structural equation modeling to test a measurement model whereby loading on the factors allows for evaluation of relationships between observed variables and unobserved variables.[9] Structural equation modeling approaches can accommodate measurement error, and are less restrictive than least-squares estimation.[9] Hypothesized models are tested against actual data, and the analysis would demonstrate loadings of observed variables on the latent variables (factors), as well as the correlation between the latent variables.[9]Exploratory factor analysis (EFA) is used to identify complex interrelationships among items and group items that are part of unified concepts.[9] The researcher makes no a priori assumptions about relationships among factors.[9]Large values of the communalities will indicate that the fitting hyperplane is rather accurately reproducing the correlation matrix. It should be noted that the mean values of the factors must also be constrained to be zero, from which it follows that the mean values of the errors will also be zero.is to be minimized, and this is accomplished by minimizing it with respect to a set of orthonormal factor vectors. It can be seen thatThe goal of factor analysis is to choose the fitting hyperplane such that the reduced correlation matrix reproduces the correlation matrix as nearly as possible, except for the diagonal elements of the correlation matrix which are known to have unit value. In other words, the goal is to reproduce as accurately as possible the cross-correlations in the data. Specifically, for the fitting hyperplane, the mean square error in the off-diagonal componentsIf the solution factors are allowed to be correlated (as in oblimin rotation, for example), then the corresponding mathematical model uses skew coordinates rather than orthogonal coordinates.This is equivalent to minimizing the off-diagonal components of the error covariance which, in the model equations have expected values of zero. This is to be contrasted with principal component analysis which seeks to minimize the mean square error of all residuals.[8] Before the advent of high speed computers, considerable effort was devoted to finding approximate solutions to the problem, particularly in estimating the communalities by other means, which then simplifies the problem considerably by yielding a known reduced correlation matrix. This was then used to estimate the factors and the loadings. With the advent of high-speed computers, the minimization problem can be solved iteratively with adequate speed, and the communalities are calculated in the process, rather than being needed beforehand. The MinRes algorithm is particularly suited to this problem, but is hardly the only iterative means of finding a solution.The values of the loadings L, the averages \u03bc, and the variances of the \"errors\" \u03b5 must be estimated given the observed data X and F (the assumption about the levels of the factors is fixed for a given F). The \"fundamental theorem\" may be derived from the above conditions:Note that, since any rotation of a solution is also a solution, this makes interpreting the factors difficult. See disadvantages below. In this particular example, if we do not know beforehand that the two types of intelligence are uncorrelated, then we cannot interpret the two factors as the two different types of intelligence. Even if they are uncorrelated, we cannot tell which factor corresponds to verbal intelligence and which corresponds to mathematical intelligence without an outside argument.Observe that by doubling the scale on which \"verbal intelligence\"\u2014the first component in each column of F\u2014is measured, and simultaneously halving the factor loadings for verbal intelligence makes no difference to the model. Thus, no generality is lost by assuming that the standard deviation of the factors for verbal intelligence is 1. Likewise for mathematical intelligence. Moreover, for similar reasons, no generality is lost by assuming the two factors are uncorrelated with each other. In other words:In matrix notation, we havewhereor, more succinctly:The factor analysis model for this particular sample is then:and the sample variance is given by:where the sample mean is:The observable data that go into factor analysis would be 10 scores of each of the 1000 students, a total of 10,000 numbers. The factor loadings and levels of the two kinds of intelligence of each student must be inferred from the data.Two students having identical degrees of verbal intelligence and identical degrees of mathematical intelligence may have different aptitudes in astronomy because individual aptitudes differ from average aptitudes. That difference is called the \"error\" \u2014 a statistical term that means the amount by which an individual differs from what is average for his or her levels of intelligence (see errors and residuals in statistics).The numbers 10 and 6 are the factor loadings associated with astronomy. Other academic subjects may have different factor loadings.Suppose a psychologist has the hypothesis that there are two kinds of intelligence, \"verbal intelligence\" and \"mathematical intelligence\", neither of which is directly observed. Evidence for the hypothesis is sought in the examination scores from each of 10 different academic fields of 1000 students. If each student is chosen randomly from a large population, then each student's 10 scores are random variables. The psychologist's hypothesis may say that for each of the 10 academic fields, the score averaged over the group of all students who share some common pair of values for verbal and mathematical \"intelligences\" is some constant times their level of verbal intelligence plus another constant times their level of mathematical intelligence, i.e., it is a combination of those two \"factors\". The numbers for a particular subject, by which the two kinds of intelligence are multiplied to obtain the expected score, are posited by the hypothesis to be the same for all intelligence level pairs, and are called \"factor loading\" for this subject. For example, the hypothesis may hold that the average student's aptitude in the field of astronomy isororIn matrix terms, we haveFactor analysis is related to principal component analysis (PCA), but the two are not identical.[1] There has been significant controversy in the field over differences between the two techniques (see section on exploratory factor analysis versus principal components analysis below). PCA is a more basic version of exploratory factor analysis (EFA) that was developed in the early days prior to the advent of high-speed computers. From the point of view of exploratory analysis, the eigenvalues of PCA are inflated component loadings, i.e., contaminated with error variance.[2][3][4][5][6][7]Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus \"error\" terms. Factor analysis aims to find independent latent variables. The theory behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in biology, psychometrics personality theories, marketing, product management, operations research, and finance. Proponents of factor analysis believe that it helps to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality.",
            "title": "Factor analysis",
            "url": "https://en.wikipedia.org/wiki/Scree%27s_test"
        },
        {
            "desc_links": [
                "/wiki/Principal_component_analysis",
                "/wiki/Exploratory_factor_analysis",
                "/wiki/Statistics",
                "/wiki/Variance",
                "/wiki/Variable_(mathematics)",
                "/wiki/Latent_variable",
                "/wiki/Linear_combination",
                "/wiki/Errors_and_residuals_in_statistics",
                "/wiki/Psychometrics",
                "/wiki/Personality",
                "/wiki/Marketing",
                "/wiki/Product_management",
                "/wiki/Operations_research",
                "/wiki/Finance"
            ],
            "links": [
                "/wiki/Oligonucleotide",
                "/wiki/DNA_microarrays",
                "/wiki/Affymetrix",
                "/wiki/RNA",
                "/wiki/Geochemistry",
                "/wiki/Geochemistry",
                "/wiki/Hydrochemistry",
                "/wiki/Astrophysics",
                "/wiki/Cosmology",
                "/wiki/Ecology",
                "/wiki/Molecular_biology",
                "/wiki/Biochemistry",
                "/wiki/R_(programming_language)",
                "/wiki/SPSS",
                "/wiki/SAS_System",
                "/wiki/Stata",
                "/wiki/STATISTICA",
                "/wiki/Random_error",
                "/wiki/Exploratory_factor_analysis",
                "/wiki/Principal_component_analysis",
                "/wiki/Psychometrics",
                "/wiki/Crystallized_intelligence",
                "/wiki/Raymond_Cattell",
                "/wiki/Fluid_and_crystallized_intelligence",
                "/wiki/16_Personality_Factors",
                "/wiki/Psychometrics",
                "/wiki/Charles_Spearman",
                "/wiki/Intelligence_(trait)",
                "/wiki/General_intelligence_factor",
                "/wiki/Dataset",
                "/wiki/Interpretability",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Varimax_rotation",
                "/wiki/Orthogonal",
                "/wiki/Occam%27s_razor",
                "/wiki/Eigenvalue,_eigenvector_and_eigenspace",
                "/wiki/Y-axis",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/SPSS",
                "/wiki/Statistical_software",
                "/wiki/Confidence_interval",
                "/wiki/John_L._Horn",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/R_(programming_language)",
                "/wiki/Anton_Formann",
                "/wiki/Sample_size",
                "/wiki/Item_response_theory#The_item_response_function",
                "/wiki/Correlation_coefficient",
                "/wiki/Angle#Types_of_angles",
                "/wiki/Coefficient",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Eigenvalue",
                "/wiki/Correlation_matrix",
                "/wiki/Multiple_regression",
                "/wiki/Principal_component_analysis",
                "/wiki/Confirmatory_factor_analysis",
                "/wiki/Structural_equation_modeling",
                "/wiki/Least-squares_estimation",
                "/wiki/Exploratory_factor_analysis",
                "/wiki/Skew_coordinates",
                "/wiki/Generalized_minimal_residual_method",
                "/wiki/Variance",
                "/wiki/Uncorrelated",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Errors_and_residuals_in_statistics",
                "/wiki/Intelligence_(trait)",
                "/wiki/Evidence",
                "/wiki/Population_(statistics)",
                "/wiki/Constant_(mathematics)",
                "/wiki/Astronomy",
                "/wiki/Principal_component_analysis",
                "/wiki/Exploratory_factor_analysis",
                "/wiki/Statistics",
                "/wiki/Variance",
                "/wiki/Variable_(mathematics)",
                "/wiki/Latent_variable",
                "/wiki/Linear_combination",
                "/wiki/Errors_and_residuals_in_statistics",
                "/wiki/Psychometrics",
                "/wiki/Personality",
                "/wiki/Marketing",
                "/wiki/Product_management",
                "/wiki/Operations_research",
                "/wiki/Finance"
            ],
            "text": "Factor analysis has been implemented in several statistical analysis programs since the 1980s:Factor analysis can be used for summarizing high-density oligonucleotide DNA microarrays data at probe level for Affymetrix GeneChips. In this case, the latent variable corresponds to the RNA concentration in a sample.[33]In geochemistry, different factors can correspond to different mineral associations, and thus to mineralisation.[32]In groundwater quality management, it is important to relate the spatial distribution of different chemical parameters to different possible sources, which have different chemical signatures. For example, a sulfide mine is likely to be associated with high levels of acidity, dissolved sulfates and transition metals. These signatures can be identified as factors through R-mode factor analysis, and the location of possible sources can be suggested by contouring the factor scores.[31]Factor analysis has also been widely used in physical sciences such as geochemistry, hydrochemistry,[30] astrophysics and cosmology, as well as biological sciences, such as ecology, molecular biology and biochemistry.The analysis will isolate the underlying factors that explain the data using a matrix of associations.[29] Factor analysis is an interdependence technique. The complete set of interdependent relationships is examined. There is no specification of dependent variables, independent variables, or causality. Factor analysis assumes that all the rating data on different attributes can be reduced down to a few important dimensions. This reduction is possible because some attributes may be related to each other. The rating given to any one attribute is partially the result of the influence of other attributes. The statistical algorithm deconstructs the rating (called a raw score) into its various components, and reconstructs the partial scores into underlying factor scores. The degree of correlation between the initial raw score and the final factor score is called a factor loading.The data collection stage is usually done by marketing research professionals. Survey questions ask the respondent to rate a product sample or descriptions of product concepts on a range of attributes. Anywhere from five to twenty attributes are chosen. They could include things like: ease of use, weight, accuracy, durability, colourfulness, price, or size. The attributes chosen will vary depending on the product being studied. The same question is asked about all the products in the study. The data for multiple products is coded and input into a statistical program such as R, SPSS, SAS, Stata, STATISTICA, JMP, and SYSTAT.The basic steps are:The differences between principal components analysis and factor analysis are further illustrated by Suhr (2009):[25]For this reason, Brown (2009) recommends using factor analysis when theoretical ideas about relationships between variables exist, whereas PCA should be used if the goal of the researcher is to explore patterns in their data.Factor analysis takes into account the random error that is inherent in measurement, whereas PCA fails to do so. This point is exemplified by Brown (2009),[28] who indicated that, in respect to the correlation matrices involved in the calculations:Fabrigar et al. (1999)[24] address a number of reasons used to suggest that principal components analysis is not equivalent to factor analysis:While exploratory factor analysis and principal component analysis are treated as synonymous techniques in some fields of statistics, this has been criticised (e.g. Fabrigar et al., 1999;[24] Suhr, 2009[25]). In factor analysis, the researcher makes the assumption that an underlying causal model exists, whereas PCA is simply a variable reduction technique.[26] Researchers have argued that the distinctions between the two techniques may mean that there are objective benefits for preferring one over the other based on the analytic goal. If the factor model is incorrectly formulated or the assumptions are not met, then factor analysis will give erroneous results. Factor analysis has been used successfully where adequate understanding of the system permits good initial model formulations. Principal component analysis employs a mathematical transformation to the original data with no assumptions about the form of the covariance matrix. The aim of PCA is to determine a few linear combinations of the original variables that can be used to summarize the data set without losing much information.[27]Factor analysis in psychology is most often associated with intelligence research. However, it also has been used to find factors in a broad range of domains such as personality, attitudes, beliefs, etc. It is linked to psychometrics, as it can assess the validity of an instrument by finding if the instrument indeed measures the postulated factors.Factor analysis is used to identify \"factors\" that explain a variety of results on different tests. For example, intelligence research found that people who get a high score on a test of verbal ability are also good on other tests that require verbal abilities. Researchers explained this by using factor analysis to isolate one factor, often called crystallized intelligence or verbal intelligence, which represents the degree to which someone is able to solve problems involving verbal skills.Raymond Cattell expanded on Spearman's idea of a two-factor theory of intelligence after performing his own tests and factor analysis. He used a multi-factor theory to explain intelligence. Cattell's theory addressed alternate factors in intellectual development, including motivation and psychology. Cattell also developed several mathematical methods for adjusting psychometric graphs, such as his \"scree\" test and similarity coefficients. His research led to the development of his theory of fluid and crystallized intelligence, as well as his 16 Personality Factors theory of personality. Cattell was a strong advocate of factor analysis and psychometrics. He believed that all theory should be derived from research, which supports the continued use of empirical observation and objective testing to study human intelligence.Charles Spearman pioneered the use of factor analysis in the field of psychology and is sometimes credited with the invention of factor analysis. He discovered that school children's scores on a wide variety of seemingly unrelated subjects were positively correlated, which led him to postulate that a general mental ability, or g, underlies and shapes human cognitive performance. His postulate now enjoys broad support in the field of intelligence research, where it is known as the g theory.Promax rotation is an alternative non-orthogonal (oblique) rotation method which is computationally faster than the direct oblimin method and therefore is sometimes used for very large datasets.Direct oblimin rotation is the standard method when one wishes a non-orthogonal (oblique) solution \u2013 that is, one in which the factors are allowed to be correlated. This will result in higher eigenvalues but diminished interpretability of the factors. See below.[clarification needed]Equimax rotation is a compromise between varimax and quartimax criteria.Quartimax rotation is an orthogonal alternative which minimizes the number of factors needed to explain each variable. This type of rotation often generates a general factor on which most variables are loaded to a high or medium degree. Such a factor structure is usually not helpful to the research purpose.Varimax rotation is an orthogonal rotation of the factor axes to maximize the variance of the squared loadings of a factor (column) on all the variables (rows) in a factor matrix, which has the effect of differentiating the original variables by extracted factor. Each factor will tend to have either large or small loadings of any particular variable. A varimax solution yields results which make it as easy as possible to identify each variable with a single factor. This is the most common rotation option. However, the orthogonality (i.e., independence) of factors is often an unrealistic assumption. Oblique rotations are inclusive of orthogonal rotation, and for that reason, oblique rotations are a preferred method.[21]The unrotated output maximizes variance accounted for by the first and subsequent factors, and forces the factors to be orthogonal. This data-compression comes at the cost of having most items load on the early factors, and usually, of having many items load substantially on more than one factor. Rotation serves to make the output more understandable, by seeking so-called \"Simple Structure\": A pattern of loadings where each item loads strongly on only one of the factors, and much more weakly on the other factors. Rotations can be orthogonal or oblique (allowing the factors to correlate).Variance explained criteria: Some researchers simply use the rule of keeping enough factors to account for 90% (sometimes 80%) of the variation. Where the researcher's goal emphasizes parsimony (explaining variance with as few factors as possible), the criterion could be as low as 50%.Scree plot: The Cattell scree test plots the components as the X axis and the corresponding eigenvalues as the Y-axis. As one moves to the right, toward later components, the eigenvalues drop. When the drop ceases and the curve makes an elbow toward less steep decline, Cattell's scree test says to drop all further components after the one starting the elbow. This rule is sometimes criticised for being amenable to researcher-controlled \"fudging\". That is, as picking the \"elbow\" can be subjective because the curve has multiple elbows or is a smooth curve, the researcher may be tempted to set the cut-off at the number of factors desired by their research agenda.[citation needed]Kaiser criterion: The Kaiser rule is to drop all components with eigenvalues under 1.0 \u2013 this being the eigenvalue equal to the information accounted for by an average single item. The Kaiser criterion is the default in SPSS and most statistical software but is not recommended when used as the sole cut-off criterion for estimating the number of factors as it tends to over-extract factors.[19] A variation of this method has been created where a researcher calculates confidence intervals for each eigenvalue and retains only factors which have the entire confidence interval greater than 1.0.[15][20]\nVelicer\u2019s (1976) MAP test[14] \u201cinvolves a complete principal components analysis followed by the examination of a series of matrices of partial correlations\u201d (p.\u00a0397). The squared correlation for Step \u201c0\u201d (see Figure 4) is the average squared off-diagonal correlation for the unpartialed correlation matrix. On Step 1, the first principal component and its associated items are partialed out. Thereafter, the average squared off-diagonal correlation for the subsequent correlation matrix is then computed for Step 1. On Step 2, the first two principal components are partialed out and the resultant average squared off-diagonal correlation is again computed. The computations are carried out for k minus one step (k representing the total number of variables in the matrix). Thereafter, all of the average squared correlations for each step are lined up and the step number in the analyses that resulted in the lowest average squared partial correlation determines the number of components or factors to retain.[14] By this method, components are maintained as long as the variance in the correlation matrix represents systematic variance, as opposed to residual or error variance. Although methodologically akin to principal components analysis, the MAP technique has been shown to perform quite well in determining the number of factors to retain in multiple simulation studies.[15][16][17] This procedure is made available through SPSS's user interface. See Courtney (2013)[18] for guidance.Horn's parallel analysis (PA): A Monte-Carlo based simulation method that compares the observed eigenvalues with those obtained from uncorrelated normal variables. A factor or component is retained if the associated eigenvalue is bigger than the 95th percentile of the distribution of eigenvalues derived from the random data. PA is one of the most recommended rules for determining the number of components to retain,[citation needed] but many programs fail to include this option (a notable exception being R ).[12] However, Formann provided both theoretical and empirical evidence that its application might not be appropriate in many cases since its performance is considerably influenced by sample size, item discrimination, and type of correlation coefficient.[13]Researchers wish to avoid such subjective or arbitrary criteria for factor retention as \"it made sense to me\". A number of objective methods have been developed to solve this problem, allowing users to determine an appropriate range of solutions to investigate. Methods may not agree. For instance, the parallel analysis may suggest 5 factors while Velicer's MAP suggests 6, so the researcher may request both 5 and 6-factor solutions and discuss each in terms of their relation to external data and theory.Factor scores (also called component scores in PCA): are the scores of each case (row) on each factor (column). To compute the factor score for a given case for a given factor, one takes the case's standardized score on each variable, multiplies by the corresponding loadings of the variable for the given factor, and sums these products. Computing factor scores allows one to look for factor outliers. Also, factor scores may be used as variables in subsequent modeling. (Explained from PCA not from Factor Analysis perspective).Extraction sums of squared loadings: Initial eigenvalues and eigenvalues after extraction (listed by SPSS as \"Extraction Sums of Squared Loadings\") are the same for PCA extraction, but for other extraction methods, eigenvalues after extraction will be lower than their initial counterparts. SPSS also prints \"Rotation Sums of Squared Loadings\" and even for PCA, these eigenvalues will differ from initial and extraction eigenvalues, though their total will be the same.Eigenvalues/characteristic roots: The eigenvalue for a given factor measures the variance in all the variables which is accounted for by that factor. The ratio of eigenvalues is the ratio of explanatory importance of the factors with respect to the variables. If a factor has a low eigenvalue, then it is contributing little to the explanation of variances in the variables and may be ignored as redundant with more important factors. Eigenvalues measure the amount of variation in the total sample accounted for by each factor.Uniqueness of a variable: That is, uniqueness is the variability of a variable minus its communality.Spurious solutions: If the communality exceeds 1.0, there is a spurious solution, which may reflect too small a sample or the researcher has too many or too few factors.Communality: The sum of the squared factor loadings for all factors for a given variable (row) is the variance in that variable accounted for by all the factors, and this is called the communality. The communality measures the percent of variance in a given variable explained by all the factors jointly and may be interpreted as the reliability of the indicator.In oblique rotation, one gets both a pattern matrix and a structure matrix. The structure matrix is simply the factor loading matrix as in orthogonal rotation, representing the variance in a measured variable explained by a factor on both a unique and common contributions basis. The pattern matrix, in contrast, contains coefficients which just represent unique contributions. The more factors, the lower the pattern coefficients as a rule since there will be more common contributions to variance explained. For oblique rotation, the researcher looks at both the structure and pattern coefficients when attributing a label to a factor. Principles of oblique rotation can be derived from both cross entropy and its dual entropy.[11]Interpreting factor loadings: By one rule of thumb in confirmatory factor analysis, loadings should be .7 or higher to confirm that independent variables identified a priori are represented by a particular factor, on the rationale that the .7 level corresponds to about half of the variance in the indicator being explained by the factor. However, the .7 standard is a high one and real-life data may well not meet this criterion, which is why some researchers, particularly for exploratory purposes, will use a lower level such as .4 for the central factor and .25 for other factors. In any event, factor loadings must be interpreted in the light of theory, not by arbitrary cutoff levels.Factor loadings: Commonality is the square of standardized outer loading of an item. Analogous to Pearson's r, the squared factor loading is the percent of variance in that indicator variable explained by the factor. To get the percent of variance in all the variables accounted for by each factor, add the sum of the squared factor loadings for that factor (column) and divide by the number of variables. (Note the number of variables equals the sum of their variances as the variance of a standardized variable is 1.) This is the same as dividing the factor's eigenvalue by the number of variables.Factor regression model is a combinatorial model of factor model and regression model; or alternatively, it can be viewed as the hybrid factor model,[10] whose factors are partially known.Alpha factoring is based on maximizing the reliability of factors, assuming variables are randomly sampled from a universe of variables. All other methods assume cases to be sampled and variables fixed.Image factoring is based on the correlation matrix of predicted variables rather than actual variables, where each variable is predicted from the others using multiple regression.Common factor analysis, also called principal factor analysis (PFA) or principal axis factoring (PAF), seeks the least number of factors which can account for the common variance (correlation) of a set of variables.Canonical factor analysis, also called Rao's canonical factoring, is a different method of computing the same model as PCA, which uses the principal axis method. Canonical factor analysis seeks factors which have the highest canonical correlation with the observed variables. Canonical factor analysis is unaffected by arbitrary rescaling of the data.Principal component analysis (PCA) is a widely used method for factor extraction, which is the first phase of EFA.[9] Factor weights are computed to extract the maximum possible variance, with successive factoring continuing until there is no further meaningful variance left.[9] The factor model must then be rotated for analysis.[9]Confirmatory factor analysis (CFA) is a more complex approach that tests the hypothesis that the items are associated with specific factors.[9] CFA uses structural equation modeling to test a measurement model whereby loading on the factors allows for evaluation of relationships between observed variables and unobserved variables.[9] Structural equation modeling approaches can accommodate measurement error, and are less restrictive than least-squares estimation.[9] Hypothesized models are tested against actual data, and the analysis would demonstrate loadings of observed variables on the latent variables (factors), as well as the correlation between the latent variables.[9]Exploratory factor analysis (EFA) is used to identify complex interrelationships among items and group items that are part of unified concepts.[9] The researcher makes no a priori assumptions about relationships among factors.[9]Large values of the communalities will indicate that the fitting hyperplane is rather accurately reproducing the correlation matrix. It should be noted that the mean values of the factors must also be constrained to be zero, from which it follows that the mean values of the errors will also be zero.is to be minimized, and this is accomplished by minimizing it with respect to a set of orthonormal factor vectors. It can be seen thatThe goal of factor analysis is to choose the fitting hyperplane such that the reduced correlation matrix reproduces the correlation matrix as nearly as possible, except for the diagonal elements of the correlation matrix which are known to have unit value. In other words, the goal is to reproduce as accurately as possible the cross-correlations in the data. Specifically, for the fitting hyperplane, the mean square error in the off-diagonal componentsIf the solution factors are allowed to be correlated (as in oblimin rotation, for example), then the corresponding mathematical model uses skew coordinates rather than orthogonal coordinates.This is equivalent to minimizing the off-diagonal components of the error covariance which, in the model equations have expected values of zero. This is to be contrasted with principal component analysis which seeks to minimize the mean square error of all residuals.[8] Before the advent of high speed computers, considerable effort was devoted to finding approximate solutions to the problem, particularly in estimating the communalities by other means, which then simplifies the problem considerably by yielding a known reduced correlation matrix. This was then used to estimate the factors and the loadings. With the advent of high-speed computers, the minimization problem can be solved iteratively with adequate speed, and the communalities are calculated in the process, rather than being needed beforehand. The MinRes algorithm is particularly suited to this problem, but is hardly the only iterative means of finding a solution.The values of the loadings L, the averages \u03bc, and the variances of the \"errors\" \u03b5 must be estimated given the observed data X and F (the assumption about the levels of the factors is fixed for a given F). The \"fundamental theorem\" may be derived from the above conditions:Note that, since any rotation of a solution is also a solution, this makes interpreting the factors difficult. See disadvantages below. In this particular example, if we do not know beforehand that the two types of intelligence are uncorrelated, then we cannot interpret the two factors as the two different types of intelligence. Even if they are uncorrelated, we cannot tell which factor corresponds to verbal intelligence and which corresponds to mathematical intelligence without an outside argument.Observe that by doubling the scale on which \"verbal intelligence\"\u2014the first component in each column of F\u2014is measured, and simultaneously halving the factor loadings for verbal intelligence makes no difference to the model. Thus, no generality is lost by assuming that the standard deviation of the factors for verbal intelligence is 1. Likewise for mathematical intelligence. Moreover, for similar reasons, no generality is lost by assuming the two factors are uncorrelated with each other. In other words:In matrix notation, we havewhereor, more succinctly:The factor analysis model for this particular sample is then:and the sample variance is given by:where the sample mean is:The observable data that go into factor analysis would be 10 scores of each of the 1000 students, a total of 10,000 numbers. The factor loadings and levels of the two kinds of intelligence of each student must be inferred from the data.Two students having identical degrees of verbal intelligence and identical degrees of mathematical intelligence may have different aptitudes in astronomy because individual aptitudes differ from average aptitudes. That difference is called the \"error\" \u2014 a statistical term that means the amount by which an individual differs from what is average for his or her levels of intelligence (see errors and residuals in statistics).The numbers 10 and 6 are the factor loadings associated with astronomy. Other academic subjects may have different factor loadings.Suppose a psychologist has the hypothesis that there are two kinds of intelligence, \"verbal intelligence\" and \"mathematical intelligence\", neither of which is directly observed. Evidence for the hypothesis is sought in the examination scores from each of 10 different academic fields of 1000 students. If each student is chosen randomly from a large population, then each student's 10 scores are random variables. The psychologist's hypothesis may say that for each of the 10 academic fields, the score averaged over the group of all students who share some common pair of values for verbal and mathematical \"intelligences\" is some constant times their level of verbal intelligence plus another constant times their level of mathematical intelligence, i.e., it is a combination of those two \"factors\". The numbers for a particular subject, by which the two kinds of intelligence are multiplied to obtain the expected score, are posited by the hypothesis to be the same for all intelligence level pairs, and are called \"factor loading\" for this subject. For example, the hypothesis may hold that the average student's aptitude in the field of astronomy isororIn matrix terms, we haveFactor analysis is related to principal component analysis (PCA), but the two are not identical.[1] There has been significant controversy in the field over differences between the two techniques (see section on exploratory factor analysis versus principal components analysis below). PCA is a more basic version of exploratory factor analysis (EFA) that was developed in the early days prior to the advent of high-speed computers. From the point of view of exploratory analysis, the eigenvalues of PCA are inflated component loadings, i.e., contaminated with error variance.[2][3][4][5][6][7]Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus \"error\" terms. Factor analysis aims to find independent latent variables. The theory behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in biology, psychometrics personality theories, marketing, product management, operations research, and finance. Proponents of factor analysis believe that it helps to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality.",
            "title": "Factor analysis",
            "url": "https://en.wikipedia.org/wiki/Factor_analysis"
        },
        {
            "desc_links": [
                "/wiki/Human_intelligence",
                "/wiki/Measurement_instruments",
                "/wiki/Confirmatory_factor_analysis",
                "/wiki/Path_analysis_(statistics)",
                "/wiki/Partial_least_squares_path_modeling",
                "/wiki/Latent_growth_modeling",
                "/wiki/Structural_model_(econometrics)",
                "/wiki/Economic_model",
                "/wiki/Latent_variables",
                "/wiki/Observable_variable",
                "/wiki/Regression_analysis"
            ],
            "links": [
                "/wiki/R_(programming_language)",
                "/wiki/OpenMx",
                "/wiki/R_(programming_language)",
                "/wiki/LISREL",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Sample_size",
                "/wiki/Statistical_power",
                "/wiki/Sample_size",
                "/wiki/Null_hypothesis",
                "/wiki/Akaike_information_criterion",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Statistical_hypothesis_testing",
                "/wiki/Path_analysis_(statistics)#Path_tracing_rules",
                "/wiki/Path_analysis_(statistics)",
                "/wiki/Covariance_matrix",
                "/wiki/Expectation%E2%80%93maximization_algorithm",
                "/wiki/Maximum_likelihood",
                "/wiki/Quasi-maximum_likelihood",
                "/wiki/Weighted_least_squares",
                "/wiki/Data_point",
                "/wiki/Factor_analysis",
                "/wiki/Path_analysis_(statistics)",
                "/wiki/Henri_Theil",
                "/wiki/Robert_Basmann",
                "/wiki/Denis_Sargan",
                "/wiki/Path_analysis_(statistics)",
                "/wiki/Sewall_Wright",
                "/wiki/Human_intelligence",
                "/wiki/Measurement_instruments",
                "/wiki/Confirmatory_factor_analysis",
                "/wiki/Path_analysis_(statistics)",
                "/wiki/Partial_least_squares_path_modeling",
                "/wiki/Latent_growth_modeling",
                "/wiki/Structural_model_(econometrics)",
                "/wiki/Economic_model",
                "/wiki/Latent_variables",
                "/wiki/Observable_variable",
                "/wiki/Regression_analysis"
            ],
            "text": "Scholars consider it good practice to report which software package and version was used for SEM analysis because they have different capabilities and may use slightly different methods to perform similarly-named techniques.[29]There are also several packages for the R open source statistical environment. The OpenMx R package provides an open source and enhanced version of the Mx application.Several software packages exist for fitting structural equation models. LISREL was the first such software, initially released in the 1970s.As in any science, subsequent replication and perhaps modification will proceed from the initial finding.Caution should always be taken when making claims of causality even when experimentation or time-ordered studies have been done. The term causal model must be understood to mean \"a model that conveys causal assumptions\", not necessarily a model that produces validated causal conclusions. Collecting data at multiple time points and using an experimental or quasi-experimental design can help rule out certain rival hypotheses but even a randomized experiment cannot rule out all such threats to causal inference. Good fit by a model consistent with one causal hypothesis invariably entails equally good fit by another model consistent with an opposing causal hypothesis. No research design, no matter how clever, can help distinguish such rival hypotheses, save for interventional experiments.[13]The set of models are then interpreted so that claims about the constructs can be made, based on the best fitting model.Sample size requirements to achieve a particular significance and power in SEM hypothesis testing are similar for the same model when any of the three algorithms (PLS-PA, LISREL or systems of regression equations) are used for testing.[citation needed]While researchers agree that large sample sizes are required to provide sufficient statistical power and precise estimates using SEM, there is no general consensus on the appropriate method for determining adequate sample size.[24][25] Generally, the considerations for determining sample size include the number of observations per parameter, the number of observations required for fit indexes to perform adequately, and the number of observations per degree of freedom.[24] Researchers have proposed guidelines based on simulation studies (Chou & Bentler, 1995),[26] professional experience (Bentler and Chou, 1987),[27] and mathematical formulas (MacCallum, Browne, and Sugawara, 1996; Westland, 2010).[25][28]Models should not be led by MI, as Maccallum (1986) demonstrated: \"even under favorable conditions, models arising from specification searches must be viewed with caution.\"[23]The model may need to be modified in order to improve the fit, thereby estimating the most likely relationships between variables. Many programs provide modification indices which may guide minor modifications. Modification indices report the change in \u03c7\u00b2 that result from freeing fixed parameters: usually, therefore adding a path to a model which is currently set to zero. Modifications that improve model fit may be flagged as potential changes that can be made to the model. Modifications to a model, especially the structural model, are changes to the theory claimed to be true. Modifications therefore must make sense in terms of the theory being tested, or be acknowledged as limitations of that theory. Changes to measurement model are effectively claims that the items/data are impure indicators of the latent variables specified by theory.[22]For each measure of fit, a decision as to what represents a good-enough fit between the model and the data must reflect other contextual factors such as sample size, the ratio of indicators to factors, and the overall complexity of the model. For example, very large samples make the Chi-squared test overly sensitive and more likely to indicate a lack of model-data fit. [21])Some of the more commonly used measures of fit include:There are differing approaches to assessing fit. Traditional approaches to modeling start from a null hypothesis, rewarding more parsimonious models (i.e. those with fewer free parameters), to others such as AIC that focus on how little the fitted values deviate from a saturated model[citation needed] (i.e. how well they reproduce the measured values), taking into account the number of free parameters used. Because different measures of fit capture different elements of the fit of the model, it is appropriate to report a selection of different fit measures. Guidelines (i.e., \"cutoff scores\") for interpreting fit measures, including the ones listed below, are the subject of much debate among SEM researchers.[15]Of course as in all statistical hypothesis tests, SEM model tests are based on the assumption that the correct and complete relevant data have been modeled. In the SEM literature, discussion of fit has led to a variety of different recommendations on the precise application of the various fit indices and hypothesis tests.Formal statistical tests and fit indices have been developed for these purposes. Individual parameters of the model can also be examined within the estimated model in order to see how well the proposed model fits the driving theory. Most, though not all, estimation methods make such tests of the model possible.It is important to examine the \"fit\" of an estimated model to determine how well it models the data. This is a basic task in SEM modeling: forming the basis for accepting or rejecting models and, more usually, accepting one competing model over another. The output of SEM programs includes matrices of the estimated relationships between variables in the model. Assessment of fit essentially calculates how similar the predicted data are to matrices containing the relationships in the actual data.Having estimated a model, analysts will want to interpret the model. Estimated paths may be tabulated and/or presented graphically as a path model. The impact of variables is assessed using path tracing rules (see path analysis).Parameter estimation is done by comparing the actual covariance matrices representing the relationships between variables and the estimated covariance matrices of the best fitting model. This is obtained through numerical maximization via expectation\u2013maximization of a fit criterion as provided by maximum likelihood estimation, quasi-maximum likelihood estimation, weighted least squares or asymptotically distribution-free methods. This is often accomplished by using a specialized SEM analysis program, of which several exist.A modeler will often specify a set of theoretically plausible models in order to assess whether the model proposed is the best of the set of possible models. Not only must the modeler account for the theoretical reasons for building the model as it is, but the modeler must also take into account the number of data points and the number of parameters that the model must estimate to identify the model. An identified model is a model where a specific parameter value uniquely identifies the model, and no other equivalent formulation can be given by a different parameter value. A data point is a variable with observed scores, like a variable containing the scores on a question or the number of times respondents buy a car. The parameter is the value of interest, which might be a regression coefficient between the exogenous and the endogenous variable or the factor loading (regression coefficient between an indicator and its factor). If there are fewer data points than the number of estimated parameters, the resulting model is \"unidentified\", since there are too few reference points to account for all the variance in the model. The solution is to constrain one of the paths to zero, which means that it is no longer part of the model.In specifying pathways in a model, the modeler can posit two types of relationships: (1) free pathways, in which hypothesized causal (in fact counterfactual) relationships between variables are tested, and therefore are left 'free' to vary, and (2) relationships between variables that already have an estimated relationship, usually based on previous studies, which are 'fixed' in the model.Two main components of models are distinguished in SEM: the structural model showing potential causal dependencies between endogenous and exogenous variables, and the measurement model showing the relations between latent variables and their indicators. Exploratory and confirmatory factor analysis models, for example, contain only the measurement part, while path diagrams can be viewed as SEMs that contain only the structural part.Although each technique in the SEM family is different, the following aspects are common to many SEM methods.Direction in the directed network models of SEM arises from presumed cause-effect assumptions made about reality. Social interactions and artifacts are often epiphenomena \u2013 secondary phenomena that are difficult to directly link to causal factors. An example of a physiological epiphenomenon is, for example, time to complete a 100-meter sprint. A person may be able to improve their sprint speed from 12 seconds to 11 seconds, but it will be difficult to attribute that improvement to any direct causal factors, like diet, attitude, weather, etc. The 1 second improvement in sprint time is an epiphenomenon \u2013 the holistic product of interaction of many individual factors.SEM path analysis methods are popular in the social sciences because of their accessibility; packaged computer programs allow researchers to obtain results without the inconvenience of understanding experimental design and control, effect and sample sizes, and numerous other factors that are part of good research design. Supporters say that this re\ufb02ects a holistic, and less blatantly causal, interpretation of many real world phenomena \u2013 especially in psychology and social interaction \u2013 than may be adopted in the natural sciences; detractors suggest that many flawed conclusions have been drawn because of this lack of experimental control.Pearl[13] has extended SEM from linear to nonparametric models, and proposed causal and counterfactual interpretations of the equations. For example, excluding a variable Z from the arguments of an equation asserts that the dependent variable is independent of interventions on the excluded variable, once we hold constant the remaining arguments. Nonparametric SEMs permit the estimation of total, direct and indirect effects without making any commitment to the form of the equations or to the distributions of the error terms. This extends mediation analysis to systems involving categorical variables in the presence of nonlinear interactions. Bollen and Pearl[14] survey the history of the causal interpretation of SEM and why it has become a source of confusions and controversies.Advances in computers made it simple for novices to apply structural equation methods in the computer-intensive analysis of large datasets in complex, unstructured problems. The most popular solution techniques fall into three classes of algorithms: (1) ordinary least squares algorithms applied independently to each path, such as applied in the so-called PLS path analysis packages which estimate with OLS; (2) covariance analysis algorithms evolving from seminal work by Wold and his student Karl J\u00f6reskog implemented in LISREL, AMOS, and EQS; and (3) simultaneous equations regression algorithms developed at the Cowles Commission by Tjalling Koopmans.Systems of regression equation approaches were developed at the Cowles Commission from the 1950s on, extending the transportation modeling of Tjalling Koopmans. Sewall Wright and other statisticians attempted to promote path analysis methods at Cowles (then at the University of Chicago). University of Chicago statisticians identified many faults with path analysis applications to the social sciences; faults which did not pose significant problems for identifying gene transmission in Wright's context, but which made path methods such as PLS-PA and LISREL problematic in the social sciences. Freedman (1987) summarized these objections in path analyses: \"failure to distinguish among causal assumptions, statistical implications, and policy claims has been one of the main reasons for the suspicion and confusion surrounding quantitative methods in the social sciences\" (see also Wold's (1987) response). Wright's path analysis never gained a large following among U.S. econometricians, but was successful in influencing Hermann Wold and his student Karl J\u00f6reskog. J\u00f6reskog's student Claes Fornell promoted LISREL in the US.Both LISREL and PLS-PA were conceived as iterative computer algorithms, with an emphasis from the start on creating an accessible graphical and data entry interface and extension of Wright's (1921) path analysis. Early Cowles Commission work on simultaneous equations estimation centered on Koopman and Hood's (1953) algorithms from the economics of transportation and optimal routing, with maximum likelihood estimation, and closed form algebraic calculations, as iterative solution search techniques were limited in the days before computers. Anderson and Rubin (1949, 1950) developed the limited information maximum likelihood estimator for the parameters of a single structural equation, which indirectly included the two-stage least squares estimator and its asymptotic distribution (Anderson, 2005) and Farebrother (1999). Two-stage least squares was originally proposed as a method of estimating the parameters of a single structural equation in a system of linear simultaneous equations, being introduced by Theil (1953a, 1953b, 1961) and more or less independently by Basmann (1957) and Sargan (1958). Anderson's limited information maximum likelihood estimation was eventually implemented in a computer search algorithm, where it competed with other iterative SEM algorithms. Of these, two-stage least squares was by far the most widely used method in the 1960s and the early 1970s.Loose and confusing terminology has been used to obscure weaknesses in the methods. In particular, PLS-PA (the Lohmoller algorithm) has been conflated with partial least squares regression PLSR, which is a substitute for ordinary least squares regression and has nothing to do with path analysis. PLS-PA has been falsely promoted as a method that works with small datasets when other estimation approaches fail. Westland (2010) decisively showed this not to be true and developed an algorithm for sample sizes in SEM. Since the 1970s, the 'small sample size' assertion has been known to be false (see for example Dhrymes, 1972, 1974; Dhrymes & Erlat, 1972; Dhrymes et al., 1972; Gupta, 1969; Sobel, 1982).Structural equation modeling, as the term is currently used in sociology, psychology, and other social sciences evolved from the earlier methods in genetic path modeling of Sewall Wright. Their modern forms came about with computer intensive implementations in the 1960s and 1970s. SEM evolved in three different streams: (1) systems of equation regression methods developed mainly at the Cowles Commission; (2) iterative maximum likelihood algorithms for path analysis developed mainly by Karl Gustav J\u00f6reskog at the Educational Testing Service and subsequently at the University of Uppsala; and (3) iterative canonical correlation fit algorithms for path analysis also developed at the University of Uppsala by Hermann Wold. Much of this development occurred at a time that automated computing was offering substantial upgrades over the existing calculator and analogue computing methods available, themselves products of the proliferation of office equipment innovations in the late 20th century. The 2015 text Structural Equation Modeling: From Paths to Networks provides a history of the methods.[12]Various methods in structural equation modeling have been used in the sciences,[8] business,[9] education,[10] and other fields. Criticism of SEM methods often addresses pitfalls in mathematical formulation, weak external validity of some accepted models and philosophical bias inherent to the standard procedures [11].A simplistic model suggesting that intelligence (as measured by four questions) can predict academic performance (as measured by SAT, ACT, and high school GPA) is shown above (top right). In SEM diagrams, latent variables are commonly shown as ovals and observed variables as rectangles. The diagram above shows how error (e) influences each intelligence question and the SAT, ACT, and GPA scores, but does not influence the latent variables. SEM provides numerical estimates for each of the parameters (arrows) in the model to indicate the strength of the relationships. Thus, in addition to testing the overall theory, SEM therefore allows the researcher to diagnose which observed variables are good indicators of the latent variables.[7]Use of SEM is commonly justified in the social sciences because of its ability to impute relationships between unobserved constructs (latent variables) from observable variables.[5] To provide a simple example, the concept of human intelligence cannot be measured directly as one could measure height or weight. Instead, psychologists develop a hypothesis of intelligence and write measurement instruments with items (questions) designed to measure intelligence according to their hypothesis.[6] They would then use SEM to test their hypothesis using data gathered from people who took their intelligence test. With SEM, \"intelligence\" would be the latent variable and the test items would be the observed variables.Structural equation modeling (SEM) includes a diverse set of mathematical models, computer algorithms, and statistical methods that fit networks of constructs to data.[1] SEM includes confirmatory factor analysis, path analysis, partial least squares path modeling, and latent growth modeling.[2] The concept should not be confused with the related concept of structural models in econometrics, nor with structural models in economics. Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables.[1][3] The links between constructs of a structural equation model may be estimated with independent regression equations or through more involved approaches such as those employed in LISREL.[4]",
            "title": "Structural equation modeling",
            "url": "https://en.wikipedia.org/wiki/Structural_equation_model"
        },
        {
            "desc_links": [
                "/wiki/Linear_algebra",
                "/wiki/Spectral_decomposition",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)",
                "/wiki/Canonical_form",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Diagonalizable_matrix"
            ],
            "links": [
                "/wiki/Matrix_pencil",
                "/wiki/Optics",
                "/wiki/Forward_Scattering_Alignment",
                "/wiki/Radar",
                "/wiki/Back_Scattering_Alignment",
                "/wiki/Generalized_eigenspace",
                "/wiki/Generalized_eigenvector",
                "/wiki/Power_iteration",
                "/wiki/Rayleigh_quotient",
                "/wiki/Hermitian_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Schur_decomposition",
                "/wiki/Backsubstitution",
                "/wiki/Divide-and-conquer_eigenvalue_algorithm",
                "/wiki/Gaussian_elimination",
                "/wiki/System_of_linear_equations#Solving_a_linear_system",
                "/wiki/System_of_linear_equations",
                "/wiki/Sequence",
                "/wiki/Almost_always",
                "/wiki/Google",
                "/wiki/PageRank",
                "/wiki/Linear_span",
                "/wiki/Arnoldi_iteration",
                "/wiki/QR_algorithm",
                "/wiki/Newton%27s_method",
                "/wiki/Round-off_error",
                "/wiki/Ill-conditioned",
                "/wiki/Abel%E2%80%93Ruffini_theorem",
                "/wiki/Iterative_method",
                "/wiki/Characteristic_polynomial",
                "/wiki/Numerical_analysis",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Orthogonal_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Holomorphic_functional_calculus",
                "/wiki/Diagonal_matrix",
                "/wiki/Power_series",
                "/wiki/Laplace_operator",
                "/wiki/Data",
                "/wiki/Inverse_function",
                "/wiki/Diagonal_matrix",
                "/wiki/Nonsingular",
                "/wiki/Linearly_independent",
                "/wiki/Geometric_multiplicity",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Factorization",
                "/wiki/Characteristic_polynomial",
                "/wiki/Spectrum_of_a_matrix",
                "/wiki/Linear_algebra",
                "/wiki/Spectral_decomposition",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)",
                "/wiki/Canonical_form",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Diagonalizable_matrix"
            ],
            "text": "The set of matrices of the form A \u2212 \u03bbB, where \u03bb is a complex number, is called a pencil; the term matrix pencil can also refer to the pair (A,B) of matrices.[9] If B is invertible, then the original problem can be written in the formAnd since P is invertible, we multiply the equation from the right by its inverse, finishing the proof.And the proof isThen the following equality holdswhere A and B are matrices. If v obeys this equation, with some \u03bb, then we call v the generalized eigenvector of A and B (in the 2nd sense), and \u03bb is called the generalized eigenvalue of A and B (in the 2nd sense) which corresponds to the generalized eigenvector v. The possible values of \u03bb must obey the following equationA generalized eigenvalue problem (2nd sense) is the problem of finding a vector v that obeysFor example, in coherent electromagnetic scattering theory, the linear transformation A represents the action performed by the scattering object, and the eigenvectors represent polarization states of the electromagnetic wave. In optics, the coordinate system is defined from the wave's viewpoint, known as the Forward Scattering Alignment (FSA), and gives rise to a regular eigenvalue equation, whereas in radar, the coordinate system is defined from the radar's viewpoint, known as the Back Scattering Alignment (BSA), and gives rise to a coneigenvalue equation.A conjugate eigenvector or coneigenvector is a vector sent after transformation to a scalar multiple of its conjugate, where the scalar is called the conjugate eigenvalue or coneigenvalue of the linear transformation. The coneigenvectors and coneigenvalues represent essentially the same information and meaning as the regular eigenvectors and eigenvalues, but arise when an alternative coordinate system is used. The corresponding equation isThis usage should not be confused with the generalized eigenvalue problem described below.Recall that the geometric multiplicity of an eigenvalue can be described as the dimension of the associated eigenspace, the nullspace of \u03bbI \u2212 A. The algebraic multiplicity can also be thought of as a dimension: it is the dimension of the associated generalized eigenspace (1st sense), which is the nullspace of the matrix (\u03bbI \u2212 A)k for any sufficiently large k. That is, it is the space of generalized eigenvectors (1st sense), where a generalized eigenvector is any vector which eventually becomes 0 if \u03bbI \u2212 A is applied to it enough times successively. Any eigenvector is a generalized eigenvector, and so each eigenspace is contained in the associated generalized eigenspace. This provides an easy proof that the geometric multiplicity is always less than or equal to the algebraic multiplicity.However, in practical large-scale eigenvalue methods, the eigenvectors are usually computed in other ways, as a byproduct of the eigenvalue computation. In power iteration, for example, the eigenvector is actually computed before the eigenvalue (which is typically computed by the Rayleigh quotient of the eigenvector).[6] In the QR algorithm for a Hermitian matrix (or any normal matrix), the orthonormal eigenvectors are obtained as a product of the Q matrices from the steps in the algorithm.[6] (For more general matrices, the QR algorithm yields the Schur decomposition first, from which the eigenvectors can be obtained by a backsubstitution procedure.[8]) For Hermitian matrices, the Divide-and-conquer eigenvalue algorithm is more efficient than the QR algorithm if both eigenvectors and eigenvalues are desired.[6]using Gaussian elimination or any other method for solving matrix equations.Once the eigenvalues are computed, the eigenvectors could be calculated by solving the equationThis sequence will almost always converge to an eigenvector corresponding to the eigenvalue of greatest magnitude, provided that v has a nonzero component of this eigenvector in the eigenvector basis (and also provided that there is only one eigenvalue of greatest magnitude). This simple algorithm is useful in some practical applications; for example, Google uses it to calculate the page rank of documents in their search engine.[7] Also, the power method is the starting point for many more sophisticated algorithms. For instance, by keeping not just the last vector in the sequence, but instead looking at the span of all the vectors in the sequence, one can get a better (faster converging) approximation for the eigenvector, and this idea is the basis of Arnoldi iteration.[6] Alternatively, the important QR algorithm is also based on a subtle transformation of a power method.[6]Iterative numerical algorithms for approximating roots of polynomials exist, such as Newton's method, but in general it is impractical to compute the characteristic polynomial and then apply these methods. One reason is that small round-off errors in the coefficients of the characteristic polynomial can lead to large errors in the eigenvalues and eigenvectors: the roots are an extremely ill-conditioned function of the coefficients.[6]In practice, eigenvalues of large matrices are not computed using the characteristic polynomial. Computing the polynomial becomes expensive in itself, and exact (symbolic) roots of a high-degree polynomial can be difficult to compute and express: the Abel\u2013Ruffini theorem implies that the roots of high-degree (5 or above) polynomials cannot in general be expressed simply using nth roots. Therefore, general algorithms to find eigenvectors and eigenvalues are iterative.Suppose that we want to compute the eigenvalues of a given matrix. If the matrix is small, we can compute them symbolically using the characteristic polynomial. However, this is often impossible for larger matrices, in which case we must use a numerical method.Note that each eigenvalue is multiplied by ni, the algebraic multiplicity.Note that each eigenvalue is raised to the power ni, the algebraic multiplicity.where Q is an orthogonal matrix whose columns are the eigenvectors of A, and \u039b is a diagonal matrix whose entries are the eigenvalues of A.As a special case, for every N\u00d7N real symmetric matrix, the eigenvalues are real and the eigenvectors can be chosen such that they are orthogonal to each other. Thus a real symmetric matrix A can be decomposed asfrom above. Once again, we find thatA similar technique works more generally with the holomorphic functional calculus, usingThe off-diagonal elements of f(\u039b) are zero; that is, f(\u039b) is also a diagonal matrix. Therefore, calculating f(A) reduces to just calculating the function on each of the eigenvalues.Because \u039b is a diagonal matrix, functions of \u039b are very easy to calculate:then we know thatThe eigendecomposition allows for much easier computation of power series of matrices. If f(x) is given bywhere the eigenvalues are subscripted with an 's' to denote being sorted. The position of the minimization is the lowest reliable eigenvalue. In measurement systems, the square root of this reliable eigenvalue is the average noise over the components of the system.If the eigenvalues are rank-sorted by value, then the reliable eigenvalue can be found by minimization of the Laplacian of the sorted eigenvalues:[5]The reliable eigenvalue can be found by assuming that eigenvalues of extremely similar and low value are a good representation of measurement noise (which is assumed low for most systems).The second mitigation extends the eigenvalue so that lower values have much less influence over inversion, but do still contribute, such that solutions near the noise will still be found.The first mitigation method is similar to a sparse sample of the original matrix, removing components that are not considered valuable. However, if the solution or detection process is near the noise level, truncating may remove components that influence the desired solution.Two mitigations have been proposed: 1) truncating small/zero eigenvalues, 2) extending the lowest reliable eigenvalue to those below it.When eigendecomposition is used on a matrix of measured, real data, the inverse may be less valid when all eigenvalues are used unmodified in the form above. This is because as eigenvalues become relatively small, their contribution to the inversion is large. Those near zero or at the \"noise\" of the measurement system will have undue influence and could hamper solutions (detection) using the inverse.Furthermore, because \u039b is a diagonal matrix, its inverse is easy to calculate:If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is nonsingular and its inverse is given byPutting the solutions back into the above simultaneous equationsThusAnd can be represented by a single vector equation involving 2 solutions as eigenvalues:The above equation can be decomposed into 2 simultaneous equations:ThenThe eigenvectors can be indexed by eigenvalues, i.e. using a double index, with vi,j being the jth eigenvector for the ith eigenvalue. The eigenvectors can also be indexed using the simpler notation of a single index vk, with k = 1, 2, ..., Nv.There will be 1 \u2264 mi \u2264 ni linearly independent solutions to each eigenvalue equation. The linear combinations of the mi solutions are the eigenvectors associated with the eigenvalue \u03bbi. The integer mi is termed the geometric multiplicity of \u03bbi. It is important to keep in mind that the algebraic multiplicity ni and geometric multiplicity mi may or may not be equal, but we always have mi \u2264 ni. The simplest case is of course when mi = ni = 1. The total number of linearly independent eigenvectors, Nv, can be calculated by summing the geometric multiplicitiesFor each eigenvalue, \u03bbi, we have a specific eigenvalue equationThe integer ni is termed the algebraic multiplicity of eigenvalue \u03bbi. The algebraic multiplicities sum to N:We can factor p asWe call p(\u03bb) the characteristic polynomial, and the equation, called the characteristic equation, is an Nth order polynomial equation in the unknown \u03bb. This equation will have N\u03bb distinct solutions, where 1 \u2264 N\u03bb \u2264 N . The set of solutions, that is, the eigenvalues, is called the spectrum of A.[1][2][3]This yields an equation for the eigenvaluesA (non-zero) vector v of dimension N is an eigenvector of a square (N\u00d7N) matrix A if it satisfies the linear equationIn linear algebra, eigendecomposition or sometimes spectral decomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way.",
            "title": "Eigendecomposition of a matrix",
            "url": "https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices"
        },
        {
            "desc_links": [
                "/wiki/Real_number",
                "/wiki/Self-adjoint_operator",
                "/wiki/Real_number",
                "/wiki/Inner_product_space",
                "/wiki/Complex_number",
                "/wiki/Hermitian_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Diagonal_matrix",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Skew-symmetric_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Transpose"
            ],
            "links": [
                "/wiki/Diagonal_matrix",
                "/wiki/Taylor%27s_theorem",
                "/wiki/Conic_section",
                "/wiki/Quadratic_form",
                "/wiki/Hessian_matrix",
                "/wiki/Non-singular_matrix",
                "/wiki/Orthogonal_matrix",
                "/wiki/Positive_definite_matrix",
                "/wiki/Polar_decomposition",
                "/wiki/Jordan_normal_form",
                "/wiki/Unitary_matrix",
                "/wiki/L%C3%A9on_Autonne",
                "/wiki/Teiji_Takagi",
                "/wiki/Singular_value",
                "/wiki/Hermitian_matrix",
                "/wiki/Eigenvalues",
                "/wiki/Spectral_theorem",
                "/wiki/Real_number",
                "/wiki/Diagonal_matrix",
                "/wiki/Orthogonal_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Up_to",
                "/wiki/Orthonormal_basis",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Linear_operator",
                "/wiki/Inner_product",
                "/wiki/Differential_geometry",
                "/wiki/Tangent_space",
                "/wiki/Manifold",
                "/wiki/Riemannian_manifold",
                "/wiki/Hilbert_space",
                "/wiki/Matrix_congruence",
                "/wiki/Normal_matrix",
                "/wiki/Square_matrix",
                "/wiki/Field_(mathematics)",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Main_diagonal",
                "/wiki/Skew-symmetric_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Commutativity",
                "/wiki/Real_number",
                "/wiki/Self-adjoint_operator",
                "/wiki/Real_number",
                "/wiki/Inner_product_space",
                "/wiki/Complex_number",
                "/wiki/Hermitian_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Diagonal_matrix",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Skew-symmetric_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Transpose"
            ],
            "text": "An n-by-n matrix A is said to be symmetrizable if there exists an invertible diagonal matrix D and symmetric matrix S such that A = DS. The transpose of a symmetrizable matrix is symmetrizable, since AT = (DS)T = SD = D\u22121 (DSD) and DSD is symmetric. A matrix A = (aij) is symmetrizable if and only if the following conditions are met:This is important partly because the second-order behavior of every smooth multi-variable function is described by the quadratic form belonging to the function's Hessian; this is a consequence of Taylor's theorem.with real numbers \u03bbi. This considerably simplifies the study of quadratic forms, as well as the study of the level sets {x\u00a0: q(x) = 1} which are generalizations of conic sections.Every quadratic form q on Rn can be uniquely written in the form q(x) = xTAx with a symmetric n-by-n matrix A. Because of the above spectral theorem, one can then say that every quadratic form, up to the choice of an orthonormal basis of Rn, \"looks like\"Symmetric n-by-n matrices of real functions appear as the Hessians of twice continuously differentiable functions of n real variables.Every complex symmetric matrix A can be diagonalized by unitary congruenceA complex symmetric matrix may not be diagonalizable by similarity; every real symmetric matrix is diagonalizable by a real orthogonal similarity.Every real non-singular matrix can be uniquely factored as the product of an orthogonal matrix and a symmetric positive definite matrix, which is called a polar decomposition. Singular matrices can also be factored, but not uniquely.Using the Jordan normal form, one can prove that every square real matrix can be written as a product of two real symmetric matrices, and every square complex matrix can be written as a product of two complex symmetric matrices.[4]A complex symmetric matrix can be 'diagonalized' using a unitary matrix: thus if A is a complex symmetric matrix, there is a unitary matrix U such that U A U T is a real diagonal matrix. This result is referred to as the Autonne\u2013Takagi factorization. It was originally proved by L\u00e9on Autonne (1915) and Teiji Takagi (1925) and rediscovered with different proofs by several other mathematicians.[2][3] In fact, the matrix B = A\u2020A is Hermitian and non-negative, so there is a unitary matrix V such that V\u2020BV is diagonal with non-negative real entries. Thus C = VTAV is complex symmetric with C\u2020C real. Writing C = X + iY with X and Y real symmetric matrices, C\u2020C = X2 + Y2 + i(XY \u2212 YX). Thus XY = YX. Since X and Y commute, there is a real orthogonal matrix W such that both WXWT and WYWT are diagonal. Setting U = WVT, the matrix UAUT is complex diagonal. Post-multiplying U by another diagonal matrix the diagonal entries can be made to be real and non-negative. Since their squares are the eigenvalues of A\u2020A, they coincide with the singular values of A. (Note, about the eigen-decomposition of a complex symmetric matrix A, the Jordan normal form of A may not be diagonal, therefore A may not be diagonalized by any similarity transformation.)Every real symmetric matrix is Hermitian, and therefore all its eigenvalues are real. (In fact, the eigenvalues are the entries in the diagonal matrix D (above), and therefore D is uniquely determined by A up to the order of its entries.) Essentially, the property of being symmetric for real matrices corresponds to the property of being Hermitian for complex matrices.The finite-dimensional spectral theorem says that any symmetric matrix whose entries are real can be diagonalized by an orthogonal matrix. More explicitly: For every symmetric real matrix A there exists a real orthogonal matrix Q such that D = QTAQ is a diagonal matrix. Every symmetric matrix is thus, up to choice of an orthonormal basis, a diagonal matrix.Since this definition is independent of the choice of basis, symmetry is a property that depends only on the linear operator A and a choice of inner product. This characterization of symmetry is useful, for example, in differential geometry, for each tangent space to a manifold may be endowed with an inner product, giving rise to what is called a Riemannian manifold. Another area where this formulation is used is in Hilbert spaces.Any matrix congruent to a symmetric matrix is again symmetric: if X is a symmetric matrix then so is AXAT for any matrix A. A symmetric matrix is necessarily a normal matrix.Notice that 1/2(X + XT) \u2208 Symn and 1/2(X \u2212 XT) \u2208 Skewn. This is true for every square matrix X with entries from any field whose characteristic is different from 2.where \u2295 denotes the direct sum. Let X \u2208 Matn thenLet Matn denote the space of n \u00d7 n matrices. A symmetric n \u00d7 n matrix is determined by n(n + 1)/2 scalars (the number of entries on or above the main diagonal). Similarly, a skew-symmetric matrix is determined by n(n \u2212 1)/2 scalars (the number of entries above the main diagonal). If Symn denotes the space of n \u00d7 n symmetric matrices and Skewn the space of n \u00d7 n skew-symmetric matrices then Matn = Symn + Skewn and Symn \u2229 Skewn = {0}, i.e.The sum and difference of two symmetric matrices is again symmetric, but this is not always true for the product: given symmetric matrices A and B, then AB is symmetric if and only if A and B commute, i.e., if AB = BA. So for integer n, An is symmetric if A is symmetric. If A\u22121 exists, it is symmetric if and only if A is symmetric.In linear algebra, a real symmetric matrix represents a self-adjoint operator[1] over a real inner product space. The corresponding object for a complex inner product space is a Hermitian matrix with complex-valued entries, which is equal to its conjugate transpose. Therefore, in linear algebra over the complex numbers, it is often assumed that a symmetric matrix refers to one which has real-valued entries. Symmetric matrices appear naturally in a variety of applications, and typical numerical linear algebra software makes special accommodations for them.Every square diagonal matrix is symmetric, since all off-diagonal elements are zero. Similarly in characteristic different from 2, each diagonal element of a skew-symmetric matrix must be zero, since each is its own negative.The following 3 \u00d7 3 matrix is symmetric:The entries of a symmetric matrix are symmetric with respect to the main diagonal. So if the entries are written as A = (aij), then aij = aji, for all indices i and j.Because equal matrices have equal dimensions, only square matrices can be symmetric.In linear algebra, a symmetric matrix is a square matrix that is equal to its transpose. Formally, matrix A is symmetric if",
            "title": "Symmetric matrix",
            "url": "https://en.wikipedia.org/wiki/Symmetric_matrix"
        },
        {
            "desc_links": [
                "/wiki/Bilinear_form",
                "/wiki/Definite_bilinear_form",
                "/wiki/Sesquilinear_form",
                "/wiki/Inner_product",
                "/wiki/Vector_space",
                "/wiki/Hermitian_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Linear_algebra",
                "/wiki/Symmetric_matrix",
                "/wiki/Real_number",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Vector_(mathematics)",
                "/wiki/Transpose"
            ],
            "links": [
                "/wiki/Bounded_operator",
                "/wiki/Polarization_identity",
                "/wiki/Schur_complement#Schur_complement_condition_for_positive_definiteness",
                "/wiki/Block_matrix",
                "/wiki/Partially_ordered_set",
                "/wiki/Functional_analysis",
                "/wiki/Positive_operator",
                "/wiki/Cholesky_decomposition",
                "/wiki/Gram_matrix",
                "/wiki/Principal_minor",
                "/wiki/Conjugate_transpose",
                "/wiki/Hermitian_matrix",
                "/wiki/Eigenvalues",
                "/wiki/Diagonalizable_matrix#Simultaneous_diagonalization",
                "/wiki/Cholesky_decomposition",
                "/wiki/Diagonalizable_matrix#Simultaneous_diagonalization",
                "/wiki/Matrix_similarity",
                "/wiki/Quadratic_function",
                "/wiki/Optimization_(mathematics)",
                "/wiki/Strictly_convex_function",
                "/wiki/Quadratic_form",
                "/wiki/Hermitian_matrix",
                "/wiki/Statistics",
                "/wiki/Covariance_matrix",
                "/wiki/Multivariate_probability_distribution",
                "/wiki/Gradient",
                "/wiki/Hessian_matrix",
                "/wiki/Quadratic_form"
            ],
            "text": "In summary, the distinguishing feature between the real and complex case is that, a bounded positive operator on a complex Hilbert space is necessarily Hermitian, or self adjoint. The general claim can be argued using the polarization identity. That is no longer true in the real case.In general, we have Re(z*Mz) > 0 for all complex nonzero vectors z if and only if the Hermitian part (M + M*)/2 of M is positive definite in the narrower sense. Similarly, we have xTMx > 0 for all real nonzero vectors x if and only if the symmetric part (M + MT)/2 of M is positive definite in the narrower sense.Indeed, with this definition, a real matrix is positive definite if and only if zTMz > 0 for all nonzero real vectors z, even if M is not symmetric.On the other hand, for a symmetric real matrix M, the condition \"zTMz > 0 for all nonzero real vectors z\" does imply that M is positive definite in the complex sense.which is not real. Therefore, M is not positive definite.then for any real vector z with entries a and b we have zTMz = (a\u2212b)a + (a+b)b = a2 + b2, which is always positive if z is not zero. However, if z is the complex vector with entries 1 and i, one getsBy this definition, a positive definite real matrix M is Hermitian, hence symmetric; and zTMz is positive for all non-zero real column vectors z. However the last condition alone is not sufficient for M to be positive definite. For example, ifFor complex matrices, the most common definition says that \"M is positive definite if and only if z*Mz is real and positive for all non-zero complex column vectors z\". This condition implies that M is Hermitian, that is, its transpose is equal to its conjugate. To see this, consider the matrices A = (M+M*)/2 and B = (M\u2212M*)/(2i), so that M = A+iB and z*Mz = z*Az + iz*Bz. The matrices A and B are Hermitian, therefore z*Az and z*Bz are individually real. If z*Mz is real, then z*Bz must be zero for all z. Then B is the zero matrix and M = A, proving that M is Hermitian.Since every real matrix is also a complex matrix, the definitions of \"positive definite\" for the two classes must agree.Converse results can be proved with stronger conditions on the blocks, for instance using the Schur complement.A similar argument can be applied to D, and thus we conclude that both A and D must be positive definite matrices, as well.We have that z*Mz \u2265 0 for all complex z, and in particular for z = ( v, 0)T. Thenwhere each block is n \u00d7 n. By applying the positivity condition, it immediately follows that A and D are hermitian, and C = B*.A positive 2n \u00d7 2n matrix may also be defined by blocks:For arbitrary square matrices M, N we write M\u00a0\u2265\u00a0N if M\u00a0\u2212\u00a0N\u00a0\u2265\u00a00; i.e., M\u00a0\u2212\u00a0N is positive semi-definite. This defines a partial ordering on the set of all square matrices. One can similarly define a strict partial ordering M\u00a0>\u00a0N.If M is a Hermitian positive semidefinite matrix, one sometimes writes M\u00a0\u2265\u00a00 and if M is positive definite one writes M\u00a0>\u00a00.[4] The notion comes from functional analysis where positive semidefinite matrices define positive operators.A Hermitian matrix which is neither positive definite, negative definite, positive semidefinite, nor negative semidefinite is called indefinite. Indefinite matrices are also characterized by having both positive and negative eigenvalues.for all x in Cn (or, all x in Rn for the real matrix).It is called negative semidefinite ifA Hermitian matrix is positive semidefinite if and only if all of its principal minors are nonnegative. It is however not enough to consider the leading principal minors only, as is checked on the diagonal matrix with entries 0 and -1.For any matrix A, the matrix A*A is positive semidefinite, and rank(A)\u00a0= rank(A*A). Conversely, any Hermitian positive semi-definite matrix M can be written as M\u00a0= LL*, where L is lower triangular; this is the Cholesky decomposition. If M is not positive definite, then some of the diagonal elements of L may be zero.A matrix M is positive semidefinite if and only if it arises as the Gram matrix of some set of vectors. In contrast to the positive definite case, these vectors need not be linearly independent.for all x in Cn (or, all x in Rn for the real matrix).M is called positive semidefinite (or sometimes nonnegative definite) ifA matrix is negative definite if its k-th order leading principal minor is negative when k is odd, and positive when k is even.for all non-zero x in Cn (or, all non-zero x in Rn for the real matrix), where x* is the conjugate transpose of x.The n \u00d7 n Hermitian matrix M is said to be negative definite ifA Hermitian matrix is negative definite, negative semidefinite, or positive semidefinite if and only if all of its eigenvalues are negative, non-positive, or non-negative, respectively.Note that this result does not contradict what is said on simultaneous diagonalization in the article Diagonalizable matrix, which refers to simultaneous diagonalization by a similarity transformation. Our result here is more akin to a simultaneous diagonalization of two quadratic forms, and is useful for optimization of one form under conditions on the other.[3]Let M be a symmetric and N a symmetric and positive definite matrix. Write the generalized eigenvalue equation as (M\u2212\u03bbN)x = 0 where we impose that x be normalized, i.e. xTNx = 1. Now we use Cholesky decomposition to write the inverse of N as QTQ. Multiplying by Q and letting x \u2192 QTy, we get Q(M\u2212\u03bbN)QTy = 0, which can be rewritten as (QMQT)y = \u03bby where yTy = 1. Manipulation now yields MX = NX\u039b where X is a matrix having as columns the generalized eigenvectors and \u039b is a diagonal matrix with the generalized eigenvalues. Now premultiplication with XT gives the final result: XTMX = \u039b and XTNX = I, but note that this is no longer an orthogonal diagonalization with respect to the inner product where yTy = 1. In fact, we diagonalized M with respect to the inner product induced by N.A symmetric matrix and another symmetric and positive definite matrix can be simultaneously diagonalized, although not necessarily via a similarity transformation. This result does not extend to the case of three or more matrices. In this section we write for the real case. Extension to the complex case is immediate.More generally, any quadratic function from Rn to R can be written as xTMx + xTb + c where M is a symmetric n \u00d7 n matrix, b is a real n-vector, and c a real constant. This quadratic function is strictly convex when M is positive definite, and hence has a unique finite global minimum, if and only if M is positive definite. For this reason, positive definite matrices play an important role in optimization problems.A symmetric matrix M is positive definite if and only if its quadratic form is a strictly convex function.The (purely) quadratic form associated with a real matrix M is the function Q\u00a0: Rn \u2192 R such that Q(x) = xTMx for all x. M can be assumed symmetric by replacing it with \u00bd(M + MT).Let M be an n \u00d7 n Hermitian matrix. The following properties are equivalent to M being positive definite:In statistics, the covariance matrix of a multivariate probability distribution is always positive semi-definite; and it is positive definite unless one variable is an exact linear function of the others. Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution.More generally, a twice-differentiable real function f on n real variables has local minimum at arguments z1, ..., zn if its gradient is zero and its Hessian (the matrix of all second derivatives) is positive semi-definite at that point. Similar statements can be made for negative definite and semi-definite matrices.A general purely quadratic real function f(z) on n real variables z1, ..., zn can always be written as zTMz where z is the column vector with those variables, and M is a symmetric real matrix. Therefore, the matrix being positive definite means that f has a unique minimum (zero) when z is zero, and is strictly positive for any other z.Some authors use more general definitions of \"positive definite\", including some non-symmetric real matrices, or non-Hermitian complex ones.",
            "title": "Positive-definite matrix",
            "url": "https://en.wikipedia.org/wiki/Positive_semidefinite_matrix"
        },
        {
            "desc_links": [
                "/wiki/Bilinear_form",
                "/wiki/Definite_bilinear_form",
                "/wiki/Sesquilinear_form",
                "/wiki/Inner_product",
                "/wiki/Vector_space",
                "/wiki/Hermitian_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Linear_algebra",
                "/wiki/Symmetric_matrix",
                "/wiki/Real_number",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Vector_(mathematics)",
                "/wiki/Transpose"
            ],
            "links": [
                "/wiki/Bounded_operator",
                "/wiki/Polarization_identity",
                "/wiki/Schur_complement#Schur_complement_condition_for_positive_definiteness",
                "/wiki/Block_matrix",
                "/wiki/Partially_ordered_set",
                "/wiki/Functional_analysis",
                "/wiki/Positive_operator",
                "/wiki/Cholesky_decomposition",
                "/wiki/Gram_matrix",
                "/wiki/Principal_minor",
                "/wiki/Conjugate_transpose",
                "/wiki/Hermitian_matrix",
                "/wiki/Eigenvalues",
                "/wiki/Diagonalizable_matrix#Simultaneous_diagonalization",
                "/wiki/Cholesky_decomposition",
                "/wiki/Diagonalizable_matrix#Simultaneous_diagonalization",
                "/wiki/Matrix_similarity",
                "/wiki/Quadratic_function",
                "/wiki/Optimization_(mathematics)",
                "/wiki/Strictly_convex_function",
                "/wiki/Quadratic_form",
                "/wiki/Hermitian_matrix",
                "/wiki/Statistics",
                "/wiki/Covariance_matrix",
                "/wiki/Multivariate_probability_distribution",
                "/wiki/Gradient",
                "/wiki/Hessian_matrix",
                "/wiki/Quadratic_form"
            ],
            "text": "In summary, the distinguishing feature between the real and complex case is that, a bounded positive operator on a complex Hilbert space is necessarily Hermitian, or self adjoint. The general claim can be argued using the polarization identity. That is no longer true in the real case.In general, we have Re(z*Mz) > 0 for all complex nonzero vectors z if and only if the Hermitian part (M + M*)/2 of M is positive definite in the narrower sense. Similarly, we have xTMx > 0 for all real nonzero vectors x if and only if the symmetric part (M + MT)/2 of M is positive definite in the narrower sense.Indeed, with this definition, a real matrix is positive definite if and only if zTMz > 0 for all nonzero real vectors z, even if M is not symmetric.On the other hand, for a symmetric real matrix M, the condition \"zTMz > 0 for all nonzero real vectors z\" does imply that M is positive definite in the complex sense.which is not real. Therefore, M is not positive definite.then for any real vector z with entries a and b we have zTMz = (a\u2212b)a + (a+b)b = a2 + b2, which is always positive if z is not zero. However, if z is the complex vector with entries 1 and i, one getsBy this definition, a positive definite real matrix M is Hermitian, hence symmetric; and zTMz is positive for all non-zero real column vectors z. However the last condition alone is not sufficient for M to be positive definite. For example, ifFor complex matrices, the most common definition says that \"M is positive definite if and only if z*Mz is real and positive for all non-zero complex column vectors z\". This condition implies that M is Hermitian, that is, its transpose is equal to its conjugate. To see this, consider the matrices A = (M+M*)/2 and B = (M\u2212M*)/(2i), so that M = A+iB and z*Mz = z*Az + iz*Bz. The matrices A and B are Hermitian, therefore z*Az and z*Bz are individually real. If z*Mz is real, then z*Bz must be zero for all z. Then B is the zero matrix and M = A, proving that M is Hermitian.Since every real matrix is also a complex matrix, the definitions of \"positive definite\" for the two classes must agree.Converse results can be proved with stronger conditions on the blocks, for instance using the Schur complement.A similar argument can be applied to D, and thus we conclude that both A and D must be positive definite matrices, as well.We have that z*Mz \u2265 0 for all complex z, and in particular for z = ( v, 0)T. Thenwhere each block is n \u00d7 n. By applying the positivity condition, it immediately follows that A and D are hermitian, and C = B*.A positive 2n \u00d7 2n matrix may also be defined by blocks:For arbitrary square matrices M, N we write M\u00a0\u2265\u00a0N if M\u00a0\u2212\u00a0N\u00a0\u2265\u00a00; i.e., M\u00a0\u2212\u00a0N is positive semi-definite. This defines a partial ordering on the set of all square matrices. One can similarly define a strict partial ordering M\u00a0>\u00a0N.If M is a Hermitian positive semidefinite matrix, one sometimes writes M\u00a0\u2265\u00a00 and if M is positive definite one writes M\u00a0>\u00a00.[4] The notion comes from functional analysis where positive semidefinite matrices define positive operators.A Hermitian matrix which is neither positive definite, negative definite, positive semidefinite, nor negative semidefinite is called indefinite. Indefinite matrices are also characterized by having both positive and negative eigenvalues.for all x in Cn (or, all x in Rn for the real matrix).It is called negative semidefinite ifA Hermitian matrix is positive semidefinite if and only if all of its principal minors are nonnegative. It is however not enough to consider the leading principal minors only, as is checked on the diagonal matrix with entries 0 and -1.For any matrix A, the matrix A*A is positive semidefinite, and rank(A)\u00a0= rank(A*A). Conversely, any Hermitian positive semi-definite matrix M can be written as M\u00a0= LL*, where L is lower triangular; this is the Cholesky decomposition. If M is not positive definite, then some of the diagonal elements of L may be zero.A matrix M is positive semidefinite if and only if it arises as the Gram matrix of some set of vectors. In contrast to the positive definite case, these vectors need not be linearly independent.for all x in Cn (or, all x in Rn for the real matrix).M is called positive semidefinite (or sometimes nonnegative definite) ifA matrix is negative definite if its k-th order leading principal minor is negative when k is odd, and positive when k is even.for all non-zero x in Cn (or, all non-zero x in Rn for the real matrix), where x* is the conjugate transpose of x.The n \u00d7 n Hermitian matrix M is said to be negative definite ifA Hermitian matrix is negative definite, negative semidefinite, or positive semidefinite if and only if all of its eigenvalues are negative, non-positive, or non-negative, respectively.Note that this result does not contradict what is said on simultaneous diagonalization in the article Diagonalizable matrix, which refers to simultaneous diagonalization by a similarity transformation. Our result here is more akin to a simultaneous diagonalization of two quadratic forms, and is useful for optimization of one form under conditions on the other.[3]Let M be a symmetric and N a symmetric and positive definite matrix. Write the generalized eigenvalue equation as (M\u2212\u03bbN)x = 0 where we impose that x be normalized, i.e. xTNx = 1. Now we use Cholesky decomposition to write the inverse of N as QTQ. Multiplying by Q and letting x \u2192 QTy, we get Q(M\u2212\u03bbN)QTy = 0, which can be rewritten as (QMQT)y = \u03bby where yTy = 1. Manipulation now yields MX = NX\u039b where X is a matrix having as columns the generalized eigenvectors and \u039b is a diagonal matrix with the generalized eigenvalues. Now premultiplication with XT gives the final result: XTMX = \u039b and XTNX = I, but note that this is no longer an orthogonal diagonalization with respect to the inner product where yTy = 1. In fact, we diagonalized M with respect to the inner product induced by N.A symmetric matrix and another symmetric and positive definite matrix can be simultaneously diagonalized, although not necessarily via a similarity transformation. This result does not extend to the case of three or more matrices. In this section we write for the real case. Extension to the complex case is immediate.More generally, any quadratic function from Rn to R can be written as xTMx + xTb + c where M is a symmetric n \u00d7 n matrix, b is a real n-vector, and c a real constant. This quadratic function is strictly convex when M is positive definite, and hence has a unique finite global minimum, if and only if M is positive definite. For this reason, positive definite matrices play an important role in optimization problems.A symmetric matrix M is positive definite if and only if its quadratic form is a strictly convex function.The (purely) quadratic form associated with a real matrix M is the function Q\u00a0: Rn \u2192 R such that Q(x) = xTMx for all x. M can be assumed symmetric by replacing it with \u00bd(M + MT).Let M be an n \u00d7 n Hermitian matrix. The following properties are equivalent to M being positive definite:In statistics, the covariance matrix of a multivariate probability distribution is always positive semi-definite; and it is positive definite unless one variable is an exact linear function of the others. Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution.More generally, a twice-differentiable real function f on n real variables has local minimum at arguments z1, ..., zn if its gradient is zero and its Hessian (the matrix of all second derivatives) is positive semi-definite at that point. Similar statements can be made for negative definite and semi-definite matrices.A general purely quadratic real function f(z) on n real variables z1, ..., zn can always be written as zTMz where z is the column vector with those variables, and M is a symmetric real matrix. Therefore, the matrix being positive definite means that f has a unique minimum (zero) when z is zero, and is strictly positive for any other z.Some authors use more general definitions of \"positive definite\", including some non-symmetric real matrices, or non-Hermitian complex ones.",
            "title": "Positive-definite matrix",
            "url": "https://en.wikipedia.org/wiki/Positive_semidefinite_matrix"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Orthogonal basis",
            "url": "https://en.wikipedia.org/wiki/Orthogonal_basis"
        },
        {
            "desc_links": [
                "/wiki/Simple_linear_regression",
                "/wiki/Multiple_regression",
                "/wiki/Probability_distribution",
                "/wiki/Statistics",
                "/wiki/Multivariate_analysis"
            ],
            "links": [
                "/wiki/Hypothesis_testing",
                "/wiki/Likelihood_ratio_test",
                "/wiki/Statistical_power",
                "/wiki/Admissible_decision_rule",
                "/wiki/Bias_of_an_estimator",
                "/wiki/Monotonicity",
                "/wiki/Inverse-Wishart_distribution",
                "/wiki/Bayesian_inference",
                "/wiki/Bayesian_multivariate_linear_regression",
                "/wiki/Hotelling%27s_T-squared_distribution",
                "/wiki/Student%27s_t-distribution",
                "/wiki/Statistical_hypothesis_testing",
                "/wiki/Probability_distribution",
                "/wiki/Univariate_analysis",
                "/wiki/Normal_distribution",
                "/wiki/Simple_linear_regression",
                "/wiki/Multiple_regression",
                "/wiki/Probability_distribution",
                "/wiki/Statistics",
                "/wiki/Multivariate_analysis"
            ],
            "text": "There are an enormous number of software packages and other tools for multivariate analysis, including:Anderson's 1958 textbook, An Introduction to Multivariate Analysis,[4] educated a generation of theorists and applied statisticians; Anderson's book emphasizes hypothesis testing via likelihood ratio tests and the properties of power functions: admissibility, unbiasedness and monotonicity.[5][6]The Inverse-Wishart distribution is important in Bayesian inference, for example in Bayesian multivariate linear regression. Additionally, Hotelling's T-squared distribution is a multivariate distribution, generalising Student's t-distribution, that is used in multivariate hypothesis testing.There is a set of probability distributions used in multivariate analyses that play a similar role to the corresponding set of distributions that are used in univariate analysis when the normal distribution is appropriate to a dataset. These multivariate distributions are:There are many different models, each with its own type of analysis:Certain types of problems involving multivariate data, for example simple linear regression and multiple regression, are not usually considered to be special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.In addition, multivariate statistics is concerned with multivariate probability distributions, in terms of bothMultivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical application of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the problem being studied.Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. The application of multivariate statistics is multivariate analysis.",
            "title": "Multivariate statistics",
            "url": "https://en.wikipedia.org/wiki/Multivariate_statistics"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Chebyshev%27s_inequality#Semivariances",
                "/wiki/Downside_risk",
                "/wiki/Investment#In_finance",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Ronald_Fisher",
                "/wiki/The_Correlation_Between_Relatives_on_the_Supposition_of_Mendelian_Inheritance",
                "/wiki/Bootstrapping_(statistics)",
                "/wiki/Resampling_(statistics)",
                "/wiki/Median",
                "/wiki/F_test",
                "/wiki/Chi_square_test",
                "/wiki/Law_of_large_numbers",
                "/wiki/Consistent_estimator",
                "/wiki/Kurtosis",
                "/wiki/Central_moment",
                "/wiki/Random_variable",
                "/wiki/Normal_distribution",
                "/wiki/Cochran%27s_theorem",
                "/wiki/Chi-squared_distribution",
                "/wiki/U-statistic",
                "/wiki/Bessel%27s_correction",
                "/wiki/Sample_covariance",
                "/wiki/Sample_standard_deviation",
                "/wiki/Concave_function",
                "/wiki/Jensen%27s_inequality",
                "/wiki/Unbiased_estimation_of_standard_deviation",
                "/wiki/Statistical_sample",
                "/wiki/Squared_deviations",
                "/wiki/Sample_(statistics)",
                "/wiki/Statistical_population",
                "/wiki/Mean_squared_error",
                "/wiki/Excess_kurtosis",
                "/wiki/Mean_squared_error#Variance",
                "/wiki/Shrinkage_estimator",
                "/wiki/Biased_estimator",
                "/wiki/Bessel%27s_correction",
                "/wiki/Consistent_estimator",
                "/wiki/Squared_deviations",
                "/wiki/Mean_squared_error",
                "/wiki/Unbiased_estimation_of_standard_deviation",
                "/wiki/Estimation_theory",
                "/wiki/Estimator",
                "/wiki/Sample_(statistics)",
                "/wiki/Observations",
                "/wiki/Population",
                "/wiki/Delta_method",
                "/wiki/Taylor_expansion",
                "/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables",
                "/wiki/Covariance",
                "/wiki/Robust_statistics",
                "/wiki/Outlier",
                "/wiki/Measurement_error",
                "/wiki/Heavy-tailed_distribution",
                "/wiki/Standard_deviation",
                "/wiki/Root_mean_square_deviation",
                "/wiki/Probability_density_function",
                "/wiki/Cumulative_distribution_function",
                "/wiki/Catastrophic_cancellation",
                "/wiki/Algorithms_for_calculating_variance",
                "/wiki/Analysis_of_variance",
                "/wiki/Independence_(probability_theory)",
                "/wiki/Covariance",
                "/wiki/Law_of_large_numbers",
                "/wiki/Spearman%E2%80%93Brown_prediction_formula",
                "/wiki/Standard_error",
                "/wiki/Covariance",
                "/wiki/Cronbach%27s_alpha",
                "/wiki/Classical_test_theory",
                "/wiki/Correlated",
                "/wiki/Covariance",
                "/wiki/Standard_error_(statistics)",
                "/wiki/Central_limit_theorem",
                "/wiki/Ir%C3%A9n%C3%A9e-Jules_Bienaym%C3%A9",
                "/wiki/Statistical_independence",
                "/wiki/Uncorrelated",
                "/wiki/Linear_combination",
                "/wiki/Invariant_(mathematics)",
                "/wiki/Location_parameter",
                "/wiki/Data_set",
                "/wiki/Central_limit_theorem",
                "/wiki/Weighted_variance",
                "/wiki/Floating_point_arithmetic",
                "/wiki/Catastrophic_cancellation",
                "/wiki/Algorithms_for_calculating_variance",
                "/wiki/Discrete_random_variable",
                "/wiki/Continuous_random_variable",
                "/wiki/Cantor_distribution",
                "/wiki/Covariance"
            ],
            "text": "For inequalities associated with the semivariance, see Chebyshev's inequality#Semivariances.The semivariance is calculated in the same manner as the variance but only those observations that fall below the mean are included in the calculation. It is sometimes described as a measure of downside risk in an investments context. For skewed distributions, the semivariance can provide additional information that a variance does not.[citation needed]That is, there is the most variance in the x direction. Physicists would consider this to have a low moment about the x axis so the moment-of-inertia tensor isThis difference between moment of inertia in physics and in statistics is clear for points that are gathered along a line. Suppose many points are close to the x axis and distributed along it. The covariance matrix might look likeThe term variance was first introduced by Ronald Fisher in his 1918 paper The Correlation Between Relatives on the Supposition of Mendelian Inheritance:[20]Resampling methods, which include the bootstrap and the jackknife, may be used to test the equality of variances.The Lehmann test is a parametric test of two variances. Of this test there are several variants known. Other tests of the equality of variances include the Box test, the Box\u2013Anderson test and the Moses test.Several non parametric tests have been proposed: these include the Barton\u2013David\u2013Ansari\u2013Freund\u2013Siegel\u2013Tukey test, the Capon test, Mood test, the Klotz test and the Sukhatme test. The Sukhatme test applies to two variances and requires that both medians be known and equal to zero. The Mood, Klotz, Capon and Barton\u2013David\u2013Ansari\u2013Freund\u2013Siegel\u2013Tukey tests also apply to two variances. They allow the median to be unknown but do require that the two medians are equal.Testing for the equality of two or more variances is difficult. The F test and chi square tests are both adversely affected by non-normality and are not recommended for this purpose.where ymin is the minimum of the sample.[19]This bound has been improved, and it is known that variance is bounded byIt has been shown[18] that for a sample {yi} of real numbers,If the conditions of the law of large numbers hold for the squared observations, s2 is a consistent estimator of\u00a0\u03c32. One can see indeed that the variance of the estimator tends asymptotically to zero. An asymptotically equivalent formula was given in Kenney and Keeping (1951:164), Rose and Smith (2002:264), and Weisstein (n.d.).[14][15][16]where \u03ba is the kurtosis of the distribution and \u03bc4 is the fourth central moment.If the yi are independent and identically distributed, but not necessarily normally distributed, then[12][13]and[11]As a direct consequence, it follows thatBeing a function of random variables, the sample variance is itself a random variable, and it is natural to study its distribution. In the case that yi are independent observations from a normal distribution, Cochran's theorem shows that s2 follows a scaled chi-squared distribution:[10]The unbiased sample variance is a U-statistic for the function \u0192(y1,\u00a0y2) =\u00a0(y1\u00a0\u2212\u00a0y2)2/2, meaning that it is obtained by averaging a 2-sample statistic over 2-element subsets of the population.The use of the term n\u00a0\u2212\u00a01 is called Bessel's correction, and it is also used in sample covariance and the sample standard deviation (the square root of variance). The square root is a concave function and thus introduces negative bias (by Jensen's inequality), which depends on the distribution, and thus the corrected sample standard deviation (using Bessel's correction) is biased. The unbiased estimation of standard deviation is a technically involved problem, though for the normal distribution using the term n\u00a0\u2212\u00a01.5 yields an almost unbiased estimator.Either estimator may be simply referred to as the sample variance when the version can be determined by context. The same proof is also applicable for samples taken from a continuous probability distribution.We take a sample with replacement of n values y1,\u00a0...,\u00a0yn from the population, where n\u00a0<\u00a0N, and estimate the variance on the basis of this sample.[9] Directly taking the variance of the sample data gives the average of the squared deviations:In many practical situations, the true variance of a population is not known a priori and must be computed somehow. When dealing with extremely large populations, it is not possible to count every object in the population, so the computation must be performed on a sample of the population.[8] Sample variance can also be applied to the estimation of the variance of a continuous distribution from a sample of that distribution.The population variance matches the variance of the generating probability distribution. In this sense, the concept of population can be extended to continuous random variables with infinite populations.This is true becauseThe population variance can also be computed usingwhere the population mean isIn general, the population variance of a finite population of size N with values xi is given bySecondly, the sample variance does not generally minimize mean squared error between sample variance and population variance. Correcting for bias often makes this worse: one can always choose a scale factor that performs better than the corrected sample variance, though the optimal scale factor depends on the excess kurtosis of the population (see mean squared error: variance), and introduces bias. This always consists of scaling down the unbiased estimator (dividing by a number larger than n\u00a0\u2212\u00a01), and is a simple example of a shrinkage estimator: one \"shrinks\" the unbiased estimator towards zero. For the normal distribution, dividing by n\u00a0+\u00a01 (instead of n\u00a0\u2212\u00a01 or n) minimizes mean squared error. The resulting estimator is biased, however, and is known as the biased sample variation.Firstly, if the omniscient mean is unknown (and is computed as the sample mean), then the sample variance is a biased estimator: it underestimates the variance by a factor of (n\u00a0\u2212\u00a01) / n; correcting by this factor (dividing by n\u00a0\u2212\u00a01 instead of n) is called Bessel's correction. The resulting estimator is unbiased, and is called the (corrected) sample variance or unbiased sample variance. For example, when n\u00a0=\u00a01 the variance of a single observation about the sample mean (itself) is obviously zero regardless of the population variance. If the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the (independently known) mean.The simplest estimators for population mean and population variance are simply the mean and variance of the sample, the sample mean and (uncorrected) sample variance \u2013 these are consistent estimators (they converge to the correct value as the number of samples increases), but can be improved. Estimating the population variance by taking the sample's variance is close to optimal in general, but can be improved in two ways. Most simply, the sample variance is computed as an average of squared deviations about the (sample) mean, by dividing by n. However, using values other than n improves the estimator in various ways. Four common values for the denominator are n, n\u00a0\u2212\u00a01, n\u00a0+\u00a01, and n\u00a0\u2212\u00a01.5: n is the simplest (population variance of the sample), n\u00a0\u2212\u00a01 eliminates bias, n\u00a0+\u00a01 minimizes mean squared error for the normal distribution, and n\u00a0\u2212\u00a01.5 mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution.Real-world observations such as the measurements of yesterday's rain throughout the day typically cannot be complete sets of all possible observations that could be made. As such, the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations. This means that one estimates the mean and variance that would have been calculated from an omniscient set of observations by using an estimator equation. The estimator is a function of the sample of n observations drawn without observational bias from the whole population of potential observations. In this example that sample would be the set of actual measurements of yesterday's rainfall from available rain gauges within the geography of interest.provided that f is twice differentiable and that the mean and variance of X are finite.The delta method uses second-order Taylor expansions to approximate the variance of a function of one or more random variables: see Taylor expansions for the moments of functions of random variables. For example, the approximate variance of a function of one variable is given byThe standard deviation and the expected absolute deviation can both be used as an indicator of the \"spread\" of a distribution. The standard deviation is more amenable to algebraic manipulation than the expected absolute deviation, and, together with variance and its generalization covariance, is used frequently in theoretical statistics; however the expected absolute deviation tends to be more robust as it is less sensitive to outliers arising from measurement anomalies or an unduly heavy-tailed distribution.Unlike expected absolute deviation, the variance of a variable has units that are the square of the units of the variable itself. For example, a variable measured in meters will have a variance measured in meters squared. For this reason, describing data sets via their standard deviation or root mean square deviation is often preferred over using the variance. In the dice example the standard deviation is \u221a2.9\u00a0\u2248\u00a01.7, slightly larger than the expected absolute deviation of\u00a01.5.This expression can be used to calculate the variance in situations where the CDF, but not the density, can be conveniently expressed.The population variance for a non-negative random variable can be expressed in terms of the cumulative distribution function F usingThis formula is also sometimes used in connection with the sample variance. While useful for hand calculations, it is not advised for computer calculations as it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude and floating point arithmetic is used. This is discussed in the article Algorithms for calculating variance.This will be useful when it is possible to derive formulae for the expected value and for the expected value of the square.A formula often used for deriving the variance of a theoretical distribution is as follows:This can also be derived from the additivity of variances, since the total (observed) score is the sum of the predicted score and the error score, where the latter two are uncorrelated.A similar formula is applied in analysis of variance, where the corresponding formula isIn general, if two variables are statistically dependent, the variance of their product is given by:Equivalently, using the basic properties of expectation, it is given byIf two variables X and Y are independent, the variance of their product is given by[6]The expression above can be extended to a weighted sum of multiple variables:This implies that in a weighted sum of variables, the variable with the largest weight will have a disproportionally large weight in the variance of the total. For example, if X and Y are uncorrelated and the weight of X is two times the weight of Y, then the weight of the variance of X will be four times the weight of the variance of Y.The scaling property and the Bienaym\u00e9 formula, along with the property of the covariance Cov(aX,\u00a0bY) = ab Cov(X,\u00a0Y) jointly imply thatTherefore, the variance of the mean of a large number of standardized variables is approximately equal to their average correlation. This makes clear that the sample mean of correlated variables does not generally converge to the population mean, even though the law of large numbers states that the sample mean will converge for independent variables.This formula is used in the Spearman\u2013Brown prediction formula of classical test theory. This converges to \u03c1 if n goes to infinity, provided that the average correlation remains constant or converges too. So for the variance of the mean of standardized variables with equal correlations or converging average correlation we haveThis implies that the variance of the mean increases with the average of the correlations. In other words, additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean. Moreover, if the variables have unit variance, for example if they are standardized, then this simplifies toSo if the variables have equal variance \u03c32 and the average correlation of distinct variables is \u03c1, then the variance of their mean isHere Cov(\u22c5, \u22c5) is the covariance, which is zero for independent random variables (if it exists). The formula states that the variance of a sum is equal to the sum of all elements in the covariance matrix of the components. The next expression states equivalently that the variance of the sum is the sum of the diagonal of covariance matrix plus two times the sum of its upper triangular elements (or its lower triangular elements); this emphasizes that the covariance matrix is symmetric. This formula is used in the theory of Cronbach's alpha in classical test theory.(Note: The second equality comes from the fact that Cov(Xi,Xi) = Var(Xi).)In general, if the variables are correlated, then the variance of their sum is the sum of their covariances:Using the linearity of the expectation operator and the assumption of independence (or uncorrelatedness) of X and Y, this further simplifies as follows:The general result then follows by induction. Starting with the definition,To prove the initial statement, it suffices to show thatThat is, the variance of the mean decreases when n increases. This formula for the variance of the mean is used in the definition of the standard error of the sample mean, which is used in the central limit theorem.This statement is called the Bienaym\u00e9 formula[2] and was discovered in 1853.[3][4] It is often made with the stronger condition that the variables are independent, but being uncorrelated suffices. So if all the variables have the same variance \u03c32, then, since division by n is a linear transformation, this formula immediately implies that the variance of their mean isOne reason for the use of the variance in preference to other measures of dispersion is that the variance of the sum (or the difference) of uncorrelated random variables is the sum of their variances:These results lead to the variance of a linear combination as:The variance of a sum of two random variables is given byIf all values are scaled by a constant, the variance is scaled by the square of that constant:Variance is invariant with respect to changes in a location parameter. That is, if a constant is added to all values of the variable, the variance is unchanged:The variance of a constant random variable is zero, and if the variance of a variable in a data set is 0, then all the entries have the same value:Variance is non-negative because the squares are positive or zero:The general formula for the variance of the outcome, X, of an n-sided die isThe role of the normal distribution in the central limit theorem is in part responsible for the prevalence of the variance in probability and statistics.(When such a discrete weighted variance is specified by weights whose sum is not\u00a01, then one divides by the sum of the weights.)or equivalentlyA mnemonic for the above expression is \"mean of square minus square of mean\". This equation should not be used for computations using floating point arithmetic because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude. There exist numerically stable alternatives.This definition encompasses random variables that are generated by processes that are discrete, continuous, neither, or mixed. The variance can also be thought of as the covariance of a random variable with itself:",
            "title": "Variance",
            "url": "https://en.wikipedia.org/wiki/Sample_variance"
        },
        {
            "desc_links": [
                "/wiki/Symmetric_matrix",
                "/wiki/Positive_semi-definite_matrix",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Covariance",
                "/wiki/Random_vector",
                "/wiki/Random_variable",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Joint_probability_distribution"
            ],
            "links": [
                "/wiki/Financial_economics",
                "/wiki/Modern_portfolio_theory",
                "/wiki/Mutual_fund_separation_theorem",
                "/wiki/Capital_asset_pricing_model",
                "/wiki/Normative_economics",
                "/wiki/Positive_economics",
                "/wiki/Diversification_(finance)",
                "/wiki/Transformation_matrix",
                "/wiki/Whitening_transformation",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Rayleigh_quotient",
                "/wiki/Principal_component_analysis",
                "/wiki/Karhunen-Lo%C3%A8ve_transform",
                "/wiki/Variance#Generalizations",
                "/wiki/Complex_number",
                "/wiki/Complex_conjugation",
                "/wiki/Multivariate_normal_distribution",
                "/wiki/Elliptical_distribution",
                "/wiki/Probability_density_function",
                "/wiki/Normal_equations",
                "/wiki/Ordinary_least_squares",
                "/wiki/Regression_analysis",
                "/wiki/Schur_complement",
                "/wiki/Conditional_variance",
                "/wiki/Conditional_mean",
                "/wiki/Cross-covariance",
                "/wiki/Off-diagonal_element",
                "/wiki/Variance",
                "/wiki/Expected_value",
                "/wiki/Random_variable",
                "/wiki/Variance",
                "/wiki/Covariance",
                "/wiki/Column_vector",
                "/wiki/Symmetric_matrix",
                "/wiki/Positive_semi-definite_matrix",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Covariance",
                "/wiki/Random_vector",
                "/wiki/Random_variable",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Joint_probability_distribution"
            ],
            "text": "The covariance matrix plays a key role in financial economics, especially in portfolio theory and its mutual fund separation theorem and in the capital asset pricing model. The matrix of covariances among various assets' returns is used to determine, under certain assumptions, the relative amounts of different assets that investors should (in a normative analysis) or are predicted to (in a positive analysis) choose to hold in a context of diversification.The covariance matrix is a useful tool in many different areas. From it a transformation matrix can be derived, called a whitening transformation, that allows one to completely decorrelate the data[citation needed] or, from a different point of view, to find an optimal basis for representing the data in a compact way[citation needed] (see Rayleigh quotient for a formal proof and additional properties of covariance matrices). This is called principal component analysis (PCA) and the Karhunen-Lo\u00e8ve transform (KL-transform).These empirical sample correlation matrices are the most straightforward and most often used estimators for the correlation matrices, but other estimators also exist, including regularised or shrinkage estimators, which may have better properties.or, if the column means were known a-priori,The variance of a complex scalar-valued random variable with expected value \u03bc is conventionally defined using complex conjugation:If a vector of n possibly correlated random variables is jointly normally distributed, or more generally elliptically distributed, then its probability density function can be expressed in terms of the covariance matrix.[5]The matrix of regression coefficients may often be given in transpose form, \u03a3XX\u22121\u03a3XY, suitable for post-multiplying a row vector of explanatory variables xT rather than pre-multiplying a column vector x. In this form they correspond to the coefficients obtained by inverting the matrix of the normal equations of ordinary least squares (OLS).The matrix \u03a3YX\u03a3XX\u22121 is known as the matrix of regression coefficients, while in linear algebra \u03a3Y|X is the Schur complement of \u03a3XX in \u03a3X,Yand conditional variancedefined by conditional meanBy comparison, the notation for the cross-covariance between two vectors isEach element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself, which always equals 1. Each off-diagonal element is between 1 and \u20131 inclusive.This form can be seen as a generalization of the scalar-valued variance to higher dimensions. Recall that for a scalar-valued random variable XThe definition above is equivalent to the matrix equalityis the expected value of the i th entry in the vector X. In other words,where the operator E denotes the expected (mean) value of its argument, andare random variables, each with finite variance, then the covariance matrix \u03a3 is the matrix whose (i,\u00a0j) entry is the covarianceIf the entries in the column vectorThroughout this article, boldfaced unsubscripted X and Y are used to refer to random vectors, and unboldfaced subscripted Xi and Yi are used to refer to random scalars.Because the covariance of the ith random variable with itself is simply that random variable's variance, each element on the principal diagonal of the covariance matrix is the variance of one of the random variables. Because the covariance of the ith random variable with the jth one is the same thing as the covariance of the jth random variable with the ith one, every covariance matrix is symmetric. In addition, every covariance matrix is positive semi-definite.Intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. As an example, the variation in a collection of random points in two-dimensional space cannot be characterized fully by a single number, nor would the variances in the x and y directions contain all of the necessary information; a 2\u00d72 matrix would be necessary to fully characterize the two-dimensional variation.In probability theory and statistics, a covariance matrix (also known as dispersion matrix or variance\u2013covariance matrix) is a matrix whose element in the i, j position is the covariance between the i\u00a0th and j\u00a0th elements of a random vector. A random vector is a random variable with multiple dimensions. Each element of the vector is a scalar random variable. Each element has either a finite number of observed empirical values or a finite or infinite number of potential values. The potential values are specified by a theoretical joint probability distribution.",
            "title": "Covariance matrix",
            "url": "https://en.wikipedia.org/wiki/Covariance_matrix"
        },
        {
            "desc_links": [
                "/wiki/Canonical_correlation",
                "/wiki/Orthogonal_coordinate_system",
                "/wiki/Factor_analysis",
                "/wiki/Eigenvectors",
                "/wiki/Dimension_(metadata)",
                "/wiki/Exploratory_data_analysis",
                "/wiki/Predictive_modeling",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Covariance",
                "/wiki/Correlation",
                "/wiki/Singular_value_decomposition",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Z-score",
                "/wiki/Karl_Pearson",
                "/wiki/Principal_axis_theorem",
                "/wiki/Harold_Hotelling",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Signal_processing",
                "/wiki/Harold_Hotelling",
                "/wiki/Proper_orthogonal_decomposition",
                "/wiki/Singular_value_decomposition",
                "/wiki/Eigendecomposition",
                "/wiki/Factor_analysis",
                "/wiki/Eckart%E2%80%93Young_theorem",
                "/wiki/Psychometrics",
                "/wiki/Empirical_orthogonal_functions",
                "/wiki/Spectral_theorem",
                "/wiki/Mode_shape",
                "/wiki/Orthogonal_transformation",
                "/wiki/Correlation_and_dependence",
                "/wiki/Variance",
                "/wiki/Orthogonal",
                "/wiki/Orthogonal_basis_set"
            ],
            "links": [
                "/wiki/Independent_component_analysis",
                "/wiki/Sparse_PCA",
                "/wiki/Robust_principal_component_analysis",
                "/wiki/Outlier",
                "/wiki/Data_mining",
                "/wiki/Correlation_clustering",
                "/wiki/Tucker_decomposition",
                "/wiki/PARAFAC",
                "/wiki/Multilinear_subspace_learning",
                "/wiki/Multilinear_principal_component_analysis",
                "/wiki/Nonlinear_dimensionality_reduction",
                "/wiki/Curve",
                "/wiki/Manifold",
                "/wiki/Approximation",
                "/wiki/Projection_(mathematics)",
                "/wiki/Elastic_map",
                "/wiki/Principal_geodesic_analysis",
                "/wiki/Kernel_PCA",
                "/wiki/Non-negative_matrix_factorization",
                "/wiki/K-means_clustering",
                "/wiki/Factor_analysis",
                "/wiki/Correspondence_analysis",
                "/wiki/Jean-Paul_Benz%C3%A9cri",
                "/wiki/Contingency_tables",
                "/wiki/Chi-squared_statistic",
                "/wiki/Detrended_correspondence_analysis",
                "/wiki/Canonical_correspondence_analysis",
                "/wiki/Multiple_correspondence_analysis",
                "/wiki/Order_parameters",
                "/wiki/Phase_transitions",
                "/wiki/Spike_sorting",
                "/wiki/Electrophysiology#Extracellular_recording",
                "/wiki/Cluster_analysis",
                "/wiki/Neuroscience",
                "/wiki/Neuron",
                "/wiki/Action_potential",
                "/wiki/Spike-triggered_covariance",
                "/wiki/White_noise",
                "/wiki/Electric_current",
                "/wiki/Covariance_matrix",
                "/wiki/Eigenvectors_and_eigenvalues",
                "/wiki/Vector_space",
                "/wiki/Stock",
                "/wiki/Asset_allocation",
                "/wiki/Stock_selection_criterion",
                "/wiki/Quantitative_finance",
                "/wiki/Risk_management",
                "/wiki/Interest_rate_derivative",
                "/wiki/Swap_(finance)",
                "/wiki/Ludovic_Lebart",
                "/wiki/Algorithm",
                "/wiki/Partial_least_squares",
                "/wiki/Genomics",
                "/wiki/Metabolomics",
                "/wiki/Non-linear_iterative_partial_least_squares",
                "/wiki/Gram%E2%80%93Schmidt",
                "/wiki/Lanczos_algorithm",
                "/wiki/Power_iteration",
                "/wiki/High_dimensional_data",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Non-negative_matrix_factorization",
                "/wiki/Principal_component_analysis#Relation_between_PCA_and_Non-negative_Matrix_Factorization",
                "/wiki/Diagonal",
                "/wiki/Regression_analysis",
                "/wiki/Pattern_recognition",
                "/wiki/Linear_discriminant_analysis",
                "/wiki/Autoencoder",
                "/wiki/Artificial_neural_network",
                "/wiki/Orthogonal",
                "/wiki/Minimum_mean_square_error",
                "/wiki/Sample_variance",
                "/wiki/Euclidean_space",
                "/wiki/Eigenvalue",
                "/wiki/Principle_Component_Analysis#PCA_and_information_theory",
                "/wiki/Dimensionality_reduction",
                "/wiki/Discrete_cosine_transform",
                "/wiki/Nonlinear_dimensionality_reduction",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Frobenius_norm",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Polar_decomposition",
                "/wiki/Diagonal_matrix",
                "/wiki/Singular_value_decomposition",
                "/wiki/Signal-to-noise_ratio",
                "/wiki/Regression_analysis",
                "/wiki/Explanatory_variable",
                "/wiki/Overfitting",
                "/wiki/Principal_component_regression",
                "/wiki/Dimensionality_reduction",
                "/wiki/Cluster_analysis",
                "/wiki/Covariance_matrix",
                "/wiki/Whitening_transformation",
                "/wiki/Rayleigh_quotient",
                "/wiki/Positive_semidefinite_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Eigenvector",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Empirical_mean",
                "/wiki/Orthogonal_transformation",
                "/wiki/Linear_transformation",
                "/wiki/Coordinate_system",
                "/wiki/Covariance_matrix",
                "/wiki/Ellipsoid",
                "/wiki/Canonical_correlation",
                "/wiki/Orthogonal_coordinate_system",
                "/wiki/Factor_analysis",
                "/wiki/Eigenvectors",
                "/wiki/Dimension_(metadata)",
                "/wiki/Exploratory_data_analysis",
                "/wiki/Predictive_modeling",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Covariance",
                "/wiki/Correlation",
                "/wiki/Singular_value_decomposition",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Z-score",
                "/wiki/Karl_Pearson",
                "/wiki/Principal_axis_theorem",
                "/wiki/Harold_Hotelling",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Signal_processing",
                "/wiki/Harold_Hotelling",
                "/wiki/Proper_orthogonal_decomposition",
                "/wiki/Singular_value_decomposition",
                "/wiki/Eigendecomposition",
                "/wiki/Factor_analysis",
                "/wiki/Eckart%E2%80%93Young_theorem",
                "/wiki/Psychometrics",
                "/wiki/Empirical_orthogonal_functions",
                "/wiki/Spectral_theorem",
                "/wiki/Mode_shape"
            ],
            "text": "then the decomposition is unique up to multiplication by a scalar.[60]Independent component analysis (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.A particular disadvantage of PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables.Robust principal component analysis (RPCA) via decomposition in low-rank and sparse matrices is a modification of the widely used statistical procedure principal component analysis (PCA) which works well with respect to grossly corrupted observations.[56][57][58][59]While PCA finds the mathematically optimal method (as in minimizing the squared error), it is sensitive to outliers in the data that produce large errors PCA tries to avoid. It therefore is common practice to remove outliers before computing PCA. However, in some contexts, outliers can be difficult to identify. For example, in data mining algorithms like correlation clustering, the assignment of points to clusters and outliers is not known beforehand. A recently proposed generalization of PCA[55] based on a weighted PCA increases robustness by assigning different weights to data objects based on their estimated relevancy.N-way principal component analysis may be performed with models such as Tucker decomposition, PARAFAC, multiple factor analysis, co-inertia analysis, STATIS, and DISTATIS.In multilinear subspace learning,[54] PCA is generalized to multilinear PCA (MPCA) that extracts features directly from tensor representations. MPCA is solved by performing PCA in each mode of the tensor iteratively. MPCA has been applied to face recognition, gait recognition, etc. MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.Most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in PCA or K-means. Pearson's original idea was to take a straight line (or plane) which will be \"the best fit\" to a set of data points. Principal curves and manifolds[53] give the natural geometric framework for PCA generalization and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold for data approximation, and by encoding using standard geometric projection onto the manifold, as it is illustrated by Fig. See also the elastic map algorithm and principal geodesic analysis. Another popular generalization is kernel PCA, which corresponds to PCA performed in a reproducing kernel Hilbert space associated with a positive definite kernel.Non-negative matrix factorization (NMF) is a dimension reduction method where only non-negative elements in the matrices are used, which is therefore a promising method in astronomy,[17][18][19] in the sense that astrophysical signals are non-negative. The PCA components are orthogonal to each other, while the NMF components are all non-negative and therefore constructs a non-orthogonal basis.It was asserted in [46][47] that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by the principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. However, that PCA is a useful relaxation of k-means clustering was not a new result (see, for example,[48]), and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.[49]Factor analysis is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Different from PCA, factor analysis is a correlation-focused approach seeking to reproduce the inter-correlations among variables, in which the factors \"represent the common variance of variables, excluding unique variance\".[44] In terms of the correlation matrix, this corresponds with focusing on explaining the off-diagonal terms (i.e. shared co-variance), while PCA focuses on explaining the terms that sit on the diagonal. However, as a side result, when trying to reproduce the on-diagonal terms, PCA also tends to fit relatively well the off-diagonal correlations.[45] Results given by PCA and factor analysis are very similar in most situations, but this is not always the case, and there are some problems where the results are significantly different. Factor analysis is generally used when the research purpose is detecting data structure (i.e., latent constructs or factors) or causal modeling.Principal component analysis creates variables that are linear combinations of the original variables. The new variables have the property that the variables are all orthogonal. The PCA transformation can be helpful as a pre-processing step before clustering. PCA is a variance-focused approach seeking to reproduce the total variable variance, in which components reflect both common and unique variance of the variable. PCA is generally preferred for purposes of data reduction (i.e., translating variable space into optimal factor space) but not when the goal is to detect the latent construct or factors.Correspondence analysis (CA) was developed by Jean-Paul Benz\u00e9cri[41] and is conceptually similar to PCA, but scales the data (which should be non-negative) so that rows and columns are treated equivalently. It is traditionally applied to contingency tables. CA decomposes the chi-squared statistic associated to this table into orthogonal factors.[42] Because CA is a descriptive technique, it can be applied to tables for which the chi-squared statistic is appropriate or not. Several variants of CA are available including detrended correspondence analysis and canonical correspondence analysis. One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data.[43]PCA as a dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles. It has been used in determining collective variables, i.e. order parameters, during phase transitions in the brain.[40]In neuroscience, PCA is also used to discern the identity of a neuron from the shape of its action potential. Spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron. In spike sorting, one first uses PCA to reduce the dimensionality of the space of action potential waveforms, and then performs clustering analysis to associate specific action potentials with individual neurons.A variant of principal components analysis is used in neuroscience to identify the specific properties of a stimulus that increase a neuron's probability of generating an action potential.[39] This technique is known as spike-triggered covariance analysis. In a typical application an experimenter presents a white noise process as a stimulus (usually either as a sensory input to a test subject, or as a current injected directly into the neuron) and records a train of action potentials, or spikes, produced by the neuron as a result. Presumably, certain features of the stimulus make the neuron more likely to spike. In order to extract these features, the experimenter calculates the covariance matrix of the spike-triggered ensemble, the set of all stimuli (defined and discretized over a finite time window, typically on the order of 100 ms) that immediately preceded a spike. The eigenvectors of the difference between the spike-triggered covariance matrix and the covariance matrix of the prior stimulus ensemble (the set of all stimuli, defined over the same length time window) then indicate the directions in the space of stimuli along which the variance of the spike-triggered ensemble differed the most from that of the prior stimulus ensemble. Specifically, the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike-triggered ensemble showed the largest positive change compared to the variance of the prior. Since these were the directions in which varying the stimulus led to a spike, they are often good approximations of the sought after relevant stimulus features.PCA has also been applied to share portfolios in a similar fashion.[36] One application is to reduce portfolio risk, where allocation strategies are applied to the \"principal portfolios\" instead of the underlying stocks.[37] A second is to enhance portfolio return, using the principal components to select stocks with upside potential.[38]In quantitative finance, principal component analysis can be directly applied to the risk management of interest rate derivatives portfolios.[35] Trading multiple swap instruments which are usually a function of 30-500 other market quotable swap instruments is sought to be reduced to usually 3 or 4 principal components, representing the path of interest rates on a macro basis. Converting risks to be represented as those to factor loadings (or multipliers) provides assessments and understanding beyond that available to simply collectively viewing risks to individual 30-500 buckets.These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, L\u00ea & Pag\u00e8s 2009 and Pag\u00e8s 2013. Few software offer this option in an \"automatic\" way. This is the case of SPAD that historically, following the work of Ludovic Lebart, was the first to propose this option, and the R package FactoMineR.In PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable species. For this, the following results are produced.In an \"online\" or \"streaming\" situation with data arriving piece by piece rather than being stored in a single batch, it is useful to make an estimate of the PCA projection that can be updated sequentially. This can be done efficiently, but requires different algorithms.[34]However, for large data matrices, or matrices that have a high degree of column collinearity, NIPALS suffers from loss of orthogonality due to machine precision limitations accumulated in each iteration step.[32] A Gram\u2013Schmidt (GS) re-orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality.[33]Non-linear iterative partial least squares (NIPALS) is an algorithm for computing the first few components in a principal component or partial least squares analysis. For very-high-dimensional datasets, such as those generated in the *omics sciences (e.g., genomics, metabolomics) it is usually only necessary to compute the first few PCs. The non-linear iterative partial least squares (NIPALS) algorithm calculates t1 and w1T from X. The outer product, t1w1T can then be subtracted from X leaving the residual matrix E1. This can be then used to calculate subsequent PCs.[31] This results in a dramatic reduction in computational time since calculation of the covariance matrix is avoided.One way to compute the eigenvalue that corresponds with each principal component is to measure the difference in mean-squared-distance between the rows and the centroid, before and after subtracting out the principal component. The eigenvalue that corresponds with the component that was removed is equal to this difference.Subsequent principal components can be computed by subtracting component r from X (see Gram\u2013Schmidt) and then repeating this algorithm to find the next principal component. However this simple approach is not numerically stable if more than a small number of principal components are required, because imprecisions in the calculations will additively affect the estimates of subsequent principal components. More advanced methods build on this basic idea, as with the closely related Lanczos algorithm.This algorithm is simply an efficient way of calculating XTX r, normalizing, and placing the result back in r (power iteration). It avoids the np2 operations of calculating the covariance matrix. r will typically get close to the first principal component of X within a small number of iterations, c. (The magnitude of s will be larger after each iteration. Convergence can be detected when it increases by an amount too small for the precision of the machine.)In practical implementations especially with high dimensional data (large p), the covariance method is rarely used because it is not efficient. One way to compute the first principal component efficiently[30] is shown in the following pseudo-code, for a data matrix X with zero mean, without ever computing its covariance matrix.This is very constructive, as cov(X) is guaranteed to be a non-negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix.Let X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data.[25] Hence we proceed by centering the data as follows:The goal is to transform a given data set X of dimension p to an alternative data set Y of smaller dimension L. Equivalently, we are seeking to find the matrix Y, where Y is the Karhunen\u2013Lo\u00e8ve transform (KLT) of matrix X:The following is a detailed description of PCA using the covariance method (see also here) as opposed to the correlation method.[24]Under the assumption thatDimensionality reduction loses information, in general. PCA-based dimensionality reduction tends to minimize that information loss, under certain signal and noise models.The other limitation is the mean-removal process before constructing the covariance matrix for PCA. In fields such as astronomy, all the signals are non-negative, and the mean-removal process will force the mean of some astrophysical exposures to be zero, which consequently creates unphysical negative fluxes,[15] and forward modeling has to be performed to recover the true magnitude of the signals.[16] As an alternative method, non-negative matrix factorization focusing only on the non-negative elements in the matrices, which is well-suited for astrophysical observations.[17][18][19] See more at Relation between PCA and Non-negative Matrix Factorization.The applicability of PCA is limited by certain assumptions[14] made in its derivation.As noted above, the results of PCA depend on the scaling of the variables. A scale-invariant form of PCA has been developed.[13]Before we look at its usage, we first look at diagonal elements,The statistical implication of this property is that the last few PCs are not simply unstructured left-overs after removing the important PCs. Because these last PCs have variances as small as possible they are useful in their own right. They can help to detect unsuspected near-constant linear relationships between the elements of x, and they may also be useful in regression, in selecting a subset of variables from x, and in outlier detection.Some properties of PCA include:[12]PCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability.[10] However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes.[11] The linear discriminant analysis is an alternative which is optimized for class separability.An autoencoder neural network with a linear hidden layer is similar to PCA. Upon convergence, the weight vectors of the K neurons in the hidden layer will form a basis for the space spanned by the first K principal components. Unlike PCA, this technique will not necessarily produce orthogonal vectors.Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation). Also see the article by Kromrey & Foster-Johnson (1998) on \"Mean-centering in Moderated Regression: Much Ado About Nothing\".Mean subtraction (a.k.a. \"mean centering\") is necessary for performing PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.[9]PCA is sensitive to the scaling of the variables. If we have just two variables and they have the same sample variance and are positively correlated, then the PCA will entail a rotation by 45\u00b0 and the \"loadings\" for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Note that Pearson's original paper was entitled \"On Lines and Planes of Closest Fit to Systems of Points in Space\" \u2013 \"in space\" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.Given a set of points in Euclidean space, the first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. The second principal component corresponds to the same concept after all correlation with the first principal component has been subtracted from the points. The singular values (in \u03a3) are the square roots of the eigenvalues of the matrix XTX. Each eigenvalue is proportional to the portion of the \"variance\" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see below). PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest \"variance\" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example and when applicable, to the discrete cosine transform, and in particular to the DCT-II which is simply known as the \"DCT\". Nonlinear dimensionality reduction techniques tend to be more computationally demanding than PCA.The truncation of a matrix M or T using a truncated singular value decomposition in this way produces a truncated matrix that is the nearest possible matrix of rank L to the original matrix, in the sense of the difference between the two having the smallest possible Frobenius norm, a result known as the Eckart\u2013Young theorem [1936].As with the eigen-decomposition, a truncated n \u00d7 L score matrix TL can be obtained by considering only the first L largest singular values and their singular vectors:Efficient algorithms exist to calculate the SVD of X without having to form the matrix XTX, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix[citation needed], unless only a handful of components are required.so each column of T is given by one of the left singular vectors of X multiplied by the corresponding singular value. This form is also the polar decomposition of T.Using the singular value decomposition the score matrix T can be writtenIn terms of this factorization, the matrix XTX can be writtenHere \u03a3 is an n-by-p rectangular diagonal matrix of positive numbers \u03c3(k), called the singular values of X; U is an n-by-n matrix, the columns of which are orthogonal unit vectors of length n called the left singular vectors of X; and W is a p-by-p whose columns are orthogonal unit vectors of length p and called the right singular vectors of X.The principal components transformation can also be associated with another matrix factorization, the singular value decomposition (SVD) of X,Dimensionality reduction may also be appropriate when the variables in a dataset are noisy. If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of T will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix W, which can be thought of as a high-dimensional rotation of the co-ordinate axes). However, with more of the total variance concentrated in the first few principal components compared to the same noise variance, the proportionate effect of the noise is less\u2014the first few components achieve a higher signal-to-noise ratio. PCA thus can have the effect of concentrating much of the signal into the first few principal components, which can usefully be captured by dimensionality reduction; while the later principal components may be dominated by noise, and so disposed of without great loss.Similarly, in regression analysis, the larger the number of explanatory variables allowed, the greater is the chance of overfitting the model, producing conclusions that fail to generalise to other datasets. One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called principal component regression.Such dimensionality reduction can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible. For example, selecting L\u00a0=\u00a02 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains clusters these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable.The transformation T = X W maps a data vector x(i) from an original space of p variables to a new space of p variables which are uncorrelated over the dataset. However, not all the principal components need to be kept. Keeping only the first L principal components, produced by using only the first L loading vectors, gives the truncated transformation(\u03bb(k) being equal to the sum of the squares over the dataset associated with each component k: \u03bb(k) = \u03a3i tk2(i) = \u03a3i (x(i) \u22c5 w(k))2)where \u039b is the diagonal matrix of eigenvalues \u03bb(k) of XTXThe empirical covariance matrix between the principal components becomesIn matrix form, the empirical covariance matrix for the original variables can be writtenAnother way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.where the eigenvalue property of w(k) has been used to move from line 2 to line 3. However eigenvectors w(j) and w(k) corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value). The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset.The sample covariance Q between two of the different principal components over the dataset is given by:XTX itself can be recognised as proportional to the empirical sample covariance matrix of the dataset X.where W is a p-by-p matrix whose columns are the eigenvectors of XTX. The transpose of W is sometimes called the whitening or sphering transformation.The full principal components decomposition of X can therefore be given asThe kth principal component of a data vector x(i) can therefore be given as a score tk(i) = x(i) \u22c5 w(k) in the transformed co-ordinates, or as the corresponding vector in the space of the original variables, {x(i) \u22c5 w(k)} w(k), where w(k) is the kth eigenvector of XTX.It turns out that this gives the remaining eigenvectors of XTX, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the loading vectors are eigenvectors of XTX.and then finding the loading vector which extracts the maximum variance from this new data matrixThe kth component can be found by subtracting the first k\u00a0\u2212\u00a01 principal components from X:With w(1) found, the first principal component of a data vector x(i) can then be given as a score t1(i) = x(i) \u22c5 w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables, {x(i) \u22c5 w(1)} w(1).The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector.Since w(1) has been defined to be a unit vector, it equivalently also satisfiesEquivalently, writing this in matrix form givesIn order to maximize variance, the first loading vector w(1) thus has to satisfyConsider a data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor).PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.[3]This procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data, and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis . The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.PCA can be thought of as fitting an n-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.[6][7]PCA is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.PCA is mostly used as a tool in exploratory data analysis and for making predictive models. It's often used to visualize genetic distance and relatedness between populations. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute.[4] The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score).[5]PCA was invented in 1901 by Karl Pearson,[1] as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s.[2] Depending on the field of application, it is also named the discrete Karhunen\u2013Lo\u00e8ve transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch.\u00a07 of Jolliffe's Principal Component Analysis[3]), Eckart\u2013Young theorem (Harman, 1960), or Schmidt\u2013Mirsky theorem in psychometrics, empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.",
            "title": "Principal component analysis",
            "url": "https://en.wikipedia.org/wiki/Principal_components_analysis"
        },
        {
            "desc_links": [
                "/wiki/Abstract_algebra",
                "/wiki/Module_homomorphism",
                "/wiki/Category_theory",
                "/wiki/Morphism",
                "/wiki/Category_of_modules",
                "/wiki/Ring_(mathematics)",
                "/wiki/Map_(mathematics)",
                "/wiki/Origin_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Straight_line",
                "/wiki/Point_(geometry)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rotation_and_reflection_linear_transformations",
                "/wiki/Endomorphism",
                "/wiki/Linear_function",
                "/wiki/Analytic_geometry",
                "/wiki/Mathematics",
                "/wiki/Transformation_(function)",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Scalar_(mathematics)"
            ],
            "links": [
                "/wiki/Compiler_optimizations",
                "/wiki/Parallelizing_compiler",
                "/wiki/Computer_graphics",
                "/wiki/Transformation_matrix",
                "/wiki/Topological_vector_space",
                "/wiki/Normed_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Continuous_linear_operator",
                "/wiki/Bounded_operator",
                "/wiki/Discontinuous_linear_operator",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Tensor",
                "/wiki/Endomorphism",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Euler_characteristic",
                "/wiki/Operator_theory",
                "/wiki/Fredholm",
                "/wiki/Atiyah%E2%80%93Singer_index_theorem",
                "/wiki/Quotient_space_(linear_algebra)",
                "/wiki/Exact_sequence",
                "/wiki/Rank_of_a_matrix",
                "/wiki/Kernel_(matrix)#Subspace_properties",
                "/wiki/Linear_subspace",
                "/wiki/Dimension",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Image_(mathematics)",
                "/wiki/Range_(mathematics)",
                "/wiki/Isomorphism",
                "/wiki/Associative_algebra",
                "/wiki/Group_isomorphism",
                "/wiki/General_linear_group",
                "/wiki/Isomorphism",
                "/wiki/Automorphism",
                "/wiki/Group_(math)",
                "/wiki/Automorphism_group",
                "/wiki/Endomorphisms",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Endomorphism",
                "/wiki/Associative_algebra",
                "/wiki/Ring_(algebra)",
                "/wiki/Identity_function",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_addition",
                "/wiki/Associative_algebra",
                "/wiki/Composition_of_maps",
                "/wiki/Pointwise",
                "/wiki/Inverse_function",
                "/wiki/Relation_composition",
                "/wiki/Class_(set_theory)",
                "/wiki/Morphism",
                "/wiki/Category_(mathematics)",
                "/wiki/Dimension",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Finite-dimensional",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Euclidean_space",
                "/wiki/Abstract_algebra",
                "/wiki/Module_homomorphism",
                "/wiki/Category_theory",
                "/wiki/Morphism",
                "/wiki/Category_of_modules",
                "/wiki/Ring_(mathematics)",
                "/wiki/Map_(mathematics)",
                "/wiki/Origin_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Straight_line",
                "/wiki/Point_(geometry)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rotation_and_reflection_linear_transformations",
                "/wiki/Endomorphism",
                "/wiki/Linear_function",
                "/wiki/Analytic_geometry",
                "/wiki/Mathematics",
                "/wiki/Transformation_(function)",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Scalar_(mathematics)"
            ],
            "text": "Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A\u2032 = B\u22121AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V \u2192 W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 \u2192 V \u2192 W \u2192 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah\u2013Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) \u2212 dim(W), by rank\u2013nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, \u03c1(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or \u03bd(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank\u2013nullity theorem:If f\u00a0: V \u2192 W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n \u00d7 n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n \u00d7 n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V \u2192 V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V \u2192 V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f\u00a0: V \u2192 W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1\u00a0: V \u2192 W and f2\u00a0: V \u2192 W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f\u00a0: V \u2192 W and g\u00a0: W \u2192 Z are linear, then so is their composition g \u2218 f\u00a0: V \u2192 Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 \u00d7 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol \u2217 denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m \u00d7 n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f\u00a0: V \u2192 W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m \u00d7 n matrix, then f(x) = Ax describes a linear map Rn \u2192 Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of\u00a0V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V \u2192 W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.",
            "title": "Linear map",
            "url": "https://en.wikipedia.org/wiki/Linear_relation"
        },
        {
            "desc_links": [
                "/wiki/Symmetric_matrix",
                "/wiki/Positive_semi-definite_matrix",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Covariance",
                "/wiki/Random_vector",
                "/wiki/Random_variable",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Joint_probability_distribution"
            ],
            "links": [
                "/wiki/Financial_economics",
                "/wiki/Modern_portfolio_theory",
                "/wiki/Mutual_fund_separation_theorem",
                "/wiki/Capital_asset_pricing_model",
                "/wiki/Normative_economics",
                "/wiki/Positive_economics",
                "/wiki/Diversification_(finance)",
                "/wiki/Transformation_matrix",
                "/wiki/Whitening_transformation",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Rayleigh_quotient",
                "/wiki/Principal_component_analysis",
                "/wiki/Karhunen-Lo%C3%A8ve_transform",
                "/wiki/Variance#Generalizations",
                "/wiki/Complex_number",
                "/wiki/Complex_conjugation",
                "/wiki/Multivariate_normal_distribution",
                "/wiki/Elliptical_distribution",
                "/wiki/Probability_density_function",
                "/wiki/Normal_equations",
                "/wiki/Ordinary_least_squares",
                "/wiki/Regression_analysis",
                "/wiki/Schur_complement",
                "/wiki/Conditional_variance",
                "/wiki/Conditional_mean",
                "/wiki/Cross-covariance",
                "/wiki/Off-diagonal_element",
                "/wiki/Variance",
                "/wiki/Expected_value",
                "/wiki/Random_variable",
                "/wiki/Variance",
                "/wiki/Covariance",
                "/wiki/Column_vector",
                "/wiki/Symmetric_matrix",
                "/wiki/Positive_semi-definite_matrix",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Covariance",
                "/wiki/Random_vector",
                "/wiki/Random_variable",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Joint_probability_distribution"
            ],
            "text": "The covariance matrix plays a key role in financial economics, especially in portfolio theory and its mutual fund separation theorem and in the capital asset pricing model. The matrix of covariances among various assets' returns is used to determine, under certain assumptions, the relative amounts of different assets that investors should (in a normative analysis) or are predicted to (in a positive analysis) choose to hold in a context of diversification.The covariance matrix is a useful tool in many different areas. From it a transformation matrix can be derived, called a whitening transformation, that allows one to completely decorrelate the data[citation needed] or, from a different point of view, to find an optimal basis for representing the data in a compact way[citation needed] (see Rayleigh quotient for a formal proof and additional properties of covariance matrices). This is called principal component analysis (PCA) and the Karhunen-Lo\u00e8ve transform (KL-transform).These empirical sample correlation matrices are the most straightforward and most often used estimators for the correlation matrices, but other estimators also exist, including regularised or shrinkage estimators, which may have better properties.or, if the column means were known a-priori,The variance of a complex scalar-valued random variable with expected value \u03bc is conventionally defined using complex conjugation:If a vector of n possibly correlated random variables is jointly normally distributed, or more generally elliptically distributed, then its probability density function can be expressed in terms of the covariance matrix.[5]The matrix of regression coefficients may often be given in transpose form, \u03a3XX\u22121\u03a3XY, suitable for post-multiplying a row vector of explanatory variables xT rather than pre-multiplying a column vector x. In this form they correspond to the coefficients obtained by inverting the matrix of the normal equations of ordinary least squares (OLS).The matrix \u03a3YX\u03a3XX\u22121 is known as the matrix of regression coefficients, while in linear algebra \u03a3Y|X is the Schur complement of \u03a3XX in \u03a3X,Yand conditional variancedefined by conditional meanBy comparison, the notation for the cross-covariance between two vectors isEach element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself, which always equals 1. Each off-diagonal element is between 1 and \u20131 inclusive.This form can be seen as a generalization of the scalar-valued variance to higher dimensions. Recall that for a scalar-valued random variable XThe definition above is equivalent to the matrix equalityis the expected value of the i th entry in the vector X. In other words,where the operator E denotes the expected (mean) value of its argument, andare random variables, each with finite variance, then the covariance matrix \u03a3 is the matrix whose (i,\u00a0j) entry is the covarianceIf the entries in the column vectorThroughout this article, boldfaced unsubscripted X and Y are used to refer to random vectors, and unboldfaced subscripted Xi and Yi are used to refer to random scalars.Because the covariance of the ith random variable with itself is simply that random variable's variance, each element on the principal diagonal of the covariance matrix is the variance of one of the random variables. Because the covariance of the ith random variable with the jth one is the same thing as the covariance of the jth random variable with the ith one, every covariance matrix is symmetric. In addition, every covariance matrix is positive semi-definite.Intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. As an example, the variation in a collection of random points in two-dimensional space cannot be characterized fully by a single number, nor would the variances in the x and y directions contain all of the necessary information; a 2\u00d72 matrix would be necessary to fully characterize the two-dimensional variation.In probability theory and statistics, a covariance matrix (also known as dispersion matrix or variance\u2013covariance matrix) is a matrix whose element in the i, j position is the covariance between the i\u00a0th and j\u00a0th elements of a random vector. A random vector is a random variable with multiple dimensions. Each element of the vector is a scalar random variable. Each element has either a finite number of observed empirical values or a finite or infinite number of potential values. The potential values are specified by a theoretical joint probability distribution.",
            "title": "Covariance matrix",
            "url": "https://en.wikipedia.org/wiki/Covariance_matrix"
        },
        {
            "desc_links": [
                "/wiki/Independence_(probability_theory)",
                "/wiki/Conditional_expectation",
                "/wiki/Correlation_coefficient",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Robust_statistics",
                "/wiki/Mutual_information",
                "/wiki/Causality",
                "/wiki/Correlation_does_not_imply_causation",
                "/wiki/Statistics",
                "/wiki/Causality",
                "/wiki/Random_variable",
                "/wiki/Bivariate_data",
                "/wiki/Line_(geometry)",
                "/wiki/Human_height",
                "/wiki/Demand_curve"
            ],
            "links": [
                "/wiki/Bivariate_normal_distribution",
                "/wiki/Marginal_distribution",
                "/wiki/Normal_distribution",
                "/wiki/Scatter_plot",
                "/wiki/Anscombe%27s_quartet",
                "/wiki/Francis_Anscombe",
                "/wiki/Outlier",
                "/wiki/Conditional_expectation",
                "/wiki/Correlation_does_not_imply_causation",
                "/wiki/Identity_(mathematics)",
                "/wiki/Tautology_(logic)",
                "/wiki/Coefficient_of_multiple_determination#Computation",
                "/wiki/Multiple_regression",
                "/wiki/Scaled_correlation",
                "/wiki/Moment_(mathematics)",
                "/wiki/Quantile",
                "/wiki/Bias_of_an_estimator",
                "/wiki/Consistent_estimator",
                "/wiki/Monotone_function",
                "/wiki/Coefficient_of_determination",
                "/wiki/Simple_linear_regression",
                "/wiki/Copula_(statistics)",
                "/wiki/Polychoric_correlation",
                "/wiki/Correlation_ratio",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Entropy_(information_theory)",
                "/wiki/Mutual_information",
                "/wiki/Total_correlation",
                "/wiki/Dual_total_correlation",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Copula_(probability_theory)",
                "/wiki/Distance_correlation",
                "/wiki/Multivariate_normal_distribution",
                "/wiki/Elliptical_distribution",
                "/wiki/Multivariate_t-distribution",
                "/wiki/Rank_correlation",
                "/wiki/Spearman%27s_rank_correlation_coefficient",
                "/wiki/Kendall%27s_tau",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Coefficient_of_determination",
                "/wiki/Standard_deviation#Uncorrected_sample_standard_deviation",
                "/wiki/Arithmetic_mean",
                "/wiki/Standard_deviation#Corrected_sample_standard_deviation",
                "/wiki/Statistical_independence",
                "/wiki/Uncorrelated",
                "/wiki/Bivariate_Gaussian_distribution",
                "/wiki/Open_interval",
                "/wiki/Linear_dependence",
                "/wiki/Cauchy%E2%80%93Schwarz_inequality",
                "/wiki/Absolute_value",
                "/wiki/Expected_value",
                "/wiki/Covariance",
                "/wiki/Random_variables",
                "/wiki/Expected_value",
                "/wiki/Standard_deviation",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Covariance",
                "/wiki/Standard_deviation",
                "/wiki/Karl_Pearson",
                "/wiki/Francis_Galton",
                "/wiki/Independence_(probability_theory)",
                "/wiki/Conditional_expectation",
                "/wiki/Correlation_coefficient",
                "/wiki/Pearson_product-moment_correlation_coefficient",
                "/wiki/Robust_statistics",
                "/wiki/Mutual_information",
                "/wiki/Causality",
                "/wiki/Correlation_does_not_imply_causation",
                "/wiki/Statistics",
                "/wiki/Causality",
                "/wiki/Random_variable",
                "/wiki/Bivariate_data",
                "/wiki/Line_(geometry)",
                "/wiki/Human_height",
                "/wiki/Demand_curve"
            ],
            "text": "If a pair (X,\u00a0Y) of random variables follows a bivariate normal distribution, the conditional mean E(X|Y) is a linear function of Y, and the conditional mean E(Y|X) is a linear function of X. The correlation coefficient r between X and Y, along with the marginal means and variances of X and Y, determines this linear relationship:These examples indicate that the correlation coefficient, as a summary statistic, cannot replace visual examination of the data. Note that the examples are sometimes said to demonstrate that the Pearson correlation assumes that the data follow a normal distribution, but this is not correct.[4]The image on the right shows scatter plots of Anscombe's quartet, a set of four different pairs of variables created by Francis Anscombe.[17] The four y variables have the same mean (7.5), variance (4.12), correlation (0.816) and regression line (y\u00a0=\u00a03\u00a0+\u00a00.5x). However, as can be seen on the plots, the distribution of the variables is very different. The first one (top left) seems to be distributed normally, and corresponds to what one would expect when considering two variables correlated and following the assumption of normality. The second one (top right) is not distributed normally; while an obvious relationship between the two variables can be observed, it is not linear. In this case the Pearson correlation coefficient does not indicate that there is an exact functional relationship: only the extent to which that relationship can be approximated by a linear relationship. In the third case (bottom left), the linear relationship is perfect, except for one outlier which exerts enough influence to lower the correlation coefficient from 1 to 0.816. Finally, the fourth example (bottom right) shows another example when one outlier is enough to produce a high correlation coefficient, even though the relationship between the two variables is not linear.The Pearson correlation coefficient indicates the strength of a linear relationship between two variables, but its value generally does not completely characterize their relationship.[16] In particular, if the conditional mean of Y given X, denoted E(Y\u00a0|\u00a0X), is not linear in X, the correlation coefficient will not fully determine the form of E(Y\u00a0|\u00a0X).A correlation between age and height in children is fairly causally transparent, but a correlation between mood and health in people is less so. Does improved mood lead to improved health, or does good health lead to good mood, or both? Or does some other factor underlie both? In other words, a correlation can be taken as evidence for a possible causal relationship, but cannot indicate what the causal relationship, if any, might be.The conventional dictum that \"correlation does not imply causation\" means that correlation cannot be used to infer a causal relationship between the variables.[15] This dictum should not be taken to mean that correlations cannot indicate the potential existence of causal relations. However, the causes underlying the correlation, if any, may be indirect and unknown, and high correlations also overlap with identity relations (tautologies), where no causal process exists. Consequently, establishing a correlation between two variables is not a sufficient condition to establish a causal relationship (in either direction).A correlation matrix appears, for example, in one formula for the coefficient of multiple determination, a measure of goodness of fit in multiple regression.The correlation matrix is symmetric because the correlation between Xi and Xj is the same as the correlation between Xj and\u00a0Xi.Sensitivity to the data distribution can be used to an advantage. For example, scaled correlation is designed to use the sensitivity to the range in order to pick out correlations between fast components of time series.[14] By reducing the range of values in a controlled manner, the correlations on long time scale are filtered out and only the correlations on short time scales are revealed.Various correlation measures in use may be undefined for certain joint distributions of X and Y. For example, the Pearson correlation coefficient is defined in terms of moments, and hence will be undefined if the moments are undefined. Measures of dependence based on quantiles are always defined. Sample-based statistics intended to estimate population measures of dependence may or may not have desirable statistical properties such as being unbiased, or asymptotically consistent, based on the spatial structure of the population from which the data were sampled.Most correlation measures are sensitive to the manner in which X and Y are sampled. Dependencies tend to be stronger if viewed over a wider range of values. Thus, if we consider the correlation coefficient between the heights of fathers and their sons over all adult males, and compare it to the same correlation coefficient calculated when the fathers are selected to be between 165\u00a0cm and 170\u00a0cm in height, the correlation will be weaker in the latter case. Several techniques have been developed that attempt to correct for range restriction in one or both variables, and are commonly used in meta-analysis; the most common are Thorndike's case II and case III equations.[13]The degree of dependence between variables X and Y does not depend on the scale on which the variables are expressed. That is, if we are analyzing the relationship between X and Y, most correlation measures are unaffected by transforming X to a\u00a0+\u00a0bX and Y to c\u00a0+\u00a0dY, where a, b, c, and d are constants (b and d being positive). This is true of some correlation statistics as well as their population analogues. Some correlation statistics, such as the rank correlation coefficient, are also invariant to monotone transformations of the marginal distributions of X and/or Y.The coefficient of determination generalizes the correlation coefficient for relationships beyond simple linear regression.One way to capture a more complete view of dependence structure is to consider a copula between them.The polychoric correlation is another correlation applied to ordinal data that aims to estimate the correlation between theorised latent variables.The correlation ratio is able to detect almost any functional dependency,[citation needed][clarification needed] and the entropy-based mutual information, total correlation and dual total correlation are capable of detecting even more general dependencies. These are sometimes referred to as multi-moment correlation measures,[citation needed] in comparison to those that consider only second moment (pairwise or quadratic) dependence.The Randomized Dependence Coefficient[12] is a computationally efficient, copula-based measure of dependence between multivariate random variables. RDC is invariant with respect to non-linear scalings of random variables, is capable of discovering a wide range of functional association patterns and takes value zero at independence.Distance correlation[10][11] was introduced to address the deficiency of Pearson's correlation that it can be zero for dependent random variables; zero distance correlation implies independence.The information given by a correlation coefficient is not enough to define the dependence structure between random variables.[9] The correlation coefficient completely defines the dependence structure only in very particular cases, for example when the distribution is a multivariate normal distribution. (See diagram above.) In the case of elliptical distributions it characterizes the (hyper-)ellipses of equal density; however, it does not completely characterize the dependence structure (for example, a multivariate t-distribution's degrees of freedom determine the level of tail dependence).As we go from each pair to the next pair x increases, and so does y. This relationship is perfect, in the sense that an increase in x is always accompanied by an increase in\u00a0y. This means that we have a perfect rank correlation, and both Spearman's and Kendall's correlation coefficients are 1, whereas in this example Pearson product-moment correlation coefficient is 0.7544, indicating that the points are far from lying on a straight line. In the same way if y always decreases when x increases, the rank correlation coefficients will be \u22121, while the Pearson product-moment correlation coefficient may or may not be close to \u22121, depending on how close the points are to a straight line. Although in the extreme cases of perfect rank correlation the two coefficients are both equal (being both +1 or both \u22121), this is not generally the case, and so values of the two coefficients cannot meaningfully be compared.[7] For example, for the three pairs (1,\u00a01) (2,\u00a03) (3,\u00a02) Spearman's coefficient is 1/2, while Kendall's coefficient is\u00a01/3.To illustrate the nature of rank correlation, and its difference from linear correlation, consider the following four pairs of numbers (x,\u00a0y):Rank correlation coefficients, such as Spearman's rank correlation coefficient and Kendall's rank correlation coefficient (\u03c4) measure the extent to which, as one variable increases, the other variable tends to increase, without requiring that increase to be represented by a linear relationship. If, as the one variable increases, the other decreases, the rank correlation coefficients will be negative. It is common to regard these rank correlation coefficients as alternatives to Pearson's coefficient, used either to reduce the amount of calculation or to make the coefficient less sensitive to non-normality in distributions. However, this view has little mathematical basis, as rank correlation coefficients measure a different type of relationship than the Pearson product-moment correlation coefficient, and are best seen as measures of a different type of association, rather than as alternative measure of the population correlation coefficient.[7][8]If x and y are results of measurements that contain measurement error, the realistic limits on the correlation coefficient are not \u22121 to +1 but a smaller range.[6] For the case of a linear model with a single independent variable, the coefficient of determination (R squared) is the square of r, Pearson's product-moment coefficient.where sx and sy are now the uncorrected sample standard deviations of X and Y.The uncorrected form of r (not standard) can be written aswhere x and y are the sample means of X and Y, and sx and sy are the corrected sample standard deviations of X and Y.If we have a series of n measurements of X and Y written as xi and yi for i = 1, 2, ..., n, then the sample correlation coefficient can be used to estimate the population Pearson correlation r between X and Y. The sample correlation coefficient is written asIf the variables are independent, Pearson's correlation coefficient is 0, but the converse is not true because the correlation coefficient detects only linear dependencies between two variables. For example, suppose the random variable X is symmetrically distributed about zero, and Y = X2. Then Y is completely determined by X, so that X and Y are perfectly dependent, but their correlation is zero; they are uncorrelated. However, in the special case when X and Y are jointly normal, uncorrelatedness is equivalent to independence.The Pearson correlation is +1 in the case of a perfect direct (increasing) linear relationship (correlation), \u22121 in the case of a perfect decreasing (inverse) linear relationship (anticorrelation),[5] and some value in the open interval (\u22121, 1) in all other cases, indicating the degree of linear dependence between the variables. As it approaches zero there is less of a relationship (closer to uncorrelated). The closer the coefficient is to either \u22121 or 1, the stronger the correlation between the variables.The Pearson correlation is defined only if both of the standard deviations are finite and nonzero. It is a corollary of the Cauchy\u2013Schwarz inequality that the correlation cannot exceed 1 in absolute value. The correlation coefficient is symmetric: corr(X,Y)\u00a0=\u00a0corr(Y,X).where E is the expected value operator, cov means covariance, and corr is a widely used alternative notation for the correlation coefficient.The population correlation coefficient \u03c1X,Y between two random variables X and Y with expected values \u03bcX and \u03bcY and standard deviations \u03c3X and \u03c3Y is defined asThe most familiar measure of dependence between two quantities is the Pearson product-moment correlation coefficient, or \"Pearson's correlation coefficient\", commonly called simply \"the correlation coefficient\". It is obtained by dividing the covariance of the two variables by the product of their standard deviations. Karl Pearson developed the coefficient from a similar but slightly different idea by Francis Galton.[4]Formally, random variables are dependent if they do not satisfy a mathematical property of probabilistic independence. In informal parlance, correlation is synonymous with dependence. However, when used in a technical sense, correlation refers to any of several specific types of relationship between mean values. There are several correlation coefficients, often denoted \u03c1 or r, measuring the degree of correlation. The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other). Other correlation coefficients have been developed to be more robust than the Pearson correlation\u00a0\u2013 that is, more sensitive to nonlinear relationships.[1][2][3] Mutual information can also be applied to measure dependence between two variables.Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation).In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. Correlation is any of a broad class of statistical relationships involving dependence, though in common usage it most often refers to how close two variables are to having a linear relationship with each other. Familiar examples of dependent phenomena include the correlation between the physical statures of parents and their offspring, and the correlation between the demand for a limited supply product and its price.",
            "title": "Correlation and dependence",
            "url": "https://en.wikipedia.org/wiki/Correlation_matrix"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Chebyshev%27s_inequality#Semivariances",
                "/wiki/Downside_risk",
                "/wiki/Investment#In_finance",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Ronald_Fisher",
                "/wiki/The_Correlation_Between_Relatives_on_the_Supposition_of_Mendelian_Inheritance",
                "/wiki/Bootstrapping_(statistics)",
                "/wiki/Resampling_(statistics)",
                "/wiki/Median",
                "/wiki/F_test",
                "/wiki/Chi_square_test",
                "/wiki/Law_of_large_numbers",
                "/wiki/Consistent_estimator",
                "/wiki/Kurtosis",
                "/wiki/Central_moment",
                "/wiki/Random_variable",
                "/wiki/Normal_distribution",
                "/wiki/Cochran%27s_theorem",
                "/wiki/Chi-squared_distribution",
                "/wiki/U-statistic",
                "/wiki/Bessel%27s_correction",
                "/wiki/Sample_covariance",
                "/wiki/Sample_standard_deviation",
                "/wiki/Concave_function",
                "/wiki/Jensen%27s_inequality",
                "/wiki/Unbiased_estimation_of_standard_deviation",
                "/wiki/Statistical_sample",
                "/wiki/Squared_deviations",
                "/wiki/Sample_(statistics)",
                "/wiki/Statistical_population",
                "/wiki/Mean_squared_error",
                "/wiki/Excess_kurtosis",
                "/wiki/Mean_squared_error#Variance",
                "/wiki/Shrinkage_estimator",
                "/wiki/Biased_estimator",
                "/wiki/Bessel%27s_correction",
                "/wiki/Consistent_estimator",
                "/wiki/Squared_deviations",
                "/wiki/Mean_squared_error",
                "/wiki/Unbiased_estimation_of_standard_deviation",
                "/wiki/Estimation_theory",
                "/wiki/Estimator",
                "/wiki/Sample_(statistics)",
                "/wiki/Observations",
                "/wiki/Population",
                "/wiki/Delta_method",
                "/wiki/Taylor_expansion",
                "/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables",
                "/wiki/Covariance",
                "/wiki/Robust_statistics",
                "/wiki/Outlier",
                "/wiki/Measurement_error",
                "/wiki/Heavy-tailed_distribution",
                "/wiki/Standard_deviation",
                "/wiki/Root_mean_square_deviation",
                "/wiki/Probability_density_function",
                "/wiki/Cumulative_distribution_function",
                "/wiki/Catastrophic_cancellation",
                "/wiki/Algorithms_for_calculating_variance",
                "/wiki/Analysis_of_variance",
                "/wiki/Independence_(probability_theory)",
                "/wiki/Covariance",
                "/wiki/Law_of_large_numbers",
                "/wiki/Spearman%E2%80%93Brown_prediction_formula",
                "/wiki/Standard_error",
                "/wiki/Covariance",
                "/wiki/Cronbach%27s_alpha",
                "/wiki/Classical_test_theory",
                "/wiki/Correlated",
                "/wiki/Covariance",
                "/wiki/Standard_error_(statistics)",
                "/wiki/Central_limit_theorem",
                "/wiki/Ir%C3%A9n%C3%A9e-Jules_Bienaym%C3%A9",
                "/wiki/Statistical_independence",
                "/wiki/Uncorrelated",
                "/wiki/Linear_combination",
                "/wiki/Invariant_(mathematics)",
                "/wiki/Location_parameter",
                "/wiki/Data_set",
                "/wiki/Central_limit_theorem",
                "/wiki/Weighted_variance",
                "/wiki/Floating_point_arithmetic",
                "/wiki/Catastrophic_cancellation",
                "/wiki/Algorithms_for_calculating_variance",
                "/wiki/Discrete_random_variable",
                "/wiki/Continuous_random_variable",
                "/wiki/Cantor_distribution",
                "/wiki/Covariance"
            ],
            "text": "For inequalities associated with the semivariance, see Chebyshev's inequality#Semivariances.The semivariance is calculated in the same manner as the variance but only those observations that fall below the mean are included in the calculation. It is sometimes described as a measure of downside risk in an investments context. For skewed distributions, the semivariance can provide additional information that a variance does not.[citation needed]That is, there is the most variance in the x direction. Physicists would consider this to have a low moment about the x axis so the moment-of-inertia tensor isThis difference between moment of inertia in physics and in statistics is clear for points that are gathered along a line. Suppose many points are close to the x axis and distributed along it. The covariance matrix might look likeThe term variance was first introduced by Ronald Fisher in his 1918 paper The Correlation Between Relatives on the Supposition of Mendelian Inheritance:[20]Resampling methods, which include the bootstrap and the jackknife, may be used to test the equality of variances.The Lehmann test is a parametric test of two variances. Of this test there are several variants known. Other tests of the equality of variances include the Box test, the Box\u2013Anderson test and the Moses test.Several non parametric tests have been proposed: these include the Barton\u2013David\u2013Ansari\u2013Freund\u2013Siegel\u2013Tukey test, the Capon test, Mood test, the Klotz test and the Sukhatme test. The Sukhatme test applies to two variances and requires that both medians be known and equal to zero. The Mood, Klotz, Capon and Barton\u2013David\u2013Ansari\u2013Freund\u2013Siegel\u2013Tukey tests also apply to two variances. They allow the median to be unknown but do require that the two medians are equal.Testing for the equality of two or more variances is difficult. The F test and chi square tests are both adversely affected by non-normality and are not recommended for this purpose.where ymin is the minimum of the sample.[19]This bound has been improved, and it is known that variance is bounded byIt has been shown[18] that for a sample {yi} of real numbers,If the conditions of the law of large numbers hold for the squared observations, s2 is a consistent estimator of\u00a0\u03c32. One can see indeed that the variance of the estimator tends asymptotically to zero. An asymptotically equivalent formula was given in Kenney and Keeping (1951:164), Rose and Smith (2002:264), and Weisstein (n.d.).[14][15][16]where \u03ba is the kurtosis of the distribution and \u03bc4 is the fourth central moment.If the yi are independent and identically distributed, but not necessarily normally distributed, then[12][13]and[11]As a direct consequence, it follows thatBeing a function of random variables, the sample variance is itself a random variable, and it is natural to study its distribution. In the case that yi are independent observations from a normal distribution, Cochran's theorem shows that s2 follows a scaled chi-squared distribution:[10]The unbiased sample variance is a U-statistic for the function \u0192(y1,\u00a0y2) =\u00a0(y1\u00a0\u2212\u00a0y2)2/2, meaning that it is obtained by averaging a 2-sample statistic over 2-element subsets of the population.The use of the term n\u00a0\u2212\u00a01 is called Bessel's correction, and it is also used in sample covariance and the sample standard deviation (the square root of variance). The square root is a concave function and thus introduces negative bias (by Jensen's inequality), which depends on the distribution, and thus the corrected sample standard deviation (using Bessel's correction) is biased. The unbiased estimation of standard deviation is a technically involved problem, though for the normal distribution using the term n\u00a0\u2212\u00a01.5 yields an almost unbiased estimator.Either estimator may be simply referred to as the sample variance when the version can be determined by context. The same proof is also applicable for samples taken from a continuous probability distribution.We take a sample with replacement of n values y1,\u00a0...,\u00a0yn from the population, where n\u00a0<\u00a0N, and estimate the variance on the basis of this sample.[9] Directly taking the variance of the sample data gives the average of the squared deviations:In many practical situations, the true variance of a population is not known a priori and must be computed somehow. When dealing with extremely large populations, it is not possible to count every object in the population, so the computation must be performed on a sample of the population.[8] Sample variance can also be applied to the estimation of the variance of a continuous distribution from a sample of that distribution.The population variance matches the variance of the generating probability distribution. In this sense, the concept of population can be extended to continuous random variables with infinite populations.This is true becauseThe population variance can also be computed usingwhere the population mean isIn general, the population variance of a finite population of size N with values xi is given bySecondly, the sample variance does not generally minimize mean squared error between sample variance and population variance. Correcting for bias often makes this worse: one can always choose a scale factor that performs better than the corrected sample variance, though the optimal scale factor depends on the excess kurtosis of the population (see mean squared error: variance), and introduces bias. This always consists of scaling down the unbiased estimator (dividing by a number larger than n\u00a0\u2212\u00a01), and is a simple example of a shrinkage estimator: one \"shrinks\" the unbiased estimator towards zero. For the normal distribution, dividing by n\u00a0+\u00a01 (instead of n\u00a0\u2212\u00a01 or n) minimizes mean squared error. The resulting estimator is biased, however, and is known as the biased sample variation.Firstly, if the omniscient mean is unknown (and is computed as the sample mean), then the sample variance is a biased estimator: it underestimates the variance by a factor of (n\u00a0\u2212\u00a01) / n; correcting by this factor (dividing by n\u00a0\u2212\u00a01 instead of n) is called Bessel's correction. The resulting estimator is unbiased, and is called the (corrected) sample variance or unbiased sample variance. For example, when n\u00a0=\u00a01 the variance of a single observation about the sample mean (itself) is obviously zero regardless of the population variance. If the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the (independently known) mean.The simplest estimators for population mean and population variance are simply the mean and variance of the sample, the sample mean and (uncorrected) sample variance \u2013 these are consistent estimators (they converge to the correct value as the number of samples increases), but can be improved. Estimating the population variance by taking the sample's variance is close to optimal in general, but can be improved in two ways. Most simply, the sample variance is computed as an average of squared deviations about the (sample) mean, by dividing by n. However, using values other than n improves the estimator in various ways. Four common values for the denominator are n, n\u00a0\u2212\u00a01, n\u00a0+\u00a01, and n\u00a0\u2212\u00a01.5: n is the simplest (population variance of the sample), n\u00a0\u2212\u00a01 eliminates bias, n\u00a0+\u00a01 minimizes mean squared error for the normal distribution, and n\u00a0\u2212\u00a01.5 mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution.Real-world observations such as the measurements of yesterday's rain throughout the day typically cannot be complete sets of all possible observations that could be made. As such, the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations. This means that one estimates the mean and variance that would have been calculated from an omniscient set of observations by using an estimator equation. The estimator is a function of the sample of n observations drawn without observational bias from the whole population of potential observations. In this example that sample would be the set of actual measurements of yesterday's rainfall from available rain gauges within the geography of interest.provided that f is twice differentiable and that the mean and variance of X are finite.The delta method uses second-order Taylor expansions to approximate the variance of a function of one or more random variables: see Taylor expansions for the moments of functions of random variables. For example, the approximate variance of a function of one variable is given byThe standard deviation and the expected absolute deviation can both be used as an indicator of the \"spread\" of a distribution. The standard deviation is more amenable to algebraic manipulation than the expected absolute deviation, and, together with variance and its generalization covariance, is used frequently in theoretical statistics; however the expected absolute deviation tends to be more robust as it is less sensitive to outliers arising from measurement anomalies or an unduly heavy-tailed distribution.Unlike expected absolute deviation, the variance of a variable has units that are the square of the units of the variable itself. For example, a variable measured in meters will have a variance measured in meters squared. For this reason, describing data sets via their standard deviation or root mean square deviation is often preferred over using the variance. In the dice example the standard deviation is \u221a2.9\u00a0\u2248\u00a01.7, slightly larger than the expected absolute deviation of\u00a01.5.This expression can be used to calculate the variance in situations where the CDF, but not the density, can be conveniently expressed.The population variance for a non-negative random variable can be expressed in terms of the cumulative distribution function F usingThis formula is also sometimes used in connection with the sample variance. While useful for hand calculations, it is not advised for computer calculations as it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude and floating point arithmetic is used. This is discussed in the article Algorithms for calculating variance.This will be useful when it is possible to derive formulae for the expected value and for the expected value of the square.A formula often used for deriving the variance of a theoretical distribution is as follows:This can also be derived from the additivity of variances, since the total (observed) score is the sum of the predicted score and the error score, where the latter two are uncorrelated.A similar formula is applied in analysis of variance, where the corresponding formula isIn general, if two variables are statistically dependent, the variance of their product is given by:Equivalently, using the basic properties of expectation, it is given byIf two variables X and Y are independent, the variance of their product is given by[6]The expression above can be extended to a weighted sum of multiple variables:This implies that in a weighted sum of variables, the variable with the largest weight will have a disproportionally large weight in the variance of the total. For example, if X and Y are uncorrelated and the weight of X is two times the weight of Y, then the weight of the variance of X will be four times the weight of the variance of Y.The scaling property and the Bienaym\u00e9 formula, along with the property of the covariance Cov(aX,\u00a0bY) = ab Cov(X,\u00a0Y) jointly imply thatTherefore, the variance of the mean of a large number of standardized variables is approximately equal to their average correlation. This makes clear that the sample mean of correlated variables does not generally converge to the population mean, even though the law of large numbers states that the sample mean will converge for independent variables.This formula is used in the Spearman\u2013Brown prediction formula of classical test theory. This converges to \u03c1 if n goes to infinity, provided that the average correlation remains constant or converges too. So for the variance of the mean of standardized variables with equal correlations or converging average correlation we haveThis implies that the variance of the mean increases with the average of the correlations. In other words, additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean. Moreover, if the variables have unit variance, for example if they are standardized, then this simplifies toSo if the variables have equal variance \u03c32 and the average correlation of distinct variables is \u03c1, then the variance of their mean isHere Cov(\u22c5, \u22c5) is the covariance, which is zero for independent random variables (if it exists). The formula states that the variance of a sum is equal to the sum of all elements in the covariance matrix of the components. The next expression states equivalently that the variance of the sum is the sum of the diagonal of covariance matrix plus two times the sum of its upper triangular elements (or its lower triangular elements); this emphasizes that the covariance matrix is symmetric. This formula is used in the theory of Cronbach's alpha in classical test theory.(Note: The second equality comes from the fact that Cov(Xi,Xi) = Var(Xi).)In general, if the variables are correlated, then the variance of their sum is the sum of their covariances:Using the linearity of the expectation operator and the assumption of independence (or uncorrelatedness) of X and Y, this further simplifies as follows:The general result then follows by induction. Starting with the definition,To prove the initial statement, it suffices to show thatThat is, the variance of the mean decreases when n increases. This formula for the variance of the mean is used in the definition of the standard error of the sample mean, which is used in the central limit theorem.This statement is called the Bienaym\u00e9 formula[2] and was discovered in 1853.[3][4] It is often made with the stronger condition that the variables are independent, but being uncorrelated suffices. So if all the variables have the same variance \u03c32, then, since division by n is a linear transformation, this formula immediately implies that the variance of their mean isOne reason for the use of the variance in preference to other measures of dispersion is that the variance of the sum (or the difference) of uncorrelated random variables is the sum of their variances:These results lead to the variance of a linear combination as:The variance of a sum of two random variables is given byIf all values are scaled by a constant, the variance is scaled by the square of that constant:Variance is invariant with respect to changes in a location parameter. That is, if a constant is added to all values of the variable, the variance is unchanged:The variance of a constant random variable is zero, and if the variance of a variable in a data set is 0, then all the entries have the same value:Variance is non-negative because the squares are positive or zero:The general formula for the variance of the outcome, X, of an n-sided die isThe role of the normal distribution in the central limit theorem is in part responsible for the prevalence of the variance in probability and statistics.(When such a discrete weighted variance is specified by weights whose sum is not\u00a01, then one divides by the sum of the weights.)or equivalentlyA mnemonic for the above expression is \"mean of square minus square of mean\". This equation should not be used for computations using floating point arithmetic because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude. There exist numerically stable alternatives.This definition encompasses random variables that are generated by processes that are discrete, continuous, neither, or mixed. The variance can also be thought of as the covariance of a random variable with itself:",
            "title": "Variance",
            "url": "https://en.wikipedia.org/wiki/Sample_variance"
        },
        {
            "desc_links": [
                "/wiki/Canonical_correlation",
                "/wiki/Orthogonal_coordinate_system",
                "/wiki/Factor_analysis",
                "/wiki/Eigenvectors",
                "/wiki/Dimension_(metadata)",
                "/wiki/Exploratory_data_analysis",
                "/wiki/Predictive_modeling",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Covariance",
                "/wiki/Correlation",
                "/wiki/Singular_value_decomposition",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Z-score",
                "/wiki/Karl_Pearson",
                "/wiki/Principal_axis_theorem",
                "/wiki/Harold_Hotelling",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Signal_processing",
                "/wiki/Harold_Hotelling",
                "/wiki/Proper_orthogonal_decomposition",
                "/wiki/Singular_value_decomposition",
                "/wiki/Eigendecomposition",
                "/wiki/Factor_analysis",
                "/wiki/Eckart%E2%80%93Young_theorem",
                "/wiki/Psychometrics",
                "/wiki/Empirical_orthogonal_functions",
                "/wiki/Spectral_theorem",
                "/wiki/Mode_shape",
                "/wiki/Orthogonal_transformation",
                "/wiki/Correlation_and_dependence",
                "/wiki/Variance",
                "/wiki/Orthogonal",
                "/wiki/Orthogonal_basis_set"
            ],
            "links": [
                "/wiki/Independent_component_analysis",
                "/wiki/Sparse_PCA",
                "/wiki/Robust_principal_component_analysis",
                "/wiki/Outlier",
                "/wiki/Data_mining",
                "/wiki/Correlation_clustering",
                "/wiki/Tucker_decomposition",
                "/wiki/PARAFAC",
                "/wiki/Multilinear_subspace_learning",
                "/wiki/Multilinear_principal_component_analysis",
                "/wiki/Nonlinear_dimensionality_reduction",
                "/wiki/Curve",
                "/wiki/Manifold",
                "/wiki/Approximation",
                "/wiki/Projection_(mathematics)",
                "/wiki/Elastic_map",
                "/wiki/Principal_geodesic_analysis",
                "/wiki/Kernel_PCA",
                "/wiki/Non-negative_matrix_factorization",
                "/wiki/K-means_clustering",
                "/wiki/Factor_analysis",
                "/wiki/Correspondence_analysis",
                "/wiki/Jean-Paul_Benz%C3%A9cri",
                "/wiki/Contingency_tables",
                "/wiki/Chi-squared_statistic",
                "/wiki/Detrended_correspondence_analysis",
                "/wiki/Canonical_correspondence_analysis",
                "/wiki/Multiple_correspondence_analysis",
                "/wiki/Order_parameters",
                "/wiki/Phase_transitions",
                "/wiki/Spike_sorting",
                "/wiki/Electrophysiology#Extracellular_recording",
                "/wiki/Cluster_analysis",
                "/wiki/Neuroscience",
                "/wiki/Neuron",
                "/wiki/Action_potential",
                "/wiki/Spike-triggered_covariance",
                "/wiki/White_noise",
                "/wiki/Electric_current",
                "/wiki/Covariance_matrix",
                "/wiki/Eigenvectors_and_eigenvalues",
                "/wiki/Vector_space",
                "/wiki/Stock",
                "/wiki/Asset_allocation",
                "/wiki/Stock_selection_criterion",
                "/wiki/Quantitative_finance",
                "/wiki/Risk_management",
                "/wiki/Interest_rate_derivative",
                "/wiki/Swap_(finance)",
                "/wiki/Ludovic_Lebart",
                "/wiki/Algorithm",
                "/wiki/Partial_least_squares",
                "/wiki/Genomics",
                "/wiki/Metabolomics",
                "/wiki/Non-linear_iterative_partial_least_squares",
                "/wiki/Gram%E2%80%93Schmidt",
                "/wiki/Lanczos_algorithm",
                "/wiki/Power_iteration",
                "/wiki/High_dimensional_data",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Non-negative_matrix_factorization",
                "/wiki/Principal_component_analysis#Relation_between_PCA_and_Non-negative_Matrix_Factorization",
                "/wiki/Diagonal",
                "/wiki/Regression_analysis",
                "/wiki/Pattern_recognition",
                "/wiki/Linear_discriminant_analysis",
                "/wiki/Autoencoder",
                "/wiki/Artificial_neural_network",
                "/wiki/Orthogonal",
                "/wiki/Minimum_mean_square_error",
                "/wiki/Sample_variance",
                "/wiki/Euclidean_space",
                "/wiki/Eigenvalue",
                "/wiki/Principle_Component_Analysis#PCA_and_information_theory",
                "/wiki/Dimensionality_reduction",
                "/wiki/Discrete_cosine_transform",
                "/wiki/Nonlinear_dimensionality_reduction",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Frobenius_norm",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Polar_decomposition",
                "/wiki/Diagonal_matrix",
                "/wiki/Singular_value_decomposition",
                "/wiki/Signal-to-noise_ratio",
                "/wiki/Regression_analysis",
                "/wiki/Explanatory_variable",
                "/wiki/Overfitting",
                "/wiki/Principal_component_regression",
                "/wiki/Dimensionality_reduction",
                "/wiki/Cluster_analysis",
                "/wiki/Covariance_matrix",
                "/wiki/Whitening_transformation",
                "/wiki/Rayleigh_quotient",
                "/wiki/Positive_semidefinite_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Eigenvector",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Empirical_mean",
                "/wiki/Orthogonal_transformation",
                "/wiki/Linear_transformation",
                "/wiki/Coordinate_system",
                "/wiki/Covariance_matrix",
                "/wiki/Ellipsoid",
                "/wiki/Canonical_correlation",
                "/wiki/Orthogonal_coordinate_system",
                "/wiki/Factor_analysis",
                "/wiki/Eigenvectors",
                "/wiki/Dimension_(metadata)",
                "/wiki/Exploratory_data_analysis",
                "/wiki/Predictive_modeling",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Covariance",
                "/wiki/Correlation",
                "/wiki/Singular_value_decomposition",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Z-score",
                "/wiki/Karl_Pearson",
                "/wiki/Principal_axis_theorem",
                "/wiki/Harold_Hotelling",
                "/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem",
                "/wiki/Signal_processing",
                "/wiki/Harold_Hotelling",
                "/wiki/Proper_orthogonal_decomposition",
                "/wiki/Singular_value_decomposition",
                "/wiki/Eigendecomposition",
                "/wiki/Factor_analysis",
                "/wiki/Eckart%E2%80%93Young_theorem",
                "/wiki/Psychometrics",
                "/wiki/Empirical_orthogonal_functions",
                "/wiki/Spectral_theorem",
                "/wiki/Mode_shape"
            ],
            "text": "then the decomposition is unique up to multiplication by a scalar.[60]Independent component analysis (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.A particular disadvantage of PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables.Robust principal component analysis (RPCA) via decomposition in low-rank and sparse matrices is a modification of the widely used statistical procedure principal component analysis (PCA) which works well with respect to grossly corrupted observations.[56][57][58][59]While PCA finds the mathematically optimal method (as in minimizing the squared error), it is sensitive to outliers in the data that produce large errors PCA tries to avoid. It therefore is common practice to remove outliers before computing PCA. However, in some contexts, outliers can be difficult to identify. For example, in data mining algorithms like correlation clustering, the assignment of points to clusters and outliers is not known beforehand. A recently proposed generalization of PCA[55] based on a weighted PCA increases robustness by assigning different weights to data objects based on their estimated relevancy.N-way principal component analysis may be performed with models such as Tucker decomposition, PARAFAC, multiple factor analysis, co-inertia analysis, STATIS, and DISTATIS.In multilinear subspace learning,[54] PCA is generalized to multilinear PCA (MPCA) that extracts features directly from tensor representations. MPCA is solved by performing PCA in each mode of the tensor iteratively. MPCA has been applied to face recognition, gait recognition, etc. MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.Most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in PCA or K-means. Pearson's original idea was to take a straight line (or plane) which will be \"the best fit\" to a set of data points. Principal curves and manifolds[53] give the natural geometric framework for PCA generalization and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold for data approximation, and by encoding using standard geometric projection onto the manifold, as it is illustrated by Fig. See also the elastic map algorithm and principal geodesic analysis. Another popular generalization is kernel PCA, which corresponds to PCA performed in a reproducing kernel Hilbert space associated with a positive definite kernel.Non-negative matrix factorization (NMF) is a dimension reduction method where only non-negative elements in the matrices are used, which is therefore a promising method in astronomy,[17][18][19] in the sense that astrophysical signals are non-negative. The PCA components are orthogonal to each other, while the NMF components are all non-negative and therefore constructs a non-orthogonal basis.It was asserted in [46][47] that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by the principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. However, that PCA is a useful relaxation of k-means clustering was not a new result (see, for example,[48]), and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.[49]Factor analysis is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Different from PCA, factor analysis is a correlation-focused approach seeking to reproduce the inter-correlations among variables, in which the factors \"represent the common variance of variables, excluding unique variance\".[44] In terms of the correlation matrix, this corresponds with focusing on explaining the off-diagonal terms (i.e. shared co-variance), while PCA focuses on explaining the terms that sit on the diagonal. However, as a side result, when trying to reproduce the on-diagonal terms, PCA also tends to fit relatively well the off-diagonal correlations.[45] Results given by PCA and factor analysis are very similar in most situations, but this is not always the case, and there are some problems where the results are significantly different. Factor analysis is generally used when the research purpose is detecting data structure (i.e., latent constructs or factors) or causal modeling.Principal component analysis creates variables that are linear combinations of the original variables. The new variables have the property that the variables are all orthogonal. The PCA transformation can be helpful as a pre-processing step before clustering. PCA is a variance-focused approach seeking to reproduce the total variable variance, in which components reflect both common and unique variance of the variable. PCA is generally preferred for purposes of data reduction (i.e., translating variable space into optimal factor space) but not when the goal is to detect the latent construct or factors.Correspondence analysis (CA) was developed by Jean-Paul Benz\u00e9cri[41] and is conceptually similar to PCA, but scales the data (which should be non-negative) so that rows and columns are treated equivalently. It is traditionally applied to contingency tables. CA decomposes the chi-squared statistic associated to this table into orthogonal factors.[42] Because CA is a descriptive technique, it can be applied to tables for which the chi-squared statistic is appropriate or not. Several variants of CA are available including detrended correspondence analysis and canonical correspondence analysis. One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data.[43]PCA as a dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles. It has been used in determining collective variables, i.e. order parameters, during phase transitions in the brain.[40]In neuroscience, PCA is also used to discern the identity of a neuron from the shape of its action potential. Spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron. In spike sorting, one first uses PCA to reduce the dimensionality of the space of action potential waveforms, and then performs clustering analysis to associate specific action potentials with individual neurons.A variant of principal components analysis is used in neuroscience to identify the specific properties of a stimulus that increase a neuron's probability of generating an action potential.[39] This technique is known as spike-triggered covariance analysis. In a typical application an experimenter presents a white noise process as a stimulus (usually either as a sensory input to a test subject, or as a current injected directly into the neuron) and records a train of action potentials, or spikes, produced by the neuron as a result. Presumably, certain features of the stimulus make the neuron more likely to spike. In order to extract these features, the experimenter calculates the covariance matrix of the spike-triggered ensemble, the set of all stimuli (defined and discretized over a finite time window, typically on the order of 100 ms) that immediately preceded a spike. The eigenvectors of the difference between the spike-triggered covariance matrix and the covariance matrix of the prior stimulus ensemble (the set of all stimuli, defined over the same length time window) then indicate the directions in the space of stimuli along which the variance of the spike-triggered ensemble differed the most from that of the prior stimulus ensemble. Specifically, the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike-triggered ensemble showed the largest positive change compared to the variance of the prior. Since these were the directions in which varying the stimulus led to a spike, they are often good approximations of the sought after relevant stimulus features.PCA has also been applied to share portfolios in a similar fashion.[36] One application is to reduce portfolio risk, where allocation strategies are applied to the \"principal portfolios\" instead of the underlying stocks.[37] A second is to enhance portfolio return, using the principal components to select stocks with upside potential.[38]In quantitative finance, principal component analysis can be directly applied to the risk management of interest rate derivatives portfolios.[35] Trading multiple swap instruments which are usually a function of 30-500 other market quotable swap instruments is sought to be reduced to usually 3 or 4 principal components, representing the path of interest rates on a macro basis. Converting risks to be represented as those to factor loadings (or multipliers) provides assessments and understanding beyond that available to simply collectively viewing risks to individual 30-500 buckets.These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, L\u00ea & Pag\u00e8s 2009 and Pag\u00e8s 2013. Few software offer this option in an \"automatic\" way. This is the case of SPAD that historically, following the work of Ludovic Lebart, was the first to propose this option, and the R package FactoMineR.In PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable species. For this, the following results are produced.In an \"online\" or \"streaming\" situation with data arriving piece by piece rather than being stored in a single batch, it is useful to make an estimate of the PCA projection that can be updated sequentially. This can be done efficiently, but requires different algorithms.[34]However, for large data matrices, or matrices that have a high degree of column collinearity, NIPALS suffers from loss of orthogonality due to machine precision limitations accumulated in each iteration step.[32] A Gram\u2013Schmidt (GS) re-orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality.[33]Non-linear iterative partial least squares (NIPALS) is an algorithm for computing the first few components in a principal component or partial least squares analysis. For very-high-dimensional datasets, such as those generated in the *omics sciences (e.g., genomics, metabolomics) it is usually only necessary to compute the first few PCs. The non-linear iterative partial least squares (NIPALS) algorithm calculates t1 and w1T from X. The outer product, t1w1T can then be subtracted from X leaving the residual matrix E1. This can be then used to calculate subsequent PCs.[31] This results in a dramatic reduction in computational time since calculation of the covariance matrix is avoided.One way to compute the eigenvalue that corresponds with each principal component is to measure the difference in mean-squared-distance between the rows and the centroid, before and after subtracting out the principal component. The eigenvalue that corresponds with the component that was removed is equal to this difference.Subsequent principal components can be computed by subtracting component r from X (see Gram\u2013Schmidt) and then repeating this algorithm to find the next principal component. However this simple approach is not numerically stable if more than a small number of principal components are required, because imprecisions in the calculations will additively affect the estimates of subsequent principal components. More advanced methods build on this basic idea, as with the closely related Lanczos algorithm.This algorithm is simply an efficient way of calculating XTX r, normalizing, and placing the result back in r (power iteration). It avoids the np2 operations of calculating the covariance matrix. r will typically get close to the first principal component of X within a small number of iterations, c. (The magnitude of s will be larger after each iteration. Convergence can be detected when it increases by an amount too small for the precision of the machine.)In practical implementations especially with high dimensional data (large p), the covariance method is rarely used because it is not efficient. One way to compute the first principal component efficiently[30] is shown in the following pseudo-code, for a data matrix X with zero mean, without ever computing its covariance matrix.This is very constructive, as cov(X) is guaranteed to be a non-negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix.Let X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data.[25] Hence we proceed by centering the data as follows:The goal is to transform a given data set X of dimension p to an alternative data set Y of smaller dimension L. Equivalently, we are seeking to find the matrix Y, where Y is the Karhunen\u2013Lo\u00e8ve transform (KLT) of matrix X:The following is a detailed description of PCA using the covariance method (see also here) as opposed to the correlation method.[24]Under the assumption thatDimensionality reduction loses information, in general. PCA-based dimensionality reduction tends to minimize that information loss, under certain signal and noise models.The other limitation is the mean-removal process before constructing the covariance matrix for PCA. In fields such as astronomy, all the signals are non-negative, and the mean-removal process will force the mean of some astrophysical exposures to be zero, which consequently creates unphysical negative fluxes,[15] and forward modeling has to be performed to recover the true magnitude of the signals.[16] As an alternative method, non-negative matrix factorization focusing only on the non-negative elements in the matrices, which is well-suited for astrophysical observations.[17][18][19] See more at Relation between PCA and Non-negative Matrix Factorization.The applicability of PCA is limited by certain assumptions[14] made in its derivation.As noted above, the results of PCA depend on the scaling of the variables. A scale-invariant form of PCA has been developed.[13]Before we look at its usage, we first look at diagonal elements,The statistical implication of this property is that the last few PCs are not simply unstructured left-overs after removing the important PCs. Because these last PCs have variances as small as possible they are useful in their own right. They can help to detect unsuspected near-constant linear relationships between the elements of x, and they may also be useful in regression, in selecting a subset of variables from x, and in outlier detection.Some properties of PCA include:[12]PCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability.[10] However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes.[11] The linear discriminant analysis is an alternative which is optimized for class separability.An autoencoder neural network with a linear hidden layer is similar to PCA. Upon convergence, the weight vectors of the K neurons in the hidden layer will form a basis for the space spanned by the first K principal components. Unlike PCA, this technique will not necessarily produce orthogonal vectors.Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation). Also see the article by Kromrey & Foster-Johnson (1998) on \"Mean-centering in Moderated Regression: Much Ado About Nothing\".Mean subtraction (a.k.a. \"mean centering\") is necessary for performing PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.[9]PCA is sensitive to the scaling of the variables. If we have just two variables and they have the same sample variance and are positively correlated, then the PCA will entail a rotation by 45\u00b0 and the \"loadings\" for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Note that Pearson's original paper was entitled \"On Lines and Planes of Closest Fit to Systems of Points in Space\" \u2013 \"in space\" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.Given a set of points in Euclidean space, the first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. The second principal component corresponds to the same concept after all correlation with the first principal component has been subtracted from the points. The singular values (in \u03a3) are the square roots of the eigenvalues of the matrix XTX. Each eigenvalue is proportional to the portion of the \"variance\" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see below). PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest \"variance\" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example and when applicable, to the discrete cosine transform, and in particular to the DCT-II which is simply known as the \"DCT\". Nonlinear dimensionality reduction techniques tend to be more computationally demanding than PCA.The truncation of a matrix M or T using a truncated singular value decomposition in this way produces a truncated matrix that is the nearest possible matrix of rank L to the original matrix, in the sense of the difference between the two having the smallest possible Frobenius norm, a result known as the Eckart\u2013Young theorem [1936].As with the eigen-decomposition, a truncated n \u00d7 L score matrix TL can be obtained by considering only the first L largest singular values and their singular vectors:Efficient algorithms exist to calculate the SVD of X without having to form the matrix XTX, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix[citation needed], unless only a handful of components are required.so each column of T is given by one of the left singular vectors of X multiplied by the corresponding singular value. This form is also the polar decomposition of T.Using the singular value decomposition the score matrix T can be writtenIn terms of this factorization, the matrix XTX can be writtenHere \u03a3 is an n-by-p rectangular diagonal matrix of positive numbers \u03c3(k), called the singular values of X; U is an n-by-n matrix, the columns of which are orthogonal unit vectors of length n called the left singular vectors of X; and W is a p-by-p whose columns are orthogonal unit vectors of length p and called the right singular vectors of X.The principal components transformation can also be associated with another matrix factorization, the singular value decomposition (SVD) of X,Dimensionality reduction may also be appropriate when the variables in a dataset are noisy. If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of T will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix W, which can be thought of as a high-dimensional rotation of the co-ordinate axes). However, with more of the total variance concentrated in the first few principal components compared to the same noise variance, the proportionate effect of the noise is less\u2014the first few components achieve a higher signal-to-noise ratio. PCA thus can have the effect of concentrating much of the signal into the first few principal components, which can usefully be captured by dimensionality reduction; while the later principal components may be dominated by noise, and so disposed of without great loss.Similarly, in regression analysis, the larger the number of explanatory variables allowed, the greater is the chance of overfitting the model, producing conclusions that fail to generalise to other datasets. One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called principal component regression.Such dimensionality reduction can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible. For example, selecting L\u00a0=\u00a02 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains clusters these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable.The transformation T = X W maps a data vector x(i) from an original space of p variables to a new space of p variables which are uncorrelated over the dataset. However, not all the principal components need to be kept. Keeping only the first L principal components, produced by using only the first L loading vectors, gives the truncated transformation(\u03bb(k) being equal to the sum of the squares over the dataset associated with each component k: \u03bb(k) = \u03a3i tk2(i) = \u03a3i (x(i) \u22c5 w(k))2)where \u039b is the diagonal matrix of eigenvalues \u03bb(k) of XTXThe empirical covariance matrix between the principal components becomesIn matrix form, the empirical covariance matrix for the original variables can be writtenAnother way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.where the eigenvalue property of w(k) has been used to move from line 2 to line 3. However eigenvectors w(j) and w(k) corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value). The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset.The sample covariance Q between two of the different principal components over the dataset is given by:XTX itself can be recognised as proportional to the empirical sample covariance matrix of the dataset X.where W is a p-by-p matrix whose columns are the eigenvectors of XTX. The transpose of W is sometimes called the whitening or sphering transformation.The full principal components decomposition of X can therefore be given asThe kth principal component of a data vector x(i) can therefore be given as a score tk(i) = x(i) \u22c5 w(k) in the transformed co-ordinates, or as the corresponding vector in the space of the original variables, {x(i) \u22c5 w(k)} w(k), where w(k) is the kth eigenvector of XTX.It turns out that this gives the remaining eigenvectors of XTX, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the loading vectors are eigenvectors of XTX.and then finding the loading vector which extracts the maximum variance from this new data matrixThe kth component can be found by subtracting the first k\u00a0\u2212\u00a01 principal components from X:With w(1) found, the first principal component of a data vector x(i) can then be given as a score t1(i) = x(i) \u22c5 w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables, {x(i) \u22c5 w(1)} w(1).The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector.Since w(1) has been defined to be a unit vector, it equivalently also satisfiesEquivalently, writing this in matrix form givesIn order to maximize variance, the first loading vector w(1) thus has to satisfyConsider a data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor).PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.[3]This procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data, and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis . The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.PCA can be thought of as fitting an n-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.[6][7]PCA is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.PCA is mostly used as a tool in exploratory data analysis and for making predictive models. It's often used to visualize genetic distance and relatedness between populations. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute.[4] The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score).[5]PCA was invented in 1901 by Karl Pearson,[1] as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s.[2] Depending on the field of application, it is also named the discrete Karhunen\u2013Lo\u00e8ve transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch.\u00a07 of Jolliffe's Principal Component Analysis[3]), Eckart\u2013Young theorem (Harman, 1960), or Schmidt\u2013Mirsky theorem in psychometrics, empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.",
            "title": "Principal component analysis",
            "url": "https://en.wikipedia.org/wiki/Principal_components_analysis"
        },
        {
            "desc_links": [
                "/wiki/Fraction_of_variance_unexplained",
                "/wiki/Residual_(statistics)",
                "/wiki/Statistics",
                "/wiki/Dispersion_(statistics)",
                "/wiki/Variance"
            ],
            "links": [
                "/wiki/Principal_component_analysis",
                "/wiki/Linear_regression",
                "/wiki/Coefficient_of_determination",
                "/wiki/Maximum_likelihood_estimation",
                "/wiki/Fraction_of_variance_unexplained",
                "/wiki/Residual_(statistics)",
                "/wiki/Statistics",
                "/wiki/Dispersion_(statistics)",
                "/wiki/Variance"
            ],
            "text": "Explained variance is routinely used in principal component analysis. The relation to the Fraser\u2013Kent information gain remains to be clarified.The fraction of variance unexplained is an established concept in the context of linear regression. The usual definition of the coefficient of determination is based on the fundamental concept of explained variance.can be interpreted as proportion of the data dispersion which is \"explained\" by X.where a factor of 2 is included for convenience. \u0393 is always nonnegative; it measures the extent to which the best model of family 1 is better than the best model of family 0 in explaining g(r).The information gain of model 1 over model 0 is written asParameters are determined by maximum likelihood estimation,Following Kent (1983),[1] we use the Fraser information (Fraser 1965)[2]The complementary part of the total variation is called unexplained or residual variation.In statistics, explained variation measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set. Often, variation is quantified as variance; then, the more specific term explained variance can be used.",
            "title": "Explained variation",
            "url": "https://en.wikipedia.org/wiki/Explained_variance"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Orthogonal basis",
            "url": "https://en.wikipedia.org/wiki/Orthogonal_basis"
        },
        {
            "desc_links": [
                "/wiki/Field_work",
                "/wiki/Petrology",
                "/wiki/Geophysical_survey",
                "/wiki/Geochemistry",
                "/wiki/Physical_experiment",
                "/wiki/Computer_simulation",
                "/wiki/Mining",
                "/wiki/Petroleum_geology",
                "/wiki/Water_resources",
                "/wiki/Natural_hazard",
                "/wiki/Environmental_Geology",
                "/wiki/Climate_change",
                "/wiki/Academic_discipline",
                "/wiki/Geotechnical_engineering",
                "/wiki/Structure_of_the_Earth",
                "/wiki/Relative_dating",
                "/wiki/Geochronology",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Geologist",
                "/wiki/History_of_the_Earth",
                "/wiki/Age_of_the_Earth",
                "/wiki/Plate_tectonics",
                "/wiki/Evolutionary_history_of_life",
                "/wiki/Paleoclimatology",
                "/wiki/Ancient_Greek",
                "/wiki/Earth_science",
                "/wiki/Solid_Earth",
                "/wiki/Rock_(geology)",
                "/wiki/Terrestrial_planet",
                "/wiki/Natural_satellite",
                "/wiki/Geology_of_Mars",
                "/wiki/Geology_of_the_Moon"
            ],
            "links": [
                "/wiki/Plate_tectonics",
                "/wiki/Seafloor_spreading",
                "/wiki/Continental_drift",
                "/wiki/Earth_sciences",
                "/wiki/Age_of_the_Earth",
                "/wiki/Radiometric_dating",
                "/wiki/Sir_Charles_Lyell",
                "/wiki/Principles_of_Geology",
                "/wiki/Charles_Darwin",
                "/wiki/Uniformitarianism",
                "/wiki/History_of_Earth",
                "/wiki/Catastrophism",
                "/wiki/Geologic_map_of_Georgia",
                "/wiki/William_Maclure",
                "/wiki/Allegheny_Mountains",
                "/wiki/American_Philosophical_Society",
                "/wiki/William_Smith_(geologist)",
                "/wiki/Plutonism",
                "/wiki/Neptunism",
                "/wiki/Abraham_Gottlob_Werner",
                "/wiki/James_Hutton",
                "/wiki/Royal_Society_of_Edinburgh",
                "/wiki/Sediment",
                "/wiki/William_Smith_(geologist)",
                "/wiki/Rock_strata",
                "/wiki/Ulisse_Aldrovandi",
                "/wiki/Jean-Andr%C3%A9_Deluc",
                "/wiki/Horace-B%C3%A9n%C3%A9dict_de_Saussure",
                "/wiki/Ancient_Greek",
                "/wiki/Logos",
                "/wiki/Nicolas_Steno",
                "/wiki/Law_of_superposition",
                "/wiki/Principle_of_original_horizontality",
                "/wiki/Principle_of_lateral_continuity",
                "/wiki/Stratigraphy",
                "/wiki/Fielding_H._Garrison",
                "/wiki/Persia",
                "/wiki/Muslim_conquests",
                "/wiki/Ab%C5%AB_al-Rayh%C4%81n_al-B%C4%ABr%C5%ABn%C4%AB",
                "/wiki/Persian_people",
                "/wiki/Geology_of_India",
                "/wiki/Indian_subcontinent",
                "/wiki/Muslim_conquests",
                "/wiki/Avicenna",
                "/wiki/Polymath",
                "/wiki/Shen_Kuo",
                "/wiki/Stratum",
                "/wiki/Deposition_(sediment)",
                "/wiki/Silt",
                "/wiki/Ancient_Greece",
                "/wiki/Theophrastus",
                "/wiki/Roman_Empire",
                "/wiki/Pliny_the_Elder",
                "/wiki/Amber",
                "/wiki/Building_code",
                "/wiki/Boreholes",
                "/wiki/Core_sample",
                "/wiki/Ice_core",
                "/wiki/Sea_level",
                "/wiki/Global_climate_change",
                "/wiki/Stream_restoration",
                "/wiki/Brownfields",
                "/wiki/Habitat_(ecology)",
                "/wiki/Hydrogeology",
                "/wiki/Civil_engineering",
                "/wiki/Petroleum_geologist",
                "/wiki/Petroleum",
                "/wiki/Natural_gas",
                "/wiki/Sedimentary_basin",
                "/wiki/Mining_geology",
                "/wiki/Gemstone",
                "/wiki/Metal",
                "/wiki/Gold",
                "/wiki/Copper",
                "/wiki/Asbestos",
                "/wiki/Perlite",
                "/wiki/Mica",
                "/wiki/Phosphate",
                "/wiki/Zeolites",
                "/wiki/Clay",
                "/wiki/Pumice",
                "/wiki/Quartz",
                "/wiki/Silica",
                "/wiki/Sulfur",
                "/wiki/Chlorine",
                "/wiki/Helium",
                "/wiki/Natural_resource",
                "/wiki/Phoenix_lander",
                "/wiki/Mars",
                "/wiki/Geology_of_Mars",
                "/wiki/Lunar_geology",
                "/wiki/Space_exploration",
                "/wiki/Earth",
                "/wiki/Planetary_geology",
                "/wiki/Biostratigraphy",
                "/wiki/Sedimentary_depositional_environment",
                "/wiki/Drill_core",
                "/wiki/Well_log",
                "/wiki/Accretionary_wedge",
                "/wiki/Mountain",
                "/wiki/Convergent_boundary",
                "/wiki/Critical_taper",
                "/wiki/Stereographic_projection",
                "/wiki/Fabric_(geology)",
                "/wiki/Analogue_modelling_(geology)",
                "/wiki/Fluid_inclusions",
                "/wiki/Subduction",
                "/wiki/Magma_chamber",
                "/wiki/Lithology",
                "/wiki/Optical_microscopy",
                "/wiki/Electron_microprobe",
                "/wiki/Optical_mineralogy",
                "/wiki/Thin_section",
                "/wiki/Petrographic_microscope",
                "/wiki/Birefringence",
                "/wiki/Pleochroism",
                "/wiki/Crystal_twinning",
                "/wiki/Conoscopy",
                "/wiki/Stable_isotope",
                "/wiki/Radioactive_isotope",
                "/wiki/Geochemistry",
                "/wiki/Field_work",
                "/wiki/Petrology",
                "/wiki/River",
                "/wiki/Landscape",
                "/wiki/Glacier",
                "/wiki/Biogeochemistry",
                "/wiki/Geophysics",
                "/wiki/Hawaiian_Islands",
                "/wiki/Basalt",
                "/wiki/Geology_of_the_Grand_Canyon_area",
                "/wiki/Cambrian",
                "/wiki/Acasta_gneiss",
                "/wiki/Slave_craton",
                "/wiki/Canada",
                "/wiki/Oldest_rock",
                "/wiki/Geological_history",
                "/wiki/Dike_(geology)",
                "/wiki/Dike_swarm",
                "/wiki/Lava_tube",
                "/wiki/Strike-slip_fault",
                "/wiki/Shear_zone",
                "/wiki/Normal_fault",
                "/wiki/Maria_Fold_and_Thrust_Belt",
                "/wiki/Grand_Canyon",
                "/wiki/Boudinage",
                "/wiki/Metamorphism",
                "/wiki/Mineral",
                "/wiki/Foliation_(geology)",
                "/wiki/Bed_(geology)",
                "/wiki/Lava",
                "/wiki/Crystalline_rock",
                "/wiki/Compression_(geology)",
                "/wiki/Incompressible_surface",
                "/wiki/Fault_(geology)",
                "/wiki/Fold_(geology)",
                "/wiki/Brittle_deformation",
                "/wiki/Law_of_superposition",
                "/wiki/Antiform",
                "/wiki/Synform",
                "/wiki/Anticline",
                "/wiki/Syncline",
                "/wiki/Deformation_(mechanics)",
                "/wiki/Metamorphism",
                "/wiki/Extension_(geology)",
                "/wiki/Strike-slip",
                "/wiki/Convergent_boundary",
                "/wiki/Divergent_boundary",
                "/wiki/Country_rock_(geology)",
                "/wiki/Lithification",
                "/wiki/Volcanic_rock",
                "/wiki/Volcanic_ash",
                "/wiki/Lava_flow",
                "/wiki/Igneous_intrusion",
                "/wiki/Batholith",
                "/wiki/Laccolith",
                "/wiki/Dike_(geology)",
                "/wiki/Sill_(geology)",
                "/wiki/Optically_stimulated_luminescence",
                "/wiki/Cosmogenic_isotope#Natural",
                "/wiki/Dendrochronology",
                "/wiki/Radiocarbon_dating",
                "/wiki/Organic_matter",
                "/wiki/Lanthanide_series",
                "/wiki/Isotope_ratio",
                "/wiki/Closure_temperature",
                "/wiki/Crystal_structure",
                "/wiki/Geochronology",
                "/wiki/Thermochronology",
                "/wiki/Uranium-lead_dating",
                "/wiki/Potassium-argon_dating",
                "/wiki/Argon-argon_dating",
                "/wiki/Uranium-thorium_dating",
                "/wiki/Lava",
                "/wiki/Volcanic_ash",
                "/wiki/Pluton",
                "/wiki/Radioactive_isotope",
                "/wiki/Absolute_dating",
                "/wiki/Principle_of_faunal_succession",
                "/wiki/Charles_Darwin",
                "/wiki/Evolution",
                "/wiki/Facies",
                "/wiki/Law_of_superposition",
                "/wiki/Tectonics",
                "/wiki/Principle_of_original_horizontality",
                "/wiki/Cross-bedding",
                "/wiki/Principle_of_inclusions_and_components",
                "/wiki/Clastic_rocks",
                "/wiki/Xenolith",
                "/wiki/Magma",
                "/wiki/Principle_of_cross-cutting_relationships",
                "/wiki/Fault_(geology)",
                "/wiki/Normal_fault",
                "/wiki/Thrust_fault",
                "/wiki/Intrusion_(geology)",
                "/wiki/Igneous_rocks",
                "/wiki/Sedimentary_rock",
                "/wiki/Laccolith",
                "/wiki/Batholith",
                "/wiki/Sill_(geology)",
                "/wiki/Dike_(geology)",
                "/wiki/Principle_of_uniformitarianism",
                "/wiki/James_Hutton",
                "/wiki/Relative_dating",
                "/wiki/Natural_science",
                "/wiki/Solar_System",
                "/wiki/Gigaannum",
                "/wiki/Hadean_eon",
                "/wiki/Holocene_epoch",
                "/wiki/Seismic_wave",
                "/wiki/Outer_core",
                "/wiki/S-wave",
                "/wiki/Inner_core",
                "/wiki/Crust_(geology)",
                "/wiki/Lithosphere",
                "/wiki/Mantle_(geology)",
                "/wiki/Seismic_tomography",
                "/wiki/Seismology",
                "/wiki/Computer_modeling",
                "/wiki/Mineralogy",
                "/wiki/Crystallography",
                "/wiki/Transform_boundary",
                "/wiki/San_Andreas_Fault",
                "/wiki/Alfred_Wegener",
                "/wiki/Continental_drift",
                "/wiki/Continents",
                "/wiki/Mantle_convection",
                "/wiki/Convection",
                "/wiki/Boundary_layer",
                "/wiki/Mantle_(geology)",
                "/wiki/Lithosphere",
                "/wiki/Crust_(geology)",
                "/wiki/Upper_mantle",
                "/wiki/Tectonic_plate",
                "/wiki/Plasticity_(physics)",
                "/wiki/Asthenosphere",
                "/wiki/Drift_(geology)",
                "/wiki/Superficial_deposits",
                "/wiki/Bedrock",
                "/wiki/Quaternary_geology",
                "/wiki/Quaternary_period",
                "/wiki/Crystallization",
                "/wiki/Magma",
                "/wiki/Lava",
                "/wiki/Weathering",
                "/wiki/Eroded",
                "/wiki/Deposition_(geology)",
                "/wiki/Lithification",
                "/wiki/Metamorphic_rock",
                "/wiki/Mineral",
                "/wiki/Fabric_(geology)",
                "/wiki/Igneous",
                "/wiki/Sedimentary",
                "/wiki/Metamorphic",
                "/wiki/Rock_cycle",
                "/wiki/Field_work",
                "/wiki/Petrology",
                "/wiki/Geophysical_survey",
                "/wiki/Geochemistry",
                "/wiki/Physical_experiment",
                "/wiki/Computer_simulation",
                "/wiki/Mining",
                "/wiki/Petroleum_geology",
                "/wiki/Water_resources",
                "/wiki/Natural_hazard",
                "/wiki/Environmental_Geology",
                "/wiki/Climate_change",
                "/wiki/Academic_discipline",
                "/wiki/Geotechnical_engineering",
                "/wiki/Structure_of_the_Earth",
                "/wiki/Relative_dating",
                "/wiki/Geochronology",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Geologist",
                "/wiki/History_of_the_Earth",
                "/wiki/Age_of_the_Earth",
                "/wiki/Plate_tectonics",
                "/wiki/Evolutionary_history_of_life",
                "/wiki/Paleoclimatology",
                "/wiki/Ancient_Greek",
                "/wiki/Earth_science",
                "/wiki/Solid_Earth",
                "/wiki/Rock_(geology)",
                "/wiki/Terrestrial_planet",
                "/wiki/Natural_satellite",
                "/wiki/Geology_of_Mars",
                "/wiki/Geology_of_the_Moon"
            ],
            "text": "Some of the most significant advances in 20th-century geology have been the development of the theory of plate tectonics in the 1960s and the refinement of estimates of the planet's age. Plate tectonics theory arose from two separate geological observations: seafloor spreading and continental drift. The theory revolutionized the Earth sciences. Today the Earth is known to be approximately 4.5 billion years old.[74]Much of 19th-century geology revolved around the question of the Earth's exact age. Estimates varied from a few hundred thousand to billions of years.[73] By the early 20th century, radiometric dating allowed the Earth's age to be estimated at two billion years. The awareness of this vast amount of time opened the door to new theories about the processes that shaped the planet.Sir Charles Lyell first published his famous book, Principles of Geology,[72] in 1830. This book, which influenced the thought of Charles Darwin, successfully promoted the doctrine of uniformitarianism. This theory states that slow geological processes have occurred throughout the Earth's history and are still occurring today. In contrast, catastrophism is the theory that Earth's features formed in single, catastrophic events and remained unchanged thereafter. Though Hutton believed in uniformitarianism, the idea was not widely accepted at the time.The first geological map of the U.S. was produced in 1809 by William Maclure.[68][69] In 1807, Maclure commenced the self-imposed task of making a geological survey of the United States. Almost every state in the Union was traversed and mapped by him, the Allegheny Mountains being crossed and recrossed some 50 times.[70] The results of his unaided labours were submitted to the American Philosophical Society in a memoir entitled Observations on the Geology of the United States explanatory of a Geological Map, and published in the Society's Transactions, together with the nation's first geological map.[71] This antedates William Smith's geological map of England by six years, although it was constructed using a different classification of rocks.Followers of Hutton were known as Plutonists because they believed that some rocks were formed by vulcanism, which is the deposition of lava from volcanoes, as opposed to the Neptunists, led by Abraham Werner, who believed that all rocks had settled out of a large ocean whose level gradually dropped over time.James Hutton is often viewed as the first modern geologist.[67] In 1785 he presented a paper entitled Theory of the Earth to the Royal Society of Edinburgh. In his paper, he explained his theory that the Earth must be much older than had previously been supposed to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea, which in turn were raised up to become dry land. Hutton published a two-volume version of his ideas in 1795 (Vol. 1, Vol. 2).William Smith (1769\u20131839) drew some of the first geological maps and began the process of ordering rock strata (layers) by examining the fossils contained in them.[53]The word geology was first used by Ulisse Aldrovandi in 1603,[59][60] then by Jean-Andr\u00e9 Deluc in 1778[61] and introduced as a fixed term by Horace-B\u00e9n\u00e9dict de Saussure in 1779.[62][63] The word is derived from the Greek \u03b3\u1fc6, g\u00ea, meaning \"earth\" and \u03bb\u03cc\u03b3\u03bf\u03c2, logos, meaning \"speech\".[64] But according to another source, the word \"geology\" comes from a Norwegian, Mikkel Peders\u00f8n Escholt (1600\u20131699), who was a priest and scholar. Escholt first used the definition in his book titled, Geologia Norvegica (1657).[65][66]Nicolas Steno (1638\u20131686) is credited with the law of superposition, the principle of original horizontality, and the principle of lateral continuity: three defining principles of stratigraphy.Some modern scholars, such as Fielding H. Garrison, are of the opinion that the origin of the science of geology can be traced to Persia after the Muslim conquests had come to an end.[54] Abu al-Rayhan al-Biruni (973\u20131048 CE) was one of the earliest Persian geologists, whose works included the earliest writings on the geology of India, hypothesizing that the Indian subcontinent was once a sea.[55] Drawing from Greek and Indian scientific literature that were not destroyed by the Muslim conquests, the Persian scholar Ibn Sina (Avicenna, 981\u20131037) proposed detailed explanations for the formation of mountains, the origin of earthquakes, and other topics central to modern geology, which provided an essential foundation for the later development of the science.[56][57] In China, the polymath Shen Kuo (1031\u20131095) formulated a hypothesis for the process of land formation: based on his observation of fossil animal shells in a geological stratum in a mountain hundreds of miles from the ocean, he inferred that the land was formed by erosion of the mountains and by deposition of silt.[58]The study of the physical material of the Earth dates back at least to ancient Greece when Theophrastus (372\u2013287 BCE) wrote the work Peri Lithon (On Stones). During the Roman period, Pliny the Elder wrote in detail of the many minerals and metals then in practical use \u2013 even correctly noting the origin of amber.Geologists and geophysicists study natural hazards in order to enact safe building codes and warning systems that are used to prevent loss of property and life.[52] Examples of important natural hazards that are pertinent to geology (as opposed those that are mainly or only pertinent to meteorology) are:Geologists also obtain data through stratigraphy, boreholes, core samples, and ice cores. Ice cores[49] and sediment cores[50] are used to for paleoclimate reconstructions, which tell geologists about past and present temperature, precipitation, and sea level across the globe. These datasets are our primary source of information on global climate change outside of instrumental data.[51]Geology and geologic principles can be applied to various environmental problems such as stream restoration, the restoration of brownfields, and the understanding of the interaction between natural habitat and the geologic environment. Groundwater hydrology, or hydrogeology, is used to locate groundwater,[46] which can often provide a ready supply of uncontaminated water and is especially important in arid regions,[47] and to monitor the spread of contaminants in groundwater wells.[46][48]In the field of civil engineering, geological principles and analyses are used in order to ascertain the mechanical principles of the material on which structures are built. This allows tunnels to be built without collapsing, bridges and skyscrapers to be built with sturdy foundations, and buildings to be built that will not settle in clay and mud.[45]Engineering geology is the application of the geologic principles to engineering practice for the purpose of assuring that the geologic factors affecting the location, design, construction, operation, and maintenance of engineering works are properly addressed.Petroleum geologists study the locations of the subsurface of the Earth which can contain extractable hydrocarbons, especially petroleum and natural gas. Because many of these reservoirs are found in sedimentary basins,[44] they study the formation of these basins, as well as their sedimentary and tectonic evolution and the present-day positions of the rock units.Mining geology consists of the extractions of mineral resources from the Earth. Some resources of economic interests include gemstones, metals such as gold and copper, and many minerals such as asbestos, perlite, mica, phosphates, zeolites, clay, pumice, quartz, and silica, as well as elements such as sulfur, chlorine, and helium.Economic geology is an important branch of geology which deals with different aspects of economic minerals being used by humankind to fulfill its various needs. The economic minerals are those which can be extracted profitably. Economic geologists help locate and manage the Earth's natural resources, such as petroleum and coal, as well as mineral resources, which include metals such as iron, copper, and uranium.Although planetary geologists are interested in studying all aspects of other planets, a significant focus is to search for evidence of past or present life on other worlds. This has led to many missions whose primary or ancillary purpose is to examine planetary bodies for evidence of life. One of these is the Phoenix lander, which analyzed Martian polar soil for water, chemical, and mineralogical constituents related to biological processes.Although the Greek-language-origin prefix geo refers to Earth, \"geology\" is often used in conjunction with the names of other planetary bodies when describing their composition and internal processes: examples are \"the geology of Mars\" and \"Lunar geology\". Specialised terms such as selenology (studies of the Moon), areology (of Mars), etc., are also in use.With the advent of space exploration in the twentieth century, geologists have begun to look at other planetary bodies in the same ways that have been developed to study the Earth. This new field of study is called planetary geology (sometimes known as astrogeology) and relies on known geologic principles to study other bodies of the solar system.In the laboratory, biostratigraphers analyze rock samples from outcrop and drill cores for the fossils found in them.[39] These fossils help scientists to date the core and to understand the depositional environment in which the rock units formed. Geochronologists precisely date rocks within the stratigraphic section to provide better absolute bounds on the timing and rates of deposition.[43] Magnetic stratigraphers look for signs of magnetic reversals in igneous rock units within the drill cores.[39] Other scientists perform stable-isotope studies on the rocks to gain information about past climate.[39]In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores.[39] Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface.[40] Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions.[41] Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth,[42] interpret past environments, and locate areas for water, coal, and hydrocarbon extraction.Among the most well-known experiments in structural geology are those involving orogenic wedges, which are zones in which mountains are built along convergent tectonic plate boundaries.[35] In the analog versions of these experiments, horizontal layers of sand are pulled along a lower surface into a back stop, which results in realistic-looking patterns of faulting and the growth of a critically tapered (all angles remain the same) orogenic wedge.[36] Numerical models work in the same way as these analog models, though they are often more sophisticated and can include patterns of erosion and uplift in the mountain belt.[37] This helps to show the relationship between erosion and the shape of a mountain range. These studies can also give useful information about pathways for metamorphism through pressure, temperature, space, and time.[38]The analysis of structures is often accomplished by plotting the orientations of various features onto stereonets. A stereonet is a stereographic projection of a sphere onto a plane, in which planes are projected as lines and lines are projected as points. These can be used to find the locations of fold axes, relationships between faults, and relationships between other geologic structures.Structural geologists use microscopic analysis of oriented thin sections of geologic samples to observe the fabric within the rocks which gives information about strain within the crystalline structure of the rocks. They also plot and combine measurements of geological structures to better understand the orientations of faults and folds to reconstruct the history of rock deformation in the area. In addition, they perform analog and numerical experiments of rock deformation in large and small settings.Petrologists can also use fluid inclusion data[31] and perform high temperature and pressure physical experiments[32] to understand the temperatures and pressures at which different mineral phases appear, and how they change through igneous[33] and metamorphic processes. This research can be extrapolated to the field to understand metamorphic processes and the conditions of crystallization of igneous rocks.[34] This work can also help to explain processes that occur within the Earth, such as subduction and magma chamber evolution.In addition to identifying rocks in the field (lithology), petrologists identify rock samples in the laboratory. Two of the primary methods for identifying rocks in the laboratory are through optical microscopy and by using an electron microprobe. In an optical mineralogy analysis, petrologists analyze thin sections of rock samples using a petrographic microscope, where the minerals can be identified through their different properties in plane-polarized and cross-polarized light, including their birefringence, pleochroism, twinning, and interference properties with a conoscopic lens.[27] In the electron microprobe, individual locations are analyzed for their exact chemical compositions and variation in composition within individual crystals.[28] Stable[29] and radioactive isotope[30] studies provide insight into the geochemical evolution of rock units.Geological field work varies depending on the task at hand. Typical fieldwork could consist of:Geologists use a number of field, laboratory, and numerical modeling methods to decipher Earth history and to understand the processes that occur on and inside the Earth. In typical geological investigations, geologists use primary information related to petrology (the study of rocks), stratigraphy (the study of sedimentary layers), and structural geology (the study of positions of rock units and their deformation). In many cases, geologists also study modern soils, rivers, landscapes, and glaciers; investigate past and current life and biogeochemical pathways, and use geophysical methods to investigate the subsurface. Sub-specialities of geology may distinguish endogenous and exogenous geology.[20]All of these processes do not necessarily occur in a single environment, and do not necessarily occur in a single order. The Hawaiian Islands, for example, consist almost entirely of layered basaltic lava flows. The sedimentary sequences of the mid-continental United States and the Grand Canyon in the southwestern United States contain almost-undeformed stacks of sedimentary rocks that have remained in place since Cambrian time. Other areas are much more geologically complex. In the southwestern United States, sedimentary, volcanic, and intrusive rocks have been metamorphosed, faulted, foliated, and folded. Even older rocks, such as the Acasta gneiss of the Slave craton in northwestern Canada, the oldest known rock in the world have been metamorphosed to the point where their origin is undiscernable without laboratory analysis. In addition, these processes can occur in stages. In many places, the Grand Canyon in the southwestern United States being a very visible example, the lower rock units were metamorphosed and deformed, and then deformation ended and the upper, undeformed units were deposited. Although any amount of rock emplacement and rock deformation can occur, and they can occur any number of times, these concepts provide a guide to understanding the geological history of an area.The addition of new rock units, both depositionally and intrusively, often occurs during deformation. Faulting and other deformational processes result in the creation of topographic gradients, causing material on the rock unit that is increasing in elevation to be eroded by hillslopes and channels. These sediments are deposited on the rock unit that is going down. Continual motion along the fault maintains the topographic gradient in spite of the movement of sediment, and continues to create accommodation space for the material to deposit. Deformational events are often also associated with volcanism and igneous activity. Volcanic ashes and lavas accumulate on the surface, and igneous intrusions enter from below. Dikes, long, planar igneous intrusions, enter along cracks, and therefore often form in large numbers in areas that are being actively deformed. This can result in the emplacement of dike swarms, such as those that are observable across the Canadian shield, or rings of dikes around the lava tube of a volcano.Where rock units slide past one another, strike-slip faults develop in shallow regions, and become shear zones at deeper depths where the rocks deform ductilely.Extension causes the rock units as a whole to become longer and thinner. This is primarily accomplished through normal faulting and through the ductile stretching and thinning. Normal faults drop rock units that are higher below those that are lower. This typically results in younger units being placed below older units. Stretching of units can result in their thinning; in fact, there is a location within the Maria Fold and Thrust Belt in which the entire sedimentary sequence of the Grand Canyon can be seen over a length of less than a meter. Rocks at the depth to be ductilely stretched are often also metamorphosed. These stretched rocks can also pinch into lenses, known as boudins, after the French word for \"sausage\", because of their visual similarity.Even higher pressures and temperatures during horizontal shortening can cause both folding and metamorphism of the rocks. This metamorphism causes changes in the mineral composition of the rocks; creates a foliation, or planar surface, that is related to mineral growth under stress. This can remove signs of the original textures of the rocks, such as bedding in sedimentary rocks, flow features of lavas, and crystal patterns in crystalline rocks.When rock units are placed under horizontal compression, they shorten and become thicker. Because rock units, other than muds, do not significantly change in volume, this is accomplished in two primary ways: through faulting and folding. In the shallow crust, where brittle deformation can occur, thrust faults form, which causes deeper rock to move on top of shallower rock. Because deeper rock is often older, as noted by the principle of superposition, this can result in older rocks moving on top of younger ones. Movement along faults can result in folding, either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault. Deeper in the Earth, rocks behave plastically and fold instead of faulting. These folds can either be those where the material in the center of the fold buckles upwards, creating \"antiforms\", or where it buckles downwards, creating \"synforms\". If the tops of the rock units within the folds remain pointing upwards, they are called anticlines and synclines, respectively. If some of the units in the fold are facing downward, the structure is called an overturned anticline or syncline, and if all of the rock units are overturned or the correct up-direction is unknown, they are simply called by the most general terms, antiforms and synforms.After the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.Rock units are first emplaced either by deposition onto the surface or intrusion into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.The geology of an area changes through time as rock units are deposited and inserted, and deformational processes change their shapes and locations.Other methods are used for more recent events. Optically stimulated luminescence and cosmogenic radionuclide dating are used to date surfaces and/or erosion rates. Dendrochronology can also be used for the dating of landscapes. Radiocarbon dating is used for geologically young materials containing organic carbon.Fractionation of the lanthanide series elements is used to compute ages since rocks were removed from the mantle.For many geologic applications, isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature, the point at which different radiometric isotopes stop diffusing into and out of the crystal lattice.[18][19] These are used in geochronologic and thermochronologic studies. Common methods include uranium-lead dating, potassium-argon dating, argon-argon dating and uranium-thorium dating. These methods are used for a variety of applications. Dating of lava and volcanic ash layers found within a stratigraphic sequence can provide absolute age data for sedimentary rock units which do not contain radioactive isotopes and calibrate relative dating techniques. These methods can also be used to determine ages of pluton emplacement. Thermochemical techniques can be used to determine temperature profiles within the crust, the uplift of mountain ranges, and paleotopography.At the beginning of the 20th century, advancement in geological science was facilitated by the ability to obtain accurate absolute dates to geologic events using radioactive isotopes and other methods. This changed the understanding of geologic time. Previously, geologists could only use fossils and stratigraphic correlation to date sections of rock relative to one another. With isotopic dates, it became possible to assign absolute ages to rock units, and these absolute dates could be applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages.Geologists also use methods to determine the absolute age of rock samples and geological events. These dates are useful on their own and may also be used in conjunction with relative dating methods or to calibrate relative methods.[17]The principle of faunal succession is based on the appearance of fossils in sedimentary rocks. As organisms exist at the same time period throughout the world, their presence or (sometimes) absence may be used to provide a relative age of the formations in which they are found. Based on principles laid out by William Smith almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession were developed independently of evolutionary thought. The principle becomes quite complex, however, given the uncertainties of fossilization, the localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils may be found globally at the same time.[16]The principle of superposition states that a sedimentary rock layer in a tectonically undisturbed sequence is younger than the one beneath it and older than the one above it. Logically a younger layer cannot slip beneath a layer previously deposited. This principle allows sedimentary layers to be viewed as a form of vertical time line, a partial or complete record of the time elapsed from deposition of the lowest layer to deposition of the highest bed.[15]The principle of original horizontality states that the deposition of sediments occurs as essentially horizontal beds. Observation of modern marine and non-marine sediments in a wide variety of environments supports this generalization (although cross-bedding is inclined, the overall orientation of cross-bedded units is horizontal).[15]The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock which contains them.The principle of cross-cutting relationships pertains to the formation of faults and the age of the sequences through which they cut. Faults are younger than the rocks they cut; accordingly, if a fault is found that penetrates some formations but not those on top of it, then the formations that were cut are older than the fault, and the ones that are not cut must be younger than the fault. Finding the key bed in these situations may help determine whether the fault is a normal fault or a thrust fault.[15]The principle of intrusive relationships concerns crosscutting intrusions. In geology, when an igneous intrusion cuts across a formation of sedimentary rock, it can be determined that the igneous intrusion is younger than the sedimentary rock. Different types of intrusions include stocks, laccoliths, batholiths, sills and dikes.The principle of uniformitarianism states that the geologic processes observed in operation that modify the Earth's crust at present have worked in much the same way over geologic time.[13] A fundamental principle of geology advanced by the 18th century Scottish physician and geologist James Hutton is that \"the present is the key to the past.\" In Hutton's words: \"the past history of our globe must be explained by what can be seen to be happening now.\"[14]Methods for relative dating were developed when geology first emerged as a natural science. Geologists still use the following principles today as a means to provide information about geologic history and the timing of geologic events.The following four timelines show the geologic time scale. The first shows the entire time from the formation of the Earth to the present, but this gives little space for the most recent eon. Therefore, the second timeline shows an expanded view of the most recent eon. In a similar way, the most recent era is expanded in the third timeline, and the most recent period is expanded in the fourth timeline.The geologic time scale encompasses the history of the Earth.[9] It is bracketed at the earliest by the dates of the first Solar System material at 4.567 Ga[10] (or 4.567 billion years ago) and the formation of the Earth at 4.54 Ga[11][12] (4.54 billion years), which is the beginning of the informally recognized Hadean eon\u00a0\u2013 a division of geologic time. At the later end of the scale, it is marked by the present day (in the Holocene epoch).Mineralogists have been able to use the pressure and temperature data from the seismic and modelling studies alongside knowledge of the elemental composition of the Earth to reproduce these conditions in experimental settings and measure changes in crystal structure. These studies explain the chemical changes associated with the major seismic discontinuities in the mantle and show the crystallographic structures expected in the inner core of the Earth.Seismologists can use the arrival times of seismic waves in reverse to image the interior of the Earth. Early advances in this field showed the existence of a liquid outer core (where shear waves were not able to propagate) and a dense solid inner core. These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. More recently, seismologists have been able to create detailed images of wave speeds inside the earth in the same way a doctor images a body in a CT scan. These images have led to a much more detailed view of the interior of the Earth, and have replaced the simplified layered model with a much more dynamic model.Advances in seismology, computer modeling, and mineralogy and crystallography at high temperatures and pressures give insights into the internal composition and structure of the Earth.Transform boundaries, such as the San Andreas Fault system, resulted in widespread powerful earthquakes. Plate tectonics also has provided a mechanism for Alfred Wegener's theory of continental drift,[8] in which the continents move across the surface of the Earth over geologic time. They also provided a driving force for crustal deformation, and a new setting for the observations of structural geology. The power of the theory of plate tectonics lies in its ability to combine all of these observations into a single theory of how the lithosphere moves over the convecting mantle.The development of plate tectonics has provided a physical basis for many observations of the solid Earth. Long linear regions of geologic features are explained as plate boundaries.[7] For example:There is an intimate coupling between the movement of the plates on the surface and the convection of the mantle (that is, the heat transfer caused by bulk movement of molecules within fluids). Thus, oceanic plates and the adjoining mantle convection currents always move in the same direction \u2014 because the oceanic lithosphere is actually the rigid upper thermal boundary layer of the convecting mantle. This coupling between rigid plates moving on the surface of the Earth and the convecting mantle is called plate tectonics.In the 1960s, it was discovered that the Earth's lithosphere, which includes the crust and rigid uppermost portion of the upper mantle, is separated into tectonic plates that move across the plastically deforming, solid, upper mantle, which is called the asthenosphere. This theory is supported by several types of observations, including seafloor spreading[5][6] and the global distribution of mountain terrain and seismicity.Geologists also study unlithified materials (referred to as drift), which typically come from more recent deposits. These materials are superficial deposits which lie above the bedrock.[4] This study is often known as Quaternary geology, after the Quaternary period of geologic history.To study all three types of rock, geologists evaluate the minerals of which they are composed. Each mineral has distinct physical properties, and there are many tests to determine each of them. The specimens can be tested for:When a rock crystallizes from melt (magma or lava), it is an igneous rock. This rock can be weathered and eroded, then redeposited and lithified into a sedimentary rock. It can then be turned into a metamorphic rock by heat and pressure that change its mineral content, resulting in a characteristic fabric. All three types may melt again, and when this happens, new magma is formed, from which an igneous rock may once more crystallize.The majority of research in geology is associated with the study of rock, as rock provides the primary record of the majority of the geologic history of the Earth. There are three major types of rock: igneous, sedimentary, and metamorphic. The rock cycle illustrates the relationships among them (see diagram).The majority of geological data comes from research on solid Earth materials. These typically fall into one of two categories: rock and unconsolidated material.Geologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding of natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology, a major academic discipline, also plays a role in geotechnical engineering.Geology describes the structure of the Earth beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks.[citation needed] By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.Geology (from the Ancient Greek \u03b3\u1fc6, g\u0113, i.e. \"earth\" and -\u03bbo\u03b3\u03af\u03b1, -logia, i.e. \"study of, discourse\"[1][2]) is an earth science concerned with the solid Earth, the rocks of which it is composed, and the processes by which they change over time. Geology can also refer to the study of the solid features of any terrestrial planet or natural satellite, (such as Mars or the Moon).",
            "title": "Geology",
            "url": "https://en.wikipedia.org/wiki/Geology"
        },
        {
            "desc_links": [
                "/wiki/Fluvial",
                "/wiki/Erosion",
                "/wiki/Glacial_motion",
                "/wiki/Glacier",
                "/wiki/Deposition_(geology)",
                "/wiki/Terminal_moraine",
                "/wiki/Moraine#Lateral_moraines",
                "/wiki/Moraine#Medial_moraine",
                "/wiki/Moraine#Ground_moraines",
                "/wiki/Sorting_(sediment)",
                "/wiki/Glacier"
            ],
            "links": [
                "/wiki/Aleksis_Dreimanis",
                "/wiki/Lithification",
                "/wiki/Sedimentary_rocks",
                "/wiki/Atlantic_Ocean",
                "/wiki/Continental_drift",
                "/wiki/Precambrian",
                "/wiki/Snowball_Earth",
                "/wiki/Gemstone",
                "/wiki/Ore",
                "/wiki/Diamond",
                "/wiki/U.S._state",
                "/wiki/Wisconsin",
                "/wiki/Indiana",
                "/wiki/Canada",
                "/wiki/Prospecting",
                "/wiki/Kimberlite",
                "/wiki/Terminal_moraine",
                "/wiki/Moraine#Lateral_moraines",
                "/wiki/Moraine#Medial_moraine",
                "/wiki/Moraine#Ground_moraines",
                "/wiki/Continental_glacier",
                "/wiki/Outwash_plain",
                "/wiki/River",
                "/wiki/Varve",
                "/wiki/Proglacial_lake",
                "/wiki/Deposition_(geology)",
                "/wiki/Sedimentary_structures",
                "/wiki/Matrix_(geology)",
                "/wiki/Sediment",
                "/wiki/Clay",
                "/wiki/Sand",
                "/wiki/Gravel",
                "/wiki/Boulder",
                "/wiki/Erosion",
                "/wiki/Glacial_motion",
                "/wiki/Glacier",
                "/wiki/Plucking_(glaciation)",
                "/wiki/Abrasion_(geology)",
                "/wiki/Clasts",
                "/wiki/Fluvial",
                "/wiki/Erosion",
                "/wiki/Glacial_motion",
                "/wiki/Glacier",
                "/wiki/Deposition_(geology)",
                "/wiki/Terminal_moraine",
                "/wiki/Moraine#Lateral_moraines",
                "/wiki/Moraine#Medial_moraine",
                "/wiki/Moraine#Ground_moraines",
                "/wiki/Sorting_(sediment)",
                "/wiki/Glacier"
            ],
            "text": "Van der Meer et al. 2003[2] have suggested that these till classifications are outdated and should instead be replaced with only one classification, that of deformation till. The reasons behind this are largely down to the difficulties in accurately classifying different tills, which are often based on inferences of the physical setting of the till rather than detailed analysis of the till fabric or particle size.Traditionally (e.g. Dreimanis, 1988[1]) a further set of divisions has been made to primary deposits, based upon the method of deposition.There are various types of classifying tills:In cases where till has been indurated or lithified by subsequent burial into solid rock, it is known as the sedimentary rock tillite. Matching beds of ancient tillites on opposite sides of the south Atlantic Ocean provided early evidence for continental drift. The same tillites also provide some support to the Precambrian Snowball Earth glaciation event hypothesis.Till may contain detectable concentrations of gems or other valuable ore minerals picked up by the glacier during its advance, for example the diamonds found in the U.S. states of Wisconsin, Indiana, and in Canada. Prospectors use trace minerals in tills as clues to follow the glacier upstream to find kimberlite diamond deposits and other types of ore deposits.Till is deposited at the terminal moraine, along the lateral and medial moraines and in the ground moraine of a glacier. As a glacier melts, especially a continental glacier, large amounts of till are washed away and deposited as outwash in sandurs by the rivers flowing from the glacier, and as varves (annual layers) in any proglacial lakes which may form.Eventually, the sedimentary assemblage forming this bed will be abandoned some distance down-ice from its various sources. This is the process of glacial till deposition. When this deposition occurs at the base of the moving ice of a glacier, the sediment is called lodgement till. Rarely, eroded unconsolidated sediments can be preserved in the till along with their original sedimentary structures. More commonly, these sediments lose their original structure through the mixture processes associated with subglacial transport and they solely contribute to form the more or less uniform matrix of the till.Glacial drift is the coarsely graded and extremely heterogeneous sediment of a glacier; till is the part of glacial drift deposited directly by the glacier. Its content may vary from clays to mixtures of clay, sand, gravel, and boulders. This material is mostly derived from the subglacial erosion and entrainment by the moving ice of the glaciers of previously available unconsolidated sediments. Bedrock can also be eroded through the action of glacial plucking and abrasion and the resulting clasts of various sizes will be incorporated to the glacier's bed.Till is classified into primary deposits, laid down directly by glaciers, and secondary deposits, reworked by fluvial transport and other processes.Till is derived from the erosion and entrainment of material by the moving ice of a glacier. It is deposited some distance down-ice to form terminal, lateral, medial and ground moraines.Till or glacial till is unsorted glacial sediment.",
            "title": "Till",
            "url": "https://en.wikipedia.org/wiki/Glacial_till"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Clastic rock",
            "url": "https://en.wikipedia.org/wiki/Clasts"
        },
        {
            "desc_links": [
                "/wiki/Quantum_chemistry",
                "/wiki/Quantum_optics",
                "/wiki/Quantum_computing",
                "/wiki/Superconducting_magnet",
                "/wiki/Light-emitting_diode",
                "/wiki/Laser",
                "/wiki/Transistor",
                "/wiki/Semiconductor",
                "/wiki/Microprocessor",
                "/wiki/Medical_imaging",
                "/wiki/Magnetic_resonance_imaging",
                "/wiki/Electron_microscope",
                "/wiki/History_of_quantum_mechanics",
                "/wiki/Max_Planck",
                "/wiki/Black-body_radiation",
                "/wiki/Albert_Einstein",
                "/wiki/Annus_Mirabilis_papers#Photoelectric_effect",
                "/wiki/Photoelectric_effect",
                "/wiki/Old_quantum_theory",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Werner_Heisenberg",
                "/wiki/Max_Born",
                "/wiki/Mathematical_formulations_of_quantum_mechanics",
                "/wiki/Wave_function",
                "/wiki/Probability_amplitude",
                "/wiki/Classical_physics",
                "/wiki/Classical_physics",
                "/wiki/Energy",
                "/wiki/Momentum",
                "/wiki/Discrete_mathematics",
                "/wiki/Quantization_(physics)",
                "/wiki/Particle",
                "/wiki/Wave",
                "/wiki/Wave-particle_duality",
                "/wiki/Uncertainty_principle",
                "/wiki/Quantum_field_theory",
                "/wiki/Physics",
                "/wiki/Nature",
                "/wiki/Energy",
                "/wiki/Atoms",
                "/wiki/Subatomic_particle"
            ],
            "links": [
                "/wiki/Boundary_condition",
                "/wiki/Derivative",
                "/wiki/Wave_vector",
                "/wiki/Hermite_polynomials",
                "/wiki/Eigenstate",
                "/wiki/Quantum_tunneling",
                "/wiki/Flash_memory",
                "/wiki/Scanning_tunneling_microscope",
                "/wiki/Superlattice",
                "/wiki/Pi",
                "/wiki/Euler%27s_formula",
                "/wiki/Kinetic_energy#Kinetic_energy_of_rigid_bodies",
                "/wiki/Free_particle",
                "/wiki/Wave%E2%80%93particle_duality",
                "/wiki/Wave",
                "/wiki/Wave_function",
                "/wiki/Observables",
                "/wiki/Uncertainty_Principle",
                "/wiki/Dirac_delta",
                "/wiki/Distribution_(mathematics)",
                "/wiki/Plane_wave",
                "/wiki/Wavelength",
                "/wiki/Planck%27s_constant",
                "/wiki/Eigenstate",
                "/wiki/Black-body_radiation",
                "/wiki/Atomic_orbital",
                "/wiki/Biological_systems",
                "/wiki/Smell_receptors",
                "/wiki/Protein_structure",
                "/wiki/Photosynthesis",
                "/wiki/Classical_physics",
                "/wiki/Quantum_number",
                "/wiki/Mechanics#Classical_versus_quantum",
                "/wiki/Superfluidity",
                "/wiki/Absolute_zero",
                "/wiki/Superconductivity",
                "/wiki/Electric_current",
                "/wiki/Fractional_quantum_Hall_effect",
                "/wiki/Topological_order",
                "/wiki/Quantum_entanglement",
                "/wiki/Quantum_teleportation",
                "/wiki/Quantum_computer",
                "/wiki/Computer",
                "/wiki/Qubits",
                "/wiki/Quantum_superposition",
                "/wiki/Integer_factorization",
                "/wiki/IBM",
                "/wiki/Artificial_intelligence",
                "/wiki/Cryptography",
                "/wiki/Eavesdropping",
                "/wiki/Observer_effect_(physics)",
                "/wiki/Eigenstate",
                "/wiki/Quantum_cryptography",
                "/wiki/Information",
                "/wiki/Quantum_tunneling",
                "/wiki/Light_switch",
                "/wiki/Flash_memory",
                "/wiki/USB_flash_drive",
                "/wiki/Resonant_tunneling_diode",
                "/wiki/Potential_barrier",
                "/wiki/Fermi_level",
                "/wiki/Laser",
                "/wiki/Transistor",
                "/wiki/Integrated_circuit",
                "/wiki/Electron_microscope",
                "/wiki/Magnetic_resonance_imaging",
                "/wiki/Semiconductor",
                "/wiki/Diode",
                "/wiki/Transistor",
                "/wiki/Electronics",
                "/wiki/Computer",
                "/wiki/Telecommunication",
                "/wiki/Light_emitting_diode",
                "/wiki/Molecules",
                "/wiki/Chemistry",
                "/wiki/Ionic_bond",
                "/wiki/Covalent_bonding",
                "/wiki/Computational_chemistry",
                "/wiki/Subatomic_particle",
                "/wiki/Electron",
                "/wiki/Proton",
                "/wiki/Neutron",
                "/wiki/Photon",
                "/wiki/String_theory",
                "/wiki/Theory_of_everything",
                "/wiki/Reductionism",
                "/wiki/Everett_many-worlds_interpretation",
                "/wiki/Multiverse",
                "/wiki/Quantum_superposition",
                "/wiki/Quantum_entanglement",
                "/wiki/John_Stewart_Bell",
                "/wiki/Quantum_decoherence",
                "/wiki/Quantum_entanglement",
                "/wiki/Photon",
                "/wiki/Speed_of_light",
                "/wiki/Bell_test",
                "/wiki/Transactional_interpretation",
                "/wiki/Relational_quantum_mechanics",
                "/wiki/Copenhagen_Interpretation",
                "/wiki/Causality",
                "/wiki/Quantum_cryptography",
                "/wiki/John_Stewart_Bell",
                "/wiki/Bell%27s_theorem",
                "/wiki/Bell_test_experiments",
                "/wiki/Determinism",
                "/wiki/Causality",
                "/wiki/Action_at_a_distance",
                "/wiki/Principle_of_locality",
                "/wiki/Bohr-Einstein_debates",
                "/wiki/Epistemological",
                "/wiki/Einstein%E2%80%93Podolsky%E2%80%93Rosen_paradox",
                "/wiki/Copenhagen_interpretation",
                "/wiki/Complementarity_(physics)",
                "/wiki/Counter-intuitive",
                "/wiki/Philosophy",
                "/wiki/Interpretations_of_quantum_mechanics",
                "/wiki/Max_Born",
                "/wiki/Born_rule",
                "/wiki/Probability_amplitude",
                "/wiki/Probability_distribution",
                "/wiki/Richard_Feynman",
                "/wiki/Steven_Weinberg",
                "/wiki/Loop_quantum_gravity",
                "/wiki/Carlo_Rovelli",
                "/wiki/Chronon",
                "/wiki/Gravity",
                "/wiki/General_relativity",
                "/wiki/Spin_networks",
                "/wiki/Planck_length",
                "/wiki/Planck_scale",
                "/wiki/Fundamental_force",
                "/wiki/Quantum_electrodynamics",
                "/wiki/Electroweak_force",
                "/wiki/Electrostrong_force",
                "/wiki/General_relativity",
                "/wiki/Edward_Witten",
                "/wiki/M-theory",
                "/wiki/String_theory",
                "/wiki/4-dimensional_spacetime",
                "/wiki/Quantum_gravity",
                "/wiki/Physical_cosmology",
                "/wiki/Theory_of_everything",
                "/wiki/Stephen_Hawking",
                "/wiki/Fundamental_interaction",
                "/wiki/Strong_interaction",
                "/wiki/Electromagnetism",
                "/wiki/Weak_interaction",
                "/wiki/Gravity",
                "/wiki/G%C3%B6del%27s_Incompleteness_Theorem",
                "/wiki/Empirical_evidence",
                "/wiki/Hamiltonian_mechanics",
                "/wiki/Lagrangian_mechanics",
                "/wiki/Planck%27s_constant",
                "/wiki/Niels_Bohr",
                "/wiki/Quantum_coherence",
                "/wiki/EPR_paradox",
                "/wiki/Local_realism",
                "/wiki/Quantum_interference#Quantum_interference",
                "/wiki/Probability_amplitude",
                "/wiki/Coherence_length",
                "/wiki/Absolute_zero",
                "/wiki/Accuracy",
                "/wiki/Correspondence_principle",
                "/wiki/Quantum_number",
                "/wiki/Chaos_theory",
                "/wiki/Quantum_chaos",
                "/wiki/Complex_domain",
                "/wiki/Gravity",
                "/wiki/Fundamental_force",
                "/wiki/Hawking_radiation",
                "/wiki/Quantum_gravity",
                "/wiki/General_relativity",
                "/wiki/String_theory",
                "/wiki/Field_(physics)",
                "/wiki/Strong_nuclear_force",
                "/wiki/Weak_nuclear_force",
                "/wiki/Quantum_chromodynamics",
                "/wiki/Quark",
                "/wiki/Gluon",
                "/wiki/Electromagnetic_force",
                "/wiki/Electroweak_theory",
                "/wiki/Abdus_Salam",
                "/wiki/Sheldon_Glashow",
                "/wiki/Steven_Weinberg",
                "/wiki/Theory_of_relativity",
                "/wiki/Classical_mechanics",
                "/wiki/Quantum_harmonic_oscillator",
                "/wiki/Kinetic_energy",
                "/wiki/Harmonic_oscillator",
                "/wiki/Hilbert_space",
                "/wiki/Inner_product",
                "/wiki/Hermitian_operators",
                "/wiki/Correspondence_principle",
                "/wiki/Werner_Heisenberg",
                "/wiki/Nobel_Prize_in_Physics",
                "/wiki/Max_Born",
                "/wiki/Festschrift",
                "/wiki/Max_Planck",
                "/wiki/Observable",
                "/wiki/Energy",
                "/wiki/Position_operator",
                "/wiki/Momentum_operator",
                "/wiki/Angular_momentum",
                "/wiki/Continuous_function",
                "/wiki/Discrete_mathematics",
                "/wiki/Feynman",
                "/wiki/Path_integral_formulation",
                "/wiki/Action_principle",
                "/wiki/Transformation_theory_(quantum_mechanics)",
                "/wiki/Paul_Dirac",
                "/wiki/Matrix_mechanics",
                "/wiki/Werner_Heisenberg",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Perturbation_theory_(quantum_mechanics)",
                "/wiki/Potential_energy",
                "/wiki/Quantum_chaos",
                "/wiki/Phase_(waves)",
                "/wiki/Interference_(wave_propagation)",
                "/wiki/List_of_quantum-mechanical_systems_with_analytical_solutions",
                "/wiki/Quantum_harmonic_oscillator",
                "/wiki/Particle_in_a_box",
                "/wiki/Dihydrogen_cation",
                "/wiki/Hydrogen_atom",
                "/wiki/Helium",
                "/wiki/Eigenstate#Schr\u00f6dinger_equation",
                "/wiki/Electron",
                "/wiki/Atom",
                "/wiki/Atomic_nucleus",
                "/wiki/Spherical_coordinate_system",
                "/wiki/File:HAtomOrbitals.png",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Newton%27s_second_law",
                "/wiki/Classical_mechanics",
                "/wiki/Quantum_measurement",
                "/wiki/Random",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hamiltonian_(quantum_mechanics)",
                "/wiki/Operator_(physics)",
                "/wiki/Total_energy",
                "/wiki/Time_evolution",
                "/wiki/Determinism",
                "/wiki/Conjugate_variables",
                "/wiki/Uncertainty_principle",
                "/wiki/Eigenstate",
                "/wiki/Wave_function_collapse",
                "/wiki/Wave_packet",
                "/wiki/Probability_distribution",
                "/wiki/Bohr_model",
                "/wiki/Probability_function",
                "/wiki/Wave_function",
                "/wiki/Eigenvalue",
                "/wiki/Probability_amplitude",
                "/wiki/Quantum_state",
                "/wiki/Eigenstate",
                "/wiki/German_language",
                "/wiki/Probability",
                "/wiki/Bohr%E2%80%93Einstein_debates",
                "/wiki/Thought_experiment",
                "/wiki/Interpretation_of_quantum_mechanics",
                "/wiki/Relative_state_interpretation",
                "/wiki/Quantum_Entanglement",
                "/wiki/Measurement_in_quantum_mechanics",
                "/wiki/Spectral_theorem",
                "/wiki/Uncertainty_principle",
                "/wiki/Commutator",
                "/wiki/Complex_number",
                "/wiki/Wave_function",
                "/wiki/Vector_space",
                "/wiki/Probability",
                "/wiki/Conjugate_variables",
                "/wiki/Uncertainty_principle",
                "/wiki/Paul_Dirac",
                "/wiki/David_Hilbert",
                "/wiki/John_von_Neumann",
                "/wiki/Hermann_Weyl",
                "/wiki/Unit_vector",
                "/wiki/Complex_number",
                "/wiki/Separable_space",
                "/wiki/Hilbert_space",
                "/wiki/State_space_(physics)",
                "/wiki/Projective_space",
                "/wiki/Complex_projective_space",
                "/wiki/Square-integrable",
                "/wiki/Hermitian_adjoint",
                "/wiki/Self-adjoint_operator",
                "/wiki/Operator_(physics)",
                "/wiki/Eigenstate",
                "/wiki/Eigenvector",
                "/wiki/Eigenvalue",
                "/wiki/Spectrum",
                "/wiki/Isotope",
                "/wiki/Chemical_element",
                "/wiki/Atom",
                "/wiki/Classical_mechanics",
                "/wiki/Circular_motion",
                "/wiki/Probability",
                "/wiki/Atomic_orbital",
                "/wiki/Electromagnetism",
                "/wiki/Latin_language",
                "/wiki/Physical_quantity",
                "/wiki/Energy",
                "/wiki/Atom",
                "/wiki/Mathematical",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Condensed_matter_physics",
                "/wiki/Solid-state_physics",
                "/wiki/Atomic_physics",
                "/wiki/Molecular_physics",
                "/wiki/Computational_physics",
                "/wiki/Computational_chemistry",
                "/wiki/Particle_physics",
                "/wiki/Nuclear_chemistry",
                "/wiki/Nuclear_physics",
                "/wiki/Wikipedia:NOTRS",
                "/wiki/Macroscopic",
                "/wiki/Superconductivity",
                "/wiki/Superfluid",
                "/wiki/David_Hilbert",
                "/wiki/Paul_Dirac",
                "/wiki/John_von_Neumann",
                "/wiki/Measurement_in_quantum_mechanics",
                "/wiki/Interpretations_of_quantum_mechanics",
                "/wiki/Quantum_electronics",
                "/wiki/Quantum_optics",
                "/wiki/Quantum_information_science",
                "/wiki/String_theory",
                "/wiki/Quantum_gravity",
                "/wiki/Periodic_table",
                "/wiki/Atoms",
                "/wiki/Chemical_bond",
                "/wiki/Electron",
                "/wiki/Semiconductor",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Subatomic_particles",
                "/wiki/Wave%E2%80%93particle_duality",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Photon",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Quantum_physics",
                "/wiki/Solvay_Conference",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Max_Planck",
                "/wiki/Niels_Bohr",
                "/wiki/Werner_Heisenberg",
                "/wiki/Louis_de_Broglie",
                "/wiki/Arthur_Compton",
                "/wiki/Albert_Einstein",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Max_Born",
                "/wiki/John_von_Neumann",
                "/wiki/Paul_Dirac",
                "/wiki/Enrico_Fermi",
                "/wiki/Wolfgang_Pauli",
                "/wiki/Max_von_Laue",
                "/wiki/Freeman_Dyson",
                "/wiki/David_Hilbert",
                "/wiki/Wilhelm_Wien",
                "/wiki/Satyendra_Nath_Bose",
                "/wiki/Arnold_Sommerfeld",
                "/wiki/Category:Quantum_physicists",
                "/wiki/Copenhagen_interpretation",
                "/wiki/Niels_Bohr",
                "/wiki/Electromagnetic_wave",
                "/wiki/Photon",
                "/wiki/Albert_Einstein",
                "/wiki/Local_realism",
                "/wiki/Photoelectric_effect",
                "/wiki/Planck_constant",
                "/wiki/Frequency",
                "/wiki/Arthur_Compton",
                "/wiki/C._V._Raman",
                "/wiki/Pieter_Zeeman",
                "/wiki/Robert_Andrews_Millikan",
                "/wiki/Photoelectric_effect",
                "/wiki/Ernest_Rutherford",
                "/wiki/Niels_Bohr",
                "/wiki/Henry_Moseley",
                "/wiki/Peter_Debye",
                "/wiki/Elliptical_orbit",
                "/wiki/Arnold_Sommerfeld",
                "/wiki/Old_quantum_theory",
                "/wiki/Max_Planck",
                "/wiki/Albert_Einstein",
                "/wiki/Photoelectric_effect",
                "/wiki/Atomic_theory",
                "/wiki/Corpuscular_theory_of_light",
                "/wiki/Matter",
                "/wiki/Electromagnetic_radiation",
                "/wiki/Wilhelm_Wien",
                "/wiki/Wien_approximation",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Planck%27s_law",
                "/wiki/Michael_Faraday",
                "/wiki/Cathode_ray",
                "/wiki/Black-body_radiation",
                "/wiki/Gustav_Kirchhoff",
                "/wiki/Ludwig_Boltzmann",
                "/wiki/Max_Planck",
                "/wiki/Robert_Hooke",
                "/wiki/Christiaan_Huygens",
                "/wiki/Leonhard_Euler",
                "/wiki/Thomas_Young_(scientist)",
                "/wiki/Polymath",
                "/wiki/Young%27s_interference_experiment",
                "/wiki/Wave_theory_of_light",
                "/wiki/Quantum_chemistry",
                "/wiki/Quantum_optics",
                "/wiki/Quantum_computing",
                "/wiki/Superconducting_magnet",
                "/wiki/Light-emitting_diode",
                "/wiki/Laser",
                "/wiki/Transistor",
                "/wiki/Semiconductor",
                "/wiki/Microprocessor",
                "/wiki/Medical_imaging",
                "/wiki/Magnetic_resonance_imaging",
                "/wiki/Electron_microscope",
                "/wiki/History_of_quantum_mechanics",
                "/wiki/Max_Planck",
                "/wiki/Black-body_radiation",
                "/wiki/Albert_Einstein",
                "/wiki/Annus_Mirabilis_papers#Photoelectric_effect",
                "/wiki/Photoelectric_effect",
                "/wiki/Old_quantum_theory",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Werner_Heisenberg",
                "/wiki/Max_Born",
                "/wiki/Mathematical_formulations_of_quantum_mechanics",
                "/wiki/Wave_function",
                "/wiki/Probability_amplitude",
                "/wiki/Classical_physics",
                "/wiki/Classical_physics",
                "/wiki/Energy",
                "/wiki/Momentum",
                "/wiki/Discrete_mathematics",
                "/wiki/Quantization_(physics)",
                "/wiki/Particle",
                "/wiki/Wave",
                "/wiki/Wave-particle_duality",
                "/wiki/Uncertainty_principle",
                "/wiki/Quantum_field_theory",
                "/wiki/Physics",
                "/wiki/Nature",
                "/wiki/Energy",
                "/wiki/Atoms",
                "/wiki/Subatomic_particle"
            ],
            "text": "Each term of the solution can be interpreted as an incident, reflected, or transmitted component of the wave, allowing the calculation of transmission and reflection coefficients. Notably, in contrast to classical mechanics, incident particles with energies greater than the potential step are partially reflected.andwith coefficients A and B determined from the boundary conditions and by imposing a continuous derivative on the solution, and where the wave vectors are related to the energy viaandThe solutions are superpositions of left- and right-moving waves:The potential in this case is given by:This is another example illustrating the quantification of energy for bound states.and the corresponding energy levels arewhere Hn are the Hermite polynomialsThis problem can either be treated by directly solving the Schr\u00f6dinger equation, which is not trivial, or by using the more elegant \"ladder method\" first proposed by Paul Dirac. The eigenstates are given byAs in the classical case, the potential for the quantum harmonic oscillator is given byThis is a model for the quantum tunneling effect which plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy. Quantum tunneling is central to physical phenomena involved in superlattices.The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well.A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth.The quantization of energy levels follows from this constraint on k, sincein which C cannot be zero as this would conflict with the Born interpretation. Therefore, since sin(kL) = 0, kL must be an integer multiple of \u03c0,and D = 0. At x = L,The infinite potential walls of the box determine the values of C, D, and k at x = 0 and x = L where \u03c8 must be zero. Thus, at x = 0,or, from Euler's formula,The general solutions of the Schr\u00f6dinger equation for the particle in a box arethe previous equation is evocative of the classic kinetic energy analogue,With the differential operator defined byFor example, consider a free particle. In quantum mechanics, a free matter is described by a wave function. The particle properties of the matter become apparent when we measure its position and velocity. The wave properties of the matter become apparent when we measure its wave properties like interference. The wave\u2013particle duality feature is incorporated in the relations of coordinates and operators in the formulation of quantum mechanics. Since the matter is free (not subject to any interactions), its quantum state can be represented as a wave of arbitrary shape and extending over space as a wave function. The position and momentum of the particle are observables. The Uncertainty Principle states that both the position and the momentum cannot simultaneously be measured with complete precision. However, one can measure the position (alone) of a moving free particle, creating an eigenstate of position with a wave function that is very large (a Dirac delta) at a particular position x, and zero everywhere else. If one performs a position measurement on such a wave function, the resultant x will be obtained with 100% probability (i.e., with full certainty, or complete precision). This is called an eigenstate of position\u2014or, stated in mathematical terms, a generalized position eigenstate (eigendistribution). If the particle is in an eigenstate of position, then its momentum is completely unknown. On the other hand, if the particle is in an eigenstate of momentum, then its position is completely unknown.[86] In an eigenstate of momentum having a plane wave form, it can be shown that the wavelength is equal to h/p, where h is Planck's constant and p is the momentum of the eigenstate.[87]Quantum theory also provides accurate descriptions for many previously unexplained phenomena, such as black-body radiation and the stability of the orbitals of electrons in atoms. It has also given insight into the workings of many different biological systems, including smell receptors and protein structures.[84] Recent work on photosynthesis has provided evidence that quantum correlations play an essential role in this fundamental process of plants and many other organisms.[85] Even so, classical physics can often provide good approximations to results otherwise obtained by quantum physics, typically in circumstances with large numbers of particles or large quantum numbers. Since classical formulas are much simpler and easier to compute than quantum formulas, classical approximations are used and preferred when the system is large enough to render the effects of quantum mechanics insignificant.While quantum mechanics primarily applies to the smaller atomic regimes of matter and energy, some systems exhibit quantum mechanical effects on a large scale. Superfluidity, the frictionless flow of a liquid at temperatures near absolute zero, is one well-known example. So is the closely related phenomenon of superconductivity, the frictionless flow of an electron gas in a conducting material (an electric current) at sufficiently low temperatures. The fractional quantum Hall effect is a topological ordered state which corresponds to patterns of long-range quantum entanglement.[83] States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.Another active research topic is quantum teleportation, which deals with techniques to transmit quantum information over arbitrary distances.A more distant goal is the development of quantum computers, which are expected to perform certain computational tasks exponentially faster than classical computers. Instead of using classical bits, quantum computers use qubits, which can be in superpositions of states. Quantum programmers are able to manipulate the superposition of qubits in order to solve problems that classical computing cannot do effectively, such as searching unsorted databases or integer factorization. IBM claims that the advent of quantum computing may progress the fields of medicine, logistics, financial services, artificial intelligence and cloud security.[82]An inherent advantage yielded by quantum cryptography when compared to classical cryptography is the detection of passive eavesdropping. This is a natural result of the behavior of quantum bits; due to the observer effect, if a bit in a superposition state were to be observed, the superposition state would collapse into an eigenstate. Because the intended recipient was expecting to receive the bit in a superposition state, the intended recipient would know there was an attack, because the bit's state would no longer be in a superposition.[81]Researchers are currently seeking robust methods of directly manipulating quantum states. Efforts are being made to more fully develop quantum cryptography, which will theoretically allow guaranteed secure transmission of information.Many electronic devices operate under effect of quantum tunneling. It even exists in the simple light switch. The switch would not work if electrons could not quantum tunnel through the layer of oxidation on the metal contact surfaces. Flash memory chips found in USB drives use quantum tunneling to erase their memory cells. Some negative differential resistance devices also utilize quantum tunneling effect, such as resonant tunneling diode. Unlike classical diodes, its current is carried by resonant tunneling through two or more potential barriers (see right figure). Its negative resistance behavior can only be understood with quantum mechanics: As the confined state moves close to Fermi level, tunnel current increases. As it moves away, current decreases. Quantum mechanics is necessary to understanding and designing such electronic devices.Many modern electronic devices are designed using quantum mechanics. Examples include the laser, the transistor (and thus the microchip), the electron microscope, and magnetic resonance imaging (MRI). The study of semiconductors led to the invention of the diode and the transistor, which are indispensable parts of modern electronics systems, computer and telecommunication devices. Another application is for making laser diode and light emitting diode which are a high-efficiency source of light.In many aspects modern technology operates at a scale where quantum effects are significant.Quantum mechanics is also critically important for understanding how individual atoms are joined by covalent bond to form molecules. The application of quantum mechanics to chemistry is known as quantum chemistry. Quantum mechanics can also provide quantitative insight into ionic and covalent bonding processes by explicitly showing which molecules are energetically favorable to which others and the magnitudes of the energies involved.[80] Furthermore, most of the calculations performed in modern computational chemistry rely on quantum mechanics.Quantum mechanics has had enormous[79] success in explaining many of the features of our universe. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Quantum mechanics has strongly influenced string theories, candidates for a Theory of Everything (see reductionism).The Everett many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[77] This is not accomplished by introducing some \"new axiom\" to quantum mechanics, but on the contrary, by removing the axiom of the collapse of the wave packet. All of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a real physical - not just formally mathematical, as in other interpretations - quantum superposition. Such a superposition of consistent state combinations of different systems is called an entangled state. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett's interpretation is perfectly consistent with John Bell's experiments and makes them intuitively understandable. However, according to the theory of quantum decoherence, these \"parallel universes\" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes entangled with both the physicist who measured it and a huge number of other particles, some of which are photons flying away at the speed of light towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring all these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one could theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist's memory). In light of these Bell tests, Cramer (1986) formulated his transactional interpretation.[78] Relational quantum mechanics appeared in the late 1990s as the modern derivative of the Copenhagen Interpretation.Entanglement, as demonstrated in Bell-type experiments, does not, however, violate causality, since no transfer of information happens. Quantum entanglement forms the basis of quantum cryptography, which is proposed for use in high-security commercial applications in banking and government.John Bell showed that this \"EPR\" paradox led to experimentally testable differences between quantum mechanics and theories that rely on added hidden variables. Experiments have been performed confirming the accuracy of quantum mechanics, thereby demonstrating that quantum mechanics cannot be improved upon by addition of hidden variables.[76] Alain Aspect's initial experiments in 1982, and many subsequent experiments since, have definitively verified quantum entanglement.Albert Einstein, himself one of the founders of quantum theory, did not accept some of the more philosophical or metaphysical interpretations of quantum mechanics, such as rejection of determinism and of causality. He is famously quoted as saying, in response to this aspect, \"God does not play with dice\".[75] He rejected the concept that the state of a physical system depends on the experimental arrangement for its measurement. He held that a state of nature occurs in its own right, regardless of whether or how it might be observed. In that view, he is supported by the currently accepted definition of a quantum state, which remains invariant under arbitrary choice of configuration space for its representation, that is to say, manner of observation. He also held that underlying quantum mechanics there should be a theory that thoroughly and directly expresses the rule against action at a distance; in other words, he insisted on the principle of locality. He considered, but rejected on theoretical grounds, a particular proposal for hidden variables to obviate the indeterminism or acausality of quantum mechanical measurement. He considered that quantum mechanics was a currently valid but not a permanently definitive theory for quantum phenomena. He thought its future replacement would require profound conceptual advances, and would not come quickly or easily. The Bohr-Einstein debates provide a vibrant critique of the Copenhagen Interpretation from an epistemological point of view. In arguing for his views, he produced a series of objections, the most famous of which has become known as the Einstein\u2013Podolsky\u2013Rosen paradox.The Copenhagen interpretation \u2014 due largely to Niels Bohr and Werner Heisenberg \u2014 remains most widely accepted amongst physicists, some 75 years after its enunciation. According to this interpretation, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but instead must be considered a final renunciation of the classical idea of \"causality.\" It is also believed therein that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the conjugate nature of evidence obtained under different experimental situations.Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. Even fundamental issues, such as Max Born's basic rules concerning probability amplitudes and probability distributions, took decades to be appreciated by society and many leading scientists. Richard Feynman once said, \"I think I can safely say that nobody understands quantum mechanics.\"[73] According to Steven Weinberg, \"There is now in my opinion no entirely satisfactory interpretation of quantum mechanics.\"[74]Another popular theory is Loop quantum gravity (LQG), a theory first proposed by Carlo Rovelli that describes the quantum properties of gravity. It is also a theory of quantum space and quantum time, because in general relativity the geometry of spacetime is a manifestation of gravity. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature of the granularity of the photons in the quantum theory of electromagnetism or the discrete levels of the energy of the atoms. But here it is space itself which is discrete. More precisely, space can be viewed as an extremely fine fabric or network \"woven\" of finite loops. These networks of loops are called spin networks. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the Planck length, which is approximately 1.616\u00d710\u221235 m. According to theory, there is no meaning to length shorter than this (cf. Planck scale energy). Therefore, LQG predicts that not just matter, but also space itself, has an atomic structure.The quest to unify the fundamental forces through quantum mechanics is still ongoing. Quantum electrodynamics (or \"quantum electromagnetism\"), which is currently (in the perturbative regime at least) the most accurately tested physical theory in competition with general relativity,[70][71] has been successfully merged with the weak nuclear force into the electroweak force and work is currently being done to merge the electroweak and strong force into the electrostrong force. Current predictions state that at around 1014 GeV the three aforementioned forces are fused into a single unified field.[72] Beyond this \"grand unification\", it is speculated that it may be possible to merge gravity with the other three gauge symmetries, expected to occur at roughly 1019 GeV. However\u00a0\u2014 and while special relativity is parsimoniously incorporated into quantum electrodynamics\u00a0\u2014 the expanded general relativity, currently the best theory describing the gravitation force, has not been fully incorporated into quantum theory. One of those searching for a coherent TOE is Edward Witten, a theoretical physicist who formulated the M-theory, which is an attempt at describing the supersymmetrical based string theory. M-theory posits that our apparent 4-dimensional spacetime is, in reality, actually an 11-dimensional spacetime containing 10 spatial dimensions and 1 time dimension, although 7 of the spatial dimensions are - at lower energies - completely \"compactified\" (or infinitely curved) and not readily amenable to measurement or probing.Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant \"Theory of Everything\" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th and 21st century physics. Many prominent physicists, including Stephen Hawking, have labored for many years in the attempt to discover a theory underlying everything. This TOE would combine not only the different models of subatomic physics, but also derive the four fundamental forces of nature - the strong force, electromagnetism, the weak force, and gravity - from a single force or phenomenon. While Stephen Hawking was initially a believer in the Theory of Everything, after considering G\u00f6del's Incompleteness Theorem, he has concluded that one is not obtainable, and has stated so publicly in his lecture \"G\u00f6del and the End of Physics\" (2002).[69]Even with the defining postulates of both Einstein's theory of general relativity and quantum theory being indisputably supported by rigorous and repeated empirical evidence, and while they do not directly contradict each other theoretically (at least with regard to their primary claims), they have proven extremely difficult to incorporate into one consistent, cohesive model.[68]Classical kinematics does not primarily demand experimental description of its phenomena. It allows completely precise description of an instantaneous state by a value in phase space, the Cartesian product of configuration and momentum spaces. This description simply assumes or imagines a state as a physically existing entity without concern about its experimental measurability. Such a description of an initial condition, together with Newton's laws of motion, allows a precise deterministic and causal prediction of a final condition, with a definite trajectory of passage. Hamiltonian dynamics can be used for this. Classical kinematics also allows the description of a process analogous to the initial and final condition description used by quantum mechanics. Lagrangian mechanics applies to this.[67] For processes that need account to be taken of actions of a small number of Planck constants, classical kinematics is not adequate; quantum mechanics is needed.For many experiments, it is possible to think of the initial and final conditions of the system as being a particle. In some cases it appears that there are potentially several spatially distinct pathways or trajectories by which a particle might pass from initial to final condition. It is an important feature of the quantum kinematic description that it does not permit a unique definite statement of which of those pathways is actually followed. Only the initial and final conditions are definite, and, as stated in the foregoing paragraph, they are defined only as precisely as allowed by the configuration space description or its equivalent. In every case for which a quantum kinematic description is needed, there is always a compelling reason for this restriction of kinematic precision. An example of such a reason is that for a particle to be experimentally found in a definite position, it must be held motionless; for it to be experimentally found to have a definite momentum, it must have free motion; these two are logically incompatible.[65][66]In Niels Bohr's mature view, quantum mechanical phenomena are required to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics.[55][56][57][58] The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, in terms of both position and momentum, of an initial condition or \"state\" (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition.[59][60] In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous \"state\" in the classical sense of that word.[61][62] Thus there are two kinds of processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined.[59] Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schr\u00f6dinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.[63][64]A big difference between classical and quantum mechanics is that they use very different kinematic descriptions.[54]Quantum coherence is an essential difference between classical and quantum theories as illustrated by the Einstein\u2013Podolsky\u2013Rosen (EPR) paradox \u2014 an attack on a certain philosophical interpretation of quantum mechanics by an appeal to local realism.[49] Quantum interference involves adding together probability amplitudes, whereas classical \"waves\" infer that there is an adding together of intensities. For microscopic bodies, the extension of the system is much smaller than the coherence length, which gives rise to long-range entanglement and other nonlocal phenomena characteristic of quantum systems.[50] Quantum coherence is not typically evident at macroscopic scales, though an exception to this rule may occur at extremely low temperatures (i.e. approaching absolute zero) at which quantum behavior may manifest itself macroscopically.[51] This is in accordance with the following observations:Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy.[46] According to the correspondence principle between classical and quantum mechanics, all objects obey the laws of quantum mechanics, and classical mechanics is just an approximation for large systems of objects (or a statistical quantum mechanics of a large collection of particles).[47] The laws of classical mechanics thus follow from the laws of quantum mechanics as a statistical average at the limit of large systems or large quantum numbers.[48] However, chaotic systems do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.Classical mechanics has also been extended into the complex domain, with complex classical mechanics exhibiting behaviors similar to quantum mechanics.[45]It has proven difficult to construct quantum models of gravity, the remaining fundamental force. Semi-classical approximations are workable, and have led to predictions such as Hawking radiation. However, the formulation of a complete theory of quantum gravity is hindered by apparent incompatibilities between general relativity (the most accurate theory of gravity currently known) and some of the fundamental assumptions of quantum theory. The resolution of these incompatibilities is an area of active research, and theories such as string theory are among the possible candidates for a future theory of quantum gravity.Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg. These three men shared the Nobel Prize in Physics in 1979 for this work.[44]When quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.The rules of quantum mechanics are fundamental. They assert that the state space of a system is a Hilbert space (crucially, that the space has an inner product) and that observables of that system are Hermitian operators acting on vectors in that space\u2014although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system. An important guide for making these choices is the correspondence principle, which states that the predictions of quantum mechanics reduce to those of classical mechanics when a system moves to higher energies or, equivalently, larger quantum numbers, i.e. whereas a single particle exhibits a degree of randomness, in systems incorporating millions of particles averaging takes over and, at the high energy limit, the statistical probability of random behaviour approaches zero. In other words, classical mechanics is simply a quantum mechanics of large systems. This \"high energy\" limit is known as the classical or correspondence limit. One can even start from an established classical model of a particular system, then attempt to guess the underlying quantum model that would give rise to the classical model in the correspondence limit.Especially since Werner Heisenberg was awarded the Nobel Prize in Physics in 1932 for the creation of quantum mechanics, the role of Max Born in the development of QM was overlooked until the 1954 Nobel award. The role is noted in a 2005 biography of Born, which recounts his role in the matrix formulation of quantum mechanics, and the use of probability amplitudes. Heisenberg himself acknowledges having learned matrices from Born, as published in a 1940 festschrift honoring Max Planck.[42] In the matrix formulation, the instantaneous state of a quantum system encodes the probabilities of its measurable properties, or \"observables\". Examples of observables include energy, position, momentum, and angular momentum. Observables can be either continuous (e.g., the position of a particle) or discrete (e.g., the energy of an electron bound to a hydrogen atom).[43] An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.There are numerous mathematically equivalent formulations of quantum mechanics. One of the oldest and most commonly used formulations is the \"transformation theory\" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics - matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schr\u00f6dinger).[41]There exist several techniques for generating approximate solutions, however. In the important method known as perturbation theory, one uses the analytic result for a simple quantum mechanical model to generate a result for a more complicated model that is related to the simpler model by (for one example) the addition of a weak potential energy. Another method is the \"semi-classical equation of motion\" approach, which applies to systems for which quantum mechanics produces only weak (small) deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.The Schr\u00f6dinger equation acts on the entire probability amplitude, not merely its absolute value. Whereas the absolute value of the probability amplitude encodes information about probabilities, its phase encodes information about the interference between quantum states. This gives rise to the \"wave-like\" behavior of quantum states. As it turns out, analytic solutions of the Schr\u00f6dinger equation are available for only a very small number of relatively simple model Hamiltonians, of which the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom are the most important representatives. Even the helium atom\u2014which contains just one more electron than does the hydrogen atom\u2014has defied all attempts at a fully analytic treatment.Some wave functions produce probability distributions that are constant, or independent of time\u2014such as when in a stationary state of constant energy, time vanishes in the absolute square of the wave function. Many systems that are treated dynamically in classical mechanics are described by such \"static\" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics it is described by a static, spherically symmetric wave function surrounding the nucleus (Fig. 1) (note, however, that only the lowest angular momentum states, labeled s, are spherically symmetric).[40]Wave functions change as time progresses. The Schr\u00f6dinger equation describes how wave functions change in time, playing a role similar to Newton's second law in classical mechanics. The Schr\u00f6dinger equation, applied to the aforementioned example of the free particle, predicts that the center of a wave packet will move through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more uncertain with time. This also has the effect of turning a position eigenstate (which can be thought of as an infinitely sharp wave packet) into a broadened wave packet that no longer represents a (definite, certain) position eigenstate.[39]During a measurement, on the other hand, the change of the initial wave function into another, later wave function is not deterministic, it is unpredictable (i.e., random). A time-evolution simulation can be seen here.[37][38]The time evolution of a quantum state is described by the Schr\u00f6dinger equation, in which the Hamiltonian (the operator corresponding to the total energy of the system) generates the time evolution. The time evolution of wave functions is deterministic in the sense that - given a wave function at an initial time - it makes a definite prediction of what the wave function will be at any later time.[36]In the everyday world, it is natural and intuitive to think of everything (every observable) as being in an eigenstate. Everything appears to have a definite position, a definite momentum, a definite energy, and a definite time of occurrence. However, quantum mechanics does not pinpoint the exact values of a particle's position and momentum (since they are conjugate pairs) or its energy and time (since they too are conjugate pairs); rather, it provides only a range of probabilities in which that particle might be given its momentum and momentum probability. Therefore, it is helpful to use different words to describe states having uncertain values and states having definite values (eigenstates). Usually, a system will not be in an eigenstate of the observable (particle) we are interested in. However, if one measures the observable, the wave function will instantaneously be an eigenstate (or \"generalized\" eigenstate) of that observable. This process is known as wave function collapse, a controversial and much-debated process[34] that involves expanding the system under study to include the measurement device. If one knows the corresponding wave function at the instant before the measurement, one will be able to compute the probability of the wave function collapsing into each of the possible eigenstates. For example, the free particle in the previous example will usually have a wave function that is a wave packet centered around some mean position x0 (neither an eigenstate of position nor of momentum). When one measures the position of the particle, it is impossible to predict with certainty the result.[30] It is probable, but not certain, that it will be near x0, where the amplitude of the wave function is large. After the measurement is performed, having obtained some result x, the wave function collapses into a position eigenstate centered at x.[35]Generally, quantum mechanics does not assign definite values. Instead, it makes a prediction using a probability distribution; that is, it describes the probability of obtaining the possible outcomes from measuring an observable. Often these results are skewed by many causes, such as dense probability clouds. Probability clouds are approximate (but better than the Bohr model) whereby electron location is given by a probability function, the wave function eigenvalue, such that the probability is the squared modulus of the complex amplitude, or quantum state nuclear attraction.[31][32] Naturally, these probabilities will depend on the quantum state at the \"instant\" of the measurement. Hence, uncertainty is involved in the value. There are, however, certain states that are associated with a definite value of a particular observable. These are known as eigenstates of the observable (\"eigen\" can be translated from German as meaning \"inherent\" or \"characteristic\").[33]The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr\u2013Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a \"measurement\" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of \"wave function collapse\" (see, for example, the relative state interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled, so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.[30]According to one interpretation, as the result of a measurement the wave function containing the probability information for a system collapses from a given initial state to a particular eigenstate. The possible results of a measurement are the eigenvalues of the operator representing the observable\u2014which explains the choice of Hermitian operators, for which all the eigenvalues are real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator. Heisenberg's uncertainty principle is represented by the statement that the operators corresponding to certain observables do not commute.In the formalism of quantum mechanics, the state of a system at a given time is described by a complex wave function, also referred to as state vector in a complex vector space.[28] This abstract mathematical object allows for the calculation of probabilities of outcomes of concrete experiments. For example, it allows one to compute the probability of finding an electron in a particular region around the nucleus at a particular time. Contrary to classical mechanics, one can never make simultaneous predictions of conjugate variables, such as position and momentum, to arbitrary precision. For instance, electrons may be considered (to a certain probability) to be located somewhere within a given region of space, but with their exact positions unknown. Contours of constant probability, often referred to as \"clouds\", may be drawn around the nucleus of an atom to conceptualize where the electron might be located with the most probability. Heisenberg's uncertainty principle quantifies the inability to precisely locate the particle given its conjugate momentum.[29]In the mathematically rigorous formulation of quantum mechanics developed by Paul Dirac,[23] David Hilbert,[24] John von Neumann,[25] and Hermann Weyl,[26] the possible states of a quantum mechanical system are symbolized[27] as unit vectors (called state vectors). Formally, these reside in a complex separable Hilbert space\u2014variously called the state space or the associated Hilbert space of the system\u2014that is well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system\u2014for example, the state space for position and momentum states is the space of square-integrable functions, while the state space for the spin of a single proton is just the product of two complex planes. Each observable is represented by a maximally Hermitian (precisely: by a self-adjoint) linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. If the operator's spectrum is discrete, the observable can attain only those discrete eigenvalues.Broadly speaking, quantum mechanics incorporates four classes of phenomena for which classical physics cannot account:Quantum mechanics was initially developed to provide a better explanation and description of the atom, especially the differences in the spectra of light emitted by different isotopes of the same chemical element, as well as subatomic particles. In short, the quantum-mechanical atomic model has succeeded spectacularly in the realm where classical mechanics and electromagnetism falter.Quantum mechanics is essential to understanding the behavior of systems at atomic length scales and smaller. If the physical nature of an atom were solely described by classical mechanics, electrons would not orbit the nucleus, since orbiting electrons emit radiation (due to circular motion) and would eventually collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, non-deterministic, smeared, probabilistic wave\u2013particle orbital about the nucleus, defying the traditional assumptions of classical mechanics and electromagnetism.[22]The word quantum derives from the Latin, meaning \"how great\" or \"how much\".[19] In quantum mechanics, it refers to a discrete unit assigned to certain physical quantities such as the energy of an atom at rest (see Figure 1). The discovery that particles are discrete packets of energy with wave-like properties led to the branch of physics dealing with atomic and subatomic systems which is today called quantum mechanics. It underlies the mathematical framework of many fields of physics and chemistry, including condensed matter physics, solid-state physics, atomic physics, molecular physics, computational physics, computational chemistry, quantum chemistry, particle physics, nuclear chemistry, and nuclear physics.[20][better\u00a0source\u00a0needed] Some fundamental aspects of the theory are still actively studied.[21]While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors,[17] and superfluids.[18]By 1930, quantum mechanics had been further unified and formalized by the work of David Hilbert, Paul Dirac and John von Neumann[16] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines including quantum chemistry, quantum electronics, quantum optics, and quantum information science. Its speculative modern developments include string theory and quantum gravity theories. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies.[citation needed]It was found that subatomic particles and electromagnetic waves are neither simply particle nor wave but have certain properties of each. This originated the concept of wave\u2013particle duality.[citation needed]In the mid-1920s, developments in quantum mechanics led to its becoming the standard formulation for atomic physics. In the summer of 1925, Bohr and Heisenberg published results that closed the old quantum theory. Out of deference to their particle-like behavior in certain processes and measurements, light quanta came to be called photons (1926). In 1926 Erwin Schr\u00f6dinger suggested a partial differential equation for the wave functions of particles like electrons. And when effectively restricted to a finite region, this equation allowed only certain modes, corresponding to discrete quantum states\u2014whose properties turned out to be exactly the same as implied by matrix mechanics.[15] From Einstein's simple postulation was born a flurry of debating, theorizing, and testing. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[citation needed]The foundations of quantum mechanics were established during the first half of the 20th century by Max Planck, Niels Bohr, Werner Heisenberg, Louis de Broglie, Arthur Compton, Albert Einstein, Erwin Schr\u00f6dinger, Max Born, John von Neumann, Paul Dirac, Enrico Fermi, Wolfgang Pauli, Max von Laue, Freeman Dyson, David Hilbert, Wilhelm Wien, Satyendra Nath Bose, Arnold Sommerfeld, and others. The Copenhagen interpretation of Niels Bohr became widely accepted.Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete quantum of energy that was dependent on its frequency.[14]Planck cautiously insisted that this was simply an aspect of the processes of absorption and emission of radiation and had nothing to do with the physical reality of the radiation itself.[12] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[13] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. He won the 1921 Nobel Prize in Physics for this work.where h is Planck's constant.According to Planck, each energy element (E) is proportional to its frequency (\u03bd):Among the first to study quantum phenomena in nature were Arthur Compton, C. V. Raman, and Pieter Zeeman, each of whom has a quantum effect named after him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Ernest Rutherford experimentally discovered the nuclear model of the atom, for which Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld.[11] This phase is known as old quantum theory.Following Max Planck's solution in 1900 to the black-body radiation problem (reported 1859), Albert Einstein offered a quantum-based theory to explain the photoelectric effect (1905, reported 1887). Around 1900-1910, the atomic theory and the corpuscular theory of light[10] first came to be widely accepted as scientific fact; these latter theories can be viewed as quantum theories of matter and electromagnetic radiation, respectively.In 1896, Wilhelm Wien empirically determined a distribution law of black-body radiation,[9] known as Wien's law in his honor. Ludwig Boltzmann independently arrived at this result by considerations of Maxwell's equations. However, it was valid only at high frequencies and underestimated the radiance at low frequencies. Later, Planck corrected this model using Boltzmann's statistical interpretation of thermodynamics and proposed what is now called Planck's law, which led to the development of quantum mechanics.In 1838, Michael Faraday discovered cathode rays. These studies were followed by the 1859 statement of the black-body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system can be discrete, and the 1900 quantum hypothesis of Max Planck.[8] Planck's hypothesis that energy is radiated and absorbed in discrete \"quanta\" (or energy packets) precisely matched the observed patterns of black-body radiation.Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[7] In 1803, Thomas Young, an English polymath, performed the famous double-slit experiment that he later described in a paper titled On the nature of light and colours. This experiment played a major role in the general acceptance of the wave theory of light.Important applications of quantum theory[5] include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.[6]Quantum mechanics gradually arose from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and from the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schr\u00f6dinger, Werner Heisenberg, Max Born and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.Classical physics (the physics existing before quantum mechanics) is a set of fundamental theories which describes nature at ordinary (macroscopic) scale. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3] Quantum mechanics differs from classical physics in that: energy, momentum and other quantities of a system may be restricted to discrete values (quantization), objects have characteristics of both particles and waves (wave-particle duality), and there are limits to the precision with which quantities can be known (uncertainty principle).[note 1]Quantum mechanics (QM; also known as quantum physics, quantum theory, the wave mechanical model, or matrix mechanics), including quantum field theory, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles.[2]",
            "title": "Quantum mechanics",
            "url": "https://en.wikipedia.org/wiki/Quantum_mechanics"
        },
        {
            "desc_links": [
                "/wiki/Atomic,_molecular,_and_optical_physics",
                "/wiki/Nuclear_power",
                "/wiki/Nuclear_weapon",
                "/wiki/Synonym",
                "/wiki/Standard_English",
                "/wiki/Nuclear_physics",
                "/wiki/Atomic_nucleus",
                "/wiki/Physics",
                "/wiki/Atom",
                "/wiki/Electron",
                "/wiki/Atomic_nucleus",
                "/wiki/Electron_configuration",
                "/wiki/Ion"
            ],
            "links": [
                "/wiki/World_War_II",
                "/wiki/Laser",
                "/wiki/Spectral_line",
                "/wiki/Joseph_von_Fraunhofer",
                "/wiki/Bohr_atom_model",
                "/wiki/Quantum_mechanics",
                "/wiki/Atomic_orbital_model",
                "/wiki/Chemistry",
                "/wiki/Quantum_chemistry",
                "/wiki/Spectroscopy",
                "/wiki/Democritus",
                "/wiki/Vaisheshika_Sutra",
                "/wiki/Kanad",
                "/wiki/Chemical_element",
                "/wiki/John_Dalton",
                "/wiki/Periodic_system_of_elements",
                "/wiki/Dmitri_Mendeleev",
                "/wiki/Selection_rule",
                "/wiki/Characteristic_x-ray",
                "/wiki/Auger_effect",
                "/wiki/Excited_state",
                "/wiki/Bound_state",
                "/wiki/Binding_energy",
                "/wiki/Kinetic_energy",
                "/wiki/Conservation_of_energy",
                "/wiki/Ionization",
                "/wiki/Electron_shells",
                "/wiki/Ground_state",
                "/wiki/Photon",
                "/wiki/Gas",
                "/wiki/Plasma_(physics)",
                "/wiki/Plasma_(physics)",
                "/wiki/Atmospheric_physics",
                "/wiki/Molecule",
                "/wiki/Solid_state_physics",
                "/wiki/Condensed_matter",
                "/wiki/Ionization",
                "/wiki/Excited_state",
                "/wiki/Atomic,_molecular,_and_optical_physics",
                "/wiki/Nuclear_power",
                "/wiki/Nuclear_weapon",
                "/wiki/Synonym",
                "/wiki/Standard_English",
                "/wiki/Nuclear_physics",
                "/wiki/Atomic_nucleus",
                "/wiki/Physics",
                "/wiki/Atom",
                "/wiki/Electron",
                "/wiki/Atomic_nucleus",
                "/wiki/Electron_configuration",
                "/wiki/Ion"
            ],
            "text": "Since the Second World War, both theoretical and experimental fields have advanced at a rapid pace. This can be attributed to progress in computing technology, which has allowed larger and more sophisticated models of atomic structure and associated collision processes. Similar technological advances in accelerators, detectors, magnetic field generation and lasers have greatly assisted experimental work.The true beginning of atomic physics is marked by the discovery of spectral lines and attempts to describe the phenomenon, most notably by Joseph von Fraunhofer. The study of these lines led to the Bohr atom model and to the birth of quantum mechanics. In seeking to explain atomic spectra an entirely new mathematical model of matter was revealed. As far as atoms and their electron shells were concerned, not only did this yield a better overall description, i.e. the atomic orbital model, but it also provided a new theoretical basis for chemistry (quantum chemistry) and spectroscopy.One of the earliest steps towards atomic physics was the recognition that matter was composed of atoms. It forms a part of the texts written in 6th century BC to 2nd century BC such as those of Democritus or Vaisheshika Sutra written by Kanad. This theory was later developed in the modern sense of the basic unit of a chemical element by the British chemist and physicist John Dalton in the 18th century. At this stage, it wasn't clear what atoms were although they could be described and classified by their properties (in bulk). The invention of the periodic system of elements by Mendeleev was another great step forward.The majority of fields in physics can be divided between theoretical work and experimental work, and atomic physics is no exception. It is usually the case, but not always, that progress goes in alternate cycles from an experimental observation, through to a theoretical explanation followed by some predictions that may or may not be confirmed by experiment, and so on. Of course, the current state of technology at any given time can put limitations on what can be achieved experimentally and theoretically so it may take considerable time for theory to be refined.There are rather strict selection rules as to the electronic configurations that can be reached by excitation by light \u2014 however there are no such rules for excitation by collision processes.If an inner electron has absorbed more than the binding energy (so that the atom ionizes), then a more outer electron may undergo a transition to fill the inner orbital. In this case, a visible photon or a characteristic x-ray is emitted, or a phenomenon known as the Auger effect may take place, where the released energy is transferred to another bound electron, causing it to go into the continuum. The Auger effect allows one to multiply ionize an atom with a single photon.If the electron absorbs a quantity of energy less than the binding energy, it will be transferred to an excited state. After a certain time, the electron in an excited state will \"jump\" (undergo a transition) to a lower state. In a neutral atom, the system will emit a photon of the difference in energy, since energy is conserved.Electrons that populate a shell are said to be in a bound state. The energy necessary to remove an electron from its shell (taking it to infinity) is called the binding energy. Any quantity of energy absorbed by the electron in excess of this amount is converted to kinetic energy according to the conservation of energy. The atom is said to have undergone the process of ionization.Electrons form notional shells around the nucleus. These are normally in a ground state but can be excited by the absorption of energy from light (photons), magnetic fields, or interaction with a colliding particle (typically ions or other electrons).While modelling atoms in isolation may not seem realistic, if one considers atoms in a gas or plasma then the time-scales for atom-atom interactions are huge in comparison to the atomic processes that are generally considered. This means that the individual atoms can be treated as if each were in isolation, as the vast majority of the time they are. By this consideration atomic physics provides the underlying theory in plasma physics and atmospheric physics, even though both deal with very large numbers of atoms.Atomic physics primarily considers atoms in isolation. Atomic models will consist of a single nucleus that may be surrounded by one or more bound electrons. It is not concerned with the formation of molecules (although much of the physics is identical), nor does it examine atoms in a solid state as condensed matter. It is concerned with processes such as ionization and excitation by photons or collisions with atomic particles.As with many scientific fields, strict delineation can be highly contrived and atomic physics is often considered in the wider context of atomic, molecular, and optical physics. Physics research groups are usually so classified.The term atomic physics can be associated with nuclear power and nuclear weapons, due to the synonymous use of atomic and nuclear in standard English. Physicists distinguish between atomic physics \u2014 which deals with the atom as a system consisting of a nucleus and electrons \u2014 and nuclear physics, which considers atomic nuclei alone.Atomic physics is the field of physics that studies atoms as an isolated system of electrons and an atomic nucleus. It is primarily concerned with the arrangement of electrons around the nucleus and the processes by which these arrangements change. This comprises ions, neutral atoms and, unless otherwise stated, it can be assumed that the term atom includes ions.[citation needed]",
            "title": "Atomic physics",
            "url": "https://en.wikipedia.org/wiki/Atomic_physics"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Atomic_orbital",
                "/wiki/Molecular_orbital",
                "/wiki/Energy_level",
                "/wiki/Spectrum",
                "/wiki/Infrared",
                "/wiki/Micrometre",
                "/wiki/Wavelength",
                "/wiki/Electromagnetic_spectrum",
                "/wiki/Ultraviolet",
                "/wiki/Molecule",
                "/wiki/Chemical_bond",
                "/wiki/Atom",
                "/wiki/Molecular_dynamics",
                "/wiki/Spectroscopy",
                "/wiki/Scattering",
                "/wiki/Atomic_physics",
                "/wiki/Theoretical_chemistry",
                "/wiki/Physical_chemistry",
                "/wiki/Chemical_physics"
            ],
            "text": "One important aspect of molecular physics is that the essential atomic orbital theory in the field of atomic physics expands to the molecular orbital theory.In addition to the electronic excitation states which are known from atoms, molecules exhibit rotational and vibrational modes whose energy levels are quantized. The smallest energy differences exist between different rotational states: pure rotational spectra are in the far infrared region (about 30 - 150 \u00b5m wavelength) of the electromagnetic spectrum. Vibrational spectra are in the near infrared (about 1 - 5\u00a0\u00b5m) and spectra resulting from electronic transitions are mostly in the visible and ultraviolet regions. From measuring rotational and vibrational spectra properties of molecules like the distance between the nuclei can be specifically calculated.Molecular physics is the study of the physical properties of molecules, the chemical bonds between atoms as well as the molecular dynamics. Its most important experimental techniques are the various types of spectroscopy; scattering is also used. The field is closely related to atomic physics and overlaps greatly with theoretical chemistry, physical chemistry and chemical physics.",
            "title": "Molecular physics",
            "url": "https://en.wikipedia.org/wiki/Molecular_physics"
        },
        {
            "desc_links": [
                "/wiki/Open_shell",
                "/wiki/Nuclear_physics",
                "/wiki/Bogoliubov_transformation",
                "/wiki/Nuclear_structure#Nuclear_pairing_phenomenon",
                "/wiki/Atomic_structure",
                "/wiki/Configuration_state_function",
                "/wiki/Quantum_number",
                "/wiki/Ground_state",
                "/wiki/Hartree_equation",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Douglas_Hartree",
                "/wiki/Hartree%E2%80%93Fock#The_Fock_operator",
                "/wiki/Iterative_method",
                "/wiki/Fixed-point_iteration",
                "/wiki/Slater_determinant",
                "/wiki/Fermion",
                "/wiki/Permanent_(mathematics)",
                "/wiki/Boson",
                "/wiki/Spin-orbital",
                "/wiki/Variational_method",
                "/wiki/Computational_physics",
                "/wiki/Computational_chemistry",
                "/wiki/Wave_function",
                "/wiki/Many-body_problem",
                "/wiki/Stationary_state"
            ],
            "links": [
                "/wiki/List_of_quantum_chemistry_and_solid_state_physics_software",
                "/wiki/Density_functional_theory",
                "/wiki/Hybrid_functional",
                "/wiki/Modern_valence_bond",
                "/wiki/Electronic_correlation",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/M%C3%B8ller%E2%80%93Plesset_perturbation_theory",
                "/wiki/Perturbation_theory",
                "/wiki/Multi-configurational_self-consistent_field",
                "/wiki/Configuration_interaction",
                "/wiki/Quadratic_configuration_interaction",
                "/wiki/Multi-configurational_self-consistent_field#Complete_active_space_SCF",
                "/wiki/Variational_Monte_Carlo",
                "/wiki/Numerical_stability",
                "/wiki/Basis_set_(chemistry)",
                "/wiki/Gram%E2%80%93Schmidt_process",
                "/wiki/Roothaan_equations",
                "/wiki/Overlap_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Generalized_eigenvalue_problem",
                "/wiki/Roothaan_equations",
                "/wiki/Linear_combination_of_atomic_orbitals",
                "/wiki/Slater-type_orbital",
                "/wiki/Gaussian_orbital",
                "/wiki/Exchange_operator",
                "/wiki/Coulomb_operator",
                "/wiki/Hamiltonian_(quantum_mechanics)",
                "/wiki/Molecular_Hamiltonian",
                "/wiki/Hartree%E2%80%93Fock#Hartree\u2013Fock_algorithm",
                "/wiki/Fock_matrix",
                "/wiki/Mean-field_theory",
                "/wiki/Fock_operator",
                "/wiki/Unitary_transformation",
                "/wiki/Coulomb%27s_law",
                "/wiki/Spin-orbital",
                "/wiki/Atomic_orbital",
                "/wiki/Molecular_orbital",
                "/wiki/Linear_combination_of_atomic_orbitals",
                "/wiki/Variational_method_(quantum_mechanics)",
                "/wiki/Expectation_value",
                "/wiki/Ground_state",
                "/wiki/Orthonormal_basis#Incomplete_orthogonal_sets",
                "/wiki/Configuration_interaction",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/Born%E2%80%93Oppenheimer_approximation",
                "/wiki/Hydrogen_atom",
                "/wiki/Iteration",
                "/wiki/Central_field_approximation",
                "/wiki/Slater_determinant",
                "/wiki/Determinant",
                "/wiki/Exchange_symmetry",
                "/wiki/Ansatz",
                "/wiki/Variational_principle",
                "/wiki/Exchange_symmetry",
                "/wiki/Group_theory",
                "/wiki/Vladimir_Fock",
                "/wiki/Exchange_symmetry",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Quantum_statistics",
                "/wiki/Ab_initio_quantum_chemistry_methods",
                "/wiki/Hartree_product",
                "/wiki/John_C._Slater",
                "/wiki/Variational_principle",
                "/wiki/Ansatz",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Douglas_Hartree",
                "/wiki/Robert_Bruce_Lindsay",
                "/wiki/Old_quantum_theory",
                "/wiki/Open_shell",
                "/wiki/Nuclear_physics",
                "/wiki/Bogoliubov_transformation",
                "/wiki/Nuclear_structure#Nuclear_pairing_phenomenon",
                "/wiki/Atomic_structure",
                "/wiki/Configuration_state_function",
                "/wiki/Quantum_number",
                "/wiki/Ground_state",
                "/wiki/Hartree_equation",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Douglas_Hartree",
                "/wiki/Hartree%E2%80%93Fock#The_Fock_operator",
                "/wiki/Iterative_method",
                "/wiki/Fixed-point_iteration",
                "/wiki/Slater_determinant",
                "/wiki/Fermion",
                "/wiki/Permanent_(mathematics)",
                "/wiki/Boson",
                "/wiki/Spin-orbital",
                "/wiki/Variational_method",
                "/wiki/Computational_physics",
                "/wiki/Computational_chemistry",
                "/wiki/Wave_function",
                "/wiki/Many-body_problem",
                "/wiki/Stationary_state"
            ],
            "text": "For a list of software packages known to handle Hartree\u2013Fock calculations, particularly for molecules and solids, see the list of quantum chemistry and solid state physics software.An alternative to Hartree\u2013Fock calculations used in some cases is density functional theory, which treats both exchange and correlation energies, albeit approximately. Indeed, it is common to use calculations that are a hybrid of the two methods\u2014the popular B3LYP scheme is one such hybrid functional method. Another option is to use modern valence bond methods.Of the five simplifications outlined in the section \"Hartree\u2013Fock algorithm\", the fifth is typically the most important. Neglect of electron correlation can lead to large deviations from experimental results. A number of approaches to this weakness, collectively called post-Hartree\u2013Fock methods, have been devised to include electron correlation to the multi-electron wave function. One of these approaches, M\u00f8ller\u2013Plesset perturbation theory, treats correlation as a perturbation of the Fock operator. Others expand the true multi-electron wave function in terms of a linear combination of Slater determinants\u2014such as multi-configurational self-consistent field, configuration interaction, quadratic configuration interaction, and complete active space SCF (CASSCF). Still others (such as variational quantum Monte Carlo) modify the Hartree\u2013Fock wave function by multiplying it by a correlation function (\"Jastrow\" factor), a term which is explicitly a function of multiple electrons that cannot be decomposed into independent single-particle functions.Numerical stability can be a problem with this procedure and there are various ways of combating this instability. One of the most basic and generally applicable is called F-mixing or damping. With F-mixing, once a single electron wave function is calculated it is not used directly. Instead, some combination of that calculated wave function and the previous wave functions for that electron is used\u2014the most common being a simple linear combination of the calculated and immediately preceding wave function. A clever dodge, employed by Hartree, for atomic calculations was to increase the nuclear charge, thus pulling all the electrons closer together. As the system stabilised, this was gradually reduced to the correct charge. In molecular calculations a similar approach is sometimes used by first calculating the wave function for a positive ion and then to use these orbitals as the starting point for the neutral molecule. Modern molecular Hartree\u2013Fock computer programs use a variety of methods to ensure convergence of the Roothaan\u2013Hall equations.Various basis sets are used in practice, most of which are composed of Gaussian functions. In some applications, an orthogonalization method such as the Gram\u2013Schmidt process is performed in order to produce a set of orthogonal basis functions. This can in principle save computational time when the computer is solving the Roothaan\u2013Hall equations by converting the overlap matrix effectively to an identity matrix. However, in most modern computer programs for molecular Hartree\u2013Fock calculations this procedure is not followed due to the high numerical cost of orthogonalization and the advent of more efficient, often sparse, algorithms for solving the generalized eigenvalue problem, of which the Roothaan\u2013Hall equations are an example.Typically, in modern Hartree\u2013Fock calculations, the one-electron wave functions are approximated by a linear combination of atomic orbitals. These atomic orbitals are called Slater-type orbitals. Furthermore, it is very common for the \"atomic orbitals\" in use to actually be composed of a linear combination of one or more Gaussian-type orbitals, rather than Slater-type orbitals, in the interests of saving large amounts of computation time.is the exchange operator, defining the electron exchange energy due to the antisymmetry of the total n-electron wave function.[6] This \"exchange energy\" operator, K, is simply an artifact of the Slater determinant. Finding the Hartree\u2013Fock one-electron wave functions is now equivalent to solving the eigenfunction equation:is the Coulomb operator, defining the electron-electron repulsion energy due to each of the two electrons in the jth orbital.[6] Finallyis the one-electron core Hamiltonian. AlsowhereBecause the electron-electron repulsion term of the molecular Hamiltonian involves the coordinates of two different electrons, it is necessary to reformulate it in an approximate way. Under this approximation, (outlined under Hartree\u2013Fock algorithm), all of the terms of the exact Hamiltonian except the nuclear-nuclear repulsion term are re-expressed as the sum of one-electron operators outlined below, for closed-shell atoms or molecules (with two electrons in each spatial orbital).[6] The \"(1)\" following each operator symbol simply indicates that the operator is 1-electron in nature.Since the Fock operator depends on the orbitals used to construct the corresponding Fock matrix, the eigenfunctions of the Fock operator are in turn new orbitals which can be used to construct a new Fock operator. In this way, the Hartree\u2013Fock orbitals are optimized iteratively until the change in total electronic energy falls below a predefined threshold. In this way, a set of self-consistent one-electron orbitals are calculated. The Hartree\u2013Fock electronic wave function is then the Slater determinant constructed out of these orbitals. Following the basic postulates of quantum mechanics, the Hartree\u2013Fock wave function can then be used to compute any desired chemical or physical property within the framework of the Hartree\u2013Fock method and the approximations employed.The orbitals above only account for the presence of other electrons in an average manner. In the Hartree\u2013Fock method, the effect of other electrons are accounted for in a mean-field theory context. The orbitals are optimized by requiring them to minimize the energy of the respective Slater determinant. The resultant variational conditions on the orbitals lead to a new one-electron operator, the Fock operator. At the minimum, the occupied orbitals are eigensolutions to the Fock operator via a unitary transformation between themselves. The Fock operator is an effective one-electron Hamiltonian operator being the sum of two terms. The first is a sum of kinetic energy operators for each electron, the internuclear repulsion energy, and a sum of nuclear-electronic Coulombic attraction terms. The second are Coulombic repulsion terms between electrons in a mean-field theory description; a net repulsion energy for each electron in the system, which is calculated by treating all of the other electrons within the molecule as a smooth distribution of negative charge. This is the major simplification inherent in the Hartree\u2013Fock method, and is equivalent to the fifth simplification in the above list.The starting point for the Hartree\u2013Fock method is a set of approximate one-electron wave functions known as spin-orbitals. For an atomic orbital calculation, these are typically the orbitals for a hydrogenic atom (an atom with only one electron, but the appropriate nuclear charge). For a molecular orbital or crystalline calculation, the initial approximate one-electron wave functions are typically a linear combination of atomic orbitals (LCAO).The variational theorem states that for a time-independent Hamiltonian operator, any trial wave function will have an energy expectation value that is greater than or equal to the true ground state wave function corresponding to the given Hamiltonian. Because of this, the Hartree\u2013Fock energy is an upper bound to the true ground state energy of a given molecule. In the context of the Hartree\u2013Fock method, the best possible solution is at the Hartree\u2013Fock limit; i.e., the limit of the Hartree\u2013Fock energy as the basis set approaches completeness. (The other is the full-CI limit, where the last two approximations of the Hartree\u2013Fock theory as described above are completely undone. It is only when both limits are attained that the exact solution, up to the Born\u2013Oppenheimer approximation, is obtained.) The Hartree\u2013Fock energy is the minimal energy for a single Slater determinant.Relaxation of the last two approximations give rise to many so-called post-Hartree\u2013Fock methods.The Hartree\u2013Fock method makes five major simplifications in order to deal with this task:The Hartree\u2013Fock method is typically used to solve the time-independent Schr\u00f6dinger equation for a multi-electron atom or molecule as described in the Born\u2013Oppenheimer approximation. Since there are no known solutions for many-electron systems (there are solutions for one-electron systems such as hydrogenic atoms and the diatomic hydrogen cation), the problem is solved numerically. Due to the nonlinearities introduced by the Hartree\u2013Fock approximation, the equations are solved using a nonlinear method such as iteration, which gives rise to the name \"self-consistent field method.\"The Hartree\u2013Fock method, despite its physically more accurate picture, was little used until the advent of electronic computers in the 1950s due to the much greater computational demands over the early Hartree method and empirical models. Initially, both the Hartree method and the Hartree\u2013Fock method were applied exclusively to atoms, where the spherical symmetry of the system allowed one to greatly simplify the problem. These approximate methods were (and are) often used together with the central field approximation, to impose that electrons in the same shell have the same radial part, and to restrict the variational solution to be a spin eigenfunction. Even so, solution by hand of the Hartree\u2013Fock equations for a medium-sized atom were laborious; small molecules required computational resources far beyond what was available before 1950.It was then shown that a Slater determinant, a determinant of one-particle orbitals first used by Heisenberg and Dirac in 1926, trivially satisfies the antisymmetric property of the exact solution and hence is a suitable ansatz for applying the variational principle. The original Hartree method can then be viewed as an approximation to the Hartree\u2013Fock method by neglecting exchange. Fock's original method relied heavily on group theory and was too abstract for contemporary physicists to understand and implement. In 1935, Hartree reformulated the method more suitably for the purposes of calculation.In 1930, Slater and V. A. Fock independently pointed out that the Hartree method did not respect the principle of antisymmetry of the wave function. The Hartree method used the Pauli exclusion principle in its older formulation, forbidding the presence of two electrons in the same quantum state. However, this was shown to be fundamentally incomplete in its neglect of quantum statistics.Hartree sought to do away with empirical parameters and solve the many-body time-independent Schr\u00f6dinger equation from fundamental physical principles, i.e., ab initio. His first proposed method of solution became known as the Hartree method or Hartree product. However, many of Hartree's contemporaries did not understand the physical reasoning behind the Hartree method: it appeared to many people to contain empirical elements, and its connection to the solution of the many-body Schr\u00f6dinger equation was unclear. However, in 1928 J. C. Slater and J. A. Gaunt independently showed that the Hartree method could be couched on a sounder theoretical basis by applying the variational principle to an ansatz (trial wave function) as a product of single-particle functions.The origin of the Hartree\u2013Fock method dates back to the end of the 1920s, soon after the discovery of the Schr\u00f6dinger equation in 1926. In 1927, D. R. Hartree introduced a procedure, which he called the self-consistent field method, to calculate approximate wave functions and energies for atoms and ions. Hartree was guided by some earlier, semi-empirical methods of the early 1920s (by E. Fues, R. B. Lindsay, and himself) set in the old quantum theory of Bohr.The rest of this article will focus on applications in electronic structure theory suitable for molecules with the atom as a special case. The discussion here is only for the Restricted Hartree\u2013Fock method, where the atom or molecule is a closed-shell system with all orbitals (atomic or molecular) doubly occupied. Open-shell systems, where some of the electrons are not paired, can be dealt with by one of two Hartree\u2013Fock methods:For both atoms and molecules, the Hartree\u2013Fock solution is the central starting point for most methods that describe the many-electron system more accurately.The Hartree\u2013Fock method finds its typical application in the solution of the Schr\u00f6dinger equation for atoms, molecules, nanostructures[2] and solids but it has also found widespread use in nuclear physics. (See Hartree\u2013Fock\u2013Bogoliubov method for a discussion of its application in nuclear structure theory). In atomic structure theory, calculations may be for a spectrum with many excited energy levels and consequently the Hartree\u2013Fock method for atoms assumes the wave function is a single configuration state function with well-defined quantum numbers and that the energy level is not necessarily the ground state.Especially in the older literature, the Hartree\u2013Fock method is also called the self-consistent field method (SCF). In deriving what is now called the Hartree equation as an approximate solution of the Schr\u00f6dinger equation, Hartree required the final field as computed from the charge distribution to be \"self-consistent\" with the assumed initial field. Thus, self-consistency was a requirement of the solution. The solutions to the non-linear Hartree\u2013Fock equations also behave as if each particle is subjected to the mean field created by all other particles (see the Fock operator below) and hence, the terminology continued. The equations are almost universally solved by means of an iterative method, although the fixed-point iteration algorithm does not always converge.[1] This solution scheme is not the only one possible and is not an essential feature of the Hartree\u2013Fock method.The Hartree\u2013Fock method often assumes that the exact, N-body wave function of the system can be approximated by a single Slater determinant (in the case where the particles are fermions) or by a single permanent (in the case of bosons) of N spin-orbitals. By invoking the variational method, one can derive a set of N-coupled equations for the N spin orbitals. A solution of these equations yields the Hartree\u2013Fock wave function and energy of the system.In computational physics and chemistry, the Hartree\u2013Fock (HF) method is a method of approximation for the determination of the wave function and the energy of a quantum many-body system in a stationary state.",
            "title": "Hartree\u2013Fock method",
            "url": "https://en.wikipedia.org/wiki/Hartree%E2%80%93Fock"
        },
        {
            "desc_links": [
                "/wiki/Hydrogen_atom",
                "/wiki/Chemical_element",
                "/wiki/Periodic_table",
                "/wiki/Aufbau_principle",
                "/wiki/Quantum_number",
                "/wiki/Energy",
                "/wiki/Angular_momentum",
                "/wiki/Vector_component",
                "/wiki/Magnetic_quantum_number",
                "/wiki/Spin_quantum_number",
                "/wiki/Electron_configuration",
                "/wiki/Alkali_metal",
                "/wiki/Spectral_line",
                "/wiki/Quantum_mechanics",
                "/wiki/Function_(mathematics)",
                "/wiki/Electron",
                "/wiki/Atom",
                "/wiki/Probability",
                "/wiki/Atomic_nucleus"
            ],
            "links": [
                "/wiki/Mercury_(element)",
                "/wiki/Gold",
                "/wiki/Caesium",
                "/wiki/Atomic_number",
                "/wiki/Valence_electron",
                "/wiki/Electron_configuration#Atoms:_Aufbau_principle_and_Madelung_rule",
                "/wiki/Periodic_table_block",
                "/wiki/Lithium",
                "/wiki/Beryllium",
                "/wiki/Sodium",
                "/wiki/Magnesium",
                "/wiki/Periodic_table",
                "/wiki/Niels_Bohr",
                "/wiki/Periodic_table",
                "/wiki/Photon",
                "/wiki/Electron_configuration",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Spin_quantum_number",
                "/wiki/Spin_quantum_number",
                "/wiki/Keplerian_orbit",
                "/wiki/Orbital_eccentricity",
                "/wiki/Particle",
                "/wiki/Limit_(mathematics)",
                "/wiki/Antinode",
                "/wiki/Vibrations_of_a_circular_drum",
                "/wiki/Heisenberg_uncertainty_principle",
                "/wiki/Radium",
                "/wiki/Wave_function",
                "/wiki/Spherical_harmonics",
                "/wiki/Spherical_harmonics",
                "/wiki/Cubic_harmonic",
                "/wiki/Torus",
                "/wiki/Ellipsoid",
                "/wiki/Point_of_tangency",
                "/wiki/Atomic_nucleus",
                "/wiki/Dumbbell",
                "/wiki/Electron_shell",
                "/wiki/Complex_number",
                "/wiki/Standing_wave",
                "/wiki/Interference_(wave_propagation)",
                "/wiki/Travelling_wave",
                "/wiki/Orbital_eccentricity",
                "/wiki/Contour_line",
                "/wiki/Absolute_value",
                "/wiki/Wave_function",
                "/wiki/Spherical_harmonic",
                "/wiki/Stern%E2%80%93Gerlach_experiment",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Spin_quantum_number",
                "/wiki/Principal_quantum_number",
                "/wiki/Positive_integer",
                "/wiki/Electron_shells",
                "/wiki/Bohr_model",
                "/wiki/Electron_shell",
                "/wiki/Electron_subshell",
                "/wiki/Quantum_state",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Linear_combination",
                "/wiki/Linear_combination_of_atomic_orbitals_molecular_orbital_method",
                "/wiki/Principal_quantum_number",
                "/wiki/Azimuthal_quantum_number",
                "/wiki/Magnetic_quantum_number",
                "/wiki/Periodic_table",
                "/wiki/Hydrogen_atom",
                "/wiki/Ion",
                "/wiki/Eigenstates",
                "/wiki/Hamiltonian_operator",
                "/wiki/Hydrogen_atom",
                "/wiki/X-ray_notation",
                "/wiki/Principal_quantum_number",
                "/wiki/Electron_shell#Subshells",
                "/wiki/Angular_quantum_number",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Linus_Pauling",
                "/wiki/Robert_S._Mulliken",
                "/wiki/Max_Born",
                "/wiki/Probability_distribution",
                "/wiki/Werner_Heisenberg",
                "/wiki/Uncertainty_principle",
                "/wiki/Niels_Bohr",
                "/wiki/Wave_packet",
                "/wiki/Hydrogen",
                "/wiki/Helium",
                "/wiki/Argon",
                "/wiki/Quantum_mechanics",
                "/wiki/Electron_shell",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Louis_de_Broglie",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hydrogen-like_atom",
                "/wiki/Albert_Einstein",
                "/wiki/Photoelectric_effect",
                "/wiki/Emission_spectra",
                "/wiki/Absorption_spectra",
                "/wiki/Matter_waves",
                "/wiki/Quantum_mechanics",
                "/wiki/Ernest_Rutherford",
                "/wiki/Niels_Bohr",
                "/wiki/Reduced_Planck_constant",
                "/wiki/Bohr_model",
                "/wiki/Hantaro_Nagaoka",
                "/wiki/Saturnian_model",
                "/wiki/J._J._Thomson",
                "/wiki/Plum_pudding_model",
                "/wiki/Robert_Mulliken",
                "/wiki/Niels_Bohr",
                "/wiki/Hantaro_Nagaoka",
                "/wiki/Quantum_mechanics",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hydrogen-like_atom",
                "/wiki/Coordinate_system",
                "/wiki/Spherical_coordinates",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Spherical_harmonics#Real_form",
                "/wiki/Spherical_harmonics",
                "/wiki/Hartree%E2%80%93Fock",
                "/wiki/Molecular_orbital_theory",
                "/wiki/Configuration_interaction",
                "/wiki/Atomic_electron_transition",
                "/wiki/Fermion",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Electron_correlation",
                "/wiki/Atomic_physics",
                "/wiki/Atomic_spectral_line",
                "/wiki/Atomic_electron_transition",
                "/wiki/Quantum_state",
                "/wiki/Quantum_number",
                "/wiki/Term_symbol",
                "/wiki/Neon",
                "/wiki/Quantum_mechanics",
                "/wiki/Eigenstate",
                "/wiki/Hamiltonian_(quantum_mechanics)",
                "/wiki/Configuration_interaction",
                "/wiki/Basis_set_(chemistry)",
                "/wiki/Linear_combination",
                "/wiki/Slater_determinant",
                "/wiki/Spin_(physics)",
                "/wiki/Nuclear_structure#The_independent-particle_model",
                "/wiki/London_dispersion_force",
                "/wiki/Uncertainty_principle",
                "/wiki/Quantum_mechanics",
                "/wiki/Wave%E2%80%93particle_duality",
                "/wiki/Hydrogen_atom",
                "/wiki/Chemical_element",
                "/wiki/Periodic_table",
                "/wiki/Aufbau_principle",
                "/wiki/Quantum_number",
                "/wiki/Energy",
                "/wiki/Angular_momentum",
                "/wiki/Vector_component",
                "/wiki/Magnetic_quantum_number",
                "/wiki/Spin_quantum_number",
                "/wiki/Electron_configuration",
                "/wiki/Alkali_metal",
                "/wiki/Spectral_line",
                "/wiki/Quantum_mechanics",
                "/wiki/Function_(mathematics)",
                "/wiki/Electron",
                "/wiki/Atom",
                "/wiki/Probability",
                "/wiki/Atomic_nucleus"
            ],
            "text": "The atomic orbital model is nevertheless an approximation to the full quantum theory, which only recognizes many electron states. The predictions of line spectra are qualitatively useful but are not quantitatively accurate for atoms and ions other than those containing only one electron.The atomic orbital model thus predicts line spectra, which are observed experimentally. This is one of the main validations of the atomic orbital model.By quantum theory, state\u00a01 has a fixed energy of E1, and state\u00a02 has a fixed energy of E2. Now, what would happen if an electron in state\u00a01 were to move to state\u00a02? For this to happen, the electron would need to gain an energy of exactly E2 \u2212 E1. If the electron receives energy that is less than or greater than this value, it cannot jump from state\u00a01 to state\u00a02. Now, suppose we irradiate the atom with a broad-spectrum of light. Photons that reach the atom that have an energy of exactly E2 \u2212 E1 will be absorbed by the electron in state\u00a01, and that electron will jump to state\u00a02. However, photons that are greater or lower in energy cannot be absorbed by the electron, because the electron can only jump to one of the orbitals, it cannot jump to a state between orbitals. The result is that only photons of a specific frequency will be absorbed by the atom. This creates a line in the spectrum, known as an absorption line, which corresponds to the energy difference between states 1 and 2.State 2) n = 2, \u2113 = 0, m\u2113 = 0 and s = +1/2State 1) n = 1, \u2113 = 0, m\u2113 = 0 and s = +1/2Consider two states of the hydrogen atom:Bound quantum states have discrete energy levels. When applied to atomic orbitals, this means that the energy differences between states are also discrete. A transition between these states (i.e., an electron absorbing or emitting a photon) can thus only happen if the photon has an energy corresponding with the exact energy difference between said states.There are no nodes in relativistic orbital densities, although individual components of the wave function will have nodes.[31]Examples of significant physical outcomes of this effect include the lowered melting temperature of mercury (which results from 6s electrons not being available for metal bonding) and the golden color of gold and caesium.[29]For elements with high atomic number Z, the effects of relativity become more pronounced, and especially so for s\u00a0electrons, which move at relativistic velocities as they penetrate the screening electrons near the core of high-Z atoms. This relativistic increase in momentum for high speed electrons causes a corresponding decrease in wavelength and contraction of 6s orbitals relative to 5d orbitals (by comparison to corresponding s and d electrons in lighter elements in the same column of the periodic table); this results in 6s valence electrons becoming lowered in energy.The number of electrons in an electrically neutral atom increases with the atomic number. The electrons in the outermost shell, or valence electrons, tend to be responsible for an element's chemical behavior. Elements that contain the same number of valence electrons can be grouped together and display similar chemical properties.Although this is the general order of orbital filling according to the Madelung rule, there are exceptions, and the actual electronic energies of each element are also dependent upon additional details of the atoms (see Electron configuration#Atoms: Aufbau principle and Madelung rule).The \"periodic\" nature of the filling of orbitals, as well as emergence of the s, p, d and f \"blocks\", is more obvious if this order of filling is given in matrix form, with increasing principal quantum numbers starting the new rows (\"periods\") in the matrix. Then, each subshell (composed of the first two quantum numbers) is repeated as many times as required for each pair of electrons it may contain. The result is a compressed periodic table, with each entry representing two successive elements:The following is the order for filling the \"subshell\" orbitals, which also gives the order of the \"blocks\" in the periodic table:The periodic table may also be divided into several numbered rectangular 'blocks'. The elements belonging to a given block have this common feature: their highest-energy electrons all belong to the same \u2113-state (but the n associated with that \u2113-state depends upon the period). For instance, the leftmost two columns constitute the 's-block'. The outermost electrons of Li and Be respectively belong to the 2s\u00a0subshell, and those of Na and Mg to the 3s\u00a0subshell.This behavior is responsible for the structure of the periodic table. The table may be divided into several rows (called 'periods'), numbered starting with 1 at the top. The presently known elements occupy seven periods. If a certain period has number i, it consists of elements whose outermost electrons fall in the ith shell. Niels Bohr was the first to propose (1923) that the periodicity in the properties of the elements might be explained by the periodic filling of the electron energy levels, resulting in the electronic structure of the atom.[28]Additionally, an electron always tends to fall to the lowest possible energy state. It is possible for it to occupy any orbital so long as it does not violate the Pauli exclusion principle, but if lower-energy orbitals are available, this condition is unstable. The electron will eventually lose energy (by releasing a photon) and drop into the lower orbital. Thus, electrons fill orbitals in the order specified by the energy sequence given above.Several rules govern the placement of electrons in orbitals (electron configuration). The first dictates that no two electrons in an atom may have the same set of values of quantum numbers (this is the Pauli exclusion principle). These quantum numbers include the three that define orbitals, as well as s, or spin quantum number. Thus, two electrons may occupy a single orbital, so long as they have different values of\u00a0s. However, only two electrons, because of their spin, can be associated with each orbital.Note: empty cells indicate non-existent sublevels, while numbers in italics indicate sublevels that could (potentially) exist, but which do not hold electrons in any element currently known.In addition, the drum modes analogous to p and d modes in an atom show spatial irregularity along the different radial directions from the center of the drum, whereas all of the modes analogous to s\u00a0modes are perfectly symmetrical in radial direction. The non radial-symmetry properties of non-s orbitals are necessary to localize a particle with angular momentum and a wave nature in an orbital where it must tend to stay away from the central attraction force, since any particle localized at the point of central attraction could have no angular momentum. For these modes, waves in the drum head tend to avoid the central point. Such features again emphasize that the shapes of atomic orbitals are a direct consequence of the wave nature of electrons.None of the other sets of modes in a drum membrane have a central antinode, and in all of them the center of the drum does not move. These correspond to a node at the nucleus for all non-s orbitals in an atom. These orbitals all have some angular momentum, and in the planetary model, they correspond to particles in orbit with eccentricity less than 1.0, so that they do not pass straight through the center of the primary body, but keep somewhat away from it.Below, a number of drum membrane vibration modes and the respective wave functions of the hydrogen atom are shown. A correspondence can be considered where the wave functions of a vibrating drum head are for a two-coordinate system \u03c8(r,\u2009\u03b8) and the wave functions for a vibrating sphere are three-coordinate \u03c8(r,\u2009\u03b8,\u2009\u03c6).A mental \"planetary orbit\" picture closest to the behavior of electrons in s\u00a0orbitals, all of which have no angular momentum, might perhaps be that of a Keplerian orbit with the orbital eccentricity of 1 but a finite major axis, not physically possible (because particles were to collide), but can be imagined as a limit of orbits with equal major axes but increasing eccentricity.This relationship means that certain key features can be observed in both drum membrane modes and atomic orbitals. For example, in all of the modes analogous to s\u00a0orbitals (the top row in the animated illustration below), it can be seen that the very center of the drum membrane vibrates most strongly, corresponding to the antinode in all s\u00a0orbitals in an atom. This antinode means the electron is most likely to be at the physical position of the nucleus (which it passes straight through without scattering or striking it), since it is moving (on average) most rapidly at that point, giving it maximal momentum.The shapes of atomic orbitals can be qualitatively understood by considering the analogous case of standing waves on a circular drum.[27] To see the analogy, the mean vibrational displacement of each bit of drum membrane from the equilibrium point over many cycles (a measure of average drum membrane velocity and momentum at that point) must be considered relative to that point's distance from the center of the drum head. If this displacement is taken as being analogous to the probability of finding an electron at a given distance from the nucleus, then it will be seen that the many modes of the vibrating disk form patterns that trace the various shapes of atomic orbitals. The basic reason for this correspondence lies in the fact that the distribution of kinetic energy and momentum in a matter-wave is predictive of where the particle associated with the wave will be. That is, the probability of finding an electron at a given place is also a function of the electron's average momentum at that point, since high electron momentum at a given position tends to \"localize\" the electron in that position, via the properties of electron wave-packets (see the Heisenberg uncertainty principle for details of the mechanism).This table shows all orbital configurations for the real hydrogen-like wave functions up to 7s, and therefore covers the simple electronic configuration for all elements in the periodic table up to radium. \"\u03c8\" graphs are shown with \u2212 and + wave function phases shown in two different colors (arbitrarily red and blue). The pz orbital is the same as the p0 orbital, but the px and py are formed by taking linear combinations of the p+1 and p\u22121 orbitals (which is why they are listed under the m = \u00b11 label). Also, the p+1 and p\u22121 are not the same shape as the p0, since they are pure spherical harmonics.Although individual orbitals are most often shown independent of each other, the orbitals coexist around the nucleus at the same time.The shapes of atomic orbitals in one-electron atom are related to 3-dimensional spherical harmonics. These shapes are not unique, and any linear combination is valid, like a transformation to cubic harmonics, in fact it is possible to generate sets where all the d's are the same shape, just like the px, py, and pz are the same shape.[25][26]Additionally, as is the case with the s orbitals, individual p, d, f and g orbitals with n values higher than the lowest possible value, exhibit an additional radial node structure which is reminiscent of harmonic waves of the same type, as compared with the lowest (or fundamental) mode of the wave. As with s orbitals, this phenomenon provides p, d, f, and g orbitals at the next higher possible value of n (for example, 3p orbitals vs. the fundamental 2p), an additional node in each lobe. Still higher values of n further increase the number of radial nodes, for each type of orbital.There are seven f-orbitals, each with shapes more complex than those of the d-orbitals.Four of the five d-orbitals for n = 3 look similar, each with four pear-shaped lobes, each lobe tangent at right angles to two others, and the centers of all four lying in one plane. Three of these planes are the xy-, xz-, and yz-planes\u2014the lobes are between the pairs of primary axes\u2014and the fourth has the centres along the x and y axes themselves. The fifth and final d-orbital consists of three regions of high probability density: a torus with two pear-shaped regions placed symmetrically on its z axis. The overall total of 18 directional lobes point in every primary axis direction and between every pair.The shapes of p, d and f-orbitals are described verbally here and shown graphically in the Orbitals table below. The three p-orbitals for n = 2 have the form of two ellipsoids with a point of tangency at the nucleus (the two-lobed shape is sometimes referred to as a \"dumbbell\"\u2014there are two lobes pointing in opposite directions from each other). The three p-orbitals in each shell are oriented at right angles to each other, as determined by their respective linear combination of values of\u00a0m\u2113. The overall result is a lobe pointing along each direction of the primary axes.Also in general terms, \u2113 determines an orbital's shape, and m\u2113 its orientation. However, since some orbitals are described by equations in complex numbers, the shape sometimes depends on m\u2113 also. Together, the whole set of orbitals for a given \u2113 and n fill space as symmetrically as possible, though with increasingly complex sets of lobes and nodes.Generally speaking, the number n determines the size and energy of the orbital for a given nucleus: as n increases, the size of the orbital increases. When comparing different elements, the higher nuclear charge Z of heavier elements causes their orbitals to contract by comparison to lighter ones, so that the overall size of the whole atom remains very roughly constant, even as the number of electrons in heavier elements (higher Z) increases.The lobes can be viewed as standing wave interference patterns between the two counter rotating, ring resonant travelling wave \"m\" and \"\u2212m\" modes, with the projection of the orbital onto the xy plane having a resonant \"m\" wavelengths around the circumference. Though rarely depicted the travelling wave solutions can be viewed as rotating banded tori, with the bands representing phase information. For each m there are two standing wave solutions \u27e8m\u27e9+\u27e8\u2212m\u27e9 and \u27e8m\u27e9\u2212\u27e8\u2212m\u27e9. For the case where m = 0 the orbital is vertical, counter rotating information is unknown, and the orbital is z-axis symmetric. For the case where \u2113 = 0 there are no counter rotating modes. There are only radial modes and the shape is spherically symmetric. For any given n, the smaller \u2113 is, the more radial nodes there are. Loosely speaking n is energy, \u2113 is analogous to eccentricity, and m is orientation. In the classical case, a ring resonant travelling wave, for example in a circular transmission line, unless actively forced, will spontaneously decay into a ring resonant standing wave because reflections will build up over time at even the smallest imperfection or discontinuity.Sometimes the \u03c8 function will be graphed to show its phases, rather than the |\u2009\u03c8(r,\u2009\u03b8,\u2009\u03c6)\u2009|2 which shows probability density but has no phases (which have been lost in the process of taking the absolute value, since \u03c8(r,\u2009\u03b8,\u2009\u03c6) is a complex number). |\u2009\u03c8(r,\u2009\u03b8,\u2009\u03c6)\u2009|2 orbital graphs tend to have less spherical, thinner lobes than \u03c8(r,\u2009\u03b8,\u2009\u03c6) graphs, but have the same number of lobes in the same places, and otherwise are recognizable. This article, in order to show wave function phases, shows mostly \u03c8(r,\u2009\u03b8,\u2009\u03c6) graphs.Simple pictures showing orbital shapes are intended to describe the angular forms of regions in space where the electrons occupying the orbital are likely to be found. The diagrams cannot show the entire region where an electron can be found, since according to quantum mechanics there is a non-zero probability of finding the electron (almost) anywhere in space. Instead the diagrams are approximate representations of boundary or contour surfaces where the probability density |\u2009\u03c8(r,\u2009\u03b8,\u2009\u03c6)\u2009|2 has a constant value, chosen so that there is a certain probability (for example 90%) of finding the electron within the contour. Although |\u2009\u03c8\u2009|2 as the square of an absolute value is everywhere non-negative, the sign of the wave function \u03c8(r,\u2009\u03b8,\u2009\u03c6) is often indicated in each subregion of the orbital picture.where p0 = Rn\u20091\u2009Y1\u20090, p1 = Rn\u20091\u2009Y1\u20091, and p\u22121 = Rn\u20091\u2009Y1\u2009\u22121, are the complex orbitals corresponding to \u2113 = 1.In the real hydrogen-like orbitals, for example, n and \u2113 have the same interpretation and significance as their complex counterparts, but m is no longer a good quantum number (though its absolute value is). The orbitals are given new names based on their shape with respect to a standardized Cartesian basis. The real hydrogen-like p\u00a0orbitals are given by the following[20][21]An atom that is embedded in a crystalline solid feels multiple preferred axes, but often no preferred direction. Instead of building atomic orbitals out of the product of radial functions and a single spherical harmonic, linear combinations of spherical harmonics are typically used, designed so that the imaginary part of the spherical harmonics cancel out. These real orbitals are the building blocks most commonly shown in orbital visualizations.The above conventions imply a preferred axis (for example, the z direction in Cartesian coordinates), and they also imply a preferred direction along this preferred axis. Otherwise there would be no sense in distinguishing m = +1 from m = \u22121. As such, the model is most useful when applied to physical systems that share these symmetries. The Stern\u2013Gerlach experiment \u2014 where an atom is exposed to a magnetic field \u2014 provides one such example.[19]The Pauli exclusion principle states that no two electrons in an atom can have the same values of all four quantum numbers. If there are two electrons in an orbital with given values for three quantum numbers, (n, l, m), these two electrons must differ in their spin.Each electron also has a spin quantum number, s, which describes the spin of each electron (spin up or spin down). The number s can be +1/2 or \u22121/2.The principal quantum number n describes the energy of the electron and is always a positive integer. In fact, it can be any positive integer, but for reasons discussed below, large numbers are seldom encountered. Each atom has, in general, many orbitals associated with each value of n; these orbitals together are sometimes called electron shells.In physics, the most common orbital descriptions are based on the solutions to the hydrogen atom, where orbitals are given by the product between a radial function and a pure spherical harmonic. The quantum numbers, together with the rules governing their possible values, are as follows:Because of the quantum mechanical nature of the electrons around a nucleus, atomic orbitals can be uniquely defined by a set of integers known as quantum numbers. These quantum numbers only occur in certain combinations of values, and their physical interpretation changes depending on whether real or complex versions of the atomic orbitals are employed.The quantum number n first appeared in the Bohr model where it determines the radius of each circular electron orbit. In modern quantum mechanics however, n determines the mean distance of the electron from the nucleus; all electrons with the same value of n lie at the same average distance. For this reason, orbitals with the same value of n are said to comprise a \"shell\". Orbitals with the same value of n and also the same value of\u00a0\u2113 are even more closely related, and are said to comprise a \"subshell\".The stationary states (quantum states) of the hydrogen-like atoms are its atomic orbitals.[clarification needed] However, in general, an electron's behavior is not fully described by a single orbital. Electron states are best represented by time-depending \"mixtures\" (linear combinations) of multiple orbitals. See Linear combination of atomic orbitals molecular orbital method.A given (hydrogen-like) atomic orbital is identified by unique values of three quantum numbers: n, \u2113, and m\u2113. The rules restricting the values of the quantum numbers, and their energies (see below), explain the electron configuration of the atoms and the periodic table.For atoms with two or more electrons, the governing equations can only be solved with the use of methods of iterative approximation. Orbitals of multi-electron atoms are qualitatively similar to those of hydrogen, and in the simplest models, they are taken to have the same form. For more rigorous and precise analysis, the numerical approximations must be used.The simplest atomic orbitals are those that are calculated for systems with a single electron, such as the hydrogen atom. An atom of any other element ionized down to a single electron is very similar to hydrogen, and the orbitals take the same form. In the Schr\u00f6dinger equation for this system of one negative and one positive particle, the atomic orbitals are the eigenstates of the Hamiltonian operator for the energy. They can be obtained analytically, meaning that the resulting orbitals are products of a polynomial series, and exponential and trigonometric functions. (see hydrogen atom).For example, the orbital 1s2 (pronounced as the individual numbers and letters: \"one ess two\") has two electrons and is the lowest energy level (n = 1) and has an angular quantum number of \u2113 = 0. In X-ray notation, the principal quantum number is given a letter associated with it. For n = 1,\u20092,\u20093,\u20094,\u20095,\u2009\u2026, the letters associated with those numbers are K, L, M, N, O, \u2026 respectively.where X is the energy level corresponding to the principal quantum number n, type is a lower-case letter denoting the shape or subshell of the orbital and it corresponds to the angular quantum number\u00a0\u2113, and y is the number of electrons in that orbital.Orbitals are given names in the form:In the quantum picture of Heisenberg, Schr\u00f6dinger and others, the Bohr atom number\u00a0n for each orbital became known as an n-sphere[citation needed] in a three dimensional atom and was pictured as the mean energy of the probability cloud of the electron's wave packet which surrounded the atom.In chemistry, Schr\u00f6dinger, Pauling, Mulliken and others noted that the consequence of Heisenberg's relation was that the electron, as a wave packet, could not be considered to have an exact location in its orbital. Max Born suggested that the electron's position needed to be described by a probability distribution which was connected with finding the electron at some point in the wave-function which described its associated wave packet. The new quantum mechanics did not give exact results, but only the probabilities for the occurrence of a variety of possible such results. Heisenberg held that the path of a moving particle has no meaning if we cannot observe it, as we cannot with electrons in an atom.Immediately after Heisenberg discovered his uncertainty principle,[17] Bohr noted that the existence of any sort of wave packet implies uncertainty in the wave frequency and wavelength, since a spread of frequencies is needed to create the packet itself.[18] In quantum mechanics, where all particle momenta are associated with waves, it is the formation of such a wave packet which localizes the wave, and thus the particle, in space. In states where a quantum mechanical particle is bound, it must be localized as a wave packet, and the existence of the packet and its minimum size implies a spread and minimal value in particle wavelength, and thus also momentum and energy. In quantum mechanics, as a particle is localized to a smaller region in space, the associated compressed wave packet requires a larger and larger range of momenta, and thus larger kinetic energy. Thus the binding energy to contain or trap a particle in a smaller region of space increases without bound as the region of space grows smaller. Particles cannot be restricted to a geometric point in space, since this would require an infinite particle momentum.The Bohr model was able to explain the emission and absorption spectra of hydrogen. The energies of electrons in the n = 1, 2, 3, etc. states in the Bohr model match those of current physics. However, this did not explain similarities between different atoms, as expressed by the periodic table, such as the fact that helium (two electrons), neon (10 electrons), and argon (18 electrons) exhibit similar chemical inertness. Modern quantum mechanics explains this in terms of electron shells and subshells which can each hold a number of electrons determined by the Pauli exclusion principle. Thus the n = 1 state can hold one or two electrons, while the n = 2 state can hold up to eight electrons in 2s and 2p subshells. In helium, all n = 1 states are fully occupied; the same for n = 1 and n = 2 in neon. In argon the 3s and 3p subshells are similarly fully occupied by eight electrons; quantum mechanics also allows a 3d subshell but this is at higher energy than the 3s and 3p in argon (contrary to the situation in the hydrogen atom) and remains empty.With de Broglie's suggestion of the existence of electron matter waves in 1924, and for a short time before the full 1926 Schr\u00f6dinger equation treatment of hydrogen-like atom, a Bohr electron \"wavelength\" could be seen to be a function of its momentum, and thus a Bohr orbiting electron was seen to orbit in a circle at a multiple of its half-wavelength (this physically incorrect Bohr model is still often taught to beginning students). The Bohr model for a short time could be seen as a classical model with an additional constraint provided by the 'wavelength' argument. However, this period was immediately superseded by the full three-dimensional wave mechanics of 1926. In our current understanding of physics, the Bohr model is called a semi-classical model because of its quantization of angular momentum, not primarily because of its relationship with electron wavelength, which appeared in hindsight a dozen years after the Bohr model was proposed.After Bohr's use of Einstein's explanation of the photoelectric effect to relate energy levels in atoms with the wavelength of emitted light, the connection between the structure of electrons in atoms and the emission and absorption spectra of atoms became an increasingly useful tool in the understanding of electrons in atoms. The most prominent feature of emission and absorption spectra (known experimentally since the middle of the 19th century), was that these atomic spectra contained discrete lines. The significance of the Bohr model was that it related the lines in emission and absorption spectra to the energy differences between the orbits that electrons could take around an atom. This was, however, not achieved by Bohr through giving the electrons some kind of wave-like properties, since the idea that electrons could behave as matter waves was not suggested until eleven years later. Still, the Bohr model's use of quantized angular momenta and therefore quantized energy levels was a significant step towards the understanding of electrons in atoms, and also a significant step towards the development of quantum mechanics in suggesting that quantized restraints must account for all discontinuous energy levels and spectra in atoms.In 1909, Ernest Rutherford discovered that the bulk of the atomic mass was tightly condensed into a nucleus, which was also found to be positively charged. It became clear from his analysis in 1911 that the plum pudding model could not explain atomic structure. In 1913 as Rutherford's post-doctoral student, Niels Bohr proposed a new model of the atom, wherein electrons orbited the nucleus with classical periods, but were only permitted to have discrete values of angular momentum, quantized in units h/2\u03c0.[10] This constraint automatically permitted only certain values of electron energies. The Bohr model of the atom fixed the problem of energy loss from radiation from a ground state (by declaring that there was no state below this), and more importantly explained the origin of spectral lines.Shortly after Thomson's discovery, Hantaro Nagaoka predicted a different model for electronic structure.[11] Unlike the plum pudding model, the positive charge in Nagaoka's \"Saturnian Model\" was concentrated into a central core, pulling the electrons into circular orbits reminiscent of Saturn's rings. Few people took notice of Nagaoka's work at the time,[15] and Nagaoka himself recognized a fundamental defect in the theory even at its conception, namely that a classical charged object cannot sustain orbital motion because it is accelerating and therefore loses energy due to electromagnetic radiation.[16] Nevertheless, the Saturnian model turned out to have more in common with modern theory than any of its contemporaries.With J. J. Thomson's discovery of the electron in 1897,[13] it became clear that atoms were not the smallest building blocks of nature, but were rather composite particles. The newly discovered structure within atoms tempted many to imagine how the atom's constituent parts might interact with each other. Thomson theorized that multiple electrons revolved in orbit-like rings within a positively charged jelly-like substance,[14] and between the electron's discovery and 1909, this \"plum pudding model\" was the most widely accepted explanation of atomic structure.The term \"orbital\" was coined by Robert Mulliken in 1932 as an abbreviation for one-electron orbital wave function.[9] However, the idea that electrons might revolve around a compact nucleus with definite angular momentum was convincingly argued at least 19 years earlier by Niels Bohr,[10] and the Japanese physicist Hantaro Nagaoka published an orbit-based hypothesis for electronic behavior as early as 1904.[11] Explaining the behavior of these electron \"orbits\" was one of the driving forces behind the development of quantum mechanics.[12]Although hydrogen-like orbitals are still used as pedagogical tools, the advent of computers has made STOs preferable for atoms and diatomic molecules since combinations of STOs can replace the nodes in hydrogen-like atomic orbital. Gaussians are typically used in molecules with three or more atoms. Although not as accurate by themselves as STOs, combinations of many Gaussians can attain the accuracy of hydrogen-like orbitals.Atomic orbitals can be the hydrogen-like \"orbitals\" which are exact solutions to the Schr\u00f6dinger equation for a hydrogen-like \"atom\" (i.e., an atom with one electron). Alternatively, atomic orbitals refer to functions that depend on the coordinates of one electron (i.e., orbitals) but are used as starting points for approximating wave functions that depend on the simultaneous coordinates of all the electrons in an atom or molecule. The coordinate systems chosen for atomic orbitals are usually spherical coordinates (r,\u2009\u03b8,\u2009\u03c6) in atoms and cartesians (x,\u2009y,\u2009z) in polyatomic molecules. The advantage of spherical coordinates (for atoms) is that an orbital wave function is a product of three factors each dependent on a single coordinate: \u03c8(r,\u2009\u03b8,\u2009\u03c6) = R(r)\u2009\u0398(\u03b8)\u2009\u03a6(\u03c6). The angular factors of atomic orbitals \u0398(\u03b8)\u2009\u03a6(\u03c6) generate s, p, d, etc. functions as real combinations of spherical harmonics Y\u2113m(\u03b8,\u2009\u03c6) (where \u2113 and m are quantum numbers). There are typically three mathematical forms for the radial functions\u00a0R(r) which can be chosen as a starting point for the calculation of the properties of atoms and molecules with many electrons:Fundamentally, an atomic orbital is a one-electron wave function, even though most electrons do not exist in one-electron atoms, and so the one-electron view is an approximation. When thinking about orbitals, we are often given an orbital visualization heavily influenced by the Hartree\u2013Fock approximation, which is one way to reduce the complexities of molecular orbital theory.This notation means that the corresponding Slater determinants have a clear higher weight in the configuration interaction expansion. The atomic orbital concept is therefore a key concept for visualizing the excitation process associated with a given transition. For example, one can say for a given transition that it corresponds to the excitation of an electron from an occupied orbital to a given unoccupied orbital. Nevertheless, one has to keep in mind that electrons are fermions ruled by the Pauli exclusion principle and cannot be distinguished from the other electrons in the atom. Moreover, it sometimes happens that the configuration interaction expansion converges very slowly and that one cannot speak about simple one-determinant wave function at all. This is the case when electron correlation is large.In atomic physics, the atomic spectral lines correspond to transitions (quantum leaps) between quantum states of an atom. These states are labeled by a set of quantum numbers summarized in the term symbol and usually associated with particular electron configurations, i.e., by occupation schemes of atomic orbitals (for example, 1s2\u00a02s2\u00a02p6 for the ground state of neon\u2014term symbol: 1S0).Atomic orbitals may be defined more precisely in formal quantum mechanical language. Specifically, in quantum mechanics, the state of an atom, i.e., an eigenstate of the atomic Hamiltonian, is approximated by an expansion (see configuration interaction expansion and basis set) into linear combinations of anti-symmetrized products (Slater determinants) of one-electron functions. The spatial components of these one-electron functions are called atomic orbitals. (When one considers also their spin component, one speaks of atomic spin orbitals.) A state is actually a function of the coordinates of all the electrons, so that their motion is correlated, but this is often approximated by this independent-particle model of products of single electron wave functions.[8] (The London dispersion force, for example, depends on the correlations of the motion of the electrons.)Thus, despite the popular analogy to planets revolving around the Sun, electrons cannot be described simply as solid particles. In addition, atomic orbitals do not closely resemble a planet's elliptical path in ordinary atoms. A more accurate analogy might be that of a large and often oddly shaped \"atmosphere\" (the electron), distributed around a relatively tiny planet (the atomic nucleus). Atomic orbitals exactly describe the shape of this \"atmosphere\" only when a single electron is present in an atom. When more electrons are added to a single atom, the additional electrons tend to more evenly fill in a volume of space around the nucleus so that the resulting collection (sometimes termed the atom's \"electron cloud\"[7]) tends toward a generally spherical zone of probability describing the electron's location, because of the uncertainty principle.Particle-like properties:Wave-like properties:With the development of quantum mechanics and experimental findings (such as the two slit diffraction of electrons), it was found that the orbiting electrons around a nucleus could not be fully described as particles, but needed to be explained by the wave-particle duality. In this sense, the electrons have the following properties:Atomic orbitals are the basic building blocks of the atomic orbital model (alternatively known as the electron cloud or wave mechanics model), a modern framework for visualizing the submicroscopic behavior of electrons in matter. In this model the electron cloud of a multi-electron atom may be seen as being built up (in approximation) in an electron configuration that is a product of simpler hydrogen-like atomic orbitals. The repeating periodicity of the blocks of 2, 6, 10, and 14 elements within sections of the periodic table arises naturally from the total number of electrons that occupy a complete set of s, p, d and f atomic orbitals, respectively, although for higher values of the quantum number n, particularly when the atom in question bears a positive charge, the energies of certain sub-shells become very similar and so the order in which they are said to be populated by electrons (e.g. Cr = [Ar]4s13d5 and Cr2+ = [Ar]3d4) can only be rationalized somewhat arbitrarily.Each orbital in an atom is characterized by a unique set of values of the three quantum numbers n, \u2113, and m, which respectively correspond to the electron's energy, angular momentum, and an angular momentum vector component (the magnetic quantum number). Each such orbital can be occupied by a maximum of two electrons, each with its own spin quantum number s. The simple names s orbital, p orbital, d orbital and f orbital refer to orbitals with angular momentum quantum number \u2113 = 0, 1, 2 and 3 respectively. These names, together with the value of\u00a0n, are used to describe the electron configurations of atoms. They are derived from the description by early spectroscopists of certain series of alkali metal spectroscopic lines as sharp, principal, diffuse, and fundamental. Orbitals for \u2113 > 3 continue alphabetically, omitting\u00a0j (g, h, i, k, \u2026)[3][4][5] because some languages do not distinguish between the letters \"i\" and \"j\".[6]In quantum mechanics, an atomic orbital is a mathematical function that describes the wave-like behavior of either one electron or a pair of electrons in an atom.[1] This function can be used to calculate the probability of finding any electron of an atom in any specific region around the atom's nucleus. The term atomic orbital may also refer to the physical region or space where the electron can be calculated to be present, as defined by the particular mathematical form of the orbital.[2]",
            "title": "Atomic orbital",
            "url": "https://en.wikipedia.org/wiki/Atomic_orbital"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Molecular orbital",
            "url": "https://en.wikipedia.org/wiki/Molecular_orbital"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Closed-shell",
                "/wiki/Atomic_orbital",
                "/wiki/Hartree%E2%80%93Fock_method",
                "/wiki/Quantum_mechanics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Energy_operator",
                "/wiki/Quantum_mechanics",
                "/wiki/Basis_set_(chemistry)",
                "/wiki/Computational_chemistry",
                "/wiki/Roothaan_equations",
                "/wiki/Hamiltonian_(quantum_theory)",
                "/wiki/Operator_(mathematics)",
                "/wiki/Electron",
                "/wiki/Coulomb_force",
                "/wiki/Electron_correlation"
            ],
            "text": "For systems with unpaired electrons there are many choices of Fock matrices.The Coulomb operator is multiplied by two since there are two electrons in each occupied orbital. The exchange operator is not multiplied by two since it has a non-zero result only for electrons which have the same spin as the i-th electron.where:The Fock matrix is defined by the Fock operator. For the restricted case which assumes closed-shell orbitals and single- determinantal wavefunctions, the Fock operator for the i-th electron is given by:[2]In the Hartree\u2013Fock method of quantum mechanics, the Fock matrix is a matrix approximating the single-electron energy operator of a given quantum system in a given set of basis vectors.[1] It is most often formed in computational chemistry when attempting to solve the Roothaan equations for an atomic or molecular system. The Fock matrix is actually an approximation to the true Hamiltonian operator of the quantum system. It includes the effects of electron-electron repulsion only in an average way. Also significant to note is that: because the Fock operator is a one-electron operator, it does not include the electron correlation energy.",
            "title": "Fock matrix",
            "url": "https://en.wikipedia.org/wiki/Fock_operator"
        },
        {
            "desc_links": [
                "/wiki/Particle_accelerator#Electrostatic_particle_accelerators",
                "/wiki/Periodic_table",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Electron_volt",
                "/wiki/Mole_(unit)",
                "/wiki/Mole_(unit)",
                "/wiki/Enthalpy",
                "/wiki/Kilojoules",
                "/wiki/Kilocalories",
                "/wiki/Endothermic_process",
                "/wiki/Electron",
                "/wiki/Valence_electron",
                "/wiki/Ion"
            ],
            "links": [
                "/wiki/Work_function",
                "/wiki/Bond_length",
                "/wiki/Molecular_orbital",
                "/wiki/Potential_energy_surface",
                "/wiki/Molecular_vibration",
                "/wiki/Quantum_harmonic_oscillator",
                "/wiki/Molecular_vibration",
                "/wiki/Excited_state",
                "/wiki/Vibrational_spectroscopy",
                "/wiki/Franck%E2%80%93Condon_principle",
                "/wiki/Adiabatic_theorem",
                "/wiki/Molecular_vibration",
                "/wiki/Ground_state",
                "/wiki/Molecular_geometry",
                "/wiki/Quantum_mechanics",
                "/wiki/Electron_cloud",
                "/wiki/Atomic_orbital",
                "/wiki/Wavefunction",
                "/wiki/Slater_determinant",
                "/wiki/Pauli%27s_exclusion_principle",
                "/wiki/Molecular_orbital",
                "/wiki/Chemical_element",
                "/wiki/Elementary_charge",
                "/wiki/Electrostatic_potential",
                "/wiki/Bohr_model",
                "/wiki/Gaussian_units",
                "/wiki/Atomic_radius",
                "/wiki/Noble_gas",
                "/wiki/Neon",
                "/wiki/Electron_shell",
                "/wiki/Particle_accelerator#Electrostatic_particle_accelerators",
                "/wiki/Periodic_table",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Electron_volt",
                "/wiki/Mole_(unit)",
                "/wiki/Mole_(unit)",
                "/wiki/Enthalpy",
                "/wiki/Kilojoules",
                "/wiki/Kilocalories",
                "/wiki/Endothermic_process",
                "/wiki/Electron",
                "/wiki/Valence_electron",
                "/wiki/Ion"
            ],
            "text": "Work function is the minimum amount of energy required to remove an electron from a solid surface.A generic term for the ionization energy that can be used for species with any charge state. For example, the electron binding energy for the chloride ion is the minimum amount of energy required to remove an electron from the chlorine atom when it has a charge of -1. In this particular example, the electron binding energy has the same magnitude as the electron affinity for the neutral chlorine atom. In another example, the electron binding energy refers the minimum amount of energy required to remove an electron from the dicarboxylate dianion \u2212O2C(CH2)8CO\u2212\n2.While the term ionization energy is largely used only for gas-phase atomic or molecular species, there are a number of analogous quantities that consider the amount of energy required to remove an electron from other physical systems.In many circumstances, the adiabatic ionization energy is often a more interesting physical quantity since it describes the difference in energy between the two potential energy surfaces. However, due to experimental limitations, the adiabatic ionization energy is often difficult to determine, whereas the vertical detachment energy is easily identifiable and measurable.For a diatomic molecule, the geometry is defined by the length of a single bond. The removal of an electron from a bonding molecular orbital weakens the bond and increases the bond length. In Figure 1, the lower potential energy curve is for the neutral molecule and the upper surface is for the positive ion. Both curves plot the potential energy as a function of bond length. The horizontal lines correspond to vibrational levels with their associated vibrational wave functions. Since the ion has a weaker bond, it will have a longer bond length. This effect is represented by shifting the minimum of the potential energy curve to the right of the neutral species. The adiabatic ionization is the diagonal transition to the vibrational ground state of the ion. Vertical ionization involves vibrational excitation of the ionic state and therefore requires greater energy.Due to the possible changes in molecular geometry that may result from ionization, additional transitions may exist between the vibrational ground state of the neutral species and vibrational excited states of the positive ion. In other words, ionization is accompanied by vibrational excitation. The intensity of such transitions are explained by the Franck\u2013Condon principle, which predicts that the most probable and intense transition corresponds to the vibrational excited state of the positive ion that has the same geometry as the neutral molecule. This transition is referred to as the \"vertical\" ionization energy since it is represented by a completely vertical line on a potential energy diagram (see Figure).The adiabatic ionization energy of a molecule is the minimum amount of energy required to remove an electron from a neutral molecule, i.e. the difference between the energy of the vibrational ground state of the neutral species (v\" = 0 level) and that of the positive ion (v' = 0). The specific equilibrium geometry of each species does not affect this value.Ionization of molecules often leads to changes in molecular geometry, and two types of (first) ionization energy are defined \u2013 adiabatic and vertical.[5]According to the more complete theory of quantum mechanics, the location of an electron is best described as a probability distribution within an electron cloud, i.e. atomic orbital. The energy can be calculated by integrating over this cloud. The cloud's underlying mathematical representation is the wavefunction which is built from Slater determinants consisting of molecular spin orbitals. These are related by Pauli's exclusion principle to the antisymmetrized products of the atomic or molecular orbitals.Now the energy can be found in terms of Z, e, and r. Using the new value for the kinetic energy in the total energy equation above, it is found that:This establishes the dependence of the radius on n. That is:Solving the angular momentum for v and substituting this into the expression for kinetic energy, we have:Velocity can be eliminated from the kinetic energy term by setting the Coulomb attraction equal to the centripetal force, giving:The total energy of the atom is the sum of the kinetic and potential energies, that is:It is possible to expand this model considerably by taking a semi-classical approach, in which momentum is quantized. This approach works very well for the hydrogen atom, which only has one electron. The magnitude of the angular momentum for a circular orbit is:This analysis is incomplete, as it leaves the distance a as an unknown variable. It can be made more rigorous by assigning to each electron of every chemical element a characteristic distance, chosen so that this relation agrees with experimental data.Since the electron is negatively charged, it is drawn inwards by this positive electrostatic potential. The energy required for the electron to \"climb out\" and leave the atom is:Consider an electron of charge -e and an atomic nucleus with charge +Ze, where Z is the number of protons in the nucleus. According to the Bohr model, if the electron were to approach and bond with the atom, it would come to rest at a certain radius a. The electrostatic potential V at distance a from the ionic nucleus, referenced to a point infinitely far away, is:Atomic ionization energy can be predicted by an analysis using electrostatic potential and the Bohr model of the atom, as follows (note that the derivation uses Gaussian units).Ionization energy is also a periodic trend within the periodic table organization. Moving left to right within a period, or upward within a group, the first ionization energy generally increases, with some exceptions such as aluminum and sulfur in the table above. As the nuclear charge of the nucleus increases across the period, the atomic radius decreases and the electron cloud becomes closer towards the nucleus.Large jumps in the successive molar ionization energies occur when passing noble gas configurations. For example, as can be seen in the table above, the first two molar ionization energies of magnesium (stripping the two 3s electrons from a magnesium atom) are much smaller than the third, which requires stripping off a 2p electron from the neon configuration of Mg2+. That electron is much closer to the nucleus than the previous 3s electron.Some values for elements of the third period are given in the following table:Generally, the (n+1)th ionization energy is larger than the nth ionization energy. When the next ionization energy involves removing an electron from the same electron shell, the increase in ionization energy is primarily due to the increased net charge of the ion from which the electron is being removed. Electrons removed from more highly charged ions of a particular element experience greater forces of electrostatic attraction; thus, their removal requires more energy. In addition, when the next ionization energy involves removing an electron from a lower electron shell, the greatly decreased distance between the nucleus and the electron also increases both the electrostatic force and the distance over which that force must be overcome to remove the electron. Both of these factors further increase the ionization energy.The term ionization potential is an older name for ionization energy,[2] because the oldest method of measuring ionization energies was based on ionizing a sample and accelerating the electron removed using an electrostatic potential. However this term is now considered obsolete.[3] Some factors affecting the ionization energy include:The nth ionization energy refers to the amount of energy required to remove an electron from the species with a charge of (n-1). For example, the first three ionization energies are defined as follows:(The latter is due to the outer electron shell being progressively further away from the nucleus with the addition of one inner shell per row as one moves down the column.)Comparison of IEs of atoms in the periodic table reveals two patterns:The units for ionization energy are different in physics and chemistry. In physics, the unit is the amount of energy required to remove a single electron from a single atom or molecule: expressed as an electron volt. In chemistry, the units are the amount of energy it takes for all of the atoms in a mole of substance to lose one electron each: molar ionization energy or enthalpy, expressed as kilojoules per mole (kJ/mol) or kilocalories per mole (kcal/mol).[1]Generally, the closer the electrons are to the nucleus of the atom, the higher the atom's ionization energy.where X is any atom or molecule capable of being ionized, X+ is that atom or molecule with an electron removed, and e\u2212 is the removed electron. This is an endothermic process.The ionization energy (IE) is qualitatively defined as the amount of energy required to remove the most loosely bound electron, the valence electron, of an isolated gaseous atom to form a cation. It is quantitatively expressed in symbols as",
            "title": "Ionization energy",
            "url": "https://en.wikipedia.org/wiki/Ionization_potential"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Koopmans' theorem",
            "url": "https://en.wikipedia.org/wiki/Koopmans%27_theorem"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Computer_science",
                "/wiki/Recursion",
                "/wiki/Algorithm"
            ],
            "links": [
                "/wiki/Iteratee",
                "/wiki/Object-oriented_programming",
                "/wiki/Iterator",
                "/wiki/Scheme_(programming_language)",
                "/wiki/Merge_sort",
                "/wiki/Functional_programming_language",
                "/wiki/Recursion",
                "/wiki/Pedagogy",
                "/wiki/Computer_program",
                "/wiki/Iterative_method",
                "/wiki/Newton%27s_method",
                "/wiki/Iterated_function",
                "/wiki/Collatz_conjecture",
                "/wiki/Juggler_sequence",
                "/wiki/Mathematics",
                "/wiki/Computer_science",
                "/wiki/Recursion",
                "/wiki/Algorithm"
            ],
            "text": "An iteratee is an abstraction which accepts or rejects data during an iteration.In Object-oriented programming, an iterator is an object that ensures iteration is executed in the same way for a range of different data structures, saving time and effort in later coding attempts.The code below is an example of a recursive algorithm in the Scheme programming language that will output the same result as the pseudocode under the previous heading.The classic example of recursion is in list-sorting algorithms such as Merge sort. The Merge Sort recursive algorithm will first repeatedly divide the list into consecutive pairs; each pair is then ordered, then each consecutive pair of pairs, and so forth until the elements of the list are in the desired order.Some types of programming languages, known as functional programming languages, are designed such that they do not set up block of statements for explicit repetition as with the for loop. Instead, those programming languages exclusively use recursion. Rather than call out a block of code to be repeated a pre-defined number of times, the executing code block instead \"divides\" the work to be done into a number separate pieces, after which the code block executes itself on each individual piece. Each piece of work will be divided repeatedly until the \"amount\" of work is as small as it can possibly be, at which point algorithm will do that work very quickly. The algorithm then \"reverses\" and reassembles the pieces into a complete whole.In algorithmic situations, recursion and iteration can be employed to the same effect. The primary difference is that recursion can be employed as a solution without prior knowledge as to how many times the action will have to repeat, while a successful iteration requires that foreknowledge.Unlike computing and math, educational iterations are not predetermined; instead, the task is repeated until success according to some external criteria (often a test) is achieved.In some schools of pedagogy, iterations are used to describe the process of teaching or guiding students to repeat experiments, assessments, or projects, until more accurate results are found, or the student has mastered the technical skill. This idea is found in the old adage, \"Practice makes perfect.\" In particular, \"iterative\" is defined as the \"process of learning and development that involves cyclical inquiry, enabling multiple opportunities for people to revisit ideas and critically reflect on their implication.\"[1]It is permissible, and often necessary, to use values from other parts of the program outside the bracketed block of statements, to perform the desired function. In the example above, the line of code is using the value of i as it increments.The pseudocode below is an example of iteration; the line of code between the brackets of the for loop will \"iterate\" three times:Iteration in computing is the technique marking out of a block of statements within a computer program for a defined number of repetitions. That block of statements is said to be iterated; a computer scientist might also refer to that block of statements as an \"iteration\".Another use of iteration in mathematics is in iterative methods which are used to produce approximate numerical solutions to certain mathematical problems. Newton's method is an example of an iterative method. Manual calculation of a number's square root is a common use and a well-known example.Iteration in mathematics may refer to the process of iterating a function i.e. applying a function repeatedly, using the output from one iteration as the input to the next. Iteration of apparently simple functions can produce complex behaviours and difficult problems \u2013 for examples, see the Collatz conjecture and juggler sequences.In the context of mathematics or computer science, iteration (along with the related technique of recursion) is a standard building block of algorithms.Iteration is the act of repeating a process, to generate a (possibly unbounded) sequence of outcomes, with the aim of approaching a desired goal, target or result. Each repetition of the process is also called an \"iteration\", and the results of one iteration are used as the starting point for the next iteration.",
            "title": "Iteration",
            "url": "https://en.wikipedia.org/wiki/Iteration"
        },
        {
            "desc_links": [
                "/wiki/Open_shell",
                "/wiki/Nuclear_physics",
                "/wiki/Bogoliubov_transformation",
                "/wiki/Nuclear_structure#Nuclear_pairing_phenomenon",
                "/wiki/Atomic_structure",
                "/wiki/Configuration_state_function",
                "/wiki/Quantum_number",
                "/wiki/Ground_state",
                "/wiki/Hartree_equation",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Douglas_Hartree",
                "/wiki/Hartree%E2%80%93Fock#The_Fock_operator",
                "/wiki/Iterative_method",
                "/wiki/Fixed-point_iteration",
                "/wiki/Slater_determinant",
                "/wiki/Fermion",
                "/wiki/Permanent_(mathematics)",
                "/wiki/Boson",
                "/wiki/Spin-orbital",
                "/wiki/Variational_method",
                "/wiki/Computational_physics",
                "/wiki/Computational_chemistry",
                "/wiki/Wave_function",
                "/wiki/Many-body_problem",
                "/wiki/Stationary_state"
            ],
            "links": [
                "/wiki/List_of_quantum_chemistry_and_solid_state_physics_software",
                "/wiki/Density_functional_theory",
                "/wiki/Hybrid_functional",
                "/wiki/Modern_valence_bond",
                "/wiki/Electronic_correlation",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/M%C3%B8ller%E2%80%93Plesset_perturbation_theory",
                "/wiki/Perturbation_theory",
                "/wiki/Multi-configurational_self-consistent_field",
                "/wiki/Configuration_interaction",
                "/wiki/Quadratic_configuration_interaction",
                "/wiki/Multi-configurational_self-consistent_field#Complete_active_space_SCF",
                "/wiki/Variational_Monte_Carlo",
                "/wiki/Numerical_stability",
                "/wiki/Basis_set_(chemistry)",
                "/wiki/Gram%E2%80%93Schmidt_process",
                "/wiki/Roothaan_equations",
                "/wiki/Overlap_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Generalized_eigenvalue_problem",
                "/wiki/Roothaan_equations",
                "/wiki/Linear_combination_of_atomic_orbitals",
                "/wiki/Slater-type_orbital",
                "/wiki/Gaussian_orbital",
                "/wiki/Exchange_operator",
                "/wiki/Coulomb_operator",
                "/wiki/Hamiltonian_(quantum_mechanics)",
                "/wiki/Molecular_Hamiltonian",
                "/wiki/Hartree%E2%80%93Fock#Hartree\u2013Fock_algorithm",
                "/wiki/Fock_matrix",
                "/wiki/Mean-field_theory",
                "/wiki/Fock_operator",
                "/wiki/Unitary_transformation",
                "/wiki/Coulomb%27s_law",
                "/wiki/Spin-orbital",
                "/wiki/Atomic_orbital",
                "/wiki/Molecular_orbital",
                "/wiki/Linear_combination_of_atomic_orbitals",
                "/wiki/Variational_method_(quantum_mechanics)",
                "/wiki/Expectation_value",
                "/wiki/Ground_state",
                "/wiki/Orthonormal_basis#Incomplete_orthogonal_sets",
                "/wiki/Configuration_interaction",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/Born%E2%80%93Oppenheimer_approximation",
                "/wiki/Hydrogen_atom",
                "/wiki/Iteration",
                "/wiki/Central_field_approximation",
                "/wiki/Slater_determinant",
                "/wiki/Determinant",
                "/wiki/Exchange_symmetry",
                "/wiki/Ansatz",
                "/wiki/Variational_principle",
                "/wiki/Exchange_symmetry",
                "/wiki/Group_theory",
                "/wiki/Vladimir_Fock",
                "/wiki/Exchange_symmetry",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Quantum_statistics",
                "/wiki/Ab_initio_quantum_chemistry_methods",
                "/wiki/Hartree_product",
                "/wiki/John_C._Slater",
                "/wiki/Variational_principle",
                "/wiki/Ansatz",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Douglas_Hartree",
                "/wiki/Robert_Bruce_Lindsay",
                "/wiki/Old_quantum_theory",
                "/wiki/Open_shell",
                "/wiki/Nuclear_physics",
                "/wiki/Bogoliubov_transformation",
                "/wiki/Nuclear_structure#Nuclear_pairing_phenomenon",
                "/wiki/Atomic_structure",
                "/wiki/Configuration_state_function",
                "/wiki/Quantum_number",
                "/wiki/Ground_state",
                "/wiki/Hartree_equation",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Douglas_Hartree",
                "/wiki/Hartree%E2%80%93Fock#The_Fock_operator",
                "/wiki/Iterative_method",
                "/wiki/Fixed-point_iteration",
                "/wiki/Slater_determinant",
                "/wiki/Fermion",
                "/wiki/Permanent_(mathematics)",
                "/wiki/Boson",
                "/wiki/Spin-orbital",
                "/wiki/Variational_method",
                "/wiki/Computational_physics",
                "/wiki/Computational_chemistry",
                "/wiki/Wave_function",
                "/wiki/Many-body_problem",
                "/wiki/Stationary_state"
            ],
            "text": "For a list of software packages known to handle Hartree\u2013Fock calculations, particularly for molecules and solids, see the list of quantum chemistry and solid state physics software.An alternative to Hartree\u2013Fock calculations used in some cases is density functional theory, which treats both exchange and correlation energies, albeit approximately. Indeed, it is common to use calculations that are a hybrid of the two methods\u2014the popular B3LYP scheme is one such hybrid functional method. Another option is to use modern valence bond methods.Of the five simplifications outlined in the section \"Hartree\u2013Fock algorithm\", the fifth is typically the most important. Neglect of electron correlation can lead to large deviations from experimental results. A number of approaches to this weakness, collectively called post-Hartree\u2013Fock methods, have been devised to include electron correlation to the multi-electron wave function. One of these approaches, M\u00f8ller\u2013Plesset perturbation theory, treats correlation as a perturbation of the Fock operator. Others expand the true multi-electron wave function in terms of a linear combination of Slater determinants\u2014such as multi-configurational self-consistent field, configuration interaction, quadratic configuration interaction, and complete active space SCF (CASSCF). Still others (such as variational quantum Monte Carlo) modify the Hartree\u2013Fock wave function by multiplying it by a correlation function (\"Jastrow\" factor), a term which is explicitly a function of multiple electrons that cannot be decomposed into independent single-particle functions.Numerical stability can be a problem with this procedure and there are various ways of combating this instability. One of the most basic and generally applicable is called F-mixing or damping. With F-mixing, once a single electron wave function is calculated it is not used directly. Instead, some combination of that calculated wave function and the previous wave functions for that electron is used\u2014the most common being a simple linear combination of the calculated and immediately preceding wave function. A clever dodge, employed by Hartree, for atomic calculations was to increase the nuclear charge, thus pulling all the electrons closer together. As the system stabilised, this was gradually reduced to the correct charge. In molecular calculations a similar approach is sometimes used by first calculating the wave function for a positive ion and then to use these orbitals as the starting point for the neutral molecule. Modern molecular Hartree\u2013Fock computer programs use a variety of methods to ensure convergence of the Roothaan\u2013Hall equations.Various basis sets are used in practice, most of which are composed of Gaussian functions. In some applications, an orthogonalization method such as the Gram\u2013Schmidt process is performed in order to produce a set of orthogonal basis functions. This can in principle save computational time when the computer is solving the Roothaan\u2013Hall equations by converting the overlap matrix effectively to an identity matrix. However, in most modern computer programs for molecular Hartree\u2013Fock calculations this procedure is not followed due to the high numerical cost of orthogonalization and the advent of more efficient, often sparse, algorithms for solving the generalized eigenvalue problem, of which the Roothaan\u2013Hall equations are an example.Typically, in modern Hartree\u2013Fock calculations, the one-electron wave functions are approximated by a linear combination of atomic orbitals. These atomic orbitals are called Slater-type orbitals. Furthermore, it is very common for the \"atomic orbitals\" in use to actually be composed of a linear combination of one or more Gaussian-type orbitals, rather than Slater-type orbitals, in the interests of saving large amounts of computation time.is the exchange operator, defining the electron exchange energy due to the antisymmetry of the total n-electron wave function.[6] This \"exchange energy\" operator, K, is simply an artifact of the Slater determinant. Finding the Hartree\u2013Fock one-electron wave functions is now equivalent to solving the eigenfunction equation:is the Coulomb operator, defining the electron-electron repulsion energy due to each of the two electrons in the jth orbital.[6] Finallyis the one-electron core Hamiltonian. AlsowhereBecause the electron-electron repulsion term of the molecular Hamiltonian involves the coordinates of two different electrons, it is necessary to reformulate it in an approximate way. Under this approximation, (outlined under Hartree\u2013Fock algorithm), all of the terms of the exact Hamiltonian except the nuclear-nuclear repulsion term are re-expressed as the sum of one-electron operators outlined below, for closed-shell atoms or molecules (with two electrons in each spatial orbital).[6] The \"(1)\" following each operator symbol simply indicates that the operator is 1-electron in nature.Since the Fock operator depends on the orbitals used to construct the corresponding Fock matrix, the eigenfunctions of the Fock operator are in turn new orbitals which can be used to construct a new Fock operator. In this way, the Hartree\u2013Fock orbitals are optimized iteratively until the change in total electronic energy falls below a predefined threshold. In this way, a set of self-consistent one-electron orbitals are calculated. The Hartree\u2013Fock electronic wave function is then the Slater determinant constructed out of these orbitals. Following the basic postulates of quantum mechanics, the Hartree\u2013Fock wave function can then be used to compute any desired chemical or physical property within the framework of the Hartree\u2013Fock method and the approximations employed.The orbitals above only account for the presence of other electrons in an average manner. In the Hartree\u2013Fock method, the effect of other electrons are accounted for in a mean-field theory context. The orbitals are optimized by requiring them to minimize the energy of the respective Slater determinant. The resultant variational conditions on the orbitals lead to a new one-electron operator, the Fock operator. At the minimum, the occupied orbitals are eigensolutions to the Fock operator via a unitary transformation between themselves. The Fock operator is an effective one-electron Hamiltonian operator being the sum of two terms. The first is a sum of kinetic energy operators for each electron, the internuclear repulsion energy, and a sum of nuclear-electronic Coulombic attraction terms. The second are Coulombic repulsion terms between electrons in a mean-field theory description; a net repulsion energy for each electron in the system, which is calculated by treating all of the other electrons within the molecule as a smooth distribution of negative charge. This is the major simplification inherent in the Hartree\u2013Fock method, and is equivalent to the fifth simplification in the above list.The starting point for the Hartree\u2013Fock method is a set of approximate one-electron wave functions known as spin-orbitals. For an atomic orbital calculation, these are typically the orbitals for a hydrogenic atom (an atom with only one electron, but the appropriate nuclear charge). For a molecular orbital or crystalline calculation, the initial approximate one-electron wave functions are typically a linear combination of atomic orbitals (LCAO).The variational theorem states that for a time-independent Hamiltonian operator, any trial wave function will have an energy expectation value that is greater than or equal to the true ground state wave function corresponding to the given Hamiltonian. Because of this, the Hartree\u2013Fock energy is an upper bound to the true ground state energy of a given molecule. In the context of the Hartree\u2013Fock method, the best possible solution is at the Hartree\u2013Fock limit; i.e., the limit of the Hartree\u2013Fock energy as the basis set approaches completeness. (The other is the full-CI limit, where the last two approximations of the Hartree\u2013Fock theory as described above are completely undone. It is only when both limits are attained that the exact solution, up to the Born\u2013Oppenheimer approximation, is obtained.) The Hartree\u2013Fock energy is the minimal energy for a single Slater determinant.Relaxation of the last two approximations give rise to many so-called post-Hartree\u2013Fock methods.The Hartree\u2013Fock method makes five major simplifications in order to deal with this task:The Hartree\u2013Fock method is typically used to solve the time-independent Schr\u00f6dinger equation for a multi-electron atom or molecule as described in the Born\u2013Oppenheimer approximation. Since there are no known solutions for many-electron systems (there are solutions for one-electron systems such as hydrogenic atoms and the diatomic hydrogen cation), the problem is solved numerically. Due to the nonlinearities introduced by the Hartree\u2013Fock approximation, the equations are solved using a nonlinear method such as iteration, which gives rise to the name \"self-consistent field method.\"The Hartree\u2013Fock method, despite its physically more accurate picture, was little used until the advent of electronic computers in the 1950s due to the much greater computational demands over the early Hartree method and empirical models. Initially, both the Hartree method and the Hartree\u2013Fock method were applied exclusively to atoms, where the spherical symmetry of the system allowed one to greatly simplify the problem. These approximate methods were (and are) often used together with the central field approximation, to impose that electrons in the same shell have the same radial part, and to restrict the variational solution to be a spin eigenfunction. Even so, solution by hand of the Hartree\u2013Fock equations for a medium-sized atom were laborious; small molecules required computational resources far beyond what was available before 1950.It was then shown that a Slater determinant, a determinant of one-particle orbitals first used by Heisenberg and Dirac in 1926, trivially satisfies the antisymmetric property of the exact solution and hence is a suitable ansatz for applying the variational principle. The original Hartree method can then be viewed as an approximation to the Hartree\u2013Fock method by neglecting exchange. Fock's original method relied heavily on group theory and was too abstract for contemporary physicists to understand and implement. In 1935, Hartree reformulated the method more suitably for the purposes of calculation.In 1930, Slater and V. A. Fock independently pointed out that the Hartree method did not respect the principle of antisymmetry of the wave function. The Hartree method used the Pauli exclusion principle in its older formulation, forbidding the presence of two electrons in the same quantum state. However, this was shown to be fundamentally incomplete in its neglect of quantum statistics.Hartree sought to do away with empirical parameters and solve the many-body time-independent Schr\u00f6dinger equation from fundamental physical principles, i.e., ab initio. His first proposed method of solution became known as the Hartree method or Hartree product. However, many of Hartree's contemporaries did not understand the physical reasoning behind the Hartree method: it appeared to many people to contain empirical elements, and its connection to the solution of the many-body Schr\u00f6dinger equation was unclear. However, in 1928 J. C. Slater and J. A. Gaunt independently showed that the Hartree method could be couched on a sounder theoretical basis by applying the variational principle to an ansatz (trial wave function) as a product of single-particle functions.The origin of the Hartree\u2013Fock method dates back to the end of the 1920s, soon after the discovery of the Schr\u00f6dinger equation in 1926. In 1927, D. R. Hartree introduced a procedure, which he called the self-consistent field method, to calculate approximate wave functions and energies for atoms and ions. Hartree was guided by some earlier, semi-empirical methods of the early 1920s (by E. Fues, R. B. Lindsay, and himself) set in the old quantum theory of Bohr.The rest of this article will focus on applications in electronic structure theory suitable for molecules with the atom as a special case. The discussion here is only for the Restricted Hartree\u2013Fock method, where the atom or molecule is a closed-shell system with all orbitals (atomic or molecular) doubly occupied. Open-shell systems, where some of the electrons are not paired, can be dealt with by one of two Hartree\u2013Fock methods:For both atoms and molecules, the Hartree\u2013Fock solution is the central starting point for most methods that describe the many-electron system more accurately.The Hartree\u2013Fock method finds its typical application in the solution of the Schr\u00f6dinger equation for atoms, molecules, nanostructures[2] and solids but it has also found widespread use in nuclear physics. (See Hartree\u2013Fock\u2013Bogoliubov method for a discussion of its application in nuclear structure theory). In atomic structure theory, calculations may be for a spectrum with many excited energy levels and consequently the Hartree\u2013Fock method for atoms assumes the wave function is a single configuration state function with well-defined quantum numbers and that the energy level is not necessarily the ground state.Especially in the older literature, the Hartree\u2013Fock method is also called the self-consistent field method (SCF). In deriving what is now called the Hartree equation as an approximate solution of the Schr\u00f6dinger equation, Hartree required the final field as computed from the charge distribution to be \"self-consistent\" with the assumed initial field. Thus, self-consistency was a requirement of the solution. The solutions to the non-linear Hartree\u2013Fock equations also behave as if each particle is subjected to the mean field created by all other particles (see the Fock operator below) and hence, the terminology continued. The equations are almost universally solved by means of an iterative method, although the fixed-point iteration algorithm does not always converge.[1] This solution scheme is not the only one possible and is not an essential feature of the Hartree\u2013Fock method.The Hartree\u2013Fock method often assumes that the exact, N-body wave function of the system can be approximated by a single Slater determinant (in the case where the particles are fermions) or by a single permanent (in the case of bosons) of N spin-orbitals. By invoking the variational method, one can derive a set of N-coupled equations for the N spin orbitals. A solution of these equations yields the Hartree\u2013Fock wave function and energy of the system.In computational physics and chemistry, the Hartree\u2013Fock (HF) method is a method of approximation for the determination of the wave function and the energy of a quantum many-body system in a stationary state.",
            "title": "Hartree\u2013Fock method",
            "url": "https://en.wikipedia.org/wiki/Self-consistent_field"
        },
        {
            "desc_links": [
                "/wiki/Chemistry",
                "/wiki/Quantum_mechanics",
                "/wiki/Physical_model"
            ],
            "links": [
                "/wiki/Quantum_state",
                "/wiki/Ernst_Stueckelberg",
                "/wiki/Lev_Davidovich_Landau",
                "/wiki/Clarence_Zener",
                "/wiki/Landau%E2%80%93Zener_transition",
                "/wiki/Diabatic",
                "/wiki/Avoided_crossing",
                "/wiki/Spin_forbidden_reactions",
                "/wiki/Spin_states_(d_electrons)",
                "/wiki/Reagent",
                "/wiki/Product_(chemistry)",
                "/wiki/Scalar_(physics)",
                "/wiki/Potential",
                "/wiki/Potential_energy_surface",
                "/wiki/Born%E2%80%93Oppenheimer_approximation",
                "/wiki/Max_Born",
                "/wiki/Robert_Oppenheimer",
                "/wiki/RRKM",
                "/wiki/Rudolph_A._Marcus",
                "/wiki/Transition_state",
                "/wiki/Henry_Eyring_(chemist)",
                "/wiki/Reaction_rates",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Molecular_Hamiltonian",
                "/wiki/Classical_mechanics",
                "/wiki/Molecular_dynamics",
                "/wiki/Monte_Carlo_method",
                "/wiki/Gas_in_a_box",
                "/wiki/L.H._Thomas",
                "/wiki/Enrico_Fermi",
                "/wiki/Electronic_density",
                "/wiki/Wave_function",
                "/wiki/Kohn-Sham_equations",
                "/wiki/Polyatomic_molecule",
                "/wiki/Macromolecule",
                "/wiki/M%C3%B8ller%E2%80%93Plesset_perturbation_theory",
                "/wiki/Coupled_cluster",
                "/wiki/Computational_chemistry",
                "/wiki/Friedrich_Hund",
                "/wiki/Robert_S._Mulliken",
                "/wiki/Electron",
                "/wiki/Molecule",
                "/wiki/Spectroscopy",
                "/wiki/Hartree%E2%80%93Fock",
                "/wiki/Post_Hartree%E2%80%93Fock",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Walter_Heitler",
                "/wiki/Fritz_London",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/John_C._Slater",
                "/wiki/Linus_Pauling",
                "/wiki/Chemical_bond",
                "/wiki/Aromaticity",
                "/wiki/Atomic_nucleus",
                "/wiki/Wave_function",
                "/wiki/Bohr_model",
                "/wiki/Electron_cloud",
                "/wiki/Molecular_orbital",
                "/wiki/Probability_amplitude",
                "/wiki/Predictive_power",
                "/wiki/Periodic_table_of_elements",
                "/wiki/Wave-particle_duality",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Hund%27s_rule_of_maximum_multiplicity",
                "/wiki/Aufbau_principle",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Dirac_equation",
                "/wiki/Relativistic_quantum_chemistry",
                "/wiki/Electronic_molecular_Hamiltonian",
                "/wiki/Dihydrogen_cation",
                "/wiki/Lambert_W_function#Generalizations",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Walter_Heitler",
                "/wiki/Fritz_London",
                "/wiki/Edward_Teller",
                "/wiki/Robert_S._Mulliken",
                "/wiki/Max_Born",
                "/wiki/J._Robert_Oppenheimer",
                "/wiki/Linus_Pauling",
                "/wiki/Erich_H%C3%BCckel",
                "/wiki/Douglas_Hartree",
                "/wiki/Vladimir_Fock",
                "/wiki/Cathode_rays",
                "/wiki/Michael_Faraday",
                "/wiki/Black_body_radiation",
                "/wiki/Gustav_Kirchhoff",
                "/wiki/Ludwig_Boltzmann",
                "/wiki/Max_Planck",
                "/wiki/Frequency",
                "/wiki/Energy",
                "/wiki/Planck%E2%80%99s_Constant",
                "/wiki/Photoelectric_effect",
                "/wiki/Albert_Einstein",
                "/wiki/Light",
                "/wiki/Photon",
                "/wiki/Linus_Pauling",
                "/wiki/Semi-empirical_quantum_chemistry_method",
                "/wiki/Ground_state",
                "/wiki/Excited_state",
                "/wiki/Transition_state",
                "/wiki/Chemical_reaction",
                "/wiki/Computational_chemistry",
                "/wiki/Many-body_problem",
                "/wiki/Computer",
                "/wiki/Back-of-the-envelope_calculation",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Spectroscopy",
                "/wiki/Quantization_(physics)",
                "/wiki/Infra-red_(IR)_spectroscopy",
                "/wiki/Nuclear_magnetic_resonance_(NMR)_spectroscopy",
                "/wiki/Scanning_probe_microscopy",
                "/wiki/Experimental",
                "/wiki/Theoretical",
                "/wiki/Chemistry",
                "/wiki/Quantum_mechanics",
                "/wiki/Physical_model"
            ],
            "text": "Non-adiabatic dynamics consists of taking the interaction between several coupled potential energy surface (corresponding to different electronic quantum states of the molecule). The coupling terms are called vibronic couplings. The pioneering work in this field was done by Stueckelberg, Landau, and Zener in the 1930s, in their work on what is now known as the Landau\u2013Zener transition. Their formula allows the transition probability between two diabatic potential curves in the neighborhood of an avoided crossing to be calculated. Spin-forbidden reactions are one type of non-adiabatic reactions where at least one change in spin state occurs when progressing from reactant to product.In adiabatic dynamics, interatomic interactions are represented by single scalar potentials called potential energy surfaces. This is the Born\u2013Oppenheimer approximation introduced by Born and Oppenheimer in 1927. Pioneering applications of this in chemistry were performed by Rice and Ramsperger in 1927 and Kassel in 1928, and generalized into the RRKM theory in 1952 by Marcus who took the transition state theory developed by Eyring in 1935 into account. These methods enable simple estimates of unimolecular reaction rates from a few characteristics of the potential surface.A further step can consist of solving the Schr\u00f6dinger equation with the total molecular Hamiltonian in order to study the motion of molecules. Direct solution of the Schr\u00f6dinger equation is called quantum molecular dynamics, within the semiclassical approximation semiclassical molecular dynamics, and within the classical mechanics framework molecular dynamics (MD). Statistical approaches, using for example Monte Carlo methods, are also possible.The Thomas\u2013Fermi model was developed independently by Thomas and Fermi in 1927. This was the first attempt to describe many-electron systems on the basis of electronic density instead of wave functions, although it was not very successful in the treatment of entire molecules. The method did provide the basis for what is now known as density functional theory. Modern day DFT uses the Kohn-Sham method, where the density functional is split into four terms; the Kohn-Sham kinetic energy, an external potential, exchange and correlation energies. A large part of the focus on developing DFT is on improving the exchange and correlation terms. Though this method is less developed than post Hartree\u2013Fock methods, its significantly lower computational requirements (scaling typically no worse than n3 with respect to n basis functions, for the pure functionals) allow it to tackle larger polyatomic molecules and even macromolecules. This computational affordability and often comparable accuracy to MP2 and CCSD(T) (post-Hartree\u2013Fock methods) has made it one of the most popular methods in computational chemistry at present.An alternative approach was developed in 1929 by Friedrich Hund and Robert S. Mulliken, in which electrons are described by mathematical functions delocalized over an entire molecule. The Hund\u2013Mulliken approach or molecular orbital (MO) method is less intuitive to chemists, but has turned out capable of predicting spectroscopic properties better than the VB method. This approach is the conceptional basis of the Hartree\u2013Fock method and further post Hartree\u2013Fock methods.Although the mathematical basis of quantum chemistry had been laid by Schr\u00f6dinger in 1926, it is generally accepted that the first true calculation in quantum chemistry was that of the German physicists Walter Heitler and Fritz London on the hydrogen (H2) molecule in 1927.[citation needed] Heitler and London's method was extended by the American theoretical physicist John C. Slater and the American theoretical chemist Linus Pauling to become the valence-bond (VB) [or Heitler\u2013London\u2013Slater\u2013Pauling (HLSP)] method. In this method, attention is primarily devoted to the pairwise interactions between atoms, and this method therefore correlates closely with classical chemists' drawings of bonds. It focuses on how the atomic orbitals of an atom combine to give individual chemical bonds when a molecule is formed. The concept of chemical bond distorts when the aromatic compounds are considered, then you need to apply resonance ideas and hybridization that doesn\u00b4t correspond to chemical view of fixed shared pair of electrons between molecules (see aromaticity).The foundation of quantum mechanics and quantum chemistry is the wave model, in which the atom is a small, dense, positively charged nucleus surrounded by electrons. The wave model is derived from the wave function, a set of possible equations derived from the time evolution of the Schr\u00f6dinger equation which is applied to the wavelike probability distribution of subatomic particles. Unlike the earlier Bohr model of the atom, however, the wave model describes electrons as \"clouds\" moving in orbitals, and their positions are represented by probability distributions rather than discrete points. The strength of this model lies in its predictive power. Specifically, it predicts the pattern of chemically similar elements found in the periodic table. The wave model is so named because electrons exhibit properties (such as interference) traditionally associated with waves. See wave-particle duality. In this model, when we solve the Schr\u00f6dinger Equation for an Hidrogenoid Atom, we obtain a solution that depends on some numbers, called quantum numbers, that describes the orbital, the most probable space where an electron can be. These are n, the principal quantum number, for the energy, l, or secondary quantum number, which correlates to the angular momentum, ml, for the orientation, and ms the spin. This model can explain the new lines that appeared in the spectroscopy of atoms. For multielectron atoms we must introduce some rules as that the electrons fill orbitals in a way to minimize the energy of the atom, in order of increasing energy, the Pauli exclusion principle, Hund's rule, and the aufbau principle.The first step in solving a quantum chemical problem is usually solving the Schr\u00f6dinger equation (or Dirac equation in relativistic quantum chemistry) with the electronic molecular Hamiltonian. This is called determining the electronic structure of the molecule. It can be said that the electronic structure of a molecule or crystal implies essentially its chemical properties. An exact solution for the Schr\u00f6dinger equation can only be obtained for the hydrogen atom (though exact solutions for the bound state energies of the hydrogen molecular ion have been identified in terms of the generalized Lambert W function). Since all other atomic, or molecular systems, involve the motions of three or more \"particles\", their Schr\u00f6dinger equations cannot be solved exactly and so approximate solutions must be sought.Some view the birth of quantum chemistry as starting with the discovery of the Schr\u00f6dinger equation and its application to the hydrogen atom in 1926.[citation needed] However, the 1927 article of Walter Heitler and Fritz London is often recognized as the first milestone in the history of quantum chemistry. This is the first application of quantum mechanics to the diatomic hydrogen molecule, and thus to the phenomenon of the chemical bond. In the following years much progress was accomplished by Edward Teller, Robert S. Mulliken, Max Born, J. Robert Oppenheimer, Linus Pauling, Erich H\u00fcckel, Douglas Hartree, Vladimir Fock, to cite a few. The history of quantum chemistry also goes through the 1838 discovery of cathode rays by Michael Faraday, the 1859 statement of the black body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system could be discrete, and the 1900 quantum hypothesis by Max Planck that any energy radiating atomic system can theoretically be divided into a number of discrete energy elements \u03b5 such that each of these energy elements is proportional to the frequency \u03bd with which they each individually radiate energy and a numerical value called Planck\u2019s Constant. Then, in 1905, to explain the photoelectric effect (1839), i.e., that shining light on certain materials can function to eject electrons from the material, Albert Einstein postulated, based on Planck\u2019s quantum hypothesis, that light itself consists of individual quantum particles, which later came to be called photons (1926). In the years to follow, this theoretical basis slowly began to be applied to chemical structure, reactivity, and bonding. Probably the greatest contribution to the field was made by Linus Pauling.On the calculations, quantum chemical studies use also semi-empirical and other methods based on quantum mechanical principles, and deal with time dependent problems. Many quantum chemical studies assume the nuclei are at rest (Born\u2013Oppenheimer approximation). Many calculations involve iterative methods that include self-consistent field methods. Major goals of quantum chemistry include increasing the accuracy of the results for small molecular systems, and increasing the size of large molecules that can be processed, which is limited by scaling considerations\u2014the computation time increases as a power of the number of atoms.Quantum chemistry studies the ground state of individual atoms and molecules, and the excited states, and transition states that occur during chemical reactions.In these ways, quantum chemists investigate chemical phenomena.Theoretical quantum chemistry, the workings of which also tend to fall under the category of computational chemistry, seeks to calculate the predictions of quantum theory as atoms and molecules can only have discrete energies; as this task, when applied to polyatomic species, invokes the many-body problem, these calculations are performed using computers rather than by analytical \"back of the envelope\" methods, pen recorder or computerized data station with a VDU.[citation needed]Experimental quantum chemists rely heavily on spectroscopy, through which information regarding the quantization of energy on a molecular scale can be obtained. Common methods are infra-red (IR) spectroscopy, nuclear magnetic resonance (NMR) spectroscopy, and scanning probe microscopy.It involves heavy interplay of experimental and theoretical methods.Quantum chemistry is a branch of chemistry whose primary focus is the application of quantum mechanics in physical models and experiments of chemical systems. It is also called molecular quantum mechanics.",
            "title": "Quantum chemistry",
            "url": "https://en.wikipedia.org/wiki/Quantum_chemistry"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Perpendicularity",
                "/wiki/Linear_algebra",
                "/wiki/Bilinear_form",
                "/wiki/Vector_space",
                "/wiki/Function_space",
                "/wiki/Orthogonal_functions",
                "/wiki/Basis_(linear_algebra)"
            ],
            "links": [
                "/wiki/Chess",
                "/wiki/Go_(game)",
                "/wiki/Neuroscience",
                "/wiki/Analytical_chemistry",
                "/wiki/New_drug_application",
                "/wiki/Organic_synthesis",
                "/wiki/Protecting_group",
                "/wiki/Functional_group",
                "/wiki/DNA",
                "/wiki/Bioorthogonal_chemistry",
                "/wiki/Supramolecular_chemistry",
                "/wiki/Non-covalent",
                "/wiki/Combinatorics",
                "/wiki/Latin_squares",
                "/wiki/Superimposition",
                "/wiki/Taxonomy_(general)",
                "/wiki/Dependent_and_independent_variables",
                "/wiki/Dependent_and_independent_variables",
                "/wiki/Simple_linear_regression",
                "/wiki/Multiple_regression",
                "/wiki/Correlation",
                "/wiki/Expected_value",
                "/wiki/Econometrics",
                "/wiki/Maximum_likelihood",
                "/wiki/Generalized_Method_of_Moments",
                "/wiki/Ordinary_Least_Squares",
                "/wiki/Subcarrier",
                "/wiki/Orthogonal_frequency-division_multiplexing",
                "/wiki/802.11",
                "/wiki/Wi-Fi",
                "/wiki/WiMAX",
                "/wiki/ITU-T",
                "/wiki/G.hn",
                "/wiki/DVB-T",
                "/wiki/ADSL",
                "/wiki/Basis_function",
                "/wiki/Time_division_multiple_access",
                "/wiki/Instruction_set",
                "/wiki/Orthogonal_instruction_set",
                "/wiki/Processor_register",
                "/wiki/Addressing_mode",
                "/wiki/Orthogonal_instruction_set",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Separation_of_concerns",
                "/wiki/Information_Hiding#Encapsulation",
                "/wiki/Adriaan_van_Wijngaarden",
                "/wiki/Algol_68",
                "/wiki/Piet_Mondrian",
                "/wiki/Burgoyne_Diller",
                "/wiki/Web_site",
                "/wiki/Thyssen-Bornemisza_Museum",
                "/wiki/Perspective_(graphical)",
                "/wiki/Vanishing_point",
                "/wiki/Kronecker_delta",
                "/wiki/Orthogonal_polynomials",
                "/wiki/Norm_(mathematics)",
                "/wiki/Integral_calculus",
                "/wiki/Inner_product",
                "/wiki/Function_(mathematics)",
                "/wiki/Weight_function",
                "/wiki/Hyperplane",
                "/wiki/Orthogonal_complement",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(mathematics)",
                "/wiki/Euclidean_space",
                "/wiki/If_and_only_if",
                "/wiki/Dot_product",
                "/wiki/Radian",
                "/wiki/Perpendicular",
                "/wiki/Bilinear_form",
                "/wiki/Pseudo-Euclidean_space",
                "/wiki/Hyperbolic_orthogonality",
                "/wiki/Surface_normal",
                "/wiki/Orthonormality",
                "/wiki/Unit_vector",
                "/wiki/Probability",
                "/wiki/Statistics",
                "/wiki/Ancient_Greek",
                "/wiki/Rectangle",
                "/wiki/Right_triangle",
                "/wiki/Mathematics",
                "/wiki/Perpendicularity",
                "/wiki/Linear_algebra",
                "/wiki/Bilinear_form",
                "/wiki/Vector_space",
                "/wiki/Function_space",
                "/wiki/Orthogonal_functions",
                "/wiki/Basis_(linear_algebra)"
            ],
            "text": "Stereo vinyl records encode both the left and right stereo channels in a single groove. The V-shaped groove in the vinyl has walls that are 90 degrees to each other, with variations in each wall separately encoding one of the two analogue channels that make up the stereo signal. The cartridge senses the motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side.[17] A pure horizontal motion corresponds to a mono signal, equivalent to a stereo signal in which both channels carry identical (in-phase) signals.In board games such as chess which feature a grid of squares, 'orthogonal' is used to mean \"in the same row/'rank' or column/'file'\". This is the counterpart to squares which are \"diagonally adjacent\".[16] In the ancient Chinese board game Go a player can capture the stones of an opponent by occupying all orthogonally-adjacent points.In neuroscience, a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map.In the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure.In analytical chemistry, analyses are \"orthogonal\" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement. This is often required as a part of a new drug application.In synthetic organic chemistry orthogonal protection is a strategy allowing the deprotection of functional groups independently of each other. In chemistry and biochemistry, an orthogonal interaction occurs when there are two pairs of substances and each substance can interact with their respective partner, but does not interact with either substance of the other pair. For example, DNA has two orthogonal pairs: cytosine and guanine form a base-pair, and adenine and thymine form another base-pair, but other base-pair combinations are strongly disfavored. As a chemical example, tetrazine reacts with transcyclooctene and azide reacts with cyclooctyne without any cross-reaction, so these are mutually orthogonal reactions, and so, can be performed simultaneously and selectively.[15] Bioorthogonal chemistry refers to chemical reactions occurring inside living systems without reacting with naturally present cellular components. In supramolecular chemistry the notion of orthogonality refers to the possibility of two or more supramolecular, often non-covalent, interactions being compatible; reversibly forming without interference from the other.In combinatorics, two n\u00d7n Latin squares are said to be orthogonal if their superimposition yields all possible n2 combinations of entries.[14]In taxonomy, an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive.When performing statistical analysis, independent variables that affect a particular dependent variable are said to be orthogonal if they are uncorrelated,[13] since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables individually with simple regression or simultaneously with multiple regression. If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions). One econometric formalism that is alternative to the maximum likelihood framework, the Generalized Method of Moments, relies on orthogonality conditions. In particular, the Ordinary Least Squares estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals.In OFDM, the subcarrier frequencies are chosen so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required.Another scheme is orthogonal frequency-division multiplexing (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other. Well known examples include (a, g, and n) versions of 802.11 Wi-Fi; WiMAX; ITU-T G.hn, DVB-T, the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of ADSL.In communications, multiple-access schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different basis functions. One such scheme is TDMA, where the orthogonal basis functions are nonoverlapping rectangular pulses (\"time slots\").An instruction set is said to be orthogonal if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task)[12] and is designed such that instructions can use any register in any addressing mode. This terminology results from considering an instruction as a vector whose components are the instruction fields. One field identifies the registers to be operated upon and another specifies the addressing mode. An orthogonal instruction set uniquely encodes all combinations of registers and addressing modes.[citation needed]Orthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the separation of concerns and encapsulation, and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them.Orthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results.[10] This usage was introduced by Van Wijngaarden in the design of Algol 68:The term \"orthogonal line\" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as Piet Mondrian and Burgoyne Diller are noted for their exclusive use of \"orthogonal lines\" \u2014 not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay at the Web site of the Thyssen-Bornemisza Museum states that \"Mondrian ... dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours.\" [1]In art, the perspective (imaginary) lines pointing to the vanishing point are referred to as \"orthogonal lines\".is the Kronecker delta. In other words, every pair of them (excluding pairing of a function with itself) is orthogonal, and the norm of each is 1. See in particular the orthogonal polynomials.whereThe members of such a set of functions are orthonormal with respect to w on the interval [a, b] ifThe members of a set of functions {fi\u00a0: i = 1, 2, 3, ...} are orthogonal with respect to w on the interval [a, b] ifWe write the norm with respect to this inner product asOrthogonality of two functions with respect to one inner product does not imply orthogonality with respect to another inner product.We say that functions f and g are orthogonal if their inner product (equivalently, the value of this integral) is zero:In simple cases, w(x) = 1.By using integral calculus, it is common to use the following to define the inner product of two functions f and g with respect to a nonnegative weight function w over an interval [a, b]:In four-dimensional Euclidean space, the orthogonal complement of a line is a hyperplane and vice versa, and that of a plane is a plane.[9]Note that the geometric concept two planes being perpendicular does not correspond to the orthogonal complement, since in three dimensions a pair of vectors, one from each of a pair of perpendicular planes, might meet at any angle.The orthogonal complement of a subspace is the space of all vectors that are orthogonal to every vector in the subspace. In a three-dimensional Euclidean vector space, the orthogonal complement of a line through the origin is the plane through the origin perpendicular to it, and vice versa.[9]In Euclidean space, two vectors are orthogonal if and only if their dot product is zero, i.e. they make an angle of 90\u00b0 (\u03c0/2 radians), or one of the vectors is zero.[8] Hence orthogonality of vectors is an extension of the concept of perpendicular vectors to spaces of any dimension.A vector space with a bilinear form generalizes the case of an inner product. When the bilinear form applied to two vectors results in zero, then they are orthogonal. The case of a pseudo-Euclidean plane uses the term hyperbolic orthogonality. In the diagram, axes x\u2032 and t\u2032 are hyperbolic-orthogonal for any given \u03d5.In certain cases, the word normal is used to mean orthogonal, particularly in the geometric sense as in the normal to a surface. For example, the y-axis is normal to the curve y = x2 at the origin. However, normal may also refer to the magnitude of a vector. In particular, a set is called orthonormal (orthogonal plus normal) if it is an orthogonal set of unit vectors. As a result, use of the term normal to mean \"orthogonal\" is often avoided. The word \"normal\" also has a different meaning in probability and statistics.A set of vectors in an inner product space is called pairwise orthogonal if each pairing of them is orthogonal. Such a set is called an orthogonal set.The word comes from the Greek \u1f40\u03c1\u03b8\u03cc\u03c2 (orthos), meaning \"upright\", and \u03b3\u03c9\u03bd\u03af\u03b1 (gonia), meaning \"angle\". The ancient Greek \u1f40\u03c1\u03b8\u03bf\u03b3\u03ce\u03bd\u03b9\u03bf\u03bd orthog\u014dnion (< \u1f40\u03c1\u03b8\u03cc\u03c2 orthos 'upright'[1] + \u03b3\u03c9\u03bd\u03af\u03b1 g\u014dnia 'angle'[2]) and classical Latin orthogonium originally denoted a rectangle.[3] Later, they came to mean a right triangle. In the 12th century, the post-classical Latin word orthogonalis came to mean a right angle or something related to a right angle.[4]By extension, orthogonality is also used to refer to the separation of specific features of a system. The term also has specialized meanings in other fields including art and chemistry.In mathematics, orthogonality is the generalization of the notion of perpendicularity to the linear algebra of bilinear forms. Two elements u and v of a vector space with bilinear form B are orthogonal when B(u, v) = 0. Depending on the bilinear form, the vector space may contain nonzero self-orthogonal vectors. In the case of function spaces, families of orthogonal functions are used to form a basis.",
            "title": "Orthogonality",
            "url": "https://en.wikipedia.org/wiki/Orthogonal"
        },
        {
            "desc_links": [
                "/wiki/Atomic_orbital",
                "/wiki/Linear_combination_of_atomic_orbitals",
                "/wiki/Plane_wave",
                "/wiki/Gaussian_orbital",
                "/wiki/Slater-type_orbital",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/Molecular_orbital",
                "/wiki/Theoretical_chemistry",
                "/wiki/Computational_chemistry",
                "/wiki/Function_(mathematics)",
                "/wiki/Hartree%E2%80%93Fock_method",
                "/wiki/Density_functional_theory"
            ],
            "links": [
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Finite_difference",
                "/wiki/Sinc_function",
                "/wiki/Wavelets",
                "/wiki/Basis_set_superposition_error",
                "/wiki/Pseudopotential",
                "/wiki/Pseudopotential",
                "/wiki/Fast_Fourier_Transform",
                "/wiki/Plane_wave",
                "/wiki/Periodic_boundary_conditions",
                "/wiki/Density-functional_theory",
                "/wiki/Computational_chemistry",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/Angular_momentum#Angular_momentum_in_quantum_mechanics",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/John_Pople",
                "/wiki/STO-nG_basis_sets",
                "/wiki/Hartree%E2%80%93Fock",
                "/wiki/S._Francis_Boys",
                "/wiki/Gaussian_orbital",
                "/wiki/John_Pople",
                "/wiki/Atomic_orbital",
                "/wiki/Linear_combination_of_atomic_orbitals",
                "/wiki/Ansatz",
                "/wiki/Slater-type_orbitals",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hydrogen-like_atom",
                "/wiki/Molecular_orbital",
                "/wiki/Hartree-Fock_method",
                "/wiki/Density-functional_theory",
                "/wiki/Kato_theorem",
                "/wiki/Wavefunction",
                "/wiki/Vector_(geometric)",
                "/wiki/Operator_(physics)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Tensors",
                "/wiki/Computational_chemistry",
                "/wiki/Quantum_chemistry",
                "/wiki/Basis_function",
                "/wiki/Atomic_orbital",
                "/wiki/Linear_combination_of_atomic_orbitals",
                "/wiki/Plane_wave",
                "/wiki/Gaussian_orbital",
                "/wiki/Slater-type_orbital",
                "/wiki/Post-Hartree%E2%80%93Fock",
                "/wiki/Molecular_orbital",
                "/wiki/Theoretical_chemistry",
                "/wiki/Computational_chemistry",
                "/wiki/Function_(mathematics)",
                "/wiki/Hartree%E2%80%93Fock_method",
                "/wiki/Density_functional_theory"
            ],
            "text": "In the wavelet case, it is possible to make the mesh adaptive, so that more points are used close to the nuclei. Wavelets rely on the use of localized functions that allow for the development of linear-scaling methods.\nSinc functions form an orthonormal, analytical, and complete basis set. The convergence to the complete basis set limit is systematic and relatively simple. Similarly to plane wave basis sets, the accuracy of sinc basis sets is controlled by an energy cutoff criterion.[citation needed]\nAnalogous to the plane wave basis sets, where the basis functions are eigenfunctions of the momentum operator, there are basis sets whose functions are eigenfunctions of the position operator, that is, points on a uniform mesh in real space. The actual implementation may use finite differences, or interpolation with sinc functions (a.k.a. Lagrange functions) or wavelets.\nDue to the assumption of periodic boundary conditions, plane-wave basis sets are less well suited to gas-phase calculations than localized basis sets. Large regions of vacuum need to be added on all sides of the gas-phase molecule in order to avoid interactions with the molecule and its periodic copies. However, the plane waves use a similar accuracy to describe the vacuum region as the region where the molecule is, meaning that obtaining the truly noninteracting limit may be computationally costly.\nFurthermore, as all functions in the basis are mutually orthogonal and are not associated with any particular atom, plane-wave basis sets do not exhibit basis-set superposition error. However, the plane-wave basis set is dependent on the size of the simulation cell, complicating cell size optimization.\nIn practice, plane-wave basis sets are often used in combination with an 'effective core potential' or pseudopotential, so that the plane waves are only used to describe the valence charge density.  This is because core electrons tend to be concentrated very close to the atomic nuclei, resulting in large wavefunction and density gradients near the nuclei which are not easily described by a plane-wave basis set unless a very high energy cutoff, and therefore small wavelength, is used.  This combined method of a plane-wave basis set with a core pseudopotential is often abbreviated as a PSPW calculation.\nIn addition, certain integrals and operations are much easier to program and carry out with plane-wave basis functions than with their localized counterparts. For example, the kinetic energy operator is diagonal in the reciprocal space. Integrals over real-space operators can be efficiently carried out using fast Fourier transforms. The properties of the Fourier Transform allow a vector representing the gradient of the total energy with respect to the plane-wave coefficients to be calculated with a computational effort that scales as NPW*ln(NPW) where NPW is the number of plane-waves. When this property is combined with separable pseudopotentials of the Kleinman-Bylander type and pre-conditioned conjugate gradient solution techniques, the dynamic simulation of periodic problems containing hundreds of atoms becomes possible.\nThe main advantage of a plane-wave basis is that it is guaranteed to converge in a smooth, monotonic manner to the target wavefunction. In contrast, when localized basis sets are used, monotonic convergence to the basis set limit may be difficult due to problems with over-completeness: in a large basis set, functions on different atoms start to look alike, and many eigenvalues of the overlap matrix approach zero.\nIn addition to localized basis sets, plane-wave basis sets can also be used in quantum-chemical simulations.  Typically, the choice of the plane wave basis set is based on a cutoff energy. The plane waves in the simulation cell that fit below the energy criterion are then included in the calculation. These basis sets are popular in calculations involving three-dimensional periodic boundary conditions.\nCompleteness-optimized basis sets are tailored to a specific property. This way, the flexibility of the basis set can be focused on the computational demands of the chosen property, typically yielding much faster convergence to the complete basis set limit than is achievable with energy-optimized basis sets.\nManninen and Vaara have proposed completeness-optimized basis sets,[8] where the exponents are obtained by maximization of the one-electron completeness profile[9] instead of minimization of the energy. Complenetess-optimized basis sets are a way to easily approach the complete basis set limit of any property at any level of theory, and the procedure is simple to automatize.[10]\nGaussian-type orbital basis sets are typically optimized to reproduce the lowest possible energy for the systems used to train the basis set. However, the convergence of the energy does not imply convergence of other properties, such as nuclear magnetic shieldings, the dipole moment, or the electron momentum density, which probe different aspects of the electronic wave function.\nKarlsruhe basis sets come in various flavors\nThe pc-n sets can be augmented with diffuse functions to obtain augpc-n sets.\nAdopting a similar methodology to the correlation-consistent series, Frank Jensen introduced polarization-consistent (pc-n) basis sets as a way to quickly converge density functional theory calculations to the complete basis set limit.[7] Like the Dunning sets, the pc-n sets can be combined with basis set extrapolation techniques to obtain CBS values.\nDensity-functional theory has recently become widely used in computational chemistry. However, the correlation-consistent basis sets described above are suboptimal for density-functional theory, because the correlation-consistent sets have been designed for Post-Hartree\u2013Fock, while density-functional theory exhibits much more rapid basis set convergence than wave function methods.\nFor example, Ar [1s, 2s, 2p, 3s, 3p] has 3 s orbitals (L=0) and 2 sets of p orbitals (L=1). Using cc-pVDZ, orbitals are [1s, 2s, 2p, 3s, 3s', 3p, 3p', 3d'] (where ' represents the added in polarisation orbitals), with 4 s orbitals, 3 sets of p orbitals and 1 set of d orbitals.\nTo understand how to get the number of functions take the cc-pVDZ basis set for H:\nThere are two s (L = 0) orbitals and one p (L = 1) orbital that has 3 components along the z-axis (mL = -1,0,1) corresponding to px, py and pz.  Thus, five spatial orbitals in total.  Note that each orbital can hold two electrons of opposite spin.\nDiffuse functions can also be added for describing anions and long-range interactions such as Van der Waals forces, or to perform electronic excited-state calculations, electric field property calculations. A recipe for constructing additional augmented functions exists; as many as five augmented functions have been used in second hyperpolarizability calculations in the literature.  Because of the rigorous construction of these basis sets, extrapolation can be done for almost any energetic property. However, care must be taken when extrapolating energy differences as the individual energy components converge at different rates: the Hartree-Fock energy converges exponentially, whereas the correlation energy converges only polynomially.\nWeighted core-valence sets (cc-pwCVXZ) have also been recently suggested. The weighted sets aim to capture core-valence correlation, while neglecting most of core-core correlation, in order to yield accurate geometries with smaller cost than the cc-pCVXZ sets.\nWhile the usual Dunning basis sets are for valence-only calculations, the sets can be augmented with further functions that describe core electron correlation. These core-valence sets (cc-pCVXZ) can be used to approach the exact solution to the all-electron problem, and they are necessary for accurate geometric and nuclear property calculations.\nFor period-3 atoms (Al-Ar), additional functions have turned out to be necessary; these are the cc-pV(N+d)Z basis sets.  Even larger atoms may employ pseudopotential basis sets, cc-pVNZ-PP, or relativistic-contracted Douglas-Kroll basis sets, cc-pVNZ-DK.\nFor first- and second-row atoms, the basis sets are cc-pVNZ where N=D,T,Q,5,6,... (D=double, T=triples, etc.). The 'cc-p', stands for 'correlation-consistent polarized' and the 'V' indicates they are valence-only basis sets. They include successively larger shells of polarization (correlating) functions (d, f, g, etc.). More recently these 'correlation-consistent polarized' basis sets have become widely used and are the current state of the art for correlated or post-Hartree\u2013Fock calculations.  Examples of these are:\nOnes of the most widely used basis sets are those developed by Dunning and coworkers,[6]  since they are designed for converging Post-Hartree\u2013Fock calculations systematically to the complete basis set limit using empirical extrapolation techniques.\nPople basis sets are somewhat outdated, as correlation-consistent or polarization-consistent basis sets typically yield better results with similar resources. Also note that some Pople basis sets have grave deficiencies that may lead to incorrect results.[5]\nThe 6-31G* basis set (defined for the atoms H through Zn) is a valence double-zeta polarized basis set that adds to the 6-31G set six d-type Cartesian-Gaussian polarization functions on each of the atoms Li through Ca and ten f-type Cartesian Gaussian polarization functions on each of the atoms Sc through Zn.\nThe notation for the split-valence basis sets arising from the group of John Pople is typically X-YZg.[4]   In this case, X represents the number of primitive Gaussians comprising each core atomic orbital basis function.  The Y and Z indicate that the valence orbitals are composed of two basis functions each, the first one composed of a linear combination of Y primitive Gaussian functions, the other composed of a linear combination of Z primitive Gaussian functions.  In this case, the presence of two numbers after the hyphens implies that this basis set is a split-valence double-zeta basis set.  Split-valence triple- and quadruple-zeta basis sets are also used, denoted as X-YZWg, X-YZWVg, etc.  Here is a list of commonly used split-valence basis sets of this type:\nDuring most molecular bonding, it is the valence electrons which principally take part in the bonding.  In recognition of this fact, it is common to represent valence orbitals by more than one basis function (each of which can in turn be composed of a fixed linear combination of primitive Gaussian functions). Basis sets in which there are multiple basis functions corresponding to each valence atomic orbital are called valence double, triple, quadruple-zeta, and so on, basis sets (zeta, \u03b6, was commonly used to represent the exponent of an STO basis function[3]). Since the different orbitals of the split have different spatial extents, the combination allows the electron density to adjust its spatial extent appropriate to the particular molecular environment. In contrast, minimal basis sets lack the flexibility to adjust to different molecular environments.\nThere are several other minimum basis sets that have been used such as the MidiX basis sets.\nThe most common  minimal basis set is STO-nG, where n is an integer.  This n value represents the number of Gaussian primitive functions comprising a single basis function.  In these basis sets, the same number of Gaussian primitives comprise core and valence orbitals.  Minimal basis sets typically give rough results that are insufficient for research-quality publication, but are much cheaper than their larger counterparts.  Commonly used minimal basis sets of this type are:\nAnother common addition to basis sets is the addition of diffuse functions. These are extended Gaussian basis functions with a small exponent, which give flexibility to the \"tail\" portion of the atomic orbitals, far away from the nucleus. Diffuse basis functions are important for describing anions or dipole moments, but they can also be important for accurate modeling of intra- and intermolecular bonding.\nThe minimal basis set is close to exact for the gas-phase atom. In the next level, additional functions are added to describe polarization of the electron density of the atom in molecules. These are called polarization functions. For example, while the minimal basis set for hydrogen is one function approximating the 1s atomic orbital, a simple polarized basis set typically has two s- and one p-function (which consists of three basis functions: px, py and pz). This adds flexibility to the basis set, effectively allowing molecular orbitals involving the hydrogen atom to be more asymmetric about the hydrogen nucleus. This is very important for modeling chemical bonding, because the bonds are often polarized. Similarly, d-type functions can be added to a basis set with valence p orbitals, and f-functions to a basis set with d-type orbitals, and so on.\nThe smallest basis sets are called minimal basis sets. A minimal basis set is one in which, on each atom in the molecule, a single basis function is used for each orbital in a Hartree\u2013Fock calculation on the free atom. For atoms such as lithium, basis functions of p type are also added to the basis functions that correspond to the 1s and 2s orbitals of the free atom, because lithium also has a 1s2p bound state. For example, each atom in the second period of the periodic system (Li - Ne) would have a basis set of five functions  (two s functions and three p functions).\nDozens of Gaussian-type orbital basis sets have been published in the literature.[1] Basis sets typically come in hierarchies of increasing size, giving a controlled way to obtain more accurate solutions, however at a higher cost.\nHowever, calculating integrals with STOs is computationally difficult and it was later realized by Frank Boys that STOs could be approximated as linear combinations of Gaussian-type orbitals (GTOs) instead. Because the product of two GTOs can be written as a linear combination of GTOs, integrals with Gaussian basis functions can be written in closed form, which leads to huge computational savings (see John Pople).\nWhen molecular calculations are performed, it is common to use a basis composed of atomic orbitals, centered at each nucleus within the molecule (linear combination of atomic orbitals ansatz). The physically best motivated basis set are Slater-type orbitals (STOs),\nwhich are solutions to the Schr\u00f6dinger equation of hydrogen-like atoms, and decay exponentially far away from the nucleus. While hydrogen-like atoms lack many-electron interactions, it can be shown that the molecular orbitals of Hartree-Fock and density-functional theory also exhibit exponential decay. Furthermore, S-type STOs also satisfy Kato's cusp condition at the nucleus, meaning that they are able to accurately describe electron density near the nucleus.\nWithin the basis set, the wavefunction is represented as a vector, the components of which correspond to coefficients of the basis functions in the linear expansion. One-electron Operators correspond to matrices, (rank two tensors), in this basis, whereas two-electron operators are rank four tensors.\nIn modern computational chemistry, quantum chemical calculations are performed using a finite set of basis functions. When the finite basis is expanded towards an (infinite) complete set of functions, calculations using such a basis set are said to approach the complete basis set (CBS) limit. In this article, basis function and atomic orbital are sometimes used interchangeably, although it should be noted that the basis functions are usually not true atomic orbitals, because many basis functions are used to describe polarization effects in molecules.\nThe basis set can either be composed of atomic orbitals (yielding the linear combination of atomic orbitals approach), which is the usual choice within the quantum chemistry community, or plane waves which are typically used within the solid state community. Several types of atomic orbitals can be used: Gaussian-type orbitals, Slater-type orbitals, or numerical atomic orbitals. Out of the three, Gaussian-type orbitals are by far the most often used, as they allow efficient implementations of Post-Hartree\u2013Fock methods.\nThe use of basis sets is equivalent to the use of an approximate resolution of the identity. The single-particle states (molecular orbitals) are then expressed as linear combinations of the basis functions.\nA basis set in theoretical and computational chemistry is a set of functions (called basis functions) that is used to represent the electronic wave function in the Hartree\u2013Fock method or density-functional theory in order to turn the partial differential equations of the model into algebraic equations suitable for efficient implementation on a computer.\n",
            "title": "Basis set (chemistry)",
            "url": "https://en.wikipedia.org/wiki/Basis_set_(chemistry)"
        },
        {
            "desc_links": [
                "/wiki/Linear_algebra",
                "/wiki/Spectral_decomposition",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)",
                "/wiki/Canonical_form",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Diagonalizable_matrix"
            ],
            "links": [
                "/wiki/Matrix_pencil",
                "/wiki/Optics",
                "/wiki/Forward_Scattering_Alignment",
                "/wiki/Radar",
                "/wiki/Back_Scattering_Alignment",
                "/wiki/Generalized_eigenspace",
                "/wiki/Generalized_eigenvector",
                "/wiki/Power_iteration",
                "/wiki/Rayleigh_quotient",
                "/wiki/Hermitian_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Schur_decomposition",
                "/wiki/Backsubstitution",
                "/wiki/Divide-and-conquer_eigenvalue_algorithm",
                "/wiki/Gaussian_elimination",
                "/wiki/System_of_linear_equations#Solving_a_linear_system",
                "/wiki/System_of_linear_equations",
                "/wiki/Sequence",
                "/wiki/Almost_always",
                "/wiki/Google",
                "/wiki/PageRank",
                "/wiki/Linear_span",
                "/wiki/Arnoldi_iteration",
                "/wiki/QR_algorithm",
                "/wiki/Newton%27s_method",
                "/wiki/Round-off_error",
                "/wiki/Ill-conditioned",
                "/wiki/Abel%E2%80%93Ruffini_theorem",
                "/wiki/Iterative_method",
                "/wiki/Characteristic_polynomial",
                "/wiki/Numerical_analysis",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Orthogonal_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Holomorphic_functional_calculus",
                "/wiki/Diagonal_matrix",
                "/wiki/Power_series",
                "/wiki/Laplace_operator",
                "/wiki/Data",
                "/wiki/Inverse_function",
                "/wiki/Diagonal_matrix",
                "/wiki/Nonsingular",
                "/wiki/Linearly_independent",
                "/wiki/Geometric_multiplicity",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Factorization",
                "/wiki/Characteristic_polynomial",
                "/wiki/Spectrum_of_a_matrix",
                "/wiki/Linear_algebra",
                "/wiki/Spectral_decomposition",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)",
                "/wiki/Canonical_form",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Diagonalizable_matrix"
            ],
            "text": "The set of matrices of the form A \u2212 \u03bbB, where \u03bb is a complex number, is called a pencil; the term matrix pencil can also refer to the pair (A,B) of matrices.[9] If B is invertible, then the original problem can be written in the formAnd since P is invertible, we multiply the equation from the right by its inverse, finishing the proof.And the proof isThen the following equality holdswhere A and B are matrices. If v obeys this equation, with some \u03bb, then we call v the generalized eigenvector of A and B (in the 2nd sense), and \u03bb is called the generalized eigenvalue of A and B (in the 2nd sense) which corresponds to the generalized eigenvector v. The possible values of \u03bb must obey the following equationA generalized eigenvalue problem (2nd sense) is the problem of finding a vector v that obeysFor example, in coherent electromagnetic scattering theory, the linear transformation A represents the action performed by the scattering object, and the eigenvectors represent polarization states of the electromagnetic wave. In optics, the coordinate system is defined from the wave's viewpoint, known as the Forward Scattering Alignment (FSA), and gives rise to a regular eigenvalue equation, whereas in radar, the coordinate system is defined from the radar's viewpoint, known as the Back Scattering Alignment (BSA), and gives rise to a coneigenvalue equation.A conjugate eigenvector or coneigenvector is a vector sent after transformation to a scalar multiple of its conjugate, where the scalar is called the conjugate eigenvalue or coneigenvalue of the linear transformation. The coneigenvectors and coneigenvalues represent essentially the same information and meaning as the regular eigenvectors and eigenvalues, but arise when an alternative coordinate system is used. The corresponding equation isThis usage should not be confused with the generalized eigenvalue problem described below.Recall that the geometric multiplicity of an eigenvalue can be described as the dimension of the associated eigenspace, the nullspace of \u03bbI \u2212 A. The algebraic multiplicity can also be thought of as a dimension: it is the dimension of the associated generalized eigenspace (1st sense), which is the nullspace of the matrix (\u03bbI \u2212 A)k for any sufficiently large k. That is, it is the space of generalized eigenvectors (1st sense), where a generalized eigenvector is any vector which eventually becomes 0 if \u03bbI \u2212 A is applied to it enough times successively. Any eigenvector is a generalized eigenvector, and so each eigenspace is contained in the associated generalized eigenspace. This provides an easy proof that the geometric multiplicity is always less than or equal to the algebraic multiplicity.However, in practical large-scale eigenvalue methods, the eigenvectors are usually computed in other ways, as a byproduct of the eigenvalue computation. In power iteration, for example, the eigenvector is actually computed before the eigenvalue (which is typically computed by the Rayleigh quotient of the eigenvector).[6] In the QR algorithm for a Hermitian matrix (or any normal matrix), the orthonormal eigenvectors are obtained as a product of the Q matrices from the steps in the algorithm.[6] (For more general matrices, the QR algorithm yields the Schur decomposition first, from which the eigenvectors can be obtained by a backsubstitution procedure.[8]) For Hermitian matrices, the Divide-and-conquer eigenvalue algorithm is more efficient than the QR algorithm if both eigenvectors and eigenvalues are desired.[6]using Gaussian elimination or any other method for solving matrix equations.Once the eigenvalues are computed, the eigenvectors could be calculated by solving the equationThis sequence will almost always converge to an eigenvector corresponding to the eigenvalue of greatest magnitude, provided that v has a nonzero component of this eigenvector in the eigenvector basis (and also provided that there is only one eigenvalue of greatest magnitude). This simple algorithm is useful in some practical applications; for example, Google uses it to calculate the page rank of documents in their search engine.[7] Also, the power method is the starting point for many more sophisticated algorithms. For instance, by keeping not just the last vector in the sequence, but instead looking at the span of all the vectors in the sequence, one can get a better (faster converging) approximation for the eigenvector, and this idea is the basis of Arnoldi iteration.[6] Alternatively, the important QR algorithm is also based on a subtle transformation of a power method.[6]Iterative numerical algorithms for approximating roots of polynomials exist, such as Newton's method, but in general it is impractical to compute the characteristic polynomial and then apply these methods. One reason is that small round-off errors in the coefficients of the characteristic polynomial can lead to large errors in the eigenvalues and eigenvectors: the roots are an extremely ill-conditioned function of the coefficients.[6]In practice, eigenvalues of large matrices are not computed using the characteristic polynomial. Computing the polynomial becomes expensive in itself, and exact (symbolic) roots of a high-degree polynomial can be difficult to compute and express: the Abel\u2013Ruffini theorem implies that the roots of high-degree (5 or above) polynomials cannot in general be expressed simply using nth roots. Therefore, general algorithms to find eigenvectors and eigenvalues are iterative.Suppose that we want to compute the eigenvalues of a given matrix. If the matrix is small, we can compute them symbolically using the characteristic polynomial. However, this is often impossible for larger matrices, in which case we must use a numerical method.Note that each eigenvalue is multiplied by ni, the algebraic multiplicity.Note that each eigenvalue is raised to the power ni, the algebraic multiplicity.where Q is an orthogonal matrix whose columns are the eigenvectors of A, and \u039b is a diagonal matrix whose entries are the eigenvalues of A.As a special case, for every N\u00d7N real symmetric matrix, the eigenvalues are real and the eigenvectors can be chosen such that they are orthogonal to each other. Thus a real symmetric matrix A can be decomposed asfrom above. Once again, we find thatA similar technique works more generally with the holomorphic functional calculus, usingThe off-diagonal elements of f(\u039b) are zero; that is, f(\u039b) is also a diagonal matrix. Therefore, calculating f(A) reduces to just calculating the function on each of the eigenvalues.Because \u039b is a diagonal matrix, functions of \u039b are very easy to calculate:then we know thatThe eigendecomposition allows for much easier computation of power series of matrices. If f(x) is given bywhere the eigenvalues are subscripted with an 's' to denote being sorted. The position of the minimization is the lowest reliable eigenvalue. In measurement systems, the square root of this reliable eigenvalue is the average noise over the components of the system.If the eigenvalues are rank-sorted by value, then the reliable eigenvalue can be found by minimization of the Laplacian of the sorted eigenvalues:[5]The reliable eigenvalue can be found by assuming that eigenvalues of extremely similar and low value are a good representation of measurement noise (which is assumed low for most systems).The second mitigation extends the eigenvalue so that lower values have much less influence over inversion, but do still contribute, such that solutions near the noise will still be found.The first mitigation method is similar to a sparse sample of the original matrix, removing components that are not considered valuable. However, if the solution or detection process is near the noise level, truncating may remove components that influence the desired solution.Two mitigations have been proposed: 1) truncating small/zero eigenvalues, 2) extending the lowest reliable eigenvalue to those below it.When eigendecomposition is used on a matrix of measured, real data, the inverse may be less valid when all eigenvalues are used unmodified in the form above. This is because as eigenvalues become relatively small, their contribution to the inversion is large. Those near zero or at the \"noise\" of the measurement system will have undue influence and could hamper solutions (detection) using the inverse.Furthermore, because \u039b is a diagonal matrix, its inverse is easy to calculate:If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is nonsingular and its inverse is given byPutting the solutions back into the above simultaneous equationsThusAnd can be represented by a single vector equation involving 2 solutions as eigenvalues:The above equation can be decomposed into 2 simultaneous equations:ThenThe eigenvectors can be indexed by eigenvalues, i.e. using a double index, with vi,j being the jth eigenvector for the ith eigenvalue. The eigenvectors can also be indexed using the simpler notation of a single index vk, with k = 1, 2, ..., Nv.There will be 1 \u2264 mi \u2264 ni linearly independent solutions to each eigenvalue equation. The linear combinations of the mi solutions are the eigenvectors associated with the eigenvalue \u03bbi. The integer mi is termed the geometric multiplicity of \u03bbi. It is important to keep in mind that the algebraic multiplicity ni and geometric multiplicity mi may or may not be equal, but we always have mi \u2264 ni. The simplest case is of course when mi = ni = 1. The total number of linearly independent eigenvectors, Nv, can be calculated by summing the geometric multiplicitiesFor each eigenvalue, \u03bbi, we have a specific eigenvalue equationThe integer ni is termed the algebraic multiplicity of eigenvalue \u03bbi. The algebraic multiplicities sum to N:We can factor p asWe call p(\u03bb) the characteristic polynomial, and the equation, called the characteristic equation, is an Nth order polynomial equation in the unknown \u03bb. This equation will have N\u03bb distinct solutions, where 1 \u2264 N\u03bb \u2264 N . The set of solutions, that is, the eigenvalues, is called the spectrum of A.[1][2][3]This yields an equation for the eigenvaluesA (non-zero) vector v of dimension N is an eigenvector of a square (N\u00d7N) matrix A if it satisfies the linear equationIn linear algebra, eigendecomposition or sometimes spectral decomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way.",
            "title": "Eigendecomposition of a matrix",
            "url": "https://en.wikipedia.org/wiki/Generalized_eigenvalue_problem"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Clemens_C._J._Roothaan",
                "/wiki/George_G._Hall",
                "/wiki/Generalized_eigenvalue_problem",
                "/wiki/Hartree%E2%80%93Fock_equation",
                "/wiki/Basis_set_(chemistry)",
                "/wiki/Gaussian_orbital",
                "/wiki/Slater-type_orbital",
                "/wiki/Molecular_orbital",
                "/wiki/Atomic_orbital"
            ],
            "text": "In contrast to the Hartree\u2013Fock equations - which are integro-differential equations - the Roothaan\u2013Hall equations have a matrix-form. Therefore, they can be solved using standard techniques.The method was developed independently by Clemens C. J. Roothaan and George G. Hall in 1951, and is thus sometimes called the Roothaan-Hall equations.[1][2][3] The Roothaan equations can be written in a form resembling generalized eigenvalue problem, although they are not a standard eigenvalue problem because they are nonlinear:The Roothaan equations are a representation of the Hartree\u2013Fock equation in a non orthonormal basis set which can be of Gaussian-type or Slater-type. It applies to closed-shell molecules or atoms where all molecular orbitals or atomic orbitals, respectively, are doubly occupied. This is generally called Restricted Hartree\u2013Fock theory.",
            "title": "Roothaan equations",
            "url": "https://en.wikipedia.org/wiki/Roothaan_equations"
        },
        {
            "desc_links": [
                "/wiki/Hyperbola",
                "/wiki/%C3%89mile_Borel",
                "/wiki/Linear_algebra",
                "/wiki/Linear_map",
                "/wiki/Area",
                "/wiki/Cartesian_plane",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Shear_mapping"
            ],
            "links": [
                "/wiki/Arithmetic_progression",
                "/wiki/Geometric_progression",
                "/wiki/Transcendental_number",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Alphonse_Antonio_de_Sarasa",
                "/wiki/Gregoire_de_Saint-Vincent",
                "/wiki/Hyperbolic_sector",
                "/wiki/Natural_logarithm",
                "/wiki/Exponential_function",
                "/wiki/Lorentz_group",
                "/wiki/Jones_calculus",
                "/wiki/Lorentz_boost",
                "/wiki/Split-complex_number",
                "/wiki/Split-complex_number#The_diagonal_basis",
                "/wiki/Theory_of_relativity",
                "/wiki/Louis_Kauffman",
                "/wiki/Wolfgang_Rindler",
                "/wiki/Ellipse",
                "/wiki/Streamlines,_streaklines_and_pathlines",
                "/wiki/Potential_flow#Power_laws_with_n_=_2",
                "/wiki/Fluid_dynamics",
                "/wiki/Incompressible_flow",
                "/wiki/Bifurcation_theory",
                "/wiki/Mathematical_model",
                "/wiki/Area",
                "/wiki/Hyperbolic_sector",
                "/wiki/Invariant_(mathematics)",
                "/wiki/Singular-value_decomposition",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Physics",
                "/wiki/Philosophy",
                "/wiki/Connected_component_(topology)",
                "/wiki/SL2(R)",
                "/wiki/Special_linear_group",
                "/wiki/Volume_form",
                "/wiki/M%C3%B6bius_transform",
                "/wiki/SL2(R)#Hyperbolic_elements",
                "/wiki/SL2(R)#Classification_of_elements",
                "/wiki/Orthogonal_group",
                "/wiki/Classical_group",
                "/wiki/Identity_component",
                "/wiki/Indefinite_orthogonal_group",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Quadratic_form",
                "/wiki/Change_of_basis",
                "/wiki/Function_composition",
                "/wiki/One-parameter_group",
                "/wiki/Multiplicative_group",
                "/wiki/Positive_real_numbers",
                "/wiki/Area",
                "/wiki/Quadrature_(mathematics)",
                "/wiki/Gr%C3%A9goire_de_Saint-Vincent",
                "/wiki/Alphonse_Antonio_de_Sarasa",
                "/wiki/Natural_logarithm",
                "/wiki/Hyperbolic_sector",
                "/wiki/Hyperbolic_angle",
                "/wiki/Angle",
                "/wiki/Invariant_measure",
                "/wiki/Hyperbolic_function",
                "/wiki/Circular_functions",
                "/wiki/Hyperbola",
                "/wiki/%C3%89mile_Borel",
                "/wiki/Linear_algebra",
                "/wiki/Linear_map",
                "/wiki/Area",
                "/wiki/Cartesian_plane",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Shear_mapping"
            ],
            "text": "which is a proto-typical arithmetic progression A + nd where A = 0 and d = 1 .corresponds to the asymptotic index achieved with each sum of areasA squeeze with r = e moves the unit angle to one between (e,\u00a01/e) and (ee,\u00a01/ee) which subtends a sector also of area one. The geometric progressionFor instance, for a standard position angle which runs from (1,\u00a01) to (x,\u00a01/x), one may ask \"When is the hyperbolic angle equal to one?\" The answer is the transcendental number x = e.Theorem (Alphonse Antonio de Sarasa 1649) As area measured against the asymptote increases in arithmetic progression, the projections upon the asymptote increase in geometric sequence. Thus the areas form logarithms of the asymptote index.Proof: An argument adding and subtracting triangles of area \u00bd, one triangle being {(0,0), (0,1), (1,1)}, shows the hyperbolic sector area is equal to the area along the asymptote. The theorem then follows from the lemma.Theorem (Gregoire de Saint-Vincent 1647) If bc = ad, then the quadrature of the hyperbola xy = 1 against the asymptote has equal areas between a and b compared to between c and d.Proof: Take parameter r = c/a so that (u,v) = (rx, y/r) takes (a, 1/a) to (c, 1/c) and (b, 1/b) to (d, 1/d).Lemma: If bc = ad, then there is a squeeze mapping that moves the sector(a,b) to sector(c,d).Definition: Sector(a,b) is the hyperbolic sector obtained with central rays to (a, 1/a) and (b, 1/b).The area-preserving property of squeeze mapping has an application in setting the foundation of the transcendental functions natural logarithm and its inverse the exponential function:The term squeeze transformation was used in this context in an article connecting the Lorentz group with Jones calculus in optics.[10]Select (0,0) for a \"here and now\" in a spacetime. Light radiant left and right through this central event tracks two lines in the spacetime, lines that can be used to give coordinates to events away from (0,0). Trajectories of lesser velocity track closer to the original timeline (0,t). Any such velocity can be viewed as a zero velocity under a squeeze mapping called a Lorentz boost. This insight follows from a study of split-complex number multiplications and the diagonal basis which corresponds to the pair of light lines. Formally, a squeeze preserves the hyperbolic metric expressed in the form xy; in a different coordinate system. This application in the theory of relativity was noted in 1912 by Wilson and Lewis,[6] by Werner Greub,[7] and by Louis Kauffman.[8] Furthermore, Wolfgang Rindler, in his popular textbook on relativity, used the squeeze mapping form of Lorentz transformations in his demonstration of their characteristic property.[9]Stocker and Hosoi then recall Moffatt's[5] consideration of \"flow in a corner between rigid boundaries, induced by an arbitrary disturbance at a large distance.\" According to Stocker and Hosoi,Stocker and Hosoi[4] described their approach to corner flow as follows:so negative K corresponds to an ellipse and positive K to a hyperbola, with the rectangular case of the squeeze mapping corresponding to K = 1.where K lies in the interval [\u22121, 1]. The streamlines follow the curvesIn 1989 Ottino[3] described the \"linear isochoric two-dimensional flow\" asFor another approach to a flow with hyperbolic streamlines, see Potential flow \u00a7\u00a0Power laws with n = 2.In fluid dynamics one of the fundamental motions of an incompressible flow involves bifurcation of a flow running up against an immovable wall. Representing the wall by the axis y = 0 and taking the parameter r = exp(t) where t is time, then the squeeze mapping with parameter r applied to an initial fluid state produces a flow with bifurcation left and right of the axis x = 0. The same model gives fluid convergence when time is run backward. Indeed, the area of any hyperbolic sector is invariant under squeezing.In studying linear algebra there are the purely abstract applications such as illustration of the singular-value decomposition or in the important role of the squeeze mapping in the structure of 2 \u00d7 2 real matrices. These applications are somewhat bland compared to two physical and a philosophical application.are not allowed, though they preserve the form (in terms of x and y these are x \u21a6 y, y \u21a6 x and x \u21a6 \u2212x, y \u21a6 \u2212y); the additional \"+\" in the hyperbolic case (as compared with the circular case) is necessary to specify the identity component because the group O(1,1) has 4 connected components, while the group O(2) has 2 components: SO(1,1) has 2 components, while SO(2) only has 1. The fact that the squeeze transforms preserve area and orientation corresponds to the inclusion of subgroups SO \u2282 SL \u2013 in this case SO(1,1)\u00a0\u2282\u00a0SL(2) \u2013 of the subgroup of hyperbolic rotations in the special linear group of transforms preserving area and orientation (a volume form). In the language of M\u00f6bius transforms, the squeeze transformations are the hyperbolic elements in the classification of elements.Note that the \"SO+\" notation corresponds to the fact that the reflectionsand corresponds geometrically to preserving hyperbolae. The perspective of the group of squeeze mappings as hyperbolic rotation is analogous to interpreting the group SO(2) (the connected component of the definite orthogonal group) preserving quadratic form x2 + y2 as being circular rotations.From the point of view of the classical groups, the group of squeeze mappings is SO+(1,1), the identity component of the indefinite orthogonal group of 2 \u00d7 2 real matrices preserving the quadratic form u2 \u2212 v2. This is equivalent to preserving the form xy via the change of basisIf r and s are positive real numbers, the composition of their squeeze mappings is the squeeze mapping of their product. Therefore, the collection of squeeze mappings forms a one-parameter group isomorphic to the multiplicative group of positive real numbers. An additive view of this group arises from consideration of hyperbolic sectors and their hyperbolic angles.The squeeze mapping sets the stage for development of the concept of logarithms. The problem of finding the area bounded by a hyperbola (such as xy = 1) is one of quadrature. The solution, found by Gr\u00e9goire de Saint-Vincent and Alphonse Antonio de Sarasa in 1647, required the natural logarithm function, a new concept. Some insight into logarithms comes through hyperbolic sectors that are permuted by squeeze mappings while preserving their area. The area of a hyperbolic sector is taken as a measure of a hyperbolic angle associated with the sector. The hyperbolic angle concept is quite independent of the ordinary circular angle, but shares a property of invariance with it: whereas circular angle is invariant under rotation, hyperbolic angle is invariant under squeeze mapping. Both circular and hyperbolic angle generate invariant measures but with respect to different transformation groups. The hyperbolic functions, which take hyperbolic angle as argument, perform the role that circular functions play with the circular angle argument.[2]is a hyperbola, if u = ax and v = y/a, then uv = xy and the points of the image of the squeeze mapping are on the same hyperbola as (x,y) is. For this reason it is natural to think of the squeeze mapping as a hyperbolic rotation, as did \u00c9mile Borel in 1914,[1] by analogy with circular rotations, which preserve circles.is the squeeze mapping with parameter a. SinceFor a fixed positive real number a, the mappingIn linear algebra, a squeeze mapping is a type of linear map that preserves Euclidean area of regions in the Cartesian plane, but is not a rotation or shear mapping.",
            "title": "Squeeze mapping",
            "url": "https://en.wikipedia.org/wiki/Squeeze_mapping"
        },
        {
            "desc_links": [
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Coefficient",
                "/wiki/Field_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Non-linear_equation",
                "/wiki/Algebraic_equation",
                "/wiki/Term_(mathematics)",
                "/wiki/Constant_term",
                "/wiki/Variable_(mathematics)",
                "/wiki/Number",
                "/wiki/Parameter",
                "/wiki/Function_(mathematics)",
                "/wiki/Linear_regression"
            ],
            "links": [
                "/wiki/Three-dimensional_space",
                "/wiki/Hyperplane",
                "/wiki/Euclidean_space",
                "/wiki/Affine_space",
                "/wiki/Equation_solving",
                "/wiki/Tax_bracket",
                "/wiki/Progressive_tax#Computation",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Linear_map",
                "/wiki/Affine_function",
                "/wiki/Graph_of_a_function",
                "/wiki/Vertical_line_test",
                "/wiki/Interpolation",
                "/wiki/Extrapolation",
                "/wiki/Simultaneous_equations",
                "/wiki/Linear_algebra",
                "/wiki/System_of_linear_equations",
                "/wiki/Gauss-Jordan",
                "/wiki/Determinant",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Line_(geometry)",
                "/wiki/Coordinate",
                "/wiki/Slope",
                "/wiki/Elementary_algebra",
                "/wiki/Constant_term",
                "/wiki/Nonlinear_system",
                "/wiki/Straight_line",
                "/wiki/Slope",
                "/wiki/Constant_term",
                "/wiki/Inconsistent_equations",
                "/wiki/Equation",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Coefficient",
                "/wiki/Field_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Non-linear_equation",
                "/wiki/Algebraic_equation",
                "/wiki/Term_(mathematics)",
                "/wiki/Constant_term",
                "/wiki/Variable_(mathematics)",
                "/wiki/Number",
                "/wiki/Parameter",
                "/wiki/Function_(mathematics)",
                "/wiki/Linear_regression"
            ],
            "text": "If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n\u00a0\u2013\u00a01)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).In other words, if ai \u2260 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.If at least one coefficient is nonzero, a permutation of the subscripts allows one to suppose a1 \u2260 0, and rewrite the equationIf all the coefficients are zero, then either b \u2260 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewrittenAn everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point\u2013slope form or slope\u2013intercept form.where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.andA linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of straight line which is not the graph of a function (it obviously fails the vertical line test).This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.Ergo,Thus,One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.andThese are two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:andSince this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.becomes:Further, this representation extends to systems of linear equations.one can rewrite the equation in matrix form:Using the order of the standard formwhere a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.Using a determinant, one gets a determinant form, easy to remember:Expanding the products and regrouping the terms leads to the general form:Multiplying both sides of this equation by (x2\u00a0\u2212\u00a0x1) yields a form of the line generally referred to as the symmetric form:where (x1,\u00a0y1) and (x2,\u00a0y2) are two points on the line with x2 \u2260 x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2\u00a0\u2212\u00a0y1)/(x2\u00a0\u2212\u00a0x1).The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y\u00a0\u2212\u00a0y1) is proportional to the difference in the x coordinate (that is, x\u00a0\u2212\u00a0x1). The proportionality constant is m (the slope of the line).where m is the slope of the line and (x1,y1) is any point on the line.where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where the line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.where A and B are not both equal to zero. The equation is usually written so that A \u2265 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is \u2212A/B. The general form is sometimes written as:In the general (or standard[1]) form the linear equation is written as:Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the \"equations of the straight line.\" In what follows, x, y, t, and \u03b8 are variables; other letters represent constants (fixed numbers).Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.where m and b designate constants (parameters). The origin of the name \"linear\" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, known as the y-intercept.A common form of a linear equation in the two variables x and y isIf a = 0, then, if b = 0, every number is a solution of the equation, and, if b \u2260 0, there are no solutions (and the equation is said to be inconsistent).If a \u2260 0, there is a unique solutionA linear equation in one unknown x may always be rewrittenThis article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.Equations with exponents greater than one are non-linear. An example of a non-linear equation of two variables is axy + b = 0, where a and b are constants and a \u2260 0. It has two variables, x and y, and is non-linear because the sum of the exponents of the variables in the first term, axy, is two.Linear equations can have one or more variables. An example of a linear equation with three variables, x, y, and z, is given by: ax + by + cz + d = 0, where a, b, c, and d are constants and a, b, and c are non-zero. Linear equations occur frequently in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some \"background\" state. An algebraic equation is linear if the sum of the exponents of the variables of each term is one.A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable (however, different variables may occur in different terms). A simple example of a linear equation with only one variable, x, may be written in the form: ax + b = 0, where a and b are constants and a \u2260 0. The constants may be numbers, parameters, or even non-linear functions of parameters, and the distinction between variables and parameters may depend on the problem (for an example, see linear regression).",
            "title": "Linear equation",
            "url": "https://en.wikipedia.org/wiki/Linear_equation"
        },
        {
            "desc_links": [
                "/wiki/Mathematical_model",
                "/wiki/System",
                "/wiki/Linear_operator",
                "/wiki/Nonlinear",
                "/wiki/Automatic_control",
                "/wiki/Signal_processing",
                "/wiki/Telecommunications"
            ],
            "links": [
                "/wiki/Dirac_delta_function",
                "/wiki/Impulse_function",
                "/wiki/Linearization",
                "/wiki/Function_(mathematics)",
                "/wiki/Vector_(geometric)",
                "/wiki/Differential_equation",
                "/wiki/Time-invariant_system",
                "/wiki/Laplace_transform",
                "/wiki/Continuous_function",
                "/wiki/Z-transform",
                "/wiki/Discrete_mathematics",
                "/wiki/Mathematical_model",
                "/wiki/System",
                "/wiki/Linear_operator",
                "/wiki/Nonlinear",
                "/wiki/Automatic_control",
                "/wiki/Signal_processing",
                "/wiki/Telecommunications"
            ],
            "text": "represents the lag time between the stimulus at time m and the response at time n.whereor equivalently for a time-invariant system on redefining h(),The output of any discrete time linear system is related to the input by the time-varying convolution sum:In applications this is usually a rational algebraic function of s. Because h(t) is zero for negative t, the integral may equally be written over the doubly infinite range and putting s = i\u03c9 follows the formula for the frequency response function:Linear time-invariant systems are most commonly characterized by the Laplace transform of the impulse response function called the transfer function which is:If the properties of the system do not depend on the time at which it is operated then it is said to be time-invariant and h() is a function only of the time difference \u03c4 = t-t' which is zero for \u03c4<0 (namely t<t'). By redefinition of h() it is then possible to write the input-output relation equivalently in any of the ways,The output of any general continuous-time linear system is related to the input by an integral which may be written over a doubly infinite range because of the causality condition:then the function h(t2,t1) is the time-varying impulse response of the system. Since the system cannot respond before the input is applied the following causality condition must be satisfied:where \u03b4(t) represents the Dirac delta function, and the corresponding response y(t) of the system isThe time-varying impulse response h(t2,t1) of a linear system is defined as the response of the system at time t = t2 to a single impulse applied at time t = t1. In other words, if the input x(t) to a linear system isA common use of linear models is to describe a nonlinear system by linearization. This is usually done for mathematical convenience.Another perspective is that solutions to linear systems comprise a system of functions which act like vectors in the geometric sense.Typical differential equations of linear time-invariant systems are well adapted to analysis using the Laplace transform in the continuous case, and the Z-transform in the discrete case (especially in computer implementations).Ifthen a linear system must satisfyas well as their respective outputsA linear system is a mathematical model of a system based on the use of a linear operator. Linear systems typically exhibit features and properties that are much simpler than the nonlinear case. As a mathematical abstraction or idealization, linear systems find important applications in automatic control theory, signal processing, and telecommunications. For example, the propagation medium for wireless communication systems can often be modeled by linear systems.",
            "title": "Linear system",
            "url": "https://en.wikipedia.org/wiki/Linear_system"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "QR algorithm",
            "url": "https://en.wikipedia.org/wiki/QR_algorithm"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Householder transformation",
            "url": "https://en.wikipedia.org/wiki/Householder_transformation"
        },
        {
            "desc_links": [
                "/wiki/Category:Wikipedia_backlog",
                "/wiki/Wikipedia:Citation_needed#How_to_help_reduce_the_backlog",
                "/wiki/Wikipedia:Verifiability",
                "/wiki/Wikipedia:Citing_sources#When_and_why_to_cite_sources",
                "/wiki/Template:Citation_needed"
            ],
            "links": [
                "/wiki/Wikipedia:Verification",
                "/wiki/Category:All_articles_with_unsourced_statements",
                "/wiki/Wikipedia:Verify",
                "/wiki/Category:Wikipedia_backlog",
                "/wiki/Category:Wikipedia_backlog",
                "/wiki/Wikipedia:Citation_needed#How_to_help_reduce_the_backlog",
                "/wiki/Wikipedia:Verifiability",
                "/wiki/Wikipedia:Citing_sources#When_and_why_to_cite_sources",
                "/wiki/Template:Citation_needed"
            ],
            "text": "Frequently the authors of statements do not return to Wikipedia to support the statement with citations, so other Wikipedia editors have to do work checking those statements. With  337,972 statements that need WP:Verification, sometimes it's hard to choose which article to work on. The tool Citation Hunt makes that easier by suggesting random articles, which you can sort by topical category membership. \nAt the moment, there are over 337,972 articles with \"Citation needed\" statements. You can browse the whole list of these articles at Category:All articles with unsourced statements.\nBefore adding a tag, at least consider the following alternatives, one of which may prove much more constructive:\nA \"citation needed\" tag is a request for another editor to verify a statement: a form of communication between members of a collaborative editing community. It is never, in itself, an \"improvement\" of an article. Though readers may be alerted by a \"citation needed\" that a particular statement is not supported, many readers don't fully understand the community's processes. Not all tags get addressed in a timely manner, staying in place for months or years, forming an ever growing Wikipedia backlog\u2014this itself can be a problem. Best practice recommends the following: \n\"Citation needed\" statements are part of  Wikipedia's backlog of outstanding problems. Currently there are 337,972 articles with \"Citation needed\" statements (see the historical number of tags). You can help reduce the backlog!\nTo ensure that all Wikipedia content is verifiable, anyone may question an uncited claim by inserting a simple {{Citation needed}} tag, or by using a more comprehensive {{Citation needed|reason=Your explanation here|date=April 2018}} clause. This displays as: \n\n",
            "title": "Wikipedia:Citation needed",
            "url": "https://en.wikipedia.org/wiki/Wikipedia:Citation_needed"
        },
        {
            "desc_links": [
                "/wiki/Charles_Hermite",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Conjugate_transpose",
                "/wiki/Symmetric_matrix",
                "/wiki/Complex_number",
                "/wiki/Square_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Complex_conjugate"
            ],
            "links": [
                "/wiki/Pauli_matrices",
                "/wiki/Gell-Mann_matrices",
                "/wiki/Theoretical_physics",
                "/wiki/Imaginary_number",
                "/wiki/Real_number",
                "/wiki/Matrix_mechanics",
                "/wiki/Werner_Heisenberg",
                "/wiki/Max_Born",
                "/wiki/Pascual_Jordan",
                "/wiki/Charles_Hermite",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Symmetric_matrix",
                "/wiki/Complex_number",
                "/wiki/Square_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Complex_conjugate"
            ],
            "text": "The Rayleigh quotient is used in the min-max theorem to get exact values of all eigenvalues. It is also used in eigenvalue algorithms to obtain an eigenvalue approximation from an eigenvector approximation. Specifically, this is the basis for Rayleigh quotient iteration.Additional facts related to Hermitian matrices include:Well-known families of Pauli matrices, Gell-Mann matrices and their generalizations are Hermitian. In theoretical physics such Hermitian matrices are often multiplied by imaginary coefficients,[1][2] which results in skew-Hermitian matrices (see below).The diagonal elements must be real, as they must be their own complex conjugate.See the following example:Hermitian matrices are fundamental to the quantum theory of matrix mechanics created by Werner Heisenberg, Max Born, and Pascual Jordan in 1925.Hermitian matrices are named after Charles Hermite, who demonstrated in 1855 that matrices of this form share a property with real symmetric matrices of always having real eigenvalues.Hermitian matrices can be understood as the complex extension of real symmetric matrices.In mathematics, a Hermitian matrix (or self-adjoint matrix) is a complex square matrix that is equal to its own conjugate transpose\u2014that is, the element in the i-th row and j-th column is equal to the complex conjugate of the element in the j-th row and i-th column, for all indices i and j:",
            "title": "Hermitian matrix",
            "url": "https://en.wikipedia.org/wiki/Hermitian_matrix"
        },
        {
            "desc_links": [
                "/wiki/Computer",
                "/wiki/Algorithm",
                "/wiki/Data_structure",
                "/wiki/Computer_memory",
                "/wiki/Data_compression",
                "/wiki/Computer_data_storage",
                "/wiki/Scientific",
                "/wiki/Engineering",
                "/wiki/Partial_differential_equation",
                "/wiki/Loosely_coupled",
                "/wiki/Combinatorics",
                "/wiki/Network_theory",
                "/wiki/Numerical_analysis",
                "/wiki/Computer_science",
                "/wiki/Matrix_(mathematics)"
            ],
            "links": [
                "/wiki/Harry_Markowitz",
                "/wiki/Iterative_method",
                "/wiki/Cholesky_decomposition",
                "/wiki/Symbolic_Cholesky_decomposition",
                "/wiki/Cholesky_decomposition",
                "/wiki/Adjacency_matrix",
                "/wiki/Undirected_graph",
                "/wiki/Adjacency_list",
                "/wiki/Diagonal_matrix",
                "/wiki/Main_diagonal",
                "/wiki/One-dimensional_array",
                "/wiki/Graph_bandwidth",
                "/wiki/Band_matrix",
                "/wiki/Lower_bandwidth_of_a_matrix",
                "/wiki/Band_matrix#upper_bandwidth",
                "/wiki/Tridiagonal_matrix",
                "/wiki/Zero-based_numbering",
                "/wiki/Associative_array",
                "/wiki/Ordered_pair",
                "/wiki/Array_data_structure",
                "/wiki/Computer",
                "/wiki/Algorithm",
                "/wiki/Data_structure",
                "/wiki/Computer_memory",
                "/wiki/Data_compression",
                "/wiki/Computer_data_storage",
                "/wiki/Scientific",
                "/wiki/Engineering",
                "/wiki/Partial_differential_equation",
                "/wiki/Loosely_coupled",
                "/wiki/Combinatorics",
                "/wiki/Network_theory",
                "/wiki/Numerical_analysis",
                "/wiki/Computer_science",
                "/wiki/Matrix_(mathematics)"
            ],
            "text": "The term sparse matrix was possibly coined by Harry Markowitz who triggered some pioneering work but then left the field.[7]Several software libraries support sparse matrices, and provide solvers for sparse matrix equations. The following are open-source:Both iterative and direct methods exist for sparse matrix solving.There are other methods than the Cholesky decomposition in use. Orthogonalization methods (such as QR factorization) are common, for example, when solving problems by least squares methods. While the theoretical fill-in is still the same, in practical terms the \"false non-zeros\" can be different for different methods. And symbolic versions of those algorithms can be used in the same manner as the symbolic Cholesky to compute worst case fill-in.The fill-in of a matrix are those entries that change from an initial zero to a non-zero value during the execution of an algorithm. To reduce the memory requirements and the number of arithmetic operations used during an algorithm, it is useful to minimize the fill-in by switching rows and columns in the matrix. The symbolic Cholesky decomposition can be used to calculate the worst possible fill-in before doing the actual Cholesky decomposition.A symmetric sparse matrix arises as the adjacency matrix of an undirected graph; it can be stored efficiently as an adjacency list.A very efficient structure for an extreme case of band matrices, the diagonal matrix, is to store just the entries in the main diagonal as a one-dimensional array, so a diagonal n \u00d7 n matrix requires only n entries.By rearranging the rows and columns of a matrix A it may be possible to obtain a matrix A\u2032 with a lower bandwidth. A number of algorithms are designed for bandwidth minimization.Matrices with reasonably small upper and lower bandwidth are known as band matrices and often lend themselves to simpler algorithms than general sparse matrices; or one can sometimes apply dense matrix algorithms and gain efficiency simply by looping over a reduced number of indices.An important special type of sparse matrices is band matrix, defined as follows. The lower bandwidth of a matrix A is the smallest number p such that the entry ai,j vanishes whenever i > j + p. Similarly, the upper bandwidth is the smallest number p such that ai,j = 0 whenever i < j \u2212 p (Golub & Van Loan 1996, \u00a71.2.1). For example, a tridiagonal matrix has lower bandwidth 1 and upper bandwidth 1. As another example, the following sparse matrix has lower and upper bandwidth both equal to 3. Notice that zeros are represented with dots for clarity.CSC is similar to CSR except that values are read first by column, a row index is stored for each value, and column pointers are stored. For example, CSC is (val, row_ind, col_ptr), where val is an array of the (top-to-bottom, then left-to-right) non-zero values of the matrix; row_ind is the row indices corresponding to the values; and, col_ptr is the list of val indexes where each column starts. The name is based on the fact that column index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, column slicing, and matrix-vector products. See scipy.sparse.csc_matrix. This is the traditional format for specifying a sparse matrix in MATLAB (via the sparse function).The (old and new) Yale sparse matrix formats are instances of the CSR scheme. The old Yale format works exactly as described above, with three arrays; the new format achieves a further compression by combining IA and JA into a single array.[6]Note that in this format, the first value of IA is always zero and the last is always NNZ, so they are in some sense redundant (although in programming languages where the array length needs to be explicitly stored, NNZ would not be redundant). Nonetheless, this does avoid the need to handle an exceptional case when computing the length of each row, as it guarantees the formula IA[i + 1] \u2212 IA[i] works for any row i. Moreover, the memory cost of this redundant storage is likely insignificant for a sufficiently large matrix.The whole is stored as 21 entries.is a 4 \u00d7 6 matrix (24 entries) with 8 nonzero elements, soIn this case the CSR representation contains 13 entries, compared to 16 in the original matrix. The CSR format saves on memory only when NNZ < (m (n \u2212 1) \u2212 1) / 2. Another example, the matrixSo, in array JA, the element \"5\" from A has column index 0, \"8\" and \"6\" have index 1, and element \"3\" has index 2.is a 4 \u00d7 4 matrix with 4 nonzero elements, henceFor example, the matrixThe CSR format stores a sparse m \u00d7 n matrix M in row form using three (one-dimensional) arrays (A, IA, JA). Let NNZ denote the number of nonzero entries in M. (Note that zero-based indices shall be used here.)The compressed sparse row (CSR) or compressed row storage (CRS) format represents a matrix M by three (one-dimensional) arrays, that respectively contain nonzero values, the extents of rows, and column indices. It is similar to COO, but compresses the row indices, hence the name. This format allows fast row access and matrix-vector multiplications (Mx). The CSR format has been in use since at least the mid-1960s, with the first complete description appearing in 1967.[4]COO stores a list of (row, column, value) tuples. Ideally, the entries are sorted first by row index and then by column index, to improve random access times. This is another format that is good for incremental matrix construction.[3]LIL stores one list per row, with each entry containing the column index and the value. Typically, these entries are kept sorted by column index for faster lookup. This is another format good for incremental matrix construction.[2]DOK consists of a dictionary that maps (row, column)-pairs to the value of the elements. Elements that are missing from the dictionary are taken to be zero. The format is good for incrementally constructing a sparse matrix in random order, but poor for iterating over non-zero values in lexicographical order. One typically constructs a matrix in this format and then converts to another more efficient format for processing.[1]Formats can be divided into two groups:In the case of a sparse matrix, substantial memory requirement reductions can be realized by storing only the non-zero entries. Depending on the number and distribution of the non-zero entries, different data structures can be used and yield huge savings in memory when compared to the basic approach. The trade-off is that accessing the individual elements becomes more complex and additional structures are needed to be able to recover the original matrix unambiguously.A matrix is typically stored as a two-dimensional array. Each entry in the array represents an element ai,j of the matrix and is accessed by the two indices i and j. Conventionally, i is the row index, numbered from top to bottom, and j is the column index, numbered from left to right. For an m \u00d7 n matrix, the amount of memory required to store the matrix in this format is proportional to m \u00d7 n (disregarding the fact that the dimensions of the matrix also need to be stored).When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and memory are wasted on the zeroes. Sparse data is by nature more easily compressed and thus requires significantly less storage. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms.Large sparse matrices often appear in scientific or engineering applications when solving partial differential equations.Conceptually, sparsity corresponds to systems that are loosely coupled. Consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls had springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in combinatorics and application areas such as network theory, which have a low density of significant data or connections.In numerical analysis and computer science, a sparse matrix or sparse array is a matrix in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The number of zero-valued elements divided by the total number of elements (e.g., m \u00d7 n for an m \u00d7 n matrix) is called the sparsity of the matrix (which is equal to 1 minus the density of the matrix).",
            "title": "Sparse matrix",
            "url": "https://en.wikipedia.org/wiki/Sparse_matrix"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Lanczos algorithm",
            "url": "https://en.wikipedia.org/wiki/Lanczos_algorithm"
        },
        {
            "desc_links": [
                "/wiki/Rounding_error",
                "/wiki/Gaussian_elimination",
                "/wiki/Nonlinear_equation",
                "/wiki/Computational_mathematics",
                "/wiki/Algorithm",
                "/wiki/Algorithm#Termination",
                "/wiki/Algorithm",
                "/wiki/Heuristic"
            ],
            "links": [
                "/wiki/D.M._Young",
                "/wiki/Cornelius_Lanczos",
                "/wiki/Magnus_Hestenes",
                "/wiki/Eduard_Stiefel",
                "/wiki/Partial_differential_equation",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/Krylov_subspace_methods",
                "/wiki/GMRES",
                "/wiki/Preconditioning",
                "/wiki/Spectrum_of_an_operator",
                "/wiki/Relaxation_(iterative_method)",
                "/wiki/Operator_(mathematics)",
                "/wiki/Residual_(numerical_analysis)",
                "/wiki/System_of_linear_equations",
                "/wiki/Krylov_subspace",
                "/wiki/Fixed_point_(mathematics)",
                "/wiki/Basin_of_attraction",
                "/wiki/Continuously_differentiable",
                "/wiki/Spectral_radius",
                "/wiki/Computational_mathematics",
                "/wiki/Algorithm",
                "/wiki/Algorithm#Termination",
                "/wiki/Algorithm",
                "/wiki/Heuristic"
            ],
            "text": "The theory of stationary iterative methods was solidly established with the work of D.M. Young starting in the 1950s. The Conjugate Gradient method was also invented in the 1950s, with independent developments by Cornelius Lanczos, Magnus Hestenes and Eduard Stiefel, but its nature and applicability were misunderstood at the time. Only in the 1970s was it realized that conjugacy based methods work very well for partial differential equations, especially the elliptic type.Probably the first iterative method for solving a linear system appeared in a letter of Gauss to a student of his. He proposed solving a 4-by-4 system of equations by repeatedly solving the component in which the residual was the largest.The approximating operator that appears in stationary iterative methods can also be incorporated in Krylov subspace methods such as GMRES (alternatively, preconditioned Krylov methods can be considered as accelerations of stationary iterative methods), where they become transformations of the original operator to a presumably better conditioned one. The construction of preconditioners is a large research area.Since these methods form a basis, it is evident that the method converges in N iterations, where N is the system size. However, in the presence of rounding errors this statement does not hold; moreover, in practice N can be very large, and the iterative process reaches sufficient accuracy already far earlier. The analysis of these methods is hard, depending on a complicated function of the spectrum of the operator.Linear stationary iterative methods are also called relaxation methods.From this follows that the iteration matrix is given byAn iterative method is defined byStationary iterative methods solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the residual), form a \"correction equation\" for which this process is repeated. While these methods are simple to derive, implement, and analyze, convergence is only guaranteed for a limited class of matrices.In the case of a system of linear equations, the two main classes of iterative methods are the stationary iterative methods, and the more general Krylov subspace methods.If an equation can be put into the form f(x) = x, and a solution x is an attractive fixed point of the function f, then one may begin with a point x1 in the basin of attraction of x, and let xn+1 = f(xn) for n\u00a0\u2265\u00a01, and the sequence {xn}n\u00a0\u2265\u00a01 will converge to the solution x. Here xn is the nth approximation or iteration of x and xn+1 is the next or n + 1 iteration of x. Alternately, superscripts in parentheses are often used in numerical methods, so as not to interfere with subscripts with other meanings. (For example, x(n+1) = f(x(n)).) If the function f is continuously differentiable, a sufficient condition for convergence is that the spectral radius of the derivative is strictly bounded by one in a neighborhood of the fixed point. If this condition holds at the fixed point, then a sufficiently small neighborhood (basin of attraction) must exist.In computational mathematics, an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the n-th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common.",
            "title": "Iterative method",
            "url": "https://en.wikipedia.org/wiki/Iterative_method"
        },
        {
            "desc_links": [
                "/wiki/Random_errors",
                "/wiki/Statistical_variability"
            ],
            "links": [
                "/wiki/Data_quality",
                "/wiki/Information_quality",
                "/wiki/Logic_simulation",
                "/wiki/Electronic_circuit_simulation",
                "/wiki/Transistor",
                "/wiki/Transistor_models",
                "/wiki/Psychometrics",
                "/wiki/Psychophysics",
                "/wiki/Validity_(statistics)",
                "/wiki/Reliability_(statistics)",
                "/wiki/Cronbach%27s_alpha",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Binary_classification",
                "/wiki/True_positive",
                "/wiki/True_negative",
                "/wiki/Rand_index",
                "/wiki/Bias_(statistics)",
                "/wiki/False_precision",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Significant_figures",
                "/wiki/Standard_error_(statistics)",
                "/wiki/Central_limit_theorem",
                "/wiki/Probability_distribution",
                "/wiki/Traceability",
                "/wiki/Technical_standard",
                "/wiki/SI",
                "/wiki/Standards_organization",
                "/wiki/National_Institute_of_Standards_and_Technology",
                "/wiki/Numerical_analysis",
                "/wiki/Sensor#Resolution",
                "/wiki/Random",
                "/wiki/Independent_variable",
                "/wiki/Systematic_error",
                "/wiki/Sample_size",
                "/wiki/Statistics",
                "/wiki/Bias_(statistics)",
                "/wiki/Variability_(statistics)",
                "/wiki/Science",
                "/wiki/Engineering",
                "/wiki/Measurement",
                "/wiki/Quantity",
                "/wiki/Value_(mathematics)",
                "/wiki/Reproducibility",
                "/wiki/Repeatability",
                "/wiki/Result",
                "/wiki/Synonymous",
                "/wiki/Colloquial",
                "/wiki/Scientific_method",
                "/wiki/Random_errors",
                "/wiki/Statistical_variability"
            ],
            "text": "The concepts of accuracy and precision have also been studied in the context of databases, information systems and their sociotechnical context. The necessary extension of these two concepts on the basis of theory of science suggests that they (as well as data quality and information quality) should be centered on accuracy defined as the closeness to the true value seen as the degree of agreement of readings or of calculated values of one same conceived entity, measured or calculated by different methods, in the context of maximum possible disagreement.[13]In logic simulation, a common mistake in evaluation of accurate models is to compare a logic simulation model to a transistor circuit simulation model. This is a comparison of differences in precision, not accuracy. Precision is measured with respect to detail and accuracy is measured with respect to reality.[11][12]In psychometrics and psychophysics, the term accuracy is interchangeably used with validity and constant error. Precision is a synonym for reliability and variable error. The validity of a measurement instrument or psychological test is established through experiment or correlation with behavior. Reliability is established with a variety of statistical techniques, classically through an internal consistency test like Cronbach's alpha to ensure sets of related questions have related responses, and then comparison of those related question between reference and target population.[citation needed]Accuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition. That is, the accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined.[7] To make the context clear by the semantics, it is often referred to as the \"Rand accuracy\" or \"Rand index\".[8][9][10] It is a parameter of the test.ISO 5725-1 and VIM also avoid the use of the term \"bias\", previously specified in BS 5497-1,[6] because it has different connotations outside the fields of science and engineering, as in medicine and law.According to ISO 5725-1,[5] the general term \"accuracy\" is used to describe the closeness of a measurement to the true value. When the term is applied to sets of measurements of the same measurand, it involves a component of random error and a component of systematic error. In this case trueness is the closeness of the mean of a set of measurement results to the actual (true) value and precision is the closeness of agreement among a set of results.A shift in the meaning of these terms appeared with the publication of the ISO 5725 series of standards in 1994, which is also reflected in the 2008 issue of the \"BIPM International Vocabulary of Metrology\" (VIM), items 2.13 and 2.14.[1]Precision includes:A reading of 8,000\u00a0m, with trailing zeroes and no decimal point, is ambiguous; the trailing zeroes may or may not be intended as significant figures. To avoid this ambiguity, the number could be represented in scientific notation: 8.0\u00a0\u00d7\u00a0103\u00a0m indicates that the first zero is significant (hence a margin of 50\u00a0m) while 8.000\u00a0\u00d7\u00a0103\u00a0m indicates that all three zeroes are significant, giving a margin of 0.5\u00a0m. Similarly, it is possible to use a multiple of the basic measurement unit: 8.0\u00a0km is equivalent to 8.0\u00a0\u00d7\u00a0103\u00a0m. In fact, it indicates a margin of 0.05\u00a0km (50\u00a0m). However, reliance on this convention can lead to false precision errors when accepting data from sources that do not obey it.[citation needed]A common convention in science and engineering is to express accuracy and/or precision implicitly by means of significant figures. Here, when not explicitly stated, the margin of error is understood to be one-half the value of the last significant place. For instance, a recording of 843.6\u00a0m, or 843.0\u00a0m, or 800.0\u00a0m would imply a margin of 0.05\u00a0m (the last significant place is the tenths place), while a recording of 8436\u00a0m would imply a margin of error of 0.5\u00a0m (the last significant digits are the units).With regard to accuracy we can distinguish:This also applies when measurements are repeated and averaged. In that case, the term standard error is properly applied: the precision of the average is equal to the known standard deviation of the process divided by the square root of the number of measurements averaged. Further, the central limit theorem shows that the probability distribution of the averaged measurements will be closer to a normal distribution than that of individual measurements.Ideally a measurement device is both accurate and precise, with measurements all close to and tightly clustered around the true value. The accuracy and precision of a measurement process is usually established by repeatedly measuring some traceable reference standard. Such standards are defined in the International System of Units (abbreviated SI from French: Syst\u00e8me international d'unit\u00e9s) and maintained by national standards organizations such as the National Institute of Standards and Technology in the United States.In industrial instrumentation, accuracy is the measurement tolerance, or transmission of the instrument and defines the limits of the errors made when the instrument is used in normal operating conditions.[4]In military terms, accuracy refers primarily to the accuracy of fire (or \"justesse de tir\"), the precision of fire expressed by the closeness of a grouping of shots at and around the centre of the target.[3]In numerical analysis, accuracy is also the nearness of a calculation to the true value; while precision is the resolution of the representation, typically defined by the number of decimal or binary digits.In addition to accuracy and precision, measurements may also have a measurement resolution, which is the smallest change in the underlying physical quantity that produces a response in the measurement.The terminology is also applied to indirect measurements\u2014that is, values obtained by a computational procedure from observed data.A measurement system is considered valid if it is both accurate and precise. Related terms include bias (non-random or directed effects caused by a factor or factors unrelated to the independent variable) and error (random variability).A measurement system can be accurate but not precise, precise but not accurate, neither, or both. For example, if an experiment contains a systematic error, then increasing the sample size generally increases precision but does not improve accuracy. The result would be a consistent yet inaccurate string of results from the flawed experiment. Eliminating the systematic error improves accuracy but does not change precision.Interestingly, the field of statistics, where the interpretation of measurements plays a central role, prefers to use the terms bias and variability instead of accuracy and precision: bias is the amount of inaccuracy and variability is the amount of imprecision.In the fields of science and engineering, the accuracy of a measurement system is the degree of closeness of measurements of a quantity to that quantity's true value.[1] The precision of a measurement system, related to reproducibility and repeatability, is the degree to which repeated measurements under unchanged conditions show the same results.[1][2] Although the two words precision and accuracy can be synonymous in colloquial use, they are deliberately contrasted in the context of the scientific method.In simplest terms, given a set of data points from repeated measurements of the same quantity, the set can be said to be precise if the values are close to each other, while the set can be said to be accurate if their average is close to the true value of the quantity being measured. The two concepts are independent of each other, so a particular set of data can be said to be either accurate, or precise, or both, or neither.Accuracy has two definitions:Precision is a description of random errors, a measure of statistical variability.",
            "title": "Accuracy and precision",
            "url": "https://en.wikipedia.org/wiki/Accuracy"
        },
        {
            "desc_links": [
                "/wiki/Ill-conditioned",
                "/wiki/Approximation",
                "/wiki/Rounding",
                "/wiki/Quantization_error",
                "/wiki/Numerical_analysis",
                "/wiki/Error_analysis_(mathematics)",
                "/wiki/Equation",
                "/wiki/Algorithm"
            ],
            "links": [
                "/wiki/Loss_of_significance",
                "/wiki/Countable",
                "/wiki/Guard_digit",
                "/wiki/Ill-conditioned",
                "/wiki/Approximation",
                "/wiki/Rounding",
                "/wiki/Quantization_error",
                "/wiki/Numerical_analysis",
                "/wiki/Error_analysis_(mathematics)",
                "/wiki/Equation",
                "/wiki/Algorithm"
            ],
            "text": "Rounding multiple times can cause error to accumulate.[8] For example, if 9.945309 is rounded to two decimal places (9.95), then rounded again to one decimal place (10.0), the total error is 0.054691. Rounding 9.945309 to one decimal place (9.9) in a single step introduces less error (0.045309). This commonly occurs when performing arithmetic operations (See Loss of Significance).Increasing the number of digits allowed in a representation reduces the magnitude of possible round-off errors, but any representation limited to finitely many digits will still cause some degree of round-off error for uncountably many real numbers. Additional digits used for intermediary steps of a calculation are known as guard digits.[7]The error introduced by attempting to represent a number using a finite string of digits is a form of round-off error called representation error.[6] Here are some examples of representation error in decimal representations:When a sequence of calculations subject to rounding error is made, errors may accumulate, sometimes dominating the calculation. In ill-conditioned problems, significant error may accumulate.[5]A round-off error,[1] also called rounding error,[2] is the difference between the calculated approximation of a number and its exact mathematical value due to rounding. This is a form of quantization error.[3] One of the goals of numerical analysis is to estimate errors in calculations, including round-off error, when using approximation equations or algorithms, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits).[4]",
            "title": "Round-off error",
            "url": "https://en.wikipedia.org/wiki/Round-off_error"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Wilkinson's polynomial",
            "url": "https://en.wikipedia.org/wiki/Wilkinson%27s_polynomial"
        },
        {
            "desc_links": [
                "/wiki/Dynamical_systems",
                "/wiki/Numerical_methods",
                "/wiki/Pure_mathematics",
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Function_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Engineering",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Biology"
            ],
            "links": [
                "/wiki/Rate_equation",
                "/wiki/Chemical_reaction",
                "/wiki/Reaction_rate",
                "/wiki/Reaction_order",
                "/wiki/Mass_balance",
                "/wiki/Thermodynamics",
                "/wiki/Quantum_mechanics",
                "/wiki/Lotka%E2%80%93Volterra_equations",
                "/wiki/Non-linear",
                "/wiki/Population_dynamics",
                "/wiki/Schr%C3%B6dinger%27s_equation",
                "/wiki/Linear_differential_equation",
                "/wiki/Partial_differential_equation",
                "/wiki/Wave_function",
                "/wiki/Einstein_field_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Albert_Einstein",
                "/wiki/General_relativity",
                "/wiki/Fundamental_interaction",
                "/wiki/Gravitation",
                "/wiki/Spacetime",
                "/wiki/Curvature",
                "/wiki/Matter",
                "/wiki/Energy",
                "/wiki/Tensor_equation",
                "/wiki/Curvature",
                "/wiki/Einstein_tensor",
                "/wiki/Momentum",
                "/wiki/Stress%E2%80%93energy_tensor",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Lorentz_force",
                "/wiki/Classical_electrodynamics",
                "/wiki/Optics",
                "/wiki/Electric_circuit",
                "/wiki/Electric_field",
                "/wiki/Magnetic_field",
                "/wiki/Electric_charge",
                "/wiki/Electric_current",
                "/wiki/James_Clerk_Maxwell",
                "/wiki/Newton%27s_second_law",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Biology",
                "/wiki/Economics",
                "/wiki/Mathematical_modelling",
                "/wiki/Partial_differential_equation",
                "/wiki/Wave_equation",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_equation",
                "/wiki/Diffusion",
                "/wiki/Black%E2%80%93Scholes",
                "/wiki/Pure_mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Physics",
                "/wiki/Engineering",
                "/wiki/Closed-form_expression",
                "/wiki/Numerical_ordinary_differential_equations",
                "/wiki/Difference_equations",
                "/wiki/Algebraic_equations",
                "/wiki/Derivative#Higher_derivatives",
                "/wiki/Second_derivative",
                "/wiki/Thin-film_equation",
                "/wiki/Linearization",
                "/wiki/Symmetry",
                "/wiki/Chaos_theory",
                "/wiki/Navier%E2%80%93Stokes_existence_and_smoothness",
                "/wiki/Sound",
                "/wiki/Heat",
                "/wiki/Electrostatics",
                "/wiki/Electrodynamics",
                "/wiki/Fluid_flow",
                "/wiki/Elasticity_(physics)",
                "/wiki/Quantum_mechanics",
                "/wiki/Dynamical_systems",
                "/wiki/Multidimensional_systems",
                "/wiki/Stochastic_partial_differential_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Multivariable_calculus",
                "/wiki/Partial_derivatives",
                "/wiki/Ordinary_differential_equations",
                "/wiki/Computer_model",
                "/wiki/Closed-form_expression",
                "/wiki/Numerical_ordinary_differential_equations",
                "/wiki/Physics",
                "/wiki/Special_functions",
                "/wiki/Holonomic_function",
                "/wiki/Linear_differential_equation",
                "/wiki/Linear_equation",
                "/wiki/Antiderivative",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Function_of_a_real_variable",
                "/wiki/Variable_(mathematics)",
                "/wiki/Independent_variable",
                "/wiki/Partial_differential_equation",
                "/wiki/Equations_of_motion",
                "/wiki/Classical_mechanics",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_flow",
                "/wiki/Newton%27s_law_of_cooling",
                "/wiki/Heat_equation",
                "/wiki/Mechanics",
                "/wiki/Lagrangian_mechanics",
                "/wiki/Euler%E2%80%93Lagrange_equation",
                "/wiki/Tautochrone",
                "/wiki/Musical_instrument",
                "/wiki/Jean_le_Rond_d%27Alembert",
                "/wiki/Leonhard_Euler",
                "/wiki/Daniel_Bernoulli",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Jacob_Bernoulli",
                "/wiki/Bernoulli_differential_equation",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Calculus",
                "/wiki/Isaac_Newton",
                "/wiki/Leibniz",
                "/wiki/Method_of_Fluxions",
                "/wiki/Dynamical_systems",
                "/wiki/Numerical_methods",
                "/wiki/Pure_mathematics",
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Function_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Engineering",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Biology"
            ],
            "text": "The rate law or rate equation for a chemical reaction is a differential equation that links the reaction rate with concentrations or pressures of reactants and constant parameters (normally rate coefficients and partial reaction orders).[18] To determine the rate equation for a particular system one combines the reaction rate with a mass balance for the system.[19] In addition, a range of differential equations are present in the study of thermodynamics and quantum mechanics.The Lotka\u2013Volterra equations, also known as the predator\u2013prey equations, are a pair of first-order, non-linear, differential equations frequently used to describe the population dynamics of two species that interact, one as a predator and the other as prey.In quantum mechanics, the analogue of Newton's law is Schr\u00f6dinger's equation (a partial differential equation) for a quantum system (usually atoms, molecules, and subatomic particles whether free, bound, or localized). It is not a simple algebraic equation, but in general a linear partial differential equation, describing the time-evolution of the system's wave function (also called a \"state function\").[17]The Einstein field equations (EFE; also known as \"Einstein's equations\") are a set of ten partial differential equations in Albert Einstein's general theory of relativity which describe the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy.[14] First published by Einstein in 1915[15] as a tensor equation, the EFE equate local spacetime curvature (expressed by the Einstein tensor) with the local energy and momentum within that spacetime (expressed by the stress\u2013energy tensor).[16]Maxwell's equations are a set of partial differential equations that, together with the Lorentz force law, form the foundation of classical electrodynamics, classical optics, and electric circuits. These fields in turn underlie modern electrical and communications technologies. Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. They are named after the Scottish physicist and mathematician James Clerk Maxwell, who published an early form of those equations between 1861 and 1862.So long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the equation of motion.Many fundamental laws of physics and chemistry can be formulated as differential equations. In biology and economics, differential equations are used to model the behavior of complex systems. The mathematical theory of differential equations first developed together with the sciences where the equations had originated and where the results found application. However, diverse problems, sometimes originating in quite distinct scientific fields, may give rise to identical differential equations. Whenever this happens, mathematical theory behind the equations can be viewed as a unifying principle behind diverse phenomena. As an example, consider the propagation of light and sound in the atmosphere, and of waves on the surface of a pond. All of them may be described by the same second-order partial differential equation, the wave equation, which allows us to think of light and sound as forms of waves, much like familiar waves in the water. Conduction of heat, the theory of which was developed by Joseph Fourier, is governed by another second-order partial differential equation, the heat equation. It turns out that many diffusion processes, while seemingly different, are described by the same equation; the Black\u2013Scholes equation in finance is, for instance, related to the heat equation.The study of differential equations is a wide field in pure and applied mathematics, physics, and engineering. All of these disciplines are concerned with the properties of differential equations of various types. Pure mathematics focuses on the existence and uniqueness of solutions, while applied mathematics emphasizes the rigorous justification of the methods for approximating solutions. Differential equations play an important role in modelling virtually every physical, technical, or biological process, from celestial motion, to bridge design, to interactions between neurons. Differential equations such as those used to solve real-life problems may not necessarily be directly solvable, i.e. do not have closed form solutions. Instead, solutions can be approximated using numerical methods.The theory of differential equations is closely related to the theory of difference equations, in which the coordinates assume only discrete values, and the relationship involves values of the unknown function or functions and values at nearby coordinates. Many methods to compute numerical solutions of differential equations or study the properties of differential equations involve the approximation of the solution of a differential equation by the solution of a corresponding difference equation.such thatHowever, this only helps us with first order initial value problems. Suppose we had a linear initial value problem of the nth order:Solving differential equations is not like solving algebraic equations. Not only are their solutions often unclear, but whether solutions are unique or exist at all are also notable subjects of interest.In the next group of examples, the unknown function u depends on two variables x and t or x and y.In the first group of examples, let u be an unknown function of x, and let c & \u03c9 be known constants. Note both ordinary and partial differential equations are broadly classified as linear and nonlinear.Differential equations are described by their order, determined by the term with the highest derivatives. An equation containing only first derivatives is a first-order differential equation, an equation containing the second derivative is a second-order differential equation, and so on.[11][12] Differential equations that describe natural phenomena almost always have only first and second order derivatives in them, but there are some exceptions, such as the thin film equation, which is a fourth order partial differential equation.Linear differential equations frequently appear as approximations to nonlinear equations. These approximations are only valid under restricted conditions. For example, the harmonic oscillator equation is an approximation to the nonlinear pendulum equation that is valid for small amplitude oscillations (see below).Non-linear differential equations are formed by the products of the unknown function and its derivatives are allowed and its degree is > 1. There are very few methods of solving nonlinear differential equations exactly; those that are known typically depend on the equation having particular symmetries. Nonlinear differential equations can exhibit very complicated behavior over extended time intervals, characteristic of chaos. Even the fundamental questions of existence, uniqueness, and extendability of solutions for nonlinear differential equations, and well-posedness of initial and boundary value problems for nonlinear PDEs are hard problems and their resolution in special cases is considered to be a significant advance in the mathematical theory (cf. Navier\u2013Stokes existence and smoothness). However, if the differential equation is a correctly formulated representation of a meaningful physical process, then one expects it to have a solution.[10]PDEs can be used to describe a wide variety of phenomena in nature such as sound, heat, electrostatics, electrodynamics, fluid flow, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.A partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. (This is in contrast to ordinary differential equations, which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved in closed form, or used to create a relevant computer model.As, in general, the solutions of a differential equation cannot be expressed by a closed-form expression, numerical methods are commonly used for solving differential equations on a computer.Most ODEs that are encountered in physics are linear, and, therefore, most special functions may be defined as solutions of linear differential equations (see Holonomic function).Linear differential equations are the differential equations that are linear in the unknown function and its derivatives. Their theory is well developed, and, in many cases, one may express their solutions in terms of integrals.An ordinary differential equation (ODE) is an equation containing an unknown function of one real or complex variable x, its derivatives, and some given functions of x. The unknown function is generally represented by a variable (often denoted y), which, therefore, depends on x. Thus x is often called the independent variable of the equation. The term \"ordinary\" is used in contrast with the term partial differential equation, which may be with respect to more than one independent variable.Differential equations can be divided into several types. Apart from describing the properties of the equation itself, these classes of differential equations can help inform the choice of approach to a solution. Commonly used distinctions include whether the equation is: Ordinary/Partial, Linear/Non-linear, and Homogeneous/Inhomogeneous. This list is far from exhaustive; there are many other properties and subclasses of differential equations which can be very useful in specific contexts.An example of modelling a real world problem using differential equations is the determination of the velocity of a ball falling through the air, considering only gravity and air resistance. The ball's acceleration towards the ground is the acceleration due to gravity minus the acceleration due to air resistance. Gravity is considered constant, and air resistance may be modeled as proportional to the ball's velocity. This means that the ball's acceleration, which is a derivative of its velocity, depends on the velocity (and the velocity depends on time). Finding the velocity as a function of time involves solving a differential equation and verifying its validity.In some cases, this differential equation (called an equation of motion) may be solved explicitly.For example, in classical mechanics, the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow (given the position, velocity, acceleration and various forces acting on the body) one to express these variables dynamically as a differential equation for the unknown position of the body as a function of time.Fourier published his work on heat flow in Th\u00e9orie analytique de la chaleur (The Analytic Theory of Heat),[9] in which he based his reasoning on Newton's law of cooling, namely, that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures. Contained in this book was Fourier's proposal of his heat equation for conductive diffusion of heat. This partial differential equation is now taught to every student of mathematical physics.Lagrange solved this problem in 1755 and sent the solution to Euler. Both further developed Lagrange's method and applied it to mechanics, which led to the formulation of Lagrangian mechanics.The Euler\u2013Lagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the tautochrone problem. This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point.Historically, the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, and Joseph-Louis Lagrange.[4][5][6][7] In 1746, d\u2019Alembert discovered the one-dimensional wave equation, and within ten years Euler discovered the three-dimensional wave equation.[8]for which the following year Leibniz obtained solutions by simplifying it.[3]Jacob Bernoulli proposed the Bernoulli differential equation in 1695.[2] This is an ordinary differential equation of the formHe solves these examples and others using infinite series and discusses the non-uniqueness of solutions.Differential equations first came into existence with the invention of calculus by Newton and Leibniz. In Chapter 2 of his 1671 work \"Methodus fluxionum et Serierum Infinitarum\",[1] Isaac Newton listed three kinds of differential equations:If a self-contained formula for the solution is not available, the solution may be numerically approximated using computers. The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy.In pure mathematics, differential equations are studied from several different perspectives, mostly concerned with their solutions\u2014the set of functions that satisfy the equation. Only the simplest differential equations are solvable by explicit formulas; however, some properties of solutions of a given differential equation may be determined without finding their exact form.A differential equation is a mathematical equation that relates some function with its derivatives. In applications, the functions usually represent physical quantities, the derivatives represent their rates of change, and the equation defines a relationship between the two. Because such relations are extremely common, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology.",
            "title": "Differential equation",
            "url": "https://en.wikipedia.org/wiki/Differential_equation"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Recursion",
                "/wiki/Sequence",
                "/wiki/Function_(mathematics)"
            ],
            "links": [
                "/wiki/Exogeny",
                "/wiki/Endogeneity_(economics)",
                "/wiki/Time_series_analysis",
                "/wiki/Comb_filter",
                "/wiki/Digital_signal_processing",
                "/wiki/Infinite_impulse_response",
                "/wiki/Digital_filter",
                "/wiki/Binary_search_algorithm",
                "/wiki/Analysis_of_algorithms",
                "/wiki/Algorithm",
                "/wiki/Divide_and_conquer_algorithm",
                "/wiki/Integrodifference_equation",
                "/wiki/Ecology",
                "/wiki/Voltinism",
                "/wiki/Logistic_map",
                "/wiki/Population_dynamics",
                "/wiki/Parasite",
                "/wiki/Population",
                "/wiki/Fibonacci_number",
                "/wiki/Discretization",
                "/wiki/Euler%27s_method",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Numerical_ordinary_differential_equations",
                "/wiki/Initial_value_problem",
                "/wiki/Chaos_theory",
                "/wiki/Logistic_map",
                "/wiki/Dyadic_transformation",
                "/wiki/Tent_map",
                "/wiki/Stability_theory",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Unity_(mathematics)",
                "/wiki/Absolute_value",
                "/wiki/Stability_theory",
                "/wiki/Eigenvalues",
                "/wiki/Unity_(mathematics)",
                "/wiki/Characteristic_polynomial",
                "/wiki/Confluent_hypergeometric_series",
                "/wiki/Bessel_function",
                "/wiki/Generalized_hypergeometric_series",
                "/wiki/Orthogonal_polynomials",
                "/wiki/Special_function",
                "/wiki/Method_of_undetermined_coefficients",
                "/wiki/Z-transform#Linear_constant-coefficient_difference_equation",
                "/wiki/Z-transform",
                "/wiki/Integral_transform",
                "/wiki/Taylor_series",
                "/wiki/Differential_equations",
                "/wiki/Ansatz",
                "/wiki/Multiplicity_(mathematics)",
                "/wiki/Characteristic_polynomial",
                "/wiki/Steady_state",
                "/wiki/Homogeneous_differential_equation",
                "/wiki/Stability_theory",
                "/wiki/Absolute_value",
                "/wiki/Characteristic_root",
                "/wiki/Ansatz",
                "/wiki/Generating_function",
                "/wiki/Formal_power_series",
                "/wiki/Characteristic_polynomial",
                "/wiki/Fibonacci_number",
                "/wiki/Lucas_number",
                "/wiki/Lucas_sequence",
                "/wiki/Jacobsthal_number",
                "/wiki/Pell_number",
                "/wiki/Pell%27s_equation",
                "/wiki/Characteristic_polynomial",
                "/wiki/Summation_equation",
                "/wiki/Integral_equation",
                "/wiki/Time_scale_calculus",
                "/wiki/Differential_equations",
                "/wiki/Ordinary_differential_equations",
                "/wiki/Ackermann_number",
                "/wiki/Binomial_transform",
                "/wiki/Rational_difference_equation",
                "/wiki/Matrix_difference_equation",
                "/wiki/Binet%27s_formula",
                "/wiki/Generating_function",
                "/wiki/Rational_function",
                "/wiki/Initial_condition",
                "/wiki/Fibonacci_number",
                "/wiki/Linear_recurrence",
                "/wiki/Closed-form_solution",
                "/wiki/Chaos_theory",
                "/wiki/Nonlinearity",
                "/wiki/Logistic_map",
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Recursion",
                "/wiki/Sequence",
                "/wiki/Function_(mathematics)"
            ],
            "text": "Recurrence relations, especially linear recurrence relations, are used extensively in both theoretical and empirical economics.[9][10] In particular, in macroeconomics one might develop a model of various broad sectors of the economy (the financial sector, the goods sector, the labor market, etc.) in which some agents' actions depend on lagged variables. The model would then be solved for current values of key variables (interest rate, real GDP, etc.) in terms of exogenous variables and lagged endogenous variables. See also time series analysis.etc.For example, the equation for a \"feedforward\" IIR comb filter of delay T is:In digital signal processing, recurrence relations can model feedback in a system, where outputs at one time become inputs for future time. They thus arise in infinite impulse response (IIR) digital filters.A better algorithm is called binary search. However, it requires a sorted vector. It will first check if the element is at the middle of the vector. If not, then it will check if the middle element is greater or lesser than the sought element. At this point, half of the vector can be discarded, and the algorithm can be run again on the other half. The number of comparisons will be given byRecurrence relations are also of fundamental importance in analysis of algorithms.[7][8] If an algorithm is designed so that it will break a problem into smaller subproblems (divide and conquer), its running time is described by a recurrence relation.Integrodifference equations are a form of recurrence relation important to spatial ecology. These and other difference equations are particularly suited to modeling univoltine populations.with Nt representing the hosts, and Pt the parasites, at time\u00a0t.The logistic map is used either directly to model population growth, or as a starting point for more detailed models of population dynamics. In this context, coupled difference equations are often used to model the interaction of two or more populations. For example, the Nicholson-Bailey model for a host-parasite interaction is given bySome of the best-known difference equations have their origins in the attempt to model population dynamics. For example, the Fibonacci numbers were once used as a model for the growth of a rabbit population.Systems of linear first order differential equations can be discretized exactly analytically using the methods shown in the discretization article.by the recurrencewith Euler's method and a step size h, one calculates the valuesWhen solving an ordinary differential equation numerically, one typically encounters a recurrence relation. For example, when solving the initial value problemIn a chaotic recurrence relation, the variable x stays in a bounded region but never converges to a fixed point or an attracting cycle; any fixed points or cycles of the equation are unstable. See also logistic map, dyadic transformation, and tent map.where x* is any point on the cycle.with f appearing k times is locally stable according to the same criterion:A nonlinear recurrence relation could also have a cycle of period k for k > 1. Such a cycle is stable, meaning that it attracts a set of initial conditions of positive measure, if the composite functionA nonlinear recurrence could have multiple fixed points, in which case some fixed points may be locally stable and others locally unstable; for continuous f two adjacent fixed points cannot both be locally stable.This recurrence is locally stable, meaning that it converges to a fixed point x* from points sufficiently close to x*, if the slope of f in the neighborhood of x* is smaller than unity in absolute value: that is,Consider the nonlinear first-order recurrencewith state vector x and transition matrix A, x converges asymptotically to the steady state vector x* if and only if all eigenvalues of the transition matrix A (whether real or complex) have an absolute value which is less than\u00a01.In the first-order matrix difference equationThe recurrence is stable, meaning that the iterates converge asymptotically to a fixed value, if and only if the eigenvalues (i.e., the roots of the characteristic equation), whether real or complex, are all less than unity in absolute value.has the characteristic equationThe linear recurrence of order d,the confluent hypergeometric series.is solved bythe Bessel function, whileis given byMany homogeneous linear recurrence relations may be solved by means of the generalized hypergeometric series. Special cases of these lead to recurrence relations for the orthogonal polynomials, and many special functions. For example, the solution toThenLetthere is also a nice method to solve it:[6]Moreover, for the general first-order non-homogeneous linear recurrence relation with variable coefficients:with constant coefficients ci is derived fromof the non-homogeneous recurrenceis the generating function of the inhomogeneity, the generating functionIfThis is a homogeneous recurrence, which can be solved by the methods explained above. In general, if a linear recurrence has the formor equivalentlySubtracting the original recurrence from this equation yieldsThis is a non-homogeneous recurrence. If we substitute n \u21a6 n+1, we obtain the recurrenceIf the recurrence is non-homogeneous, a particular solution can be found by the method of undetermined coefficients and the solution is the sum of the solution of the homogeneous and the particular solutions. Another method to solve a non-homogeneous recurrence is the method of symbolic differentiation. For example, consider the following recurrence:Certain difference equations - in particular, linear constant coefficient difference equations - can be solved using z-transforms. The z-transforms are a class of integral transforms that lead to more convenient algebraic manipulations and more straightforward solutions. There are cases in which obtaining a direct solution would be all but impossible, yet solving the problem via a thoughtfully chosen integral transform is straightforward.where there are several linked recurrences.[5]This description is really no different from general method above, however it is more succinct. It also works nicely for situations likeSolving for coefficients,is identical toA linearly recursive sequence y of order nIt is easy to see that the nth derivative of eax evaluated at 0 is anThe conversion of the differential equation to a difference equation of the Taylor coefficients ishas solutionExample: The differential equationThis example shows how problems generally solved using the power series solution method taught in normal differential equation classes can be solved in a much easier way.oris given byExample: The recurrence relationship for the Taylor series coefficients of the equation:and more generallyThe rule of thumb (for equations in which the polynomial multiplying the first term is non-zero at zero) is that:This equivalence can be used to quickly solve for the recurrence relationship for the coefficients in the power series solution of a linear differential equation.it can be seen that the coefficients of the series are given by the nth derivative of f(x) evaluated at the point a. The differential equation provides a linear difference equation relating these coefficients.This is not a coincidence. Considering the Taylor series of the solution to a linear differential equation:The method for solving linear differential equations is similar to the method above\u2014the \"intelligent guess\" (ansatz) for linear differential equations with constant coefficients is e\u03bbx where \u03bb is a complex number that is determined by substituting the guess into the differential equation.As a result of this theorem a homogeneous linear recurrence relation with constant coefficients can be solved in the following manner:such that each ci corresponds to each ci in the original recurrence relation (see the general form above). Suppose \u03bb is a root of p(t) having multiplicity r. This is to say that (t\u2212\u03bb)r divides p(t). The following two properties hold:Given a homogeneous linear recurrence relation with constant coefficients of order d, let p(t) be the characteristic polynomial (also \"auxiliary polynomial\")The stability condition stated above in terms of eigenvalues for the second-order case remains valid for the general nth-order case: the equation is stable if and only if all eigenvalues of the characteristic equation are less than one in absolute value.which can be solved as above.Then the non-homogeneous recurrence can be rewritten in homogeneous form aswith constant term K, this can be converted into homogeneous form as follows: The steady state is found by setting bn =\u00a0bn\u22121 =\u00a0bn\u22122 =\u00a0b* to obtainThe equation in the above example was homogeneous, in that there was no constant term. If one starts with the non-homogeneous recurrenceIn all cases\u2014real distinct eigenvalues, real duplicated eigenvalues, and complex conjugate eigenvalues\u2014the equation is stable (that is, the variable a converges to a fixed value [specifically, zero]) if and only if both eigenvalues are smaller than one in absolute value. In this second-order case, this condition on the eigenvalues can be shown[4] to be equivalent to |A|\u00a0<\u00a01\u00a0\u2212\u00a0B\u00a0<\u00a02, which is equivalent to |B|\u00a0<\u00a01 and |A|\u00a0<\u00a01\u00a0\u2212\u00a0B.In this way there is no need to solve for \u03bb1 and \u03bb2.where a1 and a2 are the initial conditions andone may simplify the solution given above asHere E and F (or equivalently, G and \u03b4) are real constants which depend on the initial conditions. Usingwherecan be rewritten as[3]:576\u2013585This is the most general solution; the two constants C and D can be chosen based on two given initial conditions a0 and a1 to produce a specific solution.while if they are identical (when A2\u00a0+\u00a04B\u00a0=\u00a00), we havewhich is the characteristic equation of the recurrence relation. Solve for r to obtain the two roots \u03bb1, \u03bb2: these roots are known as the characteristic roots or eigenvalues of the characteristic equation. Different solutions are obtained depending on the nature of the roots: If these roots are distinct, we have the general solutionDividing through by rn\u22122, we get that all these equations reduce to the same thing:must be true for all\u00a0n\u00a0>\u00a01.When does it have a solution of the same general form as an = rn? Substituting this guess (ansatz) in the recurrence relation, we find thatConsider, for example, a recurrence relation of the formSolutions to such recurrence relations of higher order are found by systematic means, often using the fact that an\u00a0=\u00a0rn is a solution for the recurrence exactly when t\u00a0=\u00a0r is a root of the characteristic polynomial. This can be approached directly or using generating functions (formal power series) or matrices.has the solution an\u00a0=\u00a0rn with a0\u00a0=\u00a01 and the most general solution is an\u00a0=\u00a0krn with a0\u00a0=\u00a0k. The characteristic polynomial equated to zero (the characteristic equation) is simply t\u00a0\u2212\u00a0r\u00a0=\u00a00.For order 1, the recurrenceAs well as the Fibonacci numbers, other constant-recursive sequences include the Lucas numbers and Lucas sequences, the Jacobsthal numbers, the Pell numbers and more generally the solutions to Pell's equation.where the coefficients ki are determined in order to fit the initial conditions of the recurrence. When the same roots occur multiple times, the terms in this formula corresponding to the second and later occurrences of the same root are multiplied by increasing powers of n. For instance, if the characteristic polynomial can be factored as (x\u2212r)3, with the same root r occurring three times, then the solution would take the formwhose d roots play a crucial role in finding and understanding the sequences satisfying the recurrence. If the roots r1, r2, ... are all distinct, then each solution to the recurrence takes the formThe same coefficients yield the characteristic polynomial (also \"auxiliary polynomial\")An order-d homogeneous linear recurrence with constant coefficients is an equation of the formSingle-variable or one-dimensional recurrence relations are about sequences (i.e. functions defined on one-dimensional grids). Multi-variable or n-dimensional recurrence relations are about n-dimensional grids. Functions defined on n-grids can also be studied with partial difference equations.[1]Summation equations relate to difference equations as integral equations relate to differential equations.See time scale calculus for a unification of the theory of difference equations with that of differential equations.Thus one can solve many recurrence relations by rephrasing them as difference equations, and then solving the difference equation, analogously to how one solves ordinary differential equations. However, the Ackermann numbers are an example of a recurrence relation that do not map to a difference equation, much less points on the solution to a differential equation.is equivalent to the recurrence relationSince difference equations are a very common form of recurrence, some authors use the two terms interchangeably. For example, the difference equationThus, a difference equation can be defined as an equation that involves an, an-1, an-2 etc. (or equivalently an, an+1, an+2 etc.)Actually, it is easily seen that,(The sequence and its differences are related by a binomial transform.) The more restrictive definition of difference equation is an equation composed of an and its kth differences. (A widely used broader definition treats \"difference equation\" as synonymous with \"recurrence relation\". See for example rational difference equation and matrix difference equation.)which can be simplified toThe recurrence can be solved by methods described below yielding Binet's formula, which involves powers of the two roots of the characteristic polynomial t2\u00a0=\u00a0t\u00a0+\u00a01; the generating function of the sequence is the rational functionWe obtain the sequence of Fibonacci numbers, which beginsetc.Explicitly, the recurrence yields the equationswith initial conditions (seed values)The recurrence satisfied by the Fibonacci numbers is the archetype of a homogeneous linear recurrence relation with constant coefficients (see below). The Fibonacci sequence is defined using the recurrenceSolving a recurrence relation means obtaining a closed-form solution: a non-recursive function of n.Some simply defined recurrence relations can have very complex (chaotic) behaviours, and they are a part of the field of mathematics known as nonlinear analysis.with a given constant r; given the initial term x0 each subsequent term is determined by this relation.An example of a recurrence relation is the logistic map:The term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation. However, \"difference equation\" is frequently used to refer to any recurrence relation.In mathematics, a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given: each further term of the sequence or array is defined as a function of the preceding terms.",
            "title": "Recurrence relation",
            "url": "https://en.wikipedia.org/wiki/Difference_equation"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Representation_theory",
                "/wiki/Algebra_over_a_field",
                "/wiki/Algebra_homomorphism",
                "/wiki/Representation_(mathematics)",
                "/wiki/Multiplicative_character",
                "/wiki/Group_(mathematics)",
                "/wiki/Lie_algebra_representation",
                "/wiki/Lie_algebra",
                "/wiki/Group_representation",
                "/wiki/Algebraic_group",
                "/wiki/Lie_group",
                "/wiki/Eigenvalue",
                "/wiki/Eigenspace"
            ],
            "links": [
                "/wiki/Verma_module",
                "/wiki/Eigenbasis",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Lie_group",
                "/wiki/Algebraic_group",
                "/wiki/Lie_algebra",
                "/wiki/Commutator",
                "/wiki/Derived_algebra",
                "/wiki/Abelian_Lie_algebra",
                "/wiki/Algebra_representation",
                "/wiki/Algebra_homomorphism",
                "/wiki/Algebra_over_a_field",
                "/wiki/Linear_map",
                "/wiki/Group_representation",
                "/wiki/Multiplicative_character",
                "/wiki/Group_theory",
                "/wiki/Group_(mathematics)",
                "/wiki/Multiplicative_group",
                "/wiki/Field_(mathematics)",
                "/wiki/Identity_element",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Commuting_matrices",
                "/wiki/Simultaneously_diagonalize",
                "/wiki/Semisimple_operator",
                "/wiki/Linear_transformation",
                "/wiki/Vector_space",
                "/wiki/Eigenvector",
                "/wiki/Linear_functional",
                "/wiki/Mathematics",
                "/wiki/Representation_theory",
                "/wiki/Algebra_over_a_field",
                "/wiki/Algebra_homomorphism",
                "/wiki/Representation_(mathematics)",
                "/wiki/Multiplicative_character",
                "/wiki/Group_(mathematics)",
                "/wiki/Lie_algebra_representation",
                "/wiki/Lie_algebra",
                "/wiki/Group_representation",
                "/wiki/Algebraic_group",
                "/wiki/Lie_group",
                "/wiki/Eigenvalue",
                "/wiki/Eigenspace"
            ],
            "text": "Every finite-dimensional highest weight module is irreducible.[8]The last point is the most difficult one; the representations may be constructed using Verma modules.then it is called a weight module; this corresponds to there being a common eigenbasis (a basis of simultaneous eigenvectors) for all the represented elements of the algebra, i.e., to their being simultaneously diagonalizable matrices (see diagonalizable matrix).If V is the direct sum of its weight spacesIf G is a Lie group or an algebraic group, then a multiplicative character \u03b8: G \u2192 F\u00d7 induces a weight \u03c7 = d\u03b8: g \u2192 F on its Lie algebra by differentiation. (For Lie groups, this is differentiation at the identity element of G, and the algebraic group case is an abstraction using the notion of a derivation.)If A is a Lie algebra (which is generally not an associative algebra), then instead of requiring multiplicativity of a character, one requires that it maps any Lie bracket to the corresponding commutator; but since F is commutative this simply means that this map must vanish on Lie brackets: \u03c7([a,b])=0. A weight on a Lie algebra g over a field F is a linear map \u03bb: g \u2192 F with \u03bb([x, y])=0 for all x, y in g. Any weight on a Lie algebra g vanishes on the derived algebra [g,g] and hence descends to a weight on the abelian Lie algebra g/[g,g]. Thus weights are primarily of interest for abelian Lie algebras, where they reduce to the simple notion of a generalized eigenvalue for space of commuting linear transformations.for all a, b in A. If an algebra A acts on a vector space V over F to any simultaneous eigenspace, this corresponds an algebra homomorphism from A to F assigning to each element of A its eigenvalue.The notion of multiplicative character can be extended to any algebra A over F, by replacing \u03c7: G \u2192 F\u00d7 by a linear map \u03c7: A \u2192 F with:Indeed, if G acts on a vector space V over F, each simultaneous eigenspace for every element of G, if such exists, determines a multiplicative character on G: the eigenvalue on this common eigenspace of each element of the group.The notion is closely related to the idea of a multiplicative character in group theory, which is a homomorphism \u03c7 from a group G to the multiplicative group of a field F. Thus \u03c7: G \u2192 F\u00d7 satisfies \u03c7(e) = 1 (where e is the identity element of G) andGiven a set S of matrices, each of which is diagonalizable, and any two of which commute, it is always possible to simultaneously diagonalize all of the elements of S.[note 1][note 2] Equivalently, for any set S of mutually commuting semisimple linear transformations of a finite-dimensional vector space V there exists a basis of V consisting of simultaneous eigenvectors of all elements of S. Each of these common eigenvectors v \u2208 V defines a linear functional on the subalgebra U of End(V) generated by the set of endomorphisms S; this functional is defined as the map which associates to each element of U its eigenvalue on the eigenvector v. This map is also multiplicative, and sends the identity to 1; thus it is an algebra homomorphism from U to the base field. This \"generalized eigenvalue\" is a prototype for the notion of a weight.In the mathematical field of representation theory, a weight of an algebra A over a field F is an algebra homomorphism from A to F, or equivalently, a one-dimensional representation of A over F. It is the algebra analogue of a multiplicative character of a group. The importance of the concept, however, stems from its application to representations of Lie algebras and hence also to representations of algebraic and Lie groups. In this context, a weight of a representation is a generalization of the notion of an eigenvalue, and the corresponding eigenspace is called a weight space.",
            "title": "Weight (representation theory)",
            "url": "https://en.wikipedia.org/wiki/Weight_(representation_theory)"
        },
        {
            "desc_links": [
                "/wiki/Abstract_algebra",
                "/wiki/Associative_algebra",
                "/wiki/Module_(mathematics)",
                "/wiki/Unital_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Adjoint_functors"
            ],
            "links": [
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Noncommutative_geometry",
                "/wiki/Triangular_matrix#Simultaneous_triangularisability",
                "/wiki/Abstract_algebra",
                "/wiki/Associative_algebra",
                "/wiki/Module_(mathematics)",
                "/wiki/Unital_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Adjoint_functors"
            ],
            "text": "Eigenvalues and eigenvectors can be generalized to algebra representations.In some approaches to noncommutative geometry, the free noncommutative algebra (polynomials in non-commuting variables) plays a similar role, but the analysis is much more difficult.A basic result about such representations is that, over an algebraically closed field, the representing matrices are simultaneously triangularisable.In abstract algebra, a representation of an associative algebra is a module for that algebra. Here an associative algebra is a (not necessarily unital) ring. If the algebra is not unital, it may be made so in a standard way (see the adjoint functors page); there is no essential difference between modules for the resulting unital ring, in which the identity acts by the identity mapping, and representations of the algebra.",
            "title": "Algebra representation",
            "url": "https://en.wikipedia.org/wiki/Algebra_representation"
        },
        {
            "desc_links": [
                "/wiki/Commutative_ring",
                "/wiki/Module_(mathematics)",
                "/wiki/Center_(ring_theory)",
                "/wiki/Unital_algebra",
                "/wiki/Mathematics",
                "/wiki/Algebraic_structure",
                "/wiki/Associative_property",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Algebra_over_a_field",
                "/wiki/Square_matrix",
                "/wiki/Matrix_multiplication"
            ],
            "links": [
                "/wiki/Limit_of_a_function",
                "/wiki/Lie_algebra",
                "/wiki/Comultiplication",
                "/wiki/Bialgebra",
                "/wiki/Hopf_algebra",
                "/wiki/Representation_theory",
                "/wiki/F-coalgebra",
                "/wiki/Functor",
                "/wiki/Morphism",
                "/wiki/Tensor_product#Characterization_by_a_universal_property",
                "/wiki/Categorial_duality",
                "/wiki/Commutative_diagram",
                "/wiki/Axiom",
                "/wiki/Coalgebra",
                "/wiki/Subcategory",
                "/wiki/Coslice_category",
                "/wiki/Category_of_commutative_rings",
                "/wiki/Category_(mathematics)",
                "/wiki/Noncommutative_algebraic_geometry",
                "/wiki/Derived_algebraic_geometry",
                "/wiki/Generic_matrix_ring",
                "/wiki/Structure_map",
                "/wiki/Coslice_category",
                "/wiki/Prime_spectrum",
                "/wiki/Affine_scheme",
                "/wiki/Tensor_product_of_modules",
                "/wiki/Monoid_(category_theory)",
                "/wiki/Category_of_modules",
                "/wiki/Monoidal_category",
                "/wiki/Category_of_abelian_groups",
                "/wiki/Non-associative_algebra",
                "/wiki/Bilinear_map",
                "/wiki/Commutative_ring",
                "/wiki/Abelian_group",
                "/wiki/Ring_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Scalar_multiplication",
                "/wiki/Commutative_ring",
                "/wiki/Module_(mathematics)",
                "/wiki/Center_(ring_theory)",
                "/wiki/Unital_algebra",
                "/wiki/Mathematics",
                "/wiki/Algebraic_structure",
                "/wiki/Associative_property",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Algebra_over_a_field",
                "/wiki/Square_matrix",
                "/wiki/Matrix_multiplication"
            ],
            "text": "An example of a non-unital associative algebra is given by the set of all functions f: R \u2192 R whose limit as x nears infinity is zero.Some authors use the term \"associative algebra\" to refer to structures which do not necessarily have a multiplicative identity, and hence consider homomorphisms which are not necessarily unital.This shows that this definition of a tensor product is too naive; the obvious fix is to define it such that it is antisymmetric, so that the middle two terms cancel. This leads to the concept of a Lie algebra.But, in general, this does not equalThis map is clearly linear in x, and so it does not have the problem of the earlier definition. However, it fails to preserve multiplication:so that the action on the tensor product space is given byOne can try to be more clever in defining a tensor product. Consider, for example,Such a homomorphism \u0394 is called a comultiplication if it satisfies certain axioms. The resulting structure is called a bialgebra. To be consistent with the definitions of the associative algebra, the coalgebra must be co-associative, and, if the algebra is unital, then the co-algebra must be co-unital as well. A Hopf algebra is a bialgebra with an additional piece of structure (the so-called antipode), which allows not only to define the tensor product of two representations, but also the Hom module of two representations (again, similarly to how it is done in the representation theory of groups).for k \u2208 K. One can rescue this attempt and restore linearity by imposing additional structure, by defining an algebra homomorphism \u0394: A \u2192 A \u2297 A, and defining the tensor product representation asHowever, such a map would not be linear, since one would haveA representation of an algebra A is an algebra homomorphism \u03c1: A \u2192 End(V) from A to the endomorphism algebra of some vector space (or module) V. The property of \u03c1 being an algebra homomorphism means that \u03c1 preserves the multiplicative operation (that is, \u03c1(xy)=\u03c1(x)\u03c1(y) for all x and y in A), and that \u03c1 sends the unity of A to the unity of End(V) (that is, to the identity endomorphism of V).There is also an abstract notion of F-coalgebra, where F is a functor. This is vaguely related to the notion of coalgebra discussed above.An associative algebra over K is given by a K-vector space A endowed with a bilinear map A\u00d7A\u2192A having 2 inputs (multiplicator and multiplicand) and one output (product), as well as a morphism K\u2192A identifying the scalar multiples of the multiplicative identity. If the bilinear map A\u00d7A\u2192A is reinterpreted as a linear map (i. e., morphism in the category of K-vector spaces) A\u2297A\u2192A (by the universal property of the tensor product), then we can view an associative algebra over K as a K-vector space A endowed with two morphisms (one of the form A\u2297A\u2192A and one of the form K\u2192A) satisfying certain conditions which boil down to the algebra axioms. These two morphisms can be dualized using categorial duality by reversing all arrows in the commutative diagrams which describe the algebra axioms; this defines the structure of a coalgebra.Geometry and combinatoricsAnalysisRepresentation theoryAlgebraThe most basic example is a ring itself; it is an algebra over its center or any subring lying in the center. In particular, any commutative ring is an algebra over any of its subrings. Other examples abound both from algebra and other fields of mathematics.The subcategory of commutative R-algebras can be characterized as the coslice category R/CRing where CRing is the category of commutative rings.The class of all R-algebras together with algebra homomorphisms between them form a category, sometimes denoted R-Alg.How to weaken the commutativity assumption is a subject matter of noncommutative algebraic geometry and, more recently, of derived algebraic geometry. See also: generic matrix ring.The ring homomorphism \u03b7 appearing in the above is often called a structure map. In the commutative case, one can consider the category whose objects are ring homomorphisms R \u2192 A; i.e., commutative R-algebras and whose morphisms are ring homomorphisms A \u2192 A' that are under R; i.e., R \u2192 A \u2192 A' is R \u2192 A' (i.e., the coslice category of the category of commutative rings under R.) The prime spectrum functor Spec then determines an anti-equivalence of this category to the category of affine schemes over Spec R.The associativity then refers to the identity:Pushing this idea further, some authors have introduced a \"generalized ring\" as a monoid object in some other category that behaves like the category of modules. Indeed, this reinterpretation allows one to avoid making an explicit reference to elements of an algebra A. For example, the associativity can be expressed as follows. By the universal property of a tensor product of modules, the multiplication (the R-bilinear map) corresponds to a unique R-linear mapThe definition is equivalent to saying that a unital associative R-algebra is a monoid object in R-Mod (the monoidal category of R-modules). By definition, a ring is a monoid object in the category of abelian groups; thus, the notion of an associative algebra is obtained by replacing the category of abelian groups with the category of modules.If A itself is commutative (as a ring) then it is called a commutative R-algebra.for all x, y, and z in A. (Technical note: the multiplicative identity is a datum,[1] while associativity is a property. By the uniqueness of the multiplicative identity, \"unitarity\" is often treated like a property.) If one drops the requirement for the associativity, then one obtains a non-associative algebra.In other words, A is an R-module together with (1) an R-bilinear map A \u00d7 A \u2192 A, called the multiplication, and (2) the multiplicative identity, such that the multiplication is associative:for all x \u2208 A. Note that such an element 1 must be unique.for all r \u2208 R and x, y \u2208 A. Furthermore, A is assumed to be unital, which is to say it contains an element 1 such thatLet R be a fixed commutative ring (so R could be a field). An associative R-algebra (or more simply, an R-algebra) is an additive abelian group A which has the structure of both a ring and an R-module in such a way that the scalar multiplication satisfiesMany authors consider the more general concept of an associative algebra over a commutative ring R, instead of a field: An R-algebra is an R-module with an associative R-bilinear binary operation, which also contains a multiplicative identity. For examples of this concept, if S is any ring with center C, then S is an associative C-algebra.In this article associative algebras are assumed to have a multiplicative unit, denoted 1; they are sometimes called unital associative algebras for clarification. In some areas of mathematics this assumption is not made, and we will call such structures non-unital associative algebras. We will also assume that all rings are unital, and all ring homomorphisms are unital.In mathematics, an associative algebra is an algebraic structure with compatible operations of addition, multiplication (assumed to be associative), and a scalar multiplication by elements in some field. The addition and multiplication operations together give A the structure of a ring; the addition and scalar multiplication operations together give A the structure of a vector space over K. In this article we will also use the term K-algebra to mean an associative algebra over the field K. A standard first example of a K-algebra is a ring of square matrices over a field K, with the usual matrix multiplication.",
            "title": "Associative algebra",
            "url": "https://en.wikipedia.org/wiki/Associative_algebra"
        },
        {
            "desc_links": [
                "/wiki/Representation_theory",
                "/wiki/Group_(mathematics)",
                "/wiki/Commutative_algebra",
                "/wiki/Homological_algebra",
                "/wiki/Algebraic_geometry",
                "/wiki/Algebraic_topology",
                "/wiki/Abelian_group",
                "/wiki/Semigroup_action",
                "/wiki/Mathematics",
                "/wiki/Algebraic_structure",
                "/wiki/Abstract_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Scalar_(mathematics)"
            ],
            "links": [
                "/wiki/Near-rings",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Semiring",
                "/wiki/Commutative",
                "/wiki/Monoid",
                "/wiki/Semiring",
                "/wiki/Vector_space",
                "/wiki/Ringed_space",
                "/wiki/Sheaf_(mathematics)",
                "/wiki/Sheaf_of_modules",
                "/wiki/Algebraic_geometry",
                "/wiki/Preadditive_category",
                "/wiki/Additive_functor",
                "/wiki/Functor_category",
                "/wiki/Injective",
                "/wiki/Integer",
                "/wiki/Modular_arithmetic",
                "/wiki/Group_homomorphism",
                "/wiki/Ring_homomorphism",
                "/wiki/Uniform_module",
                "/wiki/Graded_module",
                "/wiki/Graded_ring",
                "/wiki/Artinian_module",
                "/wiki/Descending_chain_condition",
                "/wiki/Noetherian_module",
                "/wiki/Ascending_chain_condition",
                "/wiki/Torsion-free_module",
                "/wiki/Faithful_module",
                "/wiki/Annihilator_(ring_theory)",
                "/wiki/Indecomposable_module",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Uniform_module",
                "/wiki/Semisimple_module",
                "/wiki/Simple_module",
                "/wiki/Torsionless_module",
                "/wiki/Flat_module",
                "/wiki/Tensor_product_of_modules",
                "/wiki/Exact_sequence",
                "/wiki/Injective_module",
                "/wiki/Projective_module",
                "/wiki/Direct_summand",
                "/wiki/Free_module",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Cyclic_module",
                "/wiki/Finitely_generated_module",
                "/wiki/Linear_combination",
                "/wiki/Category_theory",
                "/wiki/Category_of_modules",
                "/wiki/Abelian_category",
                "/wiki/Kernel_(algebra)",
                "/wiki/Isomorphism_theorem",
                "/wiki/Bijective",
                "/wiki/Isomorphism",
                "/wiki/Homomorphism",
                "/wiki/Linear_map",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_homomorphism",
                "/wiki/Lattice_(order)",
                "/wiki/Modular_lattice",
                "/wiki/Subgroup",
                "/wiki/Commutative_ring",
                "/wiki/Bimodule",
                "/wiki/Endomorphism",
                "/wiki/Ring_homomorphism",
                "/wiki/Endomorphism_ring",
                "/wiki/Group_action",
                "/wiki/Monoid_action",
                "/wiki/Representation_theory",
                "/wiki/Group_ring",
                "/wiki/Unital_algebra",
                "/wiki/Glossary_of_ring_theory",
                "/wiki/Ring_(mathematics)",
                "/wiki/Abelian_group",
                "/wiki/Well-behaved",
                "/wiki/Principal_ideal_domain",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Free_module",
                "/wiki/Invariant_basis_number",
                "/wiki/Axiom_of_choice",
                "/wiki/Lp_space",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Distributive_law",
                "/wiki/Ring_(mathematics)",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Quotient_ring",
                "/wiki/Representation_theory",
                "/wiki/Group_(mathematics)",
                "/wiki/Commutative_algebra",
                "/wiki/Homological_algebra",
                "/wiki/Algebraic_geometry",
                "/wiki/Algebraic_topology",
                "/wiki/Abelian_group",
                "/wiki/Semigroup_action",
                "/wiki/Mathematics",
                "/wiki/Algebraic_structure",
                "/wiki/Abstract_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Scalar_(mathematics)"
            ],
            "text": "Over near-rings, one can consider near-ring modules, a nonabelian generalization of modules.[citation needed]One can also consider modules over a semiring. Modules over rings are abelian groups, but modules over semirings are only commutative monoids. Most applications of modules are still possible. In particular, for any semiring S the matrices over S form a semiring over which the tuples of elements from S are a module (in this generalized sense only). This allows a further generalization of the concept of vector space incorporating the semirings from theoretical computer science.Modules over commutative rings can be generalized in a different direction: take a ringed space (X, OX) and consider the sheaves of OX-modules; see sheaf of modules for more. These form a category OX-Mod, and play an important role in modern algebraic geometry. If X has only a single point, then this is a module category in the old sense over the commutative ring OX(X).Any ring R can be viewed as a preadditive category with a single object. With this understanding, a left R-module is nothing but a (covariant) additive functor from R to the category Ab of abelian groups. Right R-modules are contravariant additive functors. This suggests that, if C is any preadditive category, a covariant additive functor from C to Ab should be considered a generalized left module over C; these functors form a functor category C-Mod which is the natural generalization of the module category R-Mod.A representation is called faithful if and only if the map R \u2192 EndZ(M) is injective. In terms of modules, this means that if r is an element of R such that rx = 0 for all x in M, then r = 0. Every abelian group is a faithful module over the integers or over some modular arithmetic Z/nZ.Such a ring homomorphism R \u2192 EndZ(M) is called a representation of R over the abelian group M; an alternative and equivalent way of defining left R-modules is to say that a left R-module is an abelian group M together with a representation of R over it.If M is a left R-module, then the action of an element r in R is defined to be the map M \u2192 M that sends each x to rx (or xr in the case of a right module), and is necessarily a group endomorphism of the abelian group (M, +). The set of all group endomorphisms of M is denoted EndZ(M) and forms a ring under addition and composition, and sending a ring element r of R to its action actually defines a ring homomorphism from R to EndZ(M).Uniform. A uniform module is a module in which all pairs of nonzero submodules have nonzero intersection.Graded. A graded module is a module with a decomposition as a direct sum M = \u2a01x Mx over a graded ring R = \u2a01x Rx such that RxMy \u2282 Mx+y for all x and y.Artinian. An Artinian module is a module which satisfies the descending chain condition on submodules, that is, every decreasing chain of submodules becomes stationary after finitely many steps.Noetherian. A Noetherian module is a module which satisfies the ascending chain condition on submodules, that is, every increasing chain of submodules becomes stationary after finitely many steps. Equivalently, every submodule is finitely generated.Torsion-free. A torsion-free module is a module over a ring such that 0 is the only element annihilated by a regular element (non zero-divisor) of the ring.Faithful. A faithful module M is one where the action of each r \u2260 0 in R on M is nontrivial (i.e. r \u22c5 x \u2260 0 for some x in M). Equivalently, the annihilator of M is the zero ideal.Indecomposable. An indecomposable module is a non-zero module that cannot be written as a direct sum of two non-zero submodules. Every simple module is indecomposable, but there are indecomposable modules which are not simple (e.g. uniform modules).Semisimple. A semisimple module is a direct sum (finite or not) of simple modules. Historically these modules are also called completely reducible.Simple. A simple module S is a module that is not {0} and whose only submodules are {0} and S. Simple modules are sometimes called irreducible.[3]Torsionless module. A module is called torsionless if it embeds into its algebraic dual.Flat. A module is called flat if taking the tensor product of it with any exact sequence of R-modules preserves exactness.Injective. Injective modules are defined dually to projective modules.Projective. Projective modules are direct summands of free modules and share many of their desirable properties.Free. A free R-module is a module that has a basis, or equivalently, one that is isomorphic to a direct sum of copies of the ring R. These are the modules that behave very much like vector spaces.Cyclic. A module is called a cyclic module if it is generated by one element.Finitely generated. An R-module M is finitely generated if there exist finitely many elements x1, ..., xn in M such that every element of M is a linear combination of those elements with coefficients from the ring R.The left R-modules, together with their module homomorphisms, form a category, written as R-Mod (see category of modules for more.) This is an abelian category.The kernel of a module homomorphism f\u00a0: M \u2192 N is the submodule of M consisting of all elements that are sent to zero by f. The isomorphism theorems familiar from groups and vector spaces are also valid for R-modules.A bijective module homomorphism is an isomorphism of modules, and the two modules are called isomorphic. Two isomorphic modules are identical for all practical purposes, differing solely in the notation for their elements.This, like any homomorphism of mathematical objects, is just a mapping which preserves the structure of the objects. Another name for a homomorphism of modules over R is an R-linear map.If M and N are left R-modules, then a map f\u00a0: M \u2192 N is a homomorphism of R-modules if, for any m, n in M and r, s in R,The set of submodules of a given module M, together with the two binary operations + and \u2229, forms a lattice which satisfies the modular law: Given submodules U, N1, N2 of M such that N1 \u2282 N2, then the following two submodules are equal: (N1 + U) \u2229 N2 = N1 + (U \u2229 N2).Suppose M is a left R-module and N is a subgroup of M. Then N is a submodule (or R-submodule, to be more explicit) if, for any n in N and any r in R, the product r \u22c5 n is in N (or n \u22c5 r for a right module).If R is commutative, then left R-modules are the same as right R-modules and are simply called R-modules.A bimodule is a module that is a left module and a right module such that the two multiplications are compatible.If one writes the scalar action as fr so that fr(x) = r \u22c5 x, and f for the map that takes each r to its corresponding map fr\u00a0, then the first axiom states that every fr is a group endomorphism of M, and the other three axioms assert that the map f\u00a0: R \u2192 End(M) given by r \u21a6 fr is a ring homomorphism from R to the endomorphism ring End(M).[2] Thus a module is a ring action on an abelian group (cf. group action. Also consider monoid action of multiplicative structure of R). In this sense, module theory generalizes representation theory, which deals with group actions on vector spaces, or equivalently group ring actions.Authors who do not require rings to be unital omit condition 4 above in the definition of an R-module, and so would call the structures defined above \"unital left R-modules\". In this article, consistent with the glossary of ring theory, all rings and modules are assumed to be unital.[1]The operation of the ring on M is called scalar multiplication, and is usually written by juxtaposition, i.e. as rx for r in R and x in M, though here it is denoted as r \u22c5 x to distinguish it from the ring multiplication operation, denoted here by juxtaposition. The notation RM indicates a left R-module M. A right R-module M or MR is defined similarly, except that the ring acts on the right; i.e., scalar multiplication takes the form \u22c5\u00a0: M \u00d7 R \u2192 M, and the above axioms are written with scalars r and s on the right of x and y.Suppose that R is a ring and 1R is its multiplicative identity. A left R-module M consists of an abelian group (M, +) and an operation \u22c5\u00a0: R \u00d7 M \u2192 M such that for all r, s in R and x, y in M, we have:Much of the theory of modules consists of extending as many of the desirable properties of vector spaces as possible to the realm of modules over a \"well-behaved\" ring, such as a principal ideal domain. However, modules can be quite a bit more complicated than vector spaces; for instance, not all modules have a basis, and even those that do, free modules, need not have a unique rank if the underlying ring does not satisfy the invariant basis number condition, unlike vector spaces, which always have a (possibly infinite) basis whose cardinality is then unique. (These last two assertions require the axiom of choice in general, but not in the case of finite-dimensional spaces, or certain well-behaved infinite-dimensional spaces such as Lp spaces.)In a vector space, the set of scalars is a field and acts on the vectors by scalar multiplication, subject to certain axioms such as the distributive law. In a module, the scalars need only be a ring, so the module concept represents a significant generalization. In commutative algebra, both ideals and quotient rings are modules, so that many arguments about ideals or quotient rings can be combined into a single argument about modules. In non-commutative algebra the distinction between left ideals, ideals, and modules becomes more pronounced, though some ring-theoretic conditions can be expressed either about left ideals or left modules.Modules are very closely related to the representation theory of groups. They are also one of the central notions of commutative algebra and homological algebra, and are used widely in algebraic geometry and algebraic topology.Thus, a module, like a vector space, is an additive abelian group; a product is defined between elements of the ring and elements of the module that is distributive over the addition operation of each parameter and is compatible with the ring multiplication.In mathematics, a module is one of the fundamental algebraic structures used in abstract algebra. A module over a ring is a generalization of the notion of vector space over a field, wherein the corresponding scalars are the elements of an arbitrary given ring (with identity) and a multiplication (on the left and/or on the right) is defined between elements of the ring and elements of the module.",
            "title": "Module (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Module_(mathematics)"
        },
        {
            "desc_links": [
                "/wiki/Category_theory",
                "/wiki/Functor",
                "/wiki/Category_of_vector_spaces",
                "/wiki/Algebraic_geometry",
                "/wiki/Module_theory",
                "/wiki/Analytic_number_theory",
                "/wiki/Differential_geometry",
                "/wiki/Operator_theory",
                "/wiki/Algebraic_combinatorics",
                "/wiki/Topology",
                "/wiki/Abstract_algebra",
                "/wiki/Linear_algebra",
                "/wiki/Hilbert_space",
                "/wiki/Mathematical_analysis",
                "/wiki/Physics",
                "/wiki/Symmetry_group",
                "/wiki/Mathematics",
                "/wiki/Abstract_algebra",
                "/wiki/Algebraic_structure",
                "/wiki/Element_(set_theory)",
                "/wiki/Linear_transformation",
                "/wiki/Vector_space",
                "/wiki/Module_(mathematics)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Algebraic_operation",
                "/wiki/Matrix_addition",
                "/wiki/Matrix_multiplication",
                "/wiki/Algebra",
                "/wiki/Group_(mathematics)",
                "/wiki/Associative_algebra",
                "/wiki/Lie_algebra",
                "/wiki/Group_representation"
            ],
            "links": [
                "/wiki/Directed_graph",
                "/wiki/Functor",
                "/wiki/Monoid",
                "/wiki/Monoid_action",
                "/wiki/Category_of_topological_spaces",
                "/wiki/Homeomorphism",
                "/wiki/Category_of_vector_spaces",
                "/wiki/Category_of_sets",
                "/wiki/Category_(mathematics)",
                "/wiki/Morphism",
                "/wiki/Functor",
                "/wiki/Automorphism_group",
                "/wiki/Bijection",
                "/wiki/Permutation",
                "/wiki/Group_homomorphism",
                "/wiki/Symmetric_group",
                "/wiki/Group_action",
                "/wiki/Group_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Quantum_group",
                "/wiki/Crystal_basis",
                "/wiki/Hopf_algebra",
                "/wiki/Associative_algebra",
                "/wiki/Group_ring",
                "/wiki/Group_algebra",
                "/wiki/Universal_enveloping_algebra",
                "/wiki/Hilbert_modular_form",
                "/wiki/Siegel_modular_form",
                "/wiki/Selberg_trace_formula",
                "/wiki/Robert_Langlands",
                "/wiki/Riemann-Roch_theorem",
                "/wiki/Algebraic_group",
                "/wiki/Adelic_algebraic_group",
                "/wiki/Langlands_program",
                "/wiki/Modular_form",
                "/wiki/Analytic_function",
                "/wiki/Several_complex_variables",
                "/wiki/PSL2(R)",
                "/wiki/Congruence_subgroup",
                "/wiki/Discrete_subgroup",
                "/wiki/Differential_form",
                "/wiki/Upper_half_space",
                "/wiki/Maximal_compact_subgroup",
                "/wiki/Symmetric_space",
                "/wiki/Semisimple_Lie_group",
                "/wiki/Felix_Klein",
                "/wiki/Erlangen_program",
                "/wiki/%C3%89lie_Cartan",
                "/wiki/Cartan_connection",
                "/wiki/Holonomy",
                "/wiki/Differential_operator",
                "/wiki/Several_complex_variables",
                "/wiki/Infinite_group",
                "/wiki/Linear_algebra",
                "/wiki/Quadratic_form",
                "/wiki/Determinant",
                "/wiki/Projective_geometry",
                "/wiki/David_Mumford",
                "/wiki/Geometric_invariant_theory",
                "/wiki/Group_action",
                "/wiki/Algebraic_variety",
                "/wiki/Polynomial_function",
                "/wiki/Linear_group",
                "/wiki/Group_scheme",
                "/wiki/Lie_group",
                "/wiki/Finite_groups_of_Lie_type",
                "/wiki/Zariski_topology",
                "/wiki/Lie_superalgebra",
                "/wiki/Theoretical_physics",
                "/wiki/Conformal_field_theory",
                "/wiki/Exactly_solvable_model",
                "/wiki/Macdonald_identities",
                "/wiki/Victor_Kac",
                "/wiki/Robert_Moody",
                "/wiki/Semisimple_Lie_algebra",
                "/wiki/%C3%89lie_Cartan",
                "/wiki/Cartan_subalgebra",
                "/wiki/Weight_(representation_theory)",
                "/wiki/Eigenspace",
                "/wiki/Lie_algebra",
                "/wiki/Skew-symmetric_graph",
                "/wiki/Bilinear_operation",
                "/wiki/Lie_bracket",
                "/wiki/Jacobi_identity",
                "/wiki/Tangent_space",
                "/wiki/Lie_group",
                "/wiki/Identity_element",
                "/wiki/Semidirect_product",
                "/wiki/Solvable_Lie_group",
                "/wiki/Levi_decomposition",
                "/wiki/Mackey_theory",
                "/wiki/Wigner%27s_classification",
                "/wiki/Semisimple_Lie_group",
                "/wiki/Weyl%27s_unitary_trick",
                "/wiki/Lie_group",
                "/wiki/Smooth_manifold",
                "/wiki/Homogeneous_spaces",
                "/wiki/Symmetric_space",
                "/wiki/Automorphic_form",
                "/wiki/Alexander_Grothendieck",
                "/wiki/Linear_algebraic_group",
                "/wiki/Tannakian_category",
                "/wiki/Category_(mathematics)",
                "/wiki/Tannaka%E2%80%93Krein_duality",
                "/wiki/Plancherel_theorem",
                "/wiki/Measure_(mathematics)",
                "/wiki/Unitary_dual",
                "/wiki/Square_integrable",
                "/wiki/L2-space",
                "/wiki/Pontrjagin_duality",
                "/wiki/Peter%E2%80%93Weyl_theorem",
                "/wiki/Fourier_series",
                "/wiki/Fourier_transform",
                "/wiki/Dual_vector_space",
                "/wiki/Harmonic_analysis",
                "/wiki/Mathematical_analysis",
                "/wiki/Local_compactness",
                "/wiki/Harish-Chandra_module",
                "/wiki/Sesquilinear_form",
                "/wiki/Semisimple_Lie_group",
                "/wiki/Lie_group",
                "/wiki/Representation_theory_of_SL2(R)",
                "/wiki/Representation_theory_of_the_Lorentz_group",
                "/wiki/Unitary_dual",
                "/wiki/Locally_compact",
                "/wiki/Topological_group",
                "/wiki/Strongly_continuous",
                "/wiki/Character_theory",
                "/wiki/Peter%E2%80%93Weyl_theorem",
                "/wiki/Hilbert_space",
                "/wiki/Unitary_operator",
                "/wiki/Quantum_mechanics",
                "/wiki/Hermann_Weyl",
                "/wiki/Representation_theory_of_the_Poincar%C3%A9_group",
                "/wiki/Eugene_Wigner",
                "/wiki/George_Mackey",
                "/wiki/Harish-Chandra",
                "/wiki/Mathematics",
                "/wiki/Algebraic_geometry",
                "/wiki/Coding_theory",
                "/wiki/Combinatorics",
                "/wiki/Number_theory",
                "/wiki/Richard_Brauer",
                "/wiki/Classification_of_finite_simple_groups",
                "/wiki/Sylow_subgroup",
                "/wiki/Group_ring",
                "/wiki/Finite_groups_of_Lie_type",
                "/wiki/Linear_algebraic_group",
                "/wiki/Lie_group",
                "/wiki/Lie_algebra_representation",
                "/wiki/Weight_(representation_theory)",
                "/wiki/Compact_group",
                "/wiki/Haar_measure",
                "/wiki/Abstract_harmonic_analysis",
                "/wiki/Orthogonal_complement",
                "/wiki/Unitary_representation",
                "/wiki/Positive_characteristic",
                "/wiki/Finite_field",
                "/wiki/Coprime",
                "/wiki/Group_order",
                "/wiki/Common_factor",
                "/wiki/Modular_representation_theory",
                "/wiki/Character_theory",
                "/wiki/Characteristic_zero",
                "/wiki/Maschke%27s_theorem",
                "/wiki/Projection_(linear_algebra)",
                "/wiki/Crystallographic_group",
                "/wiki/Clebsch%E2%80%93Gordan_coefficients",
                "/wiki/Weyl%27s_theorem_on_complete_reducibility",
                "/wiki/Direct_sum_of_representations",
                "/wiki/Direct_sum_of_vector_spaces",
                "/wiki/Schur%27s_lemma",
                "/wiki/Zero_map",
                "/wiki/Kernel_(linear_algebra)",
                "/wiki/Image_(mathematics)",
                "/wiki/Endomorphism",
                "/wiki/Division_algebra",
                "/wiki/Algebraically_closed",
                "/wiki/Zero_vector_space",
                "/wiki/Subrepresentation",
                "/wiki/Quotient_space_(linear_algebra)",
                "/wiki/Up_to_isomorphism",
                "/wiki/Isomorphism",
                "/wiki/Commutative_diagram",
                "/wiki/Faithful_representation",
                "/wiki/Injective",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Dimension_of_a_vector_space",
                "/wiki/Lie_algebra#Definition_and_first_properties",
                "/wiki/Identity_element",
                "/wiki/Group_action",
                "/wiki/Group_(mathematics)",
                "/wiki/Linear_map",
                "/wiki/Function_composition",
                "/wiki/General_linear_group#General_linear_group_of_a_vector_space",
                "/wiki/Linear_map#Endomorphisms_and_automorphisms",
                "/wiki/Algebra",
                "/wiki/Group_(mathematics)",
                "/wiki/Associative_algebra",
                "/wiki/Lie_algebra",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Column_vector",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Abstract_algebra",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Category_theory",
                "/wiki/Functor",
                "/wiki/Category_of_vector_spaces",
                "/wiki/Algebraic_geometry",
                "/wiki/Module_theory",
                "/wiki/Analytic_number_theory",
                "/wiki/Differential_geometry",
                "/wiki/Operator_theory",
                "/wiki/Algebraic_combinatorics",
                "/wiki/Topology",
                "/wiki/Abstract_algebra",
                "/wiki/Linear_algebra",
                "/wiki/Hilbert_space",
                "/wiki/Mathematical_analysis",
                "/wiki/Physics",
                "/wiki/Symmetry_group",
                "/wiki/Mathematics",
                "/wiki/Abstract_algebra",
                "/wiki/Algebraic_structure",
                "/wiki/Element_(set_theory)",
                "/wiki/Linear_transformation",
                "/wiki/Vector_space",
                "/wiki/Module_(mathematics)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Algebraic_operation",
                "/wiki/Matrix_addition",
                "/wiki/Matrix_multiplication",
                "/wiki/Algebra",
                "/wiki/Group_(mathematics)",
                "/wiki/Associative_algebra",
                "/wiki/Lie_algebra",
                "/wiki/Group_representation"
            ],
            "text": "One special case has had a significant impact on representation theory, namely the representation theory of quivers.[11] A quiver is simply a directed graph (with loops and multiple arrows allowed), but it can be made into a category (and also an algebra) by considering paths in the graph. Representations of such categories/algebras have illuminated several aspects of representation theory, for instance by allowing non-semisimple representation theory questions about a group to be reduced in some cases to semisimple representation theory questions about a quiver.More generally, one can relax the assumption that the category being represented has only one object. In full generality, this is simply the theory of functors between categories, and little can be said.Since groups are categories, one can also consider representation of other categories. The simplest generalization is to monoids, which are categories with one object. Groups are monoids for which every morphism is invertible. General monoids have representations in any category. In the category of sets, these are monoid actions, but monoid representations on vector spaces and other objects can be studied.Two types of representations closely related to linear representations are:For another example consider the category of topological spaces, Top. Representations in Top are homomorphisms from G to the homeomorphism group of a topological space X.In the case where C is VectF, the category of vector spaces over a field F, this definition is equivalent to a linear representation. Likewise, a set-theoretic representation is just a representation of G in the category of sets.Every group G can be viewed as a category with a single object; morphisms in this category are just the elements of G. Given an arbitrary category C, a representation of G in C is a functor from G to C. Such a functor selects an object X in C and a group homomorphism from G to Aut(X), the automorphism group of X.This condition and the axioms for a group imply that \u03c1(g) is a bijection (or permutation) for all g in G. Thus we may equivalently define a permutation representation to be a group homomorphism from G to the symmetric group SX of X.A set-theoretic representation (also known as a group action or permutation representation) of a group G on a set X is given by a function \u03c1 from G to XX, the set of functions from X to X, such that for all g1, g2 in G and all x in X:The Hopf algebras associated to groups have a commutative algebra structure, and so general Hopf algebras are known as quantum groups, although this term is often restricted to certain Hopf algebras arising as deformations of groups or their universal enveloping algebras. The representation theory of quantum groups has added surprising insights to the representation theory of Lie groups and Lie algebras, for instance through the crystal basis of Kashiwara.Hopf algebras provide a way to improve the representation theory of associative algebras, while retaining the representation theory of groups and Lie algebras as special cases. In particular, the tensor product of two representations is a representation, as is the dual vector space.When considering representations of an associative algebra, one can forget the underlying field, and simply regard the associative algebra as a ring, and its representations as modules. This approach is surprisingly fruitful: many results in representation theory can be interpreted as special cases of results about modules over a ring.In one sense, associative algebra representations generalize both representations of groups and Lie algebras. A representation of a group induces a representation of a corresponding group ring or group algebra, while representations of a Lie algebra correspond bijectively to representations of its universal enveloping algebra. However, the representation theory of general associative algebras does not have all of the nice properties of the representation theory of groups and Lie algebras.Before the development of the general theory, many important special cases were worked out in detail, including the Hilbert modular forms and Siegel modular forms. Important results in the theory include the Selberg trace formula and the realization by Robert Langlands that the Riemann-Roch theorem could be applied to calculate the dimension of the space of automorphic forms. The subsequent notion of \"automorphic representation\" has proved of great technical value for dealing with the case that G is an algebraic group, treated as an adelic algebraic group. As a result, an entire philosophy, the Langlands program has developed around the relation between representation and number theoretic properties of automorphic forms.[40]Automorphic forms are a generalization of modular forms to more general analytic functions, perhaps of several complex variables, with similar transformation properties.[39] The generalization involves replacing the modular group PSL2 (R) and a chosen congruence subgroup by a semisimple Lie group G and a discrete subgroup \u0393. Just as modular forms can be viewed as differential forms on a quotient of the upper half space H = PSL2 (R)/SO(2), automorphic forms can be viewed as differential forms (or similar objects) on \u0393\\G/K, where K is (typically) a maximal compact subgroup of G. Some care is required, however, as the quotient typically has singularities. The quotient of a semisimple Lie group by a compact subgroup is a symmetric space and so the theory of automorphic forms is intimately related to harmonic analysis on symmetric spaces.The representation theory of semisimple Lie groups has its roots in invariant theory[30] and the strong links between representation theory and algebraic geometry have many parallels in differential geometry, beginning with Felix Klein's Erlangen program and \u00c9lie Cartan's connections, which place groups and symmetry at the heart of geometry.[38] Modern developments link representation theory and invariant theory to areas as diverse as holonomy, differential operators and the theory of several complex variables.Invariant theory of infinite groups is inextricably linked with the development of linear algebra, especially, the theories of quadratic forms and determinants. Another subject with strong mutual influence is projective geometry, where invariant theory can be used to organize the subject, and during the 1960s, new life was breathed into the subject by David Mumford in the form of his geometric invariant theory.[37]Invariant theory studies actions on algebraic varieties from the point of view of their effect on functions, which form representations of the group. Classically, the theory dealt with the question of explicit description of polynomial functions that do not change, or are invariant, under the transformations from a given linear group. The modern approach analyses the decomposition of these representations into irreducibles.[36]Linear algebraic groups (or more generally, affine group schemes) are analogues in algebraic geometry of Lie groups, but over more general fields than just R or C. In particular, over finite fields, they give rise to finite groups of Lie type. Although linear algebraic groups have a classification that is very similar to that of Lie groups, their representation theory is rather different (and much less well understood) and requires different techniques, since the Zariski topology is relatively weak, and techniques from analysis are no longer available.[35]Lie superalgebras are generalizations of Lie algebras in which the underlying vector space has a Z2-grading, and skew-symmetry and Jacobi identity properties of the Lie bracket are modified by signs. Their representation theory is similar to the representation theory of Lie algebras.[34]Affine Lie algebras are a special case of Kac\u2013Moody algebras, which have particular importance in mathematics and theoretical physics, especially conformal field theory and the theory of exactly solvable models. Kac discovered an elegant proof of certain combinatorial identities, Macdonald identities, which is based on the representation theory of affine Kac\u2013Moody algebras.There are many classes of infinite-dimensional Lie algebras whose representations have been studied. Among these, an important class are the Kac\u2013Moody algebras.[33] They are named after Victor Kac and Robert Moody, who independently discovered them. These algebras form a generalization of finite-dimensional semisimple Lie algebras, and share many of their combinatorial properties. This means that they have a class of representations that can be understood in the same way as representations of semisimple Lie algebras.Lie algebras, like Lie groups, have a Levi decomposition into semisimple and solvable parts, with the representation theory of solvable Lie algebras being intractable in general. In contrast, the finite-dimensional representations of semisimple Lie algebras are completely understood, after work of \u00c9lie Cartan. A representation of a semisimple Lie algebra g is analysed by choosing a Cartan subalgebra, which is essentially a generic maximal subalgebra h of g on which the Lie bracket is zero (\"abelian\"). The representation of g can be decomposed into weight spaces that are eigenspaces for the action of h and the infinitesimal analogue of characters. The structure of semisimple Lie algebras then reduces the analysis of representations to easily understood combinatorics of the possible weights that can occur.[31]A Lie algebra over a field F is a vector space over F equipped with a skew-symmetric bilinear operation called the Lie bracket, which satisfies the Jacobi identity. Lie algebras arise in particular as tangent spaces to Lie groups at the identity element, leading to their interpretation as \"infinitesimal symmetries\".[31] An important approach to the representation theory of Lie groups is to study the corresponding representation theory of Lie algebras, but representations of Lie algebras also have an intrinsic interest.[32]A general Lie group is a semidirect product of a solvable Lie group and a semisimple Lie group (the Levi decomposition).[31] The classification of representations of solvable Lie groups is intractable in general, but often easy in practical cases. Representations of semidirect products can then be analysed by means of general results called Mackey theory, which is a generalization of the methods used in Wigner's classification of representations of the Poincar\u00e9 group.The representation theory of Lie groups can be developed first by considering the compact groups, to which results of compact representation theory apply.[26] This theory can be extended to finite-dimensional representations of semisimple Lie groups using Weyl's unitary trick: each semisimple real Lie group G has a complexification, which is a complex Lie group Gc, and this complex Lie group has a maximal compact subgroup K. The finite-dimensional representations of G closely correspond to those of K.A Lie group is a group that is also a smooth manifold. Many classical groups of matrices over the real or complex numbers are Lie groups.[30] Many of the groups important in physics and chemistry are Lie groups, and their representation theory is crucial to the application of group theory in those fields.[5]Harmonic analysis has also been extended from the analysis of functions on a group G to functions on homogeneous spaces for G. The theory is particularly well developed for symmetric spaces and provides a theory of automorphic forms (discussed below).If the group is neither abelian nor compact, no general theory is known with an analogue of the Plancherel theorem or Fourier inversion, although Alexander Grothendieck extended Tannaka\u2013Krein duality to a relationship between linear algebraic groups and tannakian categories.Another approach involves considering all unitary representations, not just the irreducible ones. These form a category, and Tannaka\u2013Krein duality provides a way to recover a compact group from its category of unitary representations.A major goal is to provide a general form of the Fourier transform and the Plancherel theorem. This is done by constructing a measure on the unitary dual and an isomorphism between the regular representation of G on the space L2(G) of square integrable functions on G and its representation on the space of L2 functions on the unitary dual. Pontrjagin duality and the Peter\u2013Weyl theorem achieve this for abelian and compact G respectively.[27][29]The duality between the circle group S1 and the integers Z, or more generally, between a torus Tn and Zn is well known in analysis as the theory of Fourier series, and the Fourier transform similarly expresses the fact that the space of characters on a real vector space is the dual vector space. Thus unitary representation theory and harmonic analysis are intimately related, and abstract harmonic analysis exploits this relationship, by developing the analysis of functions on locally compact topological groups and related spaces.[7]For non-compact G, the question of which representations are unitary is a subtle one. Although irreducible unitary representations must be \"admissible\" (as Harish-Chandra modules) and it is easy to detect which admissible representations have a nondegenerate invariant sesquilinear form, it is hard to determine when this form is positive definite. An effective description of the unitary dual, even for relatively well-behaved groups such as real reductive Lie groups (discussed below), remains an important open problem in representation theory. It has been solved for many particular groups, such as SL(2,R) and the Lorentz group.[28]A major goal is to describe the \"unitary dual\", the space of irreducible unitary representations of G.[26] The theory is most well-developed in the case that G is a locally compact (Hausdorff) topological group and the representations are strongly continuous.[7] For G abelian, the unitary dual is just the space of characters, while for G compact, the Peter\u2013Weyl theorem shows that the irreducible unitary representations are finite-dimensional and the unitary dual is discrete.[27] For example, if G is the circle group S1, then the characters are given by integers, and the unitary dual is Z.A unitary representation of a group G is a linear representation \u03c6 of G on a real or (usually) complex Hilbert space V such that \u03c6(g) is a unitary operator for every g \u2208 G. Such representations have been widely applied in quantum mechanics since the 1920s, thanks in particular to the influence of Hermann Weyl,[23] and this has inspired the development of the theory, most notably through the analysis of representations of the Poincar\u00e9 group by Eugene Wigner.[24] One of the pioneers in constructing a general theory of unitary representations (for any group G rather than just for particular groups useful in applications) was George Mackey, and an extensive theory was developed by Harish-Chandra and others in the 1950s and 1960s.[25]As well as having applications to group theory, modular representations arise naturally in other branches of mathematics, such as algebraic geometry, coding theory, combinatorics and number theory.Modular representations of a finite group G are representations over a field whose characteristic is not coprime to |G|, so that Maschke's theorem no longer holds (because |G| is not invertible in F and so one cannot divide by it).[21] Nevertheless, Richard Brauer extended much of character theory to modular representations, and this theory played an important role in early progress towards the classification of finite simple groups, especially for simple groups whose characterization was not amenable to purely group-theoretic methods because their Sylow 2-subgroups were \"too small\".[22]Representations of a finite group G are also linked directly to algebra representations via the group algebra F[G], which is a vector space over F with the elements of G as a basis, equipped with the multiplication operation defined by the group operation, linearity, and the requirement that the group operation and scalar multiplication commute.Over arbitrary fields, another class of finite groups that have a good representation theory are the finite groups of Lie type. Important examples are linear algebraic groups over finite fields. The representation theory of linear algebraic groups and Lie groups extends these examples to infinite-dimensional groups, the latter being intimately related to Lie algebra representations. The importance of character theory for finite groups has an analogue in the theory of weights for representations of Lie groups and Lie algebras.Results such as Maschke's theorem and the unitary property that rely on averaging can be generalized to more general groups by replacing the average with an integral, provided that a suitable notion of integral can be defined. This can be done for compact topological groups (including compact Lie groups), using Haar measure, and the resulting theory is known as abstract harmonic analysis.Unitary representations are automatically semisimple, since Maschke's result can be proven by taking the orthogonal complement of a subrepresentation. When studying representations of groups that are not finite, the unitary representations provide a good generalization of the real and complex representations of a finite group.for all g in G and v, w in W. Hence any G-representation is unitary.Maschke's theorem holds more generally for fields of positive characteristic p, such as the finite fields, as long as the prime p is coprime to the order of G. When p and |G| have a common factor, there are G-representations that are not semisimple, which are studied in a subbranch called modular representation theory.The finite-dimensional G-representations can be understood using character theory: the character of a representation \u03c6: G \u2192 GL(V) is the class function \u03c7\u03c6: G \u2192 F defined by\u03c0G is equivariant, and its kernel is the required complement.Over a field of characteristic zero, the representation of a finite group G has a number of convenient properties. First, the representations of G are semisimple (completely reducible). This is a consequence of Maschke's theorem, which states that any subrepresentation V of a G-representation W has a G-invariant complement. One proof is to choose any projection \u03c0 from W to V and replace it by its average \u03c0G defined byGroup representations are a very important tool in the study of finite groups.[19] They also arise in the applications of finite group theory to geometry and crystallography.[20] Representations of finite groups exhibit many of the features of the general theory and point the way to other branches and topics in representation theory.Representation theory is notable for the number of branches it has, and the diversity of the approaches to studying representations of groups and algebras. Although, all the theories have in common the basic concepts discussed already, they differ considerably in detail. The differences are at least 3-fold:In general, the tensor product of irreducible representations is not irreducible; the process of decomposing a tensor product as a direct sum of irreducible representations is known as Clebsch\u2013Gordan theory.In cases where complete reducibility does not hold, one must understand how indecomposable representations can be built from irreducible representations as extensions of a quotient by a subrepresentation.In favorable circumstances, every finite-dimensional representation is a direct sum of irreducible representations: such representations are said to be semisimple. In this case, it suffices to understand only the irreducible representations. Examples where this \"complete reducibility\" phenomenon occur include finite and compact groups, and semisimple Lie algebras.The direct sum of two representations carries no more information about the group G than the two representations do individually. If a representation is the direct sum of two proper nontrivial subrepresentations, it is said to be decomposable. Otherwise, it is said to be indecomposable.If (V,\u03c6) and (W,\u03c8) are representations of (say) a group G, then the direct sum of V and W is a representation, in a canonical way, via the equationIrreducible representations are the building blocks of representation theory: if a representation V is not irreducible then it is built from a subrepresentation and a quotient that are both \"simpler\" in some sense; for instance, if V is finite-dimensional, then both the subrepresentation and the quotient have smaller dimension.The definition of an irreducible representation implies Schur's lemma: an equivariant map \u03b1: V \u2192 W between irreducible representations is either the zero map or an isomorphism, since its kernel and image are subrepresentations. In particular, when V = W, this shows that the equivariant endomorphisms of V form an associative division algebra over the underlying field F. If F is algebraically closed, the only equivariant endomorphisms of an irreducible representation are the scalar multiples of the identity.If V has exactly two subrepresentations, namely the trivial subspace {0} and V itself, then the representation is said to be irreducible; if V has a proper nontrivial subrepresentation, the representation is said to be reducible.[15]If (V,\u03c8) is a representation of (say) a group G, and W is a linear subspace of V that is preserved by the action of G in the sense that g \u00b7 w \u2208 W for all w \u2208 W (Serre [14] calls these W stable under G), then W is called a subrepresentation: by defining \u03c6(g) to be the restriction of \u03c8(g) to W, (W, \u03c6) is a representation of G and the inclusion of W into V is an equivariant map. The quotient space V/W can also be made into a representation of G.Isomorphic representations are, for practical purposes, \"the same\"; they provide the same information about the group or algebra being represented. Representation theory therefore seeks to classify representations up to isomorphism.Equivariant maps for representations of an associative or Lie algebra are defined similarly. If \u03b1 is invertible, then it is said to be an isomorphism, in which case V and W (or, more precisely, \u03c6 and \u03c8) are isomorphic representations, also phrased as equivalent representations. An equivariant map is often called an intertwining map of representations. Also, in the case of a group G, it is on occasion called a G-map.for all g in G, i.e. the following diagram commutes:for all g in G and v in V. In terms of \u03c6: G \u2192 GL(V) and \u03c8: G \u2192 GL(W), this meansIf V and W are vector spaces over F, equipped with representations \u03c6 and \u03c8 of a group G, then an equivariant map from V to W is a linear map \u03b1: V \u2192 W such thatAn effective or faithful representation is a representation (V,\u03c6) for which the homomorphism \u03c6 is injective.When V is of finite dimension n, one can choose a basis for V to identify V with Fn and hence recover a matrix representation with entries in the field F.The vector space V is called the representation space of \u03c6 and its dimension (if finite) is called the dimension of the representation (sometimes degree, as in [14]). It is also common practice to refer to V itself as the representation when the homomorphism \u03c6 is clear from the context; otherwise the notation (V,\u03c6) can be used to denote a representation.and similarly in the other cases. This approach is both more concise and more abstract. From this point of view:The second way to define a representation focuses on the map \u03c6 sending g in G to a linear map \u03c6(g): V \u2192 V, which satisfieswhere [x1, x2] is the Lie bracket, which generalizes the matrix commutator MN \u2212 NM.where e is the identity element of G and g1g2 is the product in G. The requirement for associative algebras is analogous, except that associative algebras do not always have an identity element, in which case equation (1) is ignored. Equation (2) is an abstract expression of the associativity of matrix multiplication. This doesn't hold for the matrix commutator and also there is no identity element for the commutator. Hence for Lie algebras, the only requirement is that for any x1, x2 in A and v in V:with two properties. First, for any g in G (or a in A), the mapThere are two ways to say what a representation is.[13] The first uses the idea of an action, generalizing the way that matrices act on column vectors by matrix multiplication. A representation of a group G or (associative or Lie) algebra A on a vector space V is a mapThis generalizes to any field F and any vector space V over F, with linear maps replacing matrices and composition replacing matrix multiplication: there is a group GL(V,F) of automorphisms of V, an associative algebra EndF(V) of all endomorphisms of V, and a corresponding Lie algebra gl(V,F).There are three main sorts of algebraic objects for which this can be done: groups, associative algebras and Lie algebras.[12]Let V be a vector space over a field F.[3] For instance, suppose V is Rn or Cn, the standard n-dimensional space of column vectors over the real or complex numbers respectively. In this case, the idea of representation theory is to do abstract algebra concretely by using n \u00d7 n matrices of real or complex numbers.The success of representation theory has led to numerous generalizations. One of the most general is in category theory.[11] The algebraic objects to which representation theory applies can be viewed as particular kinds of categories, and the representations as functors from the object category to the category of vector spaces. This description points to two obvious generalizations: first, the algebraic objects can be replaced by more general categories; second, the target category of vector spaces can be replaced by other well-understood categories.Secondly, there are diverse approaches to representation theory. The same objects can be studied using methods from algebraic geometry, module theory, analytic number theory, differential geometry, operator theory, algebraic combinatorics and topology.[10]Representation theory is pervasive across fields of mathematics, for two reasons. First, the applications of representation theory are diverse:[6] in addition to its impact on algebra, representation theory:Representation theory is a useful method because it reduces problems in abstract algebra to problems in linear algebra, a subject that is well understood.[3] Furthermore, the vector space on which a group (for example) is represented can be infinite-dimensional, and by allowing it to be, for instance, a Hilbert space, methods of analysis can be applied to the theory of groups.[4] Representation theory is also important in physics because, for example, it describes how the symmetry group of a physical system affects the solutions of equations describing that system.[5]Representation theory is a branch of mathematics that studies abstract algebraic structures by representing their elements as linear transformations of vector spaces, and studies modules over these abstract algebraic structures.[1] In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and the algebraic operations in terms of matrix addition and matrix multiplication. The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices in such a way that the group operation is matrix multiplication.[2]",
            "title": "Representation theory",
            "url": "https://en.wikipedia.org/wiki/Representation_theory"
        },
        {
            "desc_links": [
                "/wiki/Dimension_(vector_space)",
                "/wiki/Linear_algebra",
                "/wiki/Measure_(mathematics)",
                "/wiki/Integral",
                "/wiki/Probability",
                "/wiki/Functional_(mathematics)",
                "/wiki/Calculus_of_variations",
                "/wiki/Higher-order_function",
                "/wiki/Jacques_Hadamard",
                "/wiki/Vito_Volterra",
                "/wiki/Maurice_Ren%C3%A9_Fr%C3%A9chet",
                "/wiki/Paul_L%C3%A9vy_(mathematician)",
                "/wiki/Frigyes_Riesz",
                "/wiki/Lw%C3%B3w_School_of_Mathematics",
                "/wiki/Poland",
                "/wiki/Stefan_Banach",
                "/wiki/Mathematical_analysis",
                "/wiki/Vector_space",
                "/wiki/Inner_product_space#Definition",
                "/wiki/Norm_(mathematics)#Definition",
                "/wiki/Topological_space#Definition",
                "/wiki/Linear_transformation",
                "/wiki/Function_space",
                "/wiki/Fourier_transform",
                "/wiki/Continuous_function",
                "/wiki/Unitary_operator",
                "/wiki/Differential_equations",
                "/wiki/Integral_equations"
            ],
            "links": [
                "/wiki/Vector_space_basis",
                "/wiki/Zorn%27s_lemma",
                "/wiki/Schauder_basis",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Axiom_of_choice",
                "/wiki/Boolean_prime_ideal_theorem",
                "/wiki/Baire_category_theorem",
                "/wiki/List_of_functional_analysis_topics",
                "/wiki/Topological_space",
                "/wiki/Compact_space",
                "/wiki/Hausdorff_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Baire_category_theorem",
                "/wiki/Normed_space",
                "/wiki/Fr%C3%A9chet_space",
                "/wiki/Open_mapping_theorem_(functional_analysis)",
                "/wiki/Stefan_Banach",
                "/wiki/Juliusz_Schauder",
                "/wiki/Bounded_linear_operator",
                "/wiki/Banach_space",
                "/wiki/Surjective",
                "/wiki/Open_map",
                "/wiki/Sublinear_function",
                "/wiki/Linear_functional",
                "/wiki/Linear_subspace",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Bounded_operator",
                "/wiki/Vector_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Normed_vector_space",
                "/wiki/Dual_space",
                "/wiki/Operator_theory",
                "/wiki/Spectral_measure#Spectral_measure",
                "/wiki/Multiplication_operator",
                "/wiki/Measure_space",
                "/wiki/Ess_sup",
                "/wiki/Spectral_theorem",
                "/wiki/Stefan_Banach",
                "/wiki/Hugo_Steinhaus",
                "/wiki/Hans_Hahn_(mathematician)",
                "/wiki/Uniform_boundedness_principle",
                "/wiki/Banach%E2%80%93Steinhaus_theorem",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Open_mapping_theorem_(functional_analysis)",
                "/wiki/Continuous_linear_operator",
                "/wiki/Banach_space",
                "/wiki/Derivative",
                "/wiki/Fr%C3%A9chet_derivative",
                "/wiki/Continuous_dual",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Isometry",
                "/wiki/Banach_space",
                "/wiki/Orthonormal_basis",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Linear_transformation",
                "/wiki/C*-algebra",
                "/wiki/Operator_algebra",
                "/wiki/Fr%C3%A9chet_space",
                "/wiki/Topological_vector_space",
                "/wiki/Complete_space",
                "/wiki/Normed_vector_space",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Linear_algebra",
                "/wiki/Measure_(mathematics)",
                "/wiki/Integral",
                "/wiki/Probability",
                "/wiki/Functional_(mathematics)",
                "/wiki/Calculus_of_variations",
                "/wiki/Higher-order_function",
                "/wiki/Jacques_Hadamard",
                "/wiki/Vito_Volterra",
                "/wiki/Maurice_Ren%C3%A9_Fr%C3%A9chet",
                "/wiki/Paul_L%C3%A9vy_(mathematician)",
                "/wiki/Frigyes_Riesz",
                "/wiki/Lw%C3%B3w_School_of_Mathematics",
                "/wiki/Poland",
                "/wiki/Stefan_Banach",
                "/wiki/Mathematical_analysis",
                "/wiki/Vector_space",
                "/wiki/Inner_product_space#Definition",
                "/wiki/Norm_(mathematics)#Definition",
                "/wiki/Topological_space#Definition",
                "/wiki/Linear_transformation",
                "/wiki/Function_space",
                "/wiki/Fourier_transform",
                "/wiki/Continuous_function",
                "/wiki/Unitary_operator",
                "/wiki/Differential_equations",
                "/wiki/Integral_equations"
            ],
            "text": "Functional analysis in its present form[update] includes the following tendencies:Most spaces considered in functional analysis have infinite dimension. To show the existence of a vector space basis for such spaces may require Zorn's lemma. However, a somewhat different concept, Schauder basis, is usually more relevant in functional analysis. Many very important theorems require the Hahn\u2013Banach theorem, usually proved using axiom of choice, although the strictly weaker Boolean prime ideal theorem suffices. The Baire category theorem, needed to prove many important theorems, also requires a form of axiom of choice.List of functional analysis topics.The closed graph theorem states the following: If X is a topological space and Y is a compact Hausdorff space, then the graph of a linear map T from X to Y is closed if and only if T is continuous.[3]The proof uses the Baire category theorem, and completeness of both X and Y is essential to the theorem. The statement of the theorem is no longer true if either space is just assumed to be a normed space, but is true if X and Y are taken to be Fr\u00e9chet spaces.The open mapping theorem, also known as the Banach\u2013Schauder theorem (named after Stefan Banach and Juliusz Schauder), is a fundamental result which states that if a continuous linear operator between Banach spaces is surjective then it is an open map. More precisely,:[2]then there exists a linear extension \u03c8\u00a0: V \u2192 R of \u03c6 to the whole space V, i.e., there exists a linear functional \u03c8 such thatHahn\u2013Banach theorem:[2] If p\u00a0: V \u2192 R is a sublinear function, and \u03c6\u00a0: U \u2192 R is a linear functional on a linear subspace U \u2286 V which is dominated by p on U, i.e.The Hahn\u2013Banach theorem is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are \"enough\" continuous linear functionals defined on every normed vector space to make the study of the dual space \"interesting\".This is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.where T is the multiplication operator:Theorem:[1] Let A be a bounded self-adjoint operator on a Hilbert space H. Then there is a measure space (X, \u03a3, \u03bc) and a real-valued essentially bounded measurable function f on X and a unitary operator U:H \u2192 L2\u03bc(X) such thatThere are many theorems known as the spectral theorem, but one in particular has many applications in functional analysis. Let A be the operator of multiplication by t on L2[0, 1], that isThe theorem was first published in 1927 by Stefan Banach and Hugo Steinhaus but it was also proven independently by Hans Hahn.The uniform boundedness principle or Banach\u2013Steinhaus theorem is one of the fundamental results in functional analysis. Together with the Hahn\u2013Banach theorem and the open mapping theorem, it is considered one of the cornerstones of the field. In its basic form, it asserts that for a family of continuous linear operators (and thus bounded operators) whose domain is a Banach space, pointwise boundedness is equivalent to uniform boundedness in operator norm.Important results of functional analysis include:Also, the notion of derivative can be extended to arbitrary functions between Banach spaces. See, for instance, the Fr\u00e9chet derivative article.In Banach spaces, a large part of the study involves the dual space: the space of all continuous linear maps from the space into its underlying field, so-called functionals. A Banach space can be canonically identified with a subspace of its bidual, which is the dual of its dual space. The corresponding map is an isometry but in general not onto. A general Banach space and its bidual need not even be isometrically isomorphic in any way, contrary to the finite-dimensional situation. This is explained in the dual space article.General Banach spaces are more complicated than Hilbert spaces, and cannot be classified in such a simple manner as those. In particular, many Banach spaces lack a notion analogous to an orthonormal basis.An important object of study in functional analysis are the continuous linear operators defined on Banach and Hilbert spaces. These lead naturally to the definition of C*-algebras and other operator algebras.More generally, functional analysis includes the study of Fr\u00e9chet spaces and other topological vector spaces not endowed with a norm.The basic and historically first class of spaces studied in functional analysis are complete normed vector spaces over the real or complex numbers. Such spaces are called Banach spaces. An important example is a Hilbert space, where the norm arises from an inner product. These spaces are of fundamental importance in many areas, including the mathematical formulation of quantum mechanics.In modern introductory texts to functional analysis, the subject is seen as the study of vector spaces endowed with a topology, in particular infinite-dimensional spaces. In contrast, linear algebra deals mostly with finite-dimensional spaces, and does not use topology. An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis.The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Hadamard's 1910 book on that subject. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra. The theory of nonlinear functionals was continued by students of Hadamard, in particular Fr\u00e9chet and L\u00e9vy. Hadamard also founded the modern school of linear functional analysis further developed by Riesz and the group of Polish mathematicians around Stefan Banach.Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear functions defined on these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.",
            "title": "Functional analysis",
            "url": "https://en.wikipedia.org/wiki/Functional_analysis"
        },
        {
            "desc_links": [
                "/wiki/Unital_algebra",
                "/wiki/Banach_algebra",
                "/wiki/Densely_defined_operator",
                "/wiki/Unbounded_operator",
                "/wiki/Complex_number",
                "/wiki/Closed_operator",
                "/wiki/Complex_number",
                "/wiki/Banach_space",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Vector_space",
                "/wiki/Unilateral_shift",
                "/wiki/Hilbert_space",
                "/wiki/Lp_space",
                "/wiki/Mathematics",
                "/wiki/Functional_analysis",
                "/wiki/Bounded_operator",
                "/wiki/Eigenvalue",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Inverse_function",
                "/wiki/Identity_operator",
                "/wiki/Spectral_theory",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Quantum_mechanics"
            ],
            "links": [
                "/wiki/Banach_algebra",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Complex_number",
                "/wiki/Spectral_measure",
                "/wiki/Decomposition_of_spectrum_(functional_analysis)",
                "/wiki/Closed_operator",
                "/wiki/Closed_graph_theorem",
                "/wiki/Unbounded_operator",
                "/wiki/Banach_space",
                "/wiki/Complement_(set_theory)",
                "/wiki/Hilbert_space",
                "/wiki/Normal_operator",
                "/wiki/Spectral_theorem",
                "/wiki/Compact_operator",
                "/wiki/Hydrogen_atom",
                "/wiki/Molecular_Hamiltonian",
                "/wiki/Bound_state",
                "/wiki/Rydberg_formula",
                "/wiki/Ionization",
                "/wiki/Unilateral_shift",
                "/wiki/Isometry",
                "/wiki/Normal_operator",
                "/wiki/Spectral_theorem",
                "/wiki/Multiplication_operator",
                "/wiki/Bilateral_shift",
                "/wiki/Eigenvalue",
                "/wiki/Spectral_radius",
                "/wiki/Neumann_series",
                "/wiki/Holomorphic_function",
                "/wiki/Liouville%27s_theorem_(complex_analysis)",
                "/wiki/Resolvent_formalism",
                "/wiki/Closed_set",
                "/wiki/Bounded_set",
                "/wiki/Empty_set",
                "/wiki/Complex_plane",
                "/wiki/Unital_algebra",
                "/wiki/Banach_algebra",
                "/wiki/Densely_defined_operator",
                "/wiki/Unbounded_operator",
                "/wiki/Complex_number",
                "/wiki/Closed_operator",
                "/wiki/Complex_number",
                "/wiki/Banach_space",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Vector_space",
                "/wiki/Unilateral_shift",
                "/wiki/Hilbert_space",
                "/wiki/Lp_space",
                "/wiki/Mathematics",
                "/wiki/Functional_analysis",
                "/wiki/Bounded_operator",
                "/wiki/Eigenvalue",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Inverse_function",
                "/wiki/Identity_operator",
                "/wiki/Spectral_theory",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Quantum_mechanics"
            ],
            "text": "Let B be a complex Banach algebra containing a unit e. Then we define the spectrum \u03c3(x) (or more explicitly \u03c3B(x)) of an element x of B to be the set of those complex numbers \u03bb for which \u03bbe\u00a0\u2212\u00a0x is not invertible in B. This extends the definition for bounded linear operators B(X) on a Banach space X, since B(X) is a Banach algebra.Via its spectral measures, one can define a decomposition of the spectrum of any self adjoint operator, bounded or otherwise into absolutely continuous, pure point, and singular parts.However, boundedness of the inverse does follow directly from its existence if one introduces the additional assumption that T is closed; this follows from the closed graph theorem. Therefore, as in the bounded case, a complex number \u03bb lies in the spectrum of a closed operator T if and only if \u03bbI\u00a0\u2212\u00a0T is not bijective. Note that the class of closed operators includes all bounded operators.For \u03bb to be in the resolvent (i.e. not in the spectrum), as in the bounded case \u03bbI\u00a0\u2212\u00a0T must be bijective, since it must have a two-sided inverse. As before if an inverse exists then its linearity is immediate, but in general it may not be bounded, so this condition must be checked separately.The spectrum of an unbounded operator is in general a closed, possibly empty, subset of the complex plane.A complex number \u03bb is then in the spectrum if this property fails to hold. One can classify the spectrum in exactly the same way as in the bounded case.such thathas a bounded inverse, i.e. if there exists a bounded operatorif the operatorOne can extend the definition of spectrum for unbounded operators on a Banach space X, operators which are no longer elements in the Banach algebra B(X). One proceeds in a manner similar to the bounded case. A complex number \u03bb is said to be in the resolvent set, that is, the complement of the spectrum of a linear operatorIf X is a Hilbert space and T is a normal operator, then a remarkable result known as the spectral theorem gives an analogue of the diagonalisation theorem for normal finite-dimensional operators (Hermitian matrices, for example).If T is a compact operator, then it can be shown that any nonzero \u03bb in the spectrum is an eigenvalue. In other words, the spectrum of such an operator, which was defined as a generalization of the concept of eigenvalues, consists in this case only of the usual eigenvalues, plus possibly 0.The hydrogen atom provides an example of this decomposition. The eigenfunctions of the hydrogen atom Hamiltonian are called eigenstates and are grouped into two categories. The bound states of the hydrogen atom correspond to the discrete part of the spectrum (they have a discrete set of eigenvalues that can be computed by the Rydberg formula), whereas the end result of the ionization process is described by the continuous part (the energy of the collision/ionization is not \"quantized\").The peripheral spectrum of an operator is defined as the set of points in its spectrum which have modulus equal to its spectral radius.[2]The set of all \u03bb for which \u03bbI\u00a0-\u00a0T is injective and has dense range, but is not surjective, is called the continuous spectrum of T, denoted by \u03c3c(T) . The continuous spectrum therefore consists of those approximate eigenvalues which are not eigenvalues and do not lie in the residual spectrum. That is,An operator may be injective, even bounded below, but not invertible. The unilateral shift on l 2(N) is such an example. This shift operator is an isometry, therefore bounded below by 1. But it is not invertible as it is not surjective. The set of \u03bb for which \u03bbI\u00a0-\u00a0T is injective but does not have dense range is known as the residual spectrum or compression spectrum of T and is denoted by \u03c3r(T).A unitary operator is normal. By spectral theorem, a bounded operator on a Hilbert space H is normal if and only if it is equivalent (after identification of H with an L^2 space) to a multiplication operator. It can be shown that the approximate point spectrum of a bounded multiplication operator equals its spectrum.Since T is a unitary operator, its spectrum lies on the unit circle. Therefore, the approximate point spectrum of T is its entire spectrum. This is true for a more general class of operators.then ||xn|| = 1 for all n, butwhere the \u02c6 denotes the zero-th position. Direct calculation shows T has no eigenvalues, but every \u03bb with |\u03bb| = 1 is an approximate eigenvalue; letting xn be the vectorExample Consider the bilateral shift T on l2(Z) defined byIt is easy to see that the eigenvalues lie in the approximate point spectrum.The set of approximate eigenvalues is known as the approximate point spectrum, denoted by \u03c3ap(T).More generally, T is not invertible if it is not bounded below; that is, if there is no c\u00a0>\u00a00 such that ||Tx||\u00a0\u2265\u00a0c||x|| for all x \u2208 X. So the spectrum includes the set of approximate eigenvalues, which are those \u03bb such that T -\u03bbI is not bounded below; equivalently, it is the set of \u03bb for which there is a sequence of unit vectors x1, x2, ... for whichIf an operator is not injective (so there is some nonzero x with T(x)\u00a0=\u00a00), then it is clearly not invertible. So if \u03bb is an eigenvalue of T, one necessarily has \u03bb\u00a0\u2208\u00a0\u03c3(T). The set of eigenvalues of T is also called the point spectrum of T, denoted by \u03c3p(T).The following subsections provide more details on the three parts of \u03c3(T) sketched above.Note that the approximate point spectrum and residual spectrum are not necessarily disjoint (however, the point spectrum and the residual spectrum are).A bounded operator T on a Banach space is invertible, i.e. has a bounded inverse, if and only if T is bounded below and has dense range. Accordingly, the spectrum of T can be divided into the following parts:The bound ||T|| on the spectrum can be refined somewhat. The spectral radius, r(T), of T is the radius of the smallest circle in the complex plane which is centered at the origin and contains the spectrum \u03c3(T) inside of it, i.e.The boundedness of the spectrum follows from the Neumann series expansion in \u03bb; the spectrum \u03c3(T) is bounded by ||T||. A similar result shows the closedness of the spectrum.would be defined everywhere on the complex plane and bounded. But it can be shown that the resolvent function R is holomorphic on its domain. By the vector-valued version of Liouville's theorem, this function is constant, thus everywhere zero as it is zero at infinity. This would be a contradiction.If the spectrum were empty, then the resolvent functionThe spectrum of a bounded operator T is always a closed, bounded and non-empty subset of the complex plane.The space of bounded linear operators B(X) on a Banach space X is an example of a unital Banach algebra. Since the definition of the spectrum does not mention any properties of B(X) except those that any such algebra has, the notion of a spectrum may be generalised to this context by using the same definition verbatim.The notion of spectrum extends to densely defined unbounded operators. In this case a complex number \u03bb is said to be in the spectrum of such an operator T:D\u2192X (where D is dense in X) if there is no bounded inverse (\u03bbI\u00a0\u2212\u00a0T)\u22121:X\u2192D. If T is a closed operator (which includes the case that T is a bounded operator), boundedness of such inverses follows automatically if the inverse exists at all.This has no eigenvalues, since if Rx=\u03bbx then by expanding this expression we see that x1=0, x2=0, etc. On the other hand, 0 is in the spectrum because the operator R\u00a0\u2212\u00a00 (i.e. R itself) is not invertible: it is not surjective since any vector with non-zero first component is not in its range. In fact every bounded linear operator on a complex Banach space must have a non-empty spectrum.The spectrum of an operator on a finite-dimensional vector space is precisely the set of eigenvalues. However an operator on an infinite-dimensional space may have additional elements in its spectrum, and may have no eigenvalues. For example, consider the right shift operator R on the Hilbert space \u21132,In mathematics, particularly in functional analysis, the spectrum of a bounded operator is a generalisation of the set of eigenvalues of a matrix. Specifically, a complex number \u03bb is said to be in the spectrum of a bounded linear operator T if \u03bbI\u00a0\u2212\u00a0T is not invertible, where I is the identity operator. The study of spectra and related properties is known as spectral theory, which has numerous applications, most notably the mathematical formulation of quantum mechanics.",
            "title": "Spectrum (functional analysis)",
            "url": "https://en.wikipedia.org/wiki/Spectrum_(functional_analysis)"
        },
        {
            "desc_links": [
                "/wiki/Continuous_linear_operator",
                "/wiki/Bounded_function",
                "/wiki/Locally_bounded_function",
                "/wiki/Operator_norm",
                "/wiki/Functional_analysis",
                "/wiki/Linear_transformation",
                "/wiki/Normed_vector_space",
                "/wiki/Bounded_set"
            ],
            "links": [
                "/wiki/Fr%C3%A9chet_space",
                "/wiki/LF_space",
                "/wiki/Sequentially_continuous",
                "/wiki/Bounded_set_(topological_vector_space)",
                "/wiki/Topological_vector_space",
                "/wiki/Locally_convex_spaces",
                "/wiki/Banach_space",
                "/wiki/Dense_set",
                "/wiki/Continuous_linear_extension",
                "/wiki/Lipschitz_continuous",
                "/wiki/Closed_operator",
                "/wiki/Discontinuous_linear_map",
                "/wiki/Derivative",
                "/wiki/Trigonometric_polynomial",
                "/wiki/Continuous_linear_operator",
                "/wiki/Continuous_linear_operator",
                "/wiki/Bounded_function",
                "/wiki/Locally_bounded_function"
            ],
            "text": "A converse does hold when the domain is pseudometrisable, a case which includes Fr\u00e9chet spaces. For LF spaces, a weaker converse holds; any bounded linear map from an LF space is sequentially continuous.This formulation allows one to define bounded operators between general topological vector spaces as an operator which takes bounded sets to bounded sets. In this context, it is still true that every continuous map is bounded, however the converse fails; a bounded operator need not be continuous. Clearly, this also means that boundedness is no longer equivalent to Lipschitz continuity in this context.The boundedness condition for linear operators on normed spaces can be restated. An operator is bounded if it takes every bounded set to a bounded set, and here is meant the more general condition of boundedness for sets in a topological vector space (TVS): a set is bounded if and only if it is absorbed by every neighborhood of 0. Note that the two notions of boundedness coincide for locally convex spaces.A common procedure for defining a bounded linear operator between two given Banach spaces is as follows. First, define a linear operator on a dense subset of its domain, such that it is locally bounded. Then, extend the operator by continuity to a continuous linear operator on the whole domain.is precisely the condition for L to be Lipschitz continuous at 0 (and hence, everywhere, because L is linear).The condition for L to be bounded, namely that there exists some M such that for all vThat such a basic operator as the derivative (and others) is not bounded makes it harder to study. If, however, one defines carefully the domain and range of the derivative operator, one may show that it is a closed operator. Closed operators are more general than bounded operators but still \"well-behaved\" in many ways.It turns out that this is not a singular example, but rather part of a general rule. Any linear operator defined on a finite-dimensional normed space is bounded. However, given any normed spaces X and Y with X infinite-dimensional and Y not being the zero space, one can find a linear operator which is not continuous from X to Y.Define the operator L:X\u2192X which acts by taking the derivative, so it maps a polynomial P to its derivative P\u2032. Then, forNot every linear operator between normed spaces is bounded. Let X be the space of all trigonometric polynomials defined on [\u2212\u03c0, \u03c0], with the normAs stated in the introduction, a linear operator L between normed spaces X and Y is bounded if and only if it is a continuous linear operator. The proof is as follows.A linear operator between normed spaces is bounded if and only if it is continuous, and by linearity, if and only if it is continuous at zero.A bounded linear operator is generally not a bounded function; the latter would require that the norm of L(v) be bounded for all v, which is not possible unless L(v)=0 for all v. Rather, a bounded linear operator is a locally bounded function.",
            "title": "Bounded operator",
            "url": "https://en.wikipedia.org/wiki/Bounded_operator"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/0_(number)",
                "/wiki/Algebraic_structure"
            ],
            "links": [
                "/wiki/Tensor_product",
                "/wiki/Mathematics",
                "/wiki/Tensor",
                "/wiki/0_(number)",
                "/wiki/Zero_vector",
                "/wiki/Linear_transformation",
                "/wiki/Zero_vector",
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)",
                "/wiki/0_(number)",
                "/wiki/Mathematics",
                "/wiki/Module_(mathematics)",
                "/wiki/Identity_element",
                "/wiki/Addition",
                "/wiki/Integer",
                "/wiki/0_(number)",
                "/wiki/Multiplication",
                "/wiki/Least_element",
                "/wiki/Partially_ordered_set",
                "/wiki/Lattice_(order)",
                "/wiki/Category_of_groups",
                "/wiki/Zero_morphism",
                "/wiki/Category_(mathematics)",
                "/wiki/Function_composition",
                "/wiki/Category_(mathematics)",
                "/wiki/Initial_and_terminal_objects",
                "/wiki/Coproduct",
                "/wiki/Product_(category_theory)",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Principal_ideal",
                "/wiki/Absorbing_element",
                "/wiki/Semigroup",
                "/wiki/Semiring",
                "/wiki/Additive_identity",
                "/wiki/Identity_element",
                "/wiki/Abelian_group",
                "/wiki/Mathematics",
                "/wiki/0_(number)",
                "/wiki/Algebraic_structure"
            ],
            "text": "Taking a tensor product of any tensor with any zero tensor results in another zero tensor. Adding the zero tensor is equivalent to the identity operation.In mathematics, the zero tensor is a tensor, of any order, all of whose components are zero. The zero tensor of order 1 is sometimes known as the zero vector.The zero matrix represents the linear transformation sending all vectors to the zero vector.There is exactly one zero matrix of any given size m\u2009\u00d7\u2009n having entries in a given ring, so when the context is clear one often refers to the zero matrix. In general the zero element of a ring is unique and typically denoted as 0 without any subscript to indicate the parent ring. Hence the examples above represent zero matrices over any ring.In mathematics, particularly linear algebra, a zero matrix is a matrix with all its entries being zero. Some examples of zero matrices areIn mathematics, the zero module is the module consisting of only the additive identity for the module's addition function. In the integers, this identity is zero, which gives the name zero module. That the zero module is in fact a module is simple to show; it is closed under addition and multiplication trivially.A least element in a partially ordered set or lattice may sometimes be called a zero element, and written either as 0 or \u22a5.If a category has a zero object 0, then there are canonical morphisms X \u2192 0 and 0 \u2192 Y, and composing them gives a zero morphism 0XY\u00a0: X \u2192 Y. In the category of groups, for example, zero morphisms are morphisms which always return group identities, thus generalising the function z(x) = 0.A zero morphism in a category is a generalised absorbing element under function composition: any morphism composed with a zero morphism gives a zero morphism. Specifically, if 0XY\u00a0: X \u2192 Y is the zero morphism among morphisms from X to Y, and f\u00a0: A \u2192 X and g\u00a0: Y \u2192 B are arbitrary morphisms, then g \u2218 0XY = 0XB and 0XY \u2218 f = 0AY.A zero object in a category is both an initial and terminal object (and so an identity under both coproducts and products). For example, the trivial structure (containing only the identity) is a zero object in categories where morphisms must map identities to identities. Specific examples include:Many absorbing elements are also additive identities, including the empty set and the zero function. Another important example is the distinguished element 0 in a field or ring, which is both the additive identity and the multiplicative absorbing element, and whose principal ideal is the smallest ideal.An absorbing element in a multiplicative semigroup or semiring generalises the property 0 \u22c5 x = 0. Examples include:An additive identity is the identity element in an additive group. It generalises the property 0 + x = x. Examples include:In mathematics, a zero element is one of several generalizations of the number zero to other algebraic structures. These alternate meanings may or may not reduce to the same thing, depending on the context.",
            "title": "Zero element",
            "url": "https://en.wikipedia.org/wiki/Zero_vector"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Linear_mapping",
                "/wiki/Vector_space",
                "/wiki/Linear_subspace"
            ],
            "links": [
                "/wiki/Irreducible_representation",
                "/wiki/Regular_representation",
                "/wiki/Lattice_(order)",
                "/wiki/Banach_space",
                "/wiki/Invariant_subspace_problem",
                "/wiki/Per_Enflo",
                "/wiki/Invariant_subspace_problem",
                "/wiki/Charles_Read_(mathematician)",
                "/wiki/Hilbert_space",
                "/wiki/Complex_number",
                "/wiki/Bounded_operator",
                "/wiki/Projection_operator",
                "/wiki/Direct_sum_of_vector_spaces",
                "/wiki/Matrix_(math)",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Group_representation",
                "/wiki/Group_representation",
                "/wiki/Lattice_(order)",
                "/wiki/Jordan_canonical_form",
                "/wiki/Fixed_point_(mathematics)",
                "/wiki/Eigenvector",
                "/wiki/Linear_span",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Complex_number",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Algebraically_closed",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Linear_subspace",
                "/wiki/Linear_mapping",
                "/wiki/Mathematics",
                "/wiki/Linear_mapping",
                "/wiki/Vector_space",
                "/wiki/Linear_subspace"
            ],
            "text": "The representation \u03a6' is irreducible if and only if M is a maximal left ideal, since a subspace V \u2282 A/M is an invariant under {\u03a6'(a) | a \u2208 A} if and only if its preimage under the quotient map, V + M, is a left ideal in A.If M is a left ideal of A. Consider the quotient vector space A/M. The left regular representation \u03a6 on M now descends to a representation \u03a6' on A/M. If [b] denotes an equivalence class in A/M, \u03a6'(a)[b] = [ab]. The kernel of the representation \u03a6' is the set {a \u2208 A | ab \u2208 M for all b}.The invariant subspaces of \u03a6 are precisely the left ideals of A. A left ideal M of A gives a subrepresentation of A on M.If A is an algebra, one can define a left regular representation \u03a6 on A: \u03a6(a)b = ab is a homomorphism from A to L(A), the algebra of linear transformations on AIn other words, \u03a3 is triangularizable if there exists a basis such that every element of \u03a3 has an upper-triangular matrix representation in that basis. It follows from Burnside's theorem that every commutative algebra \u03a3 in L(V) is triangularizable. Hence every commuting family in L(V) can be simultaneously upper-triangularized.A nonempty \u03a3 \u2282 L(V) is said to be triangularizable if there exists a basis {e1...en} of V such thatBurnside's theorem is of fundamental importance in linear algebra. One consequence is that every commuting family in L(V) can be simultaneously upper-triangularized.Theorem (Burnside) Assume V is a complex vector space of finite dimension. For every proper subalgebra \u03a3 of L(V), Lat(\u03a3) contains a nontrivial element.Just as the fundamental theorem of algebra ensures that every linear transformation acting on a finite dimensional complex vector space has a nontrivial invariant subspace, the fundamental theorem of noncommutative algebra asserts that Lat(\u03a3) contains nontrivial elements for certain \u03a3.A minimal element in Lat(\u03a3) in said to be a minimal invariant subspace.while the join operation isThe lattice operations are defined in a natural way: for \u03a3' \u2282 \u03a3, the meet operation is defined byGiven a nonempty \u03a3 \u2282 L(V), the invariant subspaces invariant under each element of \u03a3 form a lattice, sometimes called the invariant-subspace lattice of \u03a3 and denoted by Lat(\u03a3).In the more general case where V is hypothesized to be a Banach space, there is an example of an operator without an invariant subspace due to Per Enflo (1976). A concrete example of an operator without an invariant subspace was produced in 1985 by Charles Read.The invariant subspace problem concerns the case where V is a separable Hilbert space over the complex numbers, of dimension > 1, and T is a bounded operator. The problem is to decide whether every such T has a non-trivial, closed, invariant subspace. This problem is unsolved as of 2013[update].Colloquially, a projection that commutes with T \"diagonalizes\" T.If P is a projection (i.e. P2 = P), so is 1\u00a0\u2212\u00a0P, where 1 is the identity operator. It follows from the above that TP = PT if and only if both Ran P and Ran(1\u00a0\u2212\u00a0P) are invariant under T. In that case, T has matrix representationA straightforward calculation shows that W = Ran P, the range of P, is invariant under T if and only if PTP = TP. In other words, a subspace W being an element of Lat(T) is equivalent to the corresponding projection satisfying the relation PTP = TP.Determining whether a given subspace W is invariant under T is ostensibly a problem of geometric nature. Matrix representation allows one to phrase this problem algebraically. The projection operator P onto W is defined by P(w + w') = w, where w \u2208 W and w' \u2208 W'. The projection P has matrix representationit is clear that T21: W \u2192 W' must be zero.Viewing T as an operator matrixIn other words, given an invariant subspace W of T, V can be decomposed into the direct sumwhere the upper-left block T11 is the restriction of T to W.Suppose now W is a T invariant subspace. Pick a basis C = {v1, ..., vk} of W and complete it to a basis B of V. Then, with respect to this basis, the matrix representation of T takes the form:Over a finite dimensional vector space every linear transformation T\u00a0: V \u2192 V can be represented by a matrix once a basis of V has been chosen.As another example, let T \u2208 L(V) and \u03a3 be the algebra generated by {1,\u00a0T}, where 1 is the identity operator. Then Lat(T) = Lat(\u03a3). Because T lies in \u03a3 trivially, Lat(\u03a3) \u2282 Lat(T). On the other hand, \u03a3 consists of polynomials in 1 and T, therefore the reverse inclusion holds as well.Given a representation of a group G on a vector space V, we have a linear transformation T(g)\u00a0: V \u2192 V for every element g of G. If a subspace W of V is invariant with respect to all these transformations, then it is a subrepresentation and the group G acts on W in a natural way.For instance, it is clear that if \u03a3 = L(V), then Lat(\u03a3) = { {0}, V}.More generally, invariant subspaces are defined for sets of operators as subspaces invariant for each operator in the set. Let L(V) denote the algebra of linear transformations on V, and Lat(T) be the family of subspaces invariant under T \u2208 L(V). (The \"Lat\" notation refers to the fact that Lat(T) forms a lattice; see discussion below.) Given a nonempty set \u03a3 \u2282 L(V), one considers the invariant subspaces invariant under each T \u2208 \u03a3. In symbols,As the above examples indicate, the invariant subspaces of a given linear transformation T shed light on the structure of T. When V is a finite dimensional vector space over an algebraically closed field, linear transformations acting on V are characterized (up to similarity) by the Jordan canonical form, which decomposes V into invariant subspaces of T. Many fundamental questions regarding T can be translated to questions about invariant subspaces of T.An invariant vector (fixed point of T), other than 0, spans an invariant subspace of dimension 1. An invariant subspace of dimension 1 will be acted on by T by a scalar, and consists of invariant vectors if and only if that scalar is 1.Let v be an eigenvector of T, i.e. T v = \u03bbv. Then W = span {v} is T invariant. As a consequence of the fundamental theorem of algebra, every linear operator on a complex finite-dimensional vector space with dimension at least 2 has an eigenvector. Therefore, every such linear operator has a non-trivial invariant subspace. The fact that the complex numbers are algebraically closed is required here. Comparing with the previous example, one can see that the invariant subspaces of a linear transformation are dependent upon the underlying scalar field of V.Certainly V itself, and the subspace {0}, are trivially invariant subspaces for every linear operator T\u00a0: V \u2192 V. For certain linear operators there is no non-trivial invariant subspace; consider for instance a rotation of a two-dimensional real vector space.Next we give a few immediate examples of invariant subspaces.If W is T-invariant, we can restrict T to W to arrive at a new linear mappingfrom some vector space V to itself is a subspace W of V such that T(W) is contained in W. An invariant subspace of T is also said to be T invariant.An invariant subspace of a linear mappingTherefore, the condition for existence of a uni-dimensional invariant subspace is expressed as:In mathematics, an invariant subspace of a linear mapping T\u00a0: V \u2192 V from some vector space V to itself is a subspace W of V that is preserved by T; that is, T(W) \u2286 W.",
            "title": "Invariant subspace",
            "url": "https://en.wikipedia.org/wiki/Invariant_subspace"
        },
        {
            "desc_links": [
                "/wiki/Direct_product",
                "/wiki/Index_set",
                "/wiki/Direct_product",
                "/wiki/Abstract_algebra",
                "/wiki/Mathematics",
                "/wiki/Real_coordinate_space",
                "/wiki/Cartesian_plane",
                "/wiki/Abelian_group",
                "/wiki/Abelian_group",
                "/wiki/Ring_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Vector_space"
            ],
            "links": [
                "/wiki/Category_theory",
                "/wiki/Coproduct",
                "/wiki/Category_(mathematics)",
                "/wiki/Biproduct",
                "/wiki/Additive_category",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Module_(mathematics)",
                "/wiki/Group_action",
                "/wiki/Group_(mathematics)",
                "/wiki/Group_representation",
                "/wiki/G-module",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Module_(mathematics)"
            ],
            "text": "General case\u00a0: [7] In category theory the direct sum is often, but not always, the coproduct in the category of the mathematical objects in question. For example, in the category of abelian groups, direct sum is a coproduct. This is also true in the category of modules.In such a category finite products and coproducts agree and the direct sum is either of them, cf. biproduct.An additive category is an abstraction of the properties of the category of modules.[5] [6]The direct sum of group representations generalizes the direct sum of the underlying modules, adding a group action to it. Specifically, given a group G and two representations V and W of G (or, more generally, two G-modules), the direct sum of the representations is V \u2295 W with the action of g \u2208 G given component-wise, i.e.The most familiar examples of this construction occur when considering vector spaces, which are modules over a field. The construction may also be extended to Banach spaces and Hilbert spaces.The direct sum of modules is a construction which combines several modules into a new module.For an infinite family of abelian groups Ai for i \u2208 I, the direct sumThis definition generalizes to direct sums of finitely many abelian groups.",
            "title": "Direct sum",
            "url": "https://en.wikipedia.org/wiki/Direct_sum"
        },
        {
            "desc_links": [
                "/wiki/Abstract_algebra",
                "/wiki/Module_homomorphism",
                "/wiki/Category_theory",
                "/wiki/Morphism",
                "/wiki/Category_of_modules",
                "/wiki/Ring_(mathematics)",
                "/wiki/Map_(mathematics)",
                "/wiki/Origin_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Straight_line",
                "/wiki/Point_(geometry)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rotation_and_reflection_linear_transformations",
                "/wiki/Endomorphism",
                "/wiki/Linear_function",
                "/wiki/Analytic_geometry",
                "/wiki/Mathematics",
                "/wiki/Transformation_(function)",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Scalar_(mathematics)"
            ],
            "links": [
                "/wiki/Compiler_optimizations",
                "/wiki/Parallelizing_compiler",
                "/wiki/Computer_graphics",
                "/wiki/Transformation_matrix",
                "/wiki/Topological_vector_space",
                "/wiki/Normed_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Continuous_linear_operator",
                "/wiki/Bounded_operator",
                "/wiki/Discontinuous_linear_operator",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Tensor",
                "/wiki/Endomorphism",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Euler_characteristic",
                "/wiki/Operator_theory",
                "/wiki/Fredholm",
                "/wiki/Atiyah%E2%80%93Singer_index_theorem",
                "/wiki/Quotient_space_(linear_algebra)",
                "/wiki/Exact_sequence",
                "/wiki/Rank_of_a_matrix",
                "/wiki/Kernel_(matrix)#Subspace_properties",
                "/wiki/Linear_subspace",
                "/wiki/Dimension",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Image_(mathematics)",
                "/wiki/Range_(mathematics)",
                "/wiki/Isomorphism",
                "/wiki/Associative_algebra",
                "/wiki/Group_isomorphism",
                "/wiki/General_linear_group",
                "/wiki/Isomorphism",
                "/wiki/Automorphism",
                "/wiki/Group_(math)",
                "/wiki/Automorphism_group",
                "/wiki/Endomorphisms",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Endomorphism",
                "/wiki/Associative_algebra",
                "/wiki/Ring_(algebra)",
                "/wiki/Identity_function",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_addition",
                "/wiki/Associative_algebra",
                "/wiki/Composition_of_maps",
                "/wiki/Pointwise",
                "/wiki/Inverse_function",
                "/wiki/Relation_composition",
                "/wiki/Class_(set_theory)",
                "/wiki/Morphism",
                "/wiki/Category_(mathematics)",
                "/wiki/Dimension",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Finite-dimensional",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Euclidean_space",
                "/wiki/Abstract_algebra",
                "/wiki/Module_homomorphism",
                "/wiki/Category_theory",
                "/wiki/Morphism",
                "/wiki/Category_of_modules",
                "/wiki/Ring_(mathematics)",
                "/wiki/Map_(mathematics)",
                "/wiki/Origin_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Straight_line",
                "/wiki/Point_(geometry)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rotation_and_reflection_linear_transformations",
                "/wiki/Endomorphism",
                "/wiki/Linear_function",
                "/wiki/Analytic_geometry",
                "/wiki/Mathematics",
                "/wiki/Transformation_(function)",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Scalar_(mathematics)"
            ],
            "text": "Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A\u2032 = B\u22121AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V \u2192 W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 \u2192 V \u2192 W \u2192 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah\u2013Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) \u2212 dim(W), by rank\u2013nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, \u03c1(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or \u03bd(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank\u2013nullity theorem:If f\u00a0: V \u2192 W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n \u00d7 n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n \u00d7 n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V \u2192 V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V \u2192 V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f\u00a0: V \u2192 W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1\u00a0: V \u2192 W and f2\u00a0: V \u2192 W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f\u00a0: V \u2192 W and g\u00a0: W \u2192 Z are linear, then so is their composition g \u2218 f\u00a0: V \u2192 Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 \u00d7 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol \u2217 denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m \u00d7 n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f\u00a0: V \u2192 W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m \u00d7 n matrix, then f(x) = Ax describes a linear map Rn \u2192 Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of\u00a0V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V \u2192 W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.",
            "title": "Linear map",
            "url": "https://en.wikipedia.org/wiki/Linear_map"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Science",
                "/wiki/Engineering",
                "/wiki/System_of_linear_equations",
                "/wiki/Fourier_series",
                "/wiki/Image_compression",
                "/wiki/Partial_differential_equation",
                "/wiki/Coordinate-free",
                "/wiki/Tensor",
                "/wiki/Manifold_(mathematics)",
                "/wiki/Abstract_algebra",
                "/wiki/Analytic_geometry",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Linear_equation",
                "/wiki/Giuseppe_Peano",
                "/wiki/Euclidean_space",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Linear_algebra",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Mathematical_analysis",
                "/wiki/Function_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Topology",
                "/wiki/Continuous_function",
                "/wiki/Norm_(mathematics)",
                "/wiki/Inner_product",
                "/wiki/Metric_(mathematics)",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_vector",
                "/wiki/Physics",
                "/wiki/Force",
                "/wiki/Force_vector",
                "/wiki/Geometry",
                "/wiki/Three-dimensional_space",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Axiom"
            ],
            "links": [
                "/wiki/Parallel_(geometry)",
                "/wiki/Grassmannian_manifold",
                "/wiki/Flag_manifold",
                "/wiki/Flag_(linear_algebra)",
                "/wiki/Nullspace",
                "/wiki/Coset",
                "/wiki/Transitive_group_action",
                "/wiki/Group_action",
                "/wiki/Ring_(mathematics)",
                "/wiki/Multiplicative_inverse",
                "/wiki/Abelian_group",
                "/wiki/Modular_arithmetic",
                "/wiki/Free_module",
                "/wiki/Module_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Division_ring",
                "/wiki/Spectrum_of_a_ring",
                "/wiki/Locally_free_module",
                "/wiki/Cotangent_bundle",
                "/wiki/Cotangent_space",
                "/wiki/Section_(fiber_bundle)",
                "/wiki/Differential_form",
                "/wiki/Tangent_bundle",
                "/wiki/Tangent_space",
                "/wiki/Vector_field",
                "/wiki/Hairy_ball_theorem",
                "/wiki/2-sphere",
                "/wiki/K-theory",
                "/wiki/Division_algebra",
                "/wiki/Quaternion",
                "/wiki/Octonion",
                "/wiki/Fiber_(mathematics)",
                "/wiki/Line_bundle",
                "/wiki/Trivial_bundle",
                "/wiki/Locally",
                "/wiki/Neighborhood_(topology)",
                "/wiki/M%C3%B6bius_strip",
                "/wiki/Homeomorphism#Examples",
                "/wiki/Cylinder_(geometry)",
                "/wiki/Orientable_manifold",
                "/wiki/Topological_space",
                "/wiki/Riemannian_manifold",
                "/wiki/Riemannian_metric",
                "/wiki/Riemann_curvature_tensor",
                "/wiki/Curvature_(mathematics)",
                "/wiki/General_relativity",
                "/wiki/Einstein_curvature_tensor",
                "/wiki/Space-time",
                "/wiki/Compact_Lie_group",
                "/wiki/Tangent_plane",
                "/wiki/Linear_approximation",
                "/wiki/Linearization",
                "/wiki/Differentiable_manifold",
                "/wiki/Fast_Fourier_transform",
                "/wiki/Convolution_theorem",
                "/wiki/Convolution",
                "/wiki/Digital_filter",
                "/wiki/Multiplication_algorithm",
                "/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm",
                "/wiki/Boundary_value_problem",
                "/wiki/Partial_differential_equation",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_equation",
                "/wiki/Sampling_(signal_processing)",
                "/wiki/Discrete_Fourier_transform",
                "/wiki/Digital_signal_processing",
                "/wiki/Radar",
                "/wiki/Speech_encoding",
                "/wiki/Image_compression",
                "/wiki/JPEG",
                "/wiki/Discrete_cosine_transform",
                "/wiki/Superposition_principle",
                "/wiki/Sine_waves",
                "/wiki/Frequency_spectrum",
                "/wiki/Duality_(mathematics)",
                "/wiki/Pontryagin_duality",
                "/wiki/Group_(mathematics)",
                "/wiki/Reciprocal_lattice",
                "/wiki/Lattice_(group)",
                "/wiki/Atom",
                "/wiki/Crystal",
                "/wiki/Fourier_coefficient",
                "/wiki/Periodic_function",
                "/wiki/Trigonometric_functions",
                "/wiki/Fourier_series",
                "/wiki/Hilbert_space",
                "/wiki/Fourier_expansion",
                "/wiki/Dirac_distribution",
                "/wiki/Green%27s_function",
                "/wiki/Fundamental_solution",
                "/wiki/Lax%E2%80%93Milgram_theorem",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Test_function",
                "/wiki/Smooth_function",
                "/wiki/Compact_support",
                "/wiki/Optimization_(mathematics)",
                "/wiki/Minimax_theorem",
                "/wiki/Game_theory",
                "/wiki/Representation_theory",
                "/wiki/Group_theory",
                "/wiki/Distributive_law",
                "/wiki/Symmetric_algebra",
                "/wiki/Exterior_algebra",
                "/wiki/Tensor_algebra",
                "/wiki/Tensor",
                "/wiki/Commutator",
                "/wiki/Cross_product",
                "/wiki/Commutative_algebra",
                "/wiki/Polynomial_ring",
                "/wiki/Commutative",
                "/wiki/Associative",
                "/wiki/Quotient_ring",
                "/wiki/Algebraic_geometry",
                "/wiki/Coordinate_ring",
                "/wiki/Bilinear_operator",
                "/wiki/Banach_algebra",
                "/wiki/Differential_equation",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Quantum_mechanics",
                "/wiki/Partial_differential_equation",
                "/wiki/Wavefunction",
                "/wiki/Eigenvalue",
                "/wiki/Differential_operator",
                "/wiki/Eigenstate",
                "/wiki/Spectral_theorem",
                "/wiki/Compact_operator",
                "/wiki/Taylor_approximation",
                "/wiki/Differentiable_function",
                "/wiki/Stone%E2%80%93Weierstrass_theorem",
                "/wiki/Trigonometric_function",
                "/wiki/Fourier_expansion",
                "/wiki/Closure_(topology)",
                "/wiki/Hilbert_space_dimension",
                "/wiki/Gram%E2%80%93Schmidt_process",
                "/wiki/Orthogonal_basis",
                "/wiki/Euclidean_space",
                "/wiki/David_Hilbert",
                "/wiki/Derivative",
                "/wiki/Sobolev_space",
                "/wiki/Integrable_function",
                "/wiki/Domain_(mathematics)",
                "/wiki/Lp_space",
                "/wiki/Riemann_integral",
                "/wiki/Lebesgue_integral",
                "/wiki/Zero_vector",
                "/wiki/Stefan_Banach",
                "/wiki/Lp_space",
                "/wiki/P-norm",
                "/wiki/Functional_(mathematics)",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Functional_analysis",
                "/wiki/Cauchy_sequence",
                "/wiki/Completeness_(topology)",
                "/wiki/Topology_of_uniform_convergence",
                "/wiki/Weierstrass_approximation_theorem",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Function_space",
                "/wiki/Function_series",
                "/wiki/Modes_of_convergence",
                "/wiki/Pointwise_convergence",
                "/wiki/Uniform_convergence",
                "/wiki/Series_(mathematics)",
                "/wiki/Infinite_sum",
                "/wiki/Topological_space",
                "/wiki/Neighborhood_(topology)",
                "/wiki/Continuous_map",
                "/wiki/Law_of_cosines",
                "/wiki/Dot_product",
                "/wiki/Partial_order",
                "/wiki/Ordered_vector_space",
                "/wiki/Riesz_space",
                "/wiki/Lebesgue_integration",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Infinite_series",
                "/wiki/Functional_analysis",
                "/wiki/Tuple",
                "/wiki/Function_composition",
                "/wiki/Universal_property",
                "/wiki/Tensor",
                "/wiki/Multilinear_algebra",
                "/wiki/Cartesian_product",
                "/wiki/Bilinear_map",
                "/wiki/Derivative",
                "/wiki/Linear_differential_operator",
                "/wiki/Group_(mathematics)",
                "/wiki/Kernel_(algebra)",
                "/wiki/Image_(mathematics)",
                "/wiki/Category_of_vector_spaces",
                "/wiki/Abelian_category",
                "/wiki/Category_(mathematics)",
                "/wiki/Category_of_abelian_groups",
                "/wiki/First_isomorphism_theorem",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Modular_arithmetic",
                "/wiki/If_and_only_if",
                "/wiki/Subset",
                "/wiki/Linear_span",
                "/wiki/Linear_combination",
                "/wiki/Universal_property",
                "/wiki/Characteristic_polynomial",
                "/wiki/Algebraically_closed_field",
                "/wiki/Eigenbasis",
                "/wiki/Jordan_canonical_form",
                "/wiki/Spectral_theorem",
                "/wiki/Endomorphism",
                "/wiki/Kernel_(linear_algebra)",
                "/wiki/Identity_function",
                "/wiki/Determinant",
                "/wiki/Square_matrix",
                "/wiki/Orientation_(mathematics)",
                "/wiki/Matrix_multiplication",
                "/wiki/Bijection",
                "/wiki/Up_to",
                "/wiki/Dual_vector_space",
                "/wiki/Natural_(category_theory)",
                "/wiki/Origin_(mathematics)",
                "/wiki/Coordinate_system",
                "/wiki/Isomorphism",
                "/wiki/Inverse_map",
                "/wiki/Function_composition",
                "/wiki/Identity_function",
                "/wiki/Injective",
                "/wiki/Surjective",
                "/wiki/Function_(mathematics)",
                "/wiki/Degree_of_a_field_extension",
                "/wiki/Countably_infinite",
                "/wiki/A_fortiori",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Zorn%27s_lemma",
                "/wiki/Axiom_of_Choice",
                "/wiki/Zermelo%E2%80%93Fraenkel_set_theory",
                "/wiki/Ultrafilter_lemma",
                "/wiki/Cardinality",
                "/wiki/Dimension_theorem_for_vector_spaces",
                "/wiki/Cartesian_coordinates",
                "/wiki/Coordinate_vector",
                "/wiki/Standard_basis",
                "/wiki/Sequence",
                "/wiki/Index_set",
                "/wiki/Linearly_independent",
                "/wiki/Linear_combination",
                "/wiki/Natural_exponential_function",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Homogeneous_linear_equation",
                "/wiki/Real_line",
                "/wiki/Interval_(mathematics)",
                "/wiki/Subset",
                "/wiki/Continuous_function",
                "/wiki/Integral",
                "/wiki/Differentiability",
                "/wiki/Functional_analysis",
                "/wiki/Polynomial_ring",
                "/wiki/Polynomial_function",
                "/wiki/Complex_plane",
                "/wiki/Complex_numbers",
                "/wiki/Real_numbers",
                "/wiki/Imaginary_unit",
                "/wiki/Coordinate_space",
                "/wiki/Tuple",
                "/wiki/Function_space",
                "/wiki/Henri_Lebesgue",
                "/wiki/Stefan_Banach",
                "/wiki/David_Hilbert",
                "/wiki/Algebra",
                "/wiki/Functional_analysis",
                "/wiki/Lp_space",
                "/wiki/Hilbert_space",
                "/wiki/Giusto_Bellavitis",
                "/wiki/Complex_number",
                "/wiki/Jean-Robert_Argand",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Quaternion",
                "/wiki/Biquaternion",
                "/wiki/Linear_combination",
                "/wiki/Edmond_Laguerre",
                "/wiki/System_of_linear_equations",
                "/wiki/Affine_geometry",
                "/wiki/Coordinate",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Analytic_geometry",
                "/wiki/Curve",
                "/wiki/Bernard_Bolzano",
                "/wiki/Barycentric_coordinate_system_(mathematics)",
                "/wiki/August_Ferdinand_M%C3%B6bius",
                "/wiki/C._V._Mourey",
                "/wiki/Elementary_group_theory",
                "/wiki/Abstract_algebra",
                "/wiki/Abelian_group",
                "/wiki/Module_(mathematics)",
                "/wiki/Ring_homomorphism",
                "/wiki/Endomorphism_ring",
                "/wiki/Closure_(mathematics)",
                "/wiki/Neighborhood_(topology)",
                "/wiki/Angle",
                "/wiki/Distance",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Addition",
                "/wiki/Subtraction",
                "/wiki/Multiplication",
                "/wiki/Division_(mathematics)",
                "/wiki/Rational_number",
                "/wiki/Axiom",
                "/wiki/Field_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Cartesian_coordinates",
                "/wiki/Ordered_pair",
                "/wiki/Arrow",
                "/wiki/Plane_(geometry)",
                "/wiki/Force",
                "/wiki/Velocity",
                "/wiki/Parallelogram",
                "/wiki/Real_number",
                "/wiki/Mathematics",
                "/wiki/Science",
                "/wiki/Engineering",
                "/wiki/System_of_linear_equations",
                "/wiki/Fourier_series",
                "/wiki/Image_compression",
                "/wiki/Partial_differential_equation",
                "/wiki/Coordinate-free",
                "/wiki/Tensor",
                "/wiki/Manifold_(mathematics)",
                "/wiki/Abstract_algebra",
                "/wiki/Analytic_geometry",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Linear_equation",
                "/wiki/Giuseppe_Peano",
                "/wiki/Euclidean_space",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Linear_algebra",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Mathematical_analysis",
                "/wiki/Function_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Topology",
                "/wiki/Continuous_function",
                "/wiki/Norm_(mathematics)",
                "/wiki/Inner_product",
                "/wiki/Metric_(mathematics)",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_vector",
                "/wiki/Physics",
                "/wiki/Force",
                "/wiki/Force_vector",
                "/wiki/Geometry",
                "/wiki/Three-dimensional_space",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Axiom"
            ],
            "text": "The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[106] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.generalizing the homogeneous case b = 0 above.[105] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).If W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x \u2208 W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v \u2208 V. An important example is the space of solutions of a system of inhomogeneous linear equationsRoughly, affine spaces are vector spaces whose origins are not specified.[104] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the mapModules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[102] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term vector space to mean modules over a division ring.[103] The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 \u00d7 R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[100] K-theory studies the isomorphism classes of all vector bundles over some topological space.[101] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.such that for every x in X, the fiber \u03c0\u22121(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X \u00d7 V \u2192 X makes the product X \u00d7 V into a \"trivial\" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of \u03c0 to \u03c0\u22121(U) is isomorphic[nb 16] to the trivial bundle U \u00d7 V \u2192 U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be \"twisted\" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X \u00d7 V). For example, the M\u00f6bius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 \u00d7 R, because the latter is orientable whereas the former is not.[99]A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[94] More precisely, a vector bundle over X is a topological space E equipped with a continuous mapRiemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[95] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[96][97] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[98]The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[94]The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[89] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[90] They in turn are applied in digital filters[91] and as a rapid multiplication algorithm for polynomials and large integers (Sch\u00f6nhage\u2013Strassen algorithm).[92][93]Fourier series are used to solve boundary value problems in partial differential equations.[84] In 1822, Fourier first used this technique to solve the heat equation.[85] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[86] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[87] The JPEG image format is an application of the closely related discrete cosine transform.[88]In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[81] A complex-number form of Fourier series is also commonly used.[80] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[82] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[83]The coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[80]Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][78] The underlying vector space is usually the Hilbert space L2(0, 2\u03c0), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[79] The Fourier expansion of an L2 function f isWhen \u03a9 = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by \u03b4, which associates to a test function f its value at the p: \u03b4(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax\u2013Milgram theorem, a consequence of the Riesz representation theorem).[77]A distribution (or generalized function) is a linear map assigning a number to each \"test\" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[76] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain \u03a9:Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[74] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[75]When a field, F is explicitly stated, a common term used is F-algebra.The multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product \u2297, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 \u2297 v2 and v2 \u2297 v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 \u2297 v2 = \u2212 v2 \u2297 v1 yields the exterior algebra.[73]The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[72] As a vector space, it is spanned by symbols, called simple tensorsExamples include the vector space of n-by-n matrices, with [x, y] = xy \u2212 yx, the commutator of two matrices, and R3, endowed with the cross product.Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[70]General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[69] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone\u2013Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[66] As an example from physics, the time-dependent Schr\u00f6dinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[67] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[68]By definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[63] By the Stone\u2013Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[64] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what \"basic functions\", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram\u2013Schmidt process, it enables one to construct a basis of orthogonal vectors.[65] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[61] The Hilbert space L2(\u03a9), with inner product given byImposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[60]there exists a function f(x) belonging to the vector space Lp(\u03a9) such thatThe space of integrable functions on a given domain \u03a9 (for example an interval) satisfying |f|p < \u221e, and equipped with this norm are called Lebesgue spaces, denoted Lp(\u03a9).[nb 10] These spaces are complete.[59] (If one uses the Riemann integral instead, the space is not complete, which may be seen as a justification for Lebesgue's integration theory.[nb 11]) Concretely this means that for any sequence of Lebesgue-integrable functions f1, f2, ... with |fn|p < \u221e, satisfying the conditionMore generally than sequences of real numbers, functions f: \u03a9 \u2192 R are endowed with a norm that replaces the above sum by the Lebesgue integralis finite. The topologies on the infinite-dimensional space \u2113\u2009p are inequivalent for different p. E.g. the sequence of vectors xn = (2\u2212n, 2\u2212n, ..., 2\u2212n, 0, 0, ...), i.e. the first 2n components are 2\u2212n, the following ones are 0, converges to the zero vector for p = \u221e, but does not for p = 1:Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[58] A first example is the vector space \u2113\u2009p consisting of infinite vectors with real entries x = (x1, x2, ...) whose p-norm (1 \u2264 p \u2264 \u221e) given byFrom a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V \u2192 W, maps between topological vector spaces are required to be continuous.[56] In particular, the (topological) dual space V\u2217 consists of continuous functionals V \u2192 R (or to C). The fundamental Hahn\u2013Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[57]Banach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study\u2014a key piece of functional analysis\u2014focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[55] The image at the right shows the equivalence of the 1-norm and \u221e-norm on R2: as the unit \"balls\" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[53] In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[54] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only ifdenotes the limit of the corresponding finite partial sums of the sequence (fi)i\u2208N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.In such topological vector spaces one can consider series of vectors. The infinite sumConvergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[51][52] Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:Coordinate space Fn can be equipped with the standard dot product:where f+ denotes the positive part of f and f\u2212 the negative part.[48]A vector space may be given a partial order \u2264, under which some vectors can be compared.[47] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functionsFrom the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question\u2014crucial to analysis\u2014whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.These rules ensure that the map f from the V \u00d7 W to V \u2297 W that maps a tuple (v, w) to v \u2297 w is bilinear. The universality states that given any vector space X and any bilinear map g\u00a0: V \u00d7 W \u2192 X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v \u2297 w) = g(v, w).[46] This is called the universal property of the tensor product, an instance of the method\u2014much used in advanced abstract algebra\u2014to indirectly define objects by specifying maps from or to this object.subject to the rulesThe tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensorsThe tensor product V \u2297F W, or simply V \u2297 W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g\u00a0: V \u00d7 W \u2192 X is called bilinear if g is linear in both variables v and w. That is to say, for fixed w the map v \u21a6 g(v, w) is linear in the sense above and likewise for fixed v.The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.the derivatives of the function f appear linearly (as opposed to f\u2032\u2032(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)\u2032 = f\u2032 + g\u2009\u2032 and (c\u00b7f)\u2032 = c\u00b7f\u2032 for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).In the corresponding mapAn important example is the kernel of a linear map x \u21a6 Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equationsand the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.The kernel ker(f) of a linear map f\u00a0: V \u2192 W consists of vectors v that are mapped to 0 in W.[41] Both kernel and image im(f) = {f(v)\u00a0: v \u2208 V} are subspaces of V and W, respectively.[42] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[43] Because of this, many statements such as the first isomorphism theorem (also called rank\u2013nullity theorem in matrix-related terms)The counterpart to subspaces are quotient vector spaces.[40] Given any subspace W \u2282 V, the quotient space V/W (\"V modulo W\") is defined as follows: as a set, it consists of v + W = {v + w\u00a0: w \u2208 W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a \u00b7 (v + W) = (a \u00b7 v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space \"forgets\" information that is contained in the subspace W.A linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n \u2013 1.A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[38][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[39]In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.By spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in \u03bb, called the characteristic polynomial of f.[36] If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[37][nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.Endomorphisms, linear maps f\u00a0: V \u2192 V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying \u03bbv = f(v), where \u03bb is a scalar, is called an eigenvector of f with eigenvalue \u03bb.[nb 5][35] Equivalently, v is an element of the kernel of the difference f \u2212 \u03bb \u00b7 Id (where Id is the identity map V \u2192 V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue \u03bb is equivalent toThe determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.Moreover, after choosing bases of V and W, any linear map f\u00a0: V \u2192 W is uniquely represented by a matrix via this assignment.[33]or, using the matrix multiplication of the matrix A with the coordinate vector x:Matrices are a useful notion to encode linear maps.[32] They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the followingOnce a basis of V is chosen, linear maps f\u00a0: V \u2192 W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no \"canonical\" or preferred isomorphism; actually an isomorphism \u03c6\u00a0: Fn \u2192 V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via \u03c6. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context, see below.Linear maps V \u2192 W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V\u2217.[28] Via the injective natural map V \u2192 V\u2217\u2217, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]For example, the \"arrows in the plane\" and \"ordered pairs of numbers\" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.An isomorphism is a linear map f\u00a0: V \u2192 W such that there exists an inverse map g\u00a0: W \u2192 V, which is a map such that the two possible compositions f \u2218 g\u00a0: W \u2192 W and g \u2218 f\u00a0: V \u2192 V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26] If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.The relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure\u2014i.e., they preserve sums and scalar multiplication:A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(\u03b1) over Q depends on \u03b1. If \u03b1 satisfies some polynomial equation The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e\u2212x and xe\u2212x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo\u2013Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i \u2208 I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. \"Spanning the whole space\" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:yields f(x) = a\u2009e\u2212x + bx\u2009e\u2212x, where a and b are arbitrary constants, and ex is the natural exponential function.are given by triples with arbitrary a, b = a/2, and c = \u22125a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namelySystems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions ofand similarly for multiplication. Such function spaces occur in many geometric situations, when \u03a9 is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15] Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:Functions from any fixed set \u03a9 to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given byIn fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c \u22c5 (x + iy) = (c \u22c5 x) + i(c \u22c5 y) for real numbers x, y, a, b and c. The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.A vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of n-tuples (sequences of length n) of elements of F, such asAn important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by M\u00f6bius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example the zero vector 0 of V and the additive inverse \u2212v of any vector v are unique. Other properties follow from the distributive law, for example av equals 0 if and only if a equals 0 or v equals 0.In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]Vector addition and scalar multiplication are operations, satisfying the closure property: u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.When the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.Subtraction of two vectors and division by a (non-zero) scalar can be defined asLikewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:To qualify as a vector space, the set\u00a0V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.Elements of V are commonly called vectors. Elements of\u00a0F are commonly called scalars.A vector space over a field F is a set\u00a0V together with two operations that satisfy the eight axioms listed below.In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.andA second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (\u22121)v = \u2212v has the opposite direction and the same length as v (blue vector pointing down in the right image).The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.The concept of vector space will first be explained by describing two particular examples:Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied (\"scaled\") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.",
            "title": "Vector space",
            "url": "https://en.wikipedia.org/wiki/Vector_space"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Field (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Field_(algebra)"
        },
        {
            "desc_links": [
                "/wiki/Scalar_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Tensor",
                "/wiki/Quaternion",
                "/wiki/Inner_product",
                "/wiki/Scalar_multiplication",
                "/wiki/Inner_product_space",
                "/wiki/Linear_algebra",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Vector_(mathematics_and_physics)"
            ],
            "links": [
                "/wiki/Scaling_(geometry)",
                "/wiki/Linear_transformation",
                "/wiki/Manifold",
                "/wiki/Section_(fiber_bundle)",
                "/wiki/Tangent_bundle",
                "/wiki/Algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Commutative",
                "/wiki/Module_(mathematics)",
                "/wiki/Norm_(mathematics)",
                "/wiki/Normed_vector_space",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Isomorphism",
                "/wiki/Coordinate_vector_space",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Rational_number",
                "/wiki/Algebraic_number",
                "/wiki/Finite_field",
                "/wiki/Oxford_English_Dictionary",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Quaternion",
                "/wiki/Latin_language",
                "/wiki/Fran%C3%A7ois_Vi%C3%A8te",
                "/wiki/Wikipedia:Citing_sources",
                "/wiki/Scalar_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Tensor",
                "/wiki/Quaternion",
                "/wiki/Inner_product",
                "/wiki/Scalar_multiplication",
                "/wiki/Inner_product_space",
                "/wiki/Linear_algebra",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Vector_(mathematics_and_physics)"
            ],
            "text": "Operations that apply to a single value at a time.The scalar multiplication of vector spaces and modules is a special case of scaling, a kind of linear transformation.In this case the \"scalars\" may be complicated objects. For instance, if R is a ring, the vectors of the product space Rn can be made into a module with the n\u00d7n matrices with entries from R as the scalars. Another example comes from manifold theory, where the space of sections of the tangent bundle forms a module over the algebra of real functions on the manifold.When the requirement that the set of scalars form a field is relaxed so that it need only form a ring (so that, for example, the division of scalars need not be defined, or the scalars need not be commutative), the resulting more general algebraic structure is called a module.The norm is usually defined to be an element of V's scalar field K, which restricts the latter to fields that support the notion of sign. Moreover, if V has dimension 2 or more, K must be closed under square root, as well as the four arithmetic operations; thus the rational numbers Q are excluded, but the surd field is acceptable. For this reason, not every scalar product space is a normed vector space.Alternatively, a vector space V can be equipped with a norm function that assigns to every vector v in V a scalar ||v||. By definition, multiplying v by a scalar k also multiplies its norm by |k|. If ||v|| is interpreted as the length of v, this operation can be described as scaling the length of v by k. A vector space equipped with a norm is called a normed vector space (or normed linear space).According to a fundamental theorem of linear algebra, every vector space has a basis. It follows that every vector space over a scalar field K is isomorphic to a coordinate vector space where the coordinates are elements of K. For example, every real vector space of dimension n is isomorphic to n-dimensional real space Rn.The scalars can be taken from any field, including the rational, algebraic, real, and complex numbers, as well as finite fields.According to a citation in the Oxford English Dictionary the first recorded usage of the term \"scalar\" in English came with W. R. Hamilton in 1846, referring to the real part of a quaternion:The word scalar derives from the Latin word scalaris, an adjectival form of scala (Latin for \"ladder\"), from which the English word scale also comes. The first recorded usage of the word \"scalar\" in mathematics occurs in Fran\u00e7ois Vi\u00e8te's Analytic Art (In artem analyticem isagoge) (1591):[5][page\u00a0needed][6]The term scalar matrix is used to denote a matrix of the form kI where k is a scalar and I is the identity matrix.The term is also sometimes used informally to mean a vector, matrix, tensor, or other usually \"compound\" value that is actually reduced to a single component. Thus, for example, the product of a 1\u00d7n matrix and an n\u00d71 matrix, which is formally a 1\u00d71 matrix, is often said to be a scalar.The real component of a quaternion is also called its scalar part.A scalar product operation \u2013\u00a0not to be confused with scalar multiplication\u00a0\u2013 may be defined on a vector space, allowing two vectors to be multiplied to produce a scalar. A vector space equipped with a scalar product is called an inner product space.In linear algebra, real numbers or other elements of a field are called scalars and relate to vectors in a vector space through the operation of scalar multiplication, in which a vector can be multiplied by a number to produce another vector.[2][3][4] More generally, a vector space may be defined by using any field instead of real numbers, such as complex numbers. Then the scalars of that vector space will be the elements of the associated field.A scalar is an element of a field which is used to define a vector space. A quantity described by multiple scalars, such as having both direction and magnitude, is called a vector.[1]",
            "title": "Scalar (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Scalar_(mathematics)"
        },
        {
            "desc_links": [
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Boundary_value_problem#boundary_value_conditions",
                "/wiki/Mathematics",
                "/wiki/Linear_map",
                "/wiki/Function_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Eigenvalues_and_eigenvectors"
            ],
            "links": [
                "/wiki/LTI_system_theory",
                "/wiki/Stationary_state",
                "/wiki/Wave_function",
                "/wiki/Hamiltonian_(quantum_mechanics)",
                "/wiki/Quantum_mechanics",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Harmonic",
                "/wiki/Overtone",
                "/wiki/Separation_of_variables",
                "/wiki/Wave_equation",
                "/wiki/Vibrating_string",
                "/wiki/String_instrument",
                "/wiki/Infinitesimal",
                "/wiki/Partial_differential_equation",
                "/wiki/Sturm-Liouville_theory",
                "/wiki/Gram-Schmidt_process",
                "/wiki/Dirac_delta_function",
                "/wiki/Hermitian_matrix",
                "/wiki/Self-adjoint_operator",
                "/wiki/Hilbert_space",
                "/wiki/Fourier_series",
                "/wiki/Kronecker_delta",
                "/wiki/Identity_matrix",
                "/wiki/Orthonormal_basis",
                "/wiki/Complex_conjugate",
                "/wiki/Inner_product",
                "/wiki/Degenerate_energy_levels#mathematics",
                "/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis",
                "/wiki/Spectrum_(functional_analysis)",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Boundary_value_problem#boundary_value_conditions",
                "/wiki/Mathematics",
                "/wiki/Linear_map",
                "/wiki/Function_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Eigenvalues_and_eigenvectors"
            ],
            "text": "In the study of signals and systems, an eigenfunction of a system is a signal f(t) that, when input into the system, produces a response y(t) = \u03bbf(t), where \u03bb is a complex scalar eigenvalue.[12]The success of the Schr\u00f6dinger equation in explaining the spectral characteristics of hydrogen is considered one of the greatest triumphs of 20th century physics.or, for a system with a continuous spectrum,The Hamiltonian operator H is an example of a Hermitian operator whose eigenfunctions form an orthonormal basis. When the Hamiltonian does not depend explicitly on time, general solutions of the Schr\u00f6dinger equation are linear combinations of the stationary states multiplied by the oscillatory T(t),[11]Equation (2) is the time-independent Schr\u00f6dinger equation. The eigenfunctions \u03c6k of the Hamiltonian operator are stationary states of the quantum mechanical system, each with a corresponding energy Ek. They represent allowable energy states of the system and may be constrained by boundary conditions.Both of these differential equations are eigenvalue equations with eigenvalue E. As shown in an earlier example, the solution of Equation (3) is the exponentialcan be solved by separation of variables if the Hamiltonian does not depend explicitly on time.[10] In that case, the wave function \u03a8(r,t) = \u03c6(r)T(t) leads to the two differential equations,with the Hamiltonian operatorIn quantum mechanics, the Schr\u00f6dinger equationIn the example of a string instrument, the frequency \u03c9n is the frequency of the nth harmonic, which is called the (n \u2212 1)th overtone.This last boundary condition constrains \u03c9 to take a value \u03c9n = nc\u03c0/L, where n is any integer. Thus, the clamped string supports a family of standing waves of the formIf we impose boundary conditions, for example that the ends of the string are fixed at x = 0 and x = L, namely X(0) = X(L) = 0, and that T(0) = 0, we constrain the eigenvalues. For these boundary conditions, sin(\u03c6) = 0 and sin(\u03c8) = 0, so the phase angles \u03c6 = \u03c8 = 0, andwhere the phase angles \u03c6 and \u03c8 are arbitrary real constants.This problem is amenable to the method of separation of variables. If we assume that h(x, t) can be written as the product of the form X(x)T(t), we can form a pair of ordinary differential equations:which is called the (one-dimensional) wave equation. Here c is a constant speed that depends on the tension and mass of the string.Let h(x, t) denote the sideways displacement of a stressed elastic chord, such as the vibrating strings of a string instrument, as a function of the position x along the string and of time t. Applying the laws of mechanics to infinitesimal portions of the string, the function h satisfies the partial differential equationAs a consequence, in many important cases, the eigenfunctions of the Hermitian operator form an orthonormal basis. In these cases, an arbitrary function can be expressed as a linear combination of the eigenfunctions of the Hermitian operator.For many Hermitian operators, notably Sturm-Liouville operators, a third property isThe second condition always holds for \u03bbi \u2260 \u03bbj. For degenerate eigenfunctions with the same eigenvalue \u03bbi, orthogonal eigenfunctions can always be chosen that span the eigenspace associated with \u03bbi, for example by using the Gram-Schmidt process.[5] Depending on whether the spectrum is discrete or continuous, the eigenfunctions can be normalized by setting the inner product of the eigenfunctions equal to either a Kronecker delta or a Dirac delta function, respectively.[8][9]Consider the Hermitian operator D with eigenvalues \u03bb1, \u03bb2, ... and corresponding eigenfunctions f1(t), f2(t), ... . This Hermitian operator has the following properties:By analogy with Hermitian matrices, D is a Hermitian operator if Aij = Aji*, or[6]integrated over some range of interest for t denoted \u03a9.Many of the operators encountered in physics are Hermitian. Suppose the linear operator D acts on a function space that is a Hilbert space with an orthonormal basis given by the set of functions {u1(t), u2(t), ..., un(t)}, where n may be infinite. In this basis, the operator D has a matrix representation A with elementsThis is the matrix multiplication Ab = c written in summation notation and is a matrix equivalent of the operator D acting upon the function f(t) expressed in the orthonormal basis. If f(t) is an eigenfunction of D with eigenvalue \u03bb, then Ab = \u03bbb.Taking the inner product of each side of this equation with an arbitrary basis function ui(t),We can write the function Df(t) either as a linear combination of the basis functions or as D acting upon the expansion of f(t),Additionally, define a matrix representation of the linear operator D with elementsfor example through a Fourier expansion of f(t). The coefficients bj can be stacked into an n by 1 column vector b = [b1 b2 ... bn]T. In some special cases, such as the coefficients of the Fourier series of a sinusoidal function, this column vector has finite dimension.Functions can be written as a linear combination of the basis functions,where \u03b4ij is the Kronecker delta and can be thought of as the elements of the identity matrix.Suppose the function space has an orthonormal basis given by the set of functions {u1(t), u2(t), ..., un(t)}, where n may be infinite. For the orthonormal basis,integrated over some range of interest for t called \u03a9. The * denotes the complex conjugate.Define the inner product in the function space on which D is defined asEigenfunctions can be expressed as column vectors and linear operators can be expressed as matrices, although they may have infinite dimensions. As a result, many of the concepts related to eigenvectors of matrices carry over to the study of eigenfunctions.where \u03bb = 2 is the only eigenvalue of the differential equation that also satisfies the boundary condition.is the eigenfunction of the derivative operator, where f0 is a parameter that depends on the boundary conditions. Note that in this case the eigenfunction is itself a function of its associated eigenvalue \u03bb, which can take any real or complex value. In particular, note that for \u03bb = 0 the eigenfunction f(t) is a constant.Each value of \u03bb corresponds to one or more eigenfunctions. If multiple linearly independent eigenfunctions have the same eigenvalue, the eigenvalue is said to be degenerate and the maximum number of linearly independent eigenfunctions associated with the same eigenvalue is the eigenvalue's degree of degeneracy or geometric multiplicity.[4][5]where \u03bb is a scalar.[1][2][3] The solutions to Equation (1) may also be subject to boundary conditions. Because of the boundary conditions, the possible values of \u03bb are generally limited, for example to a discrete set \u03bb1, \u03bb2, ... or to a continuous set over some range. The set of all possible eigenvalues of D is sometimes called its spectrum, which may be discrete, continuous, or a combination of both.[1]In general, an eigenvector of a linear operator D defined on some vector space is a nonzero vector in the domain of D that, when D acts upon it, is simply scaled by some scalar value called an eigenvalue. In the special case where D is defined on a function space, the eigenvectors are referred to as eigenfunctions. That is, a function f is an eigenfunction of D if it satisfies the equationAn eigenfunction is a type of eigenvector.for some scalar eigenvalue \u03bb.[1][2][3] The solutions to this equation may also be subject to boundary conditions that limit the allowable eigenvalues and eigenfunctions.In mathematics, an eigenfunction of a linear operator D defined on some function space is any non-zero function f in that space that, when acted upon by D, is only multiplied by some scaling factor called an eigenvalue. As an equation, this condition can be written as",
            "title": "Eigenfunction",
            "url": "https://en.wikipedia.org/wiki/Eigenfunction"
        },
        {
            "desc_links": [
                "/wiki/Graph_of_a_function",
                "/wiki/Asymptote",
                "/wiki/Slope",
                "/wiki/Tangent",
                "/wiki/Inverse_function",
                "/wiki/Natural_logarithm",
                "/wiki/Pure_mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Walter_Rudin",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Engineering",
                "/wiki/Mathematical_biology",
                "/wiki/Economics",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Mathematical_object",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Characterization_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Natural_logarithm"
            ],
            "links": [
                "/wiki/Hyperbolic_tangent",
                "/wiki/Lnp1",
                "/wiki/Hewlett-Packard",
                "/wiki/HP-41C",
                "/wiki/Computer_algebra_system",
                "/wiki/C99",
                "/wiki/William_Kahan",
                "/wiki/Taylor_series",
                "/wiki/Transcendental_function",
                "/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Matrix_exponential",
                "/wiki/Banach_algebra",
                "/wiki/Multivalued_function",
                "/wiki/Exponentiation#Failure_of_power_and_logarithm_identities",
                "/wiki/Line_(mathematics)",
                "/wiki/Logarithmic_spiral",
                "/wiki/Origin_(mathematics)",
                "/wiki/Exponentiation#Failure_of_power_and_logarithm_identities",
                "/wiki/Complex_logarithm",
                "/wiki/Multivalued_function",
                "/wiki/Euler%27s_formula",
                "/wiki/Trigonometric_functions",
                "/wiki/Cauchy_product",
                "/wiki/Cauchy_product",
                "/wiki/Real_number",
                "/wiki/Complex_plane",
                "/wiki/Generalized_continued_fraction",
                "/wiki/Continued_fraction",
                "/wiki/Euler%27s_continued_fraction_formula",
                "/wiki/Chain_rule",
                "/wiki/Proportionality_(mathematics)",
                "/wiki/Malthusian_catastrophe",
                "/wiki/Interest",
                "/wiki/Radioactive_decay",
                "/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem",
                "/wiki/Entire_function",
                "/wiki/Complex_plane",
                "/wiki/Euler%27s_formula",
                "/wiki/Trigonometric_functions",
                "/wiki/Matrix_exponential",
                "/wiki/Banach_algebra",
                "/wiki/Lie_algebra",
                "/wiki/Derivative",
                "/wiki/Exponentiation",
                "/wiki/Leonhard_Euler",
                "/wiki/Characterizations_of_the_exponential_function",
                "/wiki/Series_(mathematics)",
                "/wiki/Differential_equation",
                "/wiki/Limit_of_a_function",
                "/wiki/Johann_Bernoulli",
                "/wiki/Exponential_growth",
                "/wiki/Exponential_decay",
                "/wiki/Proportionality_(mathematics)",
                "/wiki/Continuously_compounded_interest",
                "/wiki/Jacob_Bernoulli",
                "/wiki/Differential_equation",
                "/wiki/Pure_mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Walter_Rudin",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Engineering",
                "/wiki/Mathematical_biology",
                "/wiki/Economics",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Mathematical_object",
                "/wiki/E_(mathematical_constant)"
            ],
            "text": "gives a high precision value for small values of x on systems that do not implement expm1(x).An identity in terms of the hyperbolic tangent,A similar approach has been used for the logarithm (see lnp1).[nb 1]This were first implemented in 1979 in the Hewlett-Packard HP-41C calculator, and provided by several calculators,[12][13] computer algebra systems and programming languages (for example C99).[14]Following a proposal by William Kahan, it may thus be useful to have a dedicated program, often called expm1, for computing ex\u22121 directly, without passing by the computation of the exponential. For example, if the exponential is computed by using its Taylor seriesThe function ez is transcendental over C(z).For n distinct complex numbers {a1, \u2026, an}, the set {ea1z, \u2026, eanz} is linearly independent over C(z).The function ez is not in C(z) (i.e., is not the quotient of two polynomials with complex coefficients).The identity exp(x + y) = exp(x)exp(y) can fail for Lie algebra elements x and y that do not commute; the Baker\u2013Campbell\u2013Hausdorff formula supplies the necessary correction terms.Or ex can be defined as f(1), where f: R\u2192B is the solution to the differential equation f \u2032(t) = xf(t) with initial condition f(0) = 1.Some alternative definitions lead to the same function. For instance, ex can be defined asThe power series definition of the exponential function makes sense for square matrices (for which the function is called the matrix exponential) and more generally in any Banach algebra B. In this setting, e0 = 1, and ex is invertible with inverse e\u2212x for any x in B. If xy = yx, then ex + y = exey, but this identity can fail for noncommuting x and y.However, when b is not an integer, this function is multivalued, because \u03b8 is not unique (see failure of power and logarithm identities).Complex exponentiation ab can be defined by converting a to polar coordinates and using the identity (eln(a))b = ab:The exponential function maps any line in the complex plane to a logarithmic spiral in the complex plane with the center at the origin. Two special cases might be noted: when the original line is parallel to the real axis, the resulting spiral never closes in on itself; when the original line is parallel to the imaginary axis, the resulting spiral is a circle of some radius.See failure of power and logarithm identities for more about problems with combining powers.for all complex numbers z and w. This is also a multivalued function, even when z is real. This distinction is problematic, as the multivalued functions log z and zw are easily confused with their single-valued equivalents when substituting a real number for z. The rule about multiplying exponents for the case of positive real numbers must be modified in a multivalued context:We can then define a more general exponentiation:Extending the natural logarithm to complex arguments yields the complex logarithm log z, which is a multivalued function.When its domain is extended from the real line to the complex plane, the exponential function retains the following properties:where exp, cos, and sin on the right-hand side of the definition sign are to be interpreted as functions of a real variable, previously defined by other means.[11]These definitions for the exponential and trigonometric functions lead trivially to Euler's formula:The definition of the complex exponential function in turn leads to the appropriate definitions extending the trigonometric functions to complex arguments.Termwise multiplication of two copies of these power series in the Cauchy sense, permitted by Mertens' theorem, shows that the defining multiplicative property of exponential functions continues to hold for all complex arguments:As in the real case, the exponential function can be defined on the complex plane in several equivalent forms. The most common definition of the complex exponential function parallels the power series definition for real arguments, where the real variable is replaced by a complex one:This formula also converges, though more slowly, for z > 2. For example:with a special case for z = 2:or, by applying the substitution z = x/y:The following generalized continued fraction for ez converges more quickly:[9]A continued fraction for ex can be obtained via an identity of Euler:Furthermore, for any differentiable function f(x), we find, by the chain rule:If a variable's growth or decay rate is proportional to its size\u2014as is the case in unlimited population growth (see Malthusian catastrophe), continuously compounded interest, or radioactive decay\u2014then the variable can be written as a constant times an exponential function of time. Explicitly for any real constant k, a function f: R \u2192 R satisfies f\u2032 = kf if and only if f(x) = cekx for some constant c.Functions of the form cex for constant c are the only functions that are equal to their derivative (by the Picard\u2013Lindel\u00f6f theorem). Other ways of saying the same thing include:The importance of the exponential function in mathematics and the sciences stems mainly from its definition as the unique function which is equal to its derivative and is equal to 1 when x = 0. That is,The exponential function extends to an entire function on the complex plane. Euler's formula relates its values at purely imaginary arguments to trigonometric functions. The exponential function also has analogues for which the argument is a matrix, or even an element of a Banach algebra or a Lie algebra.The derivative (rate of change) of the exponential function is the exponential function itself. More generally, a function with a rate of change proportional to the function itself (rather than equal to it) is expressible in terms of the exponential function. This function property leads to exponential growth and exponential decay.which justifies the notation ex.From any of these definitions it can be shown that the exponential function obeys the basic exponentiation identity,first given by Euler.[7] This is one of a number of characterizations of the exponential function; others involve series or differential equations.If a principal amount of 1 earns interest at an annual rate of x compounded monthly, then the interest earned each month is x/12 times the current value, so each month the total value is multiplied by (1 + x/12), and the value at the end of the year is (1 + x/12)12. If instead interest is compounded daily, this becomes (1 + x/365)365. Letting the number of time intervals per year grow without bound leads to the limit definition of the exponential function,now known as e. Later, in 1697, Johann Bernoulli studied the calculus of the exponential function.[8]The exponential function arises whenever a quantity grows or decays at a rate proportional to its current value. One such situation is continuously compounded interest, and in fact it was this observation that led Jacob Bernoulli in 1683[8] to the numberThe exponential function can also be defined as the following limit:[7]Less commonly, the real exponential function is defined as the solution y to the equationsuch that y(0) = 1.The exponential function is often defined as the unique solution of the differential equationIts ubiquitous occurrence in pure and applied mathematics has led mathematician W. Rudin to opine that the exponential function is \"the most important function in mathematics\".[3] In applied settings, exponential functions model a relationship in which a constant change in the independent variable gives the same proportional change (i.e. percentage increase or decrease) in the dependent variable. This occurs widely in the natural and social sciences; thus, the exponential function also appears in a variety of contexts within physics, chemistry, engineering, mathematical biology, and economics.The argument of the exponential function can be any real or complex number or even an entirely different kind of mathematical object (e.g., a matrix).The exponential function satisfies the fundamental multiplicative identitySince changing the base of the exponential function merely results in the appearance of an additional constant factor, it is computationally convenient to reduce the study of exponential functions in mathematical analysis to the study of this particular function, conventionally called the \"natural exponential function\",[1][2] or simply, \"the exponential function\" and denoted byThe constant e\u00a0\u2248\u00a02.71828... is the unique base for which the constant of proportionality is 1, so that the function's derivative is itself:In mathematics, an exponential function is a function of the form",
            "title": "Exponential function",
            "url": "https://en.wikipedia.org/wiki/Exponential_function"
        },
        {
            "desc_links": [
                "/wiki/Pythagorean_theorem",
                "/wiki/Parallelogram_law",
                "/wiki/Altitude_(triangle)",
                "/wiki/Coordinate_axes",
                "/wiki/Orthonormal_basis",
                "/wiki/Countably_infinite",
                "/wiki/Infinite_sequence",
                "/wiki/Lp_norm",
                "/wiki/Linear_operator",
                "/wiki/Spectrum_(functional_analysis)",
                "/wiki/Mathematics",
                "/wiki/Physics",
                "/wiki/Infinite-dimensional",
                "/wiki/Function_space",
                "/wiki/David_Hilbert",
                "/wiki/Erhard_Schmidt",
                "/wiki/Frigyes_Riesz",
                "/wiki/Partial_differential_equation",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Fourier_analysis",
                "/wiki/Signal_processing",
                "/wiki/Ergodic_theory",
                "/wiki/Thermodynamics",
                "/wiki/John_von_Neumann",
                "/wiki/Functional_analysis",
                "/wiki/Lp_space",
                "/wiki/Sequence_space",
                "/wiki/Sobolev_space",
                "/wiki/Generalized_function",
                "/wiki/Hardy_space",
                "/wiki/Holomorphic_function",
                "/wiki/Mathematics",
                "/wiki/David_Hilbert",
                "/wiki/Euclidean_space",
                "/wiki/Linear_algebra",
                "/wiki/Calculus",
                "/wiki/Plane_(geometry)",
                "/wiki/Dimension",
                "/wiki/Vector_space",
                "/wiki/Mathematical_structure",
                "/wiki/Inner_product_space",
                "/wiki/Complete_metric_space",
                "/wiki/Limit_(mathematics)"
            ],
            "links": [
                "/wiki/Riesz_potential",
                "/wiki/Bessel_potential",
                "/wiki/Resolvent_operator",
                "/wiki/Continuous_functional_calculus",
                "/wiki/Pseudodifferential_operators",
                "/wiki/Spectral_mapping_theorem",
                "/wiki/Resolution_of_the_identity",
                "/wiki/Riemann%E2%80%93Stieltjes_integral",
                "/wiki/Integral_equation",
                "/wiki/Hilbert%E2%80%93Schmidt_operator",
                "/wiki/Spectral_theorem",
                "/wiki/Compact_operator",
                "/wiki/Compact_operator_on_Hilbert_space#Spectral_theorem",
                "/wiki/Spectrum_of_an_operator",
                "/wiki/Compact_set",
                "/wiki/Spectral_theory",
                "/wiki/Symmetric_matrix",
                "/wiki/Monotone_function",
                "/wiki/Closure_(topology)",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Galois_connection",
                "/wiki/Partial_order",
                "/wiki/Topological_vector_space",
                "/wiki/Inclusion_mapping",
                "/wiki/Natural_transformation",
                "/wiki/Closed_set",
                "/wiki/Quantum_field_theory",
                "/wiki/Wightman_axioms",
                "/wiki/Degrees_of_freedom_(mechanics)",
                "/wiki/Bosonic_field",
                "/wiki/Separable_space",
                "/wiki/Countable",
                "/wiki/Cardinal_number",
                "/wiki/Zorn%27s_lemma",
                "/wiki/Cardinal_number",
                "/wiki/Cardinal_number",
                "/wiki/Parseval_identity",
                "/wiki/Series_(mathematics)#Summations_over_arbitrary_index_sets",
                "/wiki/Supremum",
                "/wiki/Linear_algebra",
                "/wiki/Hamel_basis",
                "/wiki/Countable_set",
                "/wiki/Linearly_independent",
                "/wiki/Orthonormal_basis",
                "/wiki/Tensor_product",
                "/wiki/Simple_tensor",
                "/wiki/Spectral_theorem",
                "/wiki/Compact_operator",
                "/wiki/Fock_space",
                "/wiki/Degrees_of_freedom_(mechanics)",
                "/wiki/Representation_theory",
                "/wiki/Peter%E2%80%93Weyl_theorem",
                "/wiki/Unitary_representation",
                "/wiki/Compact_group",
                "/wiki/Idempotent",
                "/wiki/Cartesian_product",
                "/wiki/Ordered_pair",
                "/wiki/Direct_sum_of_modules#Direct_sum_of_Hilbert_spaces",
                "/wiki/Momentum",
                "/wiki/Position_operator",
                "/wiki/Self-adjoint_operator",
                "/wiki/Unbounded_operator",
                "/wiki/Quantum_mechanics",
                "/wiki/Densely_defined_operator",
                "/wiki/Homotopy",
                "/wiki/Differential_geometry",
                "/wiki/Atiyah%E2%80%93Singer_index_theorem",
                "/wiki/Compact_operator",
                "/wiki/Relatively_compact",
                "/wiki/Integral_operator",
                "/wiki/Hilbert%E2%80%93Schmidt_operator",
                "/wiki/Integral_equation",
                "/wiki/Fredholm_operator",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Cokernel",
                "/wiki/Unitary_operator",
                "/wiki/Group_(mathematics)",
                "/wiki/Isometry_group",
                "/wiki/Partial_order",
                "/wiki/Square_root_of_a_matrix",
                "/wiki/C*-algebra",
                "/wiki/Operator_algebra",
                "/wiki/Hermitian_adjoint",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Linear_operator",
                "/wiki/Bounded_set",
                "/wiki/Bounded_linear_operator",
                "/wiki/Norm_(mathematics)",
                "/wiki/Operator_norm",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Hyperplane",
                "/wiki/Banach_space",
                "/wiki/Open_mapping_theorem_(functional_analysis)",
                "/wiki/Continuous_function",
                "/wiki/Surjective",
                "/wiki/Open_mapping",
                "/wiki/Bounded_inverse_theorem",
                "/wiki/Bijective",
                "/wiki/Closed_graph_theorem",
                "/wiki/Closed_set",
                "/wiki/Unbounded_operator",
                "/wiki/Closed_operator",
                "/wiki/Direct_method_in_the_calculus_of_variations",
                "/wiki/Calculus_of_variations",
                "/wiki/Weak_topology",
                "/wiki/Eberlein%E2%80%93%C5%A0mulian_theorem",
                "/wiki/Alaoglu%27s_theorem",
                "/wiki/Convex_function",
                "/wiki/Bolzano%E2%80%93Weierstrass_theorem",
                "/wiki/Uniform_boundedness_principle",
                "/wiki/Weak_topology#Weak_convergence",
                "/wiki/Banach_space",
                "/wiki/Reflexive_space",
                "/wiki/Dual_space",
                "/wiki/Bra%E2%80%93ket_notation",
                "/wiki/Physics",
                "/wiki/Kernel_(algebra)",
                "/wiki/Antilinear_map",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Parallelogram_law",
                "/wiki/Continuous_dual_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Applied_mathematics",
                "/wiki/Numerical_analysis",
                "/wiki/Least_squares",
                "/wiki/Hilbert_projection_theorem",
                "/wiki/Uniformly_convex_Banach_space",
                "/wiki/Polarization_identity",
                "/wiki/Banach_space",
                "/wiki/Parallelogram_identity",
                "/wiki/Spectral_color",
                "/wiki/Trichromacy",
                "/wiki/Metamerism_(color)",
                "/wiki/Density_matrix",
                "/wiki/Trace_of_a_matrix",
                "/wiki/Positive_operator_valued_measure",
                "/wiki/Probability_amplitude",
                "/wiki/Absolute_value",
                "/wiki/Quantum_mechanics",
                "/wiki/John_von_Neumann",
                "/wiki/Pure_state",
                "/wiki/Unit_vector",
                "/wiki/State_space_(physics)",
                "/wiki/Phase_factor",
                "/wiki/Projective_space",
                "/wiki/Complex_projective_space",
                "/wiki/Square-integrable",
                "/wiki/Spinors_in_three_dimensions",
                "/wiki/Self-adjoint_operator",
                "/wiki/Linear_operator",
                "/wiki/Eigenvector",
                "/wiki/Eigenvalue",
                "/wiki/Spectral_theory",
                "/wiki/Fourier_transform",
                "/wiki/Compact_set",
                "/wiki/Continuous_spectrum",
                "/wiki/Plancherel_theorem",
                "/wiki/Isometry",
                "/wiki/Harmonic_analysis",
                "/wiki/Plancherel_theorem_for_spherical_functions",
                "/wiki/Noncommutative_harmonic_analysis",
                "/wiki/Eigenfunction",
                "/wiki/Differential_operator",
                "/wiki/Laplace_operator",
                "/wiki/Spectral_theorem",
                "/wiki/Hearing_the_shape_of_a_drum",
                "/wiki/Dirichlet_eigenvalue",
                "/wiki/Orthonormal_basis",
                "/wiki/Orthogonal_polynomials",
                "/wiki/Wavelet",
                "/wiki/Spherical_harmonics",
                "/wiki/Mean_convergence",
                "/wiki/Gibbs_phenomenon",
                "/wiki/Fourier_analysis",
                "/wiki/Linear_combination",
                "/wiki/Fourier_series",
                "/wiki/Liouville%27s_theorem_(Hamiltonian)",
                "/wiki/Measure_theory",
                "/wiki/Time_translation",
                "/wiki/Unitary_transformation",
                "/wiki/Hamiltonian_(quantum_mechanics)",
                "/wiki/Conserved_quantities",
                "/wiki/Phase_space",
                "/wiki/Ergodic_theory",
                "/wiki/Chaos_theory",
                "/wiki/Dynamical_system",
                "/wiki/Thermodynamics",
                "/wiki/Laws_of_thermodynamics",
                "/wiki/Zeroth_law_of_thermodynamics",
                "/wiki/Temperature",
                "/wiki/Parabolic_partial_differential_equation",
                "/wiki/Hyperbolic_partial_differential_equation",
                "/wiki/Elliptic_partial_differential_equation",
                "/wiki/Coercive_function",
                "/wiki/Bilinear_form",
                "/wiki/Linear_functional",
                "/wiki/Poisson_equation",
                "/wiki/Dirichlet_boundary_conditions",
                "/wiki/Partial_differential_equations",
                "/wiki/Elliptic_partial_differential_equation",
                "/wiki/Weak_derivative",
                "/wiki/Sobolev_space",
                "/wiki/Lax%E2%80%93Milgram_theorem",
                "/wiki/Galerkin_method",
                "/wiki/Finite_element_method",
                "/wiki/Compact_operator",
                "/wiki/Integral_operator",
                "/wiki/Green%27s_function",
                "/wiki/Robin_boundary_conditions",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Sturm%E2%80%93Liouville_theory",
                "/wiki/Ordinary_differential_equations",
                "/wiki/Projection_operator",
                "/wiki/Change_of_basis",
                "/wiki/Spectral_theory",
                "/wiki/Continuous_function",
                "/wiki/Self-adjoint_operator",
                "/wiki/Linear_operator",
                "/wiki/Spectral_decomposition",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Reproducing_kernel_Hilbert_space",
                "/wiki/Szeg%C5%91_kernel",
                "/wiki/Harmonic_analysis",
                "/wiki/Poisson_kernel",
                "/wiki/Harmonic_function",
                "/wiki/Unit_ball",
                "/wiki/Bergman_kernel",
                "/wiki/Integral_kernel",
                "/wiki/Cauchy%27s_integral_formula",
                "/wiki/Compact_convergence",
                "/wiki/Closed_set",
                "/wiki/Compact_space",
                "/wiki/Bergman_space",
                "/wiki/Complex_plane",
                "/wiki/Hardy_space",
                "/wiki/Complex_analysis",
                "/wiki/Harmonic_analysis",
                "/wiki/Holomorphic_function",
                "/wiki/Unit_disc",
                "/wiki/Spectral_mapping_theorem",
                "/wiki/Fourier_transform",
                "/wiki/Pseudodifferential_operator",
                "/wiki/Compact_space",
                "/wiki/Riemannian_manifold",
                "/wiki/Hodge_decomposition",
                "/wiki/Hodge_theory",
                "/wiki/Bessel_potential",
                "/wiki/Weak_derivative",
                "/wiki/Sobolev_space",
                "/wiki/Function_space",
                "/wiki/Derivative",
                "/wiki/Banach_spaces",
                "/wiki/H%C3%B6lder_space",
                "/wiki/Partial_differential_equations",
                "/wiki/Direct_method_in_calculus_of_variations",
                "/wiki/Lp_space#Weighted_Lp_spaces",
                "/wiki/Lebesgue_measure",
                "/wiki/Riemann_integral",
                "/wiki/Null_set",
                "/wiki/Function_space",
                "/wiki/Measure_(mathematics)",
                "/wiki/Sigma-algebra",
                "/wiki/Countably_additive_measure",
                "/wiki/Lebesgue_integration",
                "/wiki/Absolute_value",
                "/wiki/Observable",
                "/wiki/Werner_Heisenberg",
                "/wiki/Matrix_mechanics",
                "/wiki/Operator_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Von_Neumann_algebra",
                "/wiki/Israel_Gelfand",
                "/wiki/Mark_Naimark",
                "/wiki/Irving_Segal",
                "/wiki/C*-algebra",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Hermitian_operator",
                "/wiki/Symmetry",
                "/wiki/Unitary_operator",
                "/wiki/Quantum_measurement",
                "/wiki/Orthogonal_projection",
                "/wiki/Unitary_representation",
                "/wiki/Representation_theory",
                "/wiki/Group_(mathematics)",
                "/wiki/Koopman%E2%80%93von_Neumann_classical_mechanics",
                "/wiki/Dynamical_systems",
                "/wiki/Ergodic_theory",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Maurice_Fr%C3%A9chet",
                "/wiki/Frigyes_Riesz",
                "/wiki/John_von_Neumann",
                "/wiki/Self-adjoint_operator",
                "/wiki/Hermann_Weyl",
                "/wiki/Norbert_Wiener",
                "/wiki/Eugene_Wigner",
                "/wiki/Lebesgue_integral",
                "/wiki/Riemann_integral",
                "/wiki/Henri_Lebesgue",
                "/wiki/Frigyes_Riesz",
                "/wiki/Ernst_Sigismund_Fischer",
                "/wiki/Complete_metric_space",
                "/wiki/Joseph_Fourier",
                "/wiki/Friedrich_Bessel",
                "/wiki/Marc-Antoine_Parseval",
                "/wiki/Trigonometric_series",
                "/wiki/Riesz%E2%80%93Fischer_theorem",
                "/wiki/Eigenfunction_expansion",
                "/wiki/Orthogonality",
                "/wiki/Spectral_decomposition",
                "/wiki/David_Hilbert",
                "/wiki/Erhard_Schmidt",
                "/wiki/Integral_equations",
                "/wiki/Square-integrable",
                "/wiki/Mathematician",
                "/wiki/Vector_space",
                "/wiki/Real_numbers",
                "/wiki/Complex_numbers",
                "/wiki/Vector_(geometric)",
                "/wiki/Sequence_(mathematics)",
                "/wiki/Series_(mathematics)",
                "/wiki/Mathematical_analysis",
                "/wiki/Convergent_series",
                "/wiki/Sequence_space",
                "/wiki/Sequence_(mathematics)",
                "/wiki/Series_(mathematics)",
                "/wiki/Banach_space",
                "/wiki/Topological_vector_space",
                "/wiki/Topology",
                "/wiki/Open_set",
                "/wiki/Closed_set",
                "/wiki/Linear_subspace",
                "/wiki/Absolute_convergence",
                "/wiki/Metric_space",
                "/wiki/Complete_space",
                "/wiki/Cauchy_criterion",
                "/wiki/Cauchy_sequence",
                "/wiki/Limit_(mathematics)",
                "/wiki/Linear_independence",
                "/wiki/Cauchy%E2%80%93Schwarz_inequality",
                "/wiki/Triangle_inequality",
                "/wiki/Norm_(mathematics)",
                "/wiki/Antilinear_map",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Inner_product_space",
                "/wiki/Complete_metric_space",
                "/wiki/Dot_product",
                "/wiki/Complex_number",
                "/wiki/Complex_plane",
                "/wiki/Absolute_value",
                "/wiki/Complex_conjugate",
                "/wiki/Absolute_convergence",
                "/wiki/Multivariable_calculus",
                "/wiki/Limit_(mathematics)",
                "/wiki/Series_(mathematics)",
                "/wiki/Inner_product",
                "/wiki/Vector_space",
                "/wiki/Inner_product_space",
                "/wiki/Norm_(mathematics)",
                "/wiki/Euclidean_space",
                "/wiki/Euclidean_vector",
                "/wiki/Dot_product",
                "/wiki/Cartesian_coordinates",
                "/wiki/Pythagorean_theorem",
                "/wiki/Parallelogram_law",
                "/wiki/Altitude_(triangle)",
                "/wiki/Coordinate_axes",
                "/wiki/Orthonormal_basis",
                "/wiki/Countably_infinite",
                "/wiki/Infinite_sequence",
                "/wiki/Lp_norm",
                "/wiki/Linear_operator",
                "/wiki/Spectrum_(functional_analysis)",
                "/wiki/Mathematics",
                "/wiki/Physics",
                "/wiki/Infinite-dimensional",
                "/wiki/Function_space",
                "/wiki/David_Hilbert",
                "/wiki/Erhard_Schmidt",
                "/wiki/Frigyes_Riesz",
                "/wiki/Partial_differential_equation",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Fourier_analysis",
                "/wiki/Signal_processing",
                "/wiki/Ergodic_theory",
                "/wiki/Thermodynamics",
                "/wiki/John_von_Neumann",
                "/wiki/Functional_analysis",
                "/wiki/Lp_space",
                "/wiki/Sequence_space",
                "/wiki/Sobolev_space",
                "/wiki/Generalized_function",
                "/wiki/Hardy_space",
                "/wiki/Holomorphic_function",
                "/wiki/Mathematics",
                "/wiki/David_Hilbert",
                "/wiki/Euclidean_space",
                "/wiki/Linear_algebra",
                "/wiki/Calculus",
                "/wiki/Plane_(geometry)",
                "/wiki/Dimension",
                "/wiki/Vector_space",
                "/wiki/Mathematical_structure",
                "/wiki/Inner_product_space",
                "/wiki/Complete_metric_space",
                "/wiki/Limit_(mathematics)"
            ],
            "text": "There is also a version of the spectral theorem that applies to unbounded normal operators.A precise version of the spectral theorem in this case is:[71]fails to be a well-defined continuous operator. The self-adjointness of T still guarantees that the spectrum is real. Thus the essential idea of working with unbounded operators is to look instead at the resolvent R\u03bb where \u03bb is nonreal. This is a bounded normal operator, which admits a spectral representation that can then be transferred to a spectral representation of T itself. A similar strategy is used, for instance, to study the spectrum of the Laplace operator: rather than address the operator directly, one instead looks as an associated resolvent such as a Riesz potential or Bessel potential.The spectral theory of unbounded self-adjoint operators is only marginally more difficult than for bounded operators. The spectrum of an unbounded operator is defined in precisely the same way as for bounded operators: \u03bb is a spectral value if the resolvent operatorThe resulting continuous functional calculus has applications in particular to pseudodifferential operators.[70]A major application of spectral methods is the spectral mapping theorem, which allows one to apply to a self-adjoint operator T any continuous complex function f defined on the spectrum of T by forming the integralA somewhat similar spectral decomposition holds for normal operators, although because the spectrum may now contain non-real complex numbers, the operator-valued Stieltjes measure dE\u03bb must instead be replaced by a resolution of the identity.The integral is understood as a Riemann\u2013Stieltjes integral, convergent with respect to the norm on B(H). In particular, one has the ordinary scalar-valued integral representationThe operators E\u03bb are monotone increasing relative to the partial order defined on self-adjoint operators; the eigenvalues correspond precisely to the jump discontinuities. One has the spectral theorem, which assertsThe general spectral theorem for self-adjoint operators involves a kind of operator-valued Riemann\u2013Stieltjes integral, rather than an infinite summation.[69] The spectral family associated to T associates to each real number \u03bb an operator E\u03bb, which is the projection onto the nullspace of the operator (T \u2212 \u03bb)+, where the positive part of a self-adjoint operator is defined byThis theorem plays a fundamental role in the theory of integral equations, as many integral operators are compact, in particular those that arise from Hilbert\u2013Schmidt operators.However, the spectral theorem of a self-adjoint operator T takes a particularly simple form if, in addition, T is assumed to be a compact operator. The spectral theorem for compact self-adjoint operators states:[68]Unlike with finite matrices, not every element of the spectrum of T must be an eigenvalue: the linear operator T \u2212 \u03bb may only lack an inverse because it is not surjective. Elements of the spectrum of an operator in the general sense are known as spectral values. Since spectral values need not be eigenvalues, the spectral decomposition is often more subtle than in finite dimensions.The eigenspaces of an operator T are given byMoreover, m and M are both actually contained within the spectrum.The spectrum of an operator T, denoted \u03c3(T), is the set of complex numbers \u03bb such that T \u2212 \u03bb lacks a continuous inverse. If T is bounded, then the spectrum is always a compact set in the complex plane, and lies inside the disc |z| \u2264 ||T||. If T is self-adjoint, then the spectrum is real. In fact, it is contained in the interval [m,M] whereThere is a well-developed spectral theory for self-adjoint operators in a Hilbert space, that is roughly analogous to the study of symmetric matrices over the reals or self-adjoint matrices over the complex numbers.[67] In the same sense, one can obtain a \"diagonalization\" of a self-adjoint operator as a suitable sum (actually an integral) of orthogonal projection operators.If the Vi are in addition closed, thenThe orthogonal complement satisfies some more elementary results. It is a monotone function in the sense that if U \u2282 V, then V\u22a5 \u2286 U\u22a5 with equality holding if and only if V is contained in the closure of U. This result is a special case of the Hahn\u2013Banach theorem. The closure of a subspace can be completely characterized in terms of the orthogonal complement: if V is a subspace of H, then the closure of V is equal to V\u22a5\u22a5. The orthogonal complement is thus a Galois connection on the partial order of subspaces of a Hilbert space. In general, the orthogonal complement of a sum of subspaces is the intersection of the orthogonal complements:[66]While this result characterizes the metric structure of a Hilbert space, the structure of a Hilbert space as a topological vector space can itself be characterized in terms of the presence of complementary subspaces:[65]Every closed subspace V of a Hilbert space is therefore the image of an operator P of norm one such that P2 = P. The property of possessing appropriate projection operators characterizes Hilbert spaces:[64]The operator norm of the orthogonal projection PV onto a nonzero closed subspace V is equal to 1:for all x \u2208 V and y \u2208 H.meaning thatBy restricting the codomain to the Hilbert space V, the orthogonal projection PV gives rise to a projection mapping \u03c0\u00a0: H \u2192 V; it is the adjoint of the inclusion mappingProjections PU and PV are called mutually orthogonal if PUPV = 0. This is equivalent to U and V being orthogonal as subspaces of H. The sum of the two projections PU and PV is a projection only if U and V are orthogonal to each other, and in that case PU + PV = PU+V. The composite PUPV is generally not a projection; in fact, the composite is a projection if and only if the two projections commute, and in that case PUPV = PU\u2229V.This provides the geometrical interpretation of PV(x): it is the best approximation to x by elements of V.[63]The linear operator PV\u00a0: H \u2192 H that maps x to v is called the orthogonal projection onto V. There is a natural one-to-one correspondence between the set of all closed subspaces of H and the set of all bounded self-adjoint operators P such that P2 = P. Specifically,S\u22a5 is a closed subspace of H (can be proved easily using the linearity and continuity of the inner product) and so forms itself a Hilbert space. If V is a closed subspace of H, then V\u22a5 is called the orthogonal complement of V. In fact, every x \u2208 H can then be written uniquely as x = v + w, with v \u2208 V and w \u2208 V\u22a5. Therefore, H is the internal Hilbert direct sum of V and V\u22a5.If S is a subset of a Hilbert space H, the set of vectors orthogonal to S is defined byIn the past, Hilbert spaces were often required to be separable as part of the definition.[60] Most spaces used in physics are separable, and since these are all isomorphic to each other, one often refers to any infinite-dimensional separable Hilbert space as \"the Hilbert space\" or just \"Hilbert space\".[61] Even in quantum field theory, most of the Hilbert spaces are in fact separable, as stipulated by the Wightman axioms. However, it is sometimes argued that non-separable Hilbert spaces are also important in quantum field theory, roughly because the systems in the theory possess an infinite number of degrees of freedom and any infinite Hilbert tensor product (of spaces of dimension greater than one) is non-separable.[62] For instance, a bosonic field can be naturally thought of as an element of a tensor product whose factors represent harmonic oscillators at each point of space. From this perspective, the natural state space of a boson might seem to be a non-separable space.[62] However, it is only a small separable subspace of the full tensor product that can contain physically meaningful fields (on which the observables can be defined). Another non-separable Hilbert space models the state of an infinite collection of particles in an unbounded region of space. An orthonormal basis of the space is indexed by the density of the particles, a continuous parameter, and since the set of possible densities is uncountable, the basis is not countable.[62]A Hilbert space is separable if and only if it admits a countable orthonormal basis. All infinite-dimensional separable Hilbert spaces are therefore isometrically isomorphic to l2.for all x, y \u2208 H. The cardinal number of B is the Hilbert dimension of H. Thus every Hilbert space is isometrically isomorphic to a sequence space l2(B) for some set B.As a consequence of Parseval's identity, if {ek}k \u2208 B is an orthonormal basis of H, then the map \u03a6\u00a0: H \u2192 l2(B) defined by \u03a6(x) = \u27e8x,ek\u27e9k\u2208B is an isometric isomorphism of Hilbert spaces: it is a bijective linear mapping such thatAs a consequence of Zorn's lemma, every Hilbert space admits an orthonormal basis; furthermore, any two orthonormal bases of the same space have the same cardinality, called the Hilbert dimension of the space.[59] For instance, since l2(B) has an orthonormal basis indexed by B, its Hilbert dimension is the cardinality of B (which may be a finite integer, or a countable or uncountable cardinal number).Conversely, if {ek} is an orthonormal set such that Parseval's identity holds for every x, then {ek} is an orthonormal basis.Even if B is uncountable, Bessel's inequality guarantees that the expression is well-defined and consists only of countably many nonzero terms. This sum is called the Fourier expansion of x, and the individual coefficients \u27e8x,ek\u27e9 are the Fourier coefficients of x. Parseval's formula is thenBessel's inequality is a stepping stone to the more powerful Parseval identity, which governs the case when Bessel's inequality is actually an equality. If {ek}k \u2208 B is an orthonormal basis of H, then every element x of H may be written asGeometrically, Bessel's inequality implies that the orthogonal projection of x onto the linear subspace spanned by the fi has norm that does not exceed that of x. In two dimensions, this is the assertion that the length of the leg of a right triangle may not exceed the length of the hypotenuse.(according to the definition of the sum of an arbitrary family of non-negative real numbers).Let {fi}, i \u2208 I, be an arbitrary orthonormal system in\u00a0H. Applying the preceding inequality to every finite subset J of I gives the Bessel inequality[58]Then \u27e8x, fk\u27e9 = \u27e8y, fk\u27e9 for every k = 1, ..., n. It follows that x \u2212 y is orthogonal to each fk, hence x \u2212 y is orthogonal to\u00a0y. Using the Pythagorean identity twice, it follows thatLet f1, ..., fn be a finite orthonormal system in\u00a0H. For an arbitrary vector x \u2208 H, let An orthonormal basis of l2(B) is indexed by the set B, given byfor all x, y \u2208 l2(B). Here the sum also has only countably many nonzero terms, and is unconditionally convergent by the Cauchy\u2013Schwarz inequality.the supremum being taken over all finite subsets of\u00a0B. It follows that, for this sum to be finite, every element of l2(B) has only countably many nonzero terms. This space becomes a Hilbert space with the inner productThe summation over B is here defined byMore generally, if B is any set, then one can form a Hilbert space of sequences with index set B, defined byThis space has an orthonormal basis:of complex numbers such thatThe space l2 of square-summable sequences of complex numbers is the set of infinite sequencesIn the infinite-dimensional case, an orthonormal basis will not be a basis in the sense of linear algebra; to distinguish the two, the latter basis is also called a Hamel basis. That the span of the basis vectors is dense implies that every vector in the space can be written as the sum of an infinite series, and the orthogonality implies that this decomposition is unique.Examples of orthonormal bases include:This is related to the fact that the only vector orthogonal to a dense linear subspace is the zero vector, for if S is any orthonormal set and v is orthogonal to S, then v is orthogonal to the closure of the linear span of S, which is the whole space.A system of vectors satisfying the first two conditions basis is called an orthonormal system or an orthonormal set (or an orthonormal sequence if B is countable). Such a system is always linearly independent. Completeness of an orthonormal system of vectors of a Hilbert space can be equivalently restated as:The notion of an orthonormal basis from linear algebra generalizes over to the case of Hilbert spaces.[57] In a Hilbert space H, an orthonormal basis is a family {ek}k \u2208 B of elements of H satisfying the conditions:This example is typical in the following sense.[56] Associated to every simple tensor product x1 \u2297 x2 is the rank one operator from H\u2217\n1 to H2 that maps a given x* \u2208 H\u2217\n1 ason the square.An example is provided by the Hilbert space L2([0,1]). The Hilbertian tensor product of two copies of L2([0,1]) is isometrically and linearly isomorphic to the space L2([0,1]2) of square-integrable functions on the square [0,1]2. This isomorphism sends a simple tensor f1 \u2297 f2 to the functionIf x1, y1 \u220a H1 and x2, y2 \u220a H2, then one defines an inner product on the (ordinary) tensor product as follows. On simple tensors, letThe spectral theorem for compact self-adjoint operators on a Hilbert space H states that H splits into an orthogonal direct sum of the eigenspaces of an operator, and also gives an explicit decomposition of the operator as a sum of projections onto the eigenspaces. The direct sum of Hilbert spaces also appears in quantum mechanics as the Fock space of a system containing a variable number of particles, where each Hilbert space in the direct sum corresponds to an additional degree of freedom for the quantum mechanical system. In representation theory, the Peter\u2013Weyl theorem guarantees that any unitary representation of a compact group on a Hilbert space splits as the direct sum of finite-dimensional representations.Each of the Hi is included as a closed subspace in the direct sum of all of the Hi. Moreover, the Hi are pairwise orthogonal. Conversely, if there is a system of closed subspaces, Vi, i \u2208 I, in a Hilbert space H, that are pairwise orthogonal and whose union is dense in H, then H is canonically isomorphic to the direct sum of Vi. In this case, H is called the internal direct sum of the Vi. A direct sum (internal or external) is also equipped with a family of orthogonal projections Ei onto the ith direct summand Hi. These projections are bounded, self-adjoint, idempotent operators that satisfy the orthogonality conditionThe inner product is defined byin the Cartesian product of the Hi such thatconsists of the set of all indexed familiesMore generally, if Hi is a family of Hilbert spaces indexed by i \u2208 I, then the direct sum of the Hi, denotedconsisting of the set of all ordered pairs (x1,x2) where xi \u2208 Hi, i = 1,2, and inner product defined byTwo Hilbert spaces H1 and H2 can be combined into another Hilbert space, called the (orthogonal) direct sum,[54] and denotedThese correspond to the momentum and position observables, respectively. Note that neither A nor B is defined on all of H, since in the case of A the derivative need not exist, and in the case of B the product function need not be square integrable. In both cases, the set of possible arguments form dense subspaces of L2(\u211d).The adjoint of a densely defined unbounded operator is defined in essentially the same manner as for bounded operators. Self-adjoint unbounded operators play the role of the observables in the mathematical formulation of quantum mechanics. Examples of self-adjoint unbounded operators on the Hilbert space L2(\u211d) are:[53]Unbounded operators are also tractable in Hilbert spaces, and have important applications to quantum mechanics.[52] An unbounded operator T on a Hilbert space H is defined as a linear operator whose domain D(T) is a linear subspace of H. Often the domain D(T) is a dense subspace of H, in which case T is known as a densely defined operator.The index is homotopy invariant, and plays a deep role in differential geometry via the Atiyah\u2013Singer index theorem.An element of B(H) is compact if it sends bounded sets to relatively compact sets. Equivalently, a bounded operator T is compact if, for any bounded sequence {xk}, the sequence {Txk} has a convergent subsequence. Many integral operators are compact, and in fact define a special class of operators known as Hilbert\u2013Schmidt operators that are especially important in the study of integral equations. Fredholm operators differ from a compact operator by a multiple of the identity, and are equivalently characterized as operators with a finite dimensional kernel and cokernel. The index of a Fredholm operator T is defined byAn element U of B(H) is called unitary if U is invertible and its inverse is given by U*. This can also be expressed by requiring that U be onto and \u27e8Ux,Uy\u27e9 = \u27e8x,y\u27e9 for all x, y \u2208 H. The unitary operators form a group under composition, which is the isometry group of H.that commute with each other. Normal operators can also usefully be thought of in terms of their real and imaginary parts.In a sense made precise by the spectral theorem, self-adjoint operators can usefully be thought of as operators that are \"real\". An element A of B(H) is called normal if A*A = AA*. Normal operators decompose into the sum of a self-adjoint operators and an imaginary multiple of a self adjoint operatorAn element A of B(H) is called 'self-adjoint' or 'Hermitian' if A* = A. If A is Hermitian and \u27e8Ax,x\u27e9 \u2265 0 for every x, then A is called 'nonnegative', written A \u2265 0; if equality holds only when x = 0, then A is called 'positive'. The set of self adjoint operators admits a partial order, in which A \u2265 B if A \u2212 B \u2265 0. If A has the form B*B for some B, then A is nonnegative; if B is invertible, then A is positive. A converse is also true in the sense that, for a non-negative operator A, there exists a unique non-negative square root B such thatThe set B(H) of all bounded linear operators on H, together with the addition and composition operations, the norm and the adjoint operation, is a C*-algebra, which is a type of operator algebra.for some vector A*y in H1. This defines another bounded linear operator A*\u00a0: H2 \u2192 H1, the adjoint of A. One can see that A** = A.The sum and the composite of two bounded linear operators is again bounded and linear. For y in H2, the map that sends x\u00a0\u2208 H1 to \u27e8Ax, y\u27e9 is linear and continuous, and according to the Riesz representation theorem can therefore be represented in the formThe continuous linear operators A\u00a0: H1 \u2192 H2 from a Hilbert space H1 to a second Hilbert space H2 are bounded in the sense that they map bounded sets to bounded sets. Conversely, if an operator is bounded, then it is continuous. The space of such bounded linear operators has a norm, the operator norm given byThe (geometrical) Hahn\u2013Banach theorem asserts that a closed convex set can be separated from any point outside it by means of a hyperplane of the Hilbert space. This is an immediate consequence of the best approximation property: if y is the element of a closed convex set F closest to x, then the separating hyperplane is the plane perpendicular to the segment xy passing through its midpoint.[51]Any general property of Banach spaces continues to hold for Hilbert spaces. The open mapping theorem states that a continuous surjective linear transformation from one Banach space to another is an open mapping meaning that it sends open sets to open sets. A corollary is the bounded inverse theorem, that a continuous and bijective linear function from one Banach space to another is an isomorphism (that is, a continuous linear map whose inverse is also continuous). This theorem is considerably simpler to prove in the case of Hilbert spaces than in general Banach spaces.[49] The open mapping theorem is equivalent to the closed graph theorem, which asserts that a function from one Banach space to another is continuous if and only if its graph is a closed set.[50] In the case of Hilbert spaces, this is basic in the study of unbounded operators (see closed operator).This fact (and its various generalizations) are fundamental for direct methods in the calculus of variations. Minimization results for convex functionals are also a direct consequence of the slightly more abstract fact that closed bounded convex subsets in a Hilbert space H are weakly compact, since H is reflexive. The existence of weakly convergent subsequences is a special case of the Eberlein\u2013\u0160mulian theorem.Conversely, every bounded sequence in a Hilbert space admits weakly convergent subsequences (Alaoglu's theorem).[47] This fact may be used to prove minimization results for continuous convex functionals, in the same way that the Bolzano\u2013Weierstrass theorem is used for continuous functions on \u211dd. Among several variants, one simple statement is as follows:[48]For example, any orthonormal sequence {fn} converges weakly to\u00a00, as a consequence of Bessel's inequality. Every weakly convergent sequence {xn} is bounded, by the uniform boundedness principle.for every v \u2208 H.In a Hilbert space H, a sequence {xn} is weakly convergent to a vector x \u2208 H whenThe Riesz representation theorem relies fundamentally not just on the presence of an inner product, but also on the completeness of the space. In fact, the theorem implies that the topological dual of any inner product space can be identified with its completion. An immediate consequence of the Riesz representation theorem is also that a Hilbert space H is reflexive, meaning that the natural map from H into its double dual space is an isomorphism.The result \u27e8x|y\u27e9 can be seen as the action of the linear functional \u27e8x| (the bra) on the vector |y\u27e9 (the ket).This correspondence \u03c6 \u2194 u is exploited by the bra\u2013ket notation popular in physics. It is common in physics to assume that the inner product, denoted by \u27e8x|y\u27e9, is linear on the right,The representing vector u\u03c6 is obtained in the following way. When \u03c6 \u2260 0, the kernel F = Ker(\u03c6) is a closed vector subspace of H, not equal to H, hence there exists a nonzero vector v orthogonal to F. The vector u is a suitable scalar multiple \u03bbv of v. The requirement that \u03c6(v) = \u27e8v,u\u27e9 yieldsThe reversal of order on the right-hand side restores linearity in \u03c6 from the antilinearity of u\u03c6. In the real case, the antilinear isomorphism from H to its dual is actually an isomorphism, and so real Hilbert spaces are naturally isomorphic to their own duals.for all x \u2208 H. The inner product on the dual space H* satisfiesThe mapping u \u21a6 \u03c6u is an antilinear mapping from H to H*. The Riesz representation theorem states that this mapping is an antilinear isomorphism.[46] Thus to every element \u03c6 of the dual H* there exists one and only one u\u03c6 in H such thatThe Riesz representation theorem affords a convenient description of the dual. To every element u of H, there is a unique element \u03c6u of H*, defined byThis norm satisfies the parallelogram law, and so the dual space is also an inner product space. The dual space is also complete, and so it is a Hilbert space in its own right.The dual space H* is the space of all continuous linear functions from the space H into the base field. It carries a natural norm, defined byIn particular, when F is not equal to H, one can find a nonzero vector v orthogonal to F (select x \u2209 F and v = x \u2212 y). A very useful criterion is obtained by applying this observation to the closed subspace F generated by a subset S of H.This point y is the orthogonal projection of x onto F, and the mapping PF\u00a0: x \u2192 y is linear (see Orthogonal complements and projections). This result is especially significant in applied mathematics, especially numerical analysis, where it forms the basis of least squares methods.[45]When this result is applied to a closed subspace F of H, it can be shown that the point y \u2208 F closest to x is characterized by[44]This is equivalent to saying that there is a point with minimal norm in the translated convex set D = C \u2212 x. The proof consists in showing that every minimizing sequence (dn) \u2282 D is Cauchy (using the parallelogram identity) hence converges (using completeness) to a point in D that has minimal norm. More generally, this holds in any uniformly convex Banach space.[43]This subsection employs the Hilbert projection theorem. If C is a non-empty closed convex subset of a Hilbert space H and x a point in H, there exists a unique point y \u2208 C that minimizes the distance between x and points in C,[42]The parallelogram law implies that any Hilbert space is a uniformly convex Banach space.[41]For complex Hilbert spaces, it isConversely, every Banach space in which the parallelogram identity holds is a Hilbert space, and the inner product is uniquely determined by the norm by the polarization identity.[40] For real Hilbert spaces, the polarization identity isBy definition, every Hilbert space is also a Banach space. Furthermore, in every Hilbert space the following parallelogram identity holds:Furthermore, the sum of a series of orthogonal vectors is independent of the order in which it is taken.Whereas the Pythagorean identity as stated is valid in any inner product space, completeness is required for the extension of the Pythagorean identity to series. A series \u2211uk of orthogonal vectors converges in H if and only if the series of squares of norms converges, andBy induction on n, this is extended to any family u1, ..., un of n orthogonal vectors,When u and v are orthogonal, one hasTwo vectors u and v in a Hilbert space H are orthogonal when \u27e8u,v\u27e9 = 0. The notation for this is u \u22a5 v. More generally, when S is a subset in H, the notation u \u22a5 S means that u is orthogonal to every element from S.Any true physical color can be represented by a combination of pure spectral colors. As physical colors can be composed of any number of physical colors, the space of physical colors may aptly be represented by a Hilbert space over spectral colors. Humans have three types of cone cells for color perception, so the perceivable colors can be represented by 3-dimensional Euclidean space. The many-to-one linear mapping from the Hilbert space of physical colors to the Euclidean space of human perceivable colors explains why many distinct physical colors may be perceived by humans to be identical (e.g., pure yellow light versus a mix of red and green light, see metamerism).For a general system, states are typically not pure, but instead are represented as statistical mixtures of pure states, or mixed states, given by density matrices: self-adjoint operators of trace one on a Hilbert space. Moreover, for general quantum mechanical systems, the effects of a single measurement can influence other parts of a system in a manner that is described instead by a positive operator valued measure. Thus the structure both of the states and observables in the general theory is considerably more complicated than the idealization for pure states.The inner product between two state vectors is a complex number known as a probability amplitude. During an ideal measurement of a quantum mechanical system, the probability that a system collapses from a given initial state to a particular eigenstate is given by the square of the absolute value of the probability amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator\u2014which explains the choice of self-adjoint operators, for all the eigenvalues must be real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.In the mathematically rigorous formulation of quantum mechanics, developed by John von Neumann,[39] the possible states (more precisely, the pure states) of a quantum mechanical system are represented by unit vectors (called state vectors) residing in a complex separable Hilbert space, known as the state space, well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projectivization of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system; for example, the position and momentum states for a single non-relativistic spin zero particle is the space of all square-integrable functions, while the states for the spin of a single proton are unit elements of the two-dimensional complex Hilbert space of spinors. Each observable is represented by a self-adjoint linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate.Spectral theory also underlies certain aspects of the Fourier transform of a function. Whereas Fourier analysis decomposes a function defined on a compact set into the discrete spectrum of the Laplacian (which corresponds to the vibrations of a violin string or drum), the Fourier transform of a function is the decomposition of a function defined on all of Euclidean space into its components in the continuous spectrum of the Laplacian. The Fourier transformation is also geometrical, in a sense made precise by the Plancherel theorem, that asserts that it is an isometry of one Hilbert space (the \"time domain\") with another (the \"frequency domain\"). This isometry property of the Fourier transformation is a recurring theme in abstract harmonic analysis, as evidenced for instance by the Plancherel theorem for spherical functions occurring in noncommutative harmonic analysis.In various applications to physical problems, a function can be decomposed into physically meaningful eigenfunctions of a differential operator (typically the Laplace operator): this forms the foundation for the spectral study of functions, in reference to the spectrum of the differential operator.[37] A concrete physical application involves the problem of hearing the shape of a drum: given the fundamental modes of vibration that a drumhead is capable of producing, can one infer the shape of the drum itself?[38] The mathematical formulation of this question involves the Dirichlet eigenvalues of the Laplace equation in the plane, that represent the fundamental modes of vibration in direct analogy with the integers that represent the fundamental modes of vibration of the violin string.That this formula minimizes the difference ||f \u2212 fn||2 is a consequence of Bessel's inequality and Parseval's formula.The coefficients {aj} are selected to make the magnitude of the difference ||f \u2212 fn||2 as small as possible. Geometrically, the best approximation is the orthogonal projection of f onto the subspace consisting of all linear combinations of the {ej}, and can be calculated by[36]For instance, if en are any orthonormal basis functions of L2[0,1], then a given function in L2[0,1] can be approximated as a finite linear combination[35]The problem can also be studied from the abstract point of view: every Hilbert space has an orthonormal basis, and every element of the Hilbert space can be written in a unique way as a sum of multiples of these basis elements. The coefficients appearing on these basis elements are sometimes known abstractly as the Fourier coefficients of the element of the space.[32] The abstraction is especially useful when it is more natural to use different basis functions for a space such as L2([0,1]). In many circumstances, it is desirable not to decompose a function into trigonometric functions, but rather into orthogonal polynomials or wavelets for instance,[33] and in higher dimensions into spherical harmonics.[34]and, moreover, this series converges in the Hilbert space sense (that is, in the L2 mean).A significant problem in classical Fourier series asks in what sense the Fourier series converges, if at all, to the function f. Hilbert space methods provide one possible answer to this question.[31] The functions en(\u03b8) = e2\u03c0in\u03b8 form an orthogonal basis of the Hilbert space L2([0,1]). Consequently, any square-integrable function can be expressed as a seriesThe example of adding up the first few terms in a Fourier series for a sawtooth function is shown in the figure. The basis functions are sine waves with wavelengths \u03bb/n (for integer n) shorter than the wavelength \u03bb of the sawtooth itself (except for n = 1, the fundamental wave). All basis functions have nodes at the nodes of the sawtooth, but all but the fundamental have additional nodes. The oscillation of the summed terms about the sawtooth is called the Gibbs phenomenon.whereOne of the basic goals of Fourier analysis is to decompose a function into a (possibly infinite) linear combination of given basis functions: the associated Fourier series. The classical Fourier series associated to a function f defined on the interval [0,1] is a series of the formThat is, the long time average of an observable f is equal to its expectation value over an energy surface.For an ergodic system, the fixed set of the time evolution consists only of the constant functions, so the ergodic theorem implies the following:[30] for any function f \u2208 L2(\u03a9E,\u03bc),The von Neumann mean ergodic theorem[18] states the following:for all w on \u03a9E and all time t. Liouville's theorem implies that there exists a measure \u03bc on the energy surface that is invariant under the time translation. As a result, time translation is a unitary transformation of the Hilbert space L2(\u03a9E,\u03bc) consisting of square-integrable functions on the energy surface \u03a9E with respect to the inner productAn ergodic dynamical system is one for which, apart from the energy\u2014measured by the Hamiltonian\u2014there are no other functionally independent conserved quantities on the phase space. More explicitly, suppose that the energy E is fixed, and let \u03a9E be the subset of the phase space consisting of all states of energy E (an energy surface), and let Tt denote the evolution operator on the phase space. The dynamical system is ergodic if there are no continuous non-constant functions on \u03a9E such thatThe field of ergodic theory is the study of the long-term behavior of chaotic dynamical systems. The protypical case of a field that ergodic theory applies to is thermodynamics, in which\u2014though the microscopic state of a system is extremely complicated (it is impossible to understand the ensemble of individual collisions between particles of matter)\u2014the average behavior over sufficiently long time intervals is tractable. The laws of thermodynamics are assertions about such average behavior. In particular, one formulation of the zeroth law of thermodynamics asserts that over sufficiently long timescales, the only functionally independent measurement that one can make of a thermodynamic system in equilibrium is its total energy, in the form of temperature.Hilbert spaces allow for many elliptic partial differential equations to be formulated in a similar way, and the Lax\u2013Milgram theorem is then a basic tool in their analysis. With suitable modifications, similar techniques can be applied to parabolic partial differential equations and certain hyperbolic partial differential equations.Since the Poisson equation is elliptic, it follows from Poincar\u00e9's inequality that the bilinear form a is coercive. The Lax\u2013Milgram theorem then ensures the existence and uniqueness of solutions of this equation.where a is a continuous bilinear form, and b is a continuous linear functional, given respectively byThis can be recast in terms of the Hilbert space H1\n0(\u03a9) consisting of functions u such that u, along with its weak partial derivatives, are square integrable on \u03a9, and vanish on the boundary. The question then reduces to finding u in this space such that for all v in this spaceA typical example is the Poisson equation \u2212\u0394u = g with Dirichlet boundary conditions in a bounded domain \u03a9 in \u211d2. The weak formulation consists of finding a function u such that, for all continuously differentiable functions v in \u03a9 vanishing on the boundary:Hilbert spaces form a basic tool in the study of partial differential equations.[21] For many classes of partial differential equations, such as linear elliptic equations, it is possible to consider a generalized solution (known as a weak solution) by enlarging the class of functions. Many weak formulations involve the class of Sobolev functions, which is a Hilbert space. A suitable weak formulation reduces to a geometrical problem the analytic problem of finding a solution or, often what is more important, showing that a solution exists and is unique for given boundary data. For linear elliptic equations, one geometrical result that ensures unique solvability for a large class of problems is the Lax\u2013Milgram theorem. This strategy forms the rudiment of the Galerkin method (a finite element method) for numerical solution of partial differential equations.[29]The functions p, q, and w are given in advance, and the problem is to find the function y and constants \u03bb for which the equation has a solution. The problem only has solutions for certain values of \u03bb, called eigenvalues of the system, and this is a consequence of the spectral theorem for compact operators applied to the integral operator defined by the Green's function for the system. Furthermore, another consequence of this general result is that the eigenvalues \u03bb of the system can be arranged in an increasing sequence tending to infinity.[nb 2]for an unknown function y on an interval [a,b], satisfying general homogeneous Robin boundary conditionsIn the theory of ordinary differential equations, spectral methods on a suitable Hilbert space are used to study the behavior of eigenvalues and eigenfunctions of differential equations. For example, the Sturm\u2013Liouville problem arises in the study of the harmonics of waves in a violin string or a drum, and is a central problem in ordinary differential equations.[28] The problem is a differential equation of the formMany of the applications of Hilbert spaces exploit the fact that Hilbert spaces support generalizations of simple geometric concepts like projection and change of basis from their usual finite dimensional setting. In particular, the spectral theory of continuous self-adjoint linear operators on a Hilbert space generalizes the usual spectral decomposition of a matrix, and this often plays a major role in applications of the theory to other areas of mathematics and physics.A Bergman space is an example of a reproducing kernel Hilbert space, which is a Hilbert space of functions along with a kernel K(\u03b6,z) that verifies a reproducing property analogous to this one. The Hardy space H2(D) also admits a reproducing kernel, known as the Szeg\u0151 kernel.[27] Reproducing kernels are common in other areas of mathematics as well. For instance, in harmonic analysis the Poisson kernel is a reproducing kernel for the Hilbert space of square-integrable harmonic functions in the unit ball. That the latter is a Hilbert space at all is a consequence of the mean value theorem for harmonic functions.is known as the Bergman kernel of D. This integral kernel satisfies a reproducing propertyfor all f \u2208 L2,h(D). The integrandwhich in turn follows from Cauchy's integral formula. Thus convergence of a sequence of holomorphic functions in L2(D) implies also compact convergence, and so the limit function is also holomorphic. Another consequence of this inequality is that the linear functional that evaluates a function f at a point of D is actually continuous on L2,h(D). The Riesz representation theorem implies that the evaluation functional can be represented as an element of L2,h(D). Thus, for every z \u2208 D, there is a function \u03b7z \u2208 L2,h(D) such thatwhere the integral is taken with respect to the Lebesgue measure in D. Clearly L2,h(D) is a subspace of L2(D); in fact, it is a closed subspace, and so a Hilbert space in its own right. This is a consequence of the estimate, valid on compact subsets K of D, thatThe Bergman spaces are another family of Hilbert spaces of holomorphic functions.[26] Let D be a bounded open set in the complex plane (or a higher-dimensional complex space) and let L2,h(D) be the space of holomorphic functions f in D that are also in L2(D) in the sense thatThus H2(U) consists of those functions that are L2 on the circle, and whose negative frequency Fourier coefficients vanish.whereHardy spaces in the disc are related to Fourier series. A function f is in H2(U) if and only ifremain bounded for r < 1. The norm on this Hardy space is defined byThe Hardy spaces are function spaces, arising in complex analysis and harmonic analysis, whose elements are certain holomorphic functions in a complex domain.[25] Let U denote the unit disc in the complex plane. Then the Hardy space H2(U) is defined as the space of holomorphic functions f on U such that the meansHere \u0394 is the Laplacian and (1 \u2212 \u0394)\u2212s/2 is understood in terms of the spectral mapping theorem. Apart from providing a workable definition of Sobolev spaces for non-integer s, this definition also has particularly desirable properties under the Fourier transform that make it ideal for the study of pseudodifferential operators. Using these methods on a compact Riemannian manifold, one can obtain for instance the Hodge decomposition, which is the basis of Hodge theory.[24]Sobolev spaces are also studied from the point of view of spectral theory, relying more specifically on the Hilbert space structure. If \u03a9 is a suitable domain, then one can define the Sobolev space Hs(\u03a9) as the space of Bessel potentials;[23] roughly,where the dot indicates the dot product in the Euclidean space of partial derivatives of each order. Sobolev spaces can also be defined when s is not an integer.For s a non-negative integer and \u03a9 \u2282 \u211dn, the Sobolev space Hs(\u03a9) contains L2 functions whose weak derivatives of order up to s are also L2. The inner product in Hs(\u03a9) isSobolev spaces, denoted by Hs or Ws,2, are Hilbert spaces. These are a special kind of function space in which differentiation may be performed, but that (unlike other Banach spaces such as the H\u00f6lder spaces) support the structure of an inner product. Because differentiation is permitted, Sobolev spaces are a convenient setting for the theory of partial differential equations.[21] They also form the basis of the theory of direct methods in the calculus of variations.[22]Weighted L2 spaces like this are frequently used to study orthogonal polynomials, because different families of orthogonal polynomials are orthogonal with respect to different weighting functions.The weighted space L2\nw([0,1]) is identical with the Hilbert space L2([0,1],\u03bc) where the measure \u03bc of a Lebesgue-measurable set A is defined byis called the weighted L2 space L2\nw([0,1]), and w is called the weight function. The inner product is defined byThe Lebesgue spaces appear in many natural settings. The spaces L2(\u211d) and L2([0,1]) of square-integrable functions with respect to the Lebesgue measure on the real line and unit interval, respectively, are natural domains on which to define the Fourier transform and Fourier series. In other situations, the measure may be something other than the ordinary Lebesgue measure on the real line. For instance, if w is any positive measurable function, the space of all measurable functions f on the interval [0,1] satisfyingFor f and g in L2, this integral exists because of the Cauchy\u2013Schwarz inequality, and defines an inner product on the space. Equipped with this inner product, L2 is in fact complete.[19] The Lebesgue integral is essential to ensure completeness: on domains of real numbers, for instance, not enough functions are Riemann integrable.[20]The inner product of functions f and g in L2(X, \u03bc) is then defined asand where functions are identified if and only if they differ only on a set of measure zero.Lebesgue spaces are function spaces associated to measure spaces (X, M, \u03bc), where X is a set, M is a \u03c3-algebra of subsets of X, and \u03bc is a countably additive measure on M. Let L2(X, \u03bc) be the space of those complex-valued measurable functions on X for which the Lebesgue integral of the square of the absolute value of the function is finite, i.e., for a function f in L2(X, \u03bc),The algebra of observables in quantum mechanics is naturally an algebra of operators defined on a Hilbert space, according to Werner Heisenberg's matrix mechanics formulation of quantum theory. Von Neumann began investigating operator algebras in the 1930s, as rings of operators on a Hilbert space. The kind of algebras studied by von Neumann and his contemporaries are now known as von Neumann algebras. In the 1940s, Israel Gelfand, Mark Naimark and Irving Segal gave a definition of a kind of operator algebras called C*-algebras that on the one hand made no reference to an underlying Hilbert space, and on the other extrapolated many of the useful features of the operator algebras that had previously been studied. The spectral theorem for self-adjoint operators in particular that underlies much of the existing Hilbert space theory was generalized to C*-algebras. These techniques are now basic in abstract harmonic analysis and representation theory.The significance of the concept of a Hilbert space was underlined with the realization that it offers one of the best mathematical formulations of quantum mechanics.[17] In short, the states of a quantum mechanical system are vectors in a certain Hilbert space, the observables are hermitian operators on that space, the symmetries of the system are unitary operators, and measurements are orthogonal projections. The relation between quantum mechanical symmetries and unitary operators provided an impetus for the development of the unitary representation theory of groups, initiated in the 1928 work of Hermann Weyl.[16] On the other hand, in the early 1930s it became clear that classical mechanics can be described in terms of Hilbert space (Koopman\u2013von Neumann classical mechanics) and that certain properties of classical dynamical systems can be analyzed using Hilbert space techniques in the framework of ergodic theory.[18]Further basic results were proved in the early 20th century. For example, the Riesz representation theorem was independently established by Maurice Fr\u00e9chet and Frigyes Riesz in 1907.[12] John von Neumann coined the term abstract Hilbert space in his work on unbounded Hermitian operators.[13] Although other mathematicians such as Hermann Weyl and Norbert Wiener had already studied particular Hilbert spaces in great detail, often from a physically motivated point of view, von Neumann gave the first complete and axiomatic treatment of them.[14] Von Neumann later used them in his seminal work on the foundations of quantum mechanics,[15] and in his continued work with Eugene Wigner. The name \"Hilbert space\" was soon adopted by others, for example by Hermann Weyl in his book on quantum mechanics and the theory of groups.[16]The second development was the Lebesgue integral, an alternative to the Riemann integral introduced by Henri Lebesgue in 1904.[9] The Lebesgue integral made it possible to integrate a much broader class of functions. In 1907, Frigyes Riesz and Ernst Sigismund Fischer independently proved that the space L2 of square Lebesgue-integrable functions is a complete metric space.[10] As a consequence of the interplay between geometry and completeness, the 19th century results of Joseph Fourier, Friedrich Bessel and Marc-Antoine Parseval on trigonometric series easily carried over to these more general spaces, resulting in a geometrical and analytical apparatus now usually known as the Riesz\u2013Fischer theorem.[11]where the functions \u03c6n are orthogonal in the sense that \u27e8\u03c6n,\u03c6m\u27e9 = 0 for all n \u2260 m. The individual terms in this series are sometimes referred to as elementary product solutions. However, there are eigenfunction expansions that fail to converge in a suitable sense to a square-integrable function: the missing ingredient, which ensures convergence, is completeness.[8]where K is a continuous function symmetric in x and y. The resulting eigenfunction expansion expresses the function K as a series of the formwhich has many of the familiar properties of the Euclidean dot product. In particular, the idea of an orthogonal family of functions has meaning. Schmidt exploited the similarity of this inner product with the usual dot product to prove an analog of the spectral decomposition for an operator of the formIn the first decade of the 20th century, parallel developments led to the introduction of Hilbert spaces. The first of these was the observation, which arose during David Hilbert and Erhard Schmidt's study of integral equations,[7] that two square-integrable real-valued functions f and g on an interval [a,b] have an inner productPrior to the development of Hilbert spaces, other generalizations of Euclidean spaces were known to mathematicians and physicists. In particular, the idea of an abstract linear space had gained some traction towards the end of the 19th century:[5] this is a space whose elements can be added together and multiplied by scalars (such as real or complex numbers) without necessarily identifying these elements with \"geometric\" vectors, such as position and momentum vectors in physical systems. Other objects studied by mathematicians at the turn of the 20th century, in particular spaces of sequences (including series) and spaces of functions,[6] can naturally be thought of as linear spaces. Functions, for instance, can be added together or multiplied by constant scalars, and these operations obey the algebraic laws satisfied by addition and scalar multiplication of spatial vectors.Completeness of the space holds provided that whenever a series of elements from l2 converges absolutely (in norm), then it converges to an element of l2. The proof is basic in mathematical analysis, and permits mathematical series of elements of the space to be manipulated with the same ease as series of complex numbers (or vectors in a finite-dimensional Euclidean space).[4]with the latter series converging as a consequence of the Cauchy\u2013Schwarz inequality.converges. The inner product on l2 is defined byThe sequence space l2 consists of all infinite sequences z = (z1,z2,...) of complex numbers such that the seriesAs a complete normed space, Hilbert spaces are by definition also Banach spaces. As such they are topological vector spaces, in which topological notions like the openness and closedness of subsets are well-defined. Of special importance is the notion of a closed linear subspace of a Hilbert space that, with the inner product induced by restriction, is also complete (being a closed set in a complete metric space) and therefore a Hilbert space in its own right.then the series converges in H, in the sense that the partial sums converge to an element of H.converges absolutely in the sense thatRelative to a distance function defined in this way, any inner product space is a metric space, and sometimes is known as a pre-Hilbert space.[3] Any pre-Hilbert space that is additionally also a complete space is a Hilbert space. Completeness is expressed using a form of the Cauchy criterion for sequences in H: a pre-Hilbert space H is complete if every Cauchy sequence converges with respect to this norm to an element in the space. Completeness can be characterized by the following equivalent condition: if a series of vectorswith equality if and only if x and y are linearly dependent.This last property is ultimately a consequence of the more fundamental Cauchy\u2013Schwarz inequality, which assertsThat this function is a distance function means firstly that it is symmetric in x and y, secondly that the distance between x and itself is zero, and otherwise the distance between x and y must be positive, and lastly that the triangle inequality holds, meaning that the length of one leg of a triangle xyz cannot exceed the sum of the lengths of the other two legs:and the distance d between two points x, y in H is defined in terms of the norm byThe norm is the real-valued functionA real inner product space is defined in the same way, except that H is a real vector space and the inner product takes real values. Such an inner product will be bilinear: that is, linear in each argument.It follows from properties 1 and 2 that a complex inner product is antilinear in its second argument, meaning thatA Hilbert space H is a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product.[2] To say that H is a complex inner product space means that H is a complex vector space on which there is an inner product \u27e8x,y\u27e9 associating a complex number to each pair of elements x, y of H that satisfies the following properties:The real part of \u27e8z,w\u27e9 is then the four-dimensional Euclidean dot product. This inner product is Hermitian symmetric, which means that the result of interchanging z and w is the complex conjugate:A second example is the space \u21022 whose elements are pairs of complex numbers z = (z1, z2). Then the inner product of z with another such vector w = (w1,w2) is given byThis is complex-valued. The real part of \u27e8z,w\u27e9 gives the usual two-dimensional Euclidean dot product.The inner product of a pair of complex numbers z and w is the product of z with the complex conjugate of w:If z = x + iy is a decomposition of z into its real and imaginary parts, then the modulus is the usual Euclidean two-dimensional length:Hilbert spaces are often taken over the complex numbers. The complex plane denoted by \u2102 is equipped with a notion of magnitude, the complex modulus |z| which is defined as the square root of the product of z with its complex conjugate:This property expresses the completeness of Euclidean space: that a series that converges absolutely also converges in the ordinary sense.Just as with a series of scalars, a series of vectors that converges absolutely also converges to some limit vector L in the Euclidean space, in the sense thatconsisting of vectors in \u211d3 is absolutely convergent provided that the sum of the lengths converges as an ordinary series of real numbers:[1]Multivariable calculus in Euclidean space relies on the ability to compute limits, and to have useful criteria for concluding that limits exist. A mathematical seriesAn operation on pairs of vectors that, like the dot product, satisfies these three properties is known as a (real) inner product. A vector space equipped with such an inner product is known as a (real) inner product space. Every finite-dimensional inner product space is also a Hilbert space. The basic feature of the dot product that connects it with Euclidean geometry is that it is related to both the length (or norm) of a vector, denoted ||x||, and to the angle \u03b8 between two vectors x and y by means of the formulaThe dot product satisfies the properties:One of the most familiar examples of a Hilbert space is the Euclidean space consisting of three-dimensional vectors, denoted by \u211d3, and equipped with the dot product. The dot product takes two vectors x and y, and produces a real number x \u00b7 y. If x and y are represented in Cartesian coordinates, then the dot product is defined byGeometric intuition plays an important role in many aspects of Hilbert space theory. Exact analogs of the Pythagorean theorem and parallelogram law hold in a Hilbert space. At a deeper level, perpendicular projection onto a subspace (the analog of \"dropping the altitude\" of a triangle) plays a significant role in optimization problems and other aspects of the theory. An element of a Hilbert space can be uniquely specified by its coordinates with respect to a set of coordinate axes (an orthonormal basis), in analogy with Cartesian coordinates in the plane. When that set of axes is countably infinite, the Hilbert space can also be usefully thought of in terms of the space of infinite sequences that are square-summable. The latter space is often in the older literature referred to as the Hilbert space. Linear operators on a Hilbert space are likewise fairly concrete objects: in good cases, they are simply transformations that stretch the space by different factors in mutually perpendicular directions in a sense that is made precise by the study of their spectrum.Hilbert spaces arise naturally and frequently in mathematics and physics, typically as infinite-dimensional function spaces. The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by David Hilbert, Erhard Schmidt, and Frigyes Riesz. They are indispensable tools in the theories of partial differential equations, quantum mechanics, Fourier analysis (which includes applications to signal processing and heat transfer)\u2014and ergodic theory, which forms the mathematical underpinning of thermodynamics. John von Neumann coined the term Hilbert space for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for functional analysis. Apart from the classical Euclidean spaces, examples of Hilbert spaces include spaces of square-integrable functions, spaces of sequences, Sobolev spaces consisting of generalized functions, and Hardy spaces of holomorphic functions.The mathematical concept of a Hilbert space, named after David Hilbert, generalizes the notion of Euclidean space. It extends the methods of vector algebra and calculus from the two-dimensional Euclidean plane and three-dimensional space to spaces with any finite or infinite number of dimensions. A Hilbert space is an abstract vector space possessing the structure of an inner product that allows length and angle to be measured. Furthermore, Hilbert spaces are complete: there are enough limits in the space to allow the techniques of calculus to be used.",
            "title": "Hilbert space",
            "url": "https://en.wikipedia.org/wiki/Hilbert_space"
        },
        {
            "desc_links": [
                "/wiki/Stefan_Banach",
                "/wiki/Hans_Hahn_(mathematician)",
                "/wiki/Eduard_Helly",
                "/wiki/Function_space",
                "/wiki/David_Hilbert",
                "/wiki/Maurice_Ren%C3%A9_Fr%C3%A9chet",
                "/wiki/Frigyes_Riesz",
                "/wiki/Analysis_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Functional_analysis",
                "/wiki/Help:IPA/Polish",
                "/wiki/Complete_metric_space",
                "/wiki/Normed_vector_space",
                "/wiki/Metric_(mathematics)",
                "/wiki/Norm_(mathematics)",
                "/wiki/Cauchy_sequence",
                "/wiki/Limit_of_a_sequence"
            ],
            "links": [
                "/wiki/Distribution_(mathematics)",
                "/wiki/Fr%C3%A9chet_space",
                "/wiki/Metric_space",
                "/wiki/LF-space",
                "/wiki/Uniform_space",
                "/wiki/Fr%C3%A9chet_derivative",
                "/wiki/G%C3%A2teaux_derivative",
                "/wiki/Directional_derivative",
                "/wiki/Directional_derivative",
                "/wiki/Locally_convex",
                "/wiki/Topological_vector_space",
                "/wiki/Quasi-derivative",
                "/wiki/Order_topology",
                "/wiki/Countable_set",
                "/wiki/Ordinal_number",
                "/wiki/Homeomorphism",
                "/wiki/Banach_space#Examples_of_dual_spaces",
                "/wiki/Timothy_Gowers",
                "/wiki/Schauder_basis#Unconditionality",
                "/wiki/Nicole_Tomczak-Jaegermann",
                "/wiki/Dvoretzky%27s_theorem",
                "/wiki/Parseval%27s_theorem",
                "/wiki/Hermitian_symmetry",
                "/wiki/Inner_product_space",
                "/wiki/Antilinear_map",
                "/wiki/Lp_space",
                "/wiki/Polarization_identity",
                "/wiki/Parallelogram_identity",
                "/wiki/Approximation_property",
                "/wiki/Topological_tensor_product#Cross_norms_and_tensor_products_of_Banach_spaces",
                "/wiki/Topological_tensor_product#Cross_norms_and_tensor_products_of_Banach_spaces",
                "/wiki/Alexander_Grothendieck",
                "/wiki/Tensor_product",
                "/wiki/Universal_property",
                "/wiki/Schauder_basis#Schauder_bases_and_duality",
                "/wiki/Approximation_property",
                "/wiki/Per_Enflo",
                "/wiki/Haar_wavelet",
                "/wiki/Schauder_basis#Examples",
                "/wiki/Haar_wavelet#Haar_system_on_the_unit_interval_and_related_systems",
                "/wiki/Haar_wavelet#Haar_system_on_the_unit_interval_and_related_systems",
                "/wiki/Separable_space",
                "/wiki/Pointwise_convergence",
                "/wiki/Baire_function",
                "/wiki/Schur%27s_property",
                "/wiki/Schauder_basis#Examples",
                "/wiki/Uniform_boundedness_principle",
                "/wiki/Errett_Bishop",
                "/wiki/Robert_Phelps",
                "/wiki/James%27_theorem",
                "/wiki/Infinite-dimensional_optimization",
                "/wiki/Convex_function",
                "/wiki/Convex_set",
                "/wiki/Uniformly_convex_space",
                "/wiki/Milman%E2%80%93Pettis_theorem",
                "/wiki/Reflexive_space",
                "/wiki/Fr%C3%A9chet_space",
                "/wiki/Baire_category_theorem",
                "/wiki/Fr%C3%A9chet_space",
                "/wiki/F-space",
                "/wiki/Interior_(topology)",
                "/wiki/Hamel_basis",
                "/wiki/Banach_space#Weak_convergences_of_sequences",
                "/wiki/Goldstine_theorem",
                "/wiki/Net_(mathematics)",
                "/wiki/Surjective",
                "/wiki/Banach_space#Reflexivity",
                "/wiki/Banach_space#Dual_space",
                "/wiki/Injective",
                "/wiki/C*-algebra",
                "/wiki/Israel_Gelfand",
                "/wiki/Gelfand_representation",
                "/wiki/Spectrum_of_a_C*-algebra#Examples",
                "/wiki/Gelfand%E2%80%93Mazur_theorem",
                "/wiki/Banach_algebra#Ideals_and_characters",
                "/wiki/Hull-kernel_topology",
                "/wiki/Banach_algebra",
                "/wiki/Banach_algebra#Ideals_and_characters",
                "/wiki/Banach%E2%80%93Mazur_compactum",
                "/wiki/Radon_measure",
                "/wiki/Probability_measure",
                "/wiki/Extreme_point",
                "/wiki/Dirac_measure",
                "/wiki/Homeomorphism",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Antilinear_map",
                "/wiki/Lp_space#Properties_of_Lp_spaces",
                "/wiki/Tychonoff%27s_theorem",
                "/wiki/Metrization_theorem",
                "/wiki/Weak_topology",
                "/wiki/Banach%E2%80%93Alaoglu_theorem",
                "/wiki/Dual_space#Algebraic_dual_space",
                "/wiki/Finer_topology",
                "/wiki/Weak_topology",
                "/wiki/Comparison_of_topologies",
                "/wiki/Comparison_of_topologies",
                "/wiki/Hausdorff_space",
                "/wiki/Convex_set",
                "/wiki/Linear_span",
                "/wiki/Dense_set",
                "/wiki/Hahn%E2%80%93Banach_separation_theorem",
                "/wiki/Convex_set",
                "/wiki/Affine_space",
                "/wiki/Hyperplane",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Dual_space#Continuous_dual_space",
                "/wiki/Absolute_value",
                "/wiki/Banach_algebra",
                "/wiki/Algebra_over_a_field",
                "/wiki/Hardy_space",
                "/wiki/Sobolev_space",
                "/wiki/Harmonic_analysis",
                "/wiki/Partial_differential_equation",
                "/wiki/Inner_product_space",
                "/wiki/Hilbert_space",
                "/wiki/Banach%E2%80%93Mazur_theorem",
                "/wiki/Lp_space",
                "/wiki/Sequence_space_(mathematics)",
                "/wiki/Natural_number",
                "/wiki/Absolute_convergence",
                "/wiki/Range_(mathematics)",
                "/wiki/Projection_(linear_algebra)",
                "/wiki/Closed_set",
                "/wiki/Linear_subspace",
                "/wiki/Isometry",
                "/wiki/Reflexive_space",
                "/wiki/Separable_space",
                "/wiki/Isometry",
                "/wiki/Banach%E2%80%93Mazur_distance",
                "/wiki/Banach_algebra",
                "/wiki/Ground_field",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Linear_transformation",
                "/wiki/Bounded_operator",
                "/wiki/Unit_sphere",
                "/wiki/Operator_norm",
                "/wiki/Norm_(mathematics)#Definition",
                "/wiki/Series_(mathematics)#Generalizations",
                "/wiki/Series_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Norm_(mathematics)",
                "/wiki/Complete_metric_space",
                "/wiki/Cauchy_sequence",
                "/wiki/Stefan_Banach",
                "/wiki/Hans_Hahn_(mathematician)",
                "/wiki/Eduard_Helly",
                "/wiki/Function_space",
                "/wiki/David_Hilbert",
                "/wiki/Maurice_Ren%C3%A9_Fr%C3%A9chet",
                "/wiki/Frigyes_Riesz",
                "/wiki/Analysis_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Functional_analysis",
                "/wiki/Help:IPA/Polish",
                "/wiki/Complete_metric_space",
                "/wiki/Normed_vector_space",
                "/wiki/Metric_(mathematics)",
                "/wiki/Norm_(mathematics)",
                "/wiki/Cauchy_sequence",
                "/wiki/Limit_of_a_sequence"
            ],
            "text": "Several important spaces in functional analysis, for instance the space of all infinitely often differentiable functions R \u2192 R, or the space of all distributions on R, are complete but are not normed vector spaces and hence not Banach spaces. In Fr\u00e9chet spaces one still has a complete metric, while LF-spaces are complete uniform vector spaces arising as limits of Fr\u00e9chet spaces.Several concepts of a derivative may be defined on a Banach space. See the articles on the Fr\u00e9chet derivative and the G\u00e2teaux derivative for details. The Fr\u00e9chet derivative allows for an extension of the concept of a directional derivative to Banach spaces. The G\u00e2teaux derivative allows for an extension of a directional derivative to locally convex topological vector spaces. Fr\u00e9chet differentiability is a stronger condition than G\u00e2teaux differentiability. The quasi-derivative is another generalization of directional derivative that implies a stronger condition than G\u00e2teaux differentiability, but a weaker condition than Fr\u00e9chet differentiability.A glossary of symbols:are mutually non-isomorphic.equipped with the order topology, where \u03b1 is a countably infinite ordinal.[59] The Banach space C(K) is then isometric to C(<1, \u03b1\u2009>). When \u03b1, \u03b2 are two countably infinite ordinals, and assuming \u03b1 \u2264 \u03b2, the spaces C(<1, \u03b1\u2009>) and C(<1, \u03b2\u2009>) are isomorphic if and only if \u03b2 < \u03b1\u03c9.[60] For example, the Banach spacesThe situation is different for countably infinite compact Hausdorff spaces. Every countably infinite compact K is homeomorphic to some closed interval of ordinal numbersWhen two compact Hausdorff spaces K1 and K2 are homeomorphic, the Banach spaces C(K1) and C(K2) are isometric. Conversely, when K1 is not homeomorphic to K2, the (multiplicative) Banach\u2013Mazur distance between C(K1) and C(K2) must be greater than or equal to 2, see above the results by Amir and Cambern. Although uncountable compact metric spaces can have different homeomorphy types, one has the following result due to Milutin:[57]An infinite-dimensional Banach space is hereditarily indecomposable when no subspace of it can be isomorphic to the direct sum of two infinite-dimensional Banach spaces. The Gowers dichotomy theorem[54] asserts that every infinite-dimensional Banach space X contains, either a subspace Y with unconditional basis, or a hereditarily indecomposable subspace Z, and in particular, Z is not isomorphic to its closed hyperplanes.[55] If X is homogeneous, it must therefore have an unconditional basis. It follows then from the partial solution obtained by Komorowski and Tomczak\u2013Jaegermann, for spaces with an unconditional basis,[56] that X is isomorphic to \u21132.The next result gives the solution of the so-called homogeneous space problem. An infinite-dimensional Banach space X is said to be homogeneous if it is isomorphic to all its infinite-dimensional closed subspaces. A Banach space isomorphic to \u21132 is homogeneous, and Banach asked for the converse.[53]Lindenstrauss and Tzafriri proved that a Banach space in which every closed linear subspace is complemented (that is, is the range of a bounded linear projection) is isomorphic to a Hilbert space.[52] The proof rests upon Dvoretzky's theorem about Euclidean sections of high-dimensional centrally symmetric convex bodies. In other words, Dvoretzky's theorem states that for every integer n, any finite-dimensional normed space, with dimension sufficiently large compared to n, contains subspaces nearly isometric to the n-dimensional Euclidean space.for every integer n and all families of vectors {x1, ..., xn} \u2282 X, then the Banach space X is isomorphic to a Hilbert space.[51] Here, Ave\u00b1 denotes the average over the 2n possible choices of signs \u00b11. In the same article, Kwapie\u0144 proved that the validity of a Banach-valued Parseval's theorem for the Fourier transform characterizes Banach spaces isomorphic to Hilbert spaces.Several characterizations of spaces isomorphic (rather than isometric) to Hilbert spaces are available. The parallelogram law can be extended to more than two vectors, and weakened by the introduction of a two-sided inequality with a constant c \u2265 1: Kwapie\u0144 proved that ifTo see that the parallelogram law is sufficient, one observes in the real case that <\u2009x, y\u2009> is symmetric, and in the complex case, that it satisfies the Hermitian symmetry property and <\u2009ix, y\u2009> = i\u2009<\u2009x, y\u2009>. The parallelogram law implies that <\u2009x, y\u2009> is additive in x. It follows that it is linear over the rationals, thus linear by continuity.For complex scalars, defining the inner product so as to be C-linear in x, antilinear in y, the polarization identity gives:It follows, for example, that the Lebesgue space Lp([0, 1]) is a Hilbert space only when p = 2. If this identity is satisfied, the associated inner product is given by the polarization identity. In the case of real scalars, this gives:A necessary and sufficient condition for the norm of a Banach space X to be associated to an inner product is the parallelogram identity:is one-to-one if and only if X has the approximation property.[48]obtained by extending the identity map of the algebraic tensor product. Grothendieck related the approximation problem to the question of whether this map is one-to-one when Y is the dual of X. Precisely, for every Banach space X, the mapFor every Banach space Y, there is a natural norm 1 linear mapwhere K is a compact Hausdorff space, C(K, Y) the Banach space of continuous functions from K to Y and L1([0, 1], Y) the space of Bochner-measurable and integrable functions from [0, 1] to Y, and where the isomorphisms are isometric. The two isomorphisms above are the respective extensions of the map sending the tensor \u2009f\u2009\u2009\u2297\u2009y to the vector-valued function s \u2208 K \u2192 \u2009f\u2009(s)y \u2208 Y.There are various norms that can be placed on the tensor product of the underlying vector spaces, amongst others the projective cross norm and injective cross norm introduced by A. Grothendieck in 1955.[44]The image under T of a couple (x, y) in X \u00d7 Y is denoted by x \u2297 y, and called a simple tensor. Every element z in X \u2297 Y is a finite sum of such simple tensors.Let X and Y be two K-vector spaces. The tensor product X \u2297 Y of X and Y is a K-vector space Z with a bilinear mapping T\u00a0: X \u00d7 Y \u2192 Z which has the following universal property:Robert C. James characterized reflexivity in Banach spaces with a basis: the space X with a Schauder basis is reflexive if and only if the basis is both shrinking and boundedly complete.[43] In this case, the biorthogonal functionals form a basis of the dual of X.Since every vector x in a Banach space X with a basis is the limit of Pn(x), with Pn of finite rank and uniformly bounded, the space X satisfies the bounded approximation property. The first example by Enflo of a space failing the approximation property was at the same time the first example of a separable Banach space without a Schauder basis.[42]Most classical separable spaces have explicit bases. The Haar system {hn} is a basis for Lp([0, 1]), 1 \u2264 p < \u221e. The trigonometric system is a basis in Lp(T) when 1 < p < \u221e. The Schauder system is a basis in the space C([0, 1]).[39] The question of whether the disk algebra A(D) has a basis[40] remained open for more than forty years, until Bo\u010dkarev showed in 1974 that A(D) admits a basis constructed from the Franklin system.[41]It follows from the Banach\u2013Steinhaus theorem that the linear mappings {Pn} are uniformly bounded by some constant C. Let {e\u2217\nn} denote the coordinate functionals which assign to every x in X the coordinate xn of x in the above expansion. They are called biorthogonal functionals. When the basis vectors have norm 1, the coordinate functionals {e\u2217\nn} have norm \u2264 2C in the dual of X.Banach spaces with a Schauder basis are necessarily separable, because the countable set of finite linear combinations with rational coefficients (say) is dense.A Schauder basis in a Banach space X is a sequence {en}n\u2009\u2265\u20090 of vectors in X with the property that for every vector x in X, there exist uniquely defined scalars {xn}n\u2009\u2265\u20090 depending on x, such thatA weakly compact subset A in \u21131 is norm-compact. Indeed, every sequence in A has weakly convergent subsequences by Eberlein\u2013\u0160mulian, that are norm convergent by the Schur property of \u21131.A Banach space X is reflexive if and only if each bounded sequence in X has a weakly convergent subsequence.[38]The weak topology of a Banach space X is metrizable if and only if X is finite-dimensional.[36] If the dual X\u2009\u2032 is separable, the weak topology of the unit ball of X is metrizable. This applies in particular to separable reflexive Banach spaces. Although the weak topology of the unit ball is not metrizable in general, one can characterize weak compactness using sequences.When X is separable, the unit ball of the dual is weak*-compact by Banach\u2013Alaoglu and metrizable for the weak* topology,[17] hence every bounded sequence in the dual has weakly* convergent subsequences. This applies to separable reflexive spaces, but more is true in this case, as stated below.This function is continuous for the compact topology of K if and only if x\u2009\u2032\u2032 is actually in X, considered as subset of X\u2009\u2032\u2032. Assume in addition for the rest of the paragraph that X does not contain \u21131. By the preceding result of Odell and Rosenthal, the function x\u2009\u2032\u2032 is the pointwise limit on K of a sequence {xn} \u2282 X of continuous functions on K, it is therefore a first Baire class function on K. The unit ball of the bidual is a pointwise compact subset of the first Baire class on K.[35]When the Banach space X is separable, the unit ball of the dual X\u2009\u2032, equipped with the weak*-topology, is a metrizable compact space K,[17] and every element x\u2009\u2032\u2032 in the bidual X\u2009\u2032\u2032 defines a bounded function on K:By the Goldstine theorem, every element of the unit ball B\u2009\u2032\u2032 of X\u2009\u2032\u2032 is weak*-limit of a net in the unit ball of X. When X does not contain \u21131, every element of B\u2009\u2032\u2032 is weak*-limit of a sequence in the unit ball of X.[34]A complement to this result is due to Odell and Rosenthal\u00a0(1975).Weakly Cauchy sequences and the \u21131 basis are the opposite cases of the dichotomy established in the following deep result of\u00a0H.\u00a0P.\u00a0Rosenthal.[31]The unit vector basis of \u21131 is not weakly Cauchy. Weakly Cauchy sequences in \u21131 are weakly convergent, since L1-spaces are weakly sequentially complete. Actually, weakly convergent sequences in \u21131 are norm convergent.[30] This means that \u21131 satisfies Schur's property.An orthonormal sequence in a Hilbert space is a simple example of a weakly convergent sequence, with limit equal to the 0 vector. The unit vector basis of \u2113p, 1 < p < \u221e, or of c0, is another example of a weakly null sequence, i.e., a sequence that converges weakly to 0. For every weakly null sequence in a Banach space, there exists a sequence of convex combinations of vectors from the given sequence that is norm-converging to 0.[29]When the sequence {xn} in X is a weakly Cauchy sequence, the limit L above defines a bounded linear functional on the dual X\u2009\u2032, i.e., an element L of the bidual of X, and L is the limit of {xn} in the weak*-topology of the bidual. The Banach space X is weakly sequentially complete if every weakly Cauchy sequence is weakly convergent in X. It follows from the preceding discussion that reflexive spaces are weakly sequentially complete.A sequence {xn} in a Banach space X is weakly convergent to a vector x \u2208 X if \u2009f\u2009(xn) converges to \u2009f\u2009(x) for every continuous linear functional \u2009f\u2009 in the dual X\u2009\u2032. The sequence {xn} is a weakly Cauchy sequence if \u2009f\u2009(xn) converges to a scalar limit L(\u2009f\u2009), for every \u2009f\u2009 in X\u2009\u2032. A sequence {\u2009fn\u2009} in the dual X\u2009\u2032 is weakly* convergent to a functional \u2009f\u2009 \u2208 X\u2009\u2032 if \u2009fn\u2009(x) converges to \u2009f\u2009(x) for every x in X. Weakly Cauchy sequences, weakly convergent and weakly* convergent sequences are norm bounded, as a consequence of the Banach\u2013Steinhaus theorem.On every non-reflexive Banach space X, there exist continuous linear functionals that are not norm-attaining. However, the Bishop\u2013Phelps theorem[27] states that norm-attaining functionals are norm dense in the dual X\u2009\u2032 of X.The theorem can be extended to give a characterization of weakly compact convex sets.As a special case of the preceding result, when X is a reflexive space over R, every continuous linear functional \u2009f\u2009 in X\u2009\u2032 attains its maximum ||\u2009f\u2009|| on the unit ball of X. The following theorem of Robert C. James provides a converse statement.Weak compactness of the unit ball provides a tool for finding solutions in reflexive spaces to certain optimization problems. For example, every convex continuous function on the unit ball B of a reflexive space attains its minimum at some point in B.When X is reflexive, it follows that all closed and bounded convex subsets of X are weakly compact. In a Hilbert space H, the weak compactness of the unit ball is very often used in the following way: every bounded sequence in H has weakly convergent subsequences.Hilbert spaces are reflexive. The Lp spaces are reflexive when 1 < p < \u221e. More generally, uniformly convex spaces are reflexive, by the Milman\u2013Pettis theorem. The spaces c0, \u21131, L1([0, 1]), C([0, 1]) are not reflexive. In these examples of non-reflexive spaces X, the bidual X\u2009\u2032\u2032 is \"much larger\" than X. Namely, under the natural isometric embedding of X into X\u2009\u2032\u2032 given by the Hahn\u2013Banach theorem, the quotient X\u2009\u2032\u2032\u2009/\u2009X is infinite-dimensional, and even nonseparable. However, Robert C. James has constructed an example[25] of a non-reflexive space, usually called \"the James space\" and denoted by J,[26] such that the quotient J\u2009\u2032\u2032\u2009/\u2009J is one-dimensional. Furthermore, this space J is isometrically isomorphic to its bidual.Indeed, if the dual Y\u2009\u2032 of a Banach space Y is separable, then Y is separable. If X is reflexive and separable, then the dual of X\u2009\u2032 is separable, so X\u2009\u2032 is separable.This is a consequence of the Hahn\u2013Banach theorem. Further, by the open mapping theorem, if there is a bounded linear operator from the Banach space X onto the Banach space Y, then Y is reflexive.is surjective. Reflexive normed spaces are Banach spaces.The normed space X is called reflexive when the natural mapThis is another consequence of Banach's isomorphism theorem, applied to the continuous bijection from M1 \u2295 ... \u2295 Mn onto X sending (m1, ..., mn) to the sum m1 + ... + mn.This result is a direct consequence of the preceding Banach isomorphism theorem and of the canonical factorization of bounded linear maps.The Banach\u2013Steinhaus theorem is not limited to Banach spaces. It can be extended for example to the case where X is a Fr\u00e9chet space, provided the conclusion is modified as follows: under the same hypothesis, there exists a neighborhood U of 0 in X such that all T in F are uniformly bounded on U,Here are the main general results about Banach spaces that go back to the time of Banach's book (Banach (1932)) and are related to the Baire category theorem. According to this theorem, a complete metric space (such as a Banach space, a Fr\u00e9chet space or an F-space) cannot be equal to a union of countably many closed subsets with empty interiors. Therefore, a Banach space cannot be the union of countably many closed subspaces, unless it is already equal to one of them; a Banach space with a countable Hamel basis is finite-dimensional.The net may be replaced by a weakly*-convergent sequence when the dual X\u2009\u2032 is separable. On the other hand, elements of the bidual of \u21131 that are not in \u21131 cannot be weak*-limit of sequences in \u21131, since \u21131 is weakly sequentially complete.Using the isometric embedding FX, it is customary to consider a normed space X as a subset of its bidual. When X is a Banach space, it is viewed as a closed linear subspace of X\u2009\u2032\u2032. If X is not reflexive, the unit ball of X is a proper subset of the unit ball of X\u2009\u2032\u2032. The Goldstine theorem states that the unit ball of a normed space is weakly*-dense in the unit ball of the bidual. In other words, for every x\u2009\u2032\u2032 in the bidual, there exists a net {xj} in X so thatIf FX is surjective, then the normed space X is called reflexive (see below). Being the dual of a normed space, the bidual X\u2009\u2032\u2032 is complete, therefore, every reflexive normed space is a Banach space.For example, the dual of X = c0 is identified with \u21131, and the dual of \u21131 is identified with \u2113\u221e, the space of bounded scalar sequences. Under these identifications, FX is the inclusion map from c0 to \u2113\u221e. It is indeed isometric, but not onto.This defines FX(x) as a continuous linear functional on X\u2009\u2032, i.e., an element of X\u2009\u2032\u2032. The map FX\u00a0: x \u2192 FX(x) is a linear map from X to X\u2009\u2032\u2032. As a consequence of the existence of a norming functional \u2009f\u2009 for every x in X, this map FX is isometric, thus injective.If X is a normed space, the (continuous) dual X\u2009\u2032\u2032 of the dual X\u2009\u2032 is called bidual, or second dual of X. For every normed space X, there is a natural map,Not every unital commutative Banach algebra is of the form C(K) for some compact Hausdorff space K. However, this statement holds if one places C(K) in the smaller category of commutative C*-algebras. Gelfand's representation theorem for commutative C*-algebras states that every commutative unital C*-algebra A is isometrically isomorphic to a C(K) space.[24] The Hausdorff compact space K here is again the maximal ideal space, also called the spectrum of A in the C*-algebra context.More generally, by the Gelfand\u2013Mazur theorem, the maximal ideals of a unital commutative Banach algebra can be identified with its characters\u2014not merely as sets but as topological spaces: the former with the hull-kernel topology and the latter with the w*-topology. In this identification, the maximal ideal space can be viewed as a w*-compact subset of the unit ball in the dual A\u2009\u2032.In the commutative Banach algebra C(K), the maximal ideals are precisely kernels of Dirac mesures on K,The result has been extended by Amir[21] and Cambern[22] to the case when the multiplicative Banach\u2013Mazur distance between C(K) and C(L) is < 2. The theorem is no longer true when the distance is = 2.[23]When K is a compact Hausdorff topological space, the dual M(K) of C(K) is the space of Radon measures in the sense of Bourbaki.[18] The subset P(K) of M(K) consisting of non-negative measures of mass 1 (probability measures) is a convex w*-closed subset of the unit ball of M(K). The extreme points of P(K) are the Dirac measures on K. The set of Dirac measures on K, equipped with the w*-topology, is homeomorphic to K.defines a continuous linear functional \u2009fy\u2009 on H. The Riesz representation theorem states that every continuous linear functional on H is of the form \u2009fy\u2009 for a uniquely defined vector y in H. The mapping y \u2208 H \u2192 \u2009fy\u2009 is an antilinear isometric bijection from H onto its dual H\u2009\u2032. When the scalars are real, this map is an isometric isomorphism.For every vector y in a Hilbert space H, the mappingThe dual of \u21131 is isometrically isomorphic to \u2113\u221e. The dual of Lp([0, 1]) is isometrically isomorphic to Lq([0, 1]) when 1 \u2264 p < \u221e and \u20091/p + 1/q = 1.The dual of c0 is isometrically isomorphic to \u21131: for every bounded linear functional \u2009f\u2009 on c0, there is a unique element y = {yn} \u2208 \u21131 such thatThe Banach\u2013Alaoglu theorem depends on Tychonoff's theorem about infinite products of compact spaces. When X is separable, the unit ball B\u2009\u2032 of the dual is a metrizable compact in the weak* topology.[17]On a dual space X\u2009\u2032, there is a topology weaker than the weak topology of X\u2009\u2032, called weak* topology. It is the coarsest topology on X\u2009\u2032 for which all evaluation maps x\u2032\u2009\u2208\u2009X\u2009\u2032 \u2192 x\u2032(x), x\u2009\u2208\u2009X, are continuous. Its importance comes from the Banach\u2013Alaoglu theorem.If X is infinite-dimensional, there exist linear maps which are not continuous. The space X\u2217 of all linear maps from X to the underlying field K (this space X\u2217 is called the algebraic dual space, to distinguish it from X\u2009\u2032) also induces a topology on X which is finer than the weak topology, and much less used in functional analysis.The weak topology on a Banach space X is the coarsest topology on X for which all elements x\u2009\u2032 in the continuous dual space X\u2009\u2032 are continuous. The norm topology is therefore finer than the weak topology. It follows from the Hahn\u2013Banach separation theorem that the weak topology is Hausdorff, and that a norm-closed convex subset of a Banach space is also weakly closed.[15] A norm-continuous linear map between two Banach spaces X and Y is also weakly continuous, i.e., continuous from the weak topology of X to that of Y.[16]When X\u2009\u2032 is separable, the above criterion for totality can be used for proving the existence of a countable total subset in X.The dual of a separable Banach space need not be separable, but:The orthogonal M\u2009\u22a5 is a closed linear subspace of the dual. The dual of M is isometrically isomorphic to X\u2009\u2032\u2009/\u2009M\u2009\u22a5. The dual of X\u2009/\u2009M is isometrically isomorphic to M\u2009\u22a5.[13]If X is the direct sum of two closed linear subspaces M and N, then the dual X\u2009\u2032 of X is isomorphic to the direct sum of the duals of M and N.[12] If M is a closed linear subspace in X, one can associate the orthogonal of M in the dual,A subset S in a Banach space X is total if the linear span of S is dense in X. The subset S is total in X if and only if the only continuous linear functional that vanishes on S is the 0 functional: this equivalence follows from the Hahn\u2013Banach theorem.The Hahn\u2013Banach separation theorem states that two disjoint non-empty convex sets in a real Banach space, one of them open, can be separated by a closed affine hyperplane. The open convex set lies strictly on one side of the hyperplane, the second convex set lies on the other side but may touch the hyperplane.[11]When x is not equal to the 0 vector, the functional \u2009f\u2009 must have norm one, and is called a norming functional for x.In particular, every continuous linear functional on a subspace of a normed space can be continuously extended to the whole space, without increasing the norm of the functional.[10] An important special case is the following: for every vector x in a normed space X, there exists a continuous linear functional \u2009f\u2009 on X such thatThe main tool for proving the existence of continuous linear functionals is the Hahn\u2013Banach theorem.If X is a normed space and K the underlying field (either the real or the complex numbers), the continuous dual space is the space of continuous linear maps from X into K, or continuous linear functionals. The notation for the continuous dual is X\u2009\u2032 = B(X, K) in this article.[9] Since K is a Banach space (using the absolute value as norm), the dual X\u2009\u2032 is a Banach space, for every normed space X.A Banach algebra is a Banach space A over K = R or C, together with a structure of algebra over K, such that the product map A \u00d7 A \u220b (a, b) \u21a6 ab \u2208 A is continuous. An equivalent norm on A can be found so that ||ab|| \u2264 ||a||\u2009||b|| for all a, b \u2208 A.The Hardy spaces, the Sobolev spaces are examples of Banach spaces that are related to Lp spaces and have additional structure. They are important in different branches of analysis, Harmonic analysis and Partial differential equations among others.For example, the space L2 is a Hilbert space.is the inner product, linear in its first argument that satisfies the following:whereAny Hilbert space serves as an example of a Banach space. A Hilbert space H on K = R, C is complete for a norm of the formAccording to the Banach\u2013Mazur theorem, every Banach space is isometrically isomorphic to a subspace of some C(K).[7] For every separable Banach space X, there is a closed subspace M of \u21131 such that X \u2245 \u21131/M.[8]Basic examples[6] of Banach spaces include: the Lp spaces and their special cases, the sequence spaces \u2113p that consist of scalar sequences indexed by N; among them, the space \u21131 of absolutely summable sequences and the space \u21132 of square summable sequences; the space c0 of sequences tending to zero and the space \u2113\u221e of bounded sequences; the space C(K) of continuous scalar functions on a compact Hausdorff space K, equipped with the max norm,where the first map \u03c0 is the quotient map, and the second map T1 sends every class x + Ker(T) in the quotient to the image T(x) in Y. This is well defined because all elements in the same class have the same image. The mapping T1 is a linear bijection from X\u2009/\u2009Ker(T) onto the range T(X), whose inverse need not be bounded.Suppose that X and Y are Banach spaces and that T \u2208 B(X, Y). There exists a canonical factorization of T as[5]The closed linear subspace M of X is said to be a complemented subspace of X if M is the range of a bounded linear projection P from X onto M. In this case, the space X is isomorphic to the direct sum of M and Ker(P), the kernel of the projection P.The quotient X\u2009/\u2009M is a Banach space when X is complete.[5] The quotient map from X onto X\u2009/\u2009M, sending x in X to its class x + M, is linear, onto and has norm 1, except when M = X, in which case the quotient is the null space.If M is a closed linear subspace of a normed space X, there is a natural norm on the quotient space X\u2009/\u2009M,and give rise to isomorphic normed spaces. In this sense, the product X \u00d7 Y (or the direct sum X \u2295 Y) is complete if and only if the two factors are complete.The cartesian product X \u00d7 Y of two normed spaces is not canonically equipped with a norm. However, several equivalent norms are commonly used,[4] such asEvery normed space X can be isometrically embedded in a Banach space. More precisely, there is a Banach space Y and an isometric mapping T\u00a0: X \u2192 Y such that T(X) is dense in Y. If Z is another Banach space such that there is an isometric isomorphism from X onto a dense subset of Z, then Z is isometrically isomorphic to Y.If X and Y are normed spaces, they are isomorphic normed spaces if there exists a linear bijection T\u00a0: X \u2192 Y such that T and its inverse T\u2009\u22121 are continuous. If one of the two spaces X or Y is complete (or reflexive, separable, etc.) then so is the other space. Two normed spaces X and Y are isometrically isomorphic if in addition, T is an isometry, i.e., ||T(x)|| = ||x|| for every x in X. The Banach\u2013Mazur distance d(X, Y) between two isomorphic but not isometric spaces X and Y gives a measure of how much the two spaces X and Y differ.If X is a Banach space, the space B(X) = B(X, X) forms a unital Banach algebra; the multiplication operation is given by the composition of linear maps.For Y a Banach space, the space B(X, Y) is a Banach space with respect to this norm.If X and Y are normed spaces over the same ground field K, the set of all continuous K-linear maps T\u00a0: X \u2192 Y is denoted by B(X, Y). In infinite-dimensional spaces, not all linear maps are continuous. A linear mapping from a normed space X to another normed space is continuous if and only if it is bounded on the closed unit ball of X. Thus, the vector space B(X, Y) can be given the operator normAll norms on a finite-dimensional vector space are equivalent. Every finite-dimensional normed space over R or C is a Banach space.[3]Completeness of a normed space is preserved if the given norm is replaced by an equivalent one.The vector space structure allows one to relate the behavior of Cauchy sequences to that of converging series of vectors. A normed space X is a Banach space if and only if each absolutely convergent series in X converges,[2]or equivalently:A Banach space is a vector space X over the field R of real numbers, or over the field C of complex numbers, which is equipped with a norm and which is complete with respect to that norm, that is to say, for every Cauchy sequence {xn} in X, there exists an element x in X such thatBanach spaces are named after the Polish mathematician Stefan Banach, who introduced this concept and studied it systematically in 1920\u20131922 along with Hans Hahn and Eduard Helly.[1] Banach spaces originally grew out of the study of function spaces by Hilbert, Fr\u00e9chet, and Riesz earlier in the century. Banach spaces play a central role in functional analysis. In other areas of analysis, the spaces under study are often Banach spaces.In mathematics, more specifically in functional analysis, a Banach space (pronounced [\u02c8banax]) is a complete normed vector space. Thus, a Banach space is a vector space with a metric that allows the computation of vector length and distance between vectors and is complete in the sense that a Cauchy sequence of vectors always converges to a well defined limit that is within the space.",
            "title": "Banach space",
            "url": "https://en.wikipedia.org/wiki/Banach_space"
        },
        {
            "desc_links": [
                "/wiki/Linear_map",
                "/wiki/Schwarzian_derivative",
                "/wiki/Mathematics",
                "/wiki/Operator_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Higher-order_function",
                "/wiki/Computer_science"
            ],
            "links": [
                "/wiki/Louis_Fran%C3%A7ois_Antoine_Arbogast",
                "/wiki/Several_complex_variables",
                "/wiki/Motor_variable",
                "/wiki/Module_(mathematics)",
                "/wiki/Algebra_(ring_theory)",
                "/wiki/Commutative_algebra",
                "/wiki/Vector_bundle",
                "/wiki/Sheaf_(mathematics)",
                "/wiki/Peetre_theorem",
                "/wiki/Jet_(mathematics)",
                "/wiki/Differential_geometry",
                "/wiki/Algebraic_geometry",
                "/wiki/Coordinate",
                "/wiki/Vector_bundle",
                "/wiki/Differentiable_manifold",
                "/wiki/Vector_bundle",
                "/wiki/Jet_bundle",
                "/wiki/Partial_derivative",
                "/wiki/Symmetry_of_second_derivatives",
                "/wiki/Shift_theorem",
                "/wiki/Constant_coefficients",
                "/wiki/Differentiable_function",
                "/wiki/Ring_(mathematics)",
                "/wiki/Commutative",
                "/wiki/Quantum_mechanics",
                "/wiki/Linearity_of_differentiation",
                "/wiki/Sturm%E2%80%93Liouville_theory",
                "/wiki/Eigenfunctions",
                "/wiki/Eigenvectors",
                "/wiki/Sturm%E2%80%93Liouville_theory",
                "/wiki/Densely_defined_operator",
                "/wiki/Lp_space",
                "/wiki/Self-adjoint_operator",
                "/wiki/Square-integrable_function",
                "/wiki/Gradient",
                "/wiki/Curl_(mathematics)",
                "/wiki/Divergence",
                "/wiki/Laplacian",
                "/wiki/Euclidean_vector",
                "/wiki/Physics",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Cartesian_coordinates",
                "/wiki/Probability_current",
                "/wiki/Eigenspace",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Eigenfunction",
                "/wiki/Monomial",
                "/wiki/Theta_operator",
                "/wiki/Laplace_operator",
                "/wiki/Differential_equation",
                "/wiki/Oliver_Heaviside",
                "/wiki/Derivative",
                "/wiki/Linear_map",
                "/wiki/Schwarzian_derivative",
                "/wiki/Mathematics",
                "/wiki/Operator_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Higher-order_function",
                "/wiki/Computer_science"
            ],
            "text": "The conceptual step of writing a differential operator as something free-standing is attributed to Louis Fran\u00e7ois Antoine Arbogast in 1800.[2]This approach is also used to study functions of several complex variables and functions of a motor variable.This characterization of linear differential operators shows that they are particular mappings between modules over a commutative algebra, allowing the concept to be seen as a part of commutative algebra.This just means that for a given section s of E, the value of P(s) at a point x\u00a0\u2208\u00a0M is fully determined by the kth-order infinitesimal behavior of s in x. In particular this implies that P(s)(x) is determined by the germ of s in x, which is expressed by saying that differential operators are local. A foundational result is the Peetre theorem showing that the converse is also true: any (linear) local operator is differential.where jk: \u0393(E) \u2192 \u0393(Jk(E)) is the prolongation that associates to any section of E its k-jet.such thatIn differential geometry and algebraic geometry it is often convenient to have a coordinate-independent description of differential operators between two vector bundles. Let E and F be two vector bundles over a differentiable manifold M. An R-linear mapping of sections P\u00a0: \u0393(E) \u2192 \u0393(F) is said to be a kth-order linear differential operator if it factors through the jet bundle Jk(E). In other words, there exists a linear mapping of vector bundlesThe same constructions can be carried out with partial derivatives, differentiation with respect to different variables giving rise to operators that commute (see symmetry of second derivatives).The differential operators also obey the shift theorem.The subring of operators that are polynomials in D with constant coefficients is, by contrast, commutative. It can be characterised another way: it consists of the translation-invariant operators.Some care is then required: firstly any function coefficients in the operator D2 must be differentiable as many times as the application of D1 requires. To get a ring of such operators we must assume derivatives of all orders of the coefficients used. Secondly, this ring will not be commutative: an operator gD isn't the same in general as Dg. In fact we have for example the relation basic in quantum mechanics:Any polynomial in D with function coefficients is also a differential operator. We may also compose differential operators by the rulewhere f and g are functions, and a is a constant.Differentiation is linear, i.e.,This operator is central to Sturm\u2013Liouville theory where the eigenfunctions (analogues to eigenvectors) of this operator are considered.This property can be proven using the formal adjoint definition above.The Sturm\u2013Liouville operator is a well-known example of a formal self-adjoint operator. This second-order linear differential operator L can be written in the formfor all smooth L2 functions f, g. Since smooth functions are dense in L2, this defines the adjoint on a dense subset of L2: P* is a densely defined operator.If \u03a9 is a domain in Rn, and P a differential operator on \u03a9, then the adjoint of P is defined in L2(\u03a9) by duality in the analogous manner:A (formally) self-adjoint operator is an operator equal to its own (formal) adjoint.In the functional space of square-integrable functions on a real interval (a, b), the scalar product is defined byGiven a linear differential operator TDel is used to calculate the gradient, curl, divergence, and Laplacian of various objects.The differential operator del, also called nabla operator, is an important vector differential operator. It appears frequently in physics in places like the differential form of Maxwell's equations. In three-dimensional Cartesian coordinates, del is defined:Such a bidirectional-arrow notation is frequently used for describing the probability current of quantum mechanics.In writing, following common mathematical convention, the argument of a differential operator is usually placed on the right side of the operator itself. Sometimes an alternative notation is used: The result of applying the operator to the function on the left side of the operator and on the right side of the operator, and the difference obtained when applying the differential operator to the functions on both sides, are denoted by arrows as follows:As in one variable, the eigenspaces of \u0398 are the spaces of homogeneous polynomials.In n variables the homogeneity operator is given byThis is sometimes also called the homogeneity operator, because its eigenfunctions are the monomials in z:Another differential operator is the \u0398 operator, or theta operator, defined by[1]One of the most frequently seen differential operators is the Laplacian operator, defined byin his study of differential equations.The D notation's use and creation is credited to Oliver Heaviside, who considered differential operators of the formThe derivative of a function f of an argument x is sometimes given as either of the following:When taking higher, nth order derivatives, the operator may also be written:The most common differential operator is the action of taking the derivative itself. Common notations for taking the first derivative with respect to a variable x include:This article considers mainly linear operators, which are the most common type. However, non-linear differential operators, such as the Schwarzian derivative also exist.In mathematics, a differential operator is an operator defined as a function of the differentiation operator. It is helpful, as a matter of notation first, to consider differentiation as an abstract operation that accepts a function and returns another function (in the style of a higher-order function in computer science).",
            "title": "Differential operator",
            "url": "https://en.wikipedia.org/wiki/Differential_operator"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Set_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Domain_of_a_function",
                "/wiki/Codomain",
                "/wiki/Mathematical_structure",
                "/wiki/Vector_space",
                "/wiki/List_of_mathematical_jargon#natural",
                "/wiki/Pointwise",
                "/wiki/Topological_space",
                "/wiki/Metric_space"
            ],
            "links": [
                "/wiki/Functional_analysis",
                "/wiki/Topological_vector_space",
                "/wiki/Normed_space",
                "/wiki/Subset",
                "/wiki/Linear_subspace",
                "/wiki/Linear_map",
                "/wiki/Hom_set",
                "/wiki/Dual_space",
                "/wiki/Linear_form",
                "/wiki/Field_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Set_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Domain_of_a_function",
                "/wiki/Codomain",
                "/wiki/Mathematical_structure",
                "/wiki/Vector_space",
                "/wiki/List_of_mathematical_jargon#natural",
                "/wiki/Pointwise",
                "/wiki/Topological_space",
                "/wiki/Metric_space"
            ],
            "text": "Functional analysis is organized around adequate techniques to bring function spaces as topological vector spaces within reach of the ideas that would apply to normed spaces of finite dimension.Function spaces appear in various areas of mathematics:When the domain X has additional structure, one might consider instead the subset (or subspace) of all such functions which respect that structure. For example, if X is also vector space over F, the set of linear maps X \u2192 V form a vector space over F with pointwise operations (often denoted Hom(X,V)). One such space is the dual space of V: the set of linear functionals V \u2192 F with addition and scalar multiplication defined pointwise.Let V be a vector space over a field F and let X be any set. The functions X \u2192 V can be given the structure of a vector space over F where the operations are defined pointwise, that is, for any f, g\u00a0: X \u2192 V, any x in X, and any c in F, defineIn mathematics, a function space is a set of functions between two fixed sets. Often, the domain and/or codomain will have additional structure which is inherited by the function space. For example, the set of functions from any set X into a vector space have a natural vector space structure given by pointwise addition and scalar multiplication. In other scenarios, the function space might inherit a topological or metric structure, hence the name function space.",
            "title": "Function space",
            "url": "https://en.wikipedia.org/wiki/Function_space"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Calculus",
                "/wiki/Mathematics",
                "/wiki/Limit_(mathematics)",
                "/wiki/Function_(mathematics)"
            ],
            "text": "Calculus, known in its early history as infinitesimal calculus, is a mathematical discipline focused on limits, functions, derivatives, integrals, and infinite series. Isaac Newton and Gottfried Leibniz independently discovered calculus in the mid-17th century. However, each inventor claimed the other stole his work in a bitter dispute that continued until the end of their lives.",
            "title": "Derivative",
            "url": "https://en.wikipedia.org/wiki/Derivative"
        },
        {
            "desc_links": [
                "/wiki/Dynamical_systems",
                "/wiki/Numerical_methods",
                "/wiki/Pure_mathematics",
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Function_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Engineering",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Biology"
            ],
            "links": [
                "/wiki/Rate_equation",
                "/wiki/Chemical_reaction",
                "/wiki/Reaction_rate",
                "/wiki/Reaction_order",
                "/wiki/Mass_balance",
                "/wiki/Thermodynamics",
                "/wiki/Quantum_mechanics",
                "/wiki/Lotka%E2%80%93Volterra_equations",
                "/wiki/Non-linear",
                "/wiki/Population_dynamics",
                "/wiki/Schr%C3%B6dinger%27s_equation",
                "/wiki/Linear_differential_equation",
                "/wiki/Partial_differential_equation",
                "/wiki/Wave_function",
                "/wiki/Einstein_field_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Albert_Einstein",
                "/wiki/General_relativity",
                "/wiki/Fundamental_interaction",
                "/wiki/Gravitation",
                "/wiki/Spacetime",
                "/wiki/Curvature",
                "/wiki/Matter",
                "/wiki/Energy",
                "/wiki/Tensor_equation",
                "/wiki/Curvature",
                "/wiki/Einstein_tensor",
                "/wiki/Momentum",
                "/wiki/Stress%E2%80%93energy_tensor",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Lorentz_force",
                "/wiki/Classical_electrodynamics",
                "/wiki/Optics",
                "/wiki/Electric_circuit",
                "/wiki/Electric_field",
                "/wiki/Magnetic_field",
                "/wiki/Electric_charge",
                "/wiki/Electric_current",
                "/wiki/James_Clerk_Maxwell",
                "/wiki/Newton%27s_second_law",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Biology",
                "/wiki/Economics",
                "/wiki/Mathematical_modelling",
                "/wiki/Partial_differential_equation",
                "/wiki/Wave_equation",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_equation",
                "/wiki/Diffusion",
                "/wiki/Black%E2%80%93Scholes",
                "/wiki/Pure_mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Physics",
                "/wiki/Engineering",
                "/wiki/Closed-form_expression",
                "/wiki/Numerical_ordinary_differential_equations",
                "/wiki/Difference_equations",
                "/wiki/Algebraic_equations",
                "/wiki/Derivative#Higher_derivatives",
                "/wiki/Second_derivative",
                "/wiki/Thin-film_equation",
                "/wiki/Linearization",
                "/wiki/Symmetry",
                "/wiki/Chaos_theory",
                "/wiki/Navier%E2%80%93Stokes_existence_and_smoothness",
                "/wiki/Sound",
                "/wiki/Heat",
                "/wiki/Electrostatics",
                "/wiki/Electrodynamics",
                "/wiki/Fluid_flow",
                "/wiki/Elasticity_(physics)",
                "/wiki/Quantum_mechanics",
                "/wiki/Dynamical_systems",
                "/wiki/Multidimensional_systems",
                "/wiki/Stochastic_partial_differential_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Multivariable_calculus",
                "/wiki/Partial_derivatives",
                "/wiki/Ordinary_differential_equations",
                "/wiki/Computer_model",
                "/wiki/Closed-form_expression",
                "/wiki/Numerical_ordinary_differential_equations",
                "/wiki/Physics",
                "/wiki/Special_functions",
                "/wiki/Holonomic_function",
                "/wiki/Linear_differential_equation",
                "/wiki/Linear_equation",
                "/wiki/Antiderivative",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Function_of_a_real_variable",
                "/wiki/Variable_(mathematics)",
                "/wiki/Independent_variable",
                "/wiki/Partial_differential_equation",
                "/wiki/Equations_of_motion",
                "/wiki/Classical_mechanics",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_flow",
                "/wiki/Newton%27s_law_of_cooling",
                "/wiki/Heat_equation",
                "/wiki/Mechanics",
                "/wiki/Lagrangian_mechanics",
                "/wiki/Euler%E2%80%93Lagrange_equation",
                "/wiki/Tautochrone",
                "/wiki/Musical_instrument",
                "/wiki/Jean_le_Rond_d%27Alembert",
                "/wiki/Leonhard_Euler",
                "/wiki/Daniel_Bernoulli",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Jacob_Bernoulli",
                "/wiki/Bernoulli_differential_equation",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Calculus",
                "/wiki/Isaac_Newton",
                "/wiki/Leibniz",
                "/wiki/Method_of_Fluxions",
                "/wiki/Dynamical_systems",
                "/wiki/Numerical_methods",
                "/wiki/Pure_mathematics",
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Function_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Engineering",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Biology"
            ],
            "text": "The rate law or rate equation for a chemical reaction is a differential equation that links the reaction rate with concentrations or pressures of reactants and constant parameters (normally rate coefficients and partial reaction orders).[18] To determine the rate equation for a particular system one combines the reaction rate with a mass balance for the system.[19] In addition, a range of differential equations are present in the study of thermodynamics and quantum mechanics.The Lotka\u2013Volterra equations, also known as the predator\u2013prey equations, are a pair of first-order, non-linear, differential equations frequently used to describe the population dynamics of two species that interact, one as a predator and the other as prey.In quantum mechanics, the analogue of Newton's law is Schr\u00f6dinger's equation (a partial differential equation) for a quantum system (usually atoms, molecules, and subatomic particles whether free, bound, or localized). It is not a simple algebraic equation, but in general a linear partial differential equation, describing the time-evolution of the system's wave function (also called a \"state function\").[17]The Einstein field equations (EFE; also known as \"Einstein's equations\") are a set of ten partial differential equations in Albert Einstein's general theory of relativity which describe the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy.[14] First published by Einstein in 1915[15] as a tensor equation, the EFE equate local spacetime curvature (expressed by the Einstein tensor) with the local energy and momentum within that spacetime (expressed by the stress\u2013energy tensor).[16]Maxwell's equations are a set of partial differential equations that, together with the Lorentz force law, form the foundation of classical electrodynamics, classical optics, and electric circuits. These fields in turn underlie modern electrical and communications technologies. Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. They are named after the Scottish physicist and mathematician James Clerk Maxwell, who published an early form of those equations between 1861 and 1862.So long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the equation of motion.Many fundamental laws of physics and chemistry can be formulated as differential equations. In biology and economics, differential equations are used to model the behavior of complex systems. The mathematical theory of differential equations first developed together with the sciences where the equations had originated and where the results found application. However, diverse problems, sometimes originating in quite distinct scientific fields, may give rise to identical differential equations. Whenever this happens, mathematical theory behind the equations can be viewed as a unifying principle behind diverse phenomena. As an example, consider the propagation of light and sound in the atmosphere, and of waves on the surface of a pond. All of them may be described by the same second-order partial differential equation, the wave equation, which allows us to think of light and sound as forms of waves, much like familiar waves in the water. Conduction of heat, the theory of which was developed by Joseph Fourier, is governed by another second-order partial differential equation, the heat equation. It turns out that many diffusion processes, while seemingly different, are described by the same equation; the Black\u2013Scholes equation in finance is, for instance, related to the heat equation.The study of differential equations is a wide field in pure and applied mathematics, physics, and engineering. All of these disciplines are concerned with the properties of differential equations of various types. Pure mathematics focuses on the existence and uniqueness of solutions, while applied mathematics emphasizes the rigorous justification of the methods for approximating solutions. Differential equations play an important role in modelling virtually every physical, technical, or biological process, from celestial motion, to bridge design, to interactions between neurons. Differential equations such as those used to solve real-life problems may not necessarily be directly solvable, i.e. do not have closed form solutions. Instead, solutions can be approximated using numerical methods.The theory of differential equations is closely related to the theory of difference equations, in which the coordinates assume only discrete values, and the relationship involves values of the unknown function or functions and values at nearby coordinates. Many methods to compute numerical solutions of differential equations or study the properties of differential equations involve the approximation of the solution of a differential equation by the solution of a corresponding difference equation.such thatHowever, this only helps us with first order initial value problems. Suppose we had a linear initial value problem of the nth order:Solving differential equations is not like solving algebraic equations. Not only are their solutions often unclear, but whether solutions are unique or exist at all are also notable subjects of interest.In the next group of examples, the unknown function u depends on two variables x and t or x and y.In the first group of examples, let u be an unknown function of x, and let c & \u03c9 be known constants. Note both ordinary and partial differential equations are broadly classified as linear and nonlinear.Differential equations are described by their order, determined by the term with the highest derivatives. An equation containing only first derivatives is a first-order differential equation, an equation containing the second derivative is a second-order differential equation, and so on.[11][12] Differential equations that describe natural phenomena almost always have only first and second order derivatives in them, but there are some exceptions, such as the thin film equation, which is a fourth order partial differential equation.Linear differential equations frequently appear as approximations to nonlinear equations. These approximations are only valid under restricted conditions. For example, the harmonic oscillator equation is an approximation to the nonlinear pendulum equation that is valid for small amplitude oscillations (see below).Non-linear differential equations are formed by the products of the unknown function and its derivatives are allowed and its degree is > 1. There are very few methods of solving nonlinear differential equations exactly; those that are known typically depend on the equation having particular symmetries. Nonlinear differential equations can exhibit very complicated behavior over extended time intervals, characteristic of chaos. Even the fundamental questions of existence, uniqueness, and extendability of solutions for nonlinear differential equations, and well-posedness of initial and boundary value problems for nonlinear PDEs are hard problems and their resolution in special cases is considered to be a significant advance in the mathematical theory (cf. Navier\u2013Stokes existence and smoothness). However, if the differential equation is a correctly formulated representation of a meaningful physical process, then one expects it to have a solution.[10]PDEs can be used to describe a wide variety of phenomena in nature such as sound, heat, electrostatics, electrodynamics, fluid flow, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.A partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. (This is in contrast to ordinary differential equations, which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved in closed form, or used to create a relevant computer model.As, in general, the solutions of a differential equation cannot be expressed by a closed-form expression, numerical methods are commonly used for solving differential equations on a computer.Most ODEs that are encountered in physics are linear, and, therefore, most special functions may be defined as solutions of linear differential equations (see Holonomic function).Linear differential equations are the differential equations that are linear in the unknown function and its derivatives. Their theory is well developed, and, in many cases, one may express their solutions in terms of integrals.An ordinary differential equation (ODE) is an equation containing an unknown function of one real or complex variable x, its derivatives, and some given functions of x. The unknown function is generally represented by a variable (often denoted y), which, therefore, depends on x. Thus x is often called the independent variable of the equation. The term \"ordinary\" is used in contrast with the term partial differential equation, which may be with respect to more than one independent variable.Differential equations can be divided into several types. Apart from describing the properties of the equation itself, these classes of differential equations can help inform the choice of approach to a solution. Commonly used distinctions include whether the equation is: Ordinary/Partial, Linear/Non-linear, and Homogeneous/Inhomogeneous. This list is far from exhaustive; there are many other properties and subclasses of differential equations which can be very useful in specific contexts.An example of modelling a real world problem using differential equations is the determination of the velocity of a ball falling through the air, considering only gravity and air resistance. The ball's acceleration towards the ground is the acceleration due to gravity minus the acceleration due to air resistance. Gravity is considered constant, and air resistance may be modeled as proportional to the ball's velocity. This means that the ball's acceleration, which is a derivative of its velocity, depends on the velocity (and the velocity depends on time). Finding the velocity as a function of time involves solving a differential equation and verifying its validity.In some cases, this differential equation (called an equation of motion) may be solved explicitly.For example, in classical mechanics, the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow (given the position, velocity, acceleration and various forces acting on the body) one to express these variables dynamically as a differential equation for the unknown position of the body as a function of time.Fourier published his work on heat flow in Th\u00e9orie analytique de la chaleur (The Analytic Theory of Heat),[9] in which he based his reasoning on Newton's law of cooling, namely, that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures. Contained in this book was Fourier's proposal of his heat equation for conductive diffusion of heat. This partial differential equation is now taught to every student of mathematical physics.Lagrange solved this problem in 1755 and sent the solution to Euler. Both further developed Lagrange's method and applied it to mechanics, which led to the formulation of Lagrangian mechanics.The Euler\u2013Lagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the tautochrone problem. This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point.Historically, the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, and Joseph-Louis Lagrange.[4][5][6][7] In 1746, d\u2019Alembert discovered the one-dimensional wave equation, and within ten years Euler discovered the three-dimensional wave equation.[8]for which the following year Leibniz obtained solutions by simplifying it.[3]Jacob Bernoulli proposed the Bernoulli differential equation in 1695.[2] This is an ordinary differential equation of the formHe solves these examples and others using infinite series and discusses the non-uniqueness of solutions.Differential equations first came into existence with the invention of calculus by Newton and Leibniz. In Chapter 2 of his 1671 work \"Methodus fluxionum et Serierum Infinitarum\",[1] Isaac Newton listed three kinds of differential equations:If a self-contained formula for the solution is not available, the solution may be numerically approximated using computers. The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy.In pure mathematics, differential equations are studied from several different perspectives, mostly concerned with their solutions\u2014the set of functions that satisfy the equation. Only the simplest differential equations are solvable by explicit formulas; however, some properties of solutions of a given differential equation may be determined without finding their exact form.A differential equation is a mathematical equation that relates some function with its derivatives. In applications, the functions usually represent physical quantities, the derivatives represent their rates of change, and the equation defines a relationship between the two. Because such relations are extremely common, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology.",
            "title": "Differential equation",
            "url": "https://en.wikipedia.org/wiki/Differential_equation"
        },
        {
            "desc_links": [
                "/wiki/Numerical_analysis",
                "/wiki/LU_decomposition",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Main_diagonal",
                "/wiki/Main_diagonal",
                "/wiki/Diagonal_matrix"
            ],
            "links": [
                "/wiki/Bootstrapping_(finance)",
                "/wiki/Yield_curve",
                "/wiki/Isomorphic",
                "/wiki/Abelian_group",
                "/wiki/M%C3%B6bius_transformation",
                "/wiki/Heisenberg_group",
                "/wiki/Parabolic_subgroup",
                "/wiki/Flag_(linear_algebra)",
                "/wiki/Borel_subgroup",
                "/wiki/Group_(mathematics)",
                "/wiki/Lie_group",
                "/wiki/General_linear_group",
                "/wiki/Trapezoid",
                "/wiki/Associative_algebra",
                "/wiki/Functional_analysis",
                "/wiki/Nest_algebra",
                "/wiki/Hilbert_space",
                "/wiki/Lie%27s_theorem",
                "/wiki/Solvable_Lie_algebra",
                "/wiki/Abelian_Lie_algebra",
                "/wiki/Schur_decomposition",
                "/wiki/Unitary_matrix",
                "/wiki/Jordan_normal_form",
                "/wiki/Field_(mathematics)",
                "/wiki/Algebraically_closed_field",
                "/wiki/Transpose",
                "/wiki/Normal_matrix",
                "/wiki/Off-diagonal_element",
                "/wiki/Engel%27s_theorem",
                "/wiki/Lie_group",
                "/wiki/Main_diagonal",
                "/wiki/Unipotent",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_norm",
                "/wiki/Identity_matrix",
                "/wiki/Subalgebra",
                "/wiki/Associative_algebra",
                "/wiki/Lie_algebra",
                "/wiki/Lie_bracket",
                "/wiki/Commutator#Ring_theory",
                "/wiki/Solvable_Lie_algebra",
                "/wiki/Borel_subalgebra",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Numerical_analysis",
                "/wiki/LU_decomposition",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Main_diagonal",
                "/wiki/Main_diagonal",
                "/wiki/Diagonal_matrix"
            ],
            "text": "Forward substitution is used in financial bootstrapping to construct a yield curve.A matrix equation with an upper triangular matrix U can be solved in an analogous way, only working backwards.The resulting formulas are:The matrix equation Lx = b can be written as a system of linear equationsNotice that this does not require inverting the matrix.The group of 2 by 2 upper unitriangular matrices is isomorphic to the additive group of the field of scalars; in the case of complex numbers it corresponds to a group formed of parabolic M\u00f6bius transformations; the 3 by 3 upper unitriangular matrices form the Heisenberg group.The stabilizer of a partial flag obtained by forgetting some parts of the standard flag can be described as a set of block upper triangular matrices (but its elements are not all triangular matrices). The conjugates of such a group are the subgroups defined as the stabilizer of some partial flag. These subgroups are called parabolic subgroups.The upper triangular matrices are precisely those that stabilize the standard flag. The invertible ones among them form a subgroup of the general linear group, whose conjugate subgroups are those defined as the stabilizer of some (other) complete flag. These subgroups are Borel subgroups. The group of invertible lower triangular matrices is such a subgroup, since it is the stabilizer of the standard flag associated to the standard basis in reverse order.The set of invertible triangular matrices of a given kind (upper or lower) forms a group, indeed a Lie group, which is a subgroup of the general linear group of all invertible matrices. Note that a triangular matrix is invertible precisely when its diagonal entries are invertible (non-zero).A non-square (or sometimes any) matrix with zeros above (below) the diagonal is called a lower (upper) trapezoidal matrix. The non-zero entries form the shape of a trapezoid.Because the product of two upper triangular matrices is again upper triangular, the set of upper triangular matrices forms an algebra. Algebras of upper triangular matrices have a natural generalization in functional analysis which yields nest algebras on Hilbert spaces.This is generalized by Lie's theorem, which shows that any representation of a solvable Lie algebra is simultaneously upper triangularizable, the case of commuting matrices being the abelian Lie algebra case, abelian being a fortiori solvable.In the case of complex matrices, it is possible to say more about triangularization, namely, that any square matrix A has a Schur decomposition. This means that A is unitarily equivalent (i.e. similar, using a unitary matrix as change of basis) to an upper triangular matrix; this follows by taking an Hermitian basis for the flag.A more precise statement is given by the Jordan normal form theorem, which states that in this situation, A is similar to an upper triangular matrix of a very particular form. The simpler triangularization result is often sufficient however, and in any case used in proving the Jordan normal form theorem.[1][2]Any complex square matrix is triangularizable.[1] In fact, a matrix A over a field containing all of the eigenvalues of A (for example, any matrix over an algebraically closed field) is similar to a triangular matrix. This can be proven by using induction on the fact that A has an eigenvector, by taking the quotient space by the eigenvector and inducting to show that A stabilises a flag, and is thus triangularizable with respect to a basis for that flag.The transpose of an upper triangular matrix is a lower triangular matrix and vice versa.A matrix which is simultaneously triangular and normal is also diagonal. This can be seen by looking at the diagonal entries of A*A and AA*, where A is a normal, triangular matrix.is atomic lower triangular. Its inverse isThe matrixi.e., the off-diagonal entries are replaced in the inverse matrix by their additive inverses.The inverse of an atomic triangular matrix is again atomic triangular. Indeed, we haveAn atomic (upper or lower) triangular matrix is a special form of unitriangular matrix, where all of the off-diagonal elements are zero, except for the entries in a single column. Such a matrix is also called a Frobenius matrix, a Gauss matrix, or a Gauss transformation matrix. So an atomic lower triangular matrix is of the formIn fact, by Engel's theorem, any finite-dimensional nilpotent Lie algebra is conjugate to a subalgebra of the strictly upper triangular matrices, that is to say, a finite-dimensional nilpotent Lie algebra is simultaneously strictly upper triangularizable.The set of unitriangular matrices forms a Lie group.If the entries on the main diagonal of a (upper or lower) triangular matrix are all 1, the matrix is called (upper or lower) unitriangular. All unitriangular matrices are unipotent. Other names used for these matrices are unit (upper or lower) triangular (of which \"unitriangular\" might be a contraction), or very rarely normed (upper or lower) triangular. However a unit triangular matrix is not the same as the unit matrix, and a normed triangular matrix has nothing to do with the notion of matrix norm. The identity matrix is the only matrix which is both upper and lower unitriangular.is lower triangular.is upper triangular and this matrixThis matrixAll these results hold if \"upper triangular\" is replaced by \"lower triangular\" throughout; in particular the lower triangular matrices also form a Lie algebra. However, operations mixing upper and lower triangular matrices do not in general produce triangular matrices. For instance, the sum of an upper and a lower triangular matrix can be any matrix; the product of a lower triangular with an upper triangular matrix is not necessarily triangular either.Together these facts mean that the upper triangular matrices form a subalgebra of the associative algebra of square matrices for a given size. Additionally, this also shows that the upper triangular matrices can be viewed as a Lie subalgebra of the Lie algebra of square matrices of a fixed size, where the Lie bracket [a,b] given by the commutator ab-ba. The Lie algebra of all upper triangular matrices is a solvable Lie algebra. It is often referred to as a Borel subalgebra of the Lie algebra of all square matrices.Many operations on upper triangular matrices preserve the shape:Matrices that are similar to triangular matrices are called triangularisable.is called an upper triangular matrix or right triangular matrix. The variable L (standing for lower or left) is commonly used to represent a lower triangular matrix, while the variable U (standing for upper) or R (standing for right) is commonly used for upper triangular matrix. A matrix that is both upper and lower triangular is diagonal.is called a lower triangular matrix or left triangular matrix, and analogously a matrix of the formA matrix of the formBecause matrix equations with triangular matrices are easier to solve, they are very important in numerical analysis. By the LU decomposition algorithm, an invertible matrix may be written as the product of a lower triangular matrix L and an upper triangular matrix U if and only if all its leading principal minors are non-zero.In the mathematical discipline of linear algebra, a triangular matrix is a special kind of square matrix. A square matrix is called lower triangular if all the entries above the main diagonal are zero. Similarly, a square matrix is called upper triangular if all the entries below the main diagonal are zero. A triangular matrix is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is called a diagonal matrix.",
            "title": "Triangular matrix",
            "url": "https://en.wikipedia.org/wiki/Triangular_matrix"
        },
        {
            "desc_links": [
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Main_diagonal",
                "/wiki/Square_matrices",
                "/wiki/Identity_matrix"
            ],
            "links": [
                "/wiki/Multiplication_operator",
                "/wiki/Operator_theory",
                "/wiki/PDEs",
                "/wiki/Separable_partial_differential_equation",
                "/wiki/Integral_transform",
                "/wiki/Eigenbasis",
                "/wiki/Eigenfunction",
                "/wiki/Fourier_transform",
                "/wiki/Heat_equation",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Spectral_theorem",
                "/wiki/Normal_matrix",
                "/wiki/Matrix_similarity",
                "/wiki/Unitary_matrix",
                "/wiki/Singular_value_decomposition",
                "/wiki/Similar_matrix",
                "/wiki/Linearly_independent",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Linear_operator",
                "/wiki/Triangular_matrix",
                "/wiki/Triangular_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Zero_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Adjugate",
                "/wiki/Determinant",
                "/wiki/Eigenvalue",
                "/wiki/Eigenvectors",
                "/wiki/Subring",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_multiplication",
                "/wiki/Center_of_an_algebra",
                "/wiki/Commute_(mathematics)",
                "/wiki/Identity_matrix",
                "/wiki/Vector_(mathematics_and_physics)",
                "/wiki/Scalar_multiplication",
                "/wiki/Real_numbers",
                "/wiki/Complex_numbers",
                "/wiki/Normal_matrix"
            ],
            "text": "Especially easy are multiplication operators, which are defined as multiplication by (the values of) a fixed function\u2013the values of the function at each point correspond to the diagonal entries of a matrix.In operator theory, particularly the study of PDEs, operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation. Therefore, a key technique to understanding operators is a change of coordinates\u2013in the language of operators, an integral transform\u2013which changes the basis to an eigenbasis of eigenfunctions: which makes the equation separable. An important example of this is the Fourier transform, which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation.Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if AA\u2217 = A\u2217A then there exists a unitary matrix U such that UAU\u2217 is diagonal). Furthermore, the singular value decomposition implies that for any matrix A, there exist unitary matrices U and V such that UAV\u2217 is diagonal with positive entries.In fact, a given n-by-n matrix A is similar to a diagonal matrix (meaning that there is a matrix X such that X\u22121AX is diagonal) if and only if it has n linearly independent eigenvectors. Such matrices are said to be diagonalizable.Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or linear map by a diagonal matrix.A symmetric diagonal matrix can be defined as a matrix that is both upper- and lower-triangular. The identity matrix In and any square zero matrix are diagonal. A one-dimensional matrix is always diagonal.Any square diagonal matrix is also a symmetric matrix.A square matrix is diagonal if and only if it is triangular and normal.The adjugate of a diagonal matrix is again diagonal.The determinant of diag(a1, ..., an) is the product a1...an.In other words, the eigenvalues of diag(\u03bb1, ..., \u03bbn) are \u03bb1, ..., \u03bbn with associated eigenvectors of e1, ..., en.Multiplying an n-by-n matrix A from the left with diag(a1, ..., an) amounts to multiplying the ith row of A by ai for all i; multiplying the matrix A from the right with diag(a1, ..., an) amounts to multiplying the ith column of A by ai for all i.In particular, the diagonal matrices form a subring of the ring of all n-by-n matrices.The diagonal matrix diag(a1, ..., an) is invertible if and only if the entries a1, ..., an are all non-zero. In this case, we haveand for matrix multiplication,The operations of matrix addition and matrix multiplication are especially simple for symmetric diagonal matrices. Write diag(a1, ..., an) for a diagonal matrix whose diagonal entries starting in the upper left corner are a1, ..., an. Then, for addition, we haveThe scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size.A square diagonal matrix with all its main diagonal entries equal is a scalar matrix, that is, a scalar multiple \u03bbI of the identity matrix I. Its effect on a vector is scalar multiplication by \u03bb. For example, a 3\u00d73 scalar matrix has the form:In the remainder of this article we will consider only square matrices.If the entries are real numbers or complex numbers, then it is a normal matrix as well.The following matrix is a symmetric diagonal matrix:The term diagonal matrix may sometimes refer to a rectangular diagonal matrix, which is an m-by-n matrix with all the entries not of the form di,i being zero. For example:However, the main diagonal entries need not be zero.As stated above, the off-diagonal entries are zero. That is, the matrix D = (di,j) with n columns and n rows is diagonal if",
            "title": "Diagonal matrix",
            "url": "https://en.wikipedia.org/wiki/Diagonal_matrices"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Binary_matrix",
                "/wiki/Permutation"
            ],
            "links": [
                "/wiki/Dot_product",
                "/wiki/Permutation",
                "/wiki/Circulant_matrix#Properties",
                "/wiki/Group_theory",
                "/wiki/Transposition_(mathematics)",
                "/wiki/Elementary_matrix",
                "/wiki/Signature_of_a_permutation",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Eigenvector",
                "/wiki/Doubly_stochastic_matrix",
                "/wiki/Birkhoff%E2%80%93von_Neumann_theorem",
                "/wiki/Convex_combination",
                "/wiki/Extreme_point",
                "/wiki/Birkhoff_polytope",
                "/wiki/Convex_hull",
                "/wiki/Faithful_representation",
                "/wiki/Symmetric_group",
                "/wiki/Permutation_group",
                "/wiki/Group_(mathematics)",
                "/wiki/Identity_element",
                "/wiki/Identity_matrix",
                "/wiki/Prefix_notation",
                "/wiki/Identity_matrix",
                "/wiki/Mathematics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Binary_matrix",
                "/wiki/Permutation"
            ],
            "text": "So, permutation matrices do indeed permute the order of elements in vectors multiplied with them.So, the product of the permutation matrix with the vector v above, will be a vector in the form (ga1, ga2, ..., gaj), and that this then is a permutation of v since we have said that the permutation form isNow, in performing matrix multiplication, one essentially forms the dot product of each row of the first matrix with each column of the second. In this instance, we will be forming the dot product of each row of this matrix with the vector of elements we want to permute. That is, for example, v= (g0,...,g5)T,is the permutation form of the permutation matrix.where eai represents the ith basis vector (as a row) for Rj, and whereA permutation matrix will always be in the formGiven a vector g,Permutations of rows and columns are for example reflections (see below) and cyclic permutations (see cyclic permutation matrix).When a permutation matrix P is multiplied from the left with a matrix M to make PM it will permute the rows of M (here the elements of a column vector),\nwhen P is multiplied from the right with M to make MP it will permute the columns of M (here the elements of a row vector):From group theory we know that any permutation may be written as a product of transpositions. Therefore, any permutation matrix P factors as a product of row-interchanging elementary matrices, each having determinant \u22121. Thus the determinant of a permutation matrix P is just the signature of the corresponding permutation.The trace of a permutation matrix is the number of fixed points of the permutation. If the permutation has fixed points, so it can be written in cycle form as \u03c0 = (a1)(a2)...(ak)\u03c3 where \u03c3 has no fixed points, then ea1,ea2,...,eak are eigenvectors of the permutation matrix.A permutation matrix is itself a doubly stochastic matrix, but it also plays a special role in the theory of these matrices. The Birkhoff\u2013von Neumann theorem says that every doubly stochastic real matrix is a convex combination of permutation matrices of the same order and the permutation matrices are precisely the extreme points of the set of doubly stochastic matrices. That is, the Birkhoff polytope, the set of doubly stochastic matrices, is the convex hull of the set of permutation matrices.[3]The map Sn \u2192 A \u2282 GL(n, Z2) is a faithful representation. Thus, |A| = n!.Let Sn denote the symmetric group, or group of permutations, on {1,2,...,n}. Since there are n! permutations, there are n! permutation matrices. By the formulas above, the n \u00d7 n permutation matrices form a group under matrix multiplication with the identity matrix as the identity element.If (1) denotes the identity permutation, then P(1) is the identity matrix.Similarly,From this it follows thatTo be clear, the above formulas use the prefix notation for permutation composition, that is,The same matrices acting on row vectors (that is, post-multiplication) compose according to the same ruleGiven two permutations \u03c0 and \u03c3 of m elements, the corresponding permutation matrices P\u03c0 and P\u03c3 acting on column vectors are composed withAgain, repeated application of this result shows that post-multiplying a matrix M by the permutation matrix P\u03c0, that is, M P\u03c0, results in permuting the columns of M. Notice also thatThe column representation of a permutation matrix is used throughout this section, except when otherwise indicated.The other representation, obtained by permuting the rows of the identity matrix Im, that is, for each j, pij = 1 if i = \u03c0(j) and 0 otherwise, will be referred to as the row representation.Observe that the jth column of the I5 identity matrix now appears as the \u03c0(j)th column of P\u03c0.The m \u00d7 m permutation matrix P\u03c0 = (pij) obtained by permuting the columns of the identity matrix Im, that is, for each i, pij = 1 if j = \u03c0(i) and 0 otherwise, will be referred to as the column representation in this article.[1] Since the entries in row i are all 0 except that a 1 appears in column \u03c0(i), we may writethere are two natural ways to associate the permutation with a permutation matrix; namely, starting with the m \u00d7 m identity matrix, Im, either permute the columns or permute the rows, according to \u03c0. Both methods of defining permutation matrices appear in the literature and the properties expressed in one representation can be easily converted to the other representation. This article will primarily deal with just one of these representations and the other will only be mentioned when there is a difference to be aware of.represented in two-line form byGiven a permutation \u03c0 of m elements,In mathematics, particularly in matrix theory, a permutation matrix is a square binary matrix that has exactly one entry of 1 in each row and each column and 0s elsewhere. Each such matrix, say P, represents a permutation of m elements and, when used to multiply another matrix, say A, results in permuting the rows (when pre-multiplying, i.e., PA) or columns (when post-multiplying, AP) of the matrix A.",
            "title": "Permutation matrix",
            "url": "https://en.wikipedia.org/wiki/Permutation_matrix"
        },
        {
            "desc_links": [
                "/wiki/Hermitian_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Characteristic_polynomial",
                "/wiki/Geometric_multiplicity",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Basis_function",
                "/wiki/Eigenvector",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Linearly_independent",
                "/wiki/Generalized_eigenvector",
                "/wiki/Ordinary_differential_equation"
            ],
            "links": [
                "/wiki/Eigenvalue",
                "/wiki/Jordan_normal_form",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Jordan_matrix",
                "/wiki/Hermitian_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Characteristic_polynomial",
                "/wiki/Geometric_multiplicity",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Basis_function",
                "/wiki/Eigenvector",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Linearly_independent",
                "/wiki/Generalized_eigenvector",
                "/wiki/Ordinary_differential_equation"
            ],
            "text": "(and constant multiples thereof).\nwhich has a double eigenvalue of 3 but only one distinct eigenvector\nA simple example of a defective matrix is:\nIn fact, any defective matrix has a nontrivial Jordan normal form, which is as close as one can come to diagonalization of such a matrix.\nhas an eigenvalue, \u03bb, with algebraic multiplicity n, but only one distinct eigenvector,\nAny nontrivial Jordan block of size 2\u00d72 or larger (that is, not completely diagonal) is defective.  (A diagonal matrix is a special case of the Jordan normal form and is not defective.)  For example, the n \u00d7 n Jordan block,\nA Hermitian matrix (or the special case of a real symmetric matrix) or a unitary matrix is never defective; more generally, a normal matrix (which includes Hermitian and unitary as special cases) is never defective.\nA defective matrix always has fewer than n distinct eigenvalues, since distinct eigenvalues always have linearly independent eigenvectors.  In particular, a defective matrix has one or more eigenvalues \u03bb with algebraic multiplicity m > 1 (that is, they are multiple roots of the characteristic polynomial), but fewer than m linearly independent eigenvectors associated with \u03bb. If the algebraic multiplicity of \u03bb exceeds its geometric multiplicity (that is, the number of linearly independent eigenvectors associated with \u03bb), then \u03bb is said to be a defective eigenvalue.[1]  However, every eigenvalue with algebraic multiplicity m always has m linearly independent generalized eigenvectors.\nIn linear algebra, a defective matrix is a square matrix that does not have a complete basis of eigenvectors, and is therefore not diagonalizable.  In particular, an n\u00a0\u00d7\u00a0n matrix is defective if and only if it does not have n linearly independent eigenvectors.[1]  A complete basis is formed by augmenting the eigenvectors with generalized eigenvectors, which are necessary for solving defective systems of ordinary differential equations and other problems.\n",
            "title": "Defective matrix",
            "url": "https://en.wikipedia.org/wiki/Defective_matrix"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Generalized eigenvector",
            "url": "https://en.wikipedia.org/wiki/Generalized_eigenvector"
        },
        {
            "desc_links": [
                "/wiki/Camille_Jordan",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Diagonalizable",
                "/wiki/Normal_matrix",
                "/wiki/Square_matrix",
                "/wiki/Block_diagonal_matrix",
                "/wiki/Jordan_block",
                "/wiki/Field_(mathematics)",
                "/wiki/If_and_only_if",
                "/wiki/Eigenvalue",
                "/wiki/Characteristic_polynomial",
                "/wiki/Algebraically_closed",
                "/wiki/Complex_number",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Linear_algebra",
                "/wiki/Linear_operator",
                "/wiki/Finite-dimensional",
                "/wiki/Vector_space",
                "/wiki/Upper_triangular_matrix",
                "/wiki/Jordan_matrix",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Superdiagonal"
            ],
            "links": [
                "/wiki/Condition_number",
                "/wiki/Numerical_analysis",
                "/wiki/Schur_decomposition",
                "/wiki/Pseudospectrum",
                "/wiki/Geometric_multiplicity",
                "/wiki/Matrix_addition#Direct_sum",
                "/wiki/Characteristic_polynomial",
                "/wiki/Laurent_series",
                "/wiki/Pole_(complex_analysis)",
                "/wiki/Resolvent_formalism",
                "/wiki/Holomorphic_function",
                "/wiki/Jordan_curve",
                "/wiki/Spectrum_(functional_analysis)",
                "/wiki/Holomorphic_functional_calculus",
                "/wiki/Compact_operator",
                "/wiki/Banach_space",
                "/wiki/Ring_(mathematics)",
                "/wiki/Structure_theorem_for_finitely_generated_modules_over_a_principal_ideal_domain",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Module_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Semisimple_operator",
                "/wiki/Nilpotent_matrix",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Algebraically_closed",
                "/wiki/Direct_sum",
                "/wiki/Spectral_theorem",
                "/wiki/Holomorphic_functional_calculus",
                "/wiki/Invariant_subspace",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Monic_polynomial",
                "/wiki/Principal_ideal_domain",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/Characteristic_polynomial",
                "/wiki/Splitting_field",
                "/wiki/Functional_calculus",
                "/wiki/Complex_conjugate",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Invariant_subspace",
                "/wiki/Jordan_chain",
                "/wiki/Jordan_block",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Block_diagonal_matrix",
                "/wiki/Hamel_dimension",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Linearly_independent",
                "/wiki/Eigenvectors",
                "/wiki/Subdiagonal",
                "/wiki/Camille_Jordan",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Diagonalizable",
                "/wiki/Normal_matrix",
                "/wiki/Square_matrix",
                "/wiki/Block_diagonal_matrix",
                "/wiki/Jordan_block",
                "/wiki/Field_(mathematics)",
                "/wiki/If_and_only_if",
                "/wiki/Eigenvalue",
                "/wiki/Characteristic_polynomial",
                "/wiki/Algebraically_closed",
                "/wiki/Complex_number",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Linear_algebra",
                "/wiki/Linear_operator",
                "/wiki/Finite-dimensional",
                "/wiki/Vector_space",
                "/wiki/Upper_triangular_matrix",
                "/wiki/Jordan_matrix",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Superdiagonal"
            ],
            "text": "The following example shows the application to the power function f(z)=zn:The Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let f(z) be an analytical function of a complex argument. Applying the function on a n\u00d7n Jordan block J with eigenvalue \u03bb results in an upper triangular matrix:This ill conditioning makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in numerical analysis; the stable Schur decomposition[14] or pseudospectra[15] are better alternatives.However, for \u03b5 \u2260 0, the Jordan normal form isIf \u03b5 = 0, then the Jordan normal form is simplyIf the matrix A has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrixIf we had interchanged the order of which the chain vectors appeared, that is, changing the order of v, w and {x, y} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.A computation shows that the equation P\u22121AP = J indeed holds.The transition matrix P such that P\u22121AP = J is formed by putting these vectors next to each other as followswhere I is the 4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of A\u00a0\u2212\u00a04I, e.g., y = (1,0,0,0)T. Now, (A\u00a0\u2212\u00a04I)y = x and (A\u00a0\u2212\u00a04I)x = 0, so {y, x} is a chain of length two corresponding to the eigenvalue 4.There are three chains. Two have length one: {v} and {w}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculateThis shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation Av = \u03bb v. It is spanned by the column vector v = (\u22121, 1, 0, 0)T. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by w = (1, \u22121, 0, 1)T. Finally, the eigenspace corresponding to the eigenvalue 4 is also one-dimensional (even though this is a double eigenvalue) and is spanned by x = (1, 0, \u22121, 1)T. So, the geometric multiplicity (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix A is the direct sumThe characteristic polynomial of A iswhich is mentioned in the beginning of the article.Consider the matrixThis example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.is precisely the index of \u03bb, \u03bd(\u03bb). In other words, the function RT has a pole of order \u03bd(\u03bb) at \u03bb.But we have shown that the smallest positive integer m such thatBy the previous discussion on the functional calculus,whereConsider the annular region A centered at the eigenvalue \u03bb with sufficiently small radius \u03b5 such that the intersection of the open disc B\u03b5(\u03bb) and \u03c3(T) is {\u03bb}. The resolvent function RT is holomorphic on A. Extending a result from classical function theory, RT has a Laurent series representation on A:We will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators.has a pole of order \u03bd at \u03bb.The point \u03bb is called a pole of operator T with order \u03bd if the resolvent function RT defined byLet T be a bounded operator \u03bb be an isolated point of \u03c3(T). (As stated above, when T is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)Notice that the expression of f(T) is a finite sum because, on each neighborhood of \u03bbi, we have chosen the Taylor series expansion of f centered at \u03bbi.given in a previous section. Each ei(T) is the projection onto the subspace spanned by the Jordan chains corresponding to \u03bbi and along the subspaces spanned by the Jordan chains corresponding to \u03bbj for j \u2260 i. In other words, ei(T) = P(\u03bbi;T). This explicit identification of the operators ei(T) in turn gives an explicit form of holomorphic functional calculus for matrices:where the index i runs through the distinct eigenvalues of T. This is exactly the invariant subspace decompositionimpliesThe relationBy property 3, f(T) ei(T) = ei(T) f(T). So ei(T) is precisely the projection onto the subspacehas spectrum {0}. By property 1, f(T) can be directly computed in the Jordan form, and by inspection, we see that the operator f(T)ei(T) is the zero matrix.The spectral mapping theorem tells usis a projection. Moreoever, let \u03bdi be the index of \u03bbi andIn the finite-dimensional case, \u03c3(T) = {\u03bbi} is a finite discrete set in the complex plane. Let ei be the function that is 1 in some open neighborhood of \u03bbi and 0 elsewhere. By property 3 of the functional calculus, the operatorWe will require the following properties of this functional calculus:The open set G could vary with f and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar case. Although the integral makes sense for continuous f, we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that \u03c3(T) lie in the inside of \u0393 ensures f(T) is well defined; it does not depend on the choice of \u0393. The functional calculus is the mapping \u03a6 from Hol(T) to L(X) given byFix a bounded operator T. Consider the family Hol(T) of complex functions that is holomorphic on some open set G containing \u03c3(T). Let \u0393 = {\u03b3i} be a finite collection of Jordan curves such that \u03c3(T) lies in the inside of \u0393, we define f(T) byLet X be a Banach space, L(X) be the bounded operators on X, and \u03c3(T) denote the spectrum of T \u2208 L(X). The holomorphic functional calculus is defined as follows:In a different direction, for compact operators on a Banach space, a result analogous to the Jordan normal form holds. One restricts to compact operators because every point x in the spectrum of a compact operator T, the only exception being when x is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.The proof of the Jordan normal form is usually carried out as an application to the ring K[x] of the structure theorem for finitely generated modules over a principal ideal domain, of which it is a corollary.Similar to the case when K is the complex numbers, knowing the dimensions of the kernels of (M \u2212 \u03bbI)k for 1 \u2264 k \u2264 m, where m is the algebraic multiplicity of the eigenvalue \u03bb, allows one to determine the Jordan form of M. We may view the underlying vector space V as a K[x]-module by regarding the action of x on V as application of M and extending by K-linearity. Then the polynomials (x\u00a0\u2212\u00a0\u03bb)k are the elementary divisors of M, and the Jordan normal form is concerned with representing M in terms of blocks associated to the elementary divisors.Jordan reduction can be extended to any square matrix M whose entries lie in a field K. The result states that any M can be written as a sum D + N where D is semisimple, N is nilpotent, and DN = ND. This is called the Jordan\u2013Chevalley decomposition. Whenever K contains the eigenvalues of M, in particular when K is algebraically closed, the normal form can be expressed explicitly as the direct sum of Jordan blocks.So \u03bd(\u03bb) > 0 if and only if \u03bb is an eigenvalue of A. In the finite-dimensional case, \u03bd(\u03bb) \u2264 the algebraic multiplicity of \u03bb.It might be of interest here to note some properties of the index, \u03bd(\u03bb). More generally, for a complex number \u03bb, its index can be defined as the least non-negative integer \u03bd(\u03bb) such thatComparing the two decompositions, notice that, in general, l \u2264 k. When A is normal, the subspaces Xi's in the first decomposition are one-dimensional and mutually orthogonal. This is the spectral theorem for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.The projection onto Yi and along all the other Yj ( j \u2260 i ) is called the spectral projection of A at \u03bbi and is usually denoted by P(\u03bbi\u00a0; A). Spectral projections are mutually orthogonal in the sense that P(\u03bbi\u00a0; A) P(\u03bbj\u00a0; A) = 0 if i \u2260 j. Also they commute with A and their sum is the identity matrix. Replacing every \u03bbi in the Jordan matrix J by one and zeroising all other entries gives P(\u03bbi\u00a0; J), moreover if U J U\u22121 is the similarity transformation such that A = U J U\u22121 then P(\u03bbi\u00a0; A) = U P(\u03bbi\u00a0; J) U\u22121. They are not confined to finite dimensions. See below for their application to compact operators, and in holomorphic functional calculus for a more general discussion.where l is the number of distinct eigenvalues of A. Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where A is a multiple of the identity matrix we have k = n and l = 1.This gives the decompositionOne can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue \u03bbi, the size of its largest corresponding Jordan block si is called the index of \u03bbi and denoted by \u03bd(\u03bbi). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace Yi bywhere each Xi is the span of the corresponding Jordan chain, and k is the number of Jordan chains.The Jordan form of a n \u00d7 n matrix A is block diagonal, and therefore gives a decomposition of the n dimensional Euclidean space into invariant subspaces of A. Every Jordan block Ji corresponds to an invariant subspace Xi. Symbolically, we putThe degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, A is diagonalizable.While the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of elementary divisors. The elementary divisors of a square matrix A are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial m are the elementary divisors of the largest degree corresponding to distinct eigenvalues.Let \u03bb1, ..., \u03bbq be the distinct eigenvalues of A, and si be the size of the largest Jordan block corresponding to \u03bbi. It is clear from the Jordan normal form that the minimal polynomial of A has degree \u03a3si.The minimal polynomial P of a square matrix A is the unique monic polynomial of least degree, m, such that P(A) = 0. Alternatively, the set of polynomials that annihilate a given A form an ideal I in C[x], the principal ideal domain of polynomials with complex coefficients. The monic element that generates I is precisely P.The Cayley\u2013Hamilton theorem asserts that every matrix A satisfies its characteristic equation: if p is the characteristic polynomial of A, then p(A) = 0. This can be shown via direct calculation in the Jordan form, since any Jordan block for \u03bb is annihilated by (X \u2212 \u03bb)m where m is the multiplicity of the root \u03bb of p, the sum of the sizes of the Jordan blocks for \u03bb, and therefore no less than the size of the block in question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the splitting field of p; this field extension does not change the matrix p(A) in any way.Using the Jordan normal form, direct calculation gives a spectral mapping theorem for the polynomial functional calculus: Let A be an n \u00d7 n matrix with eigenvalues \u03bb1, ..., \u03bbn, then for any polynomial p, p(A) has eigenvalues p(\u03bb1), ..., p(\u03bbn).One can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.This real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form complex conjugate pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.This can be used to show the uniqueness of the Jordan form. Let J1 and J2 be two Jordan normal forms of A. Then J1 and J2 are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of J1 and J2. This proves the uniqueness part of the statement.is twice the number of Jordan blocks of size k1 plus the number of Jordan blocks of size k1\u22121. The general case is similar.is the number of Jordan blocks of size k1. Similarly, the rank ofis the size of the largest Jordan block in the Jordan form of A. (This number k1 is also called the index of \u03bb. See discussion in a following section.) The rank ofKnowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of A. Assuming the algebraic multiplicity m(\u03bb) of an eigenvalue \u03bb is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (A \u2212 \u03bb I)m(\u03bb). To see this, suppose an n \u00d7 n matrix A has only one eigenvalue \u03bb. So m(\u03bb) = n. The smallest integer k1 such thatIt can be shown that the Jordan normal form of a given matrix A is unique up to the order of the Jordan blocks.By construction, the union of the three sets {p1, ..., pr}, {qr\u2212s +1, ..., qr}, and {z1, ..., zt} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of A. Finally, by the rank\u2013nullity theorem, the cardinality of the union is n. In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of A, and this shows A can be put in Jordan normal form.Finally, we can pick any linearly independent set {z1, ..., zt} that spansClearly no non-trivial linear combination of the qi can lie in Ker(A \u2212 \u03bb I). Furthermore, no non-trivial linear combination of the qi can be in Ran(A \u2212 \u03bb I), for that would contradict the assumption that each pi is a lead vector in a Jordan chain. The set {qi}, being preimages of the linearly independent set {pi} under A \u2212 \u03bb I, is also linearly independent.let the dimension of Q be s \u2264 r. Each vector in Q is an eigenvector of A' corresponding to eigenvalue \u03bb. So the Jordan form of A' must contain s Jordan chains corresponding to s linearly independent eigenvectors. So the basis {p1, ..., pr} must contain s vectors, say {pr\u2212s+1, ..., pr}, that are lead vectors in these Jordan chains from the Jordan normal form of A'. We can \"extend the chains\" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(A \u2212 \u03bb I).) Let qi be such thatOtherwise, ifthe desired result follows immediately from the rank\u2013nullity theorem. This would be the case, for example, if A was Hermitian.Next consider the subspace Ker(A \u2212 \u03bb I). IfWe give a proof by induction that any complex-valued matrix A may be put in Jordan normal form. The 1 \u00d7 1 case is trivial. Let A be an n \u00d7 n matrix. Take any eigenvalue \u03bb of A. The range of A \u2212 \u03bb I, denoted by Ran(A \u2212 \u03bb I), is an invariant subspace of A. Also, since \u03bb is an eigenvalue of A, the dimension of Ran(A \u2212 \u03bb I), r, is strictly less than n. Let A' denote the restriction of A to Ran(A \u2212 \u03bb I), By inductive hypothesis, there exists a basis {p1, ..., pr} such that A' , expressed with respect to this basis, is in Jordan normal form.Therefore, the statement that every square matrix A can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of A.Thus, given an eigenvalue \u03bb, its corresponding Jordan block gives rise to a Jordan chain. The generator, or lead vector, say pr, of the chain is a generalized eigenvector such that (A \u2212 \u03bb I)rpr = 0, where r is the size of the Jordan block. The vector p1 = (A \u2212 \u03bb I)r\u22121pr is an eigenvector corresponding to \u03bb. In general, pi is a preimage of pi\u22121 under A \u2212 \u03bb I. So the lead vector generates the chain via multiplication by (A \u2212 \u03bb I).[12][13]We see thatLet P have column vectors pi, i = 1, ..., 4, thenConsider the matrix A from the example in the previous section. The Jordan normal form is obtained by some similarity transformation P\u22121AP = J, i.e.,Assuming this result, we can deduce the following properties:So there exists an invertible matrix P such that P\u22121AP = J is such that the only non-zero entries of J are on the diagonal and the superdiagonal. J is called the Jordan normal form of A. Each Ji is called a Jordan block of A. In a given Jordan block, every entry on the superdiagonal is 1.where each block Ji is a square matrix of the formIn general, a square complex matrix A is similar to a block diagonal matrixThe matrix J is almost diagonal. This is the Jordan normal form of A. The section Example below fills in the details of the computation.Including multiplicity, the eigenvalues of A are \u03bb = 1, 2, 4, 4. The dimension of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so A is not diagonalizable. However, there is an invertible matrix P such that A = PJP\u22121, whereAn n \u00d7 n matrix A is diagonalizable if and only if the sum of the dimensions of the eigenspaces is n. Or, equivalently, if and only if A has n linearly independent eigenvectors. Not all matrices are diagonalizable. Consider the following matrix:Some textbooks have the ones on the subdiagonal, i.e., immediately below the main diagonal instead of on the superdiagonal. The eigenvalues are still on the main diagonal.[9][10]The Jordan normal form is named after Camille Jordan, who first stated the Jordan decomposition theorem in 1870.[8]The Jordan\u2013Chevalley decomposition is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the Jordan normal form.[5][6][7]If the operator is originally given by a square matrix M, then its Jordan normal form is also called the Jordan normal form of M. Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given M is not entirely unique, as it is a block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.[2][3][4]Let V be a vector space over a field K. Then a basis with respect to which the matrix has the required form exists if and only if all eigenvalues of the matrix lie in K, or equivalently if the characteristic polynomial of the operator splits into linear factors over K. This condition is always satisfied if K is algebraically closed (for instance, if it is the field of complex numbers). The diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue.[2][3][4]In linear algebra, a Jordan normal form (often called Jordan canonical form)[1] of a linear operator on a finite-dimensional vector space is an upper triangular matrix of a particular form called a Jordan matrix, representing the operator with respect to some basis. Such a matrix has each non-zero off-diagonal entry equal to\u00a01, immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them.",
            "title": "Jordan normal form",
            "url": "https://en.wikipedia.org/wiki/Jordan_normal_form"
        },
        {
            "desc_links": [
                "/wiki/Camille_Jordan",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Diagonalizable",
                "/wiki/Normal_matrix",
                "/wiki/Square_matrix",
                "/wiki/Block_diagonal_matrix",
                "/wiki/Jordan_block",
                "/wiki/Field_(mathematics)",
                "/wiki/If_and_only_if",
                "/wiki/Eigenvalue",
                "/wiki/Characteristic_polynomial",
                "/wiki/Algebraically_closed",
                "/wiki/Complex_number",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Linear_algebra",
                "/wiki/Linear_operator",
                "/wiki/Finite-dimensional",
                "/wiki/Vector_space",
                "/wiki/Upper_triangular_matrix",
                "/wiki/Jordan_matrix",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Superdiagonal"
            ],
            "links": [
                "/wiki/Condition_number",
                "/wiki/Numerical_analysis",
                "/wiki/Schur_decomposition",
                "/wiki/Pseudospectrum",
                "/wiki/Geometric_multiplicity",
                "/wiki/Matrix_addition#Direct_sum",
                "/wiki/Characteristic_polynomial",
                "/wiki/Laurent_series",
                "/wiki/Pole_(complex_analysis)",
                "/wiki/Resolvent_formalism",
                "/wiki/Holomorphic_function",
                "/wiki/Jordan_curve",
                "/wiki/Spectrum_(functional_analysis)",
                "/wiki/Holomorphic_functional_calculus",
                "/wiki/Compact_operator",
                "/wiki/Banach_space",
                "/wiki/Ring_(mathematics)",
                "/wiki/Structure_theorem_for_finitely_generated_modules_over_a_principal_ideal_domain",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Module_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Semisimple_operator",
                "/wiki/Nilpotent_matrix",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Algebraically_closed",
                "/wiki/Direct_sum",
                "/wiki/Spectral_theorem",
                "/wiki/Holomorphic_functional_calculus",
                "/wiki/Invariant_subspace",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Monic_polynomial",
                "/wiki/Principal_ideal_domain",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/Characteristic_polynomial",
                "/wiki/Splitting_field",
                "/wiki/Functional_calculus",
                "/wiki/Complex_conjugate",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Invariant_subspace",
                "/wiki/Jordan_chain",
                "/wiki/Jordan_block",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Block_diagonal_matrix",
                "/wiki/Hamel_dimension",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Linearly_independent",
                "/wiki/Eigenvectors",
                "/wiki/Subdiagonal",
                "/wiki/Camille_Jordan",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Diagonalizable",
                "/wiki/Normal_matrix",
                "/wiki/Square_matrix",
                "/wiki/Block_diagonal_matrix",
                "/wiki/Jordan_block",
                "/wiki/Field_(mathematics)",
                "/wiki/If_and_only_if",
                "/wiki/Eigenvalue",
                "/wiki/Characteristic_polynomial",
                "/wiki/Algebraically_closed",
                "/wiki/Complex_number",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Linear_algebra",
                "/wiki/Linear_operator",
                "/wiki/Finite-dimensional",
                "/wiki/Vector_space",
                "/wiki/Upper_triangular_matrix",
                "/wiki/Jordan_matrix",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Superdiagonal"
            ],
            "text": "The following example shows the application to the power function f(z)=zn:The Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let f(z) be an analytical function of a complex argument. Applying the function on a n\u00d7n Jordan block J with eigenvalue \u03bb results in an upper triangular matrix:This ill conditioning makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in numerical analysis; the stable Schur decomposition[14] or pseudospectra[15] are better alternatives.However, for \u03b5 \u2260 0, the Jordan normal form isIf \u03b5 = 0, then the Jordan normal form is simplyIf the matrix A has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrixIf we had interchanged the order of which the chain vectors appeared, that is, changing the order of v, w and {x, y} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.A computation shows that the equation P\u22121AP = J indeed holds.The transition matrix P such that P\u22121AP = J is formed by putting these vectors next to each other as followswhere I is the 4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of A\u00a0\u2212\u00a04I, e.g., y = (1,0,0,0)T. Now, (A\u00a0\u2212\u00a04I)y = x and (A\u00a0\u2212\u00a04I)x = 0, so {y, x} is a chain of length two corresponding to the eigenvalue 4.There are three chains. Two have length one: {v} and {w}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculateThis shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation Av = \u03bb v. It is spanned by the column vector v = (\u22121, 1, 0, 0)T. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by w = (1, \u22121, 0, 1)T. Finally, the eigenspace corresponding to the eigenvalue 4 is also one-dimensional (even though this is a double eigenvalue) and is spanned by x = (1, 0, \u22121, 1)T. So, the geometric multiplicity (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix A is the direct sumThe characteristic polynomial of A iswhich is mentioned in the beginning of the article.Consider the matrixThis example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.is precisely the index of \u03bb, \u03bd(\u03bb). In other words, the function RT has a pole of order \u03bd(\u03bb) at \u03bb.But we have shown that the smallest positive integer m such thatBy the previous discussion on the functional calculus,whereConsider the annular region A centered at the eigenvalue \u03bb with sufficiently small radius \u03b5 such that the intersection of the open disc B\u03b5(\u03bb) and \u03c3(T) is {\u03bb}. The resolvent function RT is holomorphic on A. Extending a result from classical function theory, RT has a Laurent series representation on A:We will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators.has a pole of order \u03bd at \u03bb.The point \u03bb is called a pole of operator T with order \u03bd if the resolvent function RT defined byLet T be a bounded operator \u03bb be an isolated point of \u03c3(T). (As stated above, when T is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)Notice that the expression of f(T) is a finite sum because, on each neighborhood of \u03bbi, we have chosen the Taylor series expansion of f centered at \u03bbi.given in a previous section. Each ei(T) is the projection onto the subspace spanned by the Jordan chains corresponding to \u03bbi and along the subspaces spanned by the Jordan chains corresponding to \u03bbj for j \u2260 i. In other words, ei(T) = P(\u03bbi;T). This explicit identification of the operators ei(T) in turn gives an explicit form of holomorphic functional calculus for matrices:where the index i runs through the distinct eigenvalues of T. This is exactly the invariant subspace decompositionimpliesThe relationBy property 3, f(T) ei(T) = ei(T) f(T). So ei(T) is precisely the projection onto the subspacehas spectrum {0}. By property 1, f(T) can be directly computed in the Jordan form, and by inspection, we see that the operator f(T)ei(T) is the zero matrix.The spectral mapping theorem tells usis a projection. Moreoever, let \u03bdi be the index of \u03bbi andIn the finite-dimensional case, \u03c3(T) = {\u03bbi} is a finite discrete set in the complex plane. Let ei be the function that is 1 in some open neighborhood of \u03bbi and 0 elsewhere. By property 3 of the functional calculus, the operatorWe will require the following properties of this functional calculus:The open set G could vary with f and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar case. Although the integral makes sense for continuous f, we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that \u03c3(T) lie in the inside of \u0393 ensures f(T) is well defined; it does not depend on the choice of \u0393. The functional calculus is the mapping \u03a6 from Hol(T) to L(X) given byFix a bounded operator T. Consider the family Hol(T) of complex functions that is holomorphic on some open set G containing \u03c3(T). Let \u0393 = {\u03b3i} be a finite collection of Jordan curves such that \u03c3(T) lies in the inside of \u0393, we define f(T) byLet X be a Banach space, L(X) be the bounded operators on X, and \u03c3(T) denote the spectrum of T \u2208 L(X). The holomorphic functional calculus is defined as follows:In a different direction, for compact operators on a Banach space, a result analogous to the Jordan normal form holds. One restricts to compact operators because every point x in the spectrum of a compact operator T, the only exception being when x is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.The proof of the Jordan normal form is usually carried out as an application to the ring K[x] of the structure theorem for finitely generated modules over a principal ideal domain, of which it is a corollary.Similar to the case when K is the complex numbers, knowing the dimensions of the kernels of (M \u2212 \u03bbI)k for 1 \u2264 k \u2264 m, where m is the algebraic multiplicity of the eigenvalue \u03bb, allows one to determine the Jordan form of M. We may view the underlying vector space V as a K[x]-module by regarding the action of x on V as application of M and extending by K-linearity. Then the polynomials (x\u00a0\u2212\u00a0\u03bb)k are the elementary divisors of M, and the Jordan normal form is concerned with representing M in terms of blocks associated to the elementary divisors.Jordan reduction can be extended to any square matrix M whose entries lie in a field K. The result states that any M can be written as a sum D + N where D is semisimple, N is nilpotent, and DN = ND. This is called the Jordan\u2013Chevalley decomposition. Whenever K contains the eigenvalues of M, in particular when K is algebraically closed, the normal form can be expressed explicitly as the direct sum of Jordan blocks.So \u03bd(\u03bb) > 0 if and only if \u03bb is an eigenvalue of A. In the finite-dimensional case, \u03bd(\u03bb) \u2264 the algebraic multiplicity of \u03bb.It might be of interest here to note some properties of the index, \u03bd(\u03bb). More generally, for a complex number \u03bb, its index can be defined as the least non-negative integer \u03bd(\u03bb) such thatComparing the two decompositions, notice that, in general, l \u2264 k. When A is normal, the subspaces Xi's in the first decomposition are one-dimensional and mutually orthogonal. This is the spectral theorem for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.The projection onto Yi and along all the other Yj ( j \u2260 i ) is called the spectral projection of A at \u03bbi and is usually denoted by P(\u03bbi\u00a0; A). Spectral projections are mutually orthogonal in the sense that P(\u03bbi\u00a0; A) P(\u03bbj\u00a0; A) = 0 if i \u2260 j. Also they commute with A and their sum is the identity matrix. Replacing every \u03bbi in the Jordan matrix J by one and zeroising all other entries gives P(\u03bbi\u00a0; J), moreover if U J U\u22121 is the similarity transformation such that A = U J U\u22121 then P(\u03bbi\u00a0; A) = U P(\u03bbi\u00a0; J) U\u22121. They are not confined to finite dimensions. See below for their application to compact operators, and in holomorphic functional calculus for a more general discussion.where l is the number of distinct eigenvalues of A. Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where A is a multiple of the identity matrix we have k = n and l = 1.This gives the decompositionOne can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue \u03bbi, the size of its largest corresponding Jordan block si is called the index of \u03bbi and denoted by \u03bd(\u03bbi). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace Yi bywhere each Xi is the span of the corresponding Jordan chain, and k is the number of Jordan chains.The Jordan form of a n \u00d7 n matrix A is block diagonal, and therefore gives a decomposition of the n dimensional Euclidean space into invariant subspaces of A. Every Jordan block Ji corresponds to an invariant subspace Xi. Symbolically, we putThe degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, A is diagonalizable.While the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of elementary divisors. The elementary divisors of a square matrix A are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial m are the elementary divisors of the largest degree corresponding to distinct eigenvalues.Let \u03bb1, ..., \u03bbq be the distinct eigenvalues of A, and si be the size of the largest Jordan block corresponding to \u03bbi. It is clear from the Jordan normal form that the minimal polynomial of A has degree \u03a3si.The minimal polynomial P of a square matrix A is the unique monic polynomial of least degree, m, such that P(A) = 0. Alternatively, the set of polynomials that annihilate a given A form an ideal I in C[x], the principal ideal domain of polynomials with complex coefficients. The monic element that generates I is precisely P.The Cayley\u2013Hamilton theorem asserts that every matrix A satisfies its characteristic equation: if p is the characteristic polynomial of A, then p(A) = 0. This can be shown via direct calculation in the Jordan form, since any Jordan block for \u03bb is annihilated by (X \u2212 \u03bb)m where m is the multiplicity of the root \u03bb of p, the sum of the sizes of the Jordan blocks for \u03bb, and therefore no less than the size of the block in question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the splitting field of p; this field extension does not change the matrix p(A) in any way.Using the Jordan normal form, direct calculation gives a spectral mapping theorem for the polynomial functional calculus: Let A be an n \u00d7 n matrix with eigenvalues \u03bb1, ..., \u03bbn, then for any polynomial p, p(A) has eigenvalues p(\u03bb1), ..., p(\u03bbn).One can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.This real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form complex conjugate pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.This can be used to show the uniqueness of the Jordan form. Let J1 and J2 be two Jordan normal forms of A. Then J1 and J2 are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of J1 and J2. This proves the uniqueness part of the statement.is twice the number of Jordan blocks of size k1 plus the number of Jordan blocks of size k1\u22121. The general case is similar.is the number of Jordan blocks of size k1. Similarly, the rank ofis the size of the largest Jordan block in the Jordan form of A. (This number k1 is also called the index of \u03bb. See discussion in a following section.) The rank ofKnowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of A. Assuming the algebraic multiplicity m(\u03bb) of an eigenvalue \u03bb is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (A \u2212 \u03bb I)m(\u03bb). To see this, suppose an n \u00d7 n matrix A has only one eigenvalue \u03bb. So m(\u03bb) = n. The smallest integer k1 such thatIt can be shown that the Jordan normal form of a given matrix A is unique up to the order of the Jordan blocks.By construction, the union of the three sets {p1, ..., pr}, {qr\u2212s +1, ..., qr}, and {z1, ..., zt} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of A. Finally, by the rank\u2013nullity theorem, the cardinality of the union is n. In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of A, and this shows A can be put in Jordan normal form.Finally, we can pick any linearly independent set {z1, ..., zt} that spansClearly no non-trivial linear combination of the qi can lie in Ker(A \u2212 \u03bb I). Furthermore, no non-trivial linear combination of the qi can be in Ran(A \u2212 \u03bb I), for that would contradict the assumption that each pi is a lead vector in a Jordan chain. The set {qi}, being preimages of the linearly independent set {pi} under A \u2212 \u03bb I, is also linearly independent.let the dimension of Q be s \u2264 r. Each vector in Q is an eigenvector of A' corresponding to eigenvalue \u03bb. So the Jordan form of A' must contain s Jordan chains corresponding to s linearly independent eigenvectors. So the basis {p1, ..., pr} must contain s vectors, say {pr\u2212s+1, ..., pr}, that are lead vectors in these Jordan chains from the Jordan normal form of A'. We can \"extend the chains\" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(A \u2212 \u03bb I).) Let qi be such thatOtherwise, ifthe desired result follows immediately from the rank\u2013nullity theorem. This would be the case, for example, if A was Hermitian.Next consider the subspace Ker(A \u2212 \u03bb I). IfWe give a proof by induction that any complex-valued matrix A may be put in Jordan normal form. The 1 \u00d7 1 case is trivial. Let A be an n \u00d7 n matrix. Take any eigenvalue \u03bb of A. The range of A \u2212 \u03bb I, denoted by Ran(A \u2212 \u03bb I), is an invariant subspace of A. Also, since \u03bb is an eigenvalue of A, the dimension of Ran(A \u2212 \u03bb I), r, is strictly less than n. Let A' denote the restriction of A to Ran(A \u2212 \u03bb I), By inductive hypothesis, there exists a basis {p1, ..., pr} such that A' , expressed with respect to this basis, is in Jordan normal form.Therefore, the statement that every square matrix A can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of A.Thus, given an eigenvalue \u03bb, its corresponding Jordan block gives rise to a Jordan chain. The generator, or lead vector, say pr, of the chain is a generalized eigenvector such that (A \u2212 \u03bb I)rpr = 0, where r is the size of the Jordan block. The vector p1 = (A \u2212 \u03bb I)r\u22121pr is an eigenvector corresponding to \u03bb. In general, pi is a preimage of pi\u22121 under A \u2212 \u03bb I. So the lead vector generates the chain via multiplication by (A \u2212 \u03bb I).[12][13]We see thatLet P have column vectors pi, i = 1, ..., 4, thenConsider the matrix A from the example in the previous section. The Jordan normal form is obtained by some similarity transformation P\u22121AP = J, i.e.,Assuming this result, we can deduce the following properties:So there exists an invertible matrix P such that P\u22121AP = J is such that the only non-zero entries of J are on the diagonal and the superdiagonal. J is called the Jordan normal form of A. Each Ji is called a Jordan block of A. In a given Jordan block, every entry on the superdiagonal is 1.where each block Ji is a square matrix of the formIn general, a square complex matrix A is similar to a block diagonal matrixThe matrix J is almost diagonal. This is the Jordan normal form of A. The section Example below fills in the details of the computation.Including multiplicity, the eigenvalues of A are \u03bb = 1, 2, 4, 4. The dimension of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so A is not diagonalizable. However, there is an invertible matrix P such that A = PJP\u22121, whereAn n \u00d7 n matrix A is diagonalizable if and only if the sum of the dimensions of the eigenspaces is n. Or, equivalently, if and only if A has n linearly independent eigenvectors. Not all matrices are diagonalizable. Consider the following matrix:Some textbooks have the ones on the subdiagonal, i.e., immediately below the main diagonal instead of on the superdiagonal. The eigenvalues are still on the main diagonal.[9][10]The Jordan normal form is named after Camille Jordan, who first stated the Jordan decomposition theorem in 1870.[8]The Jordan\u2013Chevalley decomposition is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the Jordan normal form.[5][6][7]If the operator is originally given by a square matrix M, then its Jordan normal form is also called the Jordan normal form of M. Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given M is not entirely unique, as it is a block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.[2][3][4]Let V be a vector space over a field K. Then a basis with respect to which the matrix has the required form exists if and only if all eigenvalues of the matrix lie in K, or equivalently if the characteristic polynomial of the operator splits into linear factors over K. This condition is always satisfied if K is algebraically closed (for instance, if it is the field of complex numbers). The diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue.[2][3][4]In linear algebra, a Jordan normal form (often called Jordan canonical form)[1] of a linear operator on a finite-dimensional vector space is an upper triangular matrix of a particular form called a Jordan matrix, representing the operator with respect to some basis. Such a matrix has each non-zero off-diagonal entry equal to\u00a01, immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them.",
            "title": "Jordan normal form",
            "url": "https://en.wikipedia.org/wiki/Jordan_normal_form"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Generalized eigenvector",
            "url": "https://en.wikipedia.org/wiki/Generalized_eigenspace"
        },
        {
            "desc_links": [
                "/wiki/Linear_algebra",
                "/wiki/Spectral_decomposition",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)",
                "/wiki/Canonical_form",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Diagonalizable_matrix"
            ],
            "links": [
                "/wiki/Matrix_pencil",
                "/wiki/Optics",
                "/wiki/Forward_Scattering_Alignment",
                "/wiki/Radar",
                "/wiki/Back_Scattering_Alignment",
                "/wiki/Generalized_eigenspace",
                "/wiki/Generalized_eigenvector",
                "/wiki/Power_iteration",
                "/wiki/Rayleigh_quotient",
                "/wiki/Hermitian_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Schur_decomposition",
                "/wiki/Backsubstitution",
                "/wiki/Divide-and-conquer_eigenvalue_algorithm",
                "/wiki/Gaussian_elimination",
                "/wiki/System_of_linear_equations#Solving_a_linear_system",
                "/wiki/System_of_linear_equations",
                "/wiki/Sequence",
                "/wiki/Almost_always",
                "/wiki/Google",
                "/wiki/PageRank",
                "/wiki/Linear_span",
                "/wiki/Arnoldi_iteration",
                "/wiki/QR_algorithm",
                "/wiki/Newton%27s_method",
                "/wiki/Round-off_error",
                "/wiki/Ill-conditioned",
                "/wiki/Abel%E2%80%93Ruffini_theorem",
                "/wiki/Iterative_method",
                "/wiki/Characteristic_polynomial",
                "/wiki/Numerical_analysis",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Orthogonal_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Holomorphic_functional_calculus",
                "/wiki/Diagonal_matrix",
                "/wiki/Power_series",
                "/wiki/Laplace_operator",
                "/wiki/Data",
                "/wiki/Inverse_function",
                "/wiki/Diagonal_matrix",
                "/wiki/Nonsingular",
                "/wiki/Linearly_independent",
                "/wiki/Geometric_multiplicity",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Factorization",
                "/wiki/Characteristic_polynomial",
                "/wiki/Spectrum_of_a_matrix",
                "/wiki/Linear_algebra",
                "/wiki/Spectral_decomposition",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)",
                "/wiki/Canonical_form",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Diagonalizable_matrix"
            ],
            "text": "The set of matrices of the form A \u2212 \u03bbB, where \u03bb is a complex number, is called a pencil; the term matrix pencil can also refer to the pair (A,B) of matrices.[9] If B is invertible, then the original problem can be written in the formAnd since P is invertible, we multiply the equation from the right by its inverse, finishing the proof.And the proof isThen the following equality holdswhere A and B are matrices. If v obeys this equation, with some \u03bb, then we call v the generalized eigenvector of A and B (in the 2nd sense), and \u03bb is called the generalized eigenvalue of A and B (in the 2nd sense) which corresponds to the generalized eigenvector v. The possible values of \u03bb must obey the following equationA generalized eigenvalue problem (2nd sense) is the problem of finding a vector v that obeysFor example, in coherent electromagnetic scattering theory, the linear transformation A represents the action performed by the scattering object, and the eigenvectors represent polarization states of the electromagnetic wave. In optics, the coordinate system is defined from the wave's viewpoint, known as the Forward Scattering Alignment (FSA), and gives rise to a regular eigenvalue equation, whereas in radar, the coordinate system is defined from the radar's viewpoint, known as the Back Scattering Alignment (BSA), and gives rise to a coneigenvalue equation.A conjugate eigenvector or coneigenvector is a vector sent after transformation to a scalar multiple of its conjugate, where the scalar is called the conjugate eigenvalue or coneigenvalue of the linear transformation. The coneigenvectors and coneigenvalues represent essentially the same information and meaning as the regular eigenvectors and eigenvalues, but arise when an alternative coordinate system is used. The corresponding equation isThis usage should not be confused with the generalized eigenvalue problem described below.Recall that the geometric multiplicity of an eigenvalue can be described as the dimension of the associated eigenspace, the nullspace of \u03bbI \u2212 A. The algebraic multiplicity can also be thought of as a dimension: it is the dimension of the associated generalized eigenspace (1st sense), which is the nullspace of the matrix (\u03bbI \u2212 A)k for any sufficiently large k. That is, it is the space of generalized eigenvectors (1st sense), where a generalized eigenvector is any vector which eventually becomes 0 if \u03bbI \u2212 A is applied to it enough times successively. Any eigenvector is a generalized eigenvector, and so each eigenspace is contained in the associated generalized eigenspace. This provides an easy proof that the geometric multiplicity is always less than or equal to the algebraic multiplicity.However, in practical large-scale eigenvalue methods, the eigenvectors are usually computed in other ways, as a byproduct of the eigenvalue computation. In power iteration, for example, the eigenvector is actually computed before the eigenvalue (which is typically computed by the Rayleigh quotient of the eigenvector).[6] In the QR algorithm for a Hermitian matrix (or any normal matrix), the orthonormal eigenvectors are obtained as a product of the Q matrices from the steps in the algorithm.[6] (For more general matrices, the QR algorithm yields the Schur decomposition first, from which the eigenvectors can be obtained by a backsubstitution procedure.[8]) For Hermitian matrices, the Divide-and-conquer eigenvalue algorithm is more efficient than the QR algorithm if both eigenvectors and eigenvalues are desired.[6]using Gaussian elimination or any other method for solving matrix equations.Once the eigenvalues are computed, the eigenvectors could be calculated by solving the equationThis sequence will almost always converge to an eigenvector corresponding to the eigenvalue of greatest magnitude, provided that v has a nonzero component of this eigenvector in the eigenvector basis (and also provided that there is only one eigenvalue of greatest magnitude). This simple algorithm is useful in some practical applications; for example, Google uses it to calculate the page rank of documents in their search engine.[7] Also, the power method is the starting point for many more sophisticated algorithms. For instance, by keeping not just the last vector in the sequence, but instead looking at the span of all the vectors in the sequence, one can get a better (faster converging) approximation for the eigenvector, and this idea is the basis of Arnoldi iteration.[6] Alternatively, the important QR algorithm is also based on a subtle transformation of a power method.[6]Iterative numerical algorithms for approximating roots of polynomials exist, such as Newton's method, but in general it is impractical to compute the characteristic polynomial and then apply these methods. One reason is that small round-off errors in the coefficients of the characteristic polynomial can lead to large errors in the eigenvalues and eigenvectors: the roots are an extremely ill-conditioned function of the coefficients.[6]In practice, eigenvalues of large matrices are not computed using the characteristic polynomial. Computing the polynomial becomes expensive in itself, and exact (symbolic) roots of a high-degree polynomial can be difficult to compute and express: the Abel\u2013Ruffini theorem implies that the roots of high-degree (5 or above) polynomials cannot in general be expressed simply using nth roots. Therefore, general algorithms to find eigenvectors and eigenvalues are iterative.Suppose that we want to compute the eigenvalues of a given matrix. If the matrix is small, we can compute them symbolically using the characteristic polynomial. However, this is often impossible for larger matrices, in which case we must use a numerical method.Note that each eigenvalue is multiplied by ni, the algebraic multiplicity.Note that each eigenvalue is raised to the power ni, the algebraic multiplicity.where Q is an orthogonal matrix whose columns are the eigenvectors of A, and \u039b is a diagonal matrix whose entries are the eigenvalues of A.As a special case, for every N\u00d7N real symmetric matrix, the eigenvalues are real and the eigenvectors can be chosen such that they are orthogonal to each other. Thus a real symmetric matrix A can be decomposed asfrom above. Once again, we find thatA similar technique works more generally with the holomorphic functional calculus, usingThe off-diagonal elements of f(\u039b) are zero; that is, f(\u039b) is also a diagonal matrix. Therefore, calculating f(A) reduces to just calculating the function on each of the eigenvalues.Because \u039b is a diagonal matrix, functions of \u039b are very easy to calculate:then we know thatThe eigendecomposition allows for much easier computation of power series of matrices. If f(x) is given bywhere the eigenvalues are subscripted with an 's' to denote being sorted. The position of the minimization is the lowest reliable eigenvalue. In measurement systems, the square root of this reliable eigenvalue is the average noise over the components of the system.If the eigenvalues are rank-sorted by value, then the reliable eigenvalue can be found by minimization of the Laplacian of the sorted eigenvalues:[5]The reliable eigenvalue can be found by assuming that eigenvalues of extremely similar and low value are a good representation of measurement noise (which is assumed low for most systems).The second mitigation extends the eigenvalue so that lower values have much less influence over inversion, but do still contribute, such that solutions near the noise will still be found.The first mitigation method is similar to a sparse sample of the original matrix, removing components that are not considered valuable. However, if the solution or detection process is near the noise level, truncating may remove components that influence the desired solution.Two mitigations have been proposed: 1) truncating small/zero eigenvalues, 2) extending the lowest reliable eigenvalue to those below it.When eigendecomposition is used on a matrix of measured, real data, the inverse may be less valid when all eigenvalues are used unmodified in the form above. This is because as eigenvalues become relatively small, their contribution to the inversion is large. Those near zero or at the \"noise\" of the measurement system will have undue influence and could hamper solutions (detection) using the inverse.Furthermore, because \u039b is a diagonal matrix, its inverse is easy to calculate:If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is nonsingular and its inverse is given byPutting the solutions back into the above simultaneous equationsThusAnd can be represented by a single vector equation involving 2 solutions as eigenvalues:The above equation can be decomposed into 2 simultaneous equations:ThenThe eigenvectors can be indexed by eigenvalues, i.e. using a double index, with vi,j being the jth eigenvector for the ith eigenvalue. The eigenvectors can also be indexed using the simpler notation of a single index vk, with k = 1, 2, ..., Nv.There will be 1 \u2264 mi \u2264 ni linearly independent solutions to each eigenvalue equation. The linear combinations of the mi solutions are the eigenvectors associated with the eigenvalue \u03bbi. The integer mi is termed the geometric multiplicity of \u03bbi. It is important to keep in mind that the algebraic multiplicity ni and geometric multiplicity mi may or may not be equal, but we always have mi \u2264 ni. The simplest case is of course when mi = ni = 1. The total number of linearly independent eigenvectors, Nv, can be calculated by summing the geometric multiplicitiesFor each eigenvalue, \u03bbi, we have a specific eigenvalue equationThe integer ni is termed the algebraic multiplicity of eigenvalue \u03bbi. The algebraic multiplicities sum to N:We can factor p asWe call p(\u03bb) the characteristic polynomial, and the equation, called the characteristic equation, is an Nth order polynomial equation in the unknown \u03bb. This equation will have N\u03bb distinct solutions, where 1 \u2264 N\u03bb \u2264 N . The set of solutions, that is, the eigenvalues, is called the spectrum of A.[1][2][3]This yields an equation for the eigenvaluesA (non-zero) vector v of dimension N is an eigenvector of a square (N\u00d7N) matrix A if it satisfies the linear equationIn linear algebra, eigendecomposition or sometimes spectral decomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way.",
            "title": "Eigendecomposition of a matrix",
            "url": "https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix"
        },
        {
            "desc_links": [
                "/wiki/General_linear_group",
                "/wiki/Conjugacy_class",
                "/wiki/Invertible_matrix",
                "/wiki/Linear_operator",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Change_of_basis",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)"
            ],
            "links": [
                "/wiki/Permutation_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Spectral_theorem",
                "/wiki/Normal_matrix",
                "/wiki/Specht%27s_theorem",
                "/wiki/Field_extension",
                "/wiki/If_and_only_if",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Complex_number",
                "/wiki/Algebraically_closed_field",
                "/wiki/Jordan_form",
                "/wiki/Rational_canonical_form",
                "/wiki/Smith_normal_form",
                "/wiki/Equivalence_relation",
                "/wiki/General_linear_group",
                "/wiki/Conjugacy_class",
                "/wiki/Invertible_matrix",
                "/wiki/Linear_operator",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Change_of_basis",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)"
            ],
            "text": "In the definition of similarity, if the matrix P can be chosen to be a permutation matrix then A and B are permutation-similar; if P can be chosen to be a unitary matrix then A and B are unitarily equivalent. The spectral theorem says that every normal matrix is unitarily equivalent to some diagonal matrix. Specht's theorem states that two matrices are unitarily equivalent if and only if they satisfy certain trace equalities.Similarity of matrices does not depend on the base field: if L is a field containing K as a subfield, and A and B are two matrices over K, then A and B are similar as matrices over K if and only if they are similar as matrices over L. This is so because the rational canonical form over K is also the rational canonical form over L. This means that one may use Jordan forms that only exist over a larger field to determine whether the given matrices are similar.Because of this, for a given matrix A, one is interested in finding a simple \"normal form\" B which is similar to A\u2014the study of A then reduces to the study of the simpler matrix B. For example, A is called diagonalizable if it is similar to a diagonal matrix. Not all matrices are diagonalizable, but at least over the complex numbers (or any algebraically closed field), every matrix is similar to a matrix in Jordan form. Neither of these forms is unique (diagonal entries or Jordan blocks may be permuted) so they are not really normal forms; moreover their determination depends on being able to factor the minimal or characteristic polynomial of A (equivalently to find its eigenvalues). The rational canonical form does not have these drawbacks: it exists over any field, is truly unique, and it can be computed using only arithmetic operations in the field; A and B are similar if and only if they have the same rational canonical form. The rational canonical form is determined by the elementary divisors of A; these can be immediately read off from a matrix in Jordan form, but they can also be determined directly for any matrix by computing the Smith normal form, over the ring of polynomials, of the matrix (with polynomial entries) XIn \u2212 A (the same one whose determinant defines the characteristic polynomial). Note that this Smith normal form is not a normal form of A itself; moreover it is not similar to XIn \u2212 A either, but obtained from the latter by left and right multiplications by different invertible matrices (with polynomial entries).Because matrices are similar if and only if they represent the same linear operator with respect to (possibly) different bases, similar matrices share all properties of their shared underlying operator:Similarity is an equivalence relation on the space of square matrices.A transformation A \u21a6 P\u22121AP is called a similarity transformation or conjugation of the matrix A. In the general linear group, similarity is therefore the same as conjugacy, and similar matrices are also called conjugate; however in a given subgroup H of the general linear group, the notion of conjugacy may be more restrictive than similarity, since it requires that P be chosen to lie in H.for some invertible n-by-n matrix P. Similar matrices represent the same linear operator under two (possibly) different bases, with P being the change of basis matrix.[1][2]In linear algebra, two n-by-n matrices A and B are called similar if",
            "title": "Matrix similarity",
            "url": "https://en.wikipedia.org/wiki/Matrix_similarity"
        },
        {
            "desc_links": [
                "/wiki/Transformation_matrix#Eigenbasis_and_diagonal_matrix",
                "/wiki/Determinant",
                "/wiki/Inhomogeneous_dilation",
                "/wiki/Scaling_(geometry)",
                "/wiki/Homogeneous_dilation",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Basis_(linear_algebra)#Ordered_bases_and_coordinates",
                "/wiki/Diagonal_matrix",
                "/wiki/Defective_matrix"
            ],
            "links": [
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum_chemistry",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hilbert_space",
                "/wiki/Variational_principle",
                "/wiki/Linear_recursive_sequences",
                "/wiki/Fibonacci_number",
                "/wiki/Matrix_exponential",
                "/wiki/Matrix_function",
                "/wiki/Invertible_matrix#Methods_of_matrix_inversion",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvalues",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvalues",
                "/wiki/Nilpotent_matrix",
                "/wiki/Eigenvalue,_eigenvector_and_eigenspace#Algebraic_and_geometric_multiplicities",
                "/wiki/Rotation_matrix",
                "/wiki/Rotation_matrix#Independent_planes",
                "/wiki/Jordan_Normal_Form",
                "/wiki/Lie_theory",
                "/wiki/Toral_Lie_algebra",
                "/wiki/Normal_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Commuting_matrices",
                "/wiki/Eigenvalue_algorithm",
                "/wiki/Hermitian_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Orthonormal_basis",
                "/wiki/Unitary_matrix",
                "/wiki/Orthogonal_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Transpose",
                "/wiki/Right_eigenvector",
                "/wiki/Eigenvalue",
                "/wiki/Linearly_independent",
                "/wiki/Row_vector",
                "/wiki/Left_eigenvector",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Nilpotent",
                "/wiki/Jordan_form",
                "/wiki/Subset",
                "/wiki/Lebesgue_measure",
                "/wiki/Zariski_topology",
                "/wiki/Discriminant",
                "/wiki/Hypersurface",
                "/wiki/Norm_(mathematics)",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Elementary_divisor",
                "/wiki/Transformation_matrix#Eigenbasis_and_diagonal_matrix",
                "/wiki/Determinant",
                "/wiki/Inhomogeneous_dilation",
                "/wiki/Scaling_(geometry)",
                "/wiki/Homogeneous_dilation",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Basis_(linear_algebra)#Ordered_bases_and_coordinates",
                "/wiki/Diagonal_matrix",
                "/wiki/Defective_matrix"
            ],
            "text": "In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schr\u00f6dinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space). A very common approximation is to truncate Hilbert space to finite dimension, after which the Schr\u00f6dinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle, valid for Hamiltonians that are bounded from below. But also first-order perturbation theory for degenerate states leads to a matrix eigenvalue problem.thereby explaining the above phenomenon.The preceding relations, expressed in matrix form, areSwitching back to the standard basis, we haveThus, a and b are the eigenvalues corresponding to u and v, respectively. By linearity of matrix multiplication, we have thatStraightforward calculations show thatwhere ei denotes the standard basis of Rn. The reverse change of basis is given byThe above phenomenon can be explained by diagonalizing M. To accomplish this, we need a basis of R2 consisting of eigenvectors of M. One such eigenvector basis is given byCalculating the various powers of M reveals a surprising pattern:For example, consider the following matrix:This is particularly useful in finding closed form expressions for terms of linear recursive sequences, such as the Fibonacci numbers.and the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions that can be defined as power series.Diagonalization can be used to compute the powers of a matrix A efficiently, provided the matrix is diagonalizable. Suppose we have found thatThen P diagonalizes A, as a simple computation confirms, having calculated P \u22121 using any suitable method:Note that there is no preferred order of the eigenvectors in P; changing the order of the eigenvectors in P just changes the order of the eigenvalues in the diagonalized form of A.[3]Now, let P be the matrix with these eigenvectors as its columns:The eigenvectors of A areThese eigenvalues are the values that will appear in the diagonalized form of matrix A, so by finding the eigenvalues of A we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize A.A is a 3\u00d73 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly n distinct eigenvalues in an n\u00d7n matrix then this matrix is diagonalizable.This matrix has eigenvaluesConsider a matrixNote that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.then Q\u22121BQ is diagonal. It is easy to find that B is the rotation matrix which rotates counterclockwise by angle \u03b8=3\u03c0/2The matrix B does not have any real eigenvalues, so there is no real matrix Q such that Q\u22121BQ is a diagonal matrix. However, we can diagonalize B if we allow complex numbers. Indeed, if we takeSome real matrices are not diagonalizable over the reals. Consider for instance the matrixThis matrix is not diagonalizable: there is no matrix U such that U\u22121CU is a diagonal matrix. Indeed, C has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices. This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, considerIn general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to \"do the best one can\", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal - known as Jordan normal form.In the language of Lie theory, a set of simultaneously diagonalisable matrices generate a toral Lie algebra.A set consists of commuting normal matrices if and only if it is simultaneously diagonalisable by a unitary matrix; that is, there exists a unitary matrix U such that U*AU is diagonal for every A in the set.are diagonalizable but not simultaneously diagonalizable because they do not commute.The set of all n\u00d7n diagonalisable matrices (over C) with n > 1 is not simultaneously diagonalisable. For instance, the matricesA set of matrices is said to be simultaneously diagonalizable if there exists a single invertible matrix P such that P\u22121AP is a diagonal matrix for every A in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalisable.[2]In practice, matrices are diagonalized numerically using computers. Many algorithms exist to accomplish this.When the matrix A is a Hermitian matrix (resp. symmetric matrix), eigenvectors of A can be chosen to form an orthonormal basis of Cn (resp. Rn). Under such circumstance P will be a unitary matrix (resp. orthogonal matrix) and P\u22121 equals the conjugate transpose (resp. transpose) of P.So the column vectors of P are right eigenvectors of A, and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of P also suggests that the eigenvectors are linearly independent and form a basis of Fn. This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of P\u22121 are the left eigenvectors of A.the above equation can be rewritten asthen:If a matrix A can be diagonalized, that is,The Jordan\u2013Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each \"block\" is a one-by-one matrix.As a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex n\u00d7n matrices that are not diagonalizable over C, considered as a subset of Cn\u00d7n, has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology: the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface. From that follows also density in the usual (strong) topology given by a norm. The same is not true over R.The following sufficient (but not necessary) condition is often useful.Another characterization: A matrix or linear map is diagonalizable over the field F if and only if its minimal polynomial is a product of distinct linear factors over F. (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)The fundamental fact about diagonalizable maps and matrices is expressed by the following:Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle; once their eigenvalues and eigenvectors are known, one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power, and the determinant of a diagonal matrix is simply the product of all diagonal entries. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or anisotropic scaling)\u00a0\u2014 it scales the space, as does a homogeneous dilation, but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).In linear algebra, a square matrix A is called diagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix P such that P\u22121AP is a diagonal matrix. If V is a finite-dimensional vector space, then a linear map T\u00a0: V \u2192 V is called diagonalizable if there exists an ordered basis of V with respect to which T is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map.[1] A square matrix that is not diagonalizable is called defective.",
            "title": "Diagonalizable matrix",
            "url": "https://en.wikipedia.org/wiki/Diagonalizable_matrix"
        },
        {
            "desc_links": [
                "/wiki/Transformation_matrix#Eigenbasis_and_diagonal_matrix",
                "/wiki/Determinant",
                "/wiki/Inhomogeneous_dilation",
                "/wiki/Scaling_(geometry)",
                "/wiki/Homogeneous_dilation",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Basis_(linear_algebra)#Ordered_bases_and_coordinates",
                "/wiki/Diagonal_matrix",
                "/wiki/Defective_matrix"
            ],
            "links": [
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum_chemistry",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hilbert_space",
                "/wiki/Variational_principle",
                "/wiki/Linear_recursive_sequences",
                "/wiki/Fibonacci_number",
                "/wiki/Matrix_exponential",
                "/wiki/Matrix_function",
                "/wiki/Invertible_matrix#Methods_of_matrix_inversion",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvalues",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvalues",
                "/wiki/Nilpotent_matrix",
                "/wiki/Eigenvalue,_eigenvector_and_eigenspace#Algebraic_and_geometric_multiplicities",
                "/wiki/Rotation_matrix",
                "/wiki/Rotation_matrix#Independent_planes",
                "/wiki/Jordan_Normal_Form",
                "/wiki/Lie_theory",
                "/wiki/Toral_Lie_algebra",
                "/wiki/Normal_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Commuting_matrices",
                "/wiki/Eigenvalue_algorithm",
                "/wiki/Hermitian_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Orthonormal_basis",
                "/wiki/Unitary_matrix",
                "/wiki/Orthogonal_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Transpose",
                "/wiki/Right_eigenvector",
                "/wiki/Eigenvalue",
                "/wiki/Linearly_independent",
                "/wiki/Row_vector",
                "/wiki/Left_eigenvector",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Nilpotent",
                "/wiki/Jordan_form",
                "/wiki/Subset",
                "/wiki/Lebesgue_measure",
                "/wiki/Zariski_topology",
                "/wiki/Discriminant",
                "/wiki/Hypersurface",
                "/wiki/Norm_(mathematics)",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Elementary_divisor",
                "/wiki/Transformation_matrix#Eigenbasis_and_diagonal_matrix",
                "/wiki/Determinant",
                "/wiki/Inhomogeneous_dilation",
                "/wiki/Scaling_(geometry)",
                "/wiki/Homogeneous_dilation",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Basis_(linear_algebra)#Ordered_bases_and_coordinates",
                "/wiki/Diagonal_matrix",
                "/wiki/Defective_matrix"
            ],
            "text": "In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schr\u00f6dinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space). A very common approximation is to truncate Hilbert space to finite dimension, after which the Schr\u00f6dinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle, valid for Hamiltonians that are bounded from below. But also first-order perturbation theory for degenerate states leads to a matrix eigenvalue problem.thereby explaining the above phenomenon.The preceding relations, expressed in matrix form, areSwitching back to the standard basis, we haveThus, a and b are the eigenvalues corresponding to u and v, respectively. By linearity of matrix multiplication, we have thatStraightforward calculations show thatwhere ei denotes the standard basis of Rn. The reverse change of basis is given byThe above phenomenon can be explained by diagonalizing M. To accomplish this, we need a basis of R2 consisting of eigenvectors of M. One such eigenvector basis is given byCalculating the various powers of M reveals a surprising pattern:For example, consider the following matrix:This is particularly useful in finding closed form expressions for terms of linear recursive sequences, such as the Fibonacci numbers.and the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions that can be defined as power series.Diagonalization can be used to compute the powers of a matrix A efficiently, provided the matrix is diagonalizable. Suppose we have found thatThen P diagonalizes A, as a simple computation confirms, having calculated P \u22121 using any suitable method:Note that there is no preferred order of the eigenvectors in P; changing the order of the eigenvectors in P just changes the order of the eigenvalues in the diagonalized form of A.[3]Now, let P be the matrix with these eigenvectors as its columns:The eigenvectors of A areThese eigenvalues are the values that will appear in the diagonalized form of matrix A, so by finding the eigenvalues of A we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize A.A is a 3\u00d73 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly n distinct eigenvalues in an n\u00d7n matrix then this matrix is diagonalizable.This matrix has eigenvaluesConsider a matrixNote that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.then Q\u22121BQ is diagonal. It is easy to find that B is the rotation matrix which rotates counterclockwise by angle \u03b8=3\u03c0/2The matrix B does not have any real eigenvalues, so there is no real matrix Q such that Q\u22121BQ is a diagonal matrix. However, we can diagonalize B if we allow complex numbers. Indeed, if we takeSome real matrices are not diagonalizable over the reals. Consider for instance the matrixThis matrix is not diagonalizable: there is no matrix U such that U\u22121CU is a diagonal matrix. Indeed, C has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices. This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, considerIn general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to \"do the best one can\", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal - known as Jordan normal form.In the language of Lie theory, a set of simultaneously diagonalisable matrices generate a toral Lie algebra.A set consists of commuting normal matrices if and only if it is simultaneously diagonalisable by a unitary matrix; that is, there exists a unitary matrix U such that U*AU is diagonal for every A in the set.are diagonalizable but not simultaneously diagonalizable because they do not commute.The set of all n\u00d7n diagonalisable matrices (over C) with n > 1 is not simultaneously diagonalisable. For instance, the matricesA set of matrices is said to be simultaneously diagonalizable if there exists a single invertible matrix P such that P\u22121AP is a diagonal matrix for every A in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalisable.[2]In practice, matrices are diagonalized numerically using computers. Many algorithms exist to accomplish this.When the matrix A is a Hermitian matrix (resp. symmetric matrix), eigenvectors of A can be chosen to form an orthonormal basis of Cn (resp. Rn). Under such circumstance P will be a unitary matrix (resp. orthogonal matrix) and P\u22121 equals the conjugate transpose (resp. transpose) of P.So the column vectors of P are right eigenvectors of A, and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of P also suggests that the eigenvectors are linearly independent and form a basis of Fn. This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of P\u22121 are the left eigenvectors of A.the above equation can be rewritten asthen:If a matrix A can be diagonalized, that is,The Jordan\u2013Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each \"block\" is a one-by-one matrix.As a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex n\u00d7n matrices that are not diagonalizable over C, considered as a subset of Cn\u00d7n, has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology: the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface. From that follows also density in the usual (strong) topology given by a norm. The same is not true over R.The following sufficient (but not necessary) condition is often useful.Another characterization: A matrix or linear map is diagonalizable over the field F if and only if its minimal polynomial is a product of distinct linear factors over F. (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)The fundamental fact about diagonalizable maps and matrices is expressed by the following:Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle; once their eigenvalues and eigenvectors are known, one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power, and the determinant of a diagonal matrix is simply the product of all diagonal entries. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or anisotropic scaling)\u00a0\u2014 it scales the space, as does a homogeneous dilation, but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).In linear algebra, a square matrix A is called diagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix P such that P\u22121AP is a diagonal matrix. If V is a finite-dimensional vector space, then a linear map T\u00a0: V \u2192 V is called diagonalizable if there exists an ordered basis of V with respect to which T is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map.[1] A square matrix that is not diagonalizable is called defective.",
            "title": "Diagonalizable matrix",
            "url": "https://en.wikipedia.org/wiki/Diagonalizable_matrix"
        },
        {
            "desc_links": [
                "/wiki/General_linear_group",
                "/wiki/Conjugacy_class",
                "/wiki/Invertible_matrix",
                "/wiki/Linear_operator",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Change_of_basis",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)"
            ],
            "links": [
                "/wiki/Permutation_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Spectral_theorem",
                "/wiki/Normal_matrix",
                "/wiki/Specht%27s_theorem",
                "/wiki/Field_extension",
                "/wiki/If_and_only_if",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Complex_number",
                "/wiki/Algebraically_closed_field",
                "/wiki/Jordan_form",
                "/wiki/Rational_canonical_form",
                "/wiki/Smith_normal_form",
                "/wiki/Equivalence_relation",
                "/wiki/General_linear_group",
                "/wiki/Conjugacy_class",
                "/wiki/Invertible_matrix",
                "/wiki/Linear_operator",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Change_of_basis",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)"
            ],
            "text": "In the definition of similarity, if the matrix P can be chosen to be a permutation matrix then A and B are permutation-similar; if P can be chosen to be a unitary matrix then A and B are unitarily equivalent. The spectral theorem says that every normal matrix is unitarily equivalent to some diagonal matrix. Specht's theorem states that two matrices are unitarily equivalent if and only if they satisfy certain trace equalities.Similarity of matrices does not depend on the base field: if L is a field containing K as a subfield, and A and B are two matrices over K, then A and B are similar as matrices over K if and only if they are similar as matrices over L. This is so because the rational canonical form over K is also the rational canonical form over L. This means that one may use Jordan forms that only exist over a larger field to determine whether the given matrices are similar.Because of this, for a given matrix A, one is interested in finding a simple \"normal form\" B which is similar to A\u2014the study of A then reduces to the study of the simpler matrix B. For example, A is called diagonalizable if it is similar to a diagonal matrix. Not all matrices are diagonalizable, but at least over the complex numbers (or any algebraically closed field), every matrix is similar to a matrix in Jordan form. Neither of these forms is unique (diagonal entries or Jordan blocks may be permuted) so they are not really normal forms; moreover their determination depends on being able to factor the minimal or characteristic polynomial of A (equivalently to find its eigenvalues). The rational canonical form does not have these drawbacks: it exists over any field, is truly unique, and it can be computed using only arithmetic operations in the field; A and B are similar if and only if they have the same rational canonical form. The rational canonical form is determined by the elementary divisors of A; these can be immediately read off from a matrix in Jordan form, but they can also be determined directly for any matrix by computing the Smith normal form, over the ring of polynomials, of the matrix (with polynomial entries) XIn \u2212 A (the same one whose determinant defines the characteristic polynomial). Note that this Smith normal form is not a normal form of A itself; moreover it is not similar to XIn \u2212 A either, but obtained from the latter by left and right multiplications by different invertible matrices (with polynomial entries).Because matrices are similar if and only if they represent the same linear operator with respect to (possibly) different bases, similar matrices share all properties of their shared underlying operator:Similarity is an equivalence relation on the space of square matrices.A transformation A \u21a6 P\u22121AP is called a similarity transformation or conjugation of the matrix A. In the general linear group, similarity is therefore the same as conjugacy, and similar matrices are also called conjugate; however in a given subgroup H of the general linear group, the notion of conjugacy may be more restrictive than similarity, since it requires that P be chosen to lie in H.for some invertible n-by-n matrix P. Similar matrices represent the same linear operator under two (possibly) different bases, with P being the change of basis matrix.[1][2]In linear algebra, two n-by-n matrices A and B are called similar if",
            "title": "Matrix similarity",
            "url": "https://en.wikipedia.org/wiki/Matrix_similarity"
        },
        {
            "desc_links": [
                "/wiki/Set_(mathematics)",
                "/wiki/Operation_(mathematics)",
                "/wiki/Integers"
            ],
            "links": [
                "/wiki/If_and_only_if",
                "/wiki/Generating_set_of_a_group",
                "/wiki/Binary_relation",
                "/wiki/Predicate_(mathematical_logic)",
                "/wiki/Symmetric_closure",
                "/wiki/Symmetric_relation",
                "/wiki/Rewriting",
                "/wiki/Preorder",
                "/wiki/Equivalence_relation#Comparing_equivalence_relations",
                "/wiki/Term_algebra",
                "/wiki/Congruence_relation#Universal_algebra",
                "/wiki/Upper_set",
                "/wiki/Ordinal_number",
                "/wiki/Limit_point",
                "/wiki/Topological_space",
                "/wiki/First-countable_space",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Net_(mathematics)",
                "/wiki/Closed_set",
                "/wiki/Topology",
                "/wiki/Closed_interval",
                "/wiki/Subgroup",
                "/wiki/Unary_operation",
                "/wiki/Inverse_element",
                "/wiki/Group_(mathematics)",
                "/wiki/N-ary",
                "/wiki/Operation_(mathematics)",
                "/wiki/Integers",
                "/wiki/Topological_closure",
                "/wiki/Galois_connection",
                "/wiki/Monad_(category_theory)",
                "/wiki/Axiom"
            ],
            "text": "If X is contained in a set closed under the operation then every subset of X has a closure.These three properties define an abstract closure operator. Typically, an abstract closure acts on the class of all subsets of a set.An object that is its own closure is called closed. By idempotency, an object is closed if and only if it is the closure of some object.The closure of sets with respect to some operation defines a closure operator on the subsets of X. The closed sets can be determined from the closure operator; a set is closed if it is equal to its own closure. Typical structural properties of all closure operations are: [2]Given an operation on a set X, one can define the closure C(S) of a subset S of X to be the smallest subset closed under that operation that contains S as a subset, if any such subsets exist. Consequently, C(S) is the intersection of all closed sets containing S. For example, the closure of a subset of a group is the subgroup generated by that set.Any of these four closures preserves symmetry, i.e., if R is symmetric, so is any clxxx(R). [note 2] Similarly, all four preserve reflexivity. Moreover, cltrn preserves closure under clemb,\u03a3 for arbitrary \u03a3. As a consequence, the equivalence closure of an arbitrary binary relation R can be obtained as cltrn(clsym(clref(R))), and the congruence closure with respect to some \u03a3 can be obtained as cltrn(clemb,\u03a3(clsym(clref(R)))). In the latter case, the nesting order does matter; e.g. if S is the set of terms over \u03a3 = { a, b, c, f } and R = { \u27e8a,b\u27e9, \u27e8f(b),c\u27e9 }, then the pair \u27e8f(a),c\u27e9 is contained in the congruence closure cltrn(clemb,\u03a3(clsym(clref(R)))) of R, but not in the relation clemb,\u03a3(cltrn(clsym(clref(R)))).The relation R is said to have closure under some clxxx, if R = clxxx(R); for example R is called symmetric if R = clsym(R).Some important particular closures can be constructively obtained as follows:For arbitrary P and R, the P closure of R need not exist. In the above examples, these exist because reflexivity, transitivity and symmetry are closed under arbitrary intersections. In such cases, the P closure can be directly defined as the intersection of all sets with property P containing R.[1]The notion of a closure can be applied for an arbitrary binary relation R \u2286 S\u00d7S, and an arbitrary property P in the following way: the P closure of R is the least relation Q \u2286 S\u00d7S that contains R (i.e. R \u2286 Q) and for which property P holds (i.e. P(Q) is true). For instance, one can define the symmetric closure as the least symmetric relation containing R. This generalization is often encountered in the theory of rewriting systems, where one often uses more \"wordy\" notions such as the reflexive transitive closure R*\u2014the smallest preorder containing R, or the reflexive transitive symmetric closure R\u2261\u2014the smallest equivalence relation containing R, and therefore also known as the equivalence closure. When considering a particular term algebra, an equivalence relation that is compatible with all operations of the algebra [note 1] is called a congruence relation. The congruence closure of R is defined as the smallest congruence relation containing R.Upward closed and upper set are defined similarly.A partially ordered set is downward closed (and also called a lower set) if for every element of the set all smaller elements are also in it; this applies for example for the real intervals (\u2212\u221e,\u00a0p) and (\u2212\u221e,\u00a0p], and for an ordinal number p represented as interval [ 0,\u00a0p); every downward closed set of ordinal numbers is itself an ordinal number.An operation of a different sort is that of finding the limit points of a subset of a topological space (if the space is first-countable, it suffices to restrict consideration to the limits of sequences but in general one must consider at least limits of nets). A set that is closed under this operation is usually just referred to as a closed set in the context of topology. Without any further qualification, the phrase usually means closed in this sense. Closed intervals like [1,2] = {x\u00a0: 1 \u2264 x \u2264 2} are closed in this sense.Nevertheless, the closure property of an operator on a set still has some utility. Closure on a set does not necessarily imply closure on all subsets. Thus a subgroup of a group is a subset on which the binary product and the unary operation of inversion satisfy the closure axiom.A set is closed under an operation if that operation returns a member of the set when evaluated on members of the set. Sometimes the requirement that the operation be valued in a set is explicitly stated, in which case it is known as the axiom of closure. For example, one may define a group as a set with a binary product operator obeying several axioms, including an axiom that the product of any two elements of the group is again an element. However the modern definition of an operation makes this axiom superfluous; an n-ary operation on S is just a subset of Sn+1. By its very definition, an operator on a set cannot have values outside the set.The two uses of the word \"closure\" should not be confused. The former usage refers to the property of being closed, and the latter refers to the smallest closed set containing one that may not be closed. In short, the closure of a set satisfies a closure property.The set S must be a subset of a closed set in order for the closure operator to be defined. In the preceding example, it is important that the reals are closed under subtraction; in the domain of the natural numbers subtraction is not always defined.When a set S is not closed under some operations, one can usually find the smallest set containing S that is closed. This smallest closed set is called the closure of S (with respect to these operations). For example, the closure under subtraction of the set of natural numbers, viewed as a subset of the real numbers, is the set of integers. An important example is that of topological closure. The notion of closure is generalized by Galois connection, and further by monads.A set that is closed under an operation or collection of operations is said to satisfy a closure property. Often a closure property is introduced as an axiom, which is then usually called the axiom of closure. Modern set-theoretic definitions usually define operations as maps between sets, so adding closure to a structure as an axiom is superfluous; however in practice operations are often defined initially on a superset of the set in question and a closure proof is required to establish that the operation applied to pairs from that set only produces members of that set. For example, the set of even integers is closed under addition, but the set of odd integers is not.Similarly, a set is said to be closed under a collection of operations if it is closed under each of the operations individually.",
            "title": "Closure (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Closure_(mathematics)"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Distributive property",
            "url": "https://en.wikipedia.org/wiki/Distributive_property"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Binary_operation",
                "/wiki/Operand",
                "/wiki/Binary_operations",
                "/wiki/Mathematical_proof",
                "/wiki/Division_(mathematics)",
                "/wiki/Subtraction",
                "/wiki/Multiplication_(mathematics)",
                "/wiki/Addition",
                "/wiki/Binary_relation",
                "/wiki/Symmetric_relation",
                "/wiki/Equality_(mathematics)"
            ],
            "links": [
                "/wiki/Group_theory",
                "/wiki/Set_theory",
                "/wiki/Mathematical_analysis",
                "/wiki/Linear_algebra",
                "/wiki/Addition",
                "/wiki/Multiplication",
                "/wiki/Logical_connective",
                "/wiki/Propositional_logic",
                "/wiki/Logical_equivalence",
                "/wiki/Tautology_(logic)",
                "/wiki/Validity",
                "/wiki/Rule_of_replacement",
                "/wiki/Propositional_variable",
                "/wiki/Well-formed_formula",
                "/wiki/Formal_proof",
                "/wiki/Fran%C3%A7ois-Joseph_Servois",
                "/wiki/Duncan_Farquharson_Gregory",
                "/wiki/Royal_Society_of_Edinburgh",
                "/wiki/Egypt",
                "/wiki/Multiplication",
                "/wiki/Product_(mathematics)",
                "/wiki/Euclid",
                "/wiki/Euclid%27s_Elements",
                "/wiki/Cross_product",
                "/wiki/Anticommutativity",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Truth_function",
                "/wiki/Truth_table",
                "/wiki/Function_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Binary_operation",
                "/wiki/Operand",
                "/wiki/Binary_operations",
                "/wiki/Mathematical_proof",
                "/wiki/Division_(mathematics)",
                "/wiki/Subtraction",
                "/wiki/Multiplication_(mathematics)",
                "/wiki/Addition",
                "/wiki/Binary_relation",
                "/wiki/Symmetric_relation",
                "/wiki/Equality_(mathematics)"
            ],
            "text": "Some forms of symmetry can be directly linked to commutativity. When a commutative operator is written as a binary function then the resulting function is symmetric across the line y = x. As an example, if we let a function f represent addition (a commutative operation) so that f(x,y) = x + y then f is a symmetric function, which can be seen in the image on the right.Most commutative operations encountered in practice are also associative. However, commutativity does not imply associativity. A counterexample is the functionThe associative property is closely related to the commutative property. The associative property of an expression containing two or more occurrences of the same operator states that the order operations are performed in does not affect the final result, as long as the order of terms doesn't change. In contrast, the commutative property states that the order of the terms does not affect the final result.In group and set theory, many algebraic structures are called commutative when certain operands satisfy the commutative property. In higher branches of mathematics, such as analysis and linear algebra the commutativity of well-known operations (such as addition and multiplication on real and complex numbers) is often used (or implicitly assumed) in proofs.[16][17][18]Commutativity is a property of some logical connectives of truth functional propositional logic. The following logical equivalences demonstrate that commutativity is a property of particular connectives. The following are truth-functional tautologies.andIn truth-functional propositional logic, commutation,[13][14] or commutativity[15] refer to two valid rules of replacement. The rules allow one to transpose propositional variables within logical expressions in logical proofs. The rules are:The first recorded use of the term commutative was in a memoir by Fran\u00e7ois Servois in 1814,[1][11] which used the word commutatives when describing functions that have what is now called the commutative property. The word is a combination of the French word commuter meaning \"to substitute or switch\" and the suffix -ative meaning \"tending to\" so the word literally means \"tending to substitute or switch.\" The term then appeared in English in 1838[2] in Duncan Farquharson Gregory's article entitled \"On the real nature of symbolical algebra\" published in 1840 in the Transactions of the Royal Society of Edinburgh.[12]Records of the implicit use of the commutative property go back to ancient times. The Egyptians used the commutative property of multiplication to simplify computing products.[8][9] Euclid is known to have assumed the commutative property of multiplication in his book Elements.[10] Formal uses of the commutative property arose in the late 18th and early 19th centuries, when mathematicians began to work on a theory of functions. Today the commutative property is a well-known and basic property used in most branches of mathematics.The vector product (or cross product) of two vectors in three dimensions is anti-commutative; i.e., b \u00d7 a = \u2212(a \u00d7 b).Matrix multiplication is almost always noncommutative, for example:For the eight noncommutative functions, Bqp = Cpq; Mqp = Lpq; Cqp = Bpq; Lqp = Mpq; Fqp = Gpq; Iqp = Hpq; Gqp = Fpq; Hqp = Ipq.[7]Some truth functions are noncommutative, since the truth tables for the functions are different when one changes the order of the operands. For example, the truth tables for f (A, B) = A \u039b \u00acB (A AND NOT B) and f (B, A) = B \u039b \u00acA areSome noncommutative binary operations:[6]Two well-known examples of commutative binary operations:[4]The term \"commutative\" is used in several related senses.[4][5]The commutative property (or commutative law) is a property generally associated with binary operations and functions. If the commutative property holds for a pair of elements under a certain binary operation then the two elements are said to commute under that operation.In mathematics, a binary operation is commutative if changing the order of the operands does not change the result. It is a fundamental property of many binary operations, and many mathematical proofs depend on it. Most familiar as the name of the property that says \"3 + 4 = 4 + 3\" or \"2 \u00d7 5 = 5 \u00d7 2\", the property can also be used in more advanced settings. The name is needed because there are operations, such as division and subtraction, that do not have it (for example, \"3 \u2212 5 \u2260 5 \u2212 3\"); such operations are not commutative, and so are referred to as noncommutative operations. The idea that simple operations such as the multiplication and addition of numbers are commutative, was for many years implicitly assumed. Thus, this property was not named until the 19th century, when mathematics started to become formalized.[1][2] A corresponding property exists for binary relations; a binary relation is said to be symmetric if the relation applies regardless of the order of its operands; for example, equality is symmetric as two equal mathematical objects are equal regardless of their order.[3]",
            "title": "Commutative property",
            "url": "https://en.wikipedia.org/wiki/Commutative_property"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Functional_analysis",
                "/wiki/Linear_map",
                "/wiki/Vector_space",
                "/wiki/Zero_vector",
                "/wiki/Set-builder_notation"
            ],
            "links": [
                "/wiki/Lapack",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Floating-point_number",
                "/wiki/Rounding_error",
                "/wiki/Full_rank",
                "/wiki/Well-conditioned_problem",
                "/wiki/Condition_number",
                "/wiki/Cryptography",
                "/wiki/Gr%C3%B6bner_basis",
                "/wiki/Analysis_of_algorithms",
                "/wiki/Computer_hardware",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Column_echelon_form",
                "/wiki/Bareiss_algorithm",
                "/wiki/Modular_arithmetic",
                "/wiki/Finite_field",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Gaussian_elimination",
                "/wiki/Line_(geometry)",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Free_variable",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Gauss%E2%80%93Jordan_elimination",
                "/wiki/System_of_linear_equations",
                "/wiki/Real_coordinate_space",
                "/wiki/Translation_(geometry)",
                "/wiki/Fredholm_alternative",
                "/wiki/Flat_(geometry)",
                "/wiki/Transpose",
                "/wiki/Column_space",
                "/wiki/Cokernel",
                "/wiki/Fundamental_theorem_of_linear_algebra",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Row_space",
                "/wiki/Linear_span",
                "/wiki/Orthogonal_complement",
                "/wiki/Orthogonality",
                "/wiki/Dot_product",
                "/wiki/Linear_subspace",
                "/wiki/System_of_linear_equations",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Zero_vector",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Set-builder_notation",
                "/wiki/Topological_vector_space",
                "/wiki/Continuous_linear_operator",
                "/wiki/Closed_set",
                "/wiki/Homomorphism",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Submodule",
                "/wiki/Inner_product_space",
                "/wiki/Orthogonal_complement",
                "/wiki/Row_space",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Isomorphism",
                "/wiki/Quotient_space_(linear_algebra)",
                "/wiki/Linear_subspace",
                "/wiki/Domain_of_a_function",
                "/wiki/Image_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Functional_analysis",
                "/wiki/Linear_map",
                "/wiki/Vector_space",
                "/wiki/Zero_vector",
                "/wiki/Set-builder_notation"
            ],
            "text": "Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gr\u00f6bner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic, which reduces the problem to a similar one over a finite field.[citation needed]The problem of computing the kernel on a computer depends on the nature of the coefficients.are a basis of the kernel of A.The last three columns of B are zero columns. Therefore, the three last vectors of C,Putting the upper part in column echelon form by column operations on the whole matrix givesThenFor example, suppose thatIn fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.A basis of the kernel of a matrix may be computed by Gaussian elimination.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (\u22121,\u221226,16)T.which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.Note also that the following dot products are zero:The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (\u22121,\u221226,16)T constitutes a basis of the kernel of A. Thus, the nullity of A is 1.Since c is a free variable, this can be expressed equally well as,for c a scalar.Now we can express an element of the kernel:Rewriting yields:Gauss\u2013Jordan elimination reduces this to:which can be written in matrix form as:which can be expressed as a homogeneous system of linear equations involving x, y, and z:The kernel of this matrix consists of all vectors (x, y, z)\u00a0\u2208\u00a0R3 for whichConsider the matrixWe give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Geometrically, this says that the solution set to Ax\u00a0=\u00a0b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).It follows that any solution to the equation Ax\u00a0=\u00a0b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation Ax\u00a0=\u00a0b isThus, the difference of any two solutions to the equation Ax\u00a0=\u00a0b lies in the kernel of A.If u and v are two possible solutions to the above equation, thenThe kernel also plays a role in the solution to a nonhomogeneous system of linear equations:The left null space, or cokernel, of a matrix A consists of all vectors x such that xTA\u00a0=\u00a00T, where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of AT. The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank\u2013nullity theoremThe row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The product Ax can be written in terms of the dot product of vectors as follows:The kernel of an m \u00d7 n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The matrix equation is equivalent to a homogeneous system of linear equations:Consider a linear map represented as a m \u00d7 n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K. The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L:\u00a0V\u00a0\u2192\u00a0W is continuous if and only if the kernel of L is a closed subspace of V.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring. The domain of the mapping is a module, and the kernel constitutes a \"submodule\". Here, the concepts of rank and nullity do not necessarily apply.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L). This is the generalization to linear operators of the row space, or coimage, of a matrix.where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.This implies the rank\u2013nullity theorem:It follows that the image of L is isomorphic to the quotient of V by the kernel:The kernel of L is a linear subspace of the domain V.[1] In the linear map L\u00a0: V \u2192 W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L\u00a0: V \u2192 W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W. That is, in set-builder notation,",
            "title": "Kernel (linear algebra)",
            "url": "https://en.wikipedia.org/wiki/Kernel_(linear_algebra)"
        },
        {
            "desc_links": [
                "/wiki/List_of_mathematical_symbols",
                "/wiki/Set_theory",
                "/wiki/Element_(set_theory)"
            ],
            "links": [
                "/wiki/Class_(set_theory)",
                "/wiki/If_and_only_if",
                "/wiki/Existential_quantification",
                "/wiki/Finite_set",
                "/wiki/Universe_(mathematics)",
                "/wiki/Complement_(set_theory)",
                "/wiki/Boolean_algebra_(structure)",
                "/wiki/Intersection_(set_theory)",
                "/wiki/Empty_set",
                "/wiki/Identity_element",
                "/wiki/Logical_disjunction",
                "/wiki/Commutative",
                "/wiki/Associative",
                "/wiki/Cardinality",
                "/wiki/Prime_number",
                "/wiki/Even_number",
                "/wiki/List_of_mathematical_symbols",
                "/wiki/Set_theory",
                "/wiki/Element_(set_theory)"
            ],
            "text": "Whenever the symbol \"\u222a\" is placed before other symbols instead of between them, it is of a larger size.This idea subsumes the preceding sections\u2014for example, A \u222a B \u222a C is the union of the collection {A, B, C}. Also, if M is the empty collection, then the union of M is the empty set.The most general notion is the union of an arbitrary collection of sets, sometimes called an infinitary union. If M is a set or class whose elements are sets, then x is an element of the union of M if and only if there is at least one element A of M such that x is an element of A.[7] In symbols:In mathematics a finite union means any union carried out on a finite number of sets; it does not imply that the union set is a finite set.[5][6]One can take the union of several sets simultaneously. For example, the union of three sets A, B, and C contains all elements of A, all elements of B, and all elements of C, and nothing else. Thus, x is an element of A \u222a B \u222a C if and only if x is in at least one of A, B, and C.where the superscript C denotes the complement with respect to the universal set.Within a given universal set, union can be written in terms of the operations of intersection and complement asand union distributes over intersectionSince sets with unions and intersections form a Boolean algebra, intersection distributes over unionThe empty set is an identity element for the operation of union. That is, A \u222a \u2205 = A, for any set A. This follows from analogous facts about logical disjunction.The operations can be performed in any order, and the parentheses may be omitted without ambiguity (i.e., either of the above can be expressed equivalently as A \u222a B \u222a C). Similarly, union is commutative, so the sets can be written in any order.[4]Binary union is an associative operation; that is, for any sets A, B, and C,Sets cannot have duplicate elements,[2][3] so the union of the sets {1, 2, 3} and {2, 3, 4} is {1, 2, 3, 4}. Multiple occurrences of identical elements have no effect on the cardinality of a set or its contents.As another example, the number 9 is not contained in the union of the set of prime numbers {2, 3, 5, 7, 11, \u2026} and the set of even numbers {2, 4, 6, 8, 10, \u2026}, because 9 is neither prime nor even.For example, if A = {1, 3, 5, 7} and B = {1, 2, 4, 6} then A \u222a B = {1, 2, 3, 4, 5, 6, 7}. A more elaborate example (involving two infinite sets) is:The union of two sets A and B is the set of elements which are in A, in B, or in both A and B. In symbols,For explanation of the symbols used in this article, refer to the table of mathematical symbols.In set theory, the union (denoted by \u222a) of a collection of sets is the set of all elements in the collection.[1] It is one of the fundamental operations through which sets can be combined and related to each other.",
            "title": "Union (set theory)",
            "url": "https://en.wikipedia.org/wiki/Union_(set_theory)"
        },
        {
            "desc_links": [
                "/wiki/Linear_algebra",
                "/wiki/Mathematics",
                "/wiki/Vector_space",
                "/wiki/Subset",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Subspace_(disambiguation)"
            ],
            "links": [
                "/wiki/Null_space",
                "/wiki/Kernel_(matrix)#Basis",
                "/wiki/Column_space",
                "/wiki/Column_space#Basis",
                "/wiki/Row_space",
                "/wiki/Row_and_column_spaces#Basis_2",
                "/wiki/Row_reduction",
                "/wiki/Elementary_row_operation",
                "/wiki/Row_echelon_form",
                "/wiki/Reduced_row_echelon_form",
                "/wiki/Pseudo-Euclidean_space",
                "/wiki/Heyting_algebra",
                "/wiki/Null_vector",
                "/wiki/Symplectic_vector_space",
                "/wiki/Inner_product_space",
                "/wiki/Orthogonal_complement",
                "/wiki/Negation",
                "/wiki/Infinite_set",
                "/wiki/Linear_subspace#Intersection",
                "/wiki/Linear_subspace#Sum",
                "/wiki/Modular_lattice",
                "/wiki/Zero_vector_space",
                "/wiki/Least_element",
                "/wiki/Identity_element",
                "/wiki/Identity_element",
                "/wiki/Zero_vector_space",
                "/wiki/Intersection_(set_theory)",
                "/wiki/Inclusion_relation",
                "/wiki/Partial_order",
                "/wiki/Row_space",
                "/wiki/Orthogonal_complement",
                "/wiki/Column_space",
                "/wiki/Image_(mathematics)",
                "/wiki/Linear_combination",
                "/wiki/Number_field",
                "/wiki/Parametric_equations",
                "/wiki/Null_space",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Null_set",
                "/wiki/Rational_number",
                "/wiki/System_of_linear_equations",
                "/wiki/Coordinate_space",
                "/wiki/System_of_equations",
                "/wiki/Duality_(mathematics)",
                "/wiki/Linear_functional",
                "/wiki/Linear_equation",
                "/wiki/Additive_identity",
                "/wiki/Kernel_(linear_algebra)",
                "/wiki/Codimension",
                "/wiki/Dual_space",
                "/wiki/Linear_span",
                "/wiki/Equality_(mathematics)",
                "/wiki/Scalar_multiplication",
                "/wiki/Additive_identity",
                "/wiki/System_of_linear_equations",
                "/wiki/Parametric_equations",
                "/wiki/Linear_span",
                "/wiki/Null_space",
                "/wiki/Column_space",
                "/wiki/Row_space",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Flat_(geometry)",
                "/wiki/Topological_vector_space",
                "/wiki/Closed_set",
                "/wiki/Finite-dimensional",
                "/wiki/Codimension",
                "/wiki/Linear_functional",
                "/wiki/Closure_(mathematics)",
                "/wiki/Linear_combination",
                "/wiki/If_and_only_if",
                "/wiki/Finite_set",
                "/wiki/Functional_analysis",
                "/wiki/Differentiable_function",
                "/wiki/Function_(mathematics)",
                "/wiki/Continuous_function",
                "/wiki/Real_coordinate_space",
                "/wiki/Linear_equation",
                "/wiki/Cartesian_plane",
                "/wiki/Real_number",
                "/wiki/Real_coordinate_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Vector_space",
                "/wiki/Linear_algebra",
                "/wiki/Mathematics",
                "/wiki/Vector_space",
                "/wiki/Subset",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Subspace_(disambiguation)"
            ],
            "text": "See the article on null space for an example.If the final column of the reduced row echelon form contains a pivot, then the input vector v does not lie in S.This produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.See the article on column space for an example.If we instead put the matrix A into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of Kn are equal.See the article on row space for an example.Most algorithms for dealing with subspaces involve row reduction. This is the process of applying elementary row operations to a matrix until it reaches either row echelon form or reduced row echelon form. Row reduction has the following important properties:In a pseudo-Euclidean space there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a Heyting algebra) because of null subspaces, for which N \u2229 N\u22a5 = N \u2260 {0}. The same case presents the \u22a5 operation in symplectic vector spaces.If V is an inner product space, then the orthogonal complement \u22a5 of any subspace of V is again a subspace. This operation, understood as negation (\u00ac), makes the lattice of subspaces a (possibly infinite) orthocomplemented lattice (it is not a distributive lattice).The operations intersection and sum make the set of all subspaces a bounded modular lattice, where the {0} subspace, the least element, is an identity element of the sum operation, and the identical subspace V, the greatest element, is an identity element of the intersection operation.Here the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:For example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequalityIf U and W are subspaces, their sum is the subspaceFor every vector space V, the set {0} and V itself are subspaces of V.[7]Proof:Given subspaces U and W of a vector space V, then their intersection U\u00a0\u2229\u00a0W\u00a0:= {v\u00a0\u2208\u00a0V\u00a0: v\u00a0is an element of both U and\u00a0W} is also a subspace of V.[6]A subspace cannot lie in any subspace of lesser dimension. If dim\u00a0U\u00a0=\u00a0k, a finite number, and U\u00a0\u2282\u00a0W, then dim\u00a0W\u00a0=\u00a0k if and only if U\u00a0=\u00a0W.The set-theoretical inclusion binary relation specifies a partial order on the set of all subspaces (of any dimension).A basis for a subspace S is a set of linearly independent vectors whose span is S. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see algorithms, below).for (t1,\u202ft2,\u202f...\u202f,\u202ftk)\u00a0\u2260 (u1,\u202fu2,\u202f...\u202f,\u202fuk).[5] If v1, ..., vk are linearly independent, then the coordinates t1, ..., tk for a vector in the span are uniquely determined.In general, vectors v1,\u202f...\u202f,\u202fvk are called linearly independent ifIn general, a subspace of Kn determined by k parameters (or spanned by k vectors) has dimension k. However, there are exceptions to this rule. For example, the subspace of K3 spanned by the three vectors (1,\u202f0,\u202f0), (0,\u202f0,\u202f1), and (2,\u202f0,\u202f3) is just the xz-plane, with each point on the plane described by infinitely many different values of t1, t2, t3.The row space of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the orthogonal complement of the null space (see below).In this case, the subspace consists of all possible values of the vector x. In linear algebra, this subspace is known as the column space (or image) of the matrix A. It is precisely the subspace of Kn spanned by the column vectors of A.A system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:If the vectors v1,\u202f...\u202f,\u202fvk have n components, then their span is a subspace of Kn. Geometrically, the span is the flat through the origin in n-dimensional space determined by the points v1,\u202f...\u202f,\u202fvk.The set of all possible linear combinations is called the span:In general, a linear combination of vectors v1,\u202fv2,\u202f...\u202f,\u202fvk is any vector of the formThe expression on the right is called a linear combination of the vectors (2,\u202f5,\u202f\u22121) and (3,\u202f\u22124,\u202f2). These two vectors are said to span the resulting subspace.In linear algebra, the system of parametric equations can be written as a single vector equation:is a two-dimensional subspace of K3, if K is a number field (such as real or rational numbers).[4]For example, the set of all vectors (x,\u202fy,\u202fz) parameterized by the equationsThe subset of Kn described by a system of homogeneous linear parametric equations is a subspace:Every subspace of Kn can be described as the null space of some matrix (see algorithms, below).The set of solutions to this equation is known as the null space of the matrix. For example, the subspace described above is the null space of the matrixIn a finite-dimensional space, a homogeneous system of linear equations can be written as a single matrix equation:is a one-dimensional subspace. More generally, that is to say that given a set of n independent functions, the dimension of the subspace in Kk will be the dimension of the null set of A, the composite matrix of the n functions.For example (over real or rational numbers), the set of all vectors (x,\u202fy,\u202fz) satisfying the equationsThe solution set to any homogeneous system of linear equations with n variables is a subspace in the coordinate space Kn:It is generalized for higher codimensions with a system of equations. The following two subsections will present this latter description in details, and the remaining four subsections further describe the idea of linear span.A dual description is provided with linear functionals (usually implemented as linear equations). One non-zero linear functional F specifies its kernel subspace F\u00a0=\u00a00 of codimension 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the dual space):This idea is generalized for higher dimensions with linear span, but criteria for equality of k-spaces specified by sets of k vectors are not so simple.A natural description of an 1-subspace is the scalar multiplication of one non-zero vector v to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:Descriptions of subspaces include the solution set to a homogeneous system of linear equations, the subset of Euclidean space described by a system of homogeneous linear parametric equations, the span of a collection of vectors, and the null space, column space, and row space of a matrix. Geometrically (especially, over the field of real numbers and its subfields), a subspace is a flat in an n-space that passes through the origin.In a topological vector space X, a subspace W need not be closed in general, but a finite-dimensional subspace is always closed.[3] The same is true for subspaces of finite codimension, i.e. determined by a finite number of continuous linear functionals.A way to characterize subspaces is that they are closed under linear combinations. That is, a nonempty set W is a subspace if and only if every linear combination of (finitely many) elements of W also belongs to W. Conditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.Examples that extend these themes are common in functional analysis.Example IV: Keep the same field and vector space as before, but now consider the set Diff(R) of all differentiable functions. The same sort of argument as before shows that this is a subspace too.Proof:Example III: Again take the field to be R, but now let the vector space V be the set RR of all functions from R to R. Let C(R) be the subset consisting of continuous functions. Then C(R) is a subspace of RR.In general, any subset of the real coordinate space Rn that is defined by a system of homogeneous linear equations will yield a subspace. (The equation in example I was z\u00a0=\u00a00, and the equation in example II was x\u00a0=\u00a0y.) Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.Proof:Example II: Let the field be R again, but now let the vector space be the Cartesian plane R2. Take W to be the set of points (x,\u202fy) of R2 such that x\u00a0=\u00a0y. Then W is a subspace of R2.Proof:Example I: Let the field K be the set R of real numbers, and let the vector space V be the real coordinate space R3. Take W to be the set of all vectors in V whose last component is 0. Then W is a subspace of V.Let K be a field (such as the real numbers), V be a vector space over K, and let W be a subset of V. Then W is a subspace if:In linear algebra and related fields of mathematics, a linear subspace, also known as a vector subspace, or, in the older literature, a linear manifold,[1][2] is a vector space that is a subset of some other (higher-dimension) vector space. A linear subspace is usually called simply a subspace when the context serves to distinguish it from other kinds of subspaces[disambiguation needed].",
            "title": "Linear subspace",
            "url": "https://en.wikipedia.org/wiki/Linear_subspace"
        },
        {
            "desc_links": [
                "/wiki/Bernard_Bolzano",
                "/wiki/The_Paradoxes_of_the_Infinite",
                "/wiki/Mathematics",
                "/wiki/Mathematical_object",
                "/wiki/Set_theory",
                "/wiki/Mathematics_education",
                "/wiki/Venn_diagram"
            ],
            "links": [
                "/wiki/Augustus_De_Morgan",
                "/wiki/De_Morgan%27s_laws",
                "/wiki/Naive_set_theory",
                "/wiki/First-order_logic",
                "/wiki/Axiomatic_set_theory",
                "/wiki/Naive_set_theory",
                "/wiki/Well-defined",
                "/wiki/Category:Paradoxes_of_naive_set_theory",
                "/wiki/Relation_(mathematics)",
                "/wiki/Domain_(mathematics)",
                "/wiki/Codomain",
                "/wiki/Algebraic_structure",
                "/wiki/Abstract_algebra",
                "/wiki/Group_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Closure_(mathematics)",
                "/wiki/Set_(mathematics)#Cardinality",
                "/wiki/Ordered_pairs",
                "/wiki/Boolean_ring",
                "/wiki/Symmetric_difference",
                "/wiki/Universe_(mathematics)",
                "/wiki/Number_theory",
                "/wiki/Blackboard_bold",
                "/wiki/Empty_set",
                "/wiki/Unit_set",
                "/wiki/Infinite_set",
                "/wiki/Natural_number",
                "/wiki/Real_number",
                "/wiki/Straight_line",
                "/wiki/Line_segment",
                "/wiki/Plane_(mathematics)",
                "/wiki/Dimension_(mathematics)",
                "/wiki/Euclidean_space",
                "/wiki/Empty_set",
                "/wiki/Empty_set",
                "/wiki/0_(number)",
                "/wiki/Axiomatic_set_theory",
                "/wiki/Countable",
                "/wiki/Uncountable",
                "/wiki/Surjection",
                "/wiki/Partition_of_a_set",
                "/wiki/Universal_set",
                "/wiki/Empty_set",
                "/wiki/Relation_(mathematics)",
                "/wiki/Colon_(punctuation)",
                "/wiki/Vertical_bar",
                "/wiki/Set-builder_notation",
                "/wiki/Square_number",
                "/wiki/Ellipsis#In_mathematical_notation",
                "/wiki/Even_number",
                "/wiki/Sequence",
                "/wiki/Tuple",
                "/wiki/Extension_(semantics)",
                "/wiki/Extensional_and_intensional_definitions",
                "/wiki/Curly_bracket",
                "/wiki/Intensional_definition",
                "/wiki/Axiomatic_set_theory",
                "/wiki/Primitive_notion",
                "/wiki/Axiom",
                "/wiki/Capital_letters",
                "/wiki/If_and_only_if",
                "/wiki/Element_(mathematics)",
                "/wiki/Georg_Cantor",
                "/wiki/Bernard_Bolzano",
                "/wiki/The_Paradoxes_of_the_Infinite",
                "/wiki/Mathematics",
                "/wiki/Mathematical_object",
                "/wiki/Set_theory",
                "/wiki/Mathematics_education",
                "/wiki/Venn_diagram"
            ],
            "text": "The complement of A intersected with B is equal to the complement of A union to the complement of B.The complement of A union B equals the complement of A intersected with the complement of B.If A and B are any two sets then,Augustus De Morgan stated two laws about sets.A more general form of the principle can be used to find the cardinality of any finite union of sets:The inclusion\u2013exclusion principle is a counting technique that can be used to count the number of elements in a union of two sets, if the size of each set and the size of their intersection are known. It can be expressed symbolically asFor most purposes, however, naive set theory is still useful.The reason is that the phrase well-defined is not very well-defined. It was important to free set theory of these paradoxes because nearly all of mathematics was being redefined in terms of set theory. In an attempt to avoid these paradoxes, set theory was axiomatized based on first-order logic, and thus axiomatic set theory was born.Although initially naive set theory, which defines a set merely as any well-defined collection, was well accepted, it soon ran into several obstacles. It was found that this definition spawned several paradoxes, most notably:One of the main applications of naive set theory is constructing relations. A relation from a domain A to a codomain B is a subset of the Cartesian product A \u00d7 B. Given this concept, we are quick to see that the set F of all ordered pairs (x, x2), where x is real, is quite familiar. It has a domain set R and a codomain set that is also R, because the set of all squares is subset of the set of all reals. If placed in functional notation, this relation becomes f(x) = x2. The reason these two are equivalent is for any given value, y that the function is defined for, its corresponding ordered pair, (y, y2) is a member of the set F.Set theory is seen as the foundation from which virtually all of mathematics can be derived. For example, structures in abstract algebra, such as groups, fields and rings, are sets closed under one or more operations.Let A and B be finite sets; then the cardinality of the Cartesian product is the product of the cardinalities:Some basic properties of Cartesian products:Examples:A new set can be constructed by associating every element of one set with every element of another set. The Cartesian product of two sets A and B, denoted by A \u00d7 B is the set of all ordered pairs (a, b) such that a is a member of A and b is a member of B.For example, the symmetric difference of {7,8,9,10} and {9,10,11,12} is the set {7,8,11,12}. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring (with the empty set as neutral element) and intersection as the multiplication of the ring.An extension of the complement is the symmetric difference, defined for sets A, B asSome basic properties of complements:Examples:In certain settings all sets under discussion are considered to be subsets of a given universal set U. In such cases, U \\ A is called the absolute complement or simply complement of A, and is denoted by A\u2032.Two sets can also be \"subtracted\". The relative complement of B in A (also called the set-theoretic difference of A and B), denoted by A \\ B (or A \u2212 B), is the set of all elements that are members of A but not members of B. Note that it is valid to \"subtract\" members of a set that are not in the set, such as removing the element green from the set {1, 2, 3}; doing so has no effect.Some basic properties of intersections:Examples:A new set can also be constructed by determining which members two sets have \"in common\". The intersection of A and B, denoted by A \u2229 B, is the set of all things that are members of both A and B. If A \u2229 B = \u2205, then A and B are said to be disjoint.Some basic properties of unions:Examples:Two sets can be \"added\" together. The union of A and B, denoted by A\u00a0\u222a\u00a0B, is the set of all things that are members of either A or B.There are several fundamental operations for constructing new sets from given sets.Each of the above sets of numbers has an infinite number of elements, and each can be considered to be a proper subset of the sets listed below it. The primes are used less frequently than the others outside of number theory and related fields.Positive and negative sets are denoted by a superscript - or +. For example, \u211a+ represents the set of positive rational numbers.Many of these sets are represented using blackboard bold or bold typeface. Special sets of numbers includeThere are some sets or kinds of sets that hold great mathematical importance and are referred to with such regularity that they have acquired special names and notational conventions to identify them. One of these is the empty set, denoted {} or \u2205. A set with exactly one element, x, is a unit set, or singleton, {x}.[2]Some sets have infinite cardinality. The set N of natural numbers, for instance, is infinite. Some infinite cardinalities are greater than others. For instance, the set of real numbers has greater cardinality than the set of natural numbers. However, it can be shown that the cardinality of (which is to say, the number of points on) a straight line is the same as the cardinality of any segment of that line, of the entire plane, and indeed of any finite-dimensional Euclidean space.There is a unique set with no members, called the empty set (or the null set), which is denoted by the symbol \u2205 (other notations are used; see empty set). The cardinality of the empty set is zero. For example, the set of all three-sided squares has zero members and thus is the empty set. Though it may seem trivial, the empty set, like the number zero, is important in mathematics. Indeed, the existence of this set is one of the fundamental concepts of axiomatic set theory.The cardinality |\u2009S\u2009| of a set S is \"the number of members of S.\" For example, if B = {blue, white, red}, then |\u2009B\u2009| = 3.Every partition of a set S is a subset of the powerset of S.The power set of an infinite (either countable or uncountable) set is always uncountable. Moreover, the power set of a set is always strictly \"bigger\" than the original set in the sense that there is no way to pair every element of S with exactly one element of P(S). (There is never an onto map or surjection from S onto P(S).)The power set of a finite set with n elements has 2n elements. For example, the set {1, 2, 3} contains three elements, and the power set shown above contains 23 = 8 elements.The power set of a set S is the set of all subsets of S. The power set contains S itself and the empty set because these are both subsets of S. For example, the power set of the set {1, 2, 3} is {{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}, {2}, {3}, \u2205}. The power set of a set S is usually written as P(S).A partition of a set S is a set of nonempty subsets of S such that every element x in S is in exactly one of these subsets.An obvious but useful identity, which can often be used to show that two seemingly different sets are equal:Every set is a subset of the universal set:The empty set is a subset of every set and every set is a subset of itself:Examples:The expressions A \u2282 B and B \u2283 A are used differently by different authors; some authors use them to mean the same as A \u2286 B (respectively B \u2287 A), whereas others use them to mean the same as A \u228a B (respectively B \u228b A).If A is a subset of, but not equal to, B, then A is called a proper subset of B, written A \u228a B (A is a proper subset of B) or B \u228b A (B is a proper superset of A).If every member of set A is also a member of set B, then A is said to be a subset of B, written A \u2286 B (also pronounced A is contained in B). Equivalently, we can write B \u2287 A, read as B is a superset of A, B includes A, or B contains A. The relationship between sets established by \u2286 is called inclusion or containment.For example, with respect to the sets A = {1,2,3,4}, B = {blue, white, red}, and F = {n2 \u2212 4\u00a0: n is an integer; and 0 \u2264 n \u2264 19} defined above,If B is a set and x is one of the objects of B, this is denoted x \u2208 B, and is read as \"x belongs to B\", or \"x is an element of B\". If y is not a member of B then this is written as y \u2209 B, and is read as \"y does not belong to B\".In this notation, the colon (\":\") means \"such that\", and the description can be interpreted as \"F is the set of all numbers of the form n2 \u2212 4, such that n is a whole number in the range from 0 to 19 inclusive.\" Sometimes the vertical bar (\"|\") is used instead of the colon.The notation with braces may also be used in an intensional specification of a set. In this usage, the braces have the meaning \"the set of all ...\". So, E = {playing card suits} is the set whose four members are \u2660, \u2666, \u2665, and \u2663. A more general form of this is set-builder notation, through which, for instance, the set F of the twenty smallest integers that are four less than perfect square can be denotedwhere the ellipsis (\"...\") indicates that the list continues in the obvious way. Ellipses may also be used where sets have infinitely many members. Thus the set of positive even numbers can be written as {2, 4, 6, 8, ... }.For sets with many elements, the enumeration of members can be abbreviated. For instance, the set of the first thousand positive integers may be specified extensionally asIn an extensional definition, a set member can be listed two or more times, for example, {11, 6, 6}. However, per extensionality, two definitions of sets which differ only in that one of the definitions lists set members multiple times, define, in fact, the same set. Hence, the set {11, 6, 6} is exactly identical to the set {11, 6}. Moreover, the order in which the elements of a set are listed is irrelevant (unlike for a sequence or tuple). We can illustrate these two important points with an example:One often has the choice of specifying a set either intensionally or extensionally. In the examples above, for instance, A = C and B = D.The second way is by extension \u2013 that is, listing each member of the set. An extensional definition is denoted by enclosing the list of members in curly brackets:There are two ways of describing, or specifying the members of, a set. One way is by intensional definition, using a rule or semantic description:For technical reasons, Cantor's definition turned out to be inadequate; today, in contexts where more rigor is required, one can use axiomatic set theory, in which the notion of a \"set\" is taken as a primitive notion and the properties of sets are defined by a collection of axioms. The most basic properties are that a set can have elements, and that two sets are equal (one and the same) if and only if every element of each set is an element of the other; this property is called the extensionality of sets.Sets are conventionally denoted with capital letters. Sets A and B are equal if and only if they have precisely the same elements.[2]A set is a well-defined collection of distinct objects. The objects that make up a set (also known as the set's elements or members) can be anything: numbers, people, letters of the alphabet, other sets, and so on. Georg Cantor, one of the founders of set theory, gave the following definition of a set at the beginning of his Beitr\u00e4ge zur Begr\u00fcndung der transfiniten Mengenlehre:[1]The German word Menge, rendered as \"set\" in English, was coined by Bernard Bolzano in his work The Paradoxes of the Infinite.In mathematics, a set is a collection of distinct objects, considered as an object in its own right. For example, the numbers 2, 4, and 6 are distinct objects when considered separately, but when they are considered collectively they form a single set of size three, written {2,4,6}. The concept of a set is one of the most fundamental in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived. In mathematics education, elementary topics such as Venn diagrams are taught at a young age, while more advanced concepts are taught as part of a university degree.",
            "title": "Set (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Set_(mathematics)"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Multiplicity (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Multiple_roots_of_a_polynomial"
        },
        {
            "desc_links": [
                "/wiki/If_and_only_if",
                "/wiki/Polynomial_factorization",
                "/wiki/Root_of_a_polynomial",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Algebra",
                "/wiki/Algorithm",
                "/wiki/Polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Long_division",
                "/wiki/Synthetic_division"
            ],
            "links": [
                "/wiki/Cyclic_redundancy_check",
                "/wiki/Tangent",
                "/wiki/Graph_of_a_function",
                "/wiki/Quintic_function",
                "/wiki/Quartic_function",
                "/wiki/Rational_root_theorem",
                "/wiki/Algorithm",
                "/wiki/Pseudo-code",
                "/wiki/Long_division",
                "/wiki/If_and_only_if",
                "/wiki/Polynomial_factorization",
                "/wiki/Root_of_a_polynomial",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Algebra",
                "/wiki/Algorithm",
                "/wiki/Polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Long_division",
                "/wiki/Synthetic_division"
            ],
            "text": "A cyclic redundancy check uses the remainder of polynomial division to detect errors in transmitted messages.Polynomial long division can be used to find the equation of the line that is tangent to the graph of the function defined by the polynomial P(x) at a particular point x = r.[2] If R(x) is the remainder of the division of P(x) by (x \u2013 r)2, then the equation of the tangent line at x = r to the graph of the function y = P(x) is y = R(x), regardless of whether or not r is a root of the polynomial.In this way, sometimes all the roots of a polynomial of degree greater than four can be obtained, even though that is not always possible. For example, if the rational root theorem can be used to obtain a single (rational) root of a quintic polynomial, it can be factored out to obtain a quartic (fourth degree) quotient; the explicit formula for the roots of a quartic polynomial can then be used to find the other four roots of the quintic.Likewise, if more than one root is known, a linear factor (x \u2212 r) in one of them (r) can be divided out to obtain Q(x), and then a linear term in another root, s, can be divided out of Q(x), etc. Alternatively, they can all be divided out at once: for example the linear factors x \u2212 r and x \u2212 s can be multiplied together to obtain the quadratic factor x2 \u2212 (r + s)x + rs, which can then be divided into the original polynomial P(x) to obtain a quotient of degree n \u2212 2.Sometimes one or more roots of a polynomial are known, perhaps having been found using the rational root theorem. If one root r of a polynomial P(x) of degree n is known then polynomial long division can be used to factor P(x) into the form (x \u2212 r)(Q(x)) where Q(x) is a polynomial of degree n \u2212 1. Q(x) is simply the quotient obtained from the division process; since r is known to be a root of P(x), it is known that the remainder must be zero.The process of getting the uniquely defined polynomials Q and R from A and B is called Euclidean division (sometimes division transformation). Polynomial long division is thus an algorithm for Euclidean division.[1]and either R=0 or degree(R) < degree(B). Moreover (Q, R) is the unique pair of polynomials having this property.For every pair of polynomials (A, B) such that B \u2260 0, polynomial division provides a quotient Q and a remainder R such thatThis algorithm describes exactly the above paper and pencil method: d is written on the left of the \")\"; q is written, term after term, above the horizontal line, the last term being the value of t; the region under the horizontal line is used to compute and write down the successive values of r.Note that this works equally well when degree(n) < degree(d); in that case the result is just the trivial (0, n).The algorithm can be represented in pseudo-code as follows, where +, \u2212, and \u00d7 represent polynomial arithmetic, and / represents simple division of two terms:The long division algorithm for arithmetic is very similar to the above algorithm, in which the variable x is replaced by the specific number 10.The polynomial above the bar is the quotient q(x), and the number left over (\u00a05) is the remainder r(x).The quotient and remainder can then be determined as follows:The dividend is first rewritten like this:The result R=0 occurs if and only if the polynomial A has B as a factor. Thus long division is a means for testing whether one polynomial has another as a factor, and, if it does, for factoring it out. For example, if a root r of A is known, it can be factored out by dividing A by (x\u2013r).and either R = 0 or the degree of R is lower than the degree of B. These conditions uniquely define Q and R, which means that Q and R do not depend on the method used to compute them.Polynomial long division is an algorithm that implements the Euclidean division of polynomials, which starting from two polynomials A (the dividend) and B (the divisor) produces, if B is not zero, a quotient Q and a remainder R such thatIn algebra, polynomial long division is an algorithm for dividing a polynomial by another polynomial of the same or lower degree, a generalised version of the familiar arithmetic technique called long division. It can be done easily by hand, because it separates an otherwise complex division problem into smaller ones. Sometimes using a shorthand version called synthetic division is faster, with less writing and fewer calculations.",
            "title": "Polynomial long division",
            "url": "https://en.wikipedia.org/wiki/Polynomial_division"
        },
        {
            "desc_links": [
                "/wiki/Polynomials",
                "/wiki/Complex_conjugate_root_theorem",
                "/wiki/Quadratic_equation",
                "/wiki/Cubic_equation",
                "/wiki/Polar_coordinate_system#Complex_numbers",
                "/wiki/Euler%27s_formula",
                "/wiki/Mathematics",
                "/wiki/Complex_number",
                "/wiki/Real_number",
                "/wiki/Imaginary_number",
                "/wiki/Sign_(mathematics)"
            ],
            "links": [
                "/wiki/Commutative",
                "/wiki/Conjugate_transpose",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Adjoint_operator",
                "/wiki/Hilbert_space",
                "/wiki/C*-algebra",
                "/wiki/Dual_numbers",
                "/wiki/Split-complex_number",
                "/wiki/Frank_Morley",
                "/wiki/Involution_(mathematics)",
                "/wiki/Polynomials",
                "/wiki/Complex_conjugate_root_theorem",
                "/wiki/Quadratic_equation",
                "/wiki/Cubic_equation",
                "/wiki/Mathematics",
                "/wiki/Complex_number",
                "/wiki/Real_number",
                "/wiki/Imaginary_number",
                "/wiki/Sign_(mathematics)"
            ],
            "text": "Since the multiplication of planar real algebras is commutative, this reversal is not needed there.Note that all these generalizations are multiplicative only if the factors are reversed:Taking the conjugate transpose (or adjoint) of complex matrices generalizes complex conjugation. Even more general is the concept of adjoint operator for operators on (possibly infinite-dimensional) complex Hilbert spaces. All this is subsumed by the *-operations of C*-algebras.The other planar real algebras, dual numbers, and split-complex numbers are also analyzed using complex conjugation.These uses of the conjugate of z as a variable are illustrated in Frank Morley's book Inversive Geometry (1933), written with his son Frank Vigor Morley.The penultimate relation is involution; i.e., the conjugate of the conjugate of a complex number z is z. The ultimate relation is the method of choice to compute the inverse of a complex number if it is given in rectangular coordinates.For any two complex numbers w,z:A significant property of the complex conjugate is that a complex number is equal to its complex conjugate if its imaginary part is zero, that is, if the complex number is real.The following properties apply for all complex numbers z and w, unless stated otherwise, and can be proved by writing z and w in the form a + ib.Complex conjugates are important for finding roots of polynomials. According to the complex conjugate root theorem, if a complex number is a root to a polynomial in one variable with real coefficients (such as the quadratic equation or the cubic equation), so is its conjugate.In mathematics, the complex conjugate of a complex number is the number with an equal real part and an imaginary part equal in magnitude but opposite in sign.[1][2] For example, the complex conjugate of 3\u00a0+\u00a04i is 3\u00a0\u2212\u00a04i.",
            "title": "Complex conjugate",
            "url": "https://en.wikipedia.org/wiki/Complex_conjugate"
        },
        {
            "desc_links": [
                "/wiki/Corollary",
                "/wiki/Mathematical_analysis",
                "/wiki/Continuous_function",
                "/wiki/Interval_(mathematics)",
                "/wiki/Domain_of_a_function"
            ],
            "links": [
                "/wiki/Borsuk%E2%80%93Ulam_theorem",
                "/wiki/Great_circle",
                "/wiki/Temperature",
                "/wiki/Pressure",
                "/wiki/Elevation",
                "/wiki/Carbon_dioxide",
                "/wiki/Quantum#Beyond_electromagnetic_radiation",
                "/wiki/Antipodal_points",
                "/wiki/Darboux%27s_theorem_(analysis)",
                "/wiki/Derivative",
                "/wiki/Intermediate_value_property",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Limit_of_a_function",
                "/wiki/Conway_base_13_function",
                "/wiki/Darboux_function",
                "/wiki/Brouwer_fixed-point_theorem",
                "/wiki/Brouwer_fixed-point_theorem#One-dimensional_case",
                "/wiki/Total_order",
                "/wiki/Order_topology",
                "/wiki/Connectedness_(topology)",
                "/wiki/Bernard_Bolzano",
                "/wiki/Augustin-Louis_Cauchy",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Simon_Stevin",
                "/wiki/Louis_Arbogast",
                "/wiki/Infinitesimal",
                "/wiki/Non-standard_analysis",
                "/wiki/Non-standard_calculus",
                "/wiki/Completeness_(order_theory)",
                "/wiki/Corollary",
                "/wiki/Mathematical_analysis",
                "/wiki/Continuous_function",
                "/wiki/Interval_(mathematics)",
                "/wiki/Domain_of_a_function"
            ],
            "text": "The theorem also underpins the explanation of why rotating a wobbly table will bring it to stability (subject to certain easily met constraints).[8]Another generalization for which this holds is for any closed convex n (n > 1) dimensional shape. Specifically, for any continuous function whose domain is the given shape, and any point inside the shape (not necessarily its center), there exist two antipodal points with respect to the given point whose functional value is the same. The proof is identical to the one given above.This is a special case of a more general result called the Borsuk\u2013Ulam theorem.Proof: Take f to be any continuous function on a circle. Draw a line through the center of the circle, intersecting it at two opposite points A and B. Let d be defined by the difference f(A) \u2212 f(B). If the line is rotated 180 degrees, the value \u2212d will be obtained instead. Due to the intermediate value theorem there must be some intermediate rotation angle for which d = 0, and as a consequence f(A) = f(B) at this angle.The theorem implies that on any great circle around the world, for the temperature, pressure, elevation, carbon dioxide concentration, if the simplification is taken that this varies continuously, there will always exist two antipodal points that share the same value for that variable.Darboux's theorem states that all functions that result from the differentiation of some other function on some interval have the intermediate value property (even though they need not be continuous).Historically, this intermediate value property has been suggested as a definition for continuity of real-valued functions[7][citation needed]; this definition was not adopted.As an example, take the function f\u00a0: [0, \u221e) \u2192 [\u22121, 1] defined by f(x) = sin(1/x) for x > 0 and f(0) = 0. This function is not continuous at x = 0 because the limit of f(x) as x tends to 0 does not exist; yet the function has the intermediate value property. Another, more complicated example is given by the Conway base 13 function.A \"Darboux function\" is a real-valued function f that has the \"intermediate value property\", i.e., that satisfies the conclusion of the intermediate value theorem: for any two values a and b in the domain of f, and any y between f(a) and f(b), there is some c between a and b with f(c) = y. The intermediate value theorem says that every continuous function is a Darboux function. However, not every Darboux function is continuous; i.e., the converse of the intermediate value theorem is false.The Brouwer fixed-point theorem is a related theorem that, in one dimension gives a special case of the intermediate value theorem.The intermediate value theorem generalizes in a natural way: Suppose that X is a connected topological space and (Y, <) is a totally ordered set equipped with the order topology, and let f\u00a0: X \u2192 Y be a continuous map. If a and b are two points in X and u is a point in Y lying between f(a) and f(b) with respect to <, then there exists c in X such that f(c) = u. The original theorem is recovered by noting that \u211d is connected and that its natural topology is the order topology.The intermediate value theorem is an immediate consequence of these two properties of connectedness:[6]Recall the first version of the intermediate value theorem, stated previously:The intermediate value theorem is closely linked to the topological notion of connectedness and follows from the basic properties of connected sets in metric spaces and connected subsets of \u211d in particular:For u = 0 above, the statement is also known as Bolzano's theorem. This theorem was first proved by Bernard Bolzano in 1817. Augustin-Louis Cauchy provided a proof in 1821.[3] Both were inspired by the goal of formalizing the analysis of functions and the work of Joseph-Louis Lagrange. The idea that continuous functions possess the intermediate value property has an earlier origin. Simon Stevin proved the intermediate value theorem for polynomials (using a cubic as an example) by providing an algorithm for constructing the decimal expansion of the solution. The algorithm iteratively subdivides the interval into 10 parts, producing an additional decimal digit at each step of the iteration.[4] Before the formal definition of continuity was given, the intermediate value property was given as part of the definition of a continuous function. Proponents include Louis Arbogast, who assumed the functions to have no jumps, satisfy the intermediate value property and have increments whose sizes corresponded to the sizes of the increments of the variable.[5] Earlier authors held the result to be intuitively obvious and requiring no proof. The insight of Bolzano and Cauchy was to define a general notion of continuity (in terms of infinitesimals in Cauchy's case and using real inequalities in Bolzano's case), and to provide a proof based on such definitions.The intermediate value theorem can also be proved using the methods of non-standard analysis, which places \"intuitive\" arguments involving infinitesimals on a rigorous footing. (See the article: non-standard calculus.)The intermediate value theorem is an easy consequence of the basic properties of connected sets: the preservation of connectedness under continuous functions and the characterization of connected subsets of \u211d as intervals (see below for details and alternate proof). The latter characterization is ultimately a consequence of the least-upper-bound property of the real numbers.Both inequalitiesThe theorem may be proven as a consequence of the completeness property of the real numbers as follows:[2]A subset of the real numbers with no internal gap is an interval. Version I is naturally contained in Version II.The intermediate value theorem states the following.This captures an intuitive property of continuous functions: given f continuous on [1, 2] with the known values f(1) = 3 and f(2) = 5. Then the graph of y = f(x) must pass through the horizontal line y = 4 while x moves from 1 to 2. It represents the idea that the graph of a continuous function on a closed interval can be drawn without lifting your pencil from the paper.This has two important corollaries:In mathematical analysis, the intermediate value theorem states that if a continuous function, f, with an interval, [a, b], as its domain, takes values f(a) and f(b) at each end of the interval, then it also takes any value between f(a) and f(b) at some point within the interval.",
            "title": "Intermediate value theorem",
            "url": "https://en.wikipedia.org/wiki/Intermediate_value_theorem"
        },
        {
            "desc_links": [
                "/wiki/Numerical_analysis",
                "/wiki/Matrix_(mathematics)#Decomposition",
                "/wiki/Sparse_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Finite_element_method",
                "/wiki/Derivative_(calculus)",
                "/wiki/Taylor_series",
                "/wiki/Physics",
                "/wiki/Classical_mechanics",
                "/wiki/Optics",
                "/wiki/Electromagnetism",
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum_electrodynamics",
                "/wiki/Rigid-body_dynamics",
                "/wiki/Computer_graphics",
                "/wiki/3D_models",
                "/wiki/Glossary_of_computer_graphics#screen_space",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Stochastic_matrix",
                "/wiki/PageRank",
                "/wiki/Matrix_calculus",
                "/wiki/Mathematical_analysis",
                "/wiki/Derivative",
                "/wiki/Exponentials",
                "/wiki/Economics",
                "/wiki/Matrix_addition",
                "/wiki/Conformable_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_multiplication#Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Linear_transformation",
                "/wiki/Linear_functions",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Euclidean_vector",
                "/wiki/Dimension",
                "/wiki/Rotation_matrix",
                "/wiki/Column_vector",
                "/wiki/Position_(vector)",
                "/wiki/Transformation_matrices",
                "/wiki/Function_composition",
                "/wiki/Transformation_(function)",
                "/wiki/System_of_linear_equations",
                "/wiki/Determinant",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Zero",
                "/wiki/Geometry",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Mathematics",
                "/wiki/Rectangle",
                "/wiki/Number",
                "/wiki/Symbol_(formal)",
                "/wiki/Expression_(mathematics)"
            ],
            "links": [
                "/wiki/Alfred_Tarski",
                "/wiki/Truth_table",
                "/wiki/Bertrand_Russell",
                "/wiki/Alfred_North_Whitehead",
                "/wiki/Axiom_of_reducibility",
                "/wiki/Extension_(predicate_logic)",
                "/wiki/Matrix_mechanics",
                "/wiki/Werner_Heisenberg",
                "/wiki/Max_Born",
                "/wiki/Pascual_Jordan",
                "/wiki/John_von_Neumann",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Functional_analysis",
                "/wiki/Linear_operator",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_space",
                "/wiki/Hamel_dimension",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Georg_Frobenius",
                "/wiki/Bilinear_form",
                "/wiki/Gauss%E2%80%93Jordan_elimination",
                "/wiki/Gauss_elimination",
                "/wiki/Wilhelm_Jordan_(geodesist)",
                "/wiki/Hypercomplex_number",
                "/wiki/Multiplication",
                "/wiki/Eigenvalue",
                "/wiki/Carl_Gustav_Jacob_Jacobi",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Infinitesimal",
                "/wiki/Leopold_Kronecker",
                "/wiki/Karl_Weierstrass",
                "/wiki/Axiom",
                "/wiki/Number_theory",
                "/wiki/Gauss",
                "/wiki/Quadratic_form",
                "/wiki/Linear_map",
                "/wiki/Gotthold_Eisenstein",
                "/wiki/Matrix_product",
                "/wiki/Non-commutative",
                "/wiki/Augustin-Louis_Cauchy",
                "/wiki/Polynomial",
                "/wiki/Arthur_Cayley",
                "/wiki/Arthur_Cayley",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Linear_equations",
                "/wiki/Chinese_mathematics",
                "/wiki/The_Nine_Chapters_on_the_Mathematical_Art",
                "/wiki/System_of_linear_equations",
                "/wiki/Determinant",
                "/wiki/Gerolamo_Cardano",
                "/wiki/Japanese_mathematics",
                "/wiki/Seki_Kowa",
                "/wiki/Jan_de_Witt",
                "/wiki/Gottfried_Wilhelm_Leibniz",
                "/wiki/Gabriel_Cramer",
                "/wiki/Cramer%27s_rule",
                "/wiki/Electronics",
                "/wiki/Electrical_impedance",
                "/wiki/Admittance",
                "/wiki/Dimensionless_quantity",
                "/wiki/Mesh_analysis",
                "/wiki/Nodal_analysis",
                "/wiki/Geometrical_optics",
                "/wiki/Light_wave",
                "/wiki/Ray_(optics)",
                "/wiki/Ray_(geometry)",
                "/wiki/Lens_(optics)",
                "/wiki/Ray_transfer_matrix",
                "/wiki/Equation_of_motion",
                "/wiki/Eigenvector",
                "/wiki/Normal_mode",
                "/wiki/Molecules",
                "/wiki/Particle_accelerator",
                "/wiki/S-matrix",
                "/wiki/Quantum_mechanics",
                "/wiki/Werner_Heisenberg",
                "/wiki/Matrix_mechanics",
                "/wiki/Density_matrix",
                "/wiki/Eigenstates",
                "/wiki/Symmetry",
                "/wiki/Elementary_particle",
                "/wiki/Quantum_field_theory",
                "/wiki/Lorentz_group",
                "/wiki/Spin_group",
                "/wiki/Pauli_matrices",
                "/wiki/Gamma_matrices",
                "/wiki/Fermion",
                "/wiki/Spinor",
                "/wiki/Quark",
                "/wiki/Special_unitary_group",
                "/wiki/Gell-Mann_matrices",
                "/wiki/Gauge_group",
                "/wiki/Quantum_chromodynamics",
                "/wiki/Cabibbo%E2%80%93Kobayashi%E2%80%93Maskawa_matrix",
                "/wiki/Weak_interaction",
                "/wiki/Mass",
                "/wiki/Random_matrix",
                "/wiki/Probability_distribution",
                "/wiki/Matrix_normal_distribution",
                "/wiki/Number_theory",
                "/wiki/Physics",
                "/wiki/Singular_value_decomposition",
                "/wiki/Descriptive_statistics",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Dimensionality_reduction",
                "/wiki/Covariance_matrix",
                "/wiki/Variance",
                "/wiki/Random_variable",
                "/wiki/Linear_least_squares_(mathematics)",
                "/wiki/Stochastic_matrix",
                "/wiki/Probability_vector",
                "/wiki/Markov_chain",
                "/wiki/Absorbing_state",
                "/wiki/Finite_element_method",
                "/wiki/Partial_differential_equation",
                "/wiki/Elliptic_partial_differential_equation",
                "/wiki/Implicit_function_theorem",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Hessian_matrix",
                "/wiki/Differentiable_function",
                "/wiki/Second_derivative",
                "/wiki/Adjacency_matrix",
                "/wiki/Finite_graph",
                "/wiki/Graph_theory",
                "/wiki/Logical_matrix",
                "/wiki/Distance_matrix",
                "/wiki/Website",
                "/wiki/Hyperlink",
                "/wiki/Sparse_matrix",
                "/wiki/Network_theory",
                "/wiki/Chemistry",
                "/wiki/Quantum_mechanics",
                "/wiki/Chemical_bond",
                "/wiki/Spectroscopy",
                "/wiki/Overlap_matrix",
                "/wiki/Fock_matrix",
                "/wiki/Roothaan_equations",
                "/wiki/Molecular_orbital",
                "/wiki/Hartree%E2%80%93Fock",
                "/wiki/Encryption",
                "/wiki/Hill_cipher",
                "/wiki/Computer_graphics",
                "/wiki/Rotation_matrix",
                "/wiki/Polynomial_ring",
                "/wiki/Control_theory",
                "/wiki/Absolute_value",
                "/wiki/Quaternion",
                "/wiki/Clifford_algebra",
                "/wiki/Game_theory",
                "/wiki/Economics",
                "/wiki/Payoff_matrix",
                "/wiki/Text_mining",
                "/wiki/Thesaurus",
                "/wiki/Document-term_matrix",
                "/wiki/Tf-idf",
                "/wiki/Zero_vector_space",
                "/wiki/Computer_algebra_system",
                "/wiki/Empty_product",
                "/wiki/Hilbert_space#Operators_on_Hilbert_spaces",
                "/wiki/Continuous_function",
                "/wiki/Functional_analysis",
                "/wiki/Absolutely_convergent_series",
                "/wiki/Linear_combination",
                "/wiki/Finite_group",
                "/wiki/Isomorphic",
                "/wiki/Regular_representation",
                "/wiki/Symmetric_group",
                "/wiki/Representation_theory",
                "/wiki/Orthogonal_group",
                "/wiki/Determinant",
                "/wiki/Subgroup",
                "/wiki/Special_linear_group",
                "/wiki/Orthogonal_matrix",
                "/wiki/Group_(mathematics)",
                "/wiki/Binary_operation",
                "/wiki/General_linear_group",
                "/wiki/Matrix_ring",
                "/wiki/Endomorphism_ring",
                "/wiki/Matrix_equivalence",
                "/wiki/Transpose_of_a_linear_map",
                "/wiki/Dual_space",
                "/wiki/Hamel_dimension",
                "/wiki/Vector_space",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Block_matrix",
                "/wiki/Ring_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Matrix_ring",
                "/wiki/Endomorphism_ring",
                "/wiki/Module_(mathematics)",
                "/wiki/Commutative_ring",
                "/wiki/Associative_algebra",
                "/wiki/Determinant",
                "/wiki/Leibniz_formula_(determinant)",
                "/wiki/Invertible",
                "/wiki/Superring",
                "/wiki/Supermatrix",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Addition",
                "/wiki/Subtraction",
                "/wiki/Multiplication",
                "/wiki/Division_(mathematics)",
                "/wiki/Rational_number",
                "/wiki/Finite_field",
                "/wiki/Coding_theory",
                "/wiki/Eigenvalue",
                "/wiki/Algebraically_closed_field",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Tensors",
                "/wiki/Group_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Matrix_ring",
                "/wiki/Field_(mathematics)",
                "/wiki/Matrix_field",
                "/wiki/Matrix_exponential",
                "/wiki/Linear_differential_equation",
                "/wiki/Matrix_logarithm",
                "/wiki/Square_root_of_a_matrix",
                "/wiki/Condition_number",
                "/wiki/Schur_decomposition",
                "/wiki/Eigendecomposition",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Jordan_normal_form",
                "/wiki/LU_decomposition",
                "/wiki/Triangular_matrix",
                "/wiki/Forward_substitution",
                "/wiki/Row_echelon_form",
                "/wiki/Elementary_matrix",
                "/wiki/Permutation_matrix",
                "/wiki/Singular_value_decomposition",
                "/wiki/Unitary_matrix",
                "/wiki/Computer_language",
                "/wiki/HP_9830",
                "/wiki/APL_(programming_language)",
                "/wiki/List_of_numerical_analysis_software",
                "/wiki/Matrix_norm",
                "/wiki/Condition_number",
                "/wiki/Adjugate_matrix",
                "/wiki/Sparse_matrix",
                "/wiki/Conjugate_gradient_method",
                "/wiki/Upper_bound",
                "/wiki/Matrix_multiplication_algorithm",
                "/wiki/Strassen_algorithm",
                "/wiki/Numerical_linear_algebra",
                "/wiki/Complexity_analysis",
                "/wiki/Numerical_stability",
                "/wiki/Sequence_(mathematics)",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Infinity",
                "/wiki/Indeterminate_(variable)",
                "/wiki/Characteristic_polynomial",
                "/wiki/Monic_polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/Zero_matrix",
                "/wiki/Logical_equivalence",
                "/wiki/Laplace_expansion",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Linear_system",
                "/wiki/Cramer%27s_rule",
                "/wiki/Rule_of_Sarrus",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/If_and_only_if",
                "/wiki/Absolute_value",
                "/wiki/Trace_of_a_matrix",
                "/wiki/Complex_number",
                "/wiki/Unitary_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Determinant",
                "/wiki/Determinant",
                "/wiki/Linear_transformation",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Reflection_(mathematics)",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_(mathematics)#Square_matrices",
                "/wiki/Real_number",
                "/wiki/Orthogonal",
                "/wiki/Unit_vector",
                "/wiki/Orthonormality",
                "/wiki/Transpose",
                "/wiki/Invertible_matrix",
                "/wiki/Bilinear_form",
                "/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices",
                "/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices",
                "/wiki/Positive-definite_matrix",
                "/wiki/Quadratic_form",
                "/wiki/Identity_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Invertible_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Spectral_theorem",
                "/wiki/Eigenbasis",
                "/wiki/Linear_combination",
                "/wiki/Symmetric_matrix",
                "/wiki/Skew-symmetric_matrix",
                "/wiki/Hermitian_matrix",
                "/wiki/Asterisk",
                "/wiki/Conjugate_transpose",
                "/wiki/Complex_conjugate",
                "/wiki/Diagonal_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Triangular_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Square_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Rank_of_a_matrix",
                "/wiki/Linear_independence",
                "/wiki/Hamel_dimension",
                "/wiki/Image_(mathematics)",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Kernel_(matrix)",
                "/wiki/Bijection",
                "/wiki/Function_composition",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Unit_vector",
                "/wiki/Inverse_matrix",
                "/wiki/Generalized_inverse",
                "/wiki/Independent_equation",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Determinant",
                "/wiki/Linear_equation",
                "/wiki/Matrix_inverse",
                "/wiki/Hadamard_product_(matrices)",
                "/wiki/Kronecker_product",
                "/wiki/Sylvester_equation",
                "/wiki/Commutativity",
                "/wiki/Associativity",
                "/wiki/Distributivity",
                "/wiki/Dot_product",
                "/wiki/Commutative",
                "/wiki/Set_(mathematics)",
                "/wiki/Box_bracket",
                "/wiki/Parens",
                "/wiki/Row_vector",
                "/wiki/Column_vectors",
                "/wiki/Square_matrix",
                "/wiki/Number",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_numbers",
                "/wiki/Complex_numbers",
                "/wiki/Numerical_analysis",
                "/wiki/Matrix_(mathematics)#Decomposition",
                "/wiki/Sparse_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Finite_element_method",
                "/wiki/Derivative_(calculus)",
                "/wiki/Taylor_series",
                "/wiki/Physics",
                "/wiki/Classical_mechanics",
                "/wiki/Optics",
                "/wiki/Electromagnetism",
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum_electrodynamics",
                "/wiki/Rigid-body_dynamics",
                "/wiki/Computer_graphics",
                "/wiki/3D_models",
                "/wiki/Glossary_of_computer_graphics#screen_space",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Stochastic_matrix",
                "/wiki/PageRank",
                "/wiki/Matrix_calculus",
                "/wiki/Mathematical_analysis",
                "/wiki/Derivative",
                "/wiki/Exponentials",
                "/wiki/Economics",
                "/wiki/Matrix_addition",
                "/wiki/Conformable_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_multiplication#Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Linear_transformation",
                "/wiki/Linear_functions",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Euclidean_vector",
                "/wiki/Dimension",
                "/wiki/Rotation_matrix",
                "/wiki/Column_vector",
                "/wiki/Position_(vector)",
                "/wiki/Transformation_matrices",
                "/wiki/Function_composition",
                "/wiki/Transformation_(function)",
                "/wiki/System_of_linear_equations",
                "/wiki/Determinant",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Zero",
                "/wiki/Geometry",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Mathematics",
                "/wiki/Rectangle",
                "/wiki/Number",
                "/wiki/Symbol_(formal)",
                "/wiki/Expression_(mathematics)"
            ],
            "text": "Alfred Tarski in his 1946 Introduction to Logic used the word \"matrix\" synonymously with the notion of truth table as used in mathematical logic.[118]For example, a function \u03a6(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by \"considering\" the function for all possible values of \"individuals\" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, \u2200ai: \u03a6(ai, y), can be reduced to a \"matrix\" of values by \"considering\" the function for all possible values of \"individuals\" bi substituted in place of variable y:Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910\u20131913) use the word \"matrix\" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the \"bottom\" (0 order) the function is identical to its extension:The word has been used in unusual ways by at least two authors of historical importance.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.Many theorems were first established for small matrices only, for example the Cayley\u2013Hamilton theorem was proved for 2\u00d72 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4\u00d74 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss\u2013Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.where \u03a0 denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied \"functional determinants\"\u2014later called Jacobi determinants by Sylvester\u2014which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen \u00fcber die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy \u2212 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialAn English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley\u2013Hamilton theorem.[103]The term \"matrix\" (Latin for \"womb\", derived from mater\u2014mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th\u20132nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H \u00b7 A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the \"mixed\" state of a quantum system as a linear combination of elementary, \"pure\" eigenstates.[97]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo\u2013Kobayashi\u2013Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear functionStochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn \u2192 Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]The Hessian matrix of a differentiable function \u0192: Rn \u2192 R consists of the second derivatives of \u0192 with respect to the several coordinate directions, that is,[81]The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example \"yes\" and \"no\", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree\u2013Fock method.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Complex numbers can be represented by particular real 2-by-2 matrices viaThere are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant\u00a01, a fact that is often used as a part of the characterization of determinants.In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V\u2192W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector\u00a0v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A\u00b7v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or \u22121. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionA group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.More generally, the set of m\u00d7n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n\u00a0=\u00a0m composition of these maps is possible, and this gives rise to the matrix ring of n\u00d7n matrices representing the endomorphism ring of Rn.In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]Linear maps Rn \u2192 Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V \u2192 W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatMatrices do not always have all their entries in the same ring\u00a0\u2013 or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]The eigendecomposition or diagonalization expresses A as a product VDV\u22121, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues \u03bb1 to \u03bbn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaThe LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV\u2217, where U and V are unitary matrices and D is a diagonal matrix.There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace expansion (Adj (A) denotes the adjugate matrix of A)In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this \"naive\" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn\u2212A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(\u03bb)\u00a0=\u00a00 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley\u2013Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number \u03bb is an eigenvalue of an n\u00d7n-matrix A if and only if A\u2212\u03bbIn is not invertible, which is equivalent toA number \u03bb and a non-zero vector v satisfyingAdding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by \u22121.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]The determinant of a product of square matrices equals the product of their determinants:The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of 2-by-2 matrices is given byThe determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.Also, the trace of a matrix is equal to that of its transpose, that is,This is immediate from the definition of matrix multiplication:The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:The complex analogue of an orthogonal matrix is a unitary matrix.An orthogonal matrix A is necessarily invertible (with inverse A\u22121 = AT), unitary (A\u22121 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or \u22121. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.where I is the identity matrix of size n.which entailsAn orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:Allowing as input two different vectors instead yields the bilinear form associated to A:A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric n\u00d7n-matrix A is called positive-definite if for all nonzero vectors x\u00a0\u2208\u00a0Rn the associated quadratic form given bywhere In is the n\u00d7n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A\u22121.A square matrix A is called invertible or non-singular if there exists a matrix B such thatBy the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = \u2212AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A\u2217 = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank\u2013nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]The last equality follows from the above-mentioned associativity of matrix multiplication.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g\u00a0: Rm \u2192 Rk, then the composition g \u2218 f is represented by BA sinceThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.For example, the 2\u00d72 matrixMatrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn \u2192 Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn \u2192 Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.where A\u22121 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingis equivalent to the system of linear equationsMatrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n\u00d71-matrix) of n variables x1, x2, ..., xn, and b is an m\u00d71-column vector, then the matrix equationA principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.There are three types of row operations:Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.whereasthat is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m \u2260 k. Even if both products are defined, they need not be equal, that is, generallywhere 1 \u2264 i \u2264 m and 1 \u2264 j \u2264 p.[13] For example, the underlined entry 2340 in the product is calculated as (2 \u00d7 1000) + (3 \u00d7 100) + (4 \u00d7 10) = 2340:Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A\u00a0+\u00a0B\u00a0=\u00a0B\u00a0+\u00a0A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A\u00a0+\u00a0B)T\u00a0=\u00a0AT\u00a0+\u00a0BT. Finally, (AT)T\u00a0=\u00a0A.There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,\u2217 refers to the ith row of A, and a\u2217,j refers to the jth column of A. The set of all m-by-n matrices is denoted \ud835\udd44(m, n).Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-\u00d7-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 \u2264 i \u2264 m \u2212 1 and 0 \u2264 j \u2264 n \u2212 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m \u00d7 n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m \u00d7 n as a subscript. For instance, the matrix A above is 3 \u00d7 4 and can be defined as A = [i \u2212 j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i \u2212 j]3\u00d74.Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i \u2212 j.The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Matrices are commonly written in box brackets or parentheses:Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m\u00a0\u00d7\u00a0n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3\u00a0\u00d7\u00a02 matrix.The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.The individual items in an m \u00d7 n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n \u00d7 Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 \u00d7 3 (read \"two by three\"), because there are two rows and three columns:",
            "title": "Matrix (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Real_matrix"
        },
        {
            "desc_links": [
                "/wiki/Cantor%27s_diagonal_argument",
                "/wiki/Uncountable",
                "/wiki/Rational_number",
                "/wiki/Almost_all",
                "/wiki/Continued_fraction#Infinite_continued_fractions_and_convergents",
                "/wiki/Decimal_number",
                "/wiki/Natural_number",
                "/wiki/Repeating_decimal",
                "/wiki/Pi",
                "/wiki/Pi",
                "/wiki/Pi",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Golden_ratio",
                "/wiki/Square_root_of_two",
                "/wiki/Natural_number",
                "/wiki/Square_number",
                "/wiki/Mathematics",
                "/wiki/Real_number",
                "/wiki/Rational_number",
                "/wiki/Fraction_(mathematics)",
                "/wiki/Integer",
                "/wiki/Ratio",
                "/wiki/Commensurability_(mathematics)"
            ],
            "links": [
                "/wiki/Clopen_set",
                "/wiki/Euclidean_distance",
                "/wiki/Metric_space",
                "/wiki/Topological_space",
                "/wiki/Complete_(topology)",
                "/wiki/G-delta_set",
                "/wiki/Completely_metrizable",
                "/wiki/Continued_fraction",
                "/wiki/Uncountable",
                "/wiki/Countable_set",
                "/wiki/Natural_logarithm",
                "/wiki/Catalan%27s_constant",
                "/wiki/Euler%E2%80%93Mascheroni_gamma_constant",
                "/wiki/Tetration",
                "/wiki/Pi",
                "/wiki/Algebraic_independence",
                "/wiki/Gelfond%E2%80%93Schneider_theorem",
                "/wiki/Transcendental_number",
                "/wiki/Algebraic_number",
                "/wiki/Exponentiation#Powers_of_complex_numbers",
                "/wiki/Constructive_proof",
                "/wiki/Repeating_decimal",
                "/wiki/Long_division",
                "/wiki/Binary_numeral_system",
                "/wiki/Octal",
                "/wiki/Hexadecimal",
                "/wiki/Positional_notation",
                "/wiki/Numeral_system",
                "/wiki/Natural_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Countable_set",
                "/wiki/Algebraic_number",
                "/wiki/Zero_of_a_function",
                "/wiki/Polynomial",
                "/wiki/Almost_all",
                "/wiki/Transcendental_number",
                "/wiki/Prime_factor",
                "/wiki/Logarithm",
                "/wiki/Proof_by_contradiction",
                "/wiki/Fundamental_theorem_of_arithmetic",
                "/wiki/Unique_factorization",
                "/wiki/Lowest_terms",
                "/wiki/Prime_number",
                "/wiki/Square_root_of_2",
                "/wiki/Golden_ratio",
                "/wiki/Perfect_squares",
                "/wiki/Quadratic_irrational",
                "/wiki/Johann_Heinrich_Lambert",
                "/wiki/Adrien-Marie_Legendre",
                "/wiki/Bessel%E2%80%93Clifford_function",
                "/wiki/Transcendental_number",
                "/wiki/Georg_Cantor",
                "/wiki/Georg_Cantor%27s_first_set_theory_article",
                "/wiki/Charles_Hermite",
                "/wiki/Ferdinand_von_Lindemann",
                "/wiki/David_Hilbert",
                "/wiki/Adolf_Hurwitz",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Paul_Gordan",
                "/wiki/Continued_fraction",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Imaginary_number",
                "/wiki/Abraham_de_Moivre",
                "/wiki/Leonhard_Euler",
                "/wiki/Complex_number",
                "/wiki/Transcendental_numbers",
                "/wiki/Euclid",
                "/wiki/Karl_Weierstrass",
                "/wiki/Eduard_Heine",
                "/wiki/Crelle%27s_Journal",
                "/wiki/Georg_Cantor",
                "/wiki/Richard_Dedekind",
                "/wiki/Salvatore_Pincherle",
                "/wiki/Paul_Tannery",
                "/wiki/Dedekind_cut",
                "/wiki/Rational_number",
                "/wiki/Leopold_Kronecker",
                "/wiki/Charles_M%C3%A9ray",
                "/wiki/Egypt",
                "/wiki/Ab%C5%AB_K%C4%81mil_Shuj%C4%81_ibn_Aslam",
                "/wiki/Quadratic_equation",
                "/wiki/Coefficient",
                "/wiki/Equation",
                "/wiki/Nth_root",
                "/wiki/Iraq",
                "/wiki/Ab%C5%AB_Ja%27far_al-Kh%C4%81zin",
                "/wiki/Quantity",
                "/wiki/Cube_root",
                "/wiki/Arithmetic",
                "/wiki/Middle_ages",
                "/wiki/Algebra",
                "/wiki/Mathematics_in_medieval_Islam",
                "/wiki/Number",
                "/wiki/Magnitude_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Ratio",
                "/wiki/Persian_people",
                "/wiki/Al-Mahani",
                "/wiki/Quadratic_irrational",
                "/wiki/Madhava_of_Sangamagrama",
                "/wiki/Kerala_school_of_astronomy_and_mathematics",
                "/wiki/Infinite_series",
                "/wiki/Pi",
                "/wiki/Trigonometric_functions",
                "/wiki/Jye%E1%B9%A3%E1%B9%ADhadeva",
                "/wiki/Yuktibh%C4%81%E1%B9%A3%C4%81",
                "/wiki/Indian_mathematics",
                "/wiki/Manava",
                "/wiki/Square_root",
                "/wiki/Carl_Benjamin_Boyer",
                "/wiki/Vedic_period",
                "/wiki/Samhita",
                "/wiki/Brahmana",
                "/wiki/Shulba_Sutras",
                "/wiki/Eudoxus_of_Cnidus",
                "/wiki/Theodorus_of_Cyrene",
                "/wiki/Nth_root",
                "/wiki/Method_of_exhaustion",
                "/wiki/Reductio_ad_absurdum",
                "/wiki/Eudoxus_of_Cnidus",
                "/wiki/Zeno_of_Elea",
                "/wiki/Infinity",
                "/wiki/Zeno%27s_paradoxes",
                "/wiki/Greek_mathematics",
                "/wiki/Pythagoreanism",
                "/wiki/Hippasus",
                "/wiki/Pentagram",
                "/wiki/Hypotenuse",
                "/wiki/Isosceles_right_triangle",
                "/wiki/Commensurability_(mathematics)",
                "/wiki/Cantor%27s_diagonal_argument",
                "/wiki/Uncountable",
                "/wiki/Rational_number",
                "/wiki/Almost_all",
                "/wiki/Continued_fraction#Infinite_continued_fractions_and_convergents",
                "/wiki/Decimal_number",
                "/wiki/Natural_number",
                "/wiki/Repeating_decimal",
                "/wiki/Pi",
                "/wiki/Pi",
                "/wiki/Pi",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Golden_ratio",
                "/wiki/Square_root_of_two",
                "/wiki/Natural_number",
                "/wiki/Square_number",
                "/wiki/Mathematics",
                "/wiki/Real_number",
                "/wiki/Rational_number",
                "/wiki/Fraction_(mathematics)",
                "/wiki/Integer",
                "/wiki/Ratio",
                "/wiki/Commensurability_(mathematics)"
            ],
            "text": "Furthermore, the set of all irrationals is a disconnected metrizable space. In fact, the irrationals have a basis of clopen sets so the space is zero-dimensional.Under the usual (Euclidean) distance function d(x,\u00a0y) = |x\u00a0\u2212\u00a0y|, the real numbers are a metric space and hence also a topological space. Restricting the Euclidean distance function gives the irrationals the structure of a metric space. Since the subspace of irrationals is not closed, the induced metric is not complete. However, being a G-delta set\u2014i.e., a countable intersection of open subsets\u2014in a complete metric space, the space of irrationals is completely metrizable: that is, there is a metric on the irrationals inducing the same topology as the restriction of the Euclidean metric, but with respect to which the irrationals are complete. One can see this without knowing the aforementioned fact about G-delta sets: the continued fraction expansion of an irrational number defines a homeomorphism from the space of irrationals to the space of all sequences of positive integers, which is easily seen to be completely metrizable.Since the reals form an uncountable set, of which the rationals are a countable subset, the complementary set of irrationals is uncountable.It is not known whether \u03c0e, \u03c0/e, 2e, \u03c0e, \u03c0\u221a2, ln \u03c0, Catalan's constant, or the Euler\u2013Mascheroni gamma constant \u03b3 are irrational.[33][34][35] It is not known if the tetrations n\u03c0 or ne are rational for some positive integer n > 1, respectively.It is not known whether \u03c0 + e or \u03c0 \u2212 e is irrational or not. In fact, there is no pair of non-zero integers m and n for which it is known whether m\u03c0 + ne is irrational or not. Moreover, it is not known whether the set {\u03c0, e} is algebraically independent over Q.An example that provides a simple constructive proof is[31]Although the above argument does not decide between the two cases, the Gelfond\u2013Schneider theorem shows that \u221a2\u221a2 is transcendental, hence irrational. This theorem states that if a and b are both algebraic numbers, and a is not equal to 0 or 1, and b is not a rational number, then any value of ab is a transcendental number (there can be more than one value if complex number exponentiation is used).Consider \u221a2\u221a2; if this is rational, then take a = b = \u221a2. Otherwise, take a to be the irrational number \u221a2\u221a2 and b = \u221a2. Then ab = (\u221a2\u221a2)\u221a2 = \u221a2\u221a2\u00b7\u221a2 = \u221a22 = 2, which is rational.Dov Jarden gave a simple non-constructive proof that there exist two irrational numbers a and b, such that ab is rational:[30]is a ratio of integers and therefore a rational number, as required for the proof.ThenTherefore, when we subtract the 10A equation from the 10,000A equation, the tail end of 10A cancels out the tail end of 10,000A leaving us with:The result of the two multiplications gives two different expressions with exactly the same \"decimal portion\", that is, the tail end of 10,000A matches the tail end of 10A exactly. Here, both 10,000A and 10A have .162162162 ... at the end.Now we multiply this equation by 10r where r is the length of the repetend. This has the effect of moving the decimal point to be in front of the \"next\" repetend. In our example, multiply by 103:Here the repetend is 162 and the length of the repetend is 3. First, we multiply by an appropriate power of 10 to move the decimal point to the right so that it is just in front of a repetend. In this example we would multiply by 10 to obtain:Conversely, suppose we are faced with a repeating decimal, we can prove that it is a fraction of two integers. For example, consider:To show this, suppose we divide integers n by m (where m is nonzero). When long division is applied to the division of n by m, only m remainders are possible. If 0 appears as a remainder, the decimal expansion terminates. If 0 never occurs, then the algorithm can run at most m \u2212 1 steps without using any remainder more than once. After that, a remainder must recur, and then the decimal expansion repeats.The decimal expansion of an irrational number never repeats or terminates (the latter being equivalent to repeating zeroes), unlike any rational number. The same is true for binary, octal or hexadecimal expansions, and in general for expansions in every positional notation with natural bases.Because the algebraic numbers form a subfield of the real numbers, many irrational real numbers can be constructed by combining transcendental and algebraic numbers. For example, 3\u03c0\u00a0+\u00a02, \u03c0\u00a0+\u00a0\u221a2 and e\u221a3 are irrational (and even transcendental).Irrational numbers can also be found within the countable set of real algebraic numbers (essentially defined as the real roots of polynomials with integer coefficients), i.e., as real solutions of polynomial equationsAlmost all irrational numbers are transcendental and all real transcendental numbers are irrational (there are also complex transcendental numbers): the article on transcendental numbers lists several examples. So e\u00a0r and \u03c0\u00a0r are irrational for all nonzero rational\u00a0r, and, e.g., e\u03c0\u00a0is irrational, too.Cases such as log10\u00a02 can be treated similarly.However, the number 2 raised to any positive integer power must be even (because it is divisible by\u00a02) and the number\u00a03 raised to any positive integer power must be odd (since none of its prime factors will be\u00a02). Clearly, an integer cannot be both odd and even at the same time: we have a contradiction. The only assumption we made was that log2\u00a03 is rational (and so expressible as a quotient of integers m/n with n\u00a0\u2260\u00a00). The contradiction means that this assumption must be false, i.e. log2\u00a03 is irrational, and can never be expressed as a quotient of integers m/n with n\u00a0\u2260\u00a00.It follows thatAssume log2\u00a03 is rational. For some positive integers m and n, we havePerhaps the numbers most easy to prove irrational are certain logarithms. Here is a proof by contradiction that log2\u00a03 is irrational. Notice that log2\u00a03 \u2248\u00a01.58\u00a0>\u00a00.The proof above for the square root of two can be generalized using the fundamental theorem of arithmetic. This asserts that every integer has a unique factorization into primes. Using it we can show that if a rational number is not an integer then no integral power of it can be an integer, as in lowest terms there must be a prime in the denominator that does not divide into the numerator whatever power each is raised to. Therefore, if an integer is not an exact kth power of another integer then its kth root is irrational.The square root of 2 was the first number proved irrational, and that article contains a number of proofs. The golden ratio is another famous quadratic irrational and there is a simple proof of its irrationality in its article. The square roots of all natural numbers which are not perfect squares are irrational and a proof may be found in quadratic irrationals.Johann Heinrich Lambert proved (1761) that \u03c0 cannot be rational, and that en is irrational if n is rational (unless n\u00a0=\u00a00).[28] While Lambert's proof is often called incomplete, modern assessments support it as satisfactory, and in fact for its time it is unusually rigorous. Adrien-Marie Legendre (1794), after introducing the Bessel\u2013Clifford function, provided a proof to show that \u03c02 is irrational, whence it follows immediately that \u03c0 is irrational also. The existence of transcendental numbers was first established by Liouville (1844, 1851). Later, Georg Cantor (1873) proved their existence by a different method, which showed that every interval in the reals contains transcendental numbers. Charles Hermite (1873) first proved e transcendental, and Ferdinand von Lindemann (1882), starting from Hermite's conclusions, showed the same for \u03c0. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and was finally made elementary by Adolf Hurwitz[citation needed] and Paul Gordan.[29]Continued fractions, closely related to irrational numbers (and due to Cataldi, 1613), received attention at the hands of Euler, and at the opening of the 19th century were brought into prominence through the writings of Joseph-Louis Lagrange. Dirichlet also added to the general theory, as have numerous contributors to the applications of the subject.The 17th century saw imaginary numbers become a powerful tool in the hands of Abraham de Moivre, and especially of Leonhard Euler. The completion of the theory of complex numbers in the 19th century entailed the differentiation of irrationals into algebraic and transcendental numbers, the proof of the existence of transcendental numbers, and the resurgence of the scientific study of the theory of irrationals, largely ignored since Euclid. The year 1872 saw the publication of the theories of Karl Weierstrass (by his pupil Ernst Kossak), Eduard Heine (Crelle's Journal, 74), Georg Cantor (Annalen, 5), and Richard Dedekind. M\u00e9ray had taken in 1869 the same point of departure as Heine, but the theory is generally referred to the year 1872. Weierstrass's method has been completely set forth by Salvatore Pincherle in 1880,[27] and Dedekind's has received additional prominence through the author's later work (1888) and the endorsement by Paul Tannery (1894). Weierstrass, Cantor, and Heine base their theories on infinite series, while Dedekind founds his on the idea of a cut (Schnitt) in the system of all rational numbers, separating them into two groups having certain characteristic properties. The subject has received later contributions at the hands of Weierstrass, Leopold Kronecker (Crelle, 101), and Charles M\u00e9ray.The Egyptian mathematician Ab\u016b K\u0101mil Shuj\u0101 ibn Aslam (c. 850 \u2013 930) was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots.[22] In the 10th century, the Iraqi mathematician Al-Hashimi provided general proofs (rather than geometric demonstrations) for irrational numbers, as he considered multiplication, division, and other arithmetical functions.[23] Iranian mathematician, Ab\u016b Ja'far al-Kh\u0101zin (900\u2013971) provides a definition of rational and irrational magnitudes, stating that if a definite quantity is:[24]In contrast to Euclid's concept of magnitudes as lines, Al-Mahani considered integers and fractions as rational magnitudes, and square roots and cube roots as irrational magnitudes. He also introduced an arithmetical approach to the concept of irrationality, as he attributes the following to irrational magnitudes:[21]In the Middle ages, the development of algebra by Muslim mathematicians allowed irrational numbers to be treated as algebraic objects.[19] Middle Eastern mathematicians also merged the concepts of \"number\" and \"magnitude\" into a more general idea of real numbers, criticized Euclid's idea of ratios, developed the theory of composite ratios, and extended the concept of number to ratios of continuous magnitude.[20] In his commentary on Book 10 of the Elements, the Persian mathematician Al-Mahani (d. 874/884) examined and classified quadratic irrationals and cubic irrationals. He provided definitions for rational and irrational magnitudes, which he treated as irrational numbers. He dealt with them freely but explains them in geometric terms as follows:[21]During the 14th to 16th centuries, Madhava of Sangamagrama and the Kerala school of astronomy and mathematics discovered the infinite series for several irrational numbers such as \u03c0 and certain irrational values of trigonometric functions. Jye\u1e63\u1e6dhadeva provided proofs for these infinite series in the Yuktibh\u0101\u1e63\u0101.[18]Mathematicians like Brahmagupta (in 628 AD) and Bhaskara I (in 629 AD) made contributions in this area as did other mathematicians who followed. In the 12th century Bhaskara II evaluated some of these formulas and critiqued them, identifying their limitations.Later, in their treatises, Indian mathematicians wrote on the arithmetic of surds including addition, subtraction, multiplication, rationalization, as well as separation and extraction of square roots. (See Datta, Singh, Indian Journal of History of Science, 28(3), 1993).It is also suggested that Aryabhata (5th century AD), in calculating a value of pi to 5 significant figures, used the word \u0101sanna (approaching), to mean that not only is this an approximation but that the value is incommensurable (or irrational).It is suggested that the concept of irrationality was implicitly accepted by Indian mathematicians since the 7th century BC, when Manava (c. 750 \u2013 690 BC) believed that the square roots of numbers such as 2 and 61 could not be exactly determined.[16] However, historian Carl Benjamin Boyer writes that \"such claims are not well substantiated and unlikely to be true\".[17]Geometrical and mathematical problems involving irrational numbers such as square roots were addressed very early during the Vedic period in India. There are references to such calculations in the Samhitas, Brahmanas, and the Shulba Sutras (800 BC or earlier). (See Bag, Indian Journal of History of Science, 25(1-4), 1990).It wasn't until Eudoxus developed a theory of proportion that took into account irrational as well as rational ratios that a strong mathematical foundation of irrational numbers was created.[15]Theodorus of Cyrene proved the irrationality of the surds of whole numbers up to 17, but stopped there probably because the algebra he used couldn't be applied to the square root of 17.[14]As a result of the distinction between number and magnitude, geometry became the only method that could take into account incommensurable ratios. Because previous numerical foundations were still incompatible with the concept of incommensurability, Greek focus shifted away from those numerical conceptions such as algebra and focused almost exclusively on geometry. In fact, in many cases algebraic conceptions were reformulated into geometrical terms. This may account for why we still conceive of x2 or x3 as x squared and x cubed instead of x second power and x third power. Also crucial to Zeno\u2019s work with incommensurable magnitudes was the fundamental focus on deductive reasoning that resulted from the foundational shattering of earlier Greek mathematics. The realization that some basic conception within the existing theory was at odds with reality necessitated a complete and thorough investigation of the axioms and assumptions that underlie that theory. Out of this necessity, Eudoxus developed his method of exhaustion, a kind of reductio ad absurdum that \u201c\u2026established the deductive organization on the basis of explicit axioms\u2026\u201d as well as \u201c\u2026reinforced the earlier decision to rely on deductive reasoning for proof.\u201d[13] This method of exhaustion is the first step in the creation of calculus.The next step was taken by Eudoxus of Cnidus, who formalized a new theory of proportion that took into account commensurable as well as incommensurable quantities. Central to his idea was the distinction between magnitude and number. A magnitude \u201c...was not a number but stood for entities such as line segments, angles, areas, volumes, and time which could vary, as we would say, continuously. Magnitudes were opposed to numbers, which jumped from one value to another, as from 4 to 5.\u201d[11] Numbers are composed of some smallest, indivisible unit, whereas magnitudes are infinitely reducible. Because no quantitative values were assigned to magnitudes, Eudoxus was then able to account for both commensurable and incommensurable ratios by defining a ratio in terms of its magnitude, and proportion as an equality between two ratios. By taking quantitative values (numbers) out of the equation, he avoided the trap of having to express an irrational number as a number. \u201cEudoxus\u2019 theory enabled the Greek mathematicians to make tremendous progress in geometry by supplying the necessary logical foundation for incommensurable ratios.\u201d[12] This incommensurability is dealt with in Euclid's Elements, Book X, Proposition 9.The discovery of incommensurable ratios was indicative of another problem facing the Greeks: the relation of the discrete to the continuous. Brought into light by Zeno of Elea, who questioned the conception that quantities are discrete and composed of a finite number of units of a given size. Past Greek conceptions dictated that they necessarily must be, for \u201cwhole numbers represent discrete objects, and a commensurable ratio represents a relation between two collections of discrete objects.\u201d[10] However Zeno found that in fact \u201c[quantities] in general are not discrete collections of units; this is why ratios of incommensurable [quantities] appear\u2026.[Q]uantities are, in other words, continuous.\u201d[10] What this means is that, contrary to the popular conception of the time, there cannot be an indivisible, smallest unit of measure for any quantity. That in fact, these divisions of quantity must necessarily be infinite. For example, consider a line segment: this segment can be split in half, that half split in half, the half of the half in half, and so on. This process can continue infinitely, for there is always another half to be split. The more times the segment is halved, the closer the unit of measure comes to zero, but it never reaches exactly zero. This is just what Zeno sought to prove. He sought to prove this by formulating four paradoxes, which demonstrated the contradictions inherent in the mathematical thought of the time. While Zeno\u2019s paradoxes accurately demonstrated the deficiencies of current mathematical conceptions, they were not regarded as proof of the alternative. In the minds of the Greeks, disproving the validity of one view did not necessarily prove the validity of another, and therefore further investigation had to occur.Greek mathematicians termed this ratio of incommensurable magnitudes alogos, or inexpressible. Hippasus, however, was not lauded for his efforts: according to one legend, he made his discovery while out at sea, and was subsequently thrown overboard by his fellow Pythagoreans \u201c\u2026for having produced an element in the universe which denied the\u2026doctrine that all phenomena in the universe can be reduced to whole numbers and their ratios.\u201d[9] Another legend states that Hippasus was merely exiled for this revelation. Whatever the consequence to Hippasus himself, his discovery posed a very serious problem to Pythagorean mathematics, since it shattered the assumption that number and geometry were inseparable\u2013a foundation of their theory.The first proof of the existence of irrational numbers is usually attributed to a Pythagorean (possibly Hippasus of Metapontum),[5] who probably discovered them while identifying sides of the pentagram.[6] The then-current Pythagorean method would have claimed that there must be some sufficiently small, indivisible unit that could fit evenly into one of these lengths as well as the other. However, Hippasus, in the 5th century BC, was able to deduce that there was in fact no common unit of measure, and that the assertion of such an existence was in fact a contradiction. He did this by demonstrating that if the hypotenuse of an isosceles right triangle was indeed commensurable with a leg, then one of those lengths measured in that unit of measure must be both odd and even, which is impossible. His reasoning is as follows:As a consequence of Cantor's proof that the real numbers are uncountable and the rationals countable, it follows that almost all real numbers are irrational.[4]Irrational numbers may also be dealt with via non-terminating continued fractions.It can be shown that irrational numbers, when expressed in a positional numeral system (e.g. as decimal numbers, or with any other natural basis), do not terminate, nor do they repeat, i.e., do not contain a subsequence of digits, the repetition of which makes up the tail of the representation. For example, the decimal representation of the number \u03c0 starts with 3.14159, but no finite number of digits can represent \u03c0 exactly, nor does it repeat. The proof that the decimal expansion of a rational number must terminate or repeat is distinct from the proof that a decimal expansion that terminates or repeats must be a rational number, and although elementary and not lengthy, both proofs take some work. Mathematicians do not generally take \"terminating or repeating\" to be the definition of the concept of rational number.Among irrational numbers are the ratio \u03c0 of a circle's circumference to its diameter, Euler's number e, the golden ratio \u03c6, and the square root of two;[1][2][3] in fact all square roots of natural numbers, other than of perfect squares, are irrational.In mathematics, the irrational numbers are all the real numbers which are not rational numbers, the latter being the numbers constructed from ratios (or fractions) of integers. When the ratio of lengths of two line segments is an irrational number, the line segments are also described as being incommensurable, meaning that they share no \"measure\" in common, that is, there is no length (\"the measure\"), no matter how short, that could be used to express the lengths of both of the two given segments as integer multiples of itself.",
            "title": "Irrational number",
            "url": "https://en.wikipedia.org/wiki/Irrational_number"
        },
        {
            "desc_links": [
                "/wiki/Mathematical_analysis",
                "/wiki/Dense_set",
                "/wiki/Completion_(metric_space)",
                "/wiki/Cauchy_sequence",
                "/wiki/Dedekind_cut",
                "/wiki/Decimal",
                "/wiki/Addition",
                "/wiki/Multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Integer",
                "/wiki/Prime_field",
                "/wiki/Characteristic_zero",
                "/wiki/Field_extension",
                "/wiki/Algebraic_number_field",
                "/wiki/Algebraic_closure",
                "/wiki/Algebraic_number",
                "/wiki/Formalism_(mathematics)",
                "/wiki/Equivalence_class",
                "/wiki/Equivalence_relation",
                "/wiki/Iff",
                "/wiki/Real_number",
                "/wiki/Irrational_number",
                "/wiki/Square_root_of_2",
                "/wiki/Pi",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Golden_ratio",
                "/wiki/Countable_set",
                "/wiki/Uncountable_set",
                "/wiki/Almost_all",
                "/wiki/Decimal_expansion",
                "/wiki/Numerical_digit",
                "/wiki/Repeating_decimal",
                "/wiki/Sequence",
                "/wiki/Decimal",
                "/wiki/Radix",
                "/wiki/Binary_numeral_system",
                "/wiki/Hexadecimal",
                "/wiki/Mathematics",
                "/wiki/Number",
                "/wiki/Quotient",
                "/wiki/Fraction_(mathematics)",
                "/wiki/Integer",
                "/wiki/Numerator",
                "/wiki/Denominator",
                "/wiki/Set_(mathematics)",
                "/wiki/Blackboard_bold",
                "/wiki/Giuseppe_Peano",
                "/wiki/Quotient"
            ],
            "links": [
                "/wiki/P-adic_number",
                "/wiki/Ostrowski%27s_theorem",
                "/wiki/Absolute_value_(algebra)",
                "/wiki/P-adic_number",
                "/wiki/Metric_space",
                "/wiki/Prime_number",
                "/wiki/Divisor",
                "/wiki/Order_topology",
                "/wiki/Subspace_topology",
                "/wiki/Metric_space",
                "/wiki/Absolute_difference",
                "/wiki/Topological_field",
                "/wiki/Locally_compact",
                "/wiki/Countable",
                "/wiki/Topological_property",
                "/wiki/Isolated_point",
                "/wiki/Totally_disconnected_space",
                "/wiki/Completeness_(topology)",
                "/wiki/Real_numbers",
                "/wiki/Dense_set",
                "/wiki/Finite_set",
                "/wiki/Continued_fraction",
                "/wiki/Totally_ordered",
                "/wiki/Order_isomorphism",
                "/wiki/Densely_ordered",
                "/wiki/Countable",
                "/wiki/Almost_all",
                "/wiki/Lebesgue_measure",
                "/wiki/Null_set",
                "/wiki/Algebraic_closure",
                "/wiki/Algebraic_number",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Prime_field",
                "/wiki/Field_(mathematics)",
                "/wiki/Field_of_fractions",
                "/wiki/Integer",
                "/wiki/Embedding",
                "/wiki/Total_order",
                "/wiki/And_(logic)",
                "/wiki/Or_(logic)",
                "/wiki/Greatest_common_divisor",
                "/wiki/Coprime",
                "/wiki/Congruence_relation",
                "/wiki/Quotient_set",
                "/wiki/Integral_domain",
                "/wiki/Field_of_fractions",
                "/wiki/Equivalence_class",
                "/wiki/Ordered_pair",
                "/wiki/Integer",
                "/wiki/Quotient_space_(topology)",
                "/wiki/Coefficient",
                "/wiki/Euclidean_algorithm",
                "/wiki/Multiplicative_inverse",
                "/wiki/Multiplicative_inverse",
                "/wiki/Additive_inverse",
                "/wiki/Reducible_fraction",
                "/wiki/Coprime_integers",
                "/wiki/Coprime_integers",
                "/wiki/Greatest_common_divisor",
                "/wiki/Irreducible_fraction",
                "/wiki/Coprime_integers",
                "/wiki/Canonical_form",
                "/wiki/Ratio",
                "/wiki/Coefficient",
                "/wiki/Rational_point",
                "/wiki/Coordinates",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rational_fraction",
                "/wiki/Rational_function",
                "/wiki/Rational_curve",
                "/wiki/Mathematical_analysis",
                "/wiki/Dense_set",
                "/wiki/Completion_(metric_space)",
                "/wiki/Cauchy_sequence",
                "/wiki/Dedekind_cut",
                "/wiki/Decimal",
                "/wiki/Addition",
                "/wiki/Multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Integer",
                "/wiki/Prime_field",
                "/wiki/Characteristic_zero",
                "/wiki/Field_extension",
                "/wiki/Algebraic_number_field",
                "/wiki/Algebraic_closure",
                "/wiki/Algebraic_number",
                "/wiki/Formalism_(mathematics)",
                "/wiki/Equivalence_class",
                "/wiki/Equivalence_relation",
                "/wiki/Iff",
                "/wiki/Real_number",
                "/wiki/Irrational_number",
                "/wiki/Square_root_of_2",
                "/wiki/Pi",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Golden_ratio",
                "/wiki/Countable_set",
                "/wiki/Uncountable_set",
                "/wiki/Almost_all",
                "/wiki/Decimal_expansion",
                "/wiki/Numerical_digit",
                "/wiki/Repeating_decimal",
                "/wiki/Sequence",
                "/wiki/Decimal",
                "/wiki/Radix",
                "/wiki/Binary_numeral_system",
                "/wiki/Hexadecimal"
            ],
            "text": "The metric space (Q,dp) is not complete, and its completion is the p-adic number field Qp. Ostrowski's theorem states that any non-trivial absolute value on the rational numbers Q is equivalent to either the usual real absolute value or a p-adic absolute value.Then dp(x,y) = |x \u2212 y|p defines a metric on Q.In addition set |0|p = 0. For any rational number a/b, we set |a/b|p = |a|p / |b|p.Let p be a prime number and for any non-zero integer a, let |a|p = p\u2212n, where pn is the highest power of p dividing a.In addition to the absolute value metric mentioned above, there are other metrics which turn Q into a topological field:By virtue of their order, the rationals carry an order topology. The rational numbers, as a subspace of the real numbers, also carry a subspace topology. The rational numbers form a metric space by using the absolute difference metric d(x,y) = |x \u2212 y|, and this yields a third topology on Q. All three topologies coincide and turn the rationals into a topological field. The rational numbers are an important example of a space which is not locally compact. The rationals are characterized topologically as the unique countable metrizable space without isolated points. The space is also totally disconnected. The rational numbers do not form a complete metric space; the real numbers are the completion of Q under the metric d(x,y) = |x \u2212 y|, above.The rationals are a dense subset of the real numbers: every real number has rational numbers arbitrarily close to it. A related property is that rational numbers are the only numbers with finite expansions as regular continued fractions.Any totally ordered set which is countable, dense (in the above sense), and has no least or greatest element is order isomorphic to the rational numbers.The rationals are a densely ordered set: between any two rationals, there sits another one, and, therefore, infinitely many other ones. For example, for any two fractions such thatThe set of all rational numbers is countable. Since the set of all real numbers is uncountable, we say that almost all real numbers are irrational, in the sense of Lebesgue measure, i.e. the set of rational numbers is a null set.The algebraic closure of Q, i.e. the field of roots of rational polynomials, is the algebraic numbers.The rationals are the smallest field with characteristic zero: every other field of characteristic zero contains a copy of Q. The rational numbers are therefore the prime field for characteristic zero.The set Q, together with the addition and multiplication operations shown above, forms a field, the field of fractions of the integers Z.The integers may be considered to be rational numbers by the embedding that maps m to [(m,1)].We can also define a total order on Q. Let \u2227 be the and-symbol and \u2228 be the or-symbol. We say that [(m1,n1)] \u2264 [(m2,n2)] if:The canonical choice for [(m,n)] is chosen so that n is positive and gcd(m,n) = 1, i.e. m and n share no common factors, i.e. m and n are coprime. For example, we would write [(1,2)] instead of [(2,4)] or [(\u221212,\u221224)], even though [(1,2)] = [(2,4)] = [(\u221212,\u221224)].The equivalence relation (m1,n1) ~ (m2,n2) if, and only if, m1n2 \u2212 m2n1 = 0 is a congruence relation, i.e. it is compatible with the addition and multiplication defined above, and we may define Q to be the quotient set (Z \u00d7 (Z \\ {0})) / ~, i.e. we identify two pairs (m1,n1) and (m2,n2) if they are equivalent in the above sense. (This construction can be carried out in any integral domain: see field of fractions.) We denote by [(m1,n1)] the equivalence class containing (m1,n1). If (m1,n1) ~ (m2,n2) then, by definition, (m1,n1) belongs to [(m2,n2)] and (m2,n2) belongs to [(m1,n1)]; in this case we can write [(m1,n1)] = [(m2,n2)]. Given any equivalence class [(m,n)] there are a countably infinite number of representation, sinceand, if m2 \u2260 0, division byMathematically we may construct the rational numbers as equivalence classes of ordered pairs of integers (m,n), with n \u2260 0. This space of equivalence classes is the quotient space (Z \u00d7 (Z \\ {0})) / ~, where (m1,n1) ~ (m2,n2) if, and only if, m1n2 \u2212 m2n1 = 0. We can define addition and multiplication of these pairs with the following rules:are different ways to represent the same rational value.where an are integers. Every rational number a/b can be represented as a finite continued fraction, whose coefficients an can be determined by applying the Euclidean algorithm to (a,b).A finite continued fraction is an expression such asIf a \u2260 0, thenThe result is in canonical form if the same is true for a/b. In particular,If n is a non-negative integer, thenThus, dividing a/b by c/d is equivalent to multiplying a/b by the reciprocal of c/d:If both b and c are nonzero, the division rule isA nonzero rational number a/b has a multiplicative inverse, also called its reciprocal,If a/b is in canonical form, the same is true for its opposite.Every rational number a/b has an additive inverse, often called its opposite,Even if both fractions are in canonical form, the result may be a reducible fraction.The rule for multiplication is:If both fractions are in canonical form, the result is in canonical form if and only if b and d are coprime integers.If both fractions are in canonical form, the result is in canonical form if and only if b and d are coprime integers.Two fractions are added as follows:If either denominator is negative, each fraction with a negative denominator must first be converted into an equivalent form with a positive denominator by changing the signs of both its numerator and denominator.If both denominators are positive, and, in particular, if both fractions are in canonical form,If both fractions are in canonical form thenAny integer n can be expressed as the rational number n/1, which is its canonical form as a rational number.Starting from a rational number a/b, its canonical form may be obtained by dividing a and b by their greatest common divisor, and, if b < 0, changing the sign of the resulting numerator and denominator.Every rational number may be expressed in a unique way as an irreducible fraction a/b, where a and b are coprime integers, and b > 0. This is often called the canonical form.The term rational in reference to the set Q refers to the fact that a rational number represents a ratio of two integers. In mathematics, \"rational\" is often used as a noun abbreviating \"rational number\". The adjective rational sometimes means that the coefficients are rational numbers. For example, a rational point is a point with rational coordinates (that is a point whose coordinates are rational numbers; a rational matrix is a matrix of rational numbers; a rational polynomial may be a polynomial with rational coefficients, although the term \"polynomial over the rationals\" is generally preferred, for avoiding confusion with \"rational expression\" and \"rational function\" (a polynomial is a rational expression and defines a rational function, even if its coefficients are not rational numbers). However, a rational curve is not a curve defined over the rationals, but a curve which can be parameterized by rational functions.In mathematical analysis, the rational numbers form a dense subset of the real numbers. The real numbers can be constructed from the rational numbers by completion, using Cauchy sequences, Dedekind cuts, or infinite decimals.Rational numbers together with addition and multiplication form a field which contains the integers and is contained in any field containing the integers. In other words, the field of rational numbers is a prime field, and a field has characteristic zero if and only if it contains the rational numbers as a subfield. Finite extensions of Q are called algebraic number fields, and the algebraic closure of Q is the field of algebraic numbers.[3]Rational numbers can be formally defined as equivalence classes of pairs of integers (p, q) such that q \u2260 0, for the equivalence relation defined by (p1, q1) ~ (p2, q2) if, and only if p1q2 = p2q1. With this formal definition, the fraction p/q becomes the standard notation for the equivalence class of (p2, q2).A real number that is not rational is called irrational. Irrational numbers include \u221a2, \u03c0, e, and \u03c6. The decimal expansion of an irrational number continues without repeating. Since the set of rational numbers is countable, and the set of real numbers is uncountable, almost all real numbers are irrational.[1]The decimal expansion of a rational number always either terminates after a finite number of digits or begins to repeat the same finite sequence of digits over and over. Moreover, any repeating or terminating decimal represents a rational number. These statements hold true not just for base 10, but also for any other integer base (e.g. binary, hexadecimal).",
            "title": "Rational number",
            "url": "https://en.wikipedia.org/wiki/Rational_number"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Algebraic number",
            "url": "https://en.wikipedia.org/wiki/Algebraic_number"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Fundamental theorem of algebra",
            "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra"
        },
        {
            "desc_links": [
                "/wiki/Surjective_function",
                "/wiki/Injective_function",
                "/wiki/Matrix_(math)",
                "/wiki/Matrix_factorization",
                "/wiki/LU_decomposition",
                "/wiki/Lower_triangular_matrix",
                "/wiki/Upper_triangular_matrix",
                "/wiki/Permutation_matrix",
                "/wiki/Gaussian_elimination",
                "/wiki/Commutative_ring",
                "/wiki/Unique_factorization_domain",
                "/wiki/Number_system",
                "/wiki/Ring_of_algebraic_integers",
                "/wiki/Dedekind_domain",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Prime_ideal",
                "/wiki/Polynomial_factorization",
                "/wiki/Zero_of_a_function",
                "/wiki/Field_(mathematics)",
                "/wiki/Unique_factorization_domain",
                "/wiki/Irreducible_polynomial",
                "/wiki/Univariate_polynomial",
                "/wiki/Complex_number",
                "/wiki/Linear_polynomial",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Root-finding_algorithm",
                "/wiki/Computer_algebra",
                "/wiki/Algorithm",
                "/wiki/Factorization_of_polynomials",
                "/wiki/Greek_mathematics",
                "/wiki/Fundamental_theorem_of_arithmetic",
                "/wiki/Prime_number",
                "/wiki/Up_to",
                "/wiki/Integer_factorization",
                "/wiki/Integer_factorization",
                "/wiki/RSA_cryptosystem",
                "/wiki/Public-key_cryptography",
                "/wiki/Division_ring",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Rational_function",
                "/wiki/Mathematics",
                "/wiki/American_and_British_English_spelling_differences#-ise,_-ize_(-isation,_-ization)",
                "/wiki/Mathematical_object",
                "/wiki/Integer",
                "/wiki/Polynomial"
            ],
            "links": [
                "/wiki/Matrix_decomposition",
                "/wiki/Matrix_(mathematics)",
                "/wiki/LU_decomposition",
                "/wiki/Lower_triangular_matrix",
                "/wiki/Upper_triangular_matrix",
                "/wiki/Permutation_matrix",
                "/wiki/Dedekind",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Prime_ideal",
                "/wiki/Integral_domain",
                "/wiki/Dedekind_domain",
                "/wiki/Fermat%27s_Last_Theorem",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Irreducible_element",
                "/wiki/Algebraic_number_theory",
                "/wiki/Diophantine_equation",
                "/wiki/Integer",
                "/wiki/Algebraic_integer",
                "/wiki/Ring_of_algebraic_integers",
                "/wiki/Gaussian_integer",
                "/wiki/Eisenstein_integer",
                "/wiki/Principal_ideal_domain",
                "/wiki/Unique_factorization_domain",
                "/wiki/Euclidean_algorithm",
                "/wiki/Field_(mathematics)",
                "/wiki/Euclidean_domain",
                "/wiki/Euclidean_division",
                "/wiki/Greatest_common_divisor",
                "/wiki/Principal_ideal_domain",
                "/wiki/Field_(mathematics)",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Irreducible_element",
                "/wiki/Prime_number",
                "/wiki/Integral_domain",
                "/wiki/Unique_factorization_domain",
                "/wiki/Galois_theory",
                "/wiki/Vieta%27s_formulas",
                "/wiki/Cubic_function",
                "/wiki/Quartic_function",
                "/wiki/Abel%E2%80%93Ruffini_theorem",
                "/wiki/Algebraic_formula",
                "/wiki/Field_(mathematics)",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Finite_field",
                "/wiki/Quadratic_polynomial",
                "/wiki/Rational_number",
                "/wiki/Polynomial_long_division",
                "/wiki/Synthetic_division",
                "/wiki/Zero_of_a_function",
                "/wiki/Polynomial",
                "/wiki/Coprime_integers",
                "/wiki/Rational_number",
                "/wiki/Primitive_polynomial_(ring_theory)",
                "/wiki/Greatest_common_divisor",
                "/wiki/Algorithm",
                "/wiki/Computer_algebra",
                "/wiki/Factorization_of_polynomials",
                "/wiki/Integer",
                "/wiki/Rational_number",
                "/wiki/Fundamental_theorem_of_arithmetic",
                "/wiki/Unique_factorization_domain",
                "/wiki/Root-finding_algorithm",
                "/wiki/Algorithm",
                "/wiki/Complex_conjugate",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Factor_theorem",
                "/wiki/Algebraic_equation",
                "/wiki/Cosine",
                "/wiki/Algebraic_number",
                "/wiki/Nth_root",
                "/wiki/Galois_group",
                "/wiki/Trigonometric_formulas",
                "/wiki/Identity_(mathematics)",
                "/wiki/Binomial_(polynomial)",
                "/wiki/Distributive_property",
                "/wiki/Greatest_common_divisor",
                "/wiki/Polynomial",
                "/wiki/Monomial",
                "/wiki/Monomial",
                "/wiki/Binomial_(polynomial)",
                "/wiki/Trinomial",
                "/wiki/Equation",
                "/wiki/Muhammad_ibn_Musa_al-Khwarizmi",
                "/wiki/The_Compendious_Book_on_Calculation_by_Completion_and_Balancing",
                "/wiki/Quadratic_equation",
                "/wiki/Thomas_Harriot",
                "/wiki/Zero_of_a_function",
                "/wiki/Expression_(mathematics)",
                "/wiki/Algebra",
                "/wiki/Equation",
                "/wiki/RSA_cryptosystem",
                "/wiki/Internet",
                "/wiki/Decimal_digit",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Fermat_number",
                "/wiki/Sieve_of_Eratosthenes",
                "/wiki/Algorithm",
                "/wiki/Divisor",
                "/wiki/Fundamental_theorem_of_arithmetic",
                "/wiki/Integer",
                "/wiki/Prime_number",
                "/wiki/Surjective_function",
                "/wiki/Injective_function",
                "/wiki/Matrix_(math)",
                "/wiki/Matrix_factorization",
                "/wiki/LU_decomposition",
                "/wiki/Lower_triangular_matrix",
                "/wiki/Upper_triangular_matrix",
                "/wiki/Permutation_matrix",
                "/wiki/Gaussian_elimination",
                "/wiki/Commutative_ring",
                "/wiki/Unique_factorization_domain",
                "/wiki/Number_system",
                "/wiki/Ring_of_algebraic_integers",
                "/wiki/Dedekind_domain",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Prime_ideal",
                "/wiki/Polynomial_factorization",
                "/wiki/Zero_of_a_function",
                "/wiki/Field_(mathematics)",
                "/wiki/Unique_factorization_domain",
                "/wiki/Irreducible_polynomial",
                "/wiki/Univariate_polynomial",
                "/wiki/Complex_number",
                "/wiki/Linear_polynomial",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Root-finding_algorithm",
                "/wiki/Computer_algebra",
                "/wiki/Algorithm",
                "/wiki/Factorization_of_polynomials",
                "/wiki/Greek_mathematics",
                "/wiki/Fundamental_theorem_of_arithmetic",
                "/wiki/Prime_number",
                "/wiki/Up_to",
                "/wiki/Integer_factorization",
                "/wiki/Integer_factorization",
                "/wiki/RSA_cryptosystem",
                "/wiki/Public-key_cryptography",
                "/wiki/Mathematics",
                "/wiki/American_and_British_English_spelling_differences#-ise,_-ize_(-isation,_-ization)",
                "/wiki/Mathematical_object",
                "/wiki/Integer",
                "/wiki/Polynomial"
            ],
            "text": "See Matrix decomposition for the main matrix factorizations that are commonly considered.There are, in general, many ways of writing a matrix as a product of matrices. Thus, for matrices, the factorization problem consists of finding factors of specified types. For example the LU decomposition of a matrix consist of factoring it as the product of a lower triangular matrix by an upper triangular matrix. As this is not always possible, one generally consider the \"LUP decomposition\" for which a third factor is added, which is a permutation matrix.This has been solved by Dedekind, who proved that the rings of algebraic integers have unique factorization of ideals, that is, in these rings, every ideal is a product of prime ideals, and this factorization is unique up the order of the factors. The integral domains that have this unique factorization property are now called Dedekind domains. They have many nice properties that may them fundamental in algebraic number theory.This lack of unique factorization, is a major difficulty for solving Diophantine equations. For example, many wrong proofs of Fermat's Last Theorem (probably including Fermat's \"truly marvelous proof of this, which this margin is too narrow to contain\") were based on the implicit supposition that unique factorization would be always true.and all these factors are irreducible.In algebraic number theory, the study of Diophantine equations led mathematicians, during 19th century, to introduce generalizations of the integers called algebraic integers. The first ring of algebraic integers that have been considered were Gaussian integers and Eisenstein integers, which share with usual integers the property of being principal ideal domains, and have thus the unique factorization property.In a Euclidean domain, Euclidean division allows defining an Euclidean algorithm for computing greatest common divisors. However this does not implies the existence of a factorization algorithm. There is an explicit example of a field F such that there cannot exist any factorization algorithm in the Euclidean domain F[x] of the univariate polynomials over F.An Euclidean domain is a integral domain on which is defined a Euclidean division similar to that of integers. Every Euclidean domain is a principal ideal domain, and thus a UFD.Greatest common divisors exist in UFDs, and conversely, every integral domain in which greatest common divisors exist is an UFD. Every principal ideal domain is an UFD.The integers and the polynomials over a field share the property of unique factorization, that is, every nonzero element may be factorized into a product of an invertible element (a unit, 1 or \u20131 in the case of integers) and a product of irreducible elements (prime numbers, in the case of integers), and this factorization is unique up the order of the factors and the product of any irreducible factor by a unit (and the product of the unit factor by the inverse of the same unit). Many integral domains share this property, and they are called unique factorization domain, often abbreviated as UFD.where Q is a polynomial.It may occur that one knows some relationship between the roots of a polynomial and its coefficients. Using this knowledge may help factoring the polynomial and finding its roots. Galois theory is based on a systematic study of the relations between roots and coefficients, that include Vieta's formulas.There are also formulas for cubic and quartic polynomials, which are, in general, too large for being of any practical use. Abel\u2013Ruffini theorem shows that there are no general algebraic formulas in higher degree.The quadratic formula works similarly when the coefficients belong to field of characteristic different of two, and, in particular, for coefficients in a finite field with an odd number of elements.[9]and the factorizationInspection of the factors of ac = 36 leads to 4 + 9 = 13 = b, giving the two rootsFor example, let consider the quadratic polynomialChecking all pairs of integers whose product is ac gives the rational roots, if any.that isLet consider the quadratic polynomialFor quadratic polynomials, the above method may be adapted, leading to the so called ac method of factorization.[8]For example, if the polynomialthe factor theorem shows that one has a factorizationSearching rational roots of a polynomial makes sense only for polynomials with rational coefficients. Primitive part-content factorization (see above) reduces the problem of searching rational roots to the case of polynomials with integer coefficients such that the greatest common divisor of the coefficients is one,This may be useful when, either by inspection, or by using some external information, one knows a root of the polynomial. For computing Q(x), instead of using the above formula, one may also use polynomial long division or synthetic division.for i = 1, ..., n \u2013 1.where(that is P(r) = 0 ), then there is a factorizationThe factor theorem states that, if r is a root of a polynomialThis factorization may produce a result that is larger than the original polynomial (typically when there are many coprime denominators), but, even when this is the case, the primitive part is generally easier to manipulate for further factorization.Every polynomial with rational coefficients, may be factorized, in a unique way, as the product of a rational number and a polynomial with integer coefficients, which is primitive (that is, the greatest common divisor of the coefficients is 1), and has a positive leading coefficient (coefficient of the term of the highest degree). For example:There are efficient algorithms for computing this factorization, which are implemented in most computer algebra systems. See Factorization of polynomials. Unfortunately, for a paper-and-pencil computation, these algorithms are too complicate for being usable. Beside general heuristics that are described above, only a few methods are available in this case, which generally work only for polynomials of low degree, with few nonzero coefficients. The main such methods are described in next subsections.Most algebraic equations that are encountered in practice have integer or rational coefficients, and one may want a factorization with factors of the same kind. The fundamental theorem of arithmetic may be generalized to this case. That is, polynomials with integer or rational coefficients have the unique factorization property. More precisely, every polynomial with rational coefficients may be factorized in a productFor computing these real or complex factorizations, one has to know the roots of the polynomial. In general, they may not be computed exactly, and only approximative values of the roots may be obtained. See Root-finding algorithm for a summary of the numerous efficient algorithms that have been designed for this purpose.is a factor of P that has real coefficients. This grouping of non-real factors may be continued until getting eventually a factorization with real factors that are polynomials of degrees one or two.If the coefficients of P are real, one want generally a factorization where factors have real coefficients. In this case, the factors of the complete factorization may have some factors that have the degree two. This factorization may easily be deduced form the above complete factorization. In fact, if r = a + ib is a non-real root of P, then its complex conjugate s = a - ib is also a root of P. So, the productIf the coefficients of P are real or complex numbers, the fundamental theorem of algebra asserts that P has a real or complex root. Using the factor theorem recursively, it results thatwhere Q(x) is the quotient of Euclidean division of P by x \u2013 r.Conversely, the factor theorem asserts that, if r is a root of P, then P may be factored asis a factorization of P as a product of two polynomials, then the roots of P are the union of the roots of Q and the roots of R. Thus solving P is reduced to the simpler problems of solving Q and R.IfwhereFor polynomials, factorization is strongly related with the problem of solving algebraic equations. An algebraic equation has the formsince the divisors of 6 are 1, 2, 3, 6, and the divisors of 12 that do not divide 6 are 4 and 12.For example,The cosines that appear in these factorizations are algebraic numbers, and may be expressed in terms of radicals (this is possible, because their Galois group is cyclic), however these radical expressions are too complicate for being used, except for low values of n. For exampleone has the following real factorizations (one pass to one to the other by changing k into n \u2013 k or n + 1 \u2013 k, and applying usual trigonometric formulas)It follows that for any two expressions E and F, one has:Below are identities whose left-hand-side are commonly used as patterns (this means that the variables E and F that appear in these identities may represent any subexpression of the expression that has to be factorized.[6]Many identities provide an equality between a sum and a product. Above methods may be used for letting appear in an expression the sum side of some identity, which may therefore be replaced by a product.Another grouping gives the factorizationSometimes, some term grouping lets appear a part of a Recognizable pattern. It is then useful to add terms for completing the pattern, and subtract them for not changing the value of the expression.In general, this works for sums of 4 terms that have been obtained as the product of two binomials. Although not frequently, this may work also for more complicated examples.Then a simple inspection shows the common factor x + 5, leading to the factorizationone may remark that the first two terms have a common factor x, and the last two terms have the common factor y. ThusFor example, to factorGrouping terms may allow using other methods for getting a factorization.For example,[5]It may occur that all terms of a sum are products and that some factors are common to all terms. In this case, the distributive law allows factoring out this common factor. If there are several such common factors, it is worth to divide out the greatest such common factor. Also, if there are integer coefficients, one may factor out the greatest common divisor of these coefficients.The methods that are described below apply to any expression that is a sum, or may be transformed into a sum. Therefore, they are most often applied to polynomials, even if they may applied also when the terms of the sum are not monomials, that is product of variables and constantsIn his book Artis Analyticae Praxis ad Aequationes Algebraicas Resolvendas, Harriot drew, in a first section, tables for addition, subtraction, multiplication and division of monomials, binomials, and trinomials. Then, in a second section, he set up the equation aa \u2212 ba + ca = + bc, and showed that this matches the form of multiplication he had previously provided, giving the factorization (a \u2212 b)(a + c).[4]The systematic use of algebraic manipulations for simplifying expressions (more specifically equations)) may be dated to 9th century, with al-Khwarizmi's book The Compendious Book on Calculation by Completion and Balancing, which is titled with two such types of manipulation. However, even for solving quadratic equations, factoring method was not used before Harriot\u2019s work published in 1631, ten years after his death.[3]Various methods have been developed for finding factorizations; some are described below.Moreover, the factored form gives immediately the roots of the polynomial in x represented by these expressions.may be factored into the much simpler expression (two multiplications and three additions instead of 16 multiplications, 4 subtractions and 3 additions)Manipulating expressions is the basis of algebra. Factorization is one of the most important methods for expression manipulation for several reasons. If one can put an equation in a factored form E\u22c5F = 0, then the solving problem splits into two independent (and generally easier) problems E = 0 and F = 0. When an expression can be factored, the factors are often much simpler, and may, therefore, offer some insight on the problem. For example,For factoring n = 1386 into primes:There are more efficient factoring algorithms. However they remain relatively inefficient, as, with the present state of the art, one cannot factorize, even with the more powerful computers, a number of 500 decimal digits that is the product of two randomly chosen prime numbers. This insures the security of the RSA cryptosystem, which is widely used for secure internet communication.is not a prime number. In fact, applying the above method would require more than 10,000 divisions, for a number that has 10 decimal digits.This method works well for factoring small integers, but is inefficient for larger integers. For example, Pierre de Fermat was unable to discover that the 6th Fermat numberThere is no need to test all values of q for applying the method. In principle, it suffices to test only prime divisors. This needs to have a table of prime numbers that may be generated for example with the sieve of Eratosthenes. As the method of factorization does essentially the same work as the sieve of Eratosthenes, it is generally more efficient to test for a divisor only those numbers for which it is not immediately clear whether they are prime or not. Typically, one may proceed by testing 2, 3, 5, and the numbers > 5, whose last digit is 1, 3, 7, 9 and the sum of digits is not a multiple of 3.If one tests the values of q in increasing order, the first divisor that is found is necessarily a prime number, and the cofactor r = n / q cannot have any divisor smaller than q. For getting the complete factorization, it suffices thus to continue the algorithm by searching a divisor of r that is not smaller than q and not greater than \u221ar.For finding a divisor q of n, if any, it suffices to test all values of q such that 1 < q and q2 \u2264 n. In fact, if r is a divisor of n such that r2 > n, then q = n / r is a divisor of n such that q2 \u2264 n.For computing the factorization of an integer n, one needs an algorithm for finding a divisor q of n or deciding that n is prime. When such a divisor is found, the repeated application of this algorithm to the factors q and n / q gives eventually the complete factorization of n.[1]By the fundamental theorem of arithmetic, every integer greater than 1 has a unique (up to the order of the factors) factorization into prime numbers, which are those integers which cannot be further factorized into the product of integers greater than one.Factorization may also refer to more general decompositions of a mathematical object into the product of smaller or simpler objects. For example, every function may be factored into the composition of a surjective function with an injective function. Matrices possess many kinds of matrix factorizations. For example, every matrix has a unique LUP factorization as a product of a lower triangular matrix L with all diagonal entries equal to one, an upper triangular matrix U, and a permutation matrix P; this is a matrix formulation of Gaussian elimination.A commutative ring possessing the unique factorization property is called a unique factorization domain. There are number systems, such as certain rings of algebraic integers, which are not unique factorization domains. However, rings of algebraic integers satisfy the weaker property of Dedekind domains: ideals factor uniquely into prime ideals.Polynomial factorization has also been studied for centuries. In elementary algebra, factoring a polynomial reduces the problem of finding its roots to finding the roots of the factors. Polynomials with coefficients in the integers or in a field possess the unique factorization property, a version of the fundamental theorem of arithmetic with prime numbers replaced by irreducible polynomials. In particular, a univariate polynomial with complex coefficients admits a unique (up to ordering) factorization into linear polynomials: this is a version of the fundamental theorem of algebra. In this case, the factorization can be done with root-finding algorithms. The case of polynomials with integer coefficients is fundamental for computer algebra. There are efficient computer algorithms for computing (complete) factorizations within the ring of polynomials with rational number coefficients (see factorization of polynomials).Factorization was first considered by ancient Greek mathematicians in the case of integers. They proved the fundamental theorem of arithmetic, which asserts that every positive integer may be factored into a product of prime numbers, which cannot be further factored into integers greater than 1. Moreover, this factorization is unique up to the order of the factors. Although integer factorization is a sort of inverse to multiplication, it is much more difficult algorithmically, a fact which is exploited in the RSA cryptosystem to implement public-key cryptography.In mathematics, factorization (also factorisation in some forms of British English) or factoring consists of writing a number or another mathematical object as a product of several factors, usually smaller or simpler objects of the same kind. For example, 3 \u00d7 5 is a factorization of the integer 15, and (x \u2013 2)(x + 2) is a factorization of the polynomial x2 \u2013 4.",
            "title": "Factorization",
            "url": "https://en.wikipedia.org/wiki/Factorization"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Alternating_map",
                "/wiki/Multilinear",
                "/wiki/Levi-Civita_symbol",
                "/wiki/Einstein_summation_notation",
                "/wiki/Even_and_odd_permutations",
                "/wiki/Permutation",
                "/wiki/Permutation_group",
                "/wiki/Even_and_odd_permutations",
                "/wiki/Algebra",
                "/wiki/Gottfried_Leibniz",
                "/wiki/Determinant",
                "/wiki/Square_matrix"
            ],
            "text": "with these three properties.Alternating:Multilinear:Existence: We now show that F, where F is the function defined by the Leibniz formula, has these three properties.From alternation it follows that any term with repeated indices is zero. The sum can therefore be restricted to tuples with non-repeating indices, i.e. permutations:Proof.Theorem. There exists exactly one functionwhich may be more familiar to physicists.Another common notation used for the formula is in terms of the Levi-Civita symbol and makes use of the Einstein summation notation, where it becomeswhere sgn is the sign function of permutations in the permutation group Sn, which returns +1 and \u22121 for even and odd permutations, respectively.In algebra, the Leibniz formula, named in honor of Gottfried Leibniz, expresses the determinant of a square matrix in terms of permutations of the matrix elements. If A is an n\u00d7n matrix, where ai,j is the entry in the ith row and jth column of A, the formula is",
            "title": "Leibniz formula for determinants",
            "url": "https://en.wikipedia.org/wiki/Leibniz_formula_for_determinants"
        },
        {
            "desc_links": [
                "/wiki/Polynomial_equation",
                "/wiki/Word_problem_(mathematics_education)",
                "/wiki/Chemistry",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Social_science",
                "/wiki/Calculus",
                "/wiki/Numerical_analysis",
                "/wiki/Polynomial_ring",
                "/wiki/Algebraic_variety",
                "/wiki/Algebra",
                "/wiki/Algebraic_geometry",
                "/wiki/Mathematics",
                "/wiki/Expression_(mathematics)",
                "/wiki/Variable_(mathematics)",
                "/wiki/Indeterminate_(variable)",
                "/wiki/Coefficient",
                "/wiki/Addition",
                "/wiki/Subtraction",
                "/wiki/Multiplication",
                "/wiki/Integer",
                "/wiki/Exponentiation"
            ],
            "links": [
                "/wiki/Robert_Recorde",
                "/wiki/The_Whetstone_of_Witte",
                "/wiki/Michael_Stifel",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/The_Nine_Chapters_on_the_Mathematical_Art",
                "/wiki/Computational_complexity_theory",
                "/wiki/Polynomial_time",
                "/wiki/Algorithm",
                "/wiki/Characteristic_polynomial",
                "/wiki/Eigenvalue",
                "/wiki/Minimal_polynomial_(field_theory)",
                "/wiki/Algebraic_element",
                "/wiki/Chromatic_polynomial",
                "/wiki/Graph_(discrete_mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Spline_(mathematics)",
                "/wiki/Irreducible_polynomial",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Unique_factorization_domain",
                "/wiki/Factorization_of_polynomials",
                "/wiki/Computer_algebra_system",
                "/wiki/Eisenstein%27s_criterion",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Euclidean_domain",
                "/wiki/Field_(mathematics)",
                "/wiki/Commutative_algebra",
                "/wiki/Integral_domain",
                "/wiki/Polynomial_long_division",
                "/wiki/Unital_algebra",
                "/wiki/Associative_algebra",
                "/wiki/Substitution_(algebra)",
                "/wiki/Fermat%27s_little_theorem",
                "/wiki/Analysis_(mathematics)",
                "/wiki/Euclidean_division",
                "/wiki/Ideal_(ring_theory)",
                "/wiki/Finite_field",
                "/wiki/Prime_number",
                "/wiki/Modular_arithmetic",
                "/wiki/Commutative_ring",
                "/wiki/Algebra_(ring_theory)",
                "/wiki/Distributive_law",
                "/wiki/Abstract_algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Calculus",
                "/wiki/Taylor%27s_theorem",
                "/wiki/Differentiable_function",
                "/wiki/Stone%E2%80%93Weierstrass_theorem",
                "/wiki/Continuous_function",
                "/wiki/Compact_space",
                "/wiki/Interval_(mathematics)",
                "/wiki/Formal_power_series",
                "/wiki/Irrational_number",
                "/wiki/Power_series",
                "/wiki/Rational_fraction",
                "/wiki/Quotient",
                "/wiki/Algebraic_fraction",
                "/wiki/Algebraic_expression",
                "/wiki/Rational_function",
                "/wiki/Laurent_polynomial",
                "/wiki/Matrix_ring",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_polynomial",
                "/wiki/Square_matrix",
                "/wiki/Trigonometric_interpolation",
                "/wiki/Interpolation",
                "/wiki/Periodic_function",
                "/wiki/Discrete_Fourier_transform",
                "/wiki/Complex_number",
                "/wiki/Fourier_series",
                "/wiki/List_of_trigonometric_identities#Multiple-angle_formulae",
                "/wiki/List_of_trigonometric_identities#Product-to-sum_and_sum-to-product_identities",
                "/wiki/Linear_combination",
                "/wiki/Function_(mathematics)",
                "/wiki/Natural_number",
                "/wiki/Integer",
                "/wiki/Diophantine_equation",
                "/wiki/Algorithm",
                "/wiki/Hilbert%27s_tenth_problem",
                "/wiki/Fermat%27s_Last_Theorem",
                "/wiki/System_of_linear_equations",
                "/wiki/System_of_linear_equations#Solving_a_linear_system",
                "/wiki/Gaussian_elimination",
                "/wiki/Algebraic_geometry",
                "/wiki/Algorithm",
                "/wiki/Complex_number",
                "/wiki/System_of_polynomial_equations",
                "/wiki/Numerical_approximation",
                "/wiki/Continuous_function",
                "/wiki/Algorithm",
                "/wiki/Computer",
                "/wiki/Root-finding_algorithm",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Vieta%27s_formulas",
                "/wiki/Complex_number",
                "/wiki/Multiplicity_(mathematics)",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Algebra",
                "/wiki/Quadratic_formula",
                "/wiki/Cubic_function",
                "/wiki/Quartic_equations",
                "/wiki/Abel%E2%80%93Ruffini_theorem",
                "/wiki/Root-finding_algorithm",
                "/wiki/Numerical_approximation",
                "/wiki/Variable_(mathematics)",
                "/wiki/Identity_(mathematics)",
                "/wiki/Algebraic_equation",
                "/wiki/Equation",
                "/wiki/Infinity#Calculus",
                "/wiki/Absolute_value",
                "/wiki/Asymptote",
                "/wiki/Parabolic_branch",
                "/wiki/Graph_of_a_function",
                "/wiki/Continuous_function",
                "/wiki/Smooth_function",
                "/wiki/Entire_function",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Restriction_of_a_function",
                "/wiki/Expression_(mathematics)",
                "/wiki/Argument_of_a_function",
                "/wiki/Algebraic_fraction",
                "/wiki/Rational_function",
                "/wiki/Factorization_of_polynomials",
                "/wiki/Algorithm",
                "/wiki/Computer_algebra_system",
                "/wiki/Unique_factorization_domain",
                "/wiki/Field_(mathematics)",
                "/wiki/Irreducible_polynomial",
                "/wiki/Complex_number",
                "/wiki/Real_number",
                "/wiki/Rational_number",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Euclidean_division",
                "/wiki/Polynomial_long_division",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Polynomial_remainder_theorem",
                "/wiki/Associative_law",
                "/wiki/Horner%27s_method",
                "/wiki/Univariate",
                "/wiki/Real_number",
                "/wiki/S-plane",
                "/wiki/Laplace_transform",
                "/wiki/Integer",
                "/wiki/Complex_number",
                "/wiki/Distributive_law",
                "/wiki/Monomial",
                "/wiki/Binomial_(polynomial)",
                "/wiki/Commutative_law",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Euclidean_division_of_polynomials",
                "/wiki/Root_of_a_function",
                "/wiki/Constant_term",
                "/wiki/Term_(mathematics)",
                "/wiki/Coefficient",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Summation#Capital-sigma_notation",
                "/wiki/Expression_(mathematics)",
                "/wiki/Constant_(mathematics)",
                "/wiki/Addition",
                "/wiki/Multiplication",
                "/wiki/Exponentiation",
                "/wiki/Non-negative_integer",
                "/wiki/Commutative_property",
                "/wiki/Associative_property",
                "/wiki/Distributive_property",
                "/wiki/Ring_(mathematics)",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Function_(mathematics)",
                "/wiki/Hybrid_word",
                "/wiki/Binomial_(polynomial)",
                "/wiki/Polynomial_equation",
                "/wiki/Word_problem_(mathematics_education)",
                "/wiki/Chemistry",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Social_science",
                "/wiki/Calculus",
                "/wiki/Numerical_analysis",
                "/wiki/Polynomial_ring",
                "/wiki/Algebraic_variety",
                "/wiki/Algebra",
                "/wiki/Algebraic_geometry",
                "/wiki/Mathematics",
                "/wiki/Expression_(mathematics)",
                "/wiki/Variable_(mathematics)",
                "/wiki/Indeterminate_(variable)",
                "/wiki/Coefficient",
                "/wiki/Addition",
                "/wiki/Subtraction",
                "/wiki/Multiplication",
                "/wiki/Integer",
                "/wiki/Exponentiation"
            ],
            "text": "The earliest known use of the equal sign is in Robert Recorde's The Whetstone of Witte, 1557. The signs + for addition, \u2212 for subtraction, and the use of a letter for an unknown appear in Michael Stifel's Arithemetica integra, 1544. Ren\u00e9 Descartes, in La g\u00e9ometrie, 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the a's denote constants and x denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.[23]\nDetermining the roots of polynomials, or \"solving algebraic equations\", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins \"Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou.\" We would write 3x\u00a0+\u00a02y\u00a0+\u00a0z =\u00a029.\nThe term \"polynomial\", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase polynomial time means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.\nPolynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.\nPolynomials serve to approximate other functions,[22] such as the use of splines.\nAnalogously, prime polynomials (more correctly, irreducible polynomials) can be defined as non-zero polynomials which cannot be factorized into the product of two non-constant polynomials. In the case of coefficients in a ring, \"non-constant\" must be replaced by \"non-constant or non-unit\" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non-unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand-written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.\nand such that the degree of r is smaller than the degree of g (using the convention that the polynomial 0 has a negative degree). The polynomials q and r are uniquely determined by f and g. This is called Euclidean division, division with remainder or polynomial long division and shows that the ring F[x] is a Euclidean domain.\nIf F is a field and f and g are polynomials in F[x] with g \u2260 0, then there exist unique polynomials q and r in F[x] with\nIn commutative algebra, one major focus of study is divisibility among polynomials. If R is an integral domain and f and g are polynomials in R[x], it is said that f divides g or f is a divisor of g if there exists a polynomial q in R[x] such that f q = g. One can show that every zero gives rise to a linear divisor, or more formally, if f is a polynomial in R[x] and r is an element of R such that f(r) = 0, then the polynomial (x \u2212 r) divides f. The converse is also true. The quotient can be computed using the polynomial long division.[20][21]\nIf R is commutative, then one can associate to every polynomial P in R[x], a polynomial function f with domain and range equal to R (more generally one can take domain and range to be the same unital associative algebra over R). One obtains the value f(r) by substitution of the value r for the symbol x in P. One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p). This is not the case when R is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for x.\nFormation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring R[x] over the real numbers by factoring out the ideal of multiples of the polynomial x2 + 1. Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring R (see modular arithmetic).\nOne can think of the ring R[x] as arising from R by adding one new element x to R, and extending in a minimal way to a ring in which x satisfies no other relations than the obligatory ones, plus commutation with all elements of R (that is xr = rx). To do this, one must add all powers of x and their linear combinations as well.\nThus the set of all polynomials with coefficients in the ring R forms itself a ring, the ring of polynomials over R, which is denoted by R[x]. The map from R to R[x] sending r to rx0 is an injective homomorphism of rings, by which R is viewed as a subring of R[x]. If R is commutative, then R[x] is an algebra over R.\nwhere n is a natural number, the coefficients a0, . . ., an are elements of R, and x is a formal symbol, whose powers xi are just placeholders for the corresponding coefficients ai, so that the given formal expression is just a way to encode the sequence (a0, a1, . . .), where there is an n such that ai = 0 for all i > n. Two polynomials sharing the same value of n are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of n obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term aixi is interpreted as a polynomial that has zero coefficients at all powers of x other than xi. Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the rule\nIn abstract algebra, one distinguishes between polynomials and polynomial functions. A polynomial f in one indeterminate x over a ring R is defined as a formal expression of the form\nand the indefinite integral is\nthe derivative with respect to x is\nCalculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial function\nThe simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone\u2013Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.\nFormal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like irrational numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.\nThe rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.\nWhile polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.\nA rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.\nLaurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.\nA matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices A in a specified matrix ring Mn(R).\nwhere I is the identity matrix.[19]\nthis polynomial evaluated at a matrix A is\nA matrix polynomial is a polynomial with square matrices as variables.[18] Given an ordinary, scalar-valued polynomial\nTrigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.\nFor complex coefficients, there is no difference between such a function and a finite Fourier series.\nIf sin(nx) and cos(nx) are expanded in terms of sin(x) and cos(x), a trigonometric polynomial becomes a polynomial in the two variables sin(x) and cos(x) (using List of trigonometric identities#Multiple-angle formulae). Conversely, every polynomial in sin(x) and cos(x) may be converted, with Product-to-sum identities, into a linear combination of functions sin(nx) and cos(nx). This equivalence explains why linear combinations are called polynomials.\nA trigonometric polynomial is a finite linear combination of functions sin(nx) and cos(nx) with n taking on the values of one or more natural numbers.[17] The coefficients may be taken as real numbers, for real-valued functions.\n\nThere are several generalizations of the concept of polynomials.\nA polynomial equation for which one is interested only in the solutions which are integers is called a Diophantine equation. Solving Diophantine equations is generally a very hard task. It has been proved that there cannot be any general algorithm for solving them, and even for deciding whether the set of solutions is empty (see Hilbert's tenth problem). Some of the most famous problems that have been solved during the fifty last years are related to Diophantine equations, such as Fermat's Last Theorem.\nThe special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.\nFor polynomials in more than one indeterminate, the combinations of values for the variables for which the polynomial function takes the value zero are generally called zeros instead of \"roots\". The study of the sets of zeros of polynomials is the object of algebraic geometry. For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions, and, if this number is finite, for computing the solutions. See System of polynomial equations.\nWhen there is no algebraic expression for the roots, and when such an algebraic expression exists but is too complicated to be useful, the unique way of solving is to compute numerical approximations of the solutions.[16] There are many methods for that; some are restricted to polynomials and others may apply to any continuous function. The most efficient algorithms allow solving easily (on a computer) polynomial equations of degree higher than 1,000 (see Root-finding algorithm).\nSome polynomials, such as x2 + 1, do not have any roots among the real numbers. If, however, the set of accepted solutions is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors x \u2212 a, one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree\u00a01; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.\nA number a is a root of a polynomial P if and only if the linear polynomial x \u2212 a divides P, that is if there is another polynomial Q such that P = (x \u2013 a) Q. It may happen that x \u2212 a divides P more than once: if (x \u2212 a)2 divides P then a is called a multiple root of P, and otherwise a is called a simple root of P. If P is a nonzero polynomial, there is a highest power m such that (x \u2212 a)m divides P, which is called the multiplicity of the root a in P. When P is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots, as, with the above definitions, every number is a root of the zero polynomial, with an undefined multiplicity. With this exception made, the number of roots of P, even counted with their respective multiplicities, cannot exceed the degree of P.[15] \nThe relation between the coefficients of a polynomial and its roots is described by Vieta's formulas.\nThe number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.\nIn elementary algebra, methods such as the quadratic formula are taught for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel\u2013Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.\nWhen considering equations, the indeterminates (variables) of polynomials are also called unknowns, and the solutions are the possible values of the unknowns for which the equality is true (in general more than one solution may exist). A polynomial equation stands in contrast to a polynomial identity like (x + y)(x \u2212 y) = x2 \u2212 y2, where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.\nis a polynomial equation.\nFor example,\nA polynomial equation, also called algebraic equation, is an equation of the form[14]\nPolynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.\nA non-constant polynomial function tends to infinity when the variable increases indefinitely (in absolute value). If the degree is higher than one, the graph does not have any asymptote. It has two parabolic branches with vertical direction (one branch for positive x and one for negative x).\nA polynomial function in one real variable can be represented by a graph.\nEvery polynomial function is continuous, smooth, and entire. \nis a polynomial function of one variable. Polynomial functions of multiple variables are similarly defined, using polynomials in multiple indeterminates, as in\nFor example, the function f, defined by\nGenerally, unless otherwise specified, polynomial functions have real or complex coefficients and have real or complex arguments and values. In particular, a polynomial with real coefficients defines a function from the complex numbers to the complex numbers, whose restriction to the reals maps reals to reals.\nfor all arguments x, where n is a non-negative integer and a0, a1, a2, ..., an are constant coefficients.\nA polynomial function is a function that can be defined by evaluating a polynomial. A function f of one argument is thus a polynomial function if it satisfies.\nBecause subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.\nA formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a \"rational expression\" or \"rational fraction\" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example, x3/12 is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to (1/12)x3 and 1/12 is just a constant. When this expression is used as a term, its coefficient is therefore 1/12. For similar reasons, if complex coefficients are allowed, one may have a single term like (2 + 3i) x3; even though it looks like it should be expanded to two terms, the complex number 2 + 3i is one complex number, and is the coefficient of that term. The expression 1/(x2 + 1) is not a polynomial because it includes division by a non-constant polynomial. The expression (5 + y)x is not a polynomial, because it contains an indeterminate used as exponent.\nThe computation of the factored form, called factorization is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.\nover the complex numbers.\nover the integers and the reals and\nis\nAll polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree.[13] For example, the factored form of\nAs for the integers, two kinds of divisions are considered for the polynomials. The Euclidean division of polynomials that generalizes the Euclidean division of the integers. It results in two polynomials, a quotient and a remainder that are characterized by the following property of the polynomials: given two polynomials a and b such that b \u2260 0, there exists a unique pair of polynomials, q, the quotient, and r, the remainder, such that a = b q + r and degree(r) < degree(b) (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.[12]\nPolynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of f(x) by (x \u2212 a) is f(a); see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.\nwhich can be simplified to\nthen\nTo work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other.[8] For example, if\nwhich can be simplified to\nthen\nPolynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms.[8][10] For example, if\nThe evaluation of a polynomial consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:\nA polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply \"polynomials in x, y, and z\", listing the indeterminates allowed.\nA real polynomial is a polynomial with real coefficients. The argument of the polynomial is not necessarily so restricted, for instance the s-plane variable in Laplace transforms.  A real polynomial function is a function from the reals to the reals that is defined by a real polynomial. Similarly, an integer polynomial is a polynomial with integer coefficients, and a complex polynomial is a polynomial with complex coefficients.\nTwo terms with the same indeterminates raised to the same powers are called \"similar terms\" or \"like terms\", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0.[8] Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial,[9] a two-term polynomial is called a binomial, and a three-term polynomial is called a trinomial. The term \"quadrinomial\" is occasionally used for a four-term polynomial.\nThe commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in \"descending powers of x\", with the term of largest degree first, or in \"ascending powers of x\". The polynomial in the example above is written in descending powers of x. The first term has coefficient 3, indeterminate x, and exponent 2. In the second term, the coefficient is \u22125. The third term is a constant. Because the degree of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.[7]\nIn the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of degree n if all its non-zero terms have degree n. The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined.[6] For example, x3y2 + 7x2y3 \u2212 3x5 is homogeneous of degree 5. For more details, see homogeneous polynomial.\nThe polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either \u22121 or \u2212\u221e).[5] These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. The graph of the zero polynomial, f(x) = 0, is the X-axis.\nPolynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials. For higher degrees the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in x2 + 2x + 1 the term 2x is a linear term in a quadratic polynomial.\nIt consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.\nForming a sum of several terms produces a polynomial. For example, the following is a polynomial:\nis a term. The coefficient is \u22125, the indeterminates are x and y, the degree of x is two, while the degree of y is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is 2 + 1 = 3.\nFor example:\nA term with no indeterminates and a polynomial with no indeterminates are called, respectively, a constant term and a constant polynomial.[3] The degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial, 0, (which has no terms at all) is generally treated as not defined (but see below).[4]\nThat is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number\u2014called the coefficient of the term[2]\u2014and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because x = x1, the degree of an indeterminate without a written exponent is one.\nThis can be expressed more concisely by using summation notation:\nA polynomial in a single indeterminate x can always be written (or rewritten) in the form\nA polynomial is an expression that can be built from constants and symbols called indeterminates or variables by means of addition, multiplication and exponentiation to a non-negative integer power. Two such expressions that may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication are considered as defining the same polynomial.\nThis equality allows writing \"let P(x) be a polynomial\" as a shorthand for \"let P be a polynomial in the indeterminate x\". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.\nFrequently, when using this function, one supposes that a is a number. However one may use it over any domain where addition and multiplication are defined (any ring). \nIn particular, when a is the indeterminate x, then the image of x by this function is the polynomial P itself (substituting x to x does not change anything). In other words,\nwhich is the polynomial function associated to P.\nNormally, the name of the polynomial is P, not P(x). However, if a denotes a number, a variable, another polynomial, or, more generally any expression, then P(a) denotes, by convention, the result of substituting x by a in P. Thus, the polynomial P defines the function\nIt may be confusing that a polynomial P in the indeterminate x may appear in the formulas either as P or as P(x).[citation needed]\nIt is a common convention to use uppercase letters for the indeterminates and the corresponding lowercase letters for the variables (arguments) of the associated function.[citation needed]\nThe x occurring in a polynomial is commonly called either a variable or an indeterminate. When the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is \"indeterminate\"). It is thus more correct to call it an \"indeterminate\".[citation needed] However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a \"variable\". Many authors use these two words interchangeably.\nThe word polynomial joins two diverse roots: the Greek poly, meaning \"many,\" and the Latin nomen, or name. It was derived from the term binomial by replacing the Latin root bi- with the Greek poly-. The word polynomial was first used in the 17th century.[1]\n\nPolynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.\nIn mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate x is x2 \u2212 4x + 7. An example in three variables is x3 + 2xyz2 \u2212 yz + 1.\n",
            "title": "Polynomial",
            "url": "https://en.wikipedia.org/wiki/Polynomial"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Degree of a polynomial",
            "url": "https://en.wikipedia.org/wiki/Degree_of_a_polynomial"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Linear_algebra",
                "/wiki/Polynomial",
                "/wiki/Descartes",
                "/wiki/Mathematics",
                "/wiki/Term_(mathematics)",
                "/wiki/Polynomial",
                "/wiki/Series_(mathematics)",
                "/wiki/Expression_(mathematics)",
                "/wiki/Variable_(mathematics)",
                "/wiki/Parameter"
            ],
            "text": "The leading coefficient of the first row is 1; 2 is the leading coefficient of the second row; 4 is the leading coefficient of the third row, and the last row does not have a leading coefficient.In linear algebra, the leading coefficient (also leading entry) of a row in a matrix is the first nonzero entry in that row. So, for example, givenis 4.Similarly, any polynomial in one variable x can be written asit is generally supposed that x is the only variable and that a, b and c are parameters; thus the constant coefficient is c in this case.When one writesOften coefficients are numbers as in this example, although they could be parameters of the problem or any expression in these parameters. In such a case one must clearly distinguish between symbols representing variables and symbols representing parameters. Following Descartes, the variables are often denoted by x, y, ..., and the parameters by a, b, c, ..., but it is not always the case. For example, if y is considered as a parameter in the above expression, the coefficient of x is \u22123y, and the constant coefficient is 1.5 + y.the first two terms respectively have the coefficients 7 and \u22123. The third term 1.5 is a constant coefficient. The final term does not have any explicitly written coefficient, but is considered to have coefficient 1, since multiplying by that factor would not change the term.For example, inIn mathematics, a coefficient is a multiplicative factor in some term of a polynomial, a series or any expression; it is usually a number, but may be any expression. In the latter case, the variables appearing in the coefficients are often called parameters, and must be clearly distinguished from the other variables.",
            "title": "Coefficient",
            "url": "https://en.wikipedia.org/wiki/Coefficient"
        },
        {
            "desc_links": [
                "/wiki/Graph_(discrete_mathematics)",
                "/wiki/Adjacency_matrix",
                "/wiki/Graph_invariant",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Polynomial",
                "/wiki/Matrix_similarity",
                "/wiki/Eigenvalues",
                "/wiki/Root_of_a_polynomial",
                "/wiki/Determinant",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Endomorphism",
                "/wiki/Vector_space"
            ],
            "links": [
                "/wiki/Secular_phenomena",
                "/wiki/Joseph_Louis_Lagrange",
                "/wiki/Open_subset",
                "/wiki/Topological_space",
                "/wiki/Zariski_topology",
                "/wiki/Non-singular_matrix",
                "/wiki/Similar_matrices",
                "/wiki/Transpose",
                "/wiki/Triangular_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Jordan_normal_form",
                "/wiki/Similar_matrices",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Exterior_algebra",
                "/wiki/Root_of_a_function",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Polynomial_expression",
                "/wiki/Trace_(matrix)",
                "/wiki/Hyperbolic_function",
                "/wiki/Hyperbolic_angle",
                "/wiki/Determinant",
                "/wiki/Monic_polynomial",
                "/wiki/Identity_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Singular_matrix",
                "/wiki/Determinant",
                "/wiki/Eigenvector",
                "/wiki/Diagonal_matrix",
                "/wiki/Graph_(discrete_mathematics)",
                "/wiki/Adjacency_matrix",
                "/wiki/Graph_invariant",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Polynomial",
                "/wiki/Matrix_similarity",
                "/wiki/Eigenvalues",
                "/wiki/Root_of_a_polynomial",
                "/wiki/Determinant",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Endomorphism",
                "/wiki/Vector_space"
            ],
            "text": "Secular equation may have several meanings.The term secular function has been used for what is now called characteristic polynomial (in some literature the term secular function is still used). The term comes from the fact that the characteristic polynomial was used to calculate secular perturbations (on a time scale of a century, i.e. slow compared to annual motion) of planetary orbits, according to Lagrange's theory of oscillations.To prove this, one may suppose n > m, by exchanging, if needed, A and B. Then, by bordering A on the bottom by n \u2013 m rows of zeros, and B on the right, by, n \u2013 m columns of zeros, one gets two n\u00d7n matrices A' and B' such that B'A' = BA, and A'B' is equal to AB bordered by n \u2013 m rows and columns of zeros. The result follows from the case of square matrices, by comparing the characteristic polynomials of A'B' and AB.More generally, if A is a matrix of order m\u00d7n and B is a matrix of order n\u00d7m, then AB is m\u00d7m and BA is n\u00d7n matrix, and one hasFor the case where both A and B are singular, one may remark that the desired identity is an equality between polynomials in t and the coefficients of the matrices. Thus, to prove this equality, it suffices to prove that it is verified on a non-empty open subset (for the usual topology, or, more generally, for the Zariski topology) of the space of all the coefficients. As the non-singular matrices form such an open subset of the space of all matrices, this proves the result.When A is non-singular this result follows from the fact that AB and BA are similar:If A and B are two square n\u00d7n matrices then characteristic polynomials of AB and BA coincide:The matrix A and its transpose have the same characteristic polynomial. A is similar to a triangular matrix if and only if its characteristic polynomial can be completely factored into linear factors over K (the same is true with the minimal polynomial instead of the characteristic polynomial). In this case A is similar to a matrix in Jordan normal form.Two similar matrices have the same characteristic polynomial. The converse however is not true in general: two matrices with the same characteristic polynomial need not be similar.The Cayley\u2013Hamilton theorem states that replacing t by A in the characteristic polynomial (interpreting the resulting powers as matrix powers, and the constant term c as c times the identity matrix) yields the zero matrix. Informally speaking, every matrix satisfies its own characteristic equation. This statement is equivalent to saying that the minimal polynomial of A divides the characteristic polynomial of A.When the characteristic is 0 it may alternatively be computed as a single determinant, that of the k\u00d7k matrix,Using the language of exterior algebra, one may compactly express the characteristic polynomial of an n\u00d7n matrix A asFor a 2\u00d72 matrix A, the characteristic polynomial is thus given byThe polynomial pA(t) is monic (its leading coefficient is 1) and its degree is n. The most important fact about the characteristic polynomial was already mentioned in the motivational paragraph: the eigenvalues of A are precisely the roots of pA(t) (this also holds for the minimal polynomial of A, but its degree may be less than n). The coefficients of the characteristic polynomial are all polynomial expressions in the entries of the matrix. In particular its constant coefficient pA\u00a0(0)\u00a0 is det(\u2212A) = (\u22121)n det(A), the coefficient of tn is one, and the coefficient of tn\u22121 is tr(\u2212A) = \u2212tr(A), where tr(A) is the matrix trace of\u00a0A. (The signs given here correspond to the formal definition given in the previous section;[2] for the alternative definition these would instead be det(A) and (\u22121)n\u00a0\u2212\u00a01 tr(A) respectively.[3])Its characteristic polynomial isAnother example uses hyperbolic functions of a hyperbolic angle \u03c6. For the matrix takeWe now compute the determinant ofSuppose we want to compute the characteristic polynomial of the matrixSome authors define the characteristic polynomial to be det(A\u00a0-\u00a0t\u00a0I). That polynomial differs from the one defined here by a sign (\u22121)n, so it makes no difference for properties like having as roots the eigenvalues of A; however the current definition always gives a monic polynomial, whereas the alternative definition always has constant term det(A).where I denotes the n-by-n identity matrix.We consider an n\u00d7n matrix A. The characteristic polynomial of A, denoted by pA(t), is the polynomial defined by(where I is the identity matrix). Since v is non-zero, this means that the matrix \u03bb I\u00a0\u2212\u00a0A is singular (non-invertible), which in turn means that its determinant is 0. Thus the roots of the function det(\u03bb\u00a0I\u00a0\u2212\u00a0A) are the eigenvalues of A, and it is clear that this determinant is a polynomial in \u03bb.orFor a general matrix A, one can proceed as follows. A scalar \u03bb is an eigenvalue of A if and only if there is an eigenvector v \u2260 0 such thatThis works because the diagonal entries are also the eigenvalues of this matrix.Given a square matrix A, we want to find a polynomial whose zeros are the eigenvalues of A. For a diagonal matrix A, the characteristic polynomial is easy to define: if the diagonal entries are a1,\u00a0a2,\u00a0a3,\u00a0etc. then the characteristic polynomial will be:The characteristic polynomial of a graph is the characteristic polynomial of its adjacency matrix. It is a graph invariant, though it is not complete: the smallest pair of non-isomorphic graphs with the same characteristic polynomial have five nodes.[1]In linear algebra, the characteristic polynomial of a square matrix is a polynomial which is invariant under matrix similarity and has the eigenvalues as roots. It has the determinant and the trace of the matrix as coefficients. The characteristic polynomial of an endomorphism of vector spaces of finite dimension is the characteristic polynomial of the matrix of the endomorphism over any base; it does not depend on the choice of a basis. The characteristic equation is the equation obtained by equating to zero the characteristic polynomial.",
            "title": "Characteristic polynomial",
            "url": "https://en.wikipedia.org/wiki/Characteristic_polynomial"
        },
        {
            "desc_links": [
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Inverse_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Hermitian_matrix",
                "/wiki/Real_number",
                "/wiki/Coefficient",
                "/wiki/System_of_linear_equations",
                "/wiki/Calculus",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Change_of_variables",
                "/wiki/Characteristic_polynomial",
                "/wiki/Eigenvalue",
                "/wiki/Parallelepiped",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix#Square_matrices",
                "/wiki/Linear_map"
            ],
            "links": [
                "/wiki/Inverse_function_theorem",
                "/wiki/Jacobian_determinant",
                "/wiki/Integration_by_substitution",
                "/wiki/Open_subset",
                "/wiki/Differentiable_function",
                "/wiki/Jacobian_matrix",
                "/wiki/Tetrahedron",
                "/wiki/Skew_line",
                "/wiki/Spanning_tree",
                "/wiki/Absolute_value",
                "/wiki/Parallelepiped",
                "/wiki/Lebesgue_measure",
                "/wiki/Subset",
                "/wiki/Dimension",
                "/wiki/Linear_transformation",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Sequence",
                "/wiki/Orthogonal_matrix",
                "/wiki/Orthonormal_basis",
                "/wiki/Euclidean_space",
                "/wiki/Orientation_(mathematics)",
                "/wiki/Standard_basis",
                "/wiki/Analytic_function",
                "/wiki/Wronskian#The_Wronskian_and_linear_independence",
                "/wiki/Plane_(geometry)",
                "/wiki/Linear_span",
                "/wiki/Differential_equation",
                "/wiki/Wronskian",
                "/wiki/Lebesgue",
                "/wiki/Otto_Hesse",
                "/wiki/Persymmetric",
                "/wiki/Hermann_Hankel",
                "/wiki/Circulant",
                "/wiki/Eug%C3%A8ne_Charles_Catalan",
                "/wiki/William_Spottiswoode",
                "/wiki/James_Whitbread_Lee_Glaisher",
                "/wiki/Pfaffian",
                "/wiki/Orthogonal_transformation",
                "/wiki/Wronskian",
                "/wiki/Thomas_Muir_(mathematician)",
                "/wiki/Elwin_Bruno_Christoffel",
                "/wiki/Ferdinand_Georg_Frobenius",
                "/wiki/Hessian_matrix",
                "/wiki/Trudi",
                "/wiki/Carl_Gustav_Jakob_Jacobi",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Crelle%27s_Journal",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Arthur_Cayley",
                "/wiki/Jacques_Philippe_Marie_Binet",
                "/wiki/Cauchy",
                "/wiki/Cauchy%E2%80%93Binet_formula",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/Theory_of_numbers",
                "/wiki/Discriminant",
                "/wiki/Algebraic_form",
                "/wiki/Vandermonde",
                "/wiki/Laplace",
                "/wiki/Minor_(matrix)",
                "/wiki/Joseph_Louis_Lagrange",
                "/wiki/Elimination_theory",
                "/wiki/Seki_Takakazu",
                "/wiki/Gabriel_Cramer",
                "/wiki/B%C3%A9zout",
                "/wiki/System_of_linear_equations",
                "/wiki/The_Nine_Chapters_on_the_Mathematical_Art",
                "/wiki/Gerolamo_Cardano",
                "/wiki/Gottfried_Leibniz",
                "/wiki/Bit_complexity",
                "/wiki/Gaussian_elimination",
                "/wiki/Bareiss_Algorithm",
                "/wiki/Sylvester%27s_determinant_theorem",
                "/wiki/Lewis_Carroll",
                "/wiki/Alice%27s_Adventures_in_Wonderland",
                "/wiki/Dodgson_condensation",
                "/wiki/Coppersmith%E2%80%93Winograd_algorithm",
                "/wiki/Matrix_determinant_lemma",
                "/wiki/Determinant_identities",
                "/wiki/Unitriangular_matrix",
                "/wiki/Permutation_matrix",
                "/wiki/LU_decomposition",
                "/wiki/QR_decomposition",
                "/wiki/Cholesky_decomposition",
                "/wiki/Positive_definite_matrix",
                "/wiki/Numerical_linear_algebra",
                "/wiki/Computational_geometry",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/Laplace_expansion",
                "/wiki/Big_O_notation",
                "/wiki/Factorial",
                "/wiki/Permanent_(mathematics)",
                "/wiki/Immanant_of_a_matrix",
                "/wiki/Character_theory",
                "/wiki/Symmetric_group",
                "/wiki/Superring",
                "/wiki/Graded_ring",
                "/wiki/Berezinian",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Regular_element_(ring_theory)",
                "/wiki/Quasideterminant",
                "/wiki/Dieudonn%C3%A9_determinant",
                "/wiki/Capelli_determinant",
                "/wiki/Berezinian",
                "/wiki/Manin_matrices",
                "/wiki/Functional_determinant",
                "/wiki/Fredholm_determinant",
                "/wiki/Trace_class_operator",
                "/wiki/Functional_analysis",
                "/wiki/Complex_conjugate",
                "/wiki/Modular_arithmetic",
                "/wiki/Category_theory",
                "/wiki/Natural_transformation",
                "/wiki/Natural_transformation#Determinant",
                "/wiki/Algebraic_group",
                "/wiki/Multiplicative_group",
                "/wiki/Multiplicative_group",
                "/wiki/Group_homomorphism",
                "/wiki/Ring_homomorphism",
                "/wiki/Commutative_ring",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Unimodular_matrix",
                "/wiki/Alternating_form",
                "/wiki/Multilinear_map",
                "/wiki/Vector_bundle",
                "/wiki/Chain_complex",
                "/wiki/Exterior_algebra",
                "/wiki/Vector_space",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Similarity_invariance",
                "/wiki/Linear_transformation",
                "/wiki/Matrix_similarity",
                "/wiki/Tangent_space",
                "/wiki/Lie_groups",
                "/wiki/Adjugate",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/Polynomial",
                "/wiki/Derivative",
                "/wiki/Jacobi%27s_formula",
                "/wiki/Invertible_matrix",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/LU_decomposition",
                "/wiki/QR_decomposition",
                "/wiki/Singular_value_decomposition",
                "/wiki/Cramer%27s_rule",
                "/wiki/Multivariate_normal",
                "/wiki/Mercator_series",
                "/wiki/Bell_polynomial",
                "/wiki/Cayley%E2%80%93Hamilton_theorem#Illustration_for_specific_dimensions_and_practical_applications",
                "/wiki/Newton%27s_identities#Computing_coefficients",
                "/wiki/Faddeev%E2%80%93LeVerrier_algorithm",
                "/wiki/Characteristic_polynomial",
                "/wiki/Matrix_exponential",
                "/wiki/Matrix_logarithm",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Hermitian_matrix",
                "/wiki/Positive_definite_matrix",
                "/wiki/Sylvester%27s_criterion",
                "/wiki/Identity_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Characteristic_polynomial",
                "/wiki/Pseudo-determinant",
                "/wiki/Sylvester%27s_determinant_theorem",
                "/wiki/Adjugate_matrix",
                "/wiki/Laplace_expansion",
                "/wiki/Minor_(matrix)",
                "/wiki/Cofactor_(linear_algebra)",
                "/wiki/Special_linear_group",
                "/wiki/Matrix_group",
                "/wiki/Special_orthogonal_group",
                "/wiki/Rotation_matrix",
                "/wiki/Special_unitary_group",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Cauchy%E2%80%93Binet_formula",
                "/wiki/Matrix_product",
                "/wiki/Main_diagonal",
                "/wiki/Homogeneous_function",
                "/wiki/Field_(mathematics)",
                "/wiki/Gaussian_elimination",
                "/wiki/Commutative_ring",
                "/wiki/Standard_basis",
                "/wiki/Permutation",
                "/wiki/Symmetric_group",
                "/wiki/Signature_(permutation)",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/Laplace_expansion",
                "/wiki/Rule_of_Sarrus",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/Laplace_expansion",
                "/wiki/Bivector",
                "/wiki/Equiareal_map",
                "/wiki/Sine",
                "/wiki/Cosine",
                "/wiki/Scalar_product",
                "/wiki/Area_(geometry)",
                "/wiki/Identity_matrix",
                "/wiki/Linear_map",
                "/wiki/Standard_basis",
                "/wiki/Parallelogram",
                "/wiki/Unit_square",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/Characteristic_polynomial",
                "/wiki/Commutativity",
                "/wiki/Polynomial_expression",
                "/wiki/Factorial",
                "/wiki/Identity_matrix",
                "/wiki/Square_matrix",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Inverse_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Hermitian_matrix",
                "/wiki/Real_number",
                "/wiki/Coefficient",
                "/wiki/System_of_linear_equations",
                "/wiki/Calculus",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Change_of_variables",
                "/wiki/Characteristic_polynomial",
                "/wiki/Eigenvalue",
                "/wiki/Parallelepiped",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix#Square_matrices",
                "/wiki/Linear_map"
            ],
            "text": "where \u03c9j is an nth root of 1.where \u03c9 and \u03c92 are the complex cube roots of 1. In general, the nth-order circulant determinant is[34]Third orderSecond orderwhere the right-hand side is the continued product of all the differences that can be formed from the n(n\u22121)/2 pairs of numbers taken from x1, x2, \u2026, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.In general, the nth-order Vandermonde determinant is[34]The third order Vandermonde determinant isThe Jacobian also occurs in the inverse function theorem.Its determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function \u03c6: Rn \u2192 Rm is given bythe Jacobian matrix is the n \u00d7 n matrix whose entries are given byFor a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. ForBy calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)\u00b7|det(a \u2212 b, b \u2212 c, c \u2212 d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn \u2192 Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn \u2192 Rm is represented by the m \u00d7 n matrix A, then the n-dimensional volume of f(S) is given by:More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 \u00d7 2 or 3 \u00d7 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is \u22121, the basis has the opposite orientation.It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n\u22121 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 \u00d7 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), \u2026, fn(x) (supposed to be n \u2212 1 times differentiable), the Wronskian is defined to beThe study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.The next important figure was Jacobi[24] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[32][33]The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy\u2013Binet formula.) In this he used the word determinant in its present sense,[29][30] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[23][31] With him begins the theory in its generality.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.It was Vandermonde (1771) who first recognized determinants as independent functions.[23] Laplace (1772)[27][28] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.In Japan, Seki Takakazu (\u95a2 \u5b5d\u548c) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by B\u00e9zout (1764).Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant \"determines\" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (\u4e5d\u7ae0\u7b97\u8853, Chinese scholars, around the 3rd century BCE). In Europe, 2 \u00d7 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[23][24][25][26]Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[21] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[22]Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation. Unfortunately this interesting method does not always work in its original form.If two matrices of order n can be multiplied in time M(n), where M(n) \u2265 na for some a > 2, then the determinant can be computed in time O(M(n)).[20] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith\u2013Winograd algorithm.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[19] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant\u00a01, in which case the formula further simplifies toThe LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Computational geometry, however, does frequently use calculations related to determinants. [18] Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n \u00d7 n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.The permanent of a matrix is defined as the determinant, except that the factors sgn(\u03c3) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonn\u00e9 determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Another infinite-dimensional notion of determinant is the functional determinant.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaFor matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo\u00a0m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo\u00a0m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (\u22c5)\u00d7 (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,holds. In other words, the following diagram commutes:between the group of invertible n \u00d7 n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R \u2192 S, there is a map GLn(f): GLn(R) \u2192 GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityThe determinant defines a mappingThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or \u22121. Such a matrix is called unimodular.This fact also implies that every other n-linear alternating function F: Mn(K) \u2192 K satisfiesfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.from the set of all n \u00d7 n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isThe determinant can also be characterized as the unique functionThe vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T\u2032 on W, where for each w in W we define (T\u2032w)(x1, \u2026, xn) = w(Tx1, \u2026, Txn). As a linear transformation on a one-dimensional space, T\u2032 is equivalent to a scalar multiple. We call this scalar the determinant of T.For this reason, the highest non-zero exterior power \u039bn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms \u039bkV with k < n.This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 \u2227 v2 \u2227 v3 \u2227 \u2026 \u2227 vn to v2 \u2227 v1 \u2227 v3 \u2227 \u2026 \u2227 vn, say, also changes its sign.As \u039bnV is one-dimensional, the map \u039bnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThe determinant of a linear transformation A\u00a0: V \u2192 V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power \u039bnV of V. A induces a linear mapfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant is therefore also called a similarity invariant. The determinant of a linear transformationThe above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X\u22121BX. Indeed, repeatedly applying the above identities yieldsThis identity is used in describing the tangent space of certain matrix Lie groups.Yet another equivalent formulation isExpressed in terms of the entries of A, these arewhere adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveBy definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn \u00d7 n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]When D is a 1\u00d71 matrix, B is a column vector, and C is a row vector thenWhen A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)Generally, if all pairs of n \u00d7 n matrices of the np \u00d7 np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix obtained by computing the determinant of the block matrix considering its entries as the entries of a p \u00d7 p matrix.[13] As the above example shows for p = 2, this criterion is sufficient, but not necessary.When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 \u00d7 2 matrix holds:[12]as can be seen by employing the decompositionWhen A is invertible, one hasThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)Suppose A, B, C, and D are matrices of dimension n \u00d7 n, n \u00d7 m, m \u00d7 n, and m \u00d7 m, respectively. ThenIt has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.the solution is given by Cramer's rule:For a matrix equationThese inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.Also,with equality if and only if A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantis expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I+sA).where I is the identity matrix. More generally, ifAn important arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm when the expansion converges. If every eigenvalue of A is less than 1 in absolute value,This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way asThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = - (l \u2013 1)! tr(Al) aswhere the sum is taken over the set of all integers kl \u2265 0 satisfying the equationIn the general case, this may also be obtained from[9]cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev\u2013LeVerrier algorithm. That is, for generic n, detA = (\u2212)nc0 the signed constant term of the characteristic polynomial, determined recursively fromFor example, for n = 2, n = 3, and n = 4, respectively,the determinant of A is given byHere exp(A) denotes the matrix exponential of A, because every eigenvalue \u03bb of A corresponds to the eigenvalue exp(\u03bb) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingor, for real matrices A,The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,being positive, for all k between 1 and n.A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatriceswhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationThe product of all non-zero eigenvalues is referred to as pseudo-determinant.From this general result several consequences follow.where Im and In are the m \u00d7 m and n \u00d7 n identity matrices, respectively.Sylvester's determinant theorem states that for A, an m \u00d7 n matrix, and B, an n \u00d7 m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):In terms of the adjugate matrix, Laplace's expansion can be written as[7]The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,However, Laplace expansion is efficient for small matrices only.along the second column (j = 2 and the sum runs over i) is given by,Calculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 \u00d7 3 matrixLaplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n\u22121) \u00d7 (n\u22121)-matrix that results from A by removing the i-th row and the j-th column. The expression (\u22121)i+jMi,j is known as a cofactor. The determinant of A is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word \"special\" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byThus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value\u00a01 on the identity matrix, since the function Mn(K) \u2192 K that maps M \u21a6 det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy\u2013Binet formula, which also provides an independent proof of the multiplicative property.The determinant of a matrix product of square matrices equals the product of their determinants:Here, B is obtained from A by adding \u22121/2\u00d7the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = \u2212det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (\u22122) \u00b7 2 \u00b7 4.5 = \u221218. Therefore, det(A) = \u2212det(D) = +18.can be computed using the following matrices:For example, the determinant ofProperty 5 says that the determinant on n \u00d7 n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property\u00a06; this is essentially the method of Gaussian elimination.Property 2 above implies that properties for columns have their counterparts in terms of rows:Properties 1, 8 and 10 \u2014 which all follow from the Leibniz formula \u2014 completely characterize the determinant; in other words the determinant is the unique function from n \u00d7 n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property\u00a09) or else \u00b11 (by properties 1 and\u00a012 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n \u2265 2,[6] so there is no good definition of the determinant in this setting.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.The determinant has many properties. Some basic properties of determinants arewhere now each ir and each jr should be summed over 1, \u2026, n.or using two epsilon symbols asFor example, the determinant of a 3 \u00d7 3 matrix A (n = 3) isis notation for the product of the entries at positions (i, \u03c3i), where i ranges from 1 to n:Here the sum is computed over all permutations \u03c3 of the set {1, 2, \u2026, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering \u03c3 is denoted by \u03c3i. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to \u03c3 = [2, 3, 1], with \u03c31 = 2, \u03c32 = 3, and \u03c33 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation \u03c3, sgn(\u03c3) denotes the signature of \u03c3, a value that is +1 whenever the reordering given by \u03c3 can be achieved by successively interchanging two entries an even number of times, and \u22121 whenever it can be achieved by an odd number of such interchanges.The Leibniz formula for the determinant of an n \u00d7 n matrix A isThe determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The rule of Sarrus is a mnemonic for the 3 \u00d7 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 \u00d7 3 matrix does not carry over into higher dimensions.which is the Leibniz formula for the determinant of a 3 \u00d7 3 matrix.this can be expanded out to giveThe Laplace formula for the determinant of a 3 \u00d7 3 matrix isThe object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) \u2227 (c, d)) is the signed area, which is also the determinant ad \u2212 bc.[3]Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.To show that ad \u2212 bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sin\u03b8 for the angle \u03b8 between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a\u22a5 = (-b, a), such that |a\u22a5||b|cos\u03b8' , which can be determined by the pattern of the scalar product to be equal to ad \u2212 bc:The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).The absolute value of ad \u2212 bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The Leibniz formula for the determinant of a 2 \u00d7 2 matrix isThe determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.Assume A is a square matrix with n rows and n columns, so that it can be written asEquivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is \u22121 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n \u00d7 n matrix contributes n! terms), so it will first be given explicitly for the case of 2 \u00d7 2 matrices and 3 \u00d7 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Another way to define the determinant is expressed in terms of the columns of the matrix. If we write an n \u00d7 n matrix A in terms of its column vectorsThere are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted. For example, here is the result for a 4 \u00d7 4 matrix:When the entries of the matrix are taken from a field (like the real or complex numbers), it can be proven that any matrix has a unique inverse if and only if its determinant is nonzero. Various other theorems can be proved as well, including that the determinant of a product of matrices is always equal to the product of determinants; and, the determinant of a Hermitian matrix is always real.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although more efficient techniques are actually used, some of which are determinant-revealing and consist of computationally effective ways of computing the determinant itself. The use of determinants in calculus includes the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants are also used to define the characteristic polynomial of a matrix, which is essential for eigenvalue problems in linear algebra. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. Sometimes, determinants are used merely as a compact notation for expressions that would otherwise be unwieldy to write down.Each determinant of a 2 \u00d7 2 matrix in this equation is called a \"minor\" of the matrix A. The same sort of procedure can be used to find the determinant of a 4 \u00d7 4 matrix, the determinant of a 5 \u00d7 5 matrix, and so forth.Similarly, suppose we have a 3 \u00d7 3 matrix A, and we want the specific formula for its determinant |A|:In the case of a 2 \u00d7 2 matrix the specific formula for the determinant is:In linear algebra, the determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted det(A), det A, or |A|. It can be viewed as the scaling factor of the transformation described by the matrix.",
            "title": "Determinant",
            "url": "https://en.wikipedia.org/wiki/Determinant"
        },
        {
            "desc_links": [],
            "links": [
                "/wiki/Square_root_of_a_matrix",
                "/wiki/Positive-definite_matrix",
                "/wiki/Idempotent_matrix",
                "/wiki/Full_rank",
                "/wiki/Linear_independence",
                "/wiki/Kronecker_delta",
                "/wiki/Diagonal_matrix",
                "/wiki/Unit_vector",
                "/wiki/Determinant",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Linear_transformation",
                "/wiki/Identity_function",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Identity_element",
                "/wiki/General_linear_group",
                "/wiki/Invertible_matrix",
                "/wiki/Involutory_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Quantum_mechanics"
            ],
            "text": "The principal square root of an identity matrix is itself, and this is its only positive definite square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.[3]The identity matrix of a given size is the only idempotent matrix of that size having full rank. That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are linearly independent.The identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.It can also be written using the Kronecker delta notation:Using the notation that is sometimes used to concisely describe diagonal matrices, we can write:The ith column of an identity matrix is the unit vector ei. It follows that the determinant of the identity matrix is\u00a01 and the trace is\u00a0n.Where n\u00d7n matrices are used to represent linear transformations from an n-dimensional vector space to itself, In represents the identity function, regardless of the basis.In particular, the identity matrix serves as the unit of the ring of all n\u00d7n matrices, and as the identity element of the general linear group GL(n) consisting of all invertible n\u00d7n matrices. (The identity matrix itself is invertible, being its own inverse.)When A is m\u00d7n, it is a property of matrix multiplication thatIn linear algebra, the identity matrix, or sometimes ambiguously called a unit matrix, of size n is the n \u00d7 n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. (In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I.) Less frequently, some mathematics books use U or E to represent the identity matrix, meaning \"unit matrix\"[1] and the German word \"Einheitsmatrix\",[2] respectively.",
            "title": "Identity matrix",
            "url": "https://en.wikipedia.org/wiki/Identity_matrix"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Vector_space",
                "/wiki/Linear_algebra",
                "/wiki/Module_(mathematics)",
                "/wiki/Abstract_algebra",
                "/wiki/Real_number",
                "/wiki/Euclidean_vector",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Uniform_scaling",
                "/wiki/Inner_product"
            ],
            "links": [
                "/wiki/Ring_(mathematics)",
                "/wiki/Commutative",
                "/wiki/Real_numbers",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Quaternion",
                "/wiki/Commutative_ring",
                "/wiki/Module_(mathematics)",
                "/wiki/Rig_(algebra)",
                "/wiki/Commutative",
                "/wiki/External_(mathematics)",
                "/wiki/Binary_operation",
                "/wiki/Group_action",
                "/wiki/Geometric",
                "/wiki/Addition",
                "/wiki/Multiplication_by_juxtaposition",
                "/wiki/Multiplication",
                "/wiki/Boldface",
                "/wiki/Field_(algebra)",
                "/wiki/Function_(mathematics)",
                "/wiki/Mathematics",
                "/wiki/Vector_space",
                "/wiki/Linear_algebra",
                "/wiki/Module_(mathematics)",
                "/wiki/Abstract_algebra",
                "/wiki/Real_number",
                "/wiki/Euclidean_vector",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Uniform_scaling",
                "/wiki/Inner_product"
            ],
            "text": "where i, j, k are the quaternion units. The non-commutativity of quaternion multiplication prevents the transition of changing ij = +k to ji = \u2212k.For quaternion scalars and matrices:For a real scalar and matrix:When the underlying ring is commutative, for example, the real or complex number field, these two multiplications are the same, and are simply called scalar multiplication. However, for matrices over a more general ring that are not commutative, such as the quaternions, they may not be equal.explicitly:Similarly, the right scalar multiplication of a matrix A with a scalar \u03bb is defined to beexplicitly:The left scalar multiplication of a matrix A with a scalar \u03bb gives another matrix \u03bbA of the same size as A. The entries of \u03bbA are defined byThe same idea applies if K is a commutative ring and V is a module over K. K can even be a rig, but then there is no additive inverse. If K is not commutative, the distinct operations left scalar multiplication cv and right scalar multiplication vc may be defined.When V is Kn, scalar multiplication is equivalent to multiplication of each component with the scalar, and may be defined as such.As a special case, V may be taken to be K itself and scalar multiplication may then be taken to be simply the multiplication in the field.Scalar multiplication may be viewed as an external binary operation or as an action of the field on the vector space. A geometric interpretation of scalar multiplication is that it stretches, or contracts, vectors by a constant factor.Here + is addition either in the field or in the vector space, as appropriate; and 0 is the additive identity in either. Juxtaposition indicates either scalar multiplication or the multiplication operation in the field.Scalar multiplication obeys the following rules (vector in boldface):In general, if K is a field and V is a vector space over K, then scalar multiplication is a function from K \u00d7 V to V. The result of applying this function to c in K and v in V is denoted cv.In mathematics, scalar multiplication is one of the basic operations defining a vector space in linear algebra[1][2][3] (or more generally, a module in abstract algebra[4][5]). In common geometrical contexts, scalar multiplication of a real Euclidean vector by a positive real number multiplies the magnitude of the vector without changing its direction. The term \"scalar\" itself derives from this usage: a scalar is that which scales vectors. Scalar multiplication is the multiplication of a vector by a scalar (where the product is a vector), and must be distinguished from inner product of two vectors (where the product is a scalar).",
            "title": "Scalar multiplication",
            "url": "https://en.wikipedia.org/wiki/Scalar_multiplication"
        },
        {
            "desc_links": [
                "/wiki/Euclid",
                "/wiki/Parallel_postulate",
                "/wiki/Affine_geometry",
                "/wiki/Euclidean_geometry",
                "/wiki/Hyperbolic_geometry",
                "/wiki/Geometry",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Intersecting_lines",
                "/wiki/Tangent",
                "/wiki/Three-dimensional_space",
                "/wiki/Skew_lines"
            ],
            "links": [
                "/wiki/Emil_Artin",
                "/wiki/Incidence_geometry",
                "/wiki/Affine_plane_(incidence_geometry)",
                "/wiki/Transitive_relation",
                "/wiki/Binary_relation",
                "/wiki/Symmetric_relation",
                "/wiki/Reflexive_relation",
                "/wiki/Equivalence_relation",
                "/wiki/Affine_geometry",
                "/wiki/Pencil_(mathematics)",
                "/wiki/Equivalence_class",
                "/wiki/Spherical_geometry",
                "/wiki/Great_circles",
                "/wiki/Sphere",
                "/wiki/Latitude",
                "/wiki/Ultraparallel_theorem",
                "/wiki/Limiting_parallel",
                "/wiki/Ideal_point",
                "/wiki/Limiting_parallel",
                "/wiki/Non-Euclidean_geometry",
                "/wiki/Geodesic",
                "/wiki/Elliptic_geometry",
                "/wiki/Hyperbolic_geometry",
                "/wiki/If_and_only_if",
                "/wiki/Three-dimensional_space",
                "/wiki/Skew_lines",
                "/wiki/Equidistant",
                "/wiki/Primitive_notion",
                "/wiki/Wilhelm_Killing",
                "/wiki/Gottfried_Wilhelm_Leibniz",
                "/wiki/Augustus_De_Morgan",
                "/wiki/Projective_geometry",
                "/wiki/Non-Euclidean_geometry",
                "/wiki/Lewis_Carroll",
                "/wiki/Euclid%27s_Elements",
                "/wiki/Parallel_postulate",
                "/wiki/Proclus",
                "/wiki/Posidonius",
                "/wiki/Geminus",
                "/wiki/Simplicius_of_Cilicia",
                "/wiki/Euclid%27s_Fifth_Axiom",
                "/wiki/Gradient",
                "/wiki/Euclidean_space",
                "/wiki/Unicode",
                "/wiki/Euclid",
                "/wiki/Parallel_postulate",
                "/wiki/Affine_geometry",
                "/wiki/Euclidean_geometry",
                "/wiki/Hyperbolic_geometry",
                "/wiki/Geometry",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Intersecting_lines",
                "/wiki/Tangent",
                "/wiki/Three-dimensional_space",
                "/wiki/Skew_lines"
            ],
            "text": "To this end, Emil Artin (1957) adopted a definition of parallelism where two lines are parallel if they have all or none of their points in common.[16] Then a line is parallel to itself so that the reflexive and transitive properties belong to this type of parallelism, creating an equivalence relation on the set of lines. In the study of incidence geometry, this variant of parallelism is used in the affine plane.In this case, parallelism is a transitive relation. However, in case l = n, the superimposed lines are not considered parallel in Euclidean geometry. The binary relation between parallel lines is evidently a symmetric relation. According to Euclid's tenets, parallelism is not a reflexive relation and thus fails to be an equivalence relation. Nevertheless, in affine geometry a pencil of parallel lines is taken as an equivalence class in the set of lines where parallelism is an equivalence relation.[13][14][15]In spherical geometry, all geodesics are great circles. Great circles divide the sphere in two equal hemispheres and all great circles intersect each other. Thus, there are no parallel geodesics to a given geodesic, as all geodesics intersect. Equidistant curves on the sphere are called parallels of latitude analogous to the latitude lines on a globe. Parallels of latitude can be generated by the intersection of the sphere with a plane parallel to a plane through the center of the sphere.Ultra parallel lines have single common perpendicular (ultraparallel theorem), and diverge on both sides of this common perpendicular.As in the illustration through a point a not on line l there are two limiting parallel lines, one for each direction ideal point of line l. They separate the lines intersecting line l and those that are ultra parallel to line l.In the literature ultra parallel geodesics are often called non-intersecting. Geodesics intersecting at infinity are called limiting parallel.While in Euclidean geometry two geodesics can either intersect or be parallel, in hyperbolic geometry, there are three possibilities. Two geodesics belonging to the same plane can either be:In non-Euclidean geometry, it is more common to talk about geodesics than (straight) lines. A geodesic is the shortest path between two points in a given geometry. In physics this may be interpreted as the path that a particle follows if no force is applied to it. In non-Euclidean geometry (elliptic or hyperbolic geometry) the three Euclidean properties mentioned above are not equivalent and only the second one,(Line m is in the same plane as line l but does not intersect l ) since it involves no measurements is useful in non-Euclidean geometries. In general geometry the three properties above give three different types of curves, equidistant curves, parallel geodesics and geodesics sharing a common perpendicular, respectively.Two distinct planes q and r are parallel if and only if the distance from a point P in plane q to the nearest point in plane r is independent of the location of P in plane q. This will never hold if the two planes are not in the same three-dimensional space.Similar to the fact that parallel lines must be located in the same plane, parallel planes must be situated in the same three-dimensional space and contain no point in common.Equivalently, they are parallel if and only if the distance from a point P on line m to the nearest point in plane q is independent of the location of P on line m.A line m and a plane q in three-dimensional space, the line not lying in that plane, are parallel if and only if they do not intersect.Two distinct lines l and m in three-dimensional space are parallel if and only if the distance from a point P on line m to the nearest point on line l is independent of the location of P on line m. This never holds for skew lines.Two lines in the same three-dimensional space that do not intersect need not be parallel. Only if they are in a common plane are they called parallel; otherwise they are called skew lines.their distance can be expressed asWhen the lines are given by the general form of the equation of a line (horizontal and vertical lines are included):which reduces toThese formulas still give the correct point coordinates even if the parallel lines are horizontal (i.e., m = 0). The distance between the points isandto get the coordinates of the points. The solutions to the linear systems are the pointsandthe distance between the two lines can be found by locating two points (one on each line) that lie on a common perpendicular to the parallel lines and calculating the distance between them. Since the lines have slope m, a common perpendicular would have slope \u22121/m and we can take the line with equation y = \u2212x/m as a common perpendicular. Solve the linear systemsBecause parallel lines in a Euclidean plane are equidistant there is a unique distance between the two parallel lines. Given the equations of two non-vertical, non-horizontal parallel lines,The three properties above lead to three different methods of construction[12] of parallel lines.Other properties, proposed by other reformers, used as replacements for the definition of parallel lines, did not fare much better. The main difficulty, as pointed out by Dodgson, was that to use them in this way required additional axioms to be added to the system. The equidistant line definition of Posidonius, expounded by Francis Cuthbertson in his 1874 text Euclidean Geometry suffers from the problem that the points that are found at a fixed given distance on one side of a straight line must be shown to form a straight line. This can not be proved and must be assumed to be true.[11] The corresponding angles formed by a transversal property, used by W. D. Cooley in his 1860 text, The Elements of Geometry, simplified and explained requires a proof of the fact that if one transversal meets a pair of lines in congruent corresponding angles then all transversals must do so. Again, a new axiom is needed to justify this statement.One of the early reform textbooks was James Maurice Wilson's Elementary Geometry of 1868.[7] Wilson based his definition of parallel lines on the primitive notion of direction. According to Wilhelm Killing[8] the idea may be traced back to Leibniz.[9] Wilson, without defining direction since it is a primitive, uses the term in other definitions such as his sixth definition, \"Two straight lines that meet one another have different directions, and the difference of their directions is the angle between them.\" Wilson (1868, p. 2) In definition 15 he introduces parallel lines in this way; \"Straight lines which have the same direction, but are not parts of the same straight line, are called parallel lines.\" Wilson (1868, p. 12) Augustus De Morgan reviewed this text and declared it a failure, primarily on the basis of this definition and the way Wilson used it to prove things about parallel lines. Dodgson also devotes a large section of his play (Act II, Scene VI \u00a7 1) to denouncing Wilson's treatment of parallels. Wilson edited this concept out of the third and higher editions of his text.[10]At the end of the nineteenth century, in England, Euclid's Elements was still the standard textbook in secondary schools. The traditional treatment of geometry was being pressured to change by the new developments in projective geometry and non-Euclidean geometry, so several new textbooks for the teaching of geometry were written at this time. A major difference between these reform texts, both between themselves and between them and Euclid, is the treatment of parallel lines.[5] These reform texts were not without their critics and one of them, Charles Dodgson (a.k.a. Lewis Carroll), wrote a play, Euclid and His Modern Rivals, in which these texts are lambasted.[6]The definition of parallel lines as a pair of straight lines in a plane which do not meet appears as Definition 23 in Book I of Euclid's Elements.[4] Alternative definitions were discussed by other Greeks, often as part of an attempt to prove the parallel postulate. Proclus attributes a definition of parallel lines as equidistant lines to Posidonius and quotes Geminus in a similar vein. Simplicius also mentions Posidonius' definition as well as its modification by the philosopher Aganis.[4]Since these are equivalent properties, any one of them could be taken as the definition of parallel lines in Euclidean space, but the first and third properties involve measurement, and so, are \"more complicated\" than the second. Thus, the second property is the one usually chosen as the defining property of parallel lines in Euclidean geometry.[3] The other properties are then consequences of Euclid's Parallel Postulate. Another property that also involves measurement is that lines parallel to each other have the same gradient (slope).Given parallel straight lines l and m in Euclidean space, the following properties are equivalent:In the Unicode character set, the \"parallel\" and \"not parallel\" signs have codepoints U+2225 (\u2225) and U+2226 (\u2226), respectively. In addition, U+22D5 (\u22d5) represents the relation \"equal and parallel to\".[2]Parallel lines are the subject of Euclid's parallel postulate.[1] Parallelism is primarily a property of affine geometries and Euclidean geometry is a special instance of this type of geometry. In some other geometries, such as hyperbolic geometry, lines can have analogous properties that are referred to as parallelism.In geometry, parallel lines are lines in a plane which do not meet; that is, two lines in a plane that do not intersect or touch each other at any point are said to be parallel. By extension, a line and a plane, or two planes, in three-dimensional Euclidean space that do not share a point are said to be parallel. However, two lines in three-dimensional space which do not meet must be in a common plane to be considered parallel; otherwise they are called skew lines. Parallel planes are planes in the same three-dimensional space that never meet.",
            "title": "Parallel (geometry)",
            "url": "https://en.wikipedia.org/wiki/Parallel_(geometry)"
        },
        {
            "desc_links": [
                "/wiki/Geometry",
                "/wiki/Line_(geometry)"
            ],
            "links": [
                "/wiki/Collinearity_equation",
                "/wiki/Photogrammetry",
                "/wiki/Remote_sensing",
                "/wiki/Coordinates",
                "/wiki/Sensor",
                "/wiki/Projection_(mathematics)",
                "/wiki/3D_modeling",
                "/wiki/Cardinal_point_(optics)#Principal_planes_and_points",
                "/wiki/Telecommunication",
                "/wiki/Collinear_antenna_array",
                "/wiki/Tower_array",
                "/wiki/Dipole_antenna",
                "/wiki/Antenna_(radio)",
                "/wiki/Multiple_regression",
                "/wiki/Partial_geometry",
                "/wiki/Graph_(discrete_mathematics)",
                "/wiki/Adjacent_vertex",
                "/wiki/Plane_(geometry)",
                "/wiki/Duality_(projective_geometry)",
                "/wiki/Concurrent_lines",
                "/wiki/Coprime_numbers",
                "/wiki/Square_lattice",
                "/wiki/Triangle_inequality",
                "/wiki/Heron%27s_formula",
                "/wiki/Distance_geometry#Cayley\u2013Menger_determinants",
                "/wiki/Cayley%E2%80%93Menger_determinant",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Determinant",
                "/wiki/Triangle#Using_coordinates",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Coordinate_geometry",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Collineation",
                "/wiki/Linear_map",
                "/wiki/Vector_spaces",
                "/wiki/Projective_geometry",
                "/wiki/Homography",
                "/wiki/Euclidean_geometry",
                "/wiki/Line_(geometry)",
                "/wiki/Primitive_notion",
                "/wiki/Mathematical_model",
                "/wiki/Spherical_geometry",
                "/wiki/Geometry",
                "/wiki/Line_(geometry)"
            ],
            "text": "The collinearity equations are a set of two equations, used in photogrammetry and remote sensing to relate coordinates in an image (sensor) plane (in two dimensions) to object coordinates (in three dimensions). In the photography setting, the equations are derived by considering the central projection of a point of the object through the optical centre of the camera to the image in the image (sensor) plane. The three points, object point, image point and optical centre, are always collinear. Another way to say this is that the line segments joining the object points with their image points are all concurrent at the optical centre.[11]In telecommunications, a collinear (or co-linear) antenna array is an array of dipole antennas mounted in such a manner that the corresponding elements of each antenna are parallel and aligned, that is they are located along a common line or axis.The concept of lateral collinearity expands on this traditional view, and refers to collinearity between explanatory and criteria (i.e., explained) variables.[10]for all observations i. In practice, we rarely face perfect multicollinearity in a data set. More commonly, the issue of multicollinearity arises when there is a \"strong linear relationship\" among two or more independent variables, meaning thatPerfect multicollinearity refers to a situation in which k (k \u2265 2) explanatory variables in a multiple regression model are perfectly linearly related, according toThis means that if the various observations (X1i, X2i ) are plotted in the (X1, X2) plane, these points are collinear in the sense defined earlier in this article.Given a partial geometry P, where two points determine at most one line, a collinearity graph of P is a graph whose vertices are the points of P, where two vertices are adjacent if and only if they determine a line in P.In various plane geometries the notion of interchanging the roles of \"points\" and \"lines\" while preserving the relationship between them is called plane duality. Given a set of collinear points, by plane duality we obtain a set of lines all of which meet at a common point. The property that this set of lines has (meeting at a common point) is called concurrency, and the lines are said to be concurrent lines. Thus, concurrency is the plane dual notion to collinearity.Two numbers m and n are not coprime\u2014that is, they share a common factor other than 1\u2014if and only if for a rectangle plotted on a square lattice with vertices at (0,\u202f0), (m,\u202f0), (m,\u202fn), and (0,\u202fn), at least one interior point is collinear with (0,\u202f0) and (m,\u202fn).Equivalently, a set of at least three distinct points are collinear if and only if, for every three of those points A, B, and C with d(AC) greater than or equal to each of d(AB) and d(BC), the triangle inequality d(AC) \u2264 d(AB) + d(BC) holds with equality.This determinant is, by Heron's formula, equal to \u221216 times the square of the area of a triangle with side lengths d(AB), d(BC), and d(AC); so checking if this determinant equals zero is equivalent to checking whether the triangle with vertices A, B, and C has zero area (so the vertices are collinear).A set of at least three distinct points is called straight, meaning all the points are collinear, if and only if, for every three of those points A, B, and C, the following determinant of a Cayley\u2013Menger determinant is zero (with d(AB) meaning the distance between A and B, etc.):is of rank 2 or less, the points are collinear. In particular, for three points in the plane (n = 2), the above matrix is square and the points are collinear if and only if its determinant is zero; since that 3\u202f\u00d7\u202f3 determinant is plus or minus twice the area of a triangle with those three points as vertices, this is equivalent to the statement that the three points are collinear if and only if the triangle with those points as vertices has zero area.Equivalently, for every subset of three points X\u00a0=\u00a0(x1,\u202fx2,\u202f...\u202f,\u202fxn), Y\u00a0=\u00a0(y1,\u202fy2,\u202f...\u202f,\u202fyn), and Z\u00a0=\u00a0(z1,\u202fz2,\u202f...\u202f,\u202fzn), if the matrixis of rank 1 or less, the points are collinear.In coordinate geometry, in n-dimensional space, a set of three or more distinct points are collinear if and only if, the matrix of the coordinates of these vectors is of rank 1 or less. For example, given three points X\u00a0=\u00a0(x1,\u202fx2,\u202f...\u202f,\u202fxn), Y\u00a0=\u00a0(y1,\u202fy2,\u202f...\u202f,\u202fyn), and Z\u00a0=\u00a0(z1,\u202fz2,\u202f...\u202f,\u202fzn), if the matrixIn any triangle the following sets of points are collinear:A mapping of a geometry to itself which sends lines to lines is called a collineation; it preserves the collinearity property. The linear maps (or linear functions) of vector spaces, viewed as geometric maps, map lines to lines; that is, they map collinear point sets to collinear point sets and so, are collineations. In projective geometry these linear mappings are called homographies and are just one type of collineation.In any geometry, the set of points on a line are said to be collinear. In Euclidean geometry this relation is intuitively visualized by points lying in a row on a \"straight line\". However, in most geometries (including Euclidean) a line is typically a primitive (undefined) object type, so such visualizations will not necessarily be appropriate. A model for the geometry offers an interpretation of how the points, lines and other object types relate to one another and a notion such as collinearity must be interpreted within the context of that model. For instance, in spherical geometry, where lines are represented in the standard model by great circles of a sphere, sets of collinear points lie on the same great circle. Such points do not lie on a \"straight line\" in the Euclidean sense, and are not thought of as being in a row.In geometry, collinearity of a set of points is the property of their lying on a single line.[1] A set of points with this property is said to be collinear (sometimes spelled as colinear[2]). In greater generality, the term has been used for aligned objects, that is, things being \"in a line\" or \"in a row\".",
            "title": "Collinearity",
            "url": "https://en.wikipedia.org/wiki/Collinearity"
        },
        {
            "desc_links": [
                "/wiki/Istanbul",
                "/wiki/Philipp_Frank",
                "/wiki/Robert_Musil",
                "/wiki/Rainer_Maria_Rilke",
                "/wiki/Logical_positivism",
                "/wiki/Ernst_Mach",
                "/wiki/Vienna_Circle",
                "/wiki/Logical_empiricism",
                "/wiki/Philipp_Frank",
                "/wiki/Hans_Hahn_(mathematician)",
                "/wiki/Otto_Neurath",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Ludwig_von_Mises",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/Solid_mechanics",
                "/wiki/Fluid_mechanics",
                "/wiki/Aerodynamics",
                "/wiki/Aeronautics",
                "/wiki/Statistics",
                "/wiki/Probability_theory",
                "/wiki/Harvard_University"
            ],
            "links": [
                "/wiki/Probability_theory",
                "/wiki/Birthday_problem",
                "/wiki/Impossibility_of_a_gambling_system",
                "/wiki/Solid_mechanics",
                "/wiki/Plasticity_(physics)",
                "/wiki/Von_Mises_yield_criterion",
                "/wiki/Tytus_Maksymilian_Huber",
                "/wiki/Andrey_Kolmogorov",
                "/wiki/Axiomatic_system",
                "/wiki/Alexander_Ostrowski",
                "/wiki/Von_Mises_stress",
                "/wiki/Stress_(physics)",
                "/wiki/Engineer",
                "/wiki/Communist",
                "/wiki/German_Democratic_Republic",
                "/wiki/National_Socialist_German_Workers_Party",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Turkey",
                "/wiki/University_of_Istanbul",
                "/wiki/United_States",
                "/wiki/Harvard_University",
                "/wiki/Hilda_Geiringer",
                "/wiki/Hydrodynamics",
                "/wiki/Aerodynamics",
                "/wiki/Dresden",
                "/wiki/Applied_Mathematics",
                "/wiki/Erhard_Schmidt",
                "/wiki/Test_pilot",
                "/wiki/Horsepower",
                "/wiki/KW",
                "/wiki/Habilitation",
                "/wiki/Brno",
                "/wiki/Waterwheel",
                "/wiki/German_Empire",
                "/wiki/Strasbourg",
                "/wiki/Alsace",
                "/wiki/France",
                "/wiki/Kingdom_of_Prussia",
                "/wiki/Citizenship",
                "/wiki/Brno_University_of_Technology",
                "/wiki/World_War_I",
                "/wiki/Ludwig_von_Mises",
                "/wiki/Economist",
                "/wiki/Austrian_School",
                "/wiki/Lviv",
                "/wiki/Austria-Hungary",
                "/wiki/Jewish_family",
                "/wiki/Akademisches_Gymnasium",
                "/wiki/Latin",
                "/wiki/Vienna_University_of_Technology",
                "/wiki/Georg_Hamel",
                "/wiki/Brno",
                "/wiki/Istanbul",
                "/wiki/Philipp_Frank",
                "/wiki/Robert_Musil",
                "/wiki/Rainer_Maria_Rilke",
                "/wiki/Logical_positivism",
                "/wiki/Ernst_Mach",
                "/wiki/Vienna_Circle",
                "/wiki/Logical_empiricism",
                "/wiki/Philipp_Frank",
                "/wiki/Hans_Hahn_(mathematician)",
                "/wiki/Otto_Neurath",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Ludwig_von_Mises",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/Solid_mechanics",
                "/wiki/Fluid_mechanics",
                "/wiki/Aerodynamics",
                "/wiki/Aeronautics",
                "/wiki/Statistics",
                "/wiki/Probability_theory",
                "/wiki/Harvard_University"
            ],
            "text": "In probability theory, he was the person who originally proposed the now famous \"birthday problem\".[10] He also defined the impossibility of a gambling system.[11][12]The Gesellschaft f\u00fcr Angewandte Mathematik und Mechanik[8] (International Association of Applied Mathematics and Mechanics) has awarded a Richard von Mises-Preis[9] (Prize) since 1989.He is also often credited for the Principle of Maximum Plastic Dissipation.In solid mechanics, von Mises made an important contribution to the theory of plasticity by formulating what has become known as the von Mises yield criterion, independently of Tytus Maksymilian Huber.Yet Andrey Kolmogorov, whose rival axiomatisation was better received, was less severe:and also wrote:His ideas were not unanimously well received, although Alexander Ostrowski had said of him:In aerodynamics, von Mises made notable advances in boundary-layer-flow theory and airfoil design. He developed the distortion energy theory of stress, which is one of the most important concepts used by engineers in material strength calculations.In 1950 Mises declined an offer of honorary membership from the Communist-dominated East German Academy of Science.With the rise of the National Socialist Party to power in 1933, Mises felt his position threatened,[citation needed] despite his First World War military service. He moved to Turkey, where he held the newly created chair of Pure and Applied Mathematics at the University of Istanbul. In 1939 he accepted a position in the United States, where in 1944 he was appointed as Gordon-McKay Professor of Aerodynamics and Applied Mathematics at Harvard University. He married Hilda Geiringer in 1943; she had been his assistant at the Institute and followed him to Turkey and then to the U.S. after losing her position in December 1933.After the war, Mises held the new chair of hydrodynamics and aerodynamics at the Dresden Technische Hochschule. In 1919 he was appointed director (and full professor) at the new Institute of Applied Mathematics created at the behest of Erhard Schmidt at the University of Berlin. In 1921 he founded the journal Zeitschrift f\u00fcr Angewandte Mathematik und Mechanik and became its editor.[7]Before the war he had already become a pilot and lectured on the design of aircraft, and in 1913 at Strasbourg he gave the first university course on powered flight. On the outbreak of war it was natural for him to join the Austro-Hungarian army as a test pilot and a flying instructor. In 1915, he supervised the construction of a 600-horsepower (450 kW) aircraft \u2013 the \"Mises-Flugzeug\" (Mises aircraft) for the Austrian army. It was completed in 1916 but never saw active war service.In 1908 Mises was awarded a doctorate from Vienna (his dissertation was on \"the determination of flywheel masses in crank drives\") and he received his habilitation from Br\u00fcnn (now Brno) (on \"Theory of the Waterwheels\") to lecture on engineering. In 1909, at 26, he was appointed professor of applied mathematics in Stra\u00dfburg, then part of the German Empire (now Strasbourg, Alsace, France) and received Prussian citizenship. His application for a teaching position at the Brno University of Technology was interrupted by the First World War.Eighteen months after his brother, Ludwig von Mises (who later became a prominent economist of the Austrian School), Richard von Mises was born in Lviv (nowadays Ukraine, then part of Austria-Hungary and named Lemberg) into a Jewish family. His parents were Arthur Edler von Mises, a doctor of technical sciences who worked as an expert for the Austrian State Railways, and Adele Landau. Richard and Ludwig also had a younger brother, who died as an infant. Richard attended the Akademisches Gymnasium in Vienna, from which he graduated with honors in Latin and mathematics in autumn 1901. After graduating in mathematics, physics and engineering from the Vienna University of Technology, he was appointed as Georg Hamel's assistant in Br\u00fcnn (now Brno). In 1905, still a student, he published an article on the geometry of curves called \"Zur konstruktiven Infinitesimalgeometrie der ebenen Kurven,\" in the prestigious Zeitschrift f\u00fcr Mathematik und Physik.During his time in Istanbul, Mises maintained close contact with Philipp Frank,[5] a logical positivist and Professor of Physics in Prague until 1938. His literary interests included the Austrian novelist Robert Musil and the poet Rainer Maria Rilke, on whom he became a recognized expert.[6]Although best known for his mathematical work, Mises also contributed to the philosophy of science as a neo-positivist, following the line of Ernst Mach. Historians of the Vienna Circle of logical empiricism recognize a \"first phase\" from 1907 through 1914 with Philipp Frank, Hans Hahn, and Otto Neurath.[citation needed] His older brother, Ludwig von Mises, held an opposite point of view with respect to positivism and epistemology.[4]Richard Edler von Mises[1] (/\u02c8mi\u02d0zi\u02d0z/;[2] German: [f\u0254n \u02c8mi\u02d0z\u0259s]; 19 April 1883 \u2013 14 July 1953) was a scientist and mathematician who worked on solid mechanics, fluid mechanics, aerodynamics, aeronautics, statistics and probability theory. He held the position of Gordon-McKay Professor of Aerodynamics and Applied Mathematics at Harvard University. He described his work in his own words shortly before his death as being on",
            "title": "Richard von Mises",
            "url": "https://en.wikipedia.org/wiki/Richard_Edler_von_Mises"
        },
        {
            "desc_links": [
                "/wiki/Matrix_decomposition",
                "/wiki/Sparse_matrix",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Mathematics",
                "/wiki/Eigenvalue_algorithm",
                "/wiki/Diagonalizable",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Eigenvalue",
                "/wiki/Eigenvector"
            ],
            "links": [
                "/wiki/Computational_problem",
                "/wiki/Google",
                "/wiki/PageRank",
                "/wiki/Twitter",
                "/wiki/Condition_number#matrices",
                "/wiki/Sparse_matrix",
                "/wiki/Spectral_radius",
                "/wiki/PageRank",
                "/wiki/Wikipedia:Please_clarify"
            ],
            "text": "Although the power iteration method approximates only one eigenvalue of a matrix, it remains useful for certain computational problems. For instance, Google uses it to calculate the PageRank of documents in their search engine,[2] and Twitter uses it to show users recommendations of who to follow.[3] For matrices that are well-conditioned and as sparse as the web matrix, the power iteration method can be more efficient than other methods of finding the dominant eigenvector.The method can also be used to calculate the spectral radius (the largest eigenvalue of a matrix) by computing the Rayleigh quotientThis algorithm is the one used to calculate such things as the Google PageRank.One may compute this with the following algorithm (shown in Python with NumPy):converges to the dominant eigenvalue[clarification needed].",
            "title": "Power iteration",
            "url": "https://en.wikipedia.org/wiki/Power_method"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "QR algorithm",
            "url": "https://en.wikipedia.org/wiki/QR_algorithm"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "John G. F. Francis",
            "url": "https://en.wikipedia.org/wiki/John_G.F._Francis"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Vera Kublanovskaya",
            "url": "https://en.wikipedia.org/wiki/Vera_Kublanovskaya"
        },
        {
            "desc_links": [
                "/wiki/Proof_theory",
                "/wiki/Mathematical_logic",
                "/wiki/Metamathematics",
                "/wiki/Georg_Cantor",
                "/wiki/Transfinite_number",
                "/wiki/Mathematics",
                "/wiki/Hilbert%27s_problems",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/Mathematician",
                "/wiki/Invariant_theory",
                "/wiki/Hilbert%27s_axioms",
                "/wiki/Hilbert_space",
                "/wiki/Functional_analysis"
            ],
            "links": [
                "/wiki/Continuum_hypothesis",
                "/wiki/Olga_Taussky-Todd",
                "/wiki/Analytic_number_theory",
                "/wiki/Hilbert%E2%80%93P%C3%B3lya_conjecture",
                "/wiki/Class_field_theory",
                "/wiki/Hilbert_class_field",
                "/wiki/Hilbert_symbol",
                "/wiki/Local_class_field_theory",
                "/wiki/Teiji_Takagi",
                "/wiki/Algebraic_number_theory",
                "/wiki/Zahlbericht",
                "/wiki/Waring%27s_problem",
                "/wiki/Hilbert_modular_form",
                "/wiki/Integral_equations",
                "/wiki/Richard_Courant",
                "/wiki/Methoden_der_mathematischen_Physik",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Hermann_Weyl",
                "/wiki/John_von_Neumann",
                "/wiki/Werner_Heisenberg",
                "/wiki/Matrix_mechanics",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hilbert_space",
                "/wiki/General_relativity",
                "/wiki/Einstein_field_equations",
                "/wiki/Einstein%E2%80%93Hilbert_action",
                "/wiki/Relativity_priority_dispute#General_Relativity_3",
                "/wiki/Kinetic_theory_of_gases",
                "/wiki/Radiation",
                "/wiki/Albert_Einstein",
                "/wiki/Hermann_Minkowski",
                "/wiki/Integral_equation",
                "/wiki/Euclidean_space",
                "/wiki/Hilbert_space",
                "/wiki/Stefan_Banach",
                "/wiki/Banach_spaces",
                "/wiki/Functional_analysis",
                "/wiki/Spectral_theory",
                "/wiki/Proof_theory",
                "/wiki/Recursion_theory",
                "/wiki/Mathematical_logic",
                "/wiki/Theoretical_computer_science",
                "/wiki/Alonzo_Church",
                "/wiki/Alan_Turing",
                "/wiki/Kurt_G%C3%B6del",
                "/wiki/G%C3%B6del%27s_incompleteness_theorem",
                "/wiki/Finitary",
                "/wiki/Grundlagen_der_Mathematik",
                "/wiki/Philosophy_of_mathematics",
                "/wiki/Bourbaki_group",
                "/wiki/Axiomatic_method",
                "/wiki/Ignorabimus",
                "/wiki/Emil_du_Bois-Reymond",
                "/wiki/Metamathematics",
                "/wiki/Mathematics",
                "/wiki/Formalism_(mathematics)",
                "/wiki/Hilbert%27s_twenty-fourth_problem",
                "/wiki/Nicolas_Bourbaki",
                "/wiki/Giuseppe_Peano",
                "/wiki/International_Congress_of_Mathematicians",
                "/wiki/Paris",
                "/wiki/Line_segment",
                "/wiki/Criteria_of_congruence_of_angles",
                "/wiki/Angle",
                "/wiki/Euclidean_geometry",
                "/wiki/Solid_geometry",
                "/wiki/Axiomatic_method",
                "/wiki/Moritz_Pasch",
                "/wiki/Point_(geometry)",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Schoenflies",
                "/wiki/Ernst_K%C3%B6tter",
                "/wiki/Grundlagen_der_Geometrie",
                "/wiki/Euclid%27s_elements",
                "/wiki/Euclid",
                "/wiki/Leopold_Kronecker",
                "/wiki/Leopold_Kronecker",
                "/wiki/Constructivism_(mathematics)",
                "/wiki/Luitzen_Egbertus_Jan_Brouwer",
                "/wiki/Intuitionist",
                "/wiki/Hermann_Weyl",
                "/wiki/Felix_Klein",
                "/wiki/Mathematische_Annalen",
                "/wiki/Paul_Gordan",
                "/wiki/Theorem",
                "/wiki/Hilbert%27s_basis_theorem",
                "/wiki/Algebraic_form",
                "/wiki/Constructive_proof",
                "/wiki/Existence_proof",
                "/wiki/Law_of_excluded_middle",
                "/wiki/Calvinist",
                "/wiki/Prussian_Union_of_churches",
                "/wiki/Agnostic",
                "/wiki/A_priori_and_a_posteriori",
                "/wiki/Hermann_Minkowski",
                "/wiki/Kurt_G%C3%B6del",
                "/wiki/G%C3%B6del%27s_incompleteness_theorems",
                "/wiki/Elementary_proof",
                "/wiki/Peano_arithmetic",
                "/wiki/Ignoramus_et_ignorabimus",
                "/wiki/Arnold_Sommerfeld",
                "/wiki/Bernhard_Rust",
                "/wiki/Law_for_the_Restoration_of_the_Professional_Civil_Service",
                "/wiki/Georg_August_University_of_G%C3%B6ttingen",
                "/wiki/Hermann_Weyl",
                "/wiki/Emmy_Noether",
                "/wiki/Edmund_Landau",
                "/wiki/Paul_Bernays",
                "/wiki/Mathematical_logic",
                "/wiki/Grundlagen_der_Mathematik",
                "/wiki/Wilhelm_Ackermann",
                "/wiki/Principles_of_Mathematical_Logic",
                "/wiki/Helmut_Hasse",
                "/wiki/Pernicious_anemia",
                "/wiki/Eugene_Wigner",
                "/wiki/Otto_Blumenthal",
                "/wiki/Felix_Bernstein_(mathematician)",
                "/wiki/Hermann_Weyl",
                "/wiki/Richard_Courant",
                "/wiki/Erich_Hecke",
                "/wiki/Hugo_Steinhaus",
                "/wiki/Wilhelm_Ackermann",
                "/wiki/Mathematische_Annalen",
                "/wiki/Hermann_Weyl",
                "/wiki/Emanuel_Lasker",
                "/wiki/Ernst_Zermelo",
                "/wiki/Carl_Gustav_Hempel",
                "/wiki/John_von_Neumann",
                "/wiki/Emmy_Noether",
                "/wiki/Alonzo_Church",
                "/wiki/University_of_K%C3%B6nigsberg",
                "/wiki/Felix_Klein",
                "/wiki/University_of_G%C3%B6ttingen",
                "/wiki/Adolf_Hurwitz",
                "/wiki/Professor",
                "/wiki/Ferdinand_von_Lindemann",
                "/wiki/Binary_quantic",
                "/wiki/Gymnasium_(school)",
                "/wiki/Immanuel_Kant",
                "/wiki/University_of_K%C3%B6nigsberg",
                "/wiki/Hermann_Minkowski",
                "/wiki/Province_of_Prussia",
                "/wiki/Kingdom_of_Prussia",
                "/wiki/K%C3%B6nigsberg",
                "/wiki/Znamensk,_Kaliningrad_Oblast",
                "/wiki/Proof_theory",
                "/wiki/Mathematical_logic",
                "/wiki/Metamathematics",
                "/wiki/Georg_Cantor",
                "/wiki/Transfinite_number",
                "/wiki/Mathematics",
                "/wiki/Hilbert%27s_problems",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/Mathematician",
                "/wiki/Invariant_theory",
                "/wiki/Hilbert%27s_axioms",
                "/wiki/Hilbert_space",
                "/wiki/Functional_analysis"
            ],
            "text": "His collected works (Gesammelte Abhandlungen) have been published several times. The original versions of his papers contained \"many technical errors of varying degree\";[50] when the collection was first published, the errors were corrected and it was found that this could be done without major changes in the statements of the theorems, with one exception\u2014a claimed proof of the continuum hypothesis.[51][52] The errors were nonetheless so numerous and significant that it took Olga Taussky-Todd three years to make the corrections.[52]Hilbert did not work in the central areas of analytic number theory, but his name has become known for the Hilbert\u2013P\u00f3lya conjecture, for reasons that are anecdotal.He made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.[49]Hilbert unified the field of algebraic number theory with his 1897 treatise Zahlbericht (literally \"report on numbers\"). He also resolved a significant number-theory problem formulated by Waring in 1770. As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers.[48] He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.Throughout this immersion in physics, Hilbert worked on putting rigor into the mathematics of physics. While highly dependent on higher mathematics, physicists tended to be \"sloppy\" with it. To a \"pure\" mathematician like Hilbert, this was both \"ugly\" and difficult to understand. As he began to understand physics and how physicists were using mathematics, he developed a coherent mathematical theory for what he found, most importantly in the area of integral equations. When his colleague Richard Courant wrote the now classic Methoden der mathematischen Physik (Methods of Mathematical Physics) including some of Hilbert's ideas, he added Hilbert's name as author even though Hilbert had not directly contributed to the writing. Hilbert said \"Physics is too hard for physicists\", implying that the necessary mathematics was generally beyond them; the Courant-Hilbert book made it easier for them.Additionally, Hilbert's work anticipated and assisted several advances in the mathematical formulation of quantum mechanics. His work was a key aspect of Hermann Weyl and John von Neumann's work on the mathematical equivalence of Werner Heisenberg's matrix mechanics and Erwin Schr\u00f6dinger's wave equation and his namesake Hilbert space plays an important part in quantum theory. In 1926 von Neumann showed that if atomic states were understood as vectors in Hilbert space, then they would correspond with both Schr\u00f6dinger's wave function theory and Heisenberg's matrices.[47]By 1907 Einstein had framed the fundamentals of the theory of gravity, but then struggled for nearly 8 years with a confounding problem of putting the theory into final form.[42] By early summer 1915, Hilbert's interest in physics had focused on general relativity, and he invited Einstein to G\u00f6ttingen to deliver a week of lectures on the subject.[43] Einstein received an enthusiastic reception at G\u00f6ttingen.[44] Over the summer Einstein learned that Hilbert was also working on the field equations and redoubled his own efforts. During November 1915 Einstein published several papers culminating in \"The Field Equations of Gravitation\" (see Einstein field equations).[45] Nearly simultaneously David Hilbert published \"The Foundations of Physics\", an axiomatic derivation of the field equations (see Einstein\u2013Hilbert action). Hilbert fully credited Einstein as the originator of the theory, and no public priority dispute concerning the field equations ever arose between the two men during their lives.[46] See more at priority.In 1912, three years after his friend's death, Hilbert turned his focus to the subject almost exclusively. He arranged to have a \"physics tutor\" for himself.[41] He started studying kinetic gas theory and moved on to elementary radiation theory and the molecular theory of matter. Even after the war started in 1914, he continued seminars and classes where the works of Albert Einstein and others were followed closely.Until 1912, Hilbert was almost exclusively a \"pure\" mathematician. When planning a visit from Bonn, where he was immersed in studying physics, his fellow mathematician and friend Hermann Minkowski joked he had to spend 10 days in quarantine before being able to visit Hilbert. In fact, Minkowski seems responsible for most of Hilbert's physics investigations prior to 1912, including their joint seminar in the subject in 1905.Around 1909, Hilbert dedicated himself to the study of differential and integral equations; his work had direct consequences for important parts of modern functional analysis. In order to carry out these studies, Hilbert introduced the concept of an infinite dimensional Euclidean space, later called Hilbert space. His work in this part of analysis provided the basis for important contributions to the mathematics of physics in the next two decades, though from an unanticipated direction. Later on, Stefan Banach amplified the concept, defining Banach spaces. Hilbert spaces are an important class of objects in the area of functional analysis, particularly of the spectral theory of self-adjoint linear operators, that grew up around it during the 20th century.Nevertheless, the subsequent achievements of proof theory at the very least clarified consistency as it relates to theories of central concern to mathematicians. Hilbert's work had started logic on this course of clarification; the need to understand G\u00f6del's work then led to the development of recursion theory and then mathematical logic as an autonomous discipline in the 1930s. The basis for later theoretical computer science, in the work of Alonzo Church and Alan Turing, also grew directly out of this 'debate'.G\u00f6del demonstrated that any non-contradictory formal system, which was comprehensive enough to include at least arithmetic, cannot demonstrate its completeness by way of its own axioms. In 1931 his incompleteness theorem showed that Hilbert's grand plan was impossible as stated. The second point cannot in any reasonable way be combined with the first point, as long as the axiom system is genuinely finitary.Hilbert and the mathematicians who worked with him in his enterprise were committed to the project. His attempt to support axiomatized mathematics with definitive principles, which could banish theoretical uncertainties, ended in failure.Hilbert published his views on the foundations of mathematics in the 2-volume work Grundlagen der Mathematik.Hilbert wrote in 1919:This program is still recognizable in the most popular philosophy of mathematics, where it is usually called formalism. For example, the Bourbaki group adopted a watered-down and selective version of it as adequate to the requirements of their twin projects of (a) writing encyclopedic foundational works, and (b) supporting the axiomatic method as a research tool. This approach has been successful and influential in relation with Hilbert's work in algebra and functional analysis, but has failed to engage in the same way with his interests in physics and logic.He seems to have had both technical and philosophical reasons for formulating this proposal. It affirmed his dislike of what had become known as the ignorabimus, still an active issue in his time in German thought, and traced back in that formulation to Emil du Bois-Reymond.In 1920 he proposed explicitly a research project (in metamathematics, as it was then termed) that became known as Hilbert's program. He wanted mathematics to be formulated on a solid and complete logical foundation. He believed that in principle this could be done, by showing that:In an account that had become standard by the mid-century, Hilbert's problem set was also a kind of manifesto, that opened the way for the development of the formalist school, one of three major schools of mathematics of the 20th century. According to the formalist, mathematics is manipulation of symbols according to agreed upon formal rules. It is therefore an autonomous activity of thought. There is, however, room to doubt whether Hilbert's own views were simplistically formalist in this sense.Some of these were solved within a short time. Others have been discussed throughout the 20th century, with a few now taken to be unsuitably open-ended to come to closure. Some even continue to this day to remain a challenge for mathematicians.He presented fewer than half the problems at the Congress, which were published in the acts of the Congress. In a subsequent publication, he extended the panorama, and arrived at the formulation of the now-canonical 23 Problems of Hilbert. See also Hilbert's twenty-fourth problem. The full text is important, since the exegesis of the questions still can be a matter of inevitable debate, whenever it is asked how many have been solved.The problem set was launched as a talk \"The Problems of Mathematics\" presented during the course of the Second International Congress of Mathematicians held in Paris. The introduction of the speech that Hilbert gave said:After re-working the foundations of classical geometry, Hilbert could have extrapolated to the rest of mathematics. His approach differed, however, from the later 'foundationalist' Russell-Whitehead or 'encyclopedist' Nicolas Bourbaki, and from his contemporary Giuseppe Peano. The mathematical community as a whole could enlist in problems, which he had identified as crucial aspects of the areas of mathematics he took to be key.Hilbert put forth a most influential list of 23 unsolved problems at the International Congress of Mathematicians in Paris in 1900. This is generally reckoned as the most successful and deeply considered compilation of open problems ever to be produced by an individual mathematician.Hilbert first enumerates the undefined concepts: point, line, plane, lying on (a relation between points and lines, points and planes, and lines and planes), betweenness, congruence of pairs of points (line segments), and congruence of angles. The axioms unify both the plane geometry and solid geometry of Euclid in a single system.Hilbert's approach signaled the shift to the modern axiomatic method. In this, Hilbert was anticipated by Moritz Pasch's work from 1882. Axioms are not taken as self-evident truths. Geometry may treat things, about which we have powerful intuitions, but it is not necessary to assign any explicit meaning to the undefined concepts. The elements, such as point, line, plane, and others, could be substituted, as Hilbert is reported to have said to Schoenflies and K\u00f6tter, by tables, chairs, glasses of beer and other such objects.[38] It is their defined relationships that are discussed.The text Grundlagen der Geometrie (tr.: Foundations of Geometry) published by Hilbert in 1899 proposes a formal set, called Hilbert's axioms, substituting for the traditional axioms of Euclid. They avoid weaknesses identified in those of Euclid, whose works at the time were still used textbook-fashion. It is difficult to specify the axioms used by Hilbert without referring to the publication history of the Grundlagen since Hilbert changed and modified them several times. The original monograph was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902.[36][37] This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. New editions followed the 7th, but the main text was essentially not revised.For all his successes, the nature of his proof stirred up more trouble than Hilbert could have imagined at the time. Although Kronecker had conceded, Hilbert would later respond to others' similar criticisms that \"many different constructions are subsumed under one fundamental idea\" \u2014 in other words (to quote Reid): \"Through a proof of existence, Hilbert had been able to obtain a construction\"; \"the proof\" (i.e. the symbols on the page) was \"the object\".[32] Not all were convinced. While Kronecker would die soon afterwards, his constructivist philosophy would continue with the young Brouwer and his developing intuitionist \"school\", much to Hilbert's torment in his later years.[33] Indeed, Hilbert would lose his \"gifted pupil\" Weyl to intuitionism \u2014 \"Hilbert was disturbed by his former student's fascination with the ideas of Brouwer, which aroused in Hilbert the memory of Kronecker\".[34] Brouwer the intuitionist in particular opposed the use of the Law of Excluded Middle over infinite sets (as Hilbert had used it). Hilbert would respond:Later, after the usefulness of Hilbert's method was universally recognized, Gordan himself would say:Klein, on the other hand, recognized the importance of the work, and guaranteed that it would be published without any alterations. Encouraged by Klein, Hilbert extended his method in a second article, providing estimations on the maximum degree of the minimum set of generators, and he sent it once more to the Annalen. After having read the manuscript, Klein wrote to him, saying:Hilbert sent his results to the Mathematische Annalen. Gordan, the house expert on the theory of invariants for the Mathematische Annalen, could not appreciate the revolutionary nature of Hilbert's theorem and rejected the article, criticizing the exposition because it was insufficiently comprehensive. His comment was:Hilbert's first work on invariant functions led him to the demonstration in 1888 of his famous finiteness theorem. Twenty years earlier, Paul Gordan had demonstrated the theorem of the finiteness of generators for binary forms using a complex computational approach. Attempts to generalize his method to functions with more than two variables failed because of the enormous difficulty of the calculations involved. In order to solve what had become known in some circles as Gordan's Problem, Hilbert realized that it was necessary to take a completely different path. As a result, he demonstrated Hilbert's basis theorem, showing the existence of a finite set of generators, for the invariants of quantics in any number of variables, but in an abstract form. That is, while demonstrating the existence of such a set, it was not a constructive proof \u2014 it did not display \"an object\" \u2014 but rather, it was an existence proof[29] and relied on use of the law of excluded middle in an infinite extension.Hilbert was baptized and raised a Calvinist in the Prussian Evangelical Church.[25] He later on left the Church and became an agnostic.[26] He also argued that mathematical truth was independent of the existence of God or other a priori assumptions.[27][28]Hilbert considered the mathematician Hermann Minkowski to be his \"best and truest friend\".[24]Hilbert's son Franz suffered throughout his life from an undiagnosed mental illness. His inferior intellect was a terrible disappointment to his father and this misfortune was a matter of distress to the mathematicians and students at G\u00f6ttingen.[23]In 1892, Hilbert married K\u00e4the Jerosch (1864\u20131945), \"the daughter of a K\u00f6nigsberg merchant, an outspoken young lady with an independence of mind that matched his own\".[22] While at K\u00f6nigsberg they had their one child, Franz Hilbert (1893\u20131969).The day before Hilbert pronounced these phrases at the 1930 annual meeting of the Society of German Scientists and Physicians, Kurt G\u00f6del\u2014in a round table discussion during the Conference on Epistemology held jointly with the Society meetings\u2014tentatively announced the first expression of his incompleteness theorem.[21] G\u00f6del's incompleteness theorems show that even elementary axiomatic systems such as Peano arithmetic are either self-contradicting or contain logical propositions that are impossible to prove or disprove.In English:The epitaph on his tombstone in G\u00f6ttingen consists of the famous lines he spoke at the conclusion of his retirement address to the Society of German Scientists and Physicians on 8 September 1930. The words were given in response to the Latin maxim: \"Ignoramus et ignorabimus\" or \"We do not know, we shall not know\":[20]By the time Hilbert died in 1943, the Nazis had nearly completely restaffed the university, as many of the former faculty had either been Jewish or married to Jews. Hilbert's funeral was attended by fewer than a dozen people, only two of whom were fellow academics, among them Arnold Sommerfeld, a theoretical physicist and also a native of K\u00f6nigsberg.[19] News of his death only became known to the wider world six months after he had died.About a year later, Hilbert attended a banquet and was seated next to the new Minister of Education, Bernhard Rust. Rust asked whether \"the Mathematical Institute really suffered so much because of the departure of the Jews\". Hilbert replied, \"Suffered? It doesn't exist any longer, does it!\"[17][18]Hilbert lived to see the Nazis purge many of the prominent faculty members at University of G\u00f6ttingen in 1933.[16] Those forced out included Hermann Weyl (who had taken Hilbert's chair when he retired in 1930), Emmy Noether and Edmund Landau. One who had to leave Germany, Paul Bernays, had collaborated with Hilbert in mathematical logic, and co-authored with him the important book Grundlagen der Mathematik (which eventually appeared in two volumes, in 1934 and 1939). This was a sequel to the Hilbert-Ackermann book Principles of Mathematical Logic from 1928. Hermann Weyl's successor was Helmut Hasse.Around 1925, Hilbert developed pernicious anemia, a then-untreatable vitamin deficiency whose primary symptom is exhaustion; his assistant Eugene Wigner described him as subject to \"enormous fatigue\" and how he \"seemed quite old\", and that even after eventually being diagnosed and treated, he \"was hardly a scientist after 1925, and certainly not a Hilbert.\"[15]Among his 69 Ph.D. students in G\u00f6ttingen were many who later became famous mathematicians, including (with date of thesis): Otto Blumenthal (1898), Felix Bernstein (1901), Hermann Weyl (1908), Richard Courant (1910), Erich Hecke (1910), Hugo Steinhaus (1911), and Wilhelm Ackermann (1925).[13] Between 1902 and 1939 Hilbert was editor of the Mathematische Annalen, the leading mathematical journal of the time.Among Hilbert's students were Hermann Weyl, chess champion Emanuel Lasker, Ernst Zermelo, and Carl Gustav Hempel. John von Neumann was his assistant. At the University of G\u00f6ttingen, Hilbert was surrounded by a social circle of some of the most important mathematicians of the 20th century, such as Emmy Noether and Alonzo Church.Hilbert remained at the University of K\u00f6nigsberg as a Privatdozent (senior lecturer) from 1886 to 1895. In 1895, as a result of intervention on his behalf by Felix Klein, he obtained the position of Professor of Mathematics at the University of G\u00f6ttingen. During the Klein and Hilbert years, G\u00f6ttingen became the preeminent institution in the mathematical world.[12] He remained there for the rest of his life.In 1884, Adolf Hurwitz arrived from G\u00f6ttingen as an Extraordinarius (i.e., an associate professor). An intense and fruitful scientific exchange among the three began, and Minkowski and Hilbert especially would exercise a reciprocal influence over each other at various times in their scientific careers. Hilbert obtained his doctorate in 1885, with a dissertation, written under Ferdinand von Lindemann,[2] titled \u00dcber invariante Eigenschaften spezieller bin\u00e4rer Formen, insbesondere der Kugelfunktionen (\"On the invariant properties of special binary forms, in particular the spherical harmonic functions\").In late 1872, Hilbert entered the Friedrichskolleg Gymnasium (Collegium fridericianum, the same school that Immanuel Kant had attended 140 years before); but, after an unhappy period, he transferred to (late 1879) and graduated from (early 1880) the more science-oriented Wilhelm Gymnasium.[8] Upon graduation, in autumn 1880, Hilbert enrolled at the University of K\u00f6nigsberg, the \"Albertina\". In early 1882, Hermann Minkowski (two years younger than Hilbert and also a native of K\u00f6nigsberg but had gone to Berlin for three semesters),[9] returned to K\u00f6nigsberg and entered the university. Hilbert developed a lifelong friendship with the shy, gifted Minkowski.[10][11]Hilbert, the first of two children of Otto and Maria Therese (Erdtmann) Hilbert, was born in the Province of Prussia, Kingdom of Prussia, either in K\u00f6nigsberg (according to Hilbert's own statement) or in Wehlau (known since 1946 as Znamensk) near K\u00f6nigsberg where his father worked at the time of his birth.[7]Hilbert and his students contributed significantly to establishing rigor and developed important tools used in modern mathematical physics. Hilbert is known as one of the founders of proof theory and mathematical logic, as well as for being among the first to distinguish between mathematics and metamathematics.[6]Hilbert adopted and warmly defended Georg Cantor's set theory and transfinite numbers. A famous example of his leadership in mathematics is his 1900 presentation of a collection of problems that set the course for much of the mathematical research of the 20th century.David Hilbert (/\u02c8h\u026alb\u0259rt/;[4] German: [\u02c8da\u02d0v\u026at \u02c8h\u026alb\u0250t]; 23 January 1862 \u2013 14 February 1943) was a German mathematician. He is recognized as one of the most influential and universal mathematicians of the 19th and early 20th centuries. Hilbert discovered and developed a broad range of fundamental ideas in many areas, including invariant theory and the axiomatization of geometry. He also formulated the theory of Hilbert spaces,[5] one of the foundations of functional analysis.",
            "title": "David Hilbert",
            "url": "https://en.wikipedia.org/wiki/David_Hilbert"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Domain_(mathematics)"
            ],
            "links": [
                "/wiki/Integral_equation",
                "/wiki/Fredholm_theory",
                "/wiki/Compact_operator",
                "/wiki/Banach_space",
                "/wiki/Fredholm_operator",
                "/wiki/Nuclear_operator",
                "/wiki/Fredholm_kernel",
                "/wiki/Linear_operator",
                "/wiki/Generalized_function",
                "/wiki/Schwartz_kernel_theorem",
                "/wiki/Line_integral",
                "/wiki/Characteristic_equation_(calculus)",
                "/wiki/Laplace_transform",
                "/wiki/Differential_equation",
                "/wiki/Integro-differential_equation",
                "/wiki/Time_domain",
                "/wiki/Frequency_domain",
                "/wiki/Eigenvalues",
                "/wiki/Inverse_Laplace_transform",
                "/wiki/Voltage",
                "/wiki/Electronic_device",
                "/wiki/Sine",
                "/wiki/Cosine",
                "/wiki/Orthonormal_basis",
                "/wiki/Fourier_series",
                "/wiki/Fourier_transform",
                "/wiki/Stochastic_discount_factor",
                "/wiki/Kernel_(statistics)",
                "/wiki/Domain_(mathematics)",
                "/wiki/Permutation",
                "/wiki/Variable_(mathematics)",
                "/wiki/Function_(mathematics)",
                "/wiki/Operator_(mathematics)",
                "/wiki/List_of_transforms",
                "/wiki/Mathematics",
                "/wiki/Domain_(mathematics)"
            ],
            "text": "The general theory of such integral equations is known as Fredholm theory. In this theory, the kernel is understood to be a compact operator acting on a Banach space of functions. Depending on the situation, the kernel is then variously referred to as the Fredholm operator, the nuclear operator or the Fredholm kernel.Although the properties of integral transforms vary widely, they have some properties in common. For example, every integral transform is a linear operator, since the integral is a linear operator, and in fact if the kernel is allowed to be a generalized function then all linear operators are integral transforms (a properly formulated version of this statement is the Schwartz kernel theorem).Here integral transforms are defined for functions on the real numbers, but they can be defined more generally for functions on a group.Note that there are alternative notations and conventions for the Fourier transform.In the limits of integration for the inverse transform, c is a constant which depends on the nature of the transform function. For example, for the one and two-sided Laplace transform, c must be greater than the largest real part of the zeroes of the transform function.Another usage example is the kernel in path integral:The Laplace transform finds wide application in physics and particularly in electrical engineering, where the characteristic equations that describe the behavior of an electric circuit in the complex frequency domain correspond to linear combinations of exponentially damped, scaled, and time-shifted sinusoids in the time domain. Other integral transforms find special applicability within other scientific and mathematical disciplines.As an example of an application of integral transforms, consider the Laplace transform. This is a technique that maps differential or integro-differential equations in the \"time\" domain into polynomial equations in what is termed the \"complex frequency\" domain. (Complex frequency is similar to actual, physical frequency but rather more general. Specifically, the imaginary component \u03c9 of the complex frequency s = -\u03c3 + i\u03c9 corresponds to the usual concept of frequency, viz., the rate at which a sinusoid cycles, whereas the real component \u03c3 of the complex frequency corresponds to the degree of \"damping\", i.e. an exponential decrease of the amplitude.) The equation cast in terms of complex frequency is readily solved in the complex frequency domain (roots of the polynomial equations in the complex frequency domain correspond to eigenvalues in the time domain), leading to a \"solution\" formulated in the frequency domain. Employing the inverse transform, i.e., the inverse procedure of the original Laplace transform, one obtains a time-domain solution. In this example, polynomials in the complex frequency domain (typically occurring in the denominator) correspond to power series in the time domain, while axial shifts in the complex frequency domain correspond to damping by decaying exponentials in the time domain.Using the Fourier series, just about any practical function of time (the voltage across the terminals of an electronic device for example) can be represented as a sum of sines and cosines, each suitably scaled (multiplied by a constant factor), shifted (advanced or retarded in time) and \"squeezed\" or \"stretched\" (increasing or decreasing the frequency). The sines and cosines in the Fourier series are an example of an orthonormal basis.The precursor of the transforms were the Fourier series to express functions in finite intervals. Later the Fourier transform was developed to remove the requirement of finite intervals.There are many applications of probability that rely on integral transforms, such as \"pricing kernel\" or stochastic discount factor, or the smoothing of data recovered from robust statistics; see kernel (statistics).Mathematical notation aside, the motivation behind integral transforms is easy to understand. There are many classes of problems that are difficult to solve\u2014or at least quite unwieldy algebraically\u2014in their original representations. An integral transform \"maps\" an equation from its original \"domain\" into another domain. Manipulating and solving the equation in the target domain can be much easier than manipulation and solution in the original domain. The solution is then mapped back to the original domain with the inverse of the integral transform.A symmetric kernel is one that is unchanged when the two variables are permuted.Some kernels have an associated inverse kernel K\u22121(u, t) which (roughly speaking) yields an inverse transform:There are numerous useful integral transforms. Each is specified by a choice of the function K of two variables, the kernel function, integral kernel or nucleus of the transform.The input of this transform is a function f, and the output is another function Tf. An integral transform is a particular kind of mathematical operator.An integral transform is any transform T of the following form:In mathematics, an integral transform maps an equation from its original domain into another domain where it is manipulated and solved much more easily than in the original domain. The solution is then mapped back to the original domain using the inverse of the integral transform.",
            "title": "Integral transform",
            "url": "https://en.wikipedia.org/wiki/Integral_operator"
        },
        {
            "desc_links": [
                "/wiki/Inflected_language",
                "/wiki/Grammatical_case",
                "/wiki/Germanic_strong_verb",
                "/wiki/Indo-European_languages",
                "/wiki/Latin",
                "/wiki/Greek_language",
                "/wiki/French_language",
                "/wiki/English_language",
                "/wiki/German_Standard_German",
                "/wiki/Austrian_Standard_German",
                "/wiki/Swiss_Standard_German",
                "/wiki/Pluricentric_language",
                "/wiki/German_dialects",
                "/wiki/Standard_German",
                "/wiki/Low_German",
                "/wiki/Plautdietsch_language",
                "/wiki/World_language",
                "/wiki/European_Union",
                "/wiki/French_language",
                "/wiki/Spanish_language_in_the_United_States",
                "/wiki/French_language_in_the_United_States#Language_study",
                "/wiki/American_Sign_Language",
                "/wiki/English_language",
                "/wiki/Russian_language",
                "/wiki/List_of_territorial_entities_where_German_is_an_official_language",
                "/wiki/United_Kingdom",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/File:De-Deutsch.ogg",
                "/wiki/West_Germanic_languages",
                "/wiki/Central_Europe",
                "/wiki/Germany",
                "/wiki/Austria",
                "/wiki/Switzerland",
                "/wiki/South_Tyrol",
                "/wiki/Italy",
                "/wiki/German-speaking_Community_of_Belgium",
                "/wiki/Liechtenstein",
                "/wiki/Luxembourg",
                "/wiki/Afrikaans_language",
                "/wiki/Dutch_language",
                "/wiki/English_language",
                "/wiki/Frisian_languages",
                "/wiki/Low_German",
                "/wiki/Luxembourgish_language",
                "/wiki/Yiddish",
                "/wiki/Germanic_languages"
            ],
            "links": [
                "/wiki/BBC_World_Service",
                "/wiki/Johann_Wolfgang_von_Goethe",
                "/wiki/Loanword",
                "/wiki/Martin_Luther",
                "/wiki/Gotthold_Ephraim_Lessing",
                "/wiki/Johann_Wolfgang_von_Goethe",
                "/wiki/Friedrich_Schiller",
                "/wiki/Heinrich_von_Kleist",
                "/wiki/E.T.A._Hoffmann",
                "/wiki/Bertolt_Brecht",
                "/wiki/Heinrich_Heine",
                "/wiki/Franz_Kafka",
                "/wiki/Nobel_Prize_in_literature",
                "/wiki/Theodor_Mommsen",
                "/wiki/Rudolf_Christoph_Eucken",
                "/wiki/Paul_von_Heyse",
                "/wiki/Gerhart_Hauptmann",
                "/wiki/Carl_Spitteler",
                "/wiki/Thomas_Mann",
                "/wiki/Nelly_Sachs",
                "/wiki/Hermann_Hesse",
                "/wiki/Heinrich_B%C3%B6ll",
                "/wiki/Elias_Canetti",
                "/wiki/G%C3%BCnter_Grass",
                "/wiki/Elfriede_Jelinek",
                "/wiki/Herta_M%C3%BCller",
                "/wiki/Middle_Ages",
                "/wiki/Walther_von_der_Vogelweide",
                "/wiki/Wolfram_von_Eschenbach",
                "/wiki/Nibelungenlied",
                "/wiki/Brothers_Grimm",
                "/wiki/Germanic_languages",
                "/wiki/Dental_fricative",
                "/wiki/Thou",
                "/wiki/Affricate",
                "/wiki/Glottal_stop",
                "/wiki/Moon",
                "/wiki/Hessen",
                "/wiki/Noun#Proper_nouns_and_common_nouns",
                "/wiki/Mecklenburg",
                "/wiki/Jan_Hofer",
                "/wiki/Das_Erste",
                "/wiki/Marietta_Slomka",
                "/wiki/ZDF",
                "/wiki/Secondary_stress",
                "/wiki/Captain_Bluebear",
                "/wiki/North_Rhine-Westphalia",
                "/wiki/Bavaria",
                "/wiki/Comma",
                "/wiki/Long_s",
                "/wiki/Long_s",
                "/wiki/Fraktur",
                "/wiki/Antiqua_(typeface_class)",
                "/wiki/Long_s",
                "/wiki/Lower_case",
                "/wiki/Blackletter",
                "/wiki/Typeface",
                "/wiki/Fraktur",
                "/wiki/Schwabacher",
                "/wiki/Penmanship",
                "/wiki/Kurrent",
                "/wiki/S%C3%BCtterlin",
                "/wiki/Sans-serif",
                "/wiki/Antiqua_(typeface_class)",
                "/wiki/Germanic_languages",
                "/wiki/Nazism",
                "/wiki/Aryan",
                "/wiki/World_War_II",
                "/wiki/Quotation_mark",
                "/wiki/Telephone_directory",
                "/wiki/Operating_systems",
                "/wiki/Microsoft_Windows",
                "/wiki/Alt_codes",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/Proper_noun",
                "/wiki/Capital_%C3%9F",
                "/wiki/German_orthography_reform_of_1996",
                "/wiki/Vowel_length",
                "/wiki/Diphthong",
                "/wiki/Germanic_umlaut",
                "/wiki/German_orthography",
                "/wiki/Donaudampfschiffahrtselektrizit%C3%A4tenhauptbetriebswerkbauunterbeamtengesellschaft",
                "/wiki/Umlaut_(diacritic)",
                "/wiki/Scharfes_s",
                "/wiki/%C3%9F",
                "/wiki/Cognate",
                "/wiki/%C3%96sterreichisches_W%C3%B6rterbuch",
                "/wiki/%C3%96WB",
                "/wiki/Dictionary",
                "/wiki/Republic_of_Austria",
                "/wiki/Federal_Ministry_of_Education,_Arts_and_Culture",
                "/wiki/Duden",
                "/wiki/Austrian_German",
                "/wiki/Southern_Germany",
                "/wiki/Bavaria",
                "/wiki/Switzerland",
                "/wiki/German_spelling_reform_of_1996",
                "/wiki/Italy",
                "/wiki/South_Tyrol",
                "/wiki/Duden",
                "/wiki/Dictionary",
                "/wiki/Konrad_Duden",
                "/wiki/Loanword",
                "/wiki/Etymology",
                "/wiki/Pronunciation",
                "/wiki/Synonyms",
                "/wiki/Usage_dictionary",
                "/wiki/Deutsches_W%C3%B6rterbuch",
                "/wiki/Jacob_Grimm",
                "/wiki/Wilhelm_Grimm",
                "/wiki/Text_corpus",
                "/wiki/Synonym",
                "/wiki/Loanword",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Notker_Labeo",
                "/wiki/Joachim_Heinrich_Campe",
                "/wiki/Ersatz",
                "/wiki/Roman_Empire",
                "/wiki/Renaissance_humanism",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Latin_language",
                "/wiki/Greek_language",
                "/wiki/Italian_language",
                "/wiki/Joachim_Heinrich_Campe",
                "/wiki/Future",
                "/wiki/Passive_voice",
                "/wiki/Modality_(semiotics)",
                "/wiki/Perfect_(grammar)",
                "/wiki/Auxiliary_verb",
                "/wiki/Auxiliary_verb",
                "/wiki/Perfect_(grammar)",
                "/wiki/Poetic_meter",
                "/wiki/Figures_of_speech",
                "/wiki/Sentence_(linguistics)",
                "/wiki/Auxiliary_verb",
                "/wiki/V2_word_order",
                "/wiki/Topic%E2%80%93comment",
                "/wiki/V2_word_order",
                "/wiki/SOV_word_order",
                "/wiki/Clause",
                "/wiki/Finite_verb",
                "/wiki/Parenthetical_referencing",
                "/wiki/German_verbs",
                "/wiki/Finite_verb",
                "/wiki/Compound_(linguistics)",
                "/wiki/English_compounds",
                "/wiki/Rinderkennzeichnungs-_und_Rindfleischetikettierungs%C3%BCberwachungsaufgaben%C3%BCbertragungsgesetz",
                "/wiki/Luxembourgish_language",
                "/wiki/North_Frisian_language",
                "/wiki/Old_High_German",
                "/wiki/Indo-European_languages",
                "/wiki/Latin",
                "/wiki/Ancient_Greek",
                "/wiki/Sanskrit",
                "/wiki/Old_English",
                "/wiki/Icelandic_language",
                "/wiki/Russian_language",
                "/wiki/Article_(grammar)",
                "/wiki/Natural_language",
                "/wiki/German_nouns",
                "/wiki/Fusional_language",
                "/wiki/Inflection",
                "/wiki/Grammatical_gender",
                "/wiki/Austria",
                "/wiki/Vienna",
                "/wiki/Lower_Austria",
                "/wiki/Upper_Austria",
                "/wiki/Styria",
                "/wiki/Carinthia",
                "/wiki/Salzburg_(state)",
                "/wiki/Burgenland",
                "/wiki/Tyrol_(state)",
                "/wiki/Bavaria",
                "/wiki/Upper_Bavaria",
                "/wiki/Lower_Bavaria",
                "/wiki/Upper_Palatinate",
                "/wiki/South_Tyrol",
                "/wiki/Saxony",
                "/wiki/Vogtl%C3%A4ndisch_dialect",
                "/wiki/Samnaun",
                "/wiki/Vienna",
                "/wiki/Munich",
                "/wiki/Switzerland",
                "/wiki/High_Alemannic_German",
                "/wiki/Swiss_Plateau",
                "/wiki/Highest_Alemannic_German",
                "/wiki/Low_Alemannic_German",
                "/wiki/Basel",
                "/wiki/Swabian_German",
                "/wiki/Swabia_(Bavaria)",
                "/wiki/Vorarlberg",
                "/wiki/Alsace",
                "/wiki/Liechtenstein",
                "/wiki/Tyrol_(state)",
                "/wiki/Reutte_District",
                "/wiki/Alsatian_dialect",
                "/wiki/Stuttgart",
                "/wiki/Z%C3%BCrich",
                "/wiki/Upper_German",
                "/wiki/Alemannic_German",
                "/wiki/Bavarian_language",
                "/wiki/Alsace",
                "/wiki/Alsatian_dialect",
                "/wiki/Low_Alemannic_German",
                "/wiki/Karlsruhe",
                "/wiki/Heilbronn",
                "/wiki/Franconia",
                "/wiki/Saxony",
                "/wiki/Vogtland",
                "/wiki/Bavaria",
                "/wiki/Upper_Franconia",
                "/wiki/Middle_Franconia",
                "/wiki/Lower_Franconia",
                "/wiki/South_Thuringia",
                "/wiki/Thuringia",
                "/wiki/Heilbronn-Franken",
                "/wiki/Tauber_Franconia",
                "/wiki/Baden-W%C3%BCrttemberg",
                "/wiki/Nuremberg",
                "/wiki/W%C3%BCrzburg",
                "/wiki/High_Franconian_German",
                "/wiki/East_Franconian_German",
                "/wiki/South_Franconian_German",
                "/wiki/Thuringian_dialect",
                "/wiki/Upper_Saxon_German",
                "/wiki/Erzgebirgisch",
                "/wiki/Lausitzisch-neum%C3%A4rkisch_dialects",
                "/wiki/Silesia",
                "/wiki/Silesian_German",
                "/wiki/East_Prussia",
                "/wiki/High_Prussian_dialect",
                "/wiki/Berlin",
                "/wiki/Leipzig",
                "/wiki/Transylvanian_Saxon_dialect",
                "/wiki/Transylvania",
                "/wiki/Cologne",
                "/wiki/Frankfurt",
                "/wiki/Central_Franconian_dialects",
                "/wiki/Ripuarian_language",
                "/wiki/Moselle_Franconian_dialects",
                "/wiki/Rhine_Franconian_dialects",
                "/wiki/Hessian_dialects",
                "/wiki/Palatine_German_language",
                "/wiki/Central_German",
                "/wiki/Aachen",
                "/wiki/G%C3%B6rlitz",
                "/wiki/Franconian_languages",
                "/wiki/West_Central_German",
                "/wiki/East_Central_German",
                "/wiki/Central_German",
                "/wiki/High_Franconian_German",
                "/wiki/Upper_German_language",
                "/wiki/Ashkenazi_Jew",
                "/wiki/Yiddish",
                "/wiki/Hebrew_alphabet",
                "/wiki/Low_Franconian_languages",
                "/wiki/Dutch_language",
                "/wiki/Low_German",
                "/wiki/Netherlands",
                "/wiki/Belgium",
                "/wiki/North_Rhine-Westphalia",
                "/wiki/Lower_Rhine",
                "/wiki/Meuse-Rhenish",
                "/wiki/South_Guelderish",
                "/wiki/Limburgish_language",
                "/wiki/Rhine",
                "/wiki/Low_Bergish",
                "/wiki/Ripuarian_language",
                "/wiki/Central_Franconian_dialects",
                "/wiki/East_Bergish",
                "/wiki/Westphalian_language",
                "/wiki/D%C3%BCsseldorf",
                "/wiki/Duisburg",
                "/wiki/Education",
                "/wiki/Standard_German",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Missingsch",
                "/wiki/Low_Franconian",
                "/wiki/World_War_II",
                "/wiki/Hamburg",
                "/wiki/Dortmund",
                "/wiki/Middle_Low_German",
                "/wiki/Lingua_franca",
                "/wiki/Hanseatic_League",
                "/wiki/Luther_Bible",
                "/wiki/Early_New_High_German",
                "/wiki/Central_German",
                "/wiki/Upper_German",
                "/wiki/Low_German",
                "/wiki/Asia",
                "/wiki/Americas",
                "/wiki/High_German_languages",
                "/wiki/Low_German",
                "/wiki/Lower_Saxony",
                "/wiki/West_Germanic_languages",
                "/wiki/Germanic_languages",
                "/wiki/Language_family",
                "/wiki/Indo-European_language_family",
                "/wiki/Lexicon",
                "/wiki/Phonology",
                "/wiki/Syntax",
                "/wiki/Language",
                "/wiki/Mutual_intelligibility",
                "/wiki/Ethnologue",
                "/wiki/Switzerland",
                "/wiki/Diglossia",
                "/wiki/Swiss_Standard_German",
                "/wiki/Austrian_Standard_German",
                "/wiki/Linguistics",
                "/wiki/Dialect",
                "/wiki/Variety_(linguistics)",
                "/wiki/Standard_German",
                "/wiki/Pluricentric_language",
                "/wiki/Vocabulary",
                "/wiki/Pronunciation",
                "/wiki/Grammar",
                "/wiki/Orthography",
                "/wiki/Dialects",
                "/wiki/Pluricentric_language",
                "/wiki/Written_language",
                "/wiki/Northern_Germany",
                "/wiki/French_language",
                "/wiki/Spanish_language",
                "/wiki/English_language",
                "/wiki/Spanish_language",
                "/wiki/Russia",
                "/wiki/Eastern_Europe",
                "/wiki/Northern_Europe",
                "/wiki/Czech_Republic",
                "/wiki/Croatia",
                "/wiki/Denmark",
                "/wiki/Netherlands",
                "/wiki/Slovakia",
                "/wiki/Hungary",
                "/wiki/Slovenia",
                "/wiki/Sweden",
                "/wiki/Poland",
                "/wiki/Lingua_franca",
                "/wiki/German-based_creole_languages",
                "/wiki/Unserdeutsch",
                "/wiki/German_New_Guinea",
                "/wiki/Micronesia",
                "/wiki/Queensland",
                "/wiki/Western_Australia",
                "/wiki/Puhoi",
                "/wiki/Nelson,_New_Zealand",
                "/wiki/Gore,_New_Zealand",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Australia",
                "/wiki/South_Australia",
                "/wiki/Silesia",
                "/wiki/Australian_English",
                "/wiki/Barossa_German",
                "/wiki/Barossa_Valley",
                "/wiki/Adelaide",
                "/wiki/World_War_I",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Rio_Grande_do_Sul",
                "/wiki/Riograndenser_Hunsr%C3%BCckisch",
                "/wiki/Santa_Catarina_(state)",
                "/wiki/Paran%C3%A1_(state)",
                "/wiki/S%C3%A3o_Paulo",
                "/wiki/Esp%C3%ADrito_Santo",
                "/wiki/Argentina",
                "/wiki/Chile",
                "/wiki/Paraguay",
                "/wiki/Venezuela",
                "/wiki/Peru",
                "/wiki/Bolivia",
                "/wiki/German_Mexican",
                "/wiki/Mexico_City",
                "/wiki/Puebla",
                "/wiki/Mazatl%C3%A1n",
                "/wiki/Tapachula",
                "/wiki/Ecatepec_de_Morelos",
                "/wiki/Chihuahua_(state)",
                "/wiki/Durango",
                "/wiki/Zacatecas",
                "/wiki/Canada",
                "/wiki/German_Canadians",
                "/wiki/British_Columbia",
                "/wiki/Ontario",
                "/wiki/Kitchener,_Ontario",
                "/wiki/Montreal",
                "/wiki/Toronto",
                "/wiki/Vancouver",
                "/wiki/Second_World_War",
                "/wiki/German-Canadian",
                "/wiki/French_language",
                "/wiki/English_language",
                "/wiki/Hutterite_German",
                "/wiki/Austro-Bavarian",
                "/wiki/Washington_(state)",
                "/wiki/Montana",
                "/wiki/North_Dakota",
                "/wiki/South_Dakota",
                "/wiki/Minnesota",
                "/wiki/Alberta",
                "/wiki/Saskatchewan",
                "/wiki/Manitoba",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Carinthia_(state)",
                "/wiki/Texas_German",
                "/wiki/Adelsverein",
                "/wiki/Amana_Colonies",
                "/wiki/Amana_German",
                "/wiki/Plautdietsch",
                "/wiki/Minority_language",
                "/wiki/Mennonite",
                "/wiki/Pennsylvania_German_language",
                "/wiki/West_Central_German",
                "/wiki/Amish",
                "/wiki/Palatine_German_language",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/United_States_Brewers%27_Association",
                "/wiki/World_War_I",
                "/wiki/Pennsylvania",
                "/wiki/Amish",
                "/wiki/Hutterites",
                "/wiki/Dunkard_Township,_Greene_County,_Pennsylvania",
                "/wiki/Mennonites",
                "/wiki/Hutterite_German",
                "/wiki/West_Central_German",
                "/wiki/Pennsylvania_German_language",
                "/wiki/Kansas",
                "/wiki/Volga_German",
                "/wiki/History_of_Germans_in_Russia_and_the_Soviet_Union",
                "/wiki/Baltic_Germans",
                "/wiki/South_Dakota",
                "/wiki/Montana",
                "/wiki/Texas",
                "/wiki/Texas_German",
                "/wiki/Wisconsin",
                "/wiki/Indiana",
                "/wiki/Oregon",
                "/wiki/Oklahoma",
                "/wiki/Ohio",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Pietists",
                "/wiki/Iowa",
                "/wiki/Amana_Colonies",
                "/wiki/St._Louis,_Missouri",
                "/wiki/Chicago",
                "/wiki/New_York_City",
                "/wiki/Milwaukee",
                "/wiki/Pittsburgh",
                "/wiki/Cincinnati",
                "/wiki/North_Dakota",
                "/wiki/South_Dakota",
                "/wiki/New_Ulm,_Minnesota",
                "/wiki/Minnesota",
                "/wiki/Bismarck,_North_Dakota",
                "/wiki/Munich,_North_Dakota",
                "/wiki/Karlsruhe,_North_Dakota",
                "/wiki/Strasburg,_North_Dakota",
                "/wiki/New_Braunfels",
                "/wiki/Fredericksburg,_Texas",
                "/wiki/Muenster,_Texas",
                "/wiki/Kiel,_Wisconsin",
                "/wiki/Berlin,_Wisconsin",
                "/wiki/Germantown,_Wisconsin",
                "/wiki/South_Africa",
                "/wiki/Low_German",
                "/wiki/Wartburg,_KwaZulu-Natal",
                "/wiki/Kroondal",
                "/wiki/North_West_(South_African_province)",
                "/wiki/Pan_South_African_Language_Board",
                "/wiki/Deutsche_Schule_Pretoria",
                "/wiki/Afrikaans",
                "/wiki/South_Africa",
                "/wiki/Apartheid",
                "/wiki/Evangelical_Lutheran_Church_in_Namibia_(GELK)",
                "/wiki/Deutsche_H%C3%B6here_Privatschule_Windhoek",
                "/wiki/Namibian_Broadcasting_Corporation",
                "/wiki/EES_(rapper)",
                "/wiki/Allgemeine_Zeitung_(Namibia)",
                "/wiki/German_South-West_Africa",
                "/wiki/German_Empire",
                "/wiki/Pidgin",
                "/wiki/Namibian_Black_German",
                "/wiki/France",
                "/wiki/High_German",
                "/wiki/Alsatian_dialect",
                "/wiki/Moselle_Franconian",
                "/wiki/Regional_language",
                "/wiki/European_Charter_for_Regional_and_Minority_Languages",
                "/wiki/Netherlands",
                "/wiki/Limburgish_language",
                "/wiki/Frisian_languages",
                "/wiki/Low_German",
                "/wiki/Flight_and_expulsion_of_Germans",
                "/wiki/Organised_persecution_of_ethnic_Germans",
                "/wiki/World_War",
                "/wiki/German_Sprachraum",
                "/wiki/German_diaspora",
                "/wiki/Foreign_language",
                "/wiki/Geographical_distribution_of_German_speakers",
                "/wiki/Alemannic_German",
                "/wiki/Alsatian_dialect",
                "/wiki/Low_German",
                "/wiki/First_language",
                "/wiki/Second_language",
                "/wiki/Foreign_language",
                "/wiki/German_orthography_reform_of_1996",
                "/wiki/Standardization",
                "/wiki/Stage_(theatre)",
                "/wiki/B%C3%BChnendeutsch",
                "/wiki/Deutsches_W%C3%B6rterbuch",
                "/wiki/Dictionary",
                "/wiki/Brothers_Grimm",
                "/wiki/Duden_Handbook",
                "/wiki/Banat",
                "/wiki/Transylvania",
                "/wiki/Timi%C8%99oara",
                "/wiki/Sibiu",
                "/wiki/Bra%C8%99ov",
                "/wiki/Prague",
                "/wiki/Budapest",
                "/wiki/Buda",
                "/wiki/Germanization",
                "/wiki/Pozsony",
                "/wiki/Bratislava",
                "/wiki/Zagreb",
                "/wiki/Ljubljana",
                "/wiki/Habsburg_Empire",
                "/wiki/Merchant",
                "/wiki/New_Testament",
                "/wiki/Old_Testament",
                "/wiki/Saxony",
                "/wiki/Gloss_(annotation)",
                "/wiki/Chancery_(medieval_office)",
                "/wiki/Holy_Roman_Emperor",
                "/wiki/Maximilian_I,_Holy_Roman_Emperor",
                "/wiki/Electorate_of_Saxony",
                "/wiki/Duchy_of_Saxe-Wittenberg",
                "/wiki/Early_New_High_German",
                "/wiki/Philology",
                "/wiki/Wilhelm_Scherer",
                "/wiki/Thirty_Years%27_War",
                "/wiki/List_of_states_in_the_Holy_Roman_Empire",
                "/wiki/Holy_Roman_Empire",
                "/wiki/Principality",
                "/wiki/German_dialects",
                "/wiki/Printing_press",
                "/wiki/Luther_Bible",
                "/wiki/Europe",
                "/wiki/Black_Death",
                "/wiki/Nibelungenlied",
                "/wiki/Epic_poetry",
                "/wiki/Dragon",
                "/wiki/Sigurd",
                "/wiki/Iwein",
                "/wiki/King_Arthur",
                "/wiki/Hartmann_von_Aue",
                "/wiki/Lyric_poetry",
                "/wiki/Parzival",
                "/wiki/Tristan",
                "/wiki/Sachsenspiegel",
                "/wiki/Middle_Low_German",
                "/wiki/Vowel_breaking",
                "/wiki/Schwa",
                "/wiki/Middle_High_German",
                "/wiki/Elbe",
                "/wiki/Saale",
                "/wiki/Slavs",
                "/wiki/Ostsiedlung",
                "/wiki/Hohenstaufen",
                "/wiki/Swabia",
                "/wiki/Oral_literature",
                "/wiki/Phonetics",
                "/wiki/Phonology",
                "/wiki/Morphology_(linguistics)",
                "/wiki/Syntax",
                "/wiki/Standard_language",
                "/wiki/Dialect",
                "/wiki/Monastery",
                "/wiki/Scriptorium",
                "/wiki/Old_High_German",
                "/wiki/Elder_Futhark",
                "/wiki/Pforzen_buckle",
                "/wiki/Abrogans",
                "/wiki/Glossary",
                "/wiki/Latin",
                "/wiki/Muspilli",
                "/wiki/Merseburg_Incantations",
                "/wiki/Hildebrandslied",
                "/wiki/Georgslied",
                "/wiki/Ludwigslied",
                "/wiki/Bavarian_dialects",
                "/wiki/Last_Judgment",
                "/wiki/Paganism",
                "/wiki/Epic_poetry",
                "/wiki/Old_Saxon",
                "/wiki/Alemanni",
                "/wiki/Bavarian_dialects",
                "/wiki/Thuringian_dialect",
                "/wiki/Irminones",
                "/wiki/Germany",
                "/wiki/Austria",
                "/wiki/History_of_German",
                "/wiki/High_German_consonant_shift",
                "/wiki/Migration_period",
                "/wiki/Old_High_German",
                "/wiki/Old_Saxon",
                "/wiki/Sound_change",
                "/wiki/Voice_(phonetics)",
                "/wiki/Stop_consonant",
                "/wiki/Germany",
                "/wiki/Denmark",
                "/wiki/North_Frisian_language",
                "/wiki/Schleswig-Holstein",
                "/wiki/Saterland_Frisian_language",
                "/wiki/Lower_Saxony",
                "/wiki/West_Frisian_language",
                "/wiki/Netherlands",
                "/wiki/English_language",
                "/wiki/Scots_language",
                "/wiki/Anglo-Frisian_languages",
                "/wiki/Standard_German",
                "/wiki/Thuringian_dialect",
                "/wiki/Upper_Saxon_German",
                "/wiki/Central_German",
                "/wiki/High_German_languages",
                "/wiki/Luxembourgish_language",
                "/wiki/Central_Franconian_dialects",
                "/wiki/Yiddish",
                "/wiki/Upper_German",
                "/wiki/List_of_territorial_entities_where_German_is_an_official_language",
                "/wiki/Swiss_German",
                "/wiki/Alemannic_German",
                "/wiki/France",
                "/wiki/Regions_of_France",
                "/wiki/Grand_Est",
                "/wiki/Alsatian_dialect",
                "/wiki/High_Franconian_German",
                "/wiki/Lorraine_Franconian",
                "/wiki/Benrath_line",
                "/wiki/Uerdingen_line",
                "/wiki/D%C3%BCsseldorf",
                "/wiki/D%C3%BCsseldorf-Benrath",
                "/wiki/Krefeld",
                "/wiki/Uerdingen",
                "/wiki/High_German_consonant_shift",
                "/wiki/High_German_languages",
                "/wiki/Low_German",
                "/wiki/Low_Franconian_languages",
                "/wiki/Irminones",
                "/wiki/Ingvaeonic_languages",
                "/wiki/Istvaeones",
                "/wiki/Standard_German",
                "/wiki/West_Germanic_languages",
                "/wiki/Germanic_languages",
                "/wiki/Indo-European_languages",
                "/wiki/North_Germanic_languages",
                "/wiki/East_Germanic_languages",
                "/wiki/West_Germanic_languages",
                "/wiki/Danish_language",
                "/wiki/Swedish_language",
                "/wiki/Norwegian_language",
                "/wiki/Faroese_language",
                "/wiki/Icelandic_language",
                "/wiki/Old_Norse",
                "/wiki/Gothic_language",
                "/wiki/English_language",
                "/wiki/Dutch_language",
                "/wiki/Yiddish",
                "/wiki/Afrikaans",
                "/wiki/Inflected_language",
                "/wiki/Grammatical_case",
                "/wiki/Germanic_strong_verb",
                "/wiki/Indo-European_languages",
                "/wiki/Latin",
                "/wiki/Greek_language",
                "/wiki/French_language",
                "/wiki/English_language",
                "/wiki/German_Standard_German",
                "/wiki/Austrian_Standard_German",
                "/wiki/Swiss_Standard_German",
                "/wiki/Pluricentric_language",
                "/wiki/German_dialects",
                "/wiki/Standard_German",
                "/wiki/Low_German",
                "/wiki/Plautdietsch_language",
                "/wiki/World_language",
                "/wiki/European_Union",
                "/wiki/French_language",
                "/wiki/Spanish_language_in_the_United_States",
                "/wiki/French_language_in_the_United_States#Language_study",
                "/wiki/American_Sign_Language",
                "/wiki/English_language",
                "/wiki/Russian_language",
                "/wiki/List_of_territorial_entities_where_German_is_an_official_language",
                "/wiki/United_Kingdom"
            ],
            "text": "The German state broadcaster Deutsche Welle is the equivalent of the British BBC World Service and provides radio and television broadcasts in German and 30 other languages across the globe.[95] Its German language services are tailored for German language learners by being spoken at slow speed. Deutsche Welle also provides an e-learning website to learn German.The Dortmund-based Verein Deutsche Sprache (VDS), which was founded in 1997, supports the German language and is the largest language association of citizens in the world. The VDS has more than thirty-five thousand members in over seventy countries. Its founder, statistics professor Dr. Walter Kr\u00e4mer, has remained chairperson of the association from its beginnings.[94]The government-backed Goethe-Institut[93] (named after the famous German author Johann Wolfgang von Goethe) aims to enhance the knowledge of German culture and language within Europe and the rest of the world. This is done by holding exhibitions and conferences with German-related themes, and providing training and guidance in the learning and use of the German language. For example, the Goethe-Institut teaches the Goethe-Zertifikat German language qualification.The use and learning of the German language are promoted by a number of organisations.English has taken many loanwords from German, often without any change of spelling (aside from, often, the elimination of umlauts and not capitalizing nouns):Reformer and theologian Martin Luther, who was the first to translate the Bible into German, is widely credited for having set the basis for the modern \"High German\" language. Among the most well known poets and authors in German are Lessing, Goethe, Schiller, Kleist, Hoffmann, Brecht, Heine, and Kafka. Thirteen German-speaking people have won the Nobel Prize in literature: Theodor Mommsen, Rudolf Christoph Eucken, Paul von Heyse, Gerhart Hauptmann, Carl Spitteler, Thomas Mann, Nelly Sachs, Hermann Hesse, Heinrich B\u00f6ll, Elias Canetti, G\u00fcnter Grass, Elfriede Jelinek and Herta M\u00fcller.The German language is used in German literature and can be traced back to the Middle Ages, with the most notable authors of the period being Walther von der Vogelweide and Wolfram von Eschenbach. The Nibelungenlied, whose author remains unknown, is also an important work of the epoch. The fairy tales collections collected and published by Jacob and Wilhelm Grimm in the 19th century became famous throughout the world.Likewise, the gh in Germanic English words, pronounced in several different ways in modern English (as an f, or not at all), can often be linked to German ch: \"to laugh\" \u2192 lachen, \"through\" \u2192 durch, \"high\" \u2192 hoch, \"naught\" \u2192 nichts, \"light\" \u2192 leicht or Licht, \"sight\" \u2192 Sicht, \"daughter\" \u2192 Tochter, \"neighbour\" \u2192 Nachbar.German does not have any dental fricatives (as English th). The th sounds, which the English language still has, disappeared on the continent in German with the consonant shifts between the 8th and the 10th centuries.[91] It is sometimes possible to find parallels between English and German by replacing the English th with d in German: \"Thank\" \u2192 in German Dank, \"this\" and \"that\" \u2192 dies and das, \"thou\" (old 2nd person singular pronoun) \u2192 du, \"think\" \u2192 denken, \"thirsty\" \u2192 durstig and many other examples.With approximately 25 phonemes, the German consonant system exhibits an average number of consonants in comparison with other languages. One of the more noteworthy ones is the unusual affricate /p\u0361f/. The consonant inventory of the standard language is shown below.In most varieties of standard German, syllables that begin with a vowel are preceded by a glottal stop [\u0294].Additionally, the digraph ie generally represents the phoneme /i\u02d0/, which is not a diphthong. In many varieties, an /r/ at the end of a syllable is vocalised. However, a sequence of a vowel followed by such a vocalised /r/ is not a phonemic diphthong: B\u00e4r [b\u025b\u02d0\u0250\u032f] \"bear\", er [e\u02d0\u0250\u032f] \"he\", wir [vi\u02d0\u0250\u032f] \"we\", Tor [to\u02d0\u0250\u032f] \"gate\", kurz [k\u028a\u0250\u032fts] \"short\", W\u00f6rter [v\u0153\u0250\u032ft\u0250] \"words\".German vowels can form the following digraphs (in writing) and diphthongs (in pronunciation); note that the pronunciation of some of them (ei, \u00e4u, eu) is very different from what one would expect when considering the component letters:Both of these rules have exceptions (e.g. hat [hat] \"has\" is short despite the first rule; Mond [mo\u02d0nt], \"moon\" is long despite the second rule). For an i that is neither in the combination ie (making it long) nor followed by a double consonant or cluster (making it short), there is no general rule. In some cases, there are regional differences: In central Germany (Hessen), the o in the proper name \"Hoffmann\" is pronounced long, whereas most other Germans would pronounce it short; the same applies to the e in the geographical name \"Mecklenburg\" for people in that region. The word St\u00e4dte \"cities\", is pronounced with a short vowel [\u02c8\u0283t\u025bt\u0259] by some (Jan Hofer, ARD Television) and with a long vowel [\u02c8\u0283t\u025b\u02d0t\u0259] by others (Marietta Slomka, ZDF Television). Finally, a vowel followed by ch can be short (Fach [fax] \"compartment\", K\u00fcche [\u02c8k\u028f\u00e7\u0259] \"kitchen\") or long (Suche [\u02c8zu\u02d0x\u0259] \"search\", B\u00fccher [\u02c8by\u02d0\u00e7\u0250] \"books\") almost at random. Thus, Lache is homographous between [la\u02d0x\u0259] Lache \"puddle\" and [lax\u0259] Lache \"manner of laughing\" (colloquial) or lache! \"laugh!\" (imperative).Whether any particular vowel letter represents the long or short phoneme is not completely predictable, although the following regularities exist:In many varieties of standard German, an unstressed /\u025br/ is not pronounced [\u0259r], but vocalised to [\u0250].Short /\u025b/ is realized as [\u025b] in stressed syllables (including secondary stress), but as [\u0259] in unstressed syllables. Note that stressed short /\u025b/ can be spelled either with e or with \u00e4 (for instance, h\u00e4tte \"would have\" and Kette \"chain\" rhyme). In general, the short vowels are open and the long vowels are close. The one exception is the open /\u025b\u02d0/ sound of long \u00c4; in some varieties of standard German, /\u025b\u02d0/ and /e\u02d0/ have merged into [e\u02d0], removing this anomaly. In that case, pairs like B\u00e4ren/Beeren 'bears/berries' or \u00c4hre/Ehre 'spike (of wheat)/honour' become homophonous (see: Captain Bluebear).In German, vowels (excluding diphthongs; see below) are either short or long, as follows:Thus F\u00fc\u00dfe, pa\u00dft, and da\u00df. Currently only the first rule is in effect, thus F\u00fc\u00dfe, passt, and dass. The word Fu\u00df 'foot' has the letter \u00df because it contains a long vowel, even though that letter occurs at the end of a syllable. The logic of this change is that an '\u00df' is a single letter whereas 'ss' obviously are two letters, so the same distinction applies as for instance between the words den and denn.The most noticeable change was probably in the use of the letter \u00df, called scharfes s (Sharp S) or ess-zett (pronounced ess-tsett). Traditionally, this letter was used in three situations:The orthography reform of 1996 led to public controversy and considerable dispute. The states (Bundesl\u00e4nder) of North Rhine-Westphalia and Bavaria would not accept it. The dispute landed at one point in the highest court, which made a short issue of it, claiming that the states had to decide for themselves and that only in schools could the reform be made the official rule\u00a0\u2013 everybody else could continue writing as they had learned it. After 10 years, without any intervention by the federal parliament, a major revision was installed in 2006, just in time for the coming school year. In 2007, some traditional spellings were finally invalidated, whereas in 2008, on the other hand, many of the old comma rules were again put in force.A proper use of the long s, (langes s), \u017f, is essential for writing German text in Fraktur typefaces. Many Antiqua typefaces include the long s also. A specific set of rules applies for the use of long s in German text, but nowadays it is rarely used in Antiqua typesetting. Any lower case \"s\" at the beginning of a syllable would be a long s, as opposed to a terminal s or short s (the more common variation of the letter s), which marks the end of a syllable; for example, in differentiating between the words Wach\u017ftube (guard-house) and Wachstube (tube of polish/wax). One can easily decide which \"s\" to use by appropriate hyphenation, (Wach-\u017ftube vs. Wachs-tube). The long s only appears in lower case.The Fraktur script however remains present in everyday life in pub signs, beer brands and other forms of advertisement, where it is used to convey a certain rusticality and antiquity.Until the early 20th century, German was mostly printed in blackletter typefaces (mostly in Fraktur, but also in Schwabacher) and written in corresponding handwriting (for example Kurrent and S\u00fctterlin). These variants of the Latin alphabet are very different from the serif or sans-serif Antiqua typefaces used today, and the handwritten forms in particular are difficult for the untrained to read. The printed forms, however, were claimed by some to be more readable when used for Germanic languages.[87] (Often, foreign names in a text were printed in an Antiqua typeface even though the rest of the text was in Fraktur.) The Nazis initially promoted Fraktur and Schwabacher because they were considered Aryan, but they abolished them in 1941, claiming that these letters were Jewish.[88] It is also believed that the Nazi r\u00e9gime had banned this script as they realized that Fraktur would inhibit communication in the territories occupied during World War II.[89]Written German also typically uses an alternative opening inverted comma (quotation mark) as in \u201eGuten Morgen!\u201c.There is no general agreement on where letters with umlauts occur in the sorting sequence. Telephone directories treat them by replacing them with the base vowel followed by an e. Some dictionaries sort each umlauted vowel as a separate letter after the base vowel, but more commonly words with umlauts are ordered immediately after the same word without umlauts. As an example in a telephone book \u00c4rzte occurs after Adressenverlage but before Anlagenbauer (because \u00c4 is replaced by Ae). In a dictionary \u00c4rzte comes after Arzt, but in some dictionaries \u00c4rzte and all other words starting with \u00c4 may occur after all words starting with A. In some older dictionaries or indexes, initial Sch and St are treated as separate letters and are listed as separate entries after S, but they are usually treated as S+C+H and S+T.Umlaut vowels (\u00e4, \u00f6, \u00fc) are commonly transcribed with ae, oe, and ue if the umlauts are not available on the keyboard or other medium used. In the same manner \u00df can be transcribed as ss. Some operating systems use key sequences to extend the set of possible characters to include, amongst other things, umlauts; in Microsoft Windows this is done using Alt codes. German readers understand these transcriptions (although they appear unusual), but they are avoided if the regular umlauts are available because they are a makeshift, not proper spelling. (In Westphalia and Schleswig-Holstein, city and family names exist where the extra e has a vowel lengthening effect, e.g. Raesfeld [\u02c8ra\u02d0sf\u025blt], Coesfeld [\u02c8ko\u02d0sf\u025blt] and Itzehoe [\u026ats\u0259\u02c8ho\u02d0], but this use of the letter e after a/o/u does not occur in the present-day spelling of words other than proper nouns.)Since there is no traditional capital form of \u00df, it was replaced by SS when capitalization was required. For example, Ma\u00dfband (tape measure) became MASSBAND in capitals. An exception was the use of \u00df in legal documents and forms when capitalizing names. To avoid confusion with similar names, lower case \u00df was maintained (so, \"KRE\u00dfLEIN\" instead of \"KRESSLEIN\"). Capital \u00df (\u1e9e) was ultimately adopted into German orthography in 2017, ending a long orthographic debate.[86]Before the German orthography reform of 1996, \u00df replaced ss after long vowels and diphthongs and before consonants, word-, or partial-word-endings. In reformed spelling, \u00df replaces ss only after long vowels and diphthongs.Written texts in German are easily recognisable as such by distinguishing features such as umlauts and certain orthographical features\u00a0\u2013 German is the only major language that capitalizes all nouns, a relic of a widespread practice in Northern Europe in the early modern era (including English for a while, in the 1700s)\u00a0\u2013 and the frequent occurrence of long compounds. The longest German word that has been published is Donaudampfschiffahrtselektrizit\u00e4tenhauptbetriebswerkbauunterbeamtengesellschaft made of 79 characters. Because legibility and convenience set certain boundaries, compounds consisting of more than three or four nouns are almost exclusively found in humorous contexts. (In contrast, although English can also string nouns together, it usually separates the nouns with spaces. For example, \"toilet bowl cleaner\".)German is written in the Latin alphabet. In addition to the 26 standard letters, German has three vowels with Umlaut, namely \u00e4, \u00f6 and \u00fc, as well as the eszett or scharfes s (sharp s): \u00df. In Switzerland and Liechtenstein, ss is used instead of \u00df. Since \u00df can never occur at the beginning of a word, it has no traditional uppercase form.This is a selection of cognates in both English and German. Instead of the usual infinitive ending -en German verbs are indicated by a hyphen \"-\" after their stems. Words that are written with capital letters in German are nouns.The \u00d6sterreichisches W\u00f6rterbuch (\"Austrian Dictionary\"), abbreviated \u00d6WB, is the official dictionary of the German language in the Republic of Austria. It is edited by a group of linguists under the authority of the Austrian Federal Ministry of Education, Arts and Culture (German: Bundesministerium f\u00fcr Unterricht, Kunst und Kultur). It is the Austrian counterpart to the German Duden and contains a number of terms unique to Austrian German or more frequently used or differently pronounced there.[85] A considerable amount of this \"Austrian\" vocabulary is also common in Southern Germany, especially Bavaria, and some of it is used in Switzerland as well. The most recent edition is the 42nd from 2012. Since the 39th edition from 2001 the orthography of the \u00d6WB was adjusted to the German spelling reform of 1996. The dictionary is also officially used in the Italian province of South Tyrol.The Duden is the de facto official dictionary of the German language, first published by Konrad Duden in 1880. The Duden is updated regularly, with new editions appearing every four or five years. As of August 2013[update] it is in its 26th edition and in 12 volumes, each covering different aspects such as loanwords, etymology, pronunciation, synonyms, and so forth.\nThe first of these volumes, Die deutsche Rechtschreibung (German Orthography), has long been the prescriptive source for the spelling of German. The Duden has become the bible of the German language, being the definitive set of rules regarding grammar, spelling and usage of German.[84]The size of the vocabulary of German is difficult to estimate. The Deutsches W\u00f6rterbuch (The German Dictionary) initiated by Jacob and Wilhelm Grimm already contained over 330,000 headwords in its first edition. The modern German scientific vocabulary is estimated at nine million words and word groups (based on the analysis of 35 million sentences of a corpus in Leipzig, which as of July 2003 included 500 million words in total).[83]As in English, there are many pairs of synonyms due to the enrichment of the Germanic vocabulary with loanwords from Latin and Latinized Greek. These words often have different connotations from their Germanic counterparts and are usually perceived as more scholarly.At the same time, the effectiveness of the German language in forming equivalents for foreign words from its inherited Germanic stem repertory is great.[citation needed] Thus, Notker Labeo was able to translate Aristotelian treatises in pure (Old High) German in the decades after the year 1000. The tradition of loan translation was revitalized in the 18th century, with linguists like Joachim Heinrich Campe, who introduced close to 300 words that are still used in modern German. Even today, there are movements that try to promote the Ersatz (substitution) of foreign words deemed unnecessary with German alternatives.[81] It is claimed that this would also help in spreading modern or scientific notions among the less educated and as well democratise public life.Latin words were already imported into the predecessor of the German language during the Roman Empire and underwent all the characteristic phonetic changes in German. Their origin is thus no longer recognizable for most speakers (e.g. Pforte, Tafel, Mauer, K\u00e4se, K\u00f6ln from Latin porta, tabula, murus, caseus, Colonia). Borrowing from Latin continued after the fall of the Roman Empire during Christianization, mediated by the church and monasteries. Another important influx of Latin words can be observed during Renaissance humanism. In a scholarly context, the borrowings from Latin have continued until today, in the last few decades often indirectly through borrowings from English. During the 15th to 17th centuries, the influence of Italian was great, leading to many Italian loanwords in the fields of architecture, finance, and music. The influence of the French language in the 17th to 19th centuries resulted in an even greater import of French words. The English influence was already present in the 19th century, but it did not become dominant until the second half of the 20th century.Most German vocabulary is derived from the Germanic branch of the European language family.[citation needed] However, there is a significant amount of loanwords from other languages, in particular from Latin, Greek, Italian, French[78] and most recently English.[79] In the early 19th century, Joachim Heinrich Campe estimated that one fifth of the total German vocabulary was of French or Latin origin.[80]The order at the end of such strings is subject to variation, but the latter version is unusual.German subordinate clauses have all verbs clustered at the end. Given that auxiliaries encode future, passive, modality, and the perfect, very long chains of verbs at the end of the sentence can occur. In these constructions, the past participle in ge- is often replaced by the infinitive.Sentences using modal verbs place the infinitive at the end. For example, the English sentence \"Should he go home?\" would be rearranged in German to say \"Should he (to) home go?\" (Soll er nach Hause gehen?). Thus, in sentences with several subordinate or relative clauses, the infinitives are clustered at the end. Compare the similar clustering of prepositions in the following (highly contrived) English sentence: \"What did you bring that book that I do not like to be read to out of up for?\"The main verb may appear in first position to put stress on the action itself. The auxiliary verb is still in second position.When an auxiliary verb is present, it appears in second position, and the main verb appears at the end. This occurs notably in the creation of the perfect tense. Many word orders are still possible:The flexible word order also allows one to use language \"tools\" (such as poetic meter and figures of speech) more freely.Swapped object:Swapped adverbs:Another possibility:Both time expressions in front:Adverb of time in front:Object in front:Normal word order:However, German's flexibile word order allows one to emphasise specific words:The position of a noun in a German sentence has no bearing on its being a subject, an object or another argument. In a declarative sentence in English, if the subject does not occur before the predicate, the sentence could well be misunderstood.German requires for a verbal element (main verb or auxiliary verb) to appear second in the sentence. The verb is preceded by the topic of the sentence. The element in focus appears at the end of the sentence. For a sentence without an auxiliary, these are some possibilities:German word order is generally with the V2 word order restriction and also with the SOV word order restriction for main clauses. For polar questions, exclamations and wishes, the finite verb always has the first position. In subordinate clauses, the verb occurs at the very end.A selectively literal translation of this example to illustrate the point might look like this:Indeed, several parenthetical clauses may occur between the prefix of a finite verb and its complement (ankommen = to arrive, er kam an = he arrived, er ist angekommen = he has arrived):Many German verbs have a separable prefix, often with an adverbial function. In finite verb forms, it is split off and moved to the end of the clause and is hence considered by some to be a \"resultative particle\". For example, mitgehen, meaning \"to go along\", would be split, giving Gehen Sie mit? (Literal: \"Go you with?\"; Idiomatic: \"Are you going along?\").Other examples include the following: haften (to stick), verhaften (to detain); kaufen (to buy), verkaufen (to sell); h\u00f6ren (to hear), aufh\u00f6ren (to cease); fahren (to drive), erfahren (to experience).The meaning of basic verbs can be expanded and sometimes radically changed through the use of a number of prefixes. Some prefixes have a specific meaning; the prefix zer- refers to destruction, as in zerrei\u00dfen (to tear apart), zerbrechen (to break apart), zerschneiden (to cut apart). Other prefixes have only the vaguest meaning in themselves; ver- is found in a number of verbs with a large variety of meanings, as in versuchen (to try) from suchen (to seek), vernehmen (to interrogate) from nehmen (to take), verteilen (to distribute) from teilen (to share), verstehen (to understand) from stehen (to stand).The inflection of standard German verbs includes:Like the other Germanic languages, German forms noun compounds in which the first noun modifies the category given by the second,: Hundeh\u00fctte (\"dog hut\"; specifically: \"dog kennel\"). Unlike English, whose newer compounds or combinations of longer nouns are often written in \"open\" with separating spaces, German (like some other Germanic languages) nearly always uses the \"closed\" form without spaces, for example: Baumhaus (\"tree house\"). Like English, German allows arbitrarily long compounds in theory (see also English compounds). The longest German word verified to be actually in (albeit very limited) use is Rindfleischetikettierungs\u00fcberwachungsaufgaben\u00fcbertragungsgesetz, which, literally translated, is \"beef labelling supervision duty assignment law\" [from Rind (cattle), Fleisch (meat), Etikettierung(s) (labelling), \u00dcberwachung(s) (supervision), Aufgaben (duties), \u00dcbertragung(s) (assignment), Gesetz (law)]. However, examples like this are perceived by native speakers as excessively bureaucratic, stylistically awkward or even satirical.In German orthography, nouns and most words with the syntactical function of nouns are capitalised to make it easier for readers to determine the function of a word within a sentence (Am Freitag ging ich einkaufen.\u00a0\u2013 \"On Friday I went shopping.\"; Eines Tages kreuzte er endlich auf.\u00a0\u2013 \"One day he finally showed up.\") This convention is almost unique to German today (shared perhaps only by the closely related Luxembourgish language and several insular dialects of the North Frisian language), but it was historically common in other languages such as Danish (which abolished the capitalization of nouns in 1948) and English.This degree of inflection is considerably less than in Old High German and other old Indo-European languages such as Latin, Ancient Greek, and Sanskrit, and it is also somewhat less than, for instance, Old English, modern Icelandic, or Russian. The three genders have collapsed in the plural. With four cases and three genders plus plural, there are 16 permutations of case and gender/number, but there are only six forms of the definite article, which together cover all 16 permutations. In nouns, inflection for case is required in the singular for strong masculine and neuter nouns, in the genitive and sometimes in the dative. Both of these cases are losing ground to substitutes in informal speech.[77] The dative noun ending is considered somewhat old-fashioned in many contexts and is often dropped, but it is still used in proverbs and the like, in formal speech and in written language. Weak masculine nouns share a common case ending for genitive, dative and accusative in the singular. Feminine nouns are not declined in the singular. The plural has an inflection for the dative. In total, seven inflectional endings (not counting plural markers) exist in German: -s, -es, -n, -ns, -en, -ens, -e.German nouns inflect by case, gender and number:German is a fusional language with a moderate degree of inflection, with three grammatical genders; as such, there can be a large number of words derived from the same root.Bavarian dialects are spoken in Austria (Vienna, Lower- and Upper Austria, Styria, Carinthia, Salzburg, Burgenland, and in most parts of Tyrol), Bavaria (Upper- and Lower Bavaria as well as Upper Palatinate), South Tyrol, southwesternmost Saxony (Southern Vogtlandian), and in the Swiss village of Samnaun. The largest cities in the Bavarian area are Vienna and Munich.Alemannic dialects are spoken in Switzerland (High Alemannic in the densely populated Swiss Plateau, in the south also Highest Alemannic, and Low Alemannic in Basel), Baden-W\u00fcrttemberg (Swabian and Low Alemannic, in the southwest also High Alemannic), Bavarian Swabia (Swabian, in the southwesternmost part also Low Alemannic), Vorarlberg (Low-, High-, and Highest Alemannic), Alsace (Low Alemannic, in the southernmost part also High Alemannic), Liechtenstein (High- and Highest Alemannic), and in the Tyrolean district of Reutte (Swabian). The Alemannic dialects are considered as Alsatian in Alsace. The largest cities in the Alemannic area are Stuttgart and Z\u00fcrich.The Upper German dialects are the Alemannic dialects in the west, and the Bavarian dialects in the east.South Franconian is mainly spoken in northern Baden-W\u00fcrttemberg in Germany, but also in the northeasternmost part of the region of Alsace in France. While these dialects are considered as dialects of German in Baden-W\u00fcrttemberg, they are considered as dialects of Alsatian in Alsace (most Alsatian dialects are however Low Alemannic). The largest cities in the South Franconian area are Karlsruhe and Heilbronn.The East Franconian dialect branch is one of the most spoken dialect branches in Germany. These dialects are spoken in the region of Franconia and in the central parts of Saxon Vogtland. Franconia consists of the Bavarian districts of Upper-, Middle-, and Lower Franconia, the region of South Thuringia (Thuringia), and the eastern parts of the region of Heilbronn-Franken (Tauber Franconia and Hohenlohe) in Baden-W\u00fcrttemberg. The largest cities in the East Franconian area are Nuremberg and W\u00fcrzburg.The High Franconian dialects are transitional dialects between Central- and Upper German. They consist of the East- and South Franconian dialects.Further east, the non-Franconian, East Central German dialects are spoken (Thuringian, Upper Saxon, Ore Mountainian, and Lusatian-New Markish, and earlier, in the then German-speaking parts of Silesia also Silesian, and in then German southern East Prussia also High Prussian). The largest cities in the East Central German area are Berlin and Leipzig.Luxembourgish as well as the Transylvanian Saxon dialect spoken in Transylvania are based on Moselle Franconian dialects. The largest cities in the Franconian Central German area are Cologne and Frankfurt.The Franconian, West Central German dialects are the Central Franconian dialects (Ripuarian and Moselle Franconian), and the Rhine Franconian dialects (Hessian and Palatine). These dialects are considered asThe Central German dialects are spoken in Central Germany, from Aachen in the west to G\u00f6rlitz in the east. They consist of Franconian dialects in the west (West Central German), and non-Franconian dialects in the east (East Central German). Modern Standard German is mostly based on Central German dialects.The High German dialects consist of the Central German, High Franconian, and Upper German dialects. The High Franconian dialects are transitional dialects between Central- and Upper German. The High German varieties spoken by the Ashkenazi Jews have several unique features, and are considered as a separate language, Yiddish, written with the Hebrew alphabet.The Low Franconian dialects are the dialects that are more closely related to Dutch than to Low German. Most of the Low Franconian dialects are spoken in the Netherlands and in Belgium, where they are considered as dialects of Dutch, which is itself a Low Franconian language. In Germany, Low Franconian dialects are spoken in the northwest of North Rhine-Westphalia, along the Lower Rhine. The Low Franconian dialects spoken in Germany are referred to as Meuse-Rhenish or Low Rhenish. In the north of the German Low Franconian language area, North Low Franconian dialects (also referred to as Cleverlands or as dialects of South Guelderish) are spoken. These dialects are more closely related to Dutch (also North Low Franconian) than the South Low Franconian dialects (also referred to as East Limburgish and, east of the Rhine, Low Bergish), which are spoken in the south of the German Low Franconian language area. The South Low Franconian dialects are more closely related to Limburgish than to Dutch, and are transitional dialects between Low Franconian and Ripuarian (Central Franconian). The East Bergish dialects are the easternmost Low Franconian dialects, and are transitional dialects between North- and South Low Franconian, and Westphalian (Low German), with most of its features however being North Low Franconian. The largest cities in the German Low Franconian area are D\u00fcsseldorf and Duisburg.The 18th and 19th centuries were marked by mass education in Standard German in schools. Gradually, Low German came to be politically viewed as a mere dialect spoken by the uneducated. Today, Low Saxon can be divided in two groups: Low Saxon varieties with a reasonable Standard German influx[clarification needed] and varieties of Standard German with a Low Saxon influence known as Missingsch. Sometimes, Low Saxon and Low Franconian varieties are grouped together because both are unaffected by the High German consonant shift. However, the proportion of the population who can understand and speak it has decreased continuously since World War II. The largest cities in the Low German area are Hamburg and Dortmund.Middle Low German was the lingua franca of the Hanseatic League. It was the predominant language in Northern Germany until the 16th century. In 1534, the Luther Bible was published. The translation is considered to be an important step towards the evolution of the Early New High German. It aimed to be understandable to a broad audience and was based mainly on Central and Upper German varieties. The Early New High German language gained more prestige than Low German and became the language of science and literature. Around the same time, the Hanseatic League, based around northern ports, lost its importance as new trade routes to Asia and the Americas were established, and the most powerful German states of that period were located in Middle and Southern Germany.The variation among the German dialects is considerable, with often only neighbouring dialects being mutually intelligible. Some dialects are not intelligible to people who know only Standard German. However, all German dialects belong to the dialect continuum of High German and Low Saxon.The German dialect continuum is traditionally divided most broadly into High German and Low German, also called Low Saxon. However, historically, High German dialects and Low Saxon/Low German dialects do not belong to the same language. Nevertheless, in today's Germany, Low Saxon/Low German is often perceived as a dialectal variation of Standard German on a functional level even by many native speakers. The same phenomenon is found in the eastern Netherlands, as the traditional dialects are not always identified with their Low Saxon/Low German origins, but with Dutch.[76]German is a member of the West Germanic language of the Germanic family of languages, which in turn is part of the Indo-European language family. The German dialects are the traditional local varieties; many of them are hardly understandable to someone who knows only standard German, and they have great differences in lexicon, phonology and syntax. If a narrow definition of language based on mutual intelligibility is used, many German dialects are considered to be separate languages (for instance in the Ethnologue). However, such a point of view is unusual in German linguistics.[2]A mixture of dialect and standard does not normally occur in Northern Germany either. The traditional varieties there are Low German, whereas Standard German is a High German \"variety\". Because their linguistic distance to it is greater, they do not mesh with Standard German the way that High German dialects (such as Bavarian, Swabian, Hessian) can.In the German-speaking parts of Switzerland, mixtures of dialect and standard are very seldom used, and the use of Standard German is largely restricted to the written language, though about 10% of the Swiss residents speak High German (aka Standard German) at home, but mainly due to German immigrants.[75] This situation has been called a medial diglossia. Swiss Standard German is used in the Swiss education system, whereas Austrian Standard German is officially used in the Austrian education system.In German linguistics, German dialects are distinguished from varieties of standard German. The varieties of standard German refer to the different local varieties of the pluricentric standard German. They differ only slightly in lexicon and phonology. In certain regions, they have replaced the traditional German dialects, especially in Northern Germany.In most regions, the speakers use a continuum from more dialectal varieties to more standard varieties according to circumstances.Standard German differs regionally between German-speaking countries in vocabulary and some instances of pronunciation and even grammar and orthography. This variation must not be confused with the variation of local dialects. Even though the regional varieties of standard German are only somewhat influenced by the local dialects, they are very distinct. German is thus considered a pluricentric language.Standard German originated not as a traditional dialect of a specific region but as a written language. However, there are places where the traditional regional dialects have been replaced by new vernaculars based on standard German; that is the case in large stretches of Northern Germany but also in major cities in other parts of the country. It is important to note, however, that the colloquial standard German differs greatly from the formal written language, especially in grammar and syntax, in which it has been influenced by dialectal speech.Like French and Spanish, German has become a classic second foreign language in the western world, as English (Spanish in the US) is well established as the first foreign language.[3][70] German ranks second (after English) among the best known foreign languages in the EU (on a par with French)[3] as well as in Russia.[71] In terms of student numbers across all levels of education, German ranks third in the EU (after English and French)[38] as well as in the United States (after Spanish and French).[11][72] In 2015, approximately 15.4 million people were in the process of learning German across all levels of education worldwide.[70] As this number remained relatively stable since 2005 (\u00b1 1 million), roughly 75\u2013100 million people able to communicate in German as foreign language can be inferred assuming an average course duration of three years and other estimated parameters.[2] According to a 2012 survey, 47 million people within the EU (i.e., up to two thirds of the 75\u2013100 million worldwide) claimed to have sufficient German skills to have a conversation. Within the EU, not counting countries where it is an official language, German as a foreign language is most popular in Eastern and Northern Europe, namely the Czech Republic, Croatia, Denmark, the Netherlands, Slovakia, Hungary, Slovenia, Sweden and Poland.[3][38] German was once and, to some extent, is still, a lingua franca in those parts of Europe.[73]There is also an important German creole being studied and recovered, named Unserdeutsch, spoken in the former German colony of German New Guinea, across Micronesia and in northern Australia (i.e. coastal parts of Queensland and Western Australia), by a few elderly people. The risk of its extinction is serious and efforts to revive interest in the language are being implemented by scholars.[69]German migration to New Zealand in the 19th century was less pronounced than migration from Britain, Ireland, and perhaps even Scandinavia. Despite this there were significant pockets of German-speaking communities which lasted until the first decades of the 20th century. German-speakers settled principally in Puhoi, Nelson, and Gore. At the last census (2006), 37,500 people in New Zealand spoke German, making it the third most spoken European language after English and French and overall the ninth most spoken language.[citation needed]In Australia, the state of South Australia experienced a pronounced wave of immigration in the 1840s from Prussia (particularly the Silesia region). With the prolonged isolation from other German speakers and contact with Australian English, a unique dialect known as Barossa German has developed and is spoken predominantly in the Barossa Valley near Adelaide. Usage of German sharply declined with the advent of World War I, due to the prevailing anti-German sentiment in the population and related government action. It continued to be used as a first language into the twentieth century but now its use is limited to a few older speakers.[citation needed]In Brazil, the largest concentrations of German speakers are in the states of Rio Grande do Sul (where Riograndenser Hunsr\u00fcckisch developed), Santa Catarina, Paran\u00e1, S\u00e3o Paulo and Esp\u00edrito Santo.[65] There are also important concentrations of German-speaking descendants in Argentina, Chile, Paraguay, Venezuela, Peru and Bolivia.[52]In Mexico there are also large populations of German ancestry, mainly in the cities of: Mexico City,[62] Puebla, Mazatl\u00e1n, Tapachula, Ecatepec de Morelos, and larger populations scattered in the states of Chihuahua, Durango, and Zacatecas.[63][64]In Canada, there are 622,650 speakers of German according to the most recent census in 2006,[61] with people of German ancestry (German Canadians) found throughout the country. German-speaking communities are particularly found in British Columbia (118,035) and Ontario (230,330).[61] There is a large and vibrant community in the city of Kitchener, Ontario, which was at one point named Berlin. German immigrants were instrumental in the country's three largest urban areas: Montreal, Toronto, and Vancouver; post-Second World War immigrants managed to preserve a fluency in the German language in their respective neighborhoods and sections. In the first half of the 20th century, over a million German-Canadians made the language Canada's third most spoken after French and English.Hutterite German is an Upper German dialect of the Austro-Bavarian variety of the German language, which is spoken by Hutterite communities in Canada and the United States. Hutterite is spoken in the U.S. states of Washington, Montana, North Dakota, South Dakota, and Minnesota; and in the Canadian provinces of Alberta, Saskatchewan and Manitoba. Its speakers belong to some Schmiedleit, Lehrerleit, and Dariusleit Hutterite groups, but there are also speakers among the older generations of Prairieleit (the descendants of those Hutterites who chose not to settle in colonies). Hutterite children who grow up in the colonies learn to speak Hutterite German before learning English, the standard language of the surrounding areas, in school. Many of these children, though, continue with German Grammar School, in addition to public school, throughout a student's elementary education.[citation needed]The dialects of German which are or were primarily spoken in colonies or communities founded by German-speaking people resemble the dialects of the regions the founders came from. For example, Hutterite German resembles dialects of Carinthia. Texas German is a dialect spoken in the areas of Texas settled by the Adelsverein, such as New Braunfels and Fredericksburg. In the Amana Colonies in the state of Iowa, Amana German is spoken. Plautdietsch is a large minority language spoken in Northern Mexico by the Mennonite communities, and is spoken by more than 200,000 people in Mexico. Pennsylvania German is a West Central German dialect spoken by most of the Amish population of Pennsylvania, Ohio, and Indiana and resembles Palatinate German dialects.[citation needed]Between 1843 and 1910, more than 5 million Germans emigrated overseas,[57] mostly to the United States.[58] German remained an important language in churches, schools, newspapers, and even the administration of the United States Brewers' Association[59] through the early 20th century, but was severely repressed during World War I. Over the course of the 20th century, many of the descendants of 18th century and 19th century immigrants ceased speaking German at home, but small populations of speakers are still found in Pennsylvania (Amish, Hutterites, Dunkards and some Mennonites historically spoke Hutterite German and a West Central German variety of German known as Pennsylvania German or Pennsylvania Dutch), Kansas (Mennonites and Volga Germans), North Dakota (Hutterite Germans, Mennonites, Russian Germans, Volga Germans, and Baltic Germans), South Dakota, Montana, Texas (Texas German), Wisconsin, Indiana, Oregon, Oklahoma, and Ohio (72,570).[60][citation needed] A significant group of German Pietists in Iowa formed the Amana Colonies and continue to practice speaking their heritage language. Early twentieth century immigration was often to St. Louis, Chicago, New York, Milwaukee, Pittsburgh and Cincinnati.In the United States, the states of North Dakota and South Dakota are the only states where German is the most common language spoken at home after English.[55] German geographical names can be found throughout the Midwest region of the country, such as New Ulm and many other towns in Minnesota; Bismarck (North Dakota's state capital), Munich, Karlsruhe, and Strasburg (named after a town near Odessa in Ukraine)[56] in North Dakota; New Braunfels, Fredericksburg, Weimar, and Muenster in Texas; Corn (formerly Korn), Kiefer and Berlin in Oklahoma; and Kiel, Berlin, and Germantown in Wisconsin.Mostly originating from different waves of immigration during the 19th and 20th centuries, an estimated 12,000 people speak German or a German variety as a first language in South Africa.[52] One of the largest communities consists of the speakers of \"Nataler Deutsch\",[53] a variety of Low German, concentrated in and around Wartburg. The small town of Kroondal in the North-West Province also has a mostly German-speaking population. The South African constitution identifies German as a \"commonly used\" language and the Pan South African Language Board is obligated to promote and ensure respect for it.[54] The community is strong enough that several German International schools are supported such as the Deutsche Schule Pretoria.German, along with English and Afrikaans was a co-official language of Namibia from 1984 until its independence from South Africa in 1990. At this point, the Namibian government perceived Afrikaans and German as symbols of apartheid and colonialism, and decided English would be the sole official language, claiming that it was a \"neutral\" language as there were virtually no English native speakers in Namibia at that time.[50] German, Afrikaans and several indigenous languages became \"national languages\" by law, identifying them as elements of the cultural heritage of the nation and ensuring that the state acknowledged and supported their presence in the country.[2] Today, German is used in a wide variety of spheres, especially business and tourism, as well as the churches (most notably the German-speaking Evangelical Lutheran Church in Namibia (GELK)), schools (e.g. the Deutsche H\u00f6here Privatschule Windhoek), literature (German-Namibian authors include Giselher W. Hoffmann), radio (the Namibian Broadcasting Corporation produces radio programs in German), and music (e.g. artist EES). The Allgemeine Zeitung is one of the three biggest newspapers in Namibia and the only German-language daily in Africa.[50]Namibia was a colony of the German Empire from 1884 to 1919. Mostly descending from German settlers who immigrated during this time, 25\u201330,000 people still speak German as a native tongue today.[50] The period of German colonialism in Namibia also led to the evolution of a Standard German-based pidgin language called \"Namibian Black German\", which became a second language for parts of the indigenous population. Although it is nearly extinct today, some older Namibians still have some knowledge of it.[51]In France, the High German varieties of Alsatian and Moselle Franconian are identified as \"regional languages\", but the European Charter for Regional and Minority Languages of 1998 has not yet been ratified by the government.[49] In the Netherlands, the Limburgish, Frisian, and Low German languages are protected regional languages according to the European Charter for Regional and Minority Languages;[43] however, they are widely considered separate languages and neither German nor Dutch dialects.Within Europe and Asia, German is a recognized minority language in the following countries:Although expulsions and (forced) assimilation after the two World Wars greatly diminished them, minority communities of mostly bilingual German native speakers exist in areas both adjacent to and detached from the Sprachraum.[2]It is a co-official language of theIn Europe, German is the second most widely spoken mother tongue (after Russian) and the second biggest language in terms of overall speakers (after English). The area in central Europe where the majority of the population speaks German as a first language and has German as a (co-)official language is called the \"German Sprachraum\". It comprises an estimated 88 million native speakers and 10 million who speak German as a second language (e.g. immigrants).[2][17] Excluding regional minority languages, German is the only official language ofDue to the German diaspora as well as German being the second most widely spoken language in Europe and the third most widely taught foreign language in the US[11] and the EU (in upper secondary education)[38] amongst others, the geographical distribution of German speakers (or \"Germanophones\") spans all inhabited continents. As for the number of speakers of any language worldwide, an assessment is always compromised by the lack of sufficient, reliable data. For an exact, global number of native German speakers, this is further complicated by the existence of several varieties whose status as separate \"languages\" or \"dialects\" is disputed for political and/or linguistic reasons, including quantitatively strong varieties like certain forms of Alemannic (e.g., Alsatian)[2] and Low German/Plautdietsch.[5] Mostly depending on the inclusion or exclusion of certain varieties, it is estimated that approximately 90\u201395 million people speak German as a first language,[2][17][39] 10-25 million as a second language,[2][17] and 75\u2013100 million as a foreign language.[2][3] This would imply approximately 175\u2013220 million German speakers worldwide.[40] It is estimated that also including all persons who are or were taking German classes, i.e., regardless of their actual proficiency, would amount to about 280 million people worldwide with at least some knowledge of German.[2]Official revisions of some of the rules from 1901 were not issued until the controversial German orthography reform of 1996 was made the official standard by governments of all German-speaking countries.[37] Media and written works are now almost all produced in Standard German (often called Hochdeutsch, \"High German\") which is understood in all areas where German is spoken.In 1901, the 2nd Orthographical Conference ended with a complete standardization of the German language in its written form and the Duden Handbook was declared its standard definition.[35] The Deutsche B\u00fchnensprache (literally, German stage language) had established conventions for German pronunciation in theatre (B\u00fchnendeutsch[36]) three years earlier; however, this was an artificial standard that did not correspond to any traditional spoken dialect. Rather, it was based on the pronunciation of Standard German in Northern Germany, although it was subsequently regarded often as a general prescriptive norm, despite differing pronunciation traditions especially in the Upper-German-speaking regions that still characterize the dialect of the area today\u00a0\u2013 especially the pronunciation of the ending -ig as [\u026ak] instead of [\u026a\u00e7]. In Northern Germany, Standard German was a foreign language to most inhabitants, whose native dialects were subsets of Low German. It was usually encountered only in writing or formal speech; in fact, most of Standard German was a written language, not identical to any spoken dialect, throughout the German-speaking area until well into the 19th century.The most comprehensive guide to the vocabulary of the German language is found within the Deutsches W\u00f6rterbuch. This dictionary was created by the Brothers Grimm and is composed of 16 parts which were issued between 1852 and 1860.[33] In 1872, grammatical and orthographic rules first appeared in the Duden Handbook.[34]In the eastern provinces of Banat and Transylvania (German: Siebenb\u00fcrgen), German was the predominant language not only in the larger towns\u00a0\u2013 such as Temeswar (Timi\u0219oara), Hermannstadt (Sibiu) and Kronstadt (Bra\u0219ov)\u00a0\u2013 but also in many smaller localities in the surrounding areas.[31][32]Some cities, such as Prague (German: Prag) and Budapest (Buda, German: Ofen), were gradually Germanized in the years after their incorporation into the Habsburg domain. Others, such as Pozsony (German: Pressburg, now Bratislava), were originally settled during the Habsburg period, and were primarily German at that time. Prague, Budapest and Bratislava as well as cities like Zagreb (German: Agram), and Ljubljana (German: Laibach), contained significant German minorities.German was the language of commerce and government in the Habsburg Empire, which encompassed a large area of Central and Eastern Europe. Until the mid-19th century, it was essentially the language of townspeople throughout most of the Empire. Its use indicated that the speaker was a merchant or someone from an urban area, regardless of nationality.With Luther's rendering of the Bible in the vernacular German asserted itself against the dominance of Latin as a legitimate language for courtly, literary, and now ecclesiastical subject-matter. Further, his Bible was ubiquitous in the German states with nearly every household possessing a copy.[29] Nevertheless, even with the influence of Luther's Bible as an unofficial written standard, it was not until the middle of the 18th century after the ENHG period that a widely accepted standard for written German appeared.[30]One of the central events in the development of ENHG was the publication of Luther's translation of the Bible into German (the New Testament in 1522 and the Old Testament, published in parts and completed in 1534). Luther based his translation primarily on the Mei\u00dfner Deutsch of Saxony,[27] spending much time among the population of Saxony researching the dialect so as to make the work as natural and accessible to German speakers as possible. Copies of Luther's Bible featured a long list of glosses for each region that translated words which were unknown in the region into the regional dialect. Concerning his translation method Luther says the following:The ENHG period saw the rise of several important cross-regional forms of chancery German, one being gemeine tiutsch, used in the court of the Holy Roman Emperor Maximilian I, and the other being Mei\u00dfner Deutsch, used in the Electorate of Saxony in the Duchy of Saxe-Wittenberg.[25] Alongside these courtly written standards, the invention of the printing press led to the development of a number of printers' languages (Druckersprachen) aimed at making printed material readable and understandable across as many diverse dialects of German as possible.[26] The greater ease of production and increased availability of written texts brought about increased standardization in the written form of the German language.Modern German begins with the Early New High German (ENHG) period, which the influential German philologist Wilhelm Scherer dates 1350\u20131650, terminating with the end of the Thirty Years' War.[21] This period saw the further displacement of Latin by German as the primary language of courtly proceedings and, increasingly, of literature in the German states. While these states were still under the control of the Holy Roman Empire and far from any form of unification, the desire for a cohesive written language that would be understandable across the many German-speaking principalities and kingdoms was stronger than ever. As a spoken language German remained highly fractured through this period with a vast number of often mutually-incomprehensible regional dialects being spoken throughout the German states; the invention of the printing press c.1440 and the publication of Luther's vernacular translation of the Bible in 1534, however, had an immense effect on standardizing German as a supra-dialectal written language.The Middle High German period is generally seen as ending with the decimation of the population of Europe in the Black Death of 1346\u20131353.[21]A great wealth of texts survives from the MHG period. Significantly, among this repertoire are a number of impressive secular works, such as the Nibelungenlied, an epic poem telling the story of the dragon-slayer Siegfried (c. 13th century), and the Iwein, an Arthurian verse poem by Hartmann von Aue (c. 1203), as well as several lyric poems and courtly romances such as Parzival and Tristan. (Also noteworthy is the Sachsenspiegel, the first book of laws written in Middle Low German (c. 1220)). The abundance and especially the secular character of the literature of the MHG period demonstrate the beginnings of a standardized written form of German, as well as the desire of poets and authors to be understood by individuals on supra-dialectal terms.While the major changes of the MHG period were socio-cultural, German was still undergoing significant linguistic changes in syntax, phonetics, and morphology as well (e.g. diphthongization of certain vowel sounds: hus (OHG \"house\")\u2192haus (MHG), and weakening of unstressed short vowels to schwa [\u0259]: taga (OHG \"days\")\u2192tage (MHG)).[24]While there is no complete agreement over the dates of the Middle High German (MHG) period, it is generally seen as lasting from 1050 to 1350.[21][22] This was a period of significant expansion of the geographical territory occupied by Germanic tribes, and consequently of the number of German speakers. Whereas during the Old High German period the Germanic tribes extended only as far east as the Elbe and Saale rivers, the MHG period saw a number of these tribes expanding beyond this eastern boundary into Slavic territory (this is known as the Ostsiedlung). Along with the increasing wealth and geographic extent of the Germanic groups came greater use of German in the courts of nobles as the standard language of official proceedings and literature.[22][23] A clear example of this is the mittelhochdeutsche Dichtersprache employed in the Hohenstaufen court in Swabia as a standardized supra-dialectal written language. While these efforts were still regionally bound, German began to be used in place of Latin for certain official purposes, leading to a greater need for regularity in written conventions.The German language through the OHG period was still predominantly a spoken language, with a wide range of dialects and a much more extensive oral tradition than a written one. Having just emerged from the High German consonant shift, OHG was also a relatively new and volatile language still undergoing a number of phonetic, phonological, morphological, and syntactic changes. The scarcity of written work, instability of the language, and widespread illiteracy of the time thus account for the lack of standardization up to the end of the OHG period in 1050.In general, the surviving texts of OHG show a wide range of dialectal diversity with very little written uniformity. The early written tradition of OHG survived mostly through monasteries and scriptoria as local translations of Latin originals; as a result, the surviving texts are written in highly disparate regional dialects and exhibit significant Latin influence, particularly in vocabulary.[19] At this point monasteries, where most written works were produced, were dominated by Latin, and German saw only occasional use in official and ecclesiastical writing.While there is written evidence of the Old High German language in several Elder Futhark inscriptions from as early as the 6th century AD (such as the Pforzen buckle), the Old High German period is generally seen as beginning with the Abrogans (written c.765\u2013775), a Latin-German glossary supplying over 3,000 OHG words with their Latin equivalents. Following the Abrogans the first coherent works written in OHG appear in the 9th century, chief among them being the Muspilli, the Merseberg Incantations, and the Hildebrandslied, as well as a number of other religious texts (the Georgslied, the Ludwigslied, the Evangelienbuch, and translated hymns and prayers).[19][20] The Muspilli is a Christian poem written in a Bavarian dialect offering an account of the soul after the Last Judgment, and the Merseberg Incantations are transcriptions of spells and charms from the pagan Germanic tradition. Of particular interest to scholars, however, has been the Hildebrandslied, a secular epic poem telling the tale of an estranged father and son unknowingly meeting each other in battle. Linguistically this text is highly interesting due to the mixed use of Old Saxon and Old High German dialects in its composition. The written works of this period stem mainly from the Alamanni, Bavarian, and Thuringian groups, all belonging to the Elbe Germanic group (Irminones), which had settled in what is now southern-central Germany and Austria between the 2nd and 6th centuries during the great migration.[19]The history of the German language begins with the High German consonant shift during the migration period, which separated Old High German (OHG) dialects from Old Saxon. This sound shift involved a drastic change in the pronunciation of both voiced and voiceless stop consonants (b, d, g, and p, t, k, respectively). The primary effects of the shift were the following:After these High German dialects, standard German is (somewhat less closely) related to languages based on Low Franconian dialects (e.g. Dutch and Afrikaans) or Low German/Low Saxon dialects (spoken in northern Germany and southern Denmark), neither of which underwent the High German consonant shift. As has been noted, the former of these dialect types is Istvaeonic and the latter Ingvaeonic, whereas the High German dialects are all Irminonic; the differences between these languages and standard German are therefore considerable. Also related to German are the Frisian languages\u2014North Frisian (spoken in Schleswig-Holstein no. 28), Saterland Frisian (spoken in Lower Saxony \u2013 no. 27), and West Frisian (spoken in the Netherlands \u2013 no. 26)\u2014as well as the Anglic languages of English and Scots. These Anglo-Frisian dialects are all members of the Ingvaeonic family of West Germanic languages which did not take part in the High German consonant shift.Standard German is based on Thuringian-Upper Saxon dialects (no. 30 on the map), which are Central German dialects (nos. 29\u201331), belonging to the Irminonic High German dialect group. German is therefore most closely related to the other languages based on High German dialects, such as Luxembourgish (based on Central Franconian dialects \u2013 no. 29), and Yiddish. Also closely related to Standard German are the Upper German dialects spoken in the southern German-speaking countries, such as Swiss German (Alemannic dialects \u2013 no. 34), and the various dialects spoken in the French region of Grand Est, such as Alsatian (mainly Alemannic, but also Central- and\u00a0Upper Franconian\u00a0(no. 32)\u00a0dialects) and Lorraine Franconian (Central Franconian \u2013 no. 29).Within the West Germanic language dialect continuum, the Benrath and Uerdingen lines (running through D\u00fcsseldorf-Benrath and Krefeld-Uerdingen, respectively) serve to distinguish the Germanic dialects that were affected by the High German consonant shift (south of Benrath) from those that were not (north of Uerdingen). The various regional dialects spoken south of these lines are grouped as High German dialects (nos. 29\u201334 on the map), while those spoken to the north comprise the Low German/Low Saxon (nos. 19\u201324) and Low Franconian (no. 25) dialects. As members of the West Germanic language family, High German, Low German, and Low Franconian can be further distinguished historically as Irminonic, Ingvaeonic, and Istvaeonic, respectively. This classification indicates their historical descent from dialects spoken by the Irminones (also known as the Elbe group), Ingvaeones (or North Sea Germanic group), and Istvaeones (or Weser-Rhine group).[18]Modern Standard German is a West Germanic language descended from the Germanic branch of the Indo-European languages. The Germanic languages are traditionally subdivided into three branches: North Germanic, East Germanic, and West Germanic. The first of these branches survives in modern Danish, Swedish, Norwegian, Faroese, and Icelandic, all of which are descended from Old Norse. The East Germanic languages are now extinct, and the only historical member of this branch from which written texts survive is Gothic. The West Germanic languages, however, have undergone extensive dialectal subdivision and are now represented in modern languages such as English, German, Dutch, Yiddish, Afrikaans, and others.[18]German is an inflected language with four cases for nouns, pronouns, and adjectives (nominative, accusative, genitive, dative), three genders (masculine, feminine, neuter), and strong and weak verbs. German derives the majority of its vocabulary from the ancient Germanic branch of the Indo-European language family. A portion of German words are derived from Latin and Greek, and fewer are borrowed from French and English. With slightly different standardized variants (German, Austrian, and Swiss Standard German), German is a pluricentric language. Like English, German is also notable for its broad spectrum of dialects, with many unique varieties existing in Europe and also other parts of the world.[2][16] Due to the limited intelligibility between certain varieties and Standard German, as well as the lack of an undisputed, scientific difference between a \"dialect\" and a \"language\",[2] some German varieties or dialect groups (e.g. Low German or Plautdietsch[5]) are alternatively referred to as \"languages\" and \"dialects\".[17]One of the major languages of the world, German is the first language of almost 100\u00a0million people worldwide and the most widely spoken native language in the European Union.[2][8] Together with French, German is the second most commonly spoken foreign language in the EU after English, making it the second biggest language in the EU in terms of overall speakers.[9] German is also the second most widely taught foreign language in the EU after English at primary school level (but third after English and French at lower secondary level),[10] the fourth most widely taught non-English language in the US[11] (after Spanish, French and American Sign Language), and the second most commonly used scientific language[12] as well as the third most widely used language on websites (after English and Russian).[13] The German-speaking countries are ranked fifth in terms of annual publication of new books, with one tenth of all books (including e-books) in the world being published in the German language.[14] In the United Kingdom, German and French are the most-sought after foreign languages for businesses (with 49% and 50% of businesses identifying these two languages as the most useful, respectively).[15]",
            "title": "German language",
            "url": "https://en.wikipedia.org/wiki/German_language"
        },
        {
            "desc_links": [
                "/wiki/Philosopher",
                "/wiki/Philosophy_of_science",
                "/wiki/Physical_law",
                "/wiki/Aesthetics",
                "/wiki/Physics",
                "/wiki/Energy",
                "/wiki/Electrodynamics",
                "/wiki/Chemical_thermodynamics",
                "/wiki/Mechanics",
                "/wiki/Thermodynamics",
                "/wiki/Physiology",
                "/wiki/Psychology",
                "/wiki/Human_eye",
                "/wiki/Theory_of_vision",
                "/wiki/Visual_perception",
                "/wiki/Color_vision",
                "/wiki/Empiricism",
                "/wiki/Physician",
                "/wiki/Physicist",
                "/wiki/Research_institution",
                "/wiki/Helmholtz_Association_of_German_Research_Centres"
            ],
            "links": [
                "/wiki/Max_Planck",
                "/wiki/Heinrich_Kayser",
                "/wiki/Eugen_Goldstein",
                "/wiki/Wilhelm_Wien",
                "/wiki/Arthur_K%C3%B6nig",
                "/wiki/Henry_Augustus_Rowland",
                "/wiki/Albert_A._Michelson",
                "/wiki/Wilhelm_Wundt",
                "/wiki/Fernando_Sanford",
                "/wiki/Michael_I._Pupin",
                "/wiki/Leo_Koenigsberger",
                "/wiki/Helmholtz_equation",
                "/wiki/Electromagnetism",
                "/wiki/Helmholtz_equation",
                "/wiki/Heinrich_Rudolf_Hertz",
                "/wiki/Electromagnetic_radiation",
                "/wiki/Oliver_Heaviside",
                "/wiki/Longitudinal_wave",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Leyden_jar",
                "/wiki/Alexander_J._Ellis",
                "/wiki/Vowel",
                "/wiki/Alexander_Graham_Bell",
                "/wiki/Harmonic_telegraph",
                "/wiki/Sensations_of_Tone",
                "/wiki/Helmholtz_resonance",
                "/wiki/Audio_frequency",
                "/wiki/Pitch_(music)",
                "/wiki/Sine_wave",
                "/wiki/Fourier_analysis#Applications_in_signal_processing",
                "/wiki/Galvanometer",
                "/wiki/Ewald_Hering",
                "/wiki/Ophthalmology",
                "/wiki/Ophthalmoscope",
                "/wiki/Human_eye",
                "/wiki/Depth_perception",
                "/wiki/Color_vision",
                "/wiki/Motion_perception",
                "/wiki/Unconscious_inference",
                "/wiki/Optical_Society_of_America",
                "/wiki/Accommodation_reflex",
                "/wiki/Wilhelm_Wundt",
                "/wiki/Psychology",
                "/wiki/Naturphilosophie",
                "/wiki/Materialism",
                "/wiki/Helmholtz%27s_theorems",
                "/wiki/William_Thomson,_1st_Baron_Kelvin",
                "/wiki/William_Rankine",
                "/wiki/Heat_death_of_the_universe",
                "/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot",
                "/wiki/Beno%C3%AEt_Paul_%C3%89mile_Clapeyron",
                "/wiki/James_Prescott_Joule",
                "/wiki/Mechanics",
                "/wiki/Heat",
                "/wiki/Light",
                "/wiki/Electricity",
                "/wiki/Magnetism",
                "/wiki/Energy",
                "/wiki/Conservation_of_energy",
                "/wiki/Muscle",
                "/wiki/Metabolism",
                "/wiki/Naturphilosophie",
                "/wiki/University_of_K%C3%B6nigsberg",
                "/wiki/University_of_Bonn",
                "/wiki/University_of_Heidelberg",
                "/wiki/Baden",
                "/wiki/Humboldt_University_of_Berlin",
                "/wiki/Age_of_the_Earth",
                "/wiki/Solar_System",
                "/wiki/Charit%C3%A9",
                "/wiki/Potsdam",
                "/wiki/Gymnasium_(school)",
                "/wiki/Classical_philology",
                "/wiki/Philosophy",
                "/wiki/Immanuel_Hermann_Fichte",
                "/wiki/Johann_Gottlieb_Fichte",
                "/wiki/Immanuel_Kant",
                "/wiki/Physiology",
                "/wiki/Philosopher",
                "/wiki/Philosophy_of_science",
                "/wiki/Physical_law",
                "/wiki/Aesthetics",
                "/wiki/Physics",
                "/wiki/Energy",
                "/wiki/Electrodynamics",
                "/wiki/Chemical_thermodynamics",
                "/wiki/Mechanics",
                "/wiki/Thermodynamics",
                "/wiki/Physiology",
                "/wiki/Psychology",
                "/wiki/Human_eye",
                "/wiki/Theory_of_vision",
                "/wiki/Visual_perception",
                "/wiki/Color_vision",
                "/wiki/Empiricism",
                "/wiki/Physician",
                "/wiki/Physicist",
                "/wiki/Research_institution",
                "/wiki/Helmholtz_Association_of_German_Research_Centres"
            ],
            "text": "Other students and research associates of Helmholtz at Berlin included Max Planck, Heinrich Kayser, Eugen Goldstein, Wilhelm Wien, Arthur K\u00f6nig, Henry Augustus Rowland, Albert A. Michelson, Wilhelm Wundt, Fernando Sanford and Michael I. Pupin. Leo Koenigsberger, who was his colleague 1869\u20131871 in Heidelberg, wrote the definitive biography of him in 1902.Whoever, in the pursuit of science, seeks after immediate practical utility may rest assured that he seeks in vain. \u2014 Academic Discourse (Heidelberg 1862)[24]There is even a topic by the name \"Helmholtz optics\", based on the Helmholtz equation.[21][22][23]In 1871, Helmholtz moved from Heidelberg to Berlin to become a professor in physics. He became interested in electromagnetism and the Helmholtz equation is named for him. Although he did not make major contributions to this field, his student Heinrich Rudolf Hertz became famous as the first to demonstrate electromagnetic radiation. Oliver Heaviside criticised Helmholtz's electromagnetic theory because it allowed the existence of longitudinal waves. Based on work on Maxwell's equations, Heaviside pronounced that longitudinal waves could not exist in a vacuum or a homogeneous medium. Heaviside did not note, however, that longitudinal electromagnetic waves can exist at a boundary or in an enclosed space.[20]Helmholtz studied the phenomena of electrical oscillations from 1869 to 1871, and in a lecture delivered to the Naturhistorisch-medizinischen Vereins zu Heidelberg (Natural History and Medical Association of Heidelberg) on April 30, 1869, titled On Electrical Oscillations he indicated that the perceptible damped electrical oscillations in a coil joined up with a Leyden jar were about 1/50th of a second in duration.[19]The translation by Alexander J. Ellis was first published in 1875 (the first English edition was from the 1870 third German edition; Ellis's second English edition from the 1877 fourth German edition was published in 1885; the 1895 and 1912 third and fourth English editions were reprints of the second).[18]Helmholtz showed that different combinations of resonator could mimic vowel sounds: Alexander Graham Bell in particular was interested in this but, not being able to read German, misconstrued Helmholtz' diagrams as meaning that Helmholtz had transmitted multiple frequencies by wire\u2014which would allow multiplexing of telegraph signals\u2014whereas, in reality, electrical power was used only to keep the resonators in motion. Bell failed to reproduce what he thought Helmholtz had done but later said that, had he been able to read German, he would not have gone on to invent the telephone on the harmonic telegraph principle.[14][15][16][17]In 1863, Helmholtz published Sensations of Tone, once again demonstrating his interest in the physics of perception. This book influenced musicologists into the twentieth century. Helmholtz invented the Helmholtz resonator to identify the various frequencies or pitches of the pure sine wave components of complex sounds containing multiple tones.[13]In 1849, while at K\u00f6nigsberg, Helmholtz measured the speed at which the signal is carried along a nerve fibre. At that time most people believed that nerve signals passed along nerves immeasurably fast.[10] He used a recently dissected sciatic nerve of a frog and the calf muscle to which it attached. He used a galvanometer as a sensitive timing device, attaching a mirror to the needle to reflect a light beam across the room to a scale which gave much greater sensitivity.[10] Helmholtz reported[11][12] transmission speeds in the range of 24.6 - 38.4 meters per second.[10]Helmholtz continued to work for several decades on several editions of the handbook, frequently updating his work because of his dispute with Ewald Hering who held opposite views on spatial and color vision. This dispute divided the discipline of physiology during the second half of the 1800s.In 1851, Helmholtz revolutionized the field of ophthalmology with the invention of the ophthalmoscope; an instrument used to examine the inside of the human eye. This made him world-famous overnight. Helmholtz's interests at that time were increasingly focused on the physiology of the senses. His main publication, titled Handbuch der Physiologischen Optik (Handbook of Physiological Optics or Treatise on Physiological Optics), provided empirical theories on depth perception, color vision, and motion perception, and became the fundamental reference work in his field during the second half of the nineteenth century. In the third and final volume, published in 1867, Helmholtz described the importance of unconscious inferences for perception. The Handbuch was first translated into English under the editorship of James P. C. Southall on behalf of the Optical Society of America in 1924-5. His theory of accommodation went unchallenged until the final decade of the 20th century.The sensory physiology of Helmholtz was the basis of the work of Wilhelm Wundt, a student of Helmholtz, who is considered one of the founders of experimental psychology. He, more explicitly than Helmholtz, described his research as a form of empirical philosophy and as a study of the mind as something separate. Helmholtz had, in his early repudiation of Naturphilosophie, stressed the importance of materialism, and was focusing more on the unity of \"mind\" and body.[9]Helmholtz was a pioneer in the scientific study of human vision and audition. He coined the term \"psychophysics,\" to capture the distinction between the measurement of physical stimuli and their effect on human perception. For example, the amplitude of a sound wave can be varied, causing the sound to appear louder or softer, but a linear step in sound pressure amplitude does not result in a linear step in perceived loudness. The physical sound needs to be increased exponentially in order for equal steps to seem linear, a fact that is used in current electronic devices to control volume. Helmholtz paved the way in experimental studies on the relationship between the physical energy (physics) and its appreciation (psychology), with the goal in mind to develop \"psychophysical laws.\"In fluid dynamics, Helmholtz made several contributions, including Helmholtz's theorems for vortex dynamics in inviscid fluids.In the 1850s and 60s, building on the publications of William Thomson, Helmholtz and William Rankine popularized the idea of the heat death of the universe.Drawing on the earlier work of Sadi Carnot, Beno\u00eet Paul \u00c9mile Clapeyron and James Prescott Joule, he postulated a relationship between mechanics, heat, light, electricity and magnetism by treating them all as manifestations of a single force (energy in modern terms[7]). He published his theories in his book \u00dcber die Erhaltung der Kraft (On the Conservation of Force, 1847).[8]His first important scientific achievement, an 1847 treatise on the conservation of energy, was written in the context of his medical studies and philosophical background. He discovered the principle of conservation of energy while studying muscle metabolism. He tried to demonstrate that no energy is lost in muscle movement, motivated by the implication that there were no vital forces necessary to move a muscle. This was a rejection of the speculative tradition of Naturphilosophie which was at that time a dominant philosophical paradigm in German physiology.Helmholtz's first academic position was as a teacher of Anatomy at the Academy of Arts in Berlin in 1848.[6] He then moved to take a post of associate professor of physiology at the Prussian University of K\u00f6nigsberg, where he was appointed in 1849. In 1855 he accepted a full professorship of anatomy and physiology at the University of Bonn. He was not particularly happy in Bonn, however, and three years later he transferred to the University of Heidelberg, in Baden, where he served as professor of physiology. In 1871 he accepted his final university position, as professor of physics at the University of Berlin.Trained primarily in physiology, Helmholtz wrote on many other topics, ranging from theoretical physics, to the age of the Earth, to the origin of the Solar System.As a young man, Helmholtz was interested in natural science, but his father wanted him to study medicine at the Charit\u00e9 because there was financial support for medical students.Helmholtz was born in Potsdam the son of the local Gymnasium headmaster, Ferdinand Helmholtz, who had studied classical philology and philosophy, and who was a close friend of the publisher and philosopher Immanuel Hermann Fichte. Helmholtz's work is influenced by the philosophy of Johann Gottlieb Fichte and Immanuel Kant. He tried to trace their theories in empirical matters like physiology.As a philosopher, he is known for his philosophy of science, ideas on the relation between the laws of perception and the laws of nature, the science of aesthetics, and ideas on the civilizing power of science.In physics, he is known for his theories on the conservation of energy, work in electrodynamics, chemical thermodynamics, and on a mechanical foundation of thermodynamics.In physiology and psychology, he is known for his mathematics of the eye, theories of vision, ideas on the visual perception of space, color vision research, and on the sensation of tone, perception of sound, and empiricism in the physiology of perception.Hermann Ludwig Ferdinand von Helmholtz (August 31, 1821\u00a0\u2013 September 8, 1894) was a German physician and physicist who made significant contributions in several scientific fields. The largest German association of research institutions, the Helmholtz Association, is named after him.[5]",
            "title": "Hermann von Helmholtz",
            "url": "https://en.wikipedia.org/wiki/Helmholtz"
        },
        {
            "desc_links": [
                "/wiki/Royal_Society_of_London",
                "/wiki/FRSE",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French"
            ],
            "links": [
                "/wiki/Liouville_(crater)",
                "/wiki/Moon",
                "/wiki/Liouville_function",
                "/wiki/Royal_Swedish_Academy_of_Sciences",
                "/wiki/Number_theory",
                "/wiki/Complex_analysis",
                "/wiki/Differential_geometry_and_topology",
                "/wiki/Mathematical_physics",
                "/wiki/Astronomy",
                "/wiki/Liouville%27s_theorem_(complex_analysis)",
                "/wiki/Transcendental_number",
                "/wiki/Continued_fraction",
                "/wiki/Liouville_number",
                "/wiki/Sturm%E2%80%93Liouville_theory",
                "/wiki/Charles_Fran%C3%A7ois_Sturm",
                "/wiki/Integral_equation",
                "/wiki/Liouville%27s_theorem_(Hamiltonian)",
                "/wiki/Hamiltonian_mechanics",
                "/wiki/Action-angle_variables",
                "/wiki/Integrable_systems",
                "/wiki/Journal_de_Math%C3%A9matiques_Pures_et_Appliqu%C3%A9es",
                "/wiki/%C3%89variste_Galois",
                "/wiki/National_Assembly_of_France",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/%C3%89cole_Centrale_Paris",
                "/wiki/Coll%C3%A8ge_de_France",
                "/wiki/Saint-Omer",
                "/wiki/Royal_Society_of_London",
                "/wiki/FRSE",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French"
            ],
            "text": "The crater Liouville on the Moon is named after him. So is the Liouville function, an important function in number theory.In 1851, he was elected a foreign member of the Royal Swedish Academy of Sciences.Liouville worked in a number of different fields in mathematics, including number theory, complex analysis, differential geometry and topology, but also mathematical physics and even astronomy. He is remembered particularly for Liouville's theorem. In number theory, he was the first to prove the existence of transcendental numbers by a construction using continued fractions (Liouville numbers). In mathematical physics, Liouville made two fundamental contributions: the Sturm\u2013Liouville theory, which was joint work with Charles Fran\u00e7ois Sturm, and is now a standard procedure to solve certain types of integral equations by developing into eigenfunctions, and the fact (also known as Liouville's theorem) that time evolution is measure preserving for a Hamiltonian system. In Hamiltonian dynamics, Liouville also introduced the notion of action-angle variables as a description of completely integrable systems. The modern formulation of this is sometimes called the Liouville\u2013Arnold theorem, and the underlying concept of integrability is referred to as Liouville integrability.Besides his academic achievements, he was very talented in organisational matters. Liouville founded the Journal de Math\u00e9matiques Pures et Appliqu\u00e9es which retains its high reputation up to today, in order to promote other mathematicians' work. He was the first to read, and to recognize the importance of, the unpublished work of \u00c9variste Galois which appeared in his journal in 1846. Liouville was also involved in politics for some time, and he became a member of the Constituting Assembly in 1848. However, after his defeat in the legislative elections in 1849, he turned away from politics.Liouville graduated from the \u00c9cole Polytechnique in 1827. After some years as an assistant at various institutions including the \u00c9cole Centrale Paris, he was appointed as professor at the \u00c9cole Polytechnique in 1838. He obtained a chair in mathematics at the Coll\u00e8ge de France in 1850 and a chair in mechanics at the Facult\u00e9 des Sciences in 1857.He was born in Saint-Omer in France on 24 March 1809.[1]Joseph Liouville FRS FRSE FAS (/\u02ccli\u02d0u\u02d0\u02c8v\u026al/; French:\u00a0[\u0292\u0254z\u025bf ljuvil]; 24 March 1809 \u2013 8 September 1882) was a French mathematician.",
            "title": "Joseph Liouville",
            "url": "https://en.wikipedia.org/wiki/Joseph_Liouville"
        },
        {
            "desc_links": [
                "/wiki/Weak_solution",
                "/wiki/Eigenvalues",
                "/wiki/Eigenfunction",
                "/wiki/Hermitian_operator",
                "/wiki/Differential_operator",
                "/wiki/Function_space",
                "/wiki/Boundary_value_problem",
                "/wiki/Complete_metric_space",
                "/wiki/Function_space",
                "/wiki/Partial_differential_equation",
                "/wiki/Separation_of_variables",
                "/wiki/Non-trivial",
                "/wiki/Boundary_condition",
                "/wiki/Mathematics",
                "/wiki/Jacques_Charles_Fran%C3%A7ois_Sturm",
                "/wiki/Joseph_Liouville",
                "/wiki/Differential_equation"
            ],
            "links": [
                "/wiki/Sturm_separation_theorem",
                "/wiki/Finite_difference_method",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Integer",
                "/wiki/Separation_of_variables",
                "/wiki/Partial_differential_equation",
                "/wiki/Wave_equation",
                "/wiki/Lp_space",
                "/wiki/Convergence_of_Fourier_series",
                "/wiki/Quantum_mechanics",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Variation_of_parameters",
                "/wiki/Integral_operator",
                "/wiki/Green%27s_function",
                "/wiki/Arzel%C3%A0%E2%80%93Ascoli_theorem",
                "/wiki/Compact_operator_on_Hilbert_space",
                "/wiki/Eigenvalue",
                "/wiki/Hilbert_space",
                "/wiki/Lp_space#Weighted_Lp_spaces",
                "/wiki/Linear_operator",
                "/wiki/Functional_analysis",
                "/wiki/Integrating_factor",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Integrating_factor",
                "/wiki/Partial_differential_equation",
                "/wiki/Weak_solution",
                "/wiki/Eigenvalues",
                "/wiki/Eigenfunction",
                "/wiki/Hermitian_operator",
                "/wiki/Differential_operator",
                "/wiki/Function_space",
                "/wiki/Boundary_value_problem",
                "/wiki/Complete_metric_space",
                "/wiki/Function_space",
                "/wiki/Partial_differential_equation",
                "/wiki/Separation_of_variables",
                "/wiki/Non-trivial",
                "/wiki/Boundary_condition",
                "/wiki/Mathematics",
                "/wiki/Jacques_Charles_Fran%C3%A7ois_Sturm",
                "/wiki/Joseph_Liouville",
                "/wiki/Differential_equation"
            ],
            "text": "wherewhereThen our above PDE may be written as:Let us apply separation of variables, which in doing we must impose that:For a linear second-order in one spatial dimension and first-order in time of the form:The SPPS method can, itself, be used to find a starting solution y0. Consider the equation (py')'\u00a0=\u00a0\u03bcqy; i.e., q, w, and \u03bb are replaced in (1) by 0, \u2212q, and \u03bc respectively. Then the constant function 1 is a nonvanishing solution corresponding to the eigenvalue \u03bc0\u00a0=\u00a00. While there is no guarantee that u0 or u1 will not vanish, the complex function y0\u00a0=\u00a0u0\u00a0+\u00a0iu1 will never vanish because two linearly independent solutions of a regular S\u2013L equation cannot vanish simultaneously as a consequence of the Sturm separation theorem. This trick gives a solution y0 of (1) for the value \u03bb0\u00a0=\u00a00. In practice if (1) has real coefficients, the solutions based on y0 will have very small imaginary parts which must be discarded.When \u03bb\u00a0=\u00a0\u03bb0, this reduces to the original construction described above for a solution linearly independent to a given one. The representations ('5'),('6') also have theoretical applications in Sturm\u2013Liouville theory.[3]Next one chooses coefficients c0, c1 so that the combination y = c0u0 + c1u1 satisfies the first boundary condition (2). This is simple to do since X(n)(a)\u00a0=\u00a00 and X\u00a0~(n)(a)\u00a0=\u00a00, for n > 0. The values of X(n)(b) and X\u00a0~(n)(b) provide the values of u0(b) and u1(b) and the derivatives u0'(b) and u1'(b), so the second boundary condition (3) becomes an equation in a power series in\u00a0\u03bb. For numerical work one may truncate this series to a finite number of terms, producing a calculable polynomial in \u03bb whose roots are approximations of the sought-after eigenvalues.Then for any \u03bb (real or complex), u0 and u1 are linearly independent solutions of the corresponding equation (1). (The functions p(x) and q(x) take part in this construction through their influence on the choice of y0.)when n\u00a0>\u00a00. The resulting iterated integrals are now applied as coefficients in the following two power series in\u00a0\u03bb:is a solution of the same equation and is linearly independent from y. Further, all solutions are linear combinations of these two solutions. In the SPPS algorithm, one must begin with an arbitrary value \u03bb0* (often \u03bb0*\u00a0=\u00a00; it does not need to be an eigenvalue) and any solution y0 of (1) with \u03bb\u00a0=\u00a0\u03bb0* which does not vanish on [a,\u00a0b]. (Discussion below of ways to find appropriate y0 and \u03bb0*.) Two sequences of functions X(n)(t), X\u00a0~(n)(t) on [a,\u00a0b], referred to as iterated integrals, are defined recursively as follows. First when n\u00a0=\u00a00, they are taken to be identically equal to 1 on [a,\u00a0b]. To obtain the next functions they are multiplied alternately by 1/(py02) and wy02 and integrated, specifically3. The Spectral Parameter Power Series (SPPS) method[3] makes use of a generalization of the following fact about second-order ordinary differential equations: if y is a solution which does not vanish at any point of [a,b], then the function2. Finite difference method.1. Shooting methods.[1][2] These methods proceed by guessing a value of \u03bb, solving an initial value problem defined by the boundary conditions at one endpoint, say, a, of the interval [a,\u00a0b], comparing the value this solution takes at the other endpoint b with the other desired boundary condition, and finally increasing or decreasing \u03bb as necessary to correct the original value. This strategy is not applicable for locating complex eigenvalues.[clarification needed]The Sturm\u2013Liouville differential equation (1) with boundary conditions may be solved in practice by a variety of numerical methods. In difficult cases, one may need to carry out the intermediate calculations to several hundred decimal places of accuracy in order to obtain the eigenvalues correctly to a few decimal places.where m and n are non-zero integers, Amn are arbitrary constants, andThe method of separation of variables suggests looking first for solutions of the simple form W = X(x) \u00d7 Y(y) \u00d7 T(t). For such a function W the partial differential equation becomes X\"/X + Y\"/Y = (1/c2)T\"/T. Since the three terms of this equation are functions of x,y,t separately, they must be constants. For example, the first term gives X\"\u00a0=\u00a0\u03bbX for a constant\u00a0\u03bb. The boundary conditions (\"held in a rectangular frame\") are W\u00a0=\u00a00 when x\u00a0=\u00a00, L1 or y = 0, L2 and define the simplest possible S\u2013L eigenvalue problems as in the example, yielding the \"normal mode solutions\" for W with harmonic time dependence,Certain partial differential equations can be solved with the help of S\u2013L theory. Suppose we are interested in the modes of vibration of a thin membrane, held in a rectangular frame, 0\u00a0\u2264\u00a0x\u00a0\u2264\u00a0L1, 0\u00a0\u2264\u00a0y\u00a0\u2264\u00a0L2. The equation of motion for the vertical membrane's displacement, W(x, y, t) is given by the wave equation:whose Fourier series agrees with the solution we found. The anti-differentiation technique is no longer useful in most cases when the differential equation is in many variables.In this case, we could have found the answer using anti-differentiation. This technique yieldsTherefore, by using formula (4), we obtain that the solution isThis particular Fourier series is troublesome because of its poor convergence properties. It is not clear a priori whether the series converges pointwise. Because of Fourier analysis, since the Fourier coefficients are \"square-summable\", the Fourier series converges in L2 which is all we need for this particular theory to function. We mention for the interested reader that in this case we may rely on a result which says that Fourier's series converge at every point of differentiability, and at jump points (the function x, considered as a periodic function, has a jump at\u00a0\u03c0) converges to the average of the left and right limits (see convergence of Fourier series).with the same boundary conditions. In this case, we must write f(x) = x in a Fourier series. The reader may check, either by integrating \u222be ikxx\u00a0dx or by consulting a table of Fourier transforms, that we thus obtainGiven the preceding, let us now solve the inhomogeneous problemis a solution with eigenvalue \u03bb = k2. We know that the solutions of a S\u2013L problem form an orthogonal basis, and we know from Fourier series that this set of sinusoidal functions is an orthogonal basis. Since orthogonal bases are always maximal (by definition) we conclude that the S\u2013L problem in this case has no other eigenvectors.Observe that if k is any integer, then the functionwhere the unknowns are \u03bb and u(x). As above, we must add boundary conditions, we take for exampleWe wish to find a function u(x) which solves the following Sturm\u2013Liouville problem:Then a solution to the proposed equation is simply:But if in place of specifying initial values at a single point, it is desired to specify values at two different points (so-called boundary values), e.g. y(a) = 0 and y(b) = 1, the problem turns out to be much more difficult. Notice that by adding a suitable known differentiable function to y, whose values at a and b satisfy the desired boundary conditions, and injecting inside the proposed differential equation, it can be assumed without loss of generality that the boundary conditions are of the form y(a) = 0 and y(b) = 0.In general, if initial conditions at some point are specified, for example y(a) = 0 and y'(a) = 0, a second order differential equation can be solved using ordinary methods and a well known fundamental theorem ensures that the differential equation has a unique solution in a neighbourhood of the point where the initial conditions have been specified.Thus, the first member of the proposed equation can be transformed in a Sturm-Liouville operator L as in the previous section, and the equation can now be written:A solution isThe operator L in the previous section can be writtenIf the interval is unbounded, or if the coefficients have singularities at the boundary points, one calls L singular. In this case, the spectrum no longer consists of eigenvalues alone and can contain a continuous component. There is still an associated eigenfunction expansion (similar to Fourier series versus Fourier transform). This is important in quantum mechanics, since the one-dimensional time-independent Schr\u00f6dinger equation is a special case of a S\u2013L equation.are equivalent.where z is chosen to be some real number which is not an eigenvalue. Then, computing the resolvent amounts to solving the inhomogeneous equation, which can be done using the variation of parameters formula. This shows that the resolvent is an integral operator with a continuous symmetric kernel (the Green's function of the problem). As a consequence of the Arzel\u00e0\u2013Ascoli theorem, this integral operator is compact and existence of a sequence of eigenvalues \u03b1n which converge to 0 and eigenfunctions which form an orthonormal basis follows from the spectral theorem for compact operators. Finally, note thatThis is precisely the eigenvalue problem; that is, one is trying to find the eigenvalues \u03bb1, \u03bb2, \u03bb3, ... and the corresponding eigenvectors u1, u2, u3, ... of the L operator. The proper setting for this problem is the Hilbert space L2([a,\u2009b], w(x)\u2009dx) with scalar productcan be viewed as a linear operator mapping a function u to another function Lu. One may study this linear operator in the context of functional analysis. In fact, equation (1) can be written asThe mapor, explicitly,and then collecting gives the Sturm\u2013Liouville form:multiplying through by the integrating factorso the differential equation is equivalent towhich can be easily put into Sturm\u2013Liouville form sincegivesMultiplying throughout by an integrating factor ofDivide throughout by x3:which can be written in Sturm\u2013Liouville form asThe differential equation (1) is said to be in Sturm\u2013Liouville form or self-adjoint form. All second-order linear ordinary differential equations can be recast in the form on the left-hand side of (1) by multiplying both sides of the equation by an appropriate integrating factor (although the same is not true of second-order partial differential equations, or if y is a vector.)Note that, unless p(x) is continuously differentiable and q(x), w(x) are continuous, the equation has to be understood in a weak sense.Under the assumption that the S\u2013L problem is regular, the main tenet of Sturm\u2013Liouville theory states that:A Sturm\u2013Liouville (S\u2013L) problem is said to be regular if p(x), w(x)\u00a0>\u00a00, and p(x), p\u2032(x), q(x), and w(x) are continuous functions over the finite interval [a,\u00a0b], and has separated boundary conditions of the formSuch values of \u03bb, when they exist, are called the eigenvalues of the boundary value problem defined by (1) and the prescribed set of boundary conditions. The corresponding solutions (for each such \u03bb) are the eigenfunctions of this problem. Under normal assumptions on the coefficient functions p(x), q(x), and w(x) above, they induce a Hermitian differential operator in some function space defined by boundary conditions. The resulting theory of the existence and asymptotic behavior of the eigenvalues, the corresponding qualitative theory of the eigenfunctions and their completeness in a suitable function space became known as Sturm\u2013Liouville theory. This theory is important in applied mathematics, where S\u2013L problems occur very commonly, particularly when dealing with linear partial differential equations that are separable.The value of \u03bb is not specified in the equation; finding the values of \u03bb for which there exists a non-trivial solution of (1) satisfying the boundary conditions is part of the Sturm\u2013Liouville (S\u2013L) problem.where y is a function of the free variable x. Here the functions p(x), q(x), and w(x)\u00a0>\u00a00 are specified at the outset. In the simplest of cases all coefficients are continuous on the finite closed interval [a,b], and p has continuous derivative. In this simplest of all cases, this function y is called a solution if it is continuously differentiable on (a,b) and satisfies the equation (1) at every point in (a,b). In addition, the unknown function y is typically required to satisfy some boundary conditions at a and b. The function w(x), which is sometimes also called r(x), is called the \"weight\" or \"density\" function.In mathematics and its applications, a classical Sturm\u2013Liouville theory, named after Jacques Charles Fran\u00e7ois Sturm (1803\u20131855) and Joseph Liouville (1809\u20131882), is the theory of a real second-order linear differential equation of the form",
            "title": "Sturm\u2013Liouville theory",
            "url": "https://en.wikipedia.org/wiki/Sturm%E2%80%93Liouville_theory"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Hermann Schwarz",
            "url": "https://en.wikipedia.org/wiki/Hermann_Schwarz"
        },
        {
            "desc_links": [
                "/wiki/Poisson%27s_equation",
                "/wiki/Elliptic_partial_differential_equation",
                "/wiki/Potential_theory",
                "/wiki/Harmonic_function",
                "/wiki/Electromagnetism",
                "/wiki/Astronomy",
                "/wiki/Fluid_dynamics",
                "/wiki/Potential",
                "/wiki/Heat_conduction",
                "/wiki/Steady-state",
                "/wiki/Heat_equation",
                "/wiki/Laplace_operator",
                "/wiki/Mathematics",
                "/wiki/Partial_differential_equation",
                "/wiki/Pierre-Simon_Laplace"
            ],
            "links": [
                "/wiki/Green%27s_identities",
                "/wiki/Green%27s_function",
                "/wiki/Natural_logarithm",
                "/wiki/Potential",
                "/wiki/Potential_flow",
                "/wiki/Point_particle",
                "/wiki/Euler_equations_(fluid_dynamics)",
                "/wiki/Incompressible_flow",
                "/wiki/Physics",
                "/wiki/Potential",
                "/wiki/Point_particle",
                "/wiki/Inverse-square_law",
                "/wiki/Poisson_equation",
                "/wiki/Gauss%27_divergence_theorem",
                "/wiki/Dirac_delta_function",
                "/wiki/Distribution_(mathematics)",
                "/wiki/Weak_solution",
                "/wiki/Positive_operator",
                "/wiki/Fundamental_solution",
                "/wiki/Poisson_equation",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Velocity_potential",
                "/wiki/Stream_function",
                "/wiki/Streamlines,_streaklines_and_pathlines",
                "/wiki/De_Moivre%27s_formula#Formulas_for_cosine_and_sine_individually",
                "/wiki/Fourier_series",
                "/wiki/Wave_equation",
                "/wiki/Stokes%27_theorem",
                "/wiki/Cauchy%E2%80%93Riemann_equations",
                "/wiki/Analytic_function",
                "/wiki/Harmonic_function",
                "/wiki/Analytic_function",
                "/wiki/Superposition_principle",
                "/wiki/Neumann_boundary_condition",
                "/wiki/Normal_derivative",
                "/wiki/Dirichlet_problem",
                "/wiki/Heat_equation",
                "/wiki/Helmholtz_equation",
                "/wiki/Poisson%27s_equation",
                "/wiki/Divergence",
                "/wiki/Operator_(mathematics)",
                "/wiki/Gradient",
                "/wiki/Laplace_operator",
                "/wiki/Curvilinear_coordinates",
                "/wiki/Spherical_coordinates",
                "/wiki/Cylindrical_coordinates",
                "/wiki/Cartesian_coordinates",
                "/wiki/Differentiable_function",
                "/wiki/Poisson%27s_equation",
                "/wiki/Elliptic_partial_differential_equation",
                "/wiki/Potential_theory",
                "/wiki/Harmonic_function",
                "/wiki/Electromagnetism",
                "/wiki/Astronomy",
                "/wiki/Fluid_dynamics",
                "/wiki/Potential",
                "/wiki/Heat_conduction",
                "/wiki/Steady-state",
                "/wiki/Heat_equation",
                "/wiki/Mathematics",
                "/wiki/Partial_differential_equation",
                "/wiki/Pierre-Simon_Laplace"
            ],
            "text": "A potential that doesn't satisfy Laplace's equation together with the boundary condition is an invalid electrostatic potential.Using a uniqueness theorem and showing that a potential satisfies Laplace's equation (second derivative of V should be zero i.e. in free space) and the potential has the correct values at the boundaries, the potential is then uniquely defined.In the particular case of the empty space (\u03c1 = 0) Poisson's equation reduces to Laplace's equation for the electric potential.Taking the divergence of the electrostatic field, we obtain Poisson's equation, that relates charge density and electric potentialTaking the gradient of the electric potential we get the electrostatic fieldIn free space the Laplace equation of any electrostatic potential must equal zero since \u03c1 (charge density) is zero in free space.whereNote that if P is inside the sphere, then P' will be outside the sphere. The Green's function is then given byThus the Green's function describes the influence at (x\u2032, y\u2032, z\u2032) of the data f and g. For the case of the interior of a sphere of radius a, the Green's function may be obtained by means of a reflection (Sommerfeld 1949): the source point P at distance \u03c1 from the center of the sphere is reflected along its radial line to a point P' that is at a distanceThe notations un and Gn denote normal derivatives on S. In view of the conditions satisfied by u and G, this result simplifies toand u assumes the boundary values g on S, then we may apply Green's identity, (a consequence of the divergence theorem) which states thatNow if u is any solution of the Poisson equation in V:may satisfyA Green's function is a fundamental solution that also satisfies a suitable condition on the boundary S of a volume V. For instance,where log(r) denotes the natural logarithm. Note that, with the opposite sign convention, this is the potential generated by a pointlike sink (see point particle), which is the solution of the Euler equations in two-dimensional incompressible flow.Note that, with the opposite sign convention (used in physics), this is the potential generated by a point particle, for an inverse-square law force, arising in the solution of Poisson equation. A similar argument shows that in two dimensionson a sphere of radius r that is centered on the source point, and henceIt follows thatThe Laplace equation is unchanged under a rotation of coordinates, and hence we can expect that a fundamental solution may be obtained among solutions that only depend upon the distance r from the source point. If we choose the volume to be a ball of radius a around the source point, then Gauss' divergence theorem implies thatwhere the Dirac delta function \u03b4 denotes a unit source concentrated at the point (x\u2032, y\u2032, z\u2032). No function has this property: in fact it is a distribution rather than a function; but it can be thought of as a limit of functions whose integrals over space are unity, and whose support (the region where the function is non-zero) shrinks to a point (see weak solution). It is common to take a different sign convention for this equation than one typically does when defining fundamental solutions. This choice of sign is often convenient to work with because \u2212\u0394 is a positive operator. The definition of the fundamental solution thus implies that, if the Laplacian of u is integrated over any volume that encloses the source point, thenA fundamental solution of Laplace's equation satisfieswhich is the Poisson equation. It is important to note that the Laplace equation can be used in three-dimensional problems in electrostatics and fluid flow just as in two dimensions.The second of Maxwell's equations then implies thatso the electric potential \u03c6 may be constructed to satisfyAccording to Maxwell's equations, an electric field (u,v) in two space dimensions that is independent of time satisfiesThus every analytic function corresponds to a steady incompressible, irrotational fluid flow in the plane. The real part is the velocity potential, and the imaginary part is the stream function.and the irrotationality condition implies that \u03c8 satisfies the Laplace equation. The harmonic function \u03c6 that is conjugate to \u03c8 is called the velocity potential. The Cauchy\u2013Riemann equations imply thatthen the continuity condition is the integrability condition for this differential: the resulting function is called the stream function because it is constant along flow lines. The first derivatives of \u03c8 are given byIf we define the differential of a function \u03c8 byand the condition that the flow be irrotational is thatLet the quantities u and v be the horizontal and vertical components of the velocity field of a steady incompressible, irrotational flow in two dimensions. The continuity condition for an incompressible flow is thatwhich is a Fourier series for f. These trigonometric functions can themselves be expanded, using multiple angle formulae.Thereforewith suitably defined coefficients whose real and imaginary parts are given byThere is an intimate connection between power series and Fourier series. If we expand a function f in a power series inside a circle of radius R, this means thatThe close connection between the Laplace equation and analytic functions implies that any solution of the Laplace equation has derivatives of all orders, and can be expanded in a power series, at least inside a circle that does not enclose a singularity. This is in sharp contrast to solutions of the wave equation, which generally have less regularity.However, the angle \u03b8 is single-valued only in a region that does not enclose the origin.then a corresponding analytic function isand thus \u03c8 may be defined by a line integral. The integrability condition and Stokes' theorem implies that the value of the line integral connecting two points is independent of the path. The resulting pair of solutions of the Laplace equation are called conjugate harmonic functions. This construction is only valid locally, or provided that the path does not loop around a singularity. For example, if r and \u03b8 are polar coordinates andThe Laplace equation for \u03c6 implies that the integrability condition for \u03c8 is satisfied:This relation does not determine \u03c8, but only its increments:then the Cauchy\u2013Riemann equations will be satisfied if we setTherefore u satisfies the Laplace equation. A similar calculation shows that v also satisfies the Laplace equation. Conversely, given a harmonic function, it is the real part of an analytic function, f(z) (at least locally). If a trial form iswhere ux is the first partial derivative of u with respect to x. It follows thatthen the necessary condition that f(z) be analytic is that the Cauchy\u2013Riemann equations be satisfied:The real and imaginary parts of a complex analytic function both satisfy the Laplace equation. That is, if z = x + iy, and ifThe Laplace equation in two independent variables has the formSolutions of Laplace's equation are called harmonic functions; they are all analytic within the domain where the equation is satisfied. If any two functions are solutions to Laplace's equation (or any linear homogeneous differential equation), their sum (or any linear combination) is also a solution. This property, called the principle of superposition, is very useful, e.g., solutions to complex problems can be constructed by summing simple solutions.The Neumann boundary conditions for Laplace's equation specify not the function \u03c6 itself on the boundary of D, but its normal derivative. Physically, this corresponds to the construction of a potential for a vector field whose effect is known at the boundary of D alone.The Dirichlet problem for Laplace's equation consists of finding a solution \u03c6 on some domain D such that \u03c6 on the boundary of D is equal to some given function. Since the Laplace operator appears in the heat equation, one physical interpretation of this problem is as follows: fix the temperature on the boundary of the domain according to the given specification of the boundary condition. Allow heat to flow until a stationary state is reached in which the temperature at each point on the domain doesn't change anymore. The temperature distribution in the interior will then be given by the solution to the corresponding Dirichlet problem.The Laplace equation is also a special case of the Helmholtz equation.then it is called \"Poisson's equation\".If the right-hand side is specified as a given function, h(x, y, z), i.e., if the whole equation is written aswhere \u2207\u00a0\u2022\u00a0 is the divergence operator (also symbolized \"div\") which maps vector functions to scalar functions, and \u2207\u00a0 is the gradient operator (also symbolized \"grad\") which maps scalar functions to vector functions. Hence the Laplacian \u0394f \u225d div grad f maps the scalar function f to a scalar function.where \u2206\u00a0=\u00a0\u22072 is the Laplace operator or \"Laplacian\"or, especially in more general contexts,This is often written asorIn curvilinear coordinates,In spherical coordinates,In cylindrical coordinates,In Cartesian coordinatesIn three dimensions, the problem is to find twice-differentiable real-valued functions f, of real variables x, y, and z, such thatLaplace's equation and Poisson's equation are the simplest examples of elliptic partial differential equations. The general theory of solutions to Laplace's equation is known as potential theory. The solutions of Laplace's equation are the harmonic functions, which are important in many fields of science, notably the fields of electromagnetism, astronomy, and fluid dynamics, because they can be used to accurately describe the behavior of electric, gravitational, and fluid potentials. In the study of heat conduction, the Laplace equation is the steady-state heat equation.In mathematics, Laplace's equation is a second-order partial differential equation named after Pierre-Simon Laplace who first studied its properties. This is often written as:",
            "title": "Laplace's equation",
            "url": "https://en.wikipedia.org/wiki/Laplace%27s_equation"
        },
        {
            "desc_links": [
                "/wiki/Poincar%C3%A9_group",
                "/wiki/Lorentz_transformations",
                "/wiki/Hendrik_Lorentz",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Special_relativity",
                "/wiki/Gravitational_wave",
                "/wiki/Pure_mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Mathematical_physics",
                "/wiki/Celestial_mechanics",
                "/wiki/Poincar%C3%A9_conjecture",
                "/wiki/Unsolved_problems_in_mathematics",
                "/wiki/Grigori_Perelman",
                "/wiki/Three-body_problem",
                "/wiki/Chaos_theory",
                "/wiki/Topology",
                "/wiki/Help:IPA/French",
                "/wiki/File:Fr-Henri_Poincar%C3%A9.ogg",
                "/wiki/Mathematician",
                "/wiki/Theoretical_physicist",
                "/wiki/Philosophy_of_science",
                "/wiki/Polymath"
            ],
            "links": [
                "/wiki/Daniel_Dennett",
                "/wiki/Randomness",
                "/wiki/Science_and_Hypothesis",
                "/wiki/The_Value_of_Science",
                "/wiki/Jacques_Hadamard",
                "/wiki/Non-Euclidean_geometry",
                "/wiki/Conventionalism",
                "/wiki/Newton%27s_first_law",
                "/wiki/Euclidean_geometry",
                "/wiki/Arithmetic",
                "/wiki/Analytic/synthetic_distinction",
                "/wiki/Peano%27s_axioms",
                "/wiki/A_priori_and_a_posteriori",
                "/wiki/Immanuel_Kant",
                "/wiki/Set_theory",
                "/wiki/Impredicativity",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Bertrand_Russell",
                "/wiki/Gottlob_Frege",
                "/wiki/Logic",
                "/wiki/Intuition_(knowledge)",
                "/wiki/Science_and_Hypothesis",
                "/wiki/Nobel_Prize_in_Physics",
                "/wiki/Henri_Becquerel",
                "/wiki/G%C3%B6sta_Mittag-Leffler",
                "/wiki/Hendrik_Lorentz",
                "/wiki/Pieter_Zeeman",
                "/wiki/Marie_Curie",
                "/wiki/Albert_Michelson",
                "/wiki/Gabriel_Lippmann",
                "/wiki/Guglielmo_Marconi",
                "/wiki/Georg_Cantor",
                "/wiki/Transfinite_number",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Jacques_Hadamard",
                "/wiki/Qualitative_theory_of_differential_equations",
                "/wiki/Mathematical_physics",
                "/wiki/Celestial_mechanics",
                "/wiki/Chaos_theory",
                "/wiki/Poincar%C3%A9_recurrence_theorem",
                "/wiki/Dynamical_system",
                "/wiki/Isaac_Newton",
                "/wiki/Homotopy",
                "/wiki/Homology_(mathematics)",
                "/wiki/Fundamental_group",
                "/wiki/Felix_Klein",
                "/wiki/Johann_Benedict_Listing",
                "/wiki/Enrico_Betti",
                "/wiki/Bernhard_Riemann",
                "/wiki/Group_theory",
                "/wiki/Lorentz_transformations",
                "/wiki/E._T._Whittaker",
                "/wiki/Einstein_synchronisation",
                "/wiki/Special_relativity",
                "/wiki/Hans_Vaihinger",
                "/wiki/Non-Euclidean_geometry",
                "/wiki/Gravitational_waves",
                "/wiki/Albert_Einstein",
                "/wiki/Mass%E2%80%93energy_equivalence",
                "/wiki/Recoil",
                "/wiki/Lorentz_boost",
                "/wiki/Perpetual_motion",
                "/wiki/Mass%E2%80%93energy_equivalence#Electromagnetic_rest_mass",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Lorentz_ether_theory",
                "/wiki/Center_of_gravity",
                "/wiki/Fluid",
                "/wiki/Center_of_mass_frame",
                "/wiki/Principle_of_relativity",
                "/wiki/Karl_Weierstrass",
                "/wiki/Chaos_theory",
                "/wiki/Karl_F._Sundman",
                "/wiki/Qiudong_Wang",
                "/wiki/Isaac_Newton",
                "/wiki/N-body_problem",
                "/wiki/Oscar_II_of_Sweden",
                "/wiki/G%C3%B6sta_Mittag-Leffler",
                "/wiki/Celestial_mechanics",
                "/wiki/Fluid_mechanics",
                "/wiki/Optics",
                "/wiki/Telegraphy",
                "/wiki/Capillarity",
                "/wiki/Elasticity_(physics)",
                "/wiki/Thermodynamics",
                "/wiki/Potential_theory",
                "/wiki/Quantum_mechanics",
                "/wiki/Theory_of_relativity",
                "/wiki/Physical_cosmology",
                "/wiki/Claude_All%C3%A8gre",
                "/wiki/Panth%C3%A9on,_Paris",
                "/wiki/Prostate",
                "/wiki/Embolism",
                "/wiki/Cimeti%C3%A8re_du_Montparnasse",
                "/wiki/Louis_Bachelier",
                "/wiki/Dimitrie_Pompeiu",
                "/wiki/Soci%C3%A9t%C3%A9_astronomique_de_France",
                "/wiki/Alfred_Dreyfus",
                "/wiki/Bureau_des_Longitudes",
                "/wiki/Clock_synchronization",
                "/wiki/Decimal_degrees",
                "/wiki/Longitude",
                "/wiki/Oscar_II_of_Sweden",
                "/wiki/Three-body_problem",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Acad%C3%A9mie_fran%C3%A7aise",
                "/wiki/University_of_Paris",
                "/wiki/Celestial_mechanics",
                "/wiki/Mathematical_physics",
                "/wiki/University_of_Paris",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Caen_University",
                "/wiki/Automorphic_function",
                "/wiki/Doctorate_in_Science",
                "/wiki/Differential_equations",
                "/wiki/Solar_system",
                "/wiki/University_of_Paris",
                "/wiki/Corps_des_Mines",
                "/wiki/Vesoul",
                "/wiki/Magny-l%C3%A8s-Jussey",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Charles_Hermite",
                "/wiki/%C3%89cole_des_Mines",
                "/wiki/Franco-Prussian_War",
                "/wiki/Nancy,_Meurthe-et-Moselle",
                "/wiki/Henri_Poincar%C3%A9_University",
                "/wiki/Concours_g%C3%A9n%C3%A9ral",
                "/wiki/Diphtheria",
                "/wiki/Nancy,_Meurthe-et-Moselle",
                "/wiki/University_of_Nancy",
                "/wiki/Emile_Boutroux",
                "/wiki/Raymond_Poincar%C3%A9",
                "/wiki/Acad%C3%A9mie_fran%C3%A7aise",
                "/wiki/Poincar%C3%A9_group",
                "/wiki/Lorentz_transformations",
                "/wiki/Hendrik_Lorentz",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Special_relativity",
                "/wiki/Gravitational_wave",
                "/wiki/Pure_mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Mathematical_physics",
                "/wiki/Celestial_mechanics",
                "/wiki/Poincar%C3%A9_conjecture",
                "/wiki/Unsolved_problems_in_mathematics",
                "/wiki/Grigori_Perelman",
                "/wiki/Three-body_problem",
                "/wiki/Chaos_theory",
                "/wiki/Topology"
            ],
            "text": "Poincar\u00e9's two stages\u2014random combinations followed by selection\u2014became the basis for Daniel Dennett's two-stage model of free will.[75]Although he most often spoke of a deterministic universe, Poincar\u00e9 said that the subconscious generation of new possibilities involves chance.Poincar\u00e9's famous lectures before the Soci\u00e9t\u00e9 de Psychologie in Paris (published as Science and Hypothesis, The Value of Science, and Science and Method) were cited by Jacques Hadamard as the source for the idea that creativity and invention consist of two mental stages, first random combinations of possible solutions to a problem, followed by a critical evaluation.[73]However, Poincar\u00e9 did not share Kantian views in all branches of philosophy and mathematics. For example, in geometry, Poincar\u00e9 believed that the structure of non-Euclidean space can be known analytically. Poincar\u00e9 held that convention plays an important role in physics. His view (and some later, more extreme versions of it) came to be known as \"conventionalism\". Poincar\u00e9 believed that Newton's first law was not empirical but is a conventional framework assumption for mechanics (Gargani, 2012).[71] He also believed that the geometry of physical space is conventional. He considered examples in which either the geometry of the physical fields or gradients of temperature can be changed, either describing a space as non-Euclidean measured by rigid rulers, or as a Euclidean space where the rulers are expanded or shrunk by a variable heat distribution. However, Poincar\u00e9 thought that we were so accustomed to Euclidean geometry that we would prefer to change the physical laws to save Euclidean geometry rather than shift to a non-Euclidean physical geometry.[72]Poincar\u00e9 believed that arithmetic is a synthetic science. He argued that Peano's axioms cannot be proven non-circularly with the principle of induction (Murzi, 1998), therefore concluding that arithmetic is a priori synthetic and not analytic. Poincar\u00e9 then went on to say that mathematics cannot be deduced from logic since it is not analytic. His views were similar to those of Immanuel Kant (Kolak, 2001, Folina 1992). He strongly opposed Cantorian set theory, objecting to its use of impredicative definitions[citation needed].Poincar\u00e9 had philosophical views opposite to those of Bertrand Russell and Gottlob Frege, who believed that mathematics was a branch of logic. Poincar\u00e9 strongly disagreed, claiming that intuition was the life of mathematics. Poincar\u00e9 gives an interesting point of view in his book Science and Hypothesis:The fact that renowned theoretical physicists like Poincar\u00e9, Boltzmann or Gibbs were not awarded the Nobel Prize is seen as evidence that the Nobel committee had more regard for experimentation than theory.[69][70] In Poincar\u00e9's case, several of those who nominated him pointed out that the greatest problem was to name a specific discovery, invention, or technique.[66]Henri Poincar\u00e9 did not receive the Nobel Prize in Physics, but he had influential advocates like Henri Becquerel or committee member G\u00f6sta Mittag-Leffler.[66][67] The nomination archive reveals that Poincar\u00e9 received a total of 51 nominations between 1904 and 1912, the year of his death.[68] Of the 58 nominations for the 1910 Nobel Prize, 34 named Poincar\u00e9.[68] Nominators included Nobel laureates Hendrik Lorentz and Pieter Zeeman (both of 1902), Marie Curie (of 1903), Albert Michelson (of 1907), Gabriel Lippmann (of 1908) and Guglielmo Marconi (of 1909).[68]Named after himAwardsPoincar\u00e9 was dismayed by Georg Cantor's theory of transfinite numbers, and referred to it as a \"disease\" from which mathematics would eventually be cured.[63] Poincar\u00e9 said, \"There is no actual infinite; the Cantorians have forgotten this, and that is why they have fallen into contradiction.\"[64]His method of thinking is well summarised as:In addition, Toulouse stated that most mathematicians worked from principles already established while Poincar\u00e9 started from basic principles each time (O'Connor et al., 2002).These abilities were offset to some extent by his shortcomings:Poincar\u00e9's mental organisation was not only interesting to Poincar\u00e9 himself but also to \u00c9douard Toulouse, a psychologist of the Psychology Laboratory of the School of Higher Studies in Paris. Toulouse wrote a book entitled Henri Poincar\u00e9 (1910).[61][62] In it, he discussed Poincar\u00e9's regular schedule:The mathematician Darboux claimed he was un intuitif (intuitive), arguing that this is demonstrated by the fact that he worked so often by visual representation. He did not care about being rigorous and disliked logic.[citation needed] (Despite this opinion, Jacques Hadamard wrote that Poincar\u00e9's research demonstrated marvelous clarity[60] and Poincar\u00e9 himself wrote that he believed that logic was not a way to invent but a way to structure ideas and that logic limits ideas.)Poincar\u00e9's work habits have been compared to a bee flying from flower to flower. Poincar\u00e9 was interested in the way his mind worked; he studied his habits and gave a talk about his observations in 1908 at the Institute of General Psychology in Paris. He linked his way of thinking to how he made several discoveries.After defending his doctoral thesis on the study of singular points of the system of differential equations, Poincar\u00e9 wrote a series of memoirs under the title \"On curves defined by differential equations\" (1881\u20131882).[57] In these articles, he built a new branch of mathematics, called \"qualitative theory of differential equations\". Poincar\u00e9 showed that even if the differential equation can not be solved in terms of known functions, yet from the very form of the equation, a wealth of information about the properties and behavior of the solutions can be found. In particular, Poincar\u00e9 investigated the nature of the trajectories of the integral curves in the plane, gave a classification of singular points (saddle, focus, center, node), introduced the concept of a limit cycle and the loop index, and showed that the number of limit cycles is always finite, except for some special cases. Poincar\u00e9 also developed a general theory of integral invariants and solutions of the variational equations. For the finite-difference equations, he created a new direction \u2013 the asymptotic analysis of the solutions. He applied all these achievements to study practical problems of mathematical physics and celestial mechanics, and the methods used were the basis of its topological works.[58][59]These monographs include an idea of Poincar\u00e9, which later became the base for mathematical \"chaos theory\" (see, in particular, the Poincar\u00e9 recurrence theorem) and the general theory of dynamical systems. Poincar\u00e9 authored important works on astronomy for the equilibrium figures of a gravitating rotating fluid. He introduced the important concept of bifurcation points and proved the existence of equilibrium figures such as the non-ellipsoids, including ring-shaped and pear-shaped figures, and their stability. For this discovery, Poincar\u00e9 received the Gold Medal of the Royal Astronomical Society (1900).[56]Poincar\u00e9 published two now classical monographs, \"New Methods of Celestial Mechanics\" (1892\u20131899) and \"Lectures on Celestial Mechanics\" (1905\u20131910). In them, he successfully applied the results of their research to the problem of the motion of three bodies and studied in detail the behavior of solutions (frequency, stability, asymptotic, and so on). They introduced the small parameter method, fixed points, integral invariants, variational equations, the convergence of the asymptotic expansions. Generalizing a theory of Bruns (1887), Poincar\u00e9 showed that the three-body problem is not integrable. In other words, the general solution of the three-body problem can not be expressed in terms of algebraic and transcendental functions through unambiguous coordinates and velocities of the bodies. His work in this area was the first major achievement in celestial mechanics since Isaac Newton.[55]His research in geometry led to the abstract topological definition of homotopy and homology. He also first introduced the basic concepts and invariants of combinatorial topology, such as Betti numbers and the fundamental group. Poincar\u00e9 proved a formula relating the number of edges, vertices and faces of n-dimensional polyhedron (the Euler\u2013Poincar\u00e9 theorem) and gave the first precise formulation of the intuitive notion of dimension.[54]The subject is clearly defined by Felix Klein in his \"Erlangen Program\" (1872): the geometry invariants of arbitrary continuous transformation, a kind of geometry. The term \"topology\" was introduced, as suggested by Johann Benedict Listing, instead of previously used \"Analysis situs\". Some important concepts were introduced by Enrico Betti and Bernhard Riemann. But the foundation of this science, for a space of any dimension, was created by Poincar\u00e9. His first article on this topic appeared in 1894.[53]Poincar\u00e9 introduced group theory to physics, and was the first to study the group of Lorentz transformations.[52] He also made major contributions to the theory of discrete groups and their representations.While this is the view of most historians, a minority go much further, such as E. T. Whittaker, who held that Poincar\u00e9 and Lorentz were the true discoverers of relativity.[51]Poincar\u00e9's work in the development of special relativity is well recognised,[39] though most historians stress that despite many similarities with Einstein's work, the two had very different research agendas and interpretations of the work.[45] Poincar\u00e9 developed a similar physical interpretation of local time and noticed the connection to signal velocity, but contrary to Einstein he continued to use the ether-concept in his papers and argued that clocks in the ether show the \"true\" time, and moving clocks show the local time. So Poincar\u00e9 tried to keep the relativity principle in accordance with classical concepts, while Einstein developed a mathematically equivalent kinematics based on the new physical concepts of the relativity of space and time.[46][47][48][49][50]Einstein's first paper on relativity was published three months after Poincar\u00e9's short paper,[35] but before Poincar\u00e9's longer version.[36] Einstein relied on the principle of relativity to derive the Lorentz transformations and used a similar clock synchronisation procedure (Einstein synchronisation) to the one that Poincar\u00e9 (1900) had described, but Einstein's paper was remarkable in that it contained no references at all. Poincar\u00e9 never acknowledged Einstein's work on special relativity. However, Einstein expressed sympathy with Poincar\u00e9's outlook obliquely in a letter to Hans Vaihinger on 3 May 1919, when Einstein considered Vaihinger's general outlook to be close to his own and Poincar\u00e9's to be close to Vaihinger's.[43] In public, Einstein acknowledged Poincar\u00e9 posthumously in the text of a lecture in 1921 called Geometrie und Erfahrung in connection with non-Euclidean geometry, but not in connection with special relativity. A few years before his death, Einstein commented on Poincar\u00e9 as being one of the pioneers of relativity, saying \"Lorentz had already recognised that the transformation named after him is essential for the analysis of Maxwell's equations, and Poincar\u00e9 deepened this insight still further ....\"[44]In 1905 Henri Poincar\u00e9 first proposed gravitational waves (ondes gravifiques) emanating from a body and propagating at the speed of light.[42] \"Il importait d'examiner cette hypoth\u00e8se de plus pr\u00e8s et en particulier de rechercher quelles modifications elle nous obligerait \u00e0 apporter aux lois de la gravitation. C'est ce que j'ai cherch\u00e9 \u00e0 d\u00e9terminer\u00a0; j'ai \u00e9t\u00e9 d'abord conduit \u00e0 supposer que la propagation de la gravitation n'est pas instantan\u00e9e, mais se fait avec la vitesse de la lumi\u00e8re.\"It was Albert Einstein's concept of mass\u2013energy equivalence (1905) that a body losing energy as radiation or heat was losing mass of amount m\u00a0=\u00a0E/c2 that resolved[39] Poincar\u00e9's paradox, without using any compensating mechanism within the ether.[40] The Hertzian oscillator loses mass in the emission process, and momentum is conserved in any frame. However, concerning Poincar\u00e9's solution of the Center of Gravity problem, Einstein noted that Poincar\u00e9's formulation and his own from 1906 were mathematically equivalent.[41]Poincar\u00e9 himself came back to this topic in his St. Louis lecture (1904).[32] This time (and later also in 1908) he rejected[38] the possibility that energy carries mass and criticized the ether solution to compensate the above-mentioned problems:However, Poincar\u00e9's resolution led to a paradox when changing frames: if a Hertzian oscillator radiates in a certain direction, it will suffer a recoil from the inertia of the fictitious fluid. Poincar\u00e9 performed a Lorentz boost (to order v/c) to the frame of the moving source. He noted that energy conservation holds in both frames, but that the law of conservation of momentum is violated. This would allow perpetual motion, a notion which he abhorred. The laws of nature would have to be different in the frames of reference, and the relativity principle would not hold. Therefore, he argued that also in this case there has to be another compensating mechanism in the ether.Like others before, Poincar\u00e9 (1900) discovered a relation between mass and electromagnetic energy. While studying the conflict between the action/reaction principle and Lorentz ether theory, he tried to determine whether the center of gravity still moves with a uniform velocity when electromagnetic fields are included.[28] He noticed that the action/reaction principle does not hold for matter alone, but that the electromagnetic field has its own momentum. Poincar\u00e9 concluded that the electromagnetic field energy of an electromagnetic wave behaves like a fictitious fluid (fluide fictif) with a mass density of E/c2. If the center of mass frame is defined by both the mass of matter and the mass of the fictitious fluid, and if the fictitious fluid is indestructible\u2014it's neither created or destroyed\u2014then the motion of the center of mass frame remains uniform. But electromagnetic energy can be converted into other forms of energy. So Poincar\u00e9 assumed that there exists a non-electric energy fluid at each point of space, into which electromagnetic energy can be transformed and which also carries a mass proportional to the energy. In this way, the motion of the center of mass remains uniform. Poincar\u00e9 said that one should not be too surprised by these assumptions, since they are only mathematical fictions.He discussed the \"principle of relative motion\" in two papers in 1900[28][31] and named it the principle of relativity in 1904, according to which no physical experiment can discriminate between a state of uniform motion and a state of rest.[32] In 1905 Poincar\u00e9 wrote to Lorentz about Lorentz's paper of 1904, which Poincar\u00e9 described as a \"paper of supreme importance.\" In this letter he pointed out an error Lorentz had made when he had applied his transformation to one of Maxwell's equations, that for charge-occupied space, and also questioned the time dilation factor given by Lorentz.[33] In a second letter to Lorentz, Poincar\u00e9 gave his own reason why Lorentz's time dilation factor was indeed correct after all\u2014it was necessary to make the Lorentz transformation form a group\u2014and he gave what is now known as the relativistic velocity-addition law.[34] Poincar\u00e9 later delivered a paper at the meeting of the Academy of Sciences in Paris on 5 June 1905 in which these issues were addressed. In the published version of that he wrote:[35]In case the problem could not be solved, any other important contribution to classical mechanics would then be considered to be prizeworthy. The prize was finally awarded to Poincar\u00e9, even though he did not solve the original problem. One of the judges, the distinguished Karl Weierstrass, said, \"This work cannot indeed be considered as furnishing the complete solution of the question proposed, but that it is nevertheless of such importance that its publication will inaugurate a new era in the history of celestial mechanics.\" (The first version of his contribution even contained a serious error; for details see the article by Diacu[22] and the book by Barrow-Green[23]). The version finally printed[24] contained many important ideas which led to the theory of chaos. The problem as stated originally was finally solved by Karl F. Sundman for n\u00a0=\u00a03 in 1912 and was generalised to the case of n\u00a0>\u00a03 bodies by Qiudong Wang in the 1990s.The problem of finding the general solution to the motion of more than two orbiting bodies in the solar system had eluded mathematicians since Newton's time. This was known originally as the three-body problem and later the n-body problem, where n is any number of more than two orbiting bodies. The n-body solution was considered very important and challenging at the close of the 19th century. Indeed, in 1887, in honour of his 60th birthday, Oscar II, King of Sweden, advised by G\u00f6sta Mittag-Leffler, established a prize for anyone who could find the solution to the problem. The announcement was quite specific:Among the specific topics he contributed to are the following:He was also a populariser of mathematics and physics and wrote several books for the lay public.Poincar\u00e9 made many contributions to different fields of pure and applied mathematics such as: celestial mechanics, fluid mechanics, optics, electricity, telegraphy, capillarity, elasticity, thermodynamics, potential theory, quantum theory, theory of relativity and physical cosmology.A former French Minister of Education, Claude All\u00e8gre, proposed in 2004 that Poincar\u00e9 be reburied in the Panth\u00e9on in Paris, which is reserved for French citizens only of the highest honour.[19]In 1912, Poincar\u00e9 underwent surgery for a prostate problem and subsequently died from an embolism on 17 July 1912, in Paris. He was 58 years of age. He is buried in the Poincar\u00e9 family vault in the Cemetery of Montparnasse, Paris.Poincar\u00e9 had two notable doctoral students at the University of Paris, Louis Bachelier (1900) and Dimitrie Pompeiu (1905).[18]Poincar\u00e9 was the President of the Soci\u00e9t\u00e9 Astronomique de France (SAF), the French astronomical society, from 1901 to 1903.[17]In 1899, and again more successfully in 1904, he intervened in the trials of Alfred Dreyfus. He attacked the spurious scientific claims of some of the evidence brought against Dreyfus, who was a Jewish officer in the French army charged with treason by colleagues.In 1893, Poincar\u00e9 joined the French Bureau des Longitudes, which engaged him in the synchronisation of time around the world. In 1897 Poincar\u00e9 backed an unsuccessful proposal for the decimalisation of circular measure, and hence time and longitude.[16] It was this post which led him to consider the question of establishing international time zones and the synchronisation of time between bodies in relative motion. (See work on relativity section below.)In 1887, he won Oscar II, King of Sweden's mathematical competition for a resolution of the three-body problem concerning the free motion of multiple orbiting bodies. (See three-body problem section below.)In 1887, at the young age of 32, Poincar\u00e9 was elected to the French Academy of Sciences. He became its president in 1906, and was elected to the Acad\u00e9mie fran\u00e7aise on 5 March 1908.Beginning in 1881 and for the rest of his career, he taught at the University of Paris (the Sorbonne). He was initially appointed as the ma\u00eetre de conf\u00e9rences d'analyse (associate professor of analysis).[15] Eventually, he held the chairs of Physical and Experimental Mechanics, Mathematical Physics and Theory of Probability, and Celestial Mechanics and Astronomy.He never fully abandoned his mining career to mathematics. He worked at the Ministry of Public Services as an engineer in charge of northern railway development from 1881 to 1885. He eventually became chief engineer of the Corps de Mines in 1893 and inspector general in 1910.In 1881\u20131882, Poincar\u00e9 created a new branch of mathematics: the qualitative theory of differential equations. He showed how it is possible to derive the most important information about the behavior of a family of solutions without having to solve the equation (since this may not always be possible). He successfully used this approach to problems in celestial mechanics and mathematical physics.Poincar\u00e9 immediately established himself among the greatest mathematicians of Europe, attracting the attention of many prominent mathematicians. In 1881 Poincar\u00e9 was invited to take a teaching position at the Faculty of Sciences of the University of Paris; he accepted the invitation. During the years of 1883 to 1897, he taught mathematical analysis in \u00c9cole Polytechnique.There, in Caen, he met his future wife, Louise Poulin d'Andesi (Louise Poulain d'Andecy) and on 20 April 1881, they married. Together they had four children: Jeanne (born 1887), Yvonne (born 1889), Henriette (born 1891), and L\u00e9on (born 1893).After receiving his degree, Poincar\u00e9 began teaching as junior lecturer in mathematics at the University of Caen in Normandy (in December 1879). At the same time he published his first major article concerning the treatment of a class of automorphic functions.At the same time, Poincar\u00e9 was preparing for his Doctorate in Science in mathematics under the supervision of Charles Hermite. His doctoral thesis was in the field of differential equations. It was named Sur les propri\u00e9t\u00e9s des fonctions d\u00e9finies par les \u00e9quations aux diff\u00e9rences partielles. Poincar\u00e9 devised a new way of studying the properties of these equations. He not only faced the question of determining the integral of such equations, but also was the first person to study their general geometric properties. He realised that they could be used to model the behaviour of multiple bodies in free motion within the solar system. Poincar\u00e9 graduated from the University of Paris in 1879.As a graduate of the \u00c9cole des Mines, he joined the Corps des Mines as an inspector for the Vesoul region in northeast France. He was on the scene of a mining disaster at Magny in August 1879 in which 18 miners died. He carried out the official investigation into the accident in a characteristically thorough and humane way.Poincar\u00e9 entered the \u00c9cole Polytechnique in 1873 and graduated in 1875. There he studied mathematics as a student of Charles Hermite, continuing to excel and publishing his first paper (D\u00e9monstration nouvelle des propri\u00e9t\u00e9s de l'indicatrice d'une surface) in 1874. From November 1875 to June 1878 he studied at the \u00c9cole des Mines, while continuing the study of mathematics in addition to the mining engineering syllabus, and received the degree of ordinary mining engineer in March 1879.[14]During the Franco-Prussian War of 1870, he served alongside his father in the Ambulance Corps.In 1862, Henri entered the Lyc\u00e9e in Nancy (now renamed the Lyc\u00e9e Henri-Poincar\u00e9\u00a0(fr) in his honour, along with Henri Poincar\u00e9 University, also in Nancy). He spent eleven years at the Lyc\u00e9e and during this time he proved to be one of the top students in every topic he studied. He excelled in written composition. His mathematics teacher described him as a \"monster of mathematics\" and he won first prizes in the concours g\u00e9n\u00e9ral, a competition between the top pupils from all the Lyc\u00e9es across France. His poorest subjects were music and physical education, where he was described as \"average at best\".[12] However, poor eyesight and a tendency towards absentmindedness may explain these difficulties.[13] He graduated from the Lyc\u00e9e in 1871 with a bachelor's degree in letters and sciences.During his childhood he was seriously ill for a time with diphtheria and received special instruction from his mother, Eug\u00e9nie Launois (1830\u20131897).Poincar\u00e9 was born on 29 April 1854 in Cit\u00e9 Ducale neighborhood, Nancy, Meurthe-et-Moselle into an influential family.[6] His father Leon Poincar\u00e9 (1828\u20131892) was a professor of medicine at the University of Nancy.[7] His younger sister Aline married the spiritual philosopher Emile Boutroux. Another notable member of Henri's family was his cousin, Raymond Poincar\u00e9, a fellow member of the Acad\u00e9mie fran\u00e7aise, who would serve as President of France from 1913 to 1920.[8] Poincar\u00e9 was raised in the Roman Catholic faith, but later left the religion. He became a freethinker, believing the universe to be sufficient truth and was said to be an atheist.[9][10][11]The Poincar\u00e9 group used in physics and mathematics was named after him.Poincar\u00e9 made clear the importance of paying attention to the invariance of laws of physics under different transformations, and was the first to present the Lorentz transformations in their modern symmetrical form. Poincar\u00e9 discovered the remaining relativistic velocity transformations and recorded them in a letter to Hendrik Lorentz in 1905. Thus he obtained perfect invariance of all of Maxwell's equations, an important step in the formulation of the theory of special relativity. In 1905, Poincar\u00e9 first proposed gravitational waves (ondes gravifiques) emanating from a body and propagating at the speed of light as being required by the Lorentz transformations.As a mathematician and physicist, he made many original fundamental contributions to pure and applied mathematics, mathematical physics, and celestial mechanics.[5] He was responsible for formulating the Poincar\u00e9 conjecture, which was one of the most famous unsolved problems in mathematics until it was solved in 2002\u20132003 by Grigori Perelman. In his research on the three-body problem, Poincar\u00e9 became the first person to discover a chaotic deterministic system which laid the foundations of modern chaos theory. He is also considered to be one of the founders of the field of topology.",
            "title": "Henri Poincar\u00e9",
            "url": "https://en.wikipedia.org/wiki/Henri_Poincar%C3%A9"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Partial_differential_equation",
                "/wiki/Mechanical_engineering",
                "/wiki/Theoretical_physics",
                "/wiki/Laplace%27s_equation",
                "/wiki/France",
                "/wiki/Mathematician",
                "/wiki/Geometer",
                "/wiki/Physicist",
                "/wiki/Sim%C3%A9on_Denis_Poisson"
            ],
            "links": [
                "/wiki/Octree",
                "/wiki/Implicit_function",
                "/wiki/Euclidean_vector",
                "/wiki/Integral",
                "/wiki/Gradient",
                "/wiki/Curl_(mathematics)",
                "/wiki/Least-squares",
                "/wiki/Inverse_problem",
                "/wiki/Point_cloud",
                "/wiki/Surface_normal",
                "/wiki/Electrical_potential",
                "/wiki/Error_function",
                "/wiki/Gaussian_distribution",
                "/wiki/Coulomb_gauge",
                "/wiki/Magnetic_vector_potential",
                "/wiki/Mathematical_descriptions_of_the_electromagnetic_field#Maxwell's_equations_in_potential_formulation",
                "/wiki/Laplace%27s_equation",
                "/wiki/Boltzmann_distribution",
                "/wiki/Poisson-Boltzmann_equation",
                "/wiki/Debye%E2%80%93H%C3%BCckel_equation",
                "/wiki/Curl_(mathematics)",
                "/wiki/Helmholtz_decomposition",
                "/wiki/Curl_(mathematics)",
                "/wiki/Permittivity",
                "/wiki/Electric_field",
                "/wiki/Polarization_density",
                "/wiki/Constitutive_equation#Electromagnetism",
                "/wiki/Gauss%27s_law",
                "/wiki/Maxwell%27s_equations",
                "/wiki/SI",
                "/wiki/Gaussian_units",
                "/wiki/Electromagnetism",
                "/wiki/Newton%27s_law_of_universal_gravitation",
                "/wiki/Fundamental_solution",
                "/wiki/Irrotational",
                "/wiki/Green%27s_function",
                "/wiki/Screened_Poisson_equation",
                "/wiki/Relaxation_method",
                "/wiki/Cartesian_coordinate",
                "/wiki/Mathematics",
                "/wiki/Partial_differential_equation",
                "/wiki/Mechanical_engineering",
                "/wiki/Theoretical_physics",
                "/wiki/Laplace%27s_equation",
                "/wiki/France",
                "/wiki/Mathematician",
                "/wiki/Geometer",
                "/wiki/Physicist",
                "/wiki/Sim%C3%A9on_Denis_Poisson"
            ],
            "text": "In order to effectively apply Poisson's equation to the problem of surface reconstruction, it is necessary to find a good discretization of the vector field V. The basic approach is to bound the data with a finite difference grid. For a function valued at the nodes of such a grid, its gradient can be represented as valued on staggered grids, i.e. on grids whose nodes lie in between the nodes of the original grid. It is convenient to define three staggered grids, each shifted in one and only one direction corresponding to the components of the normal data. On each staggered grid we perform [trilinear interpolation] on the set of points. The interpolation weights are then used to distribute the magnitude of the associated component of ni onto the nodes of the particular staggered grid cell containing pi. In (Kazhdan et al., 2006),[4] the authors give a more accurate method of discretization using an adaptive finite difference grid, i.e. the cells of the grid are smaller (the grid is more finely divided) where there are more data points. They suggest implementing this technique with an adaptive octree.The goal of this technique is to reconstruct an implicit function f whose value is zero at the points pi and whose gradient at the points pi equals the normal vectors ni. The set of (pi, ni) is thus modeled as a continuous vector field V. The implicit function f is found by integrating the vector field V. Since not every vector field is the gradient of a function, the problem may or may not have a solution: the necessary and sufficient condition for a smooth vector field V to be the gradient of a function f is that the curl of V must be identically zero. In case this condition is difficult to impose, it is still possible to perform a least-squares fit to minimize the difference between V and the gradient of f.Surface reconstruction is an inverse problem. The goal is to digitally reconstruct a smooth surface based on a large number of points pi (a point cloud) where each point also carries an estimate of the local surface normal ni.[2] Poisson's equation can be utilized to solve this problem with a technique called Poisson Surface Reconstruction first published in (Kazhdan et al., 2006)[3]as one would expect. Furthermore, the erf function approaches 1 extremely quickly as its argument increases; in practice for r > 3\u03c3 the relative error is smaller than one part in a thousand.Note that, for r much greater than \u03c3, the erf function approaches unity and the potential \u03c6(r) approaches the point charge potentialThis solution can be checked explicitly by evaluating \u22072\u03c6.where erf(x) is the error function.is given bywhere Q is the total charge, then the solution \u03c6(r) of Poisson's equation,If there is a static spherically symmetric Gaussian charge densityThe above discussion assumes that the magnetic field is not varying in time. The same Poisson equation arises even if it does vary in time, as long as the Coulomb gauge is used. In this more general context, computing \u03c6 is no longer sufficient to calculate E, since E also depends on the magnetic vector potential A, which must be independently computed. See Maxwell's equation in potential formulation for more on \u03c6 and A in Maxwell's equations and how Poisson's equation is obtained in this case.Using Green's Function, the potential at distance r from a central point charge Q (i.e.: the Fundamental Solution) is:Solving Poisson's equation for the potential requires knowing the charge density distribution. If the charge density is zero, then Laplace's equation results. If the charge density follows a Boltzmann distribution, then the Poisson-Boltzmann equation results. The Poisson\u2013Boltzmann equation plays a role in the development of the Debye\u2013H\u00fcckel theory of dilute electrolyte solutions.directly produces Poisson's equation for electrostatics, which isThe derivation of Poisson's equation under these circumstances is straightforward. Substituting the potential gradient for the electric field,Since the curl of the electric field is zero, it is defined by a scalar electric potential field, \u03c6 (see Helmholtz decomposition).where \u2207\u00d7 is the curl operator and t is the time.Substituting this into Gauss's law and assuming \u03b5 is spatially constant in the region of interest yieldswhere \u03b5 = permittivity of the medium and E = electric field.Assuming the medium is linear, isotropic, and homogeneous (see polarization density), we have the constitutive equation,Starting with Gauss's law for electricity (also one of Maxwell's equations) in differential form, one hasThe mathematical details behind Poisson's equation in electrostatics are as follows (SI units are used rather than Gaussian units, which are also frequently used in electromagnetism).which is equivalent to Newton's law of universal gravitation.If the mass density is zero, Poisson's equation reduces to Laplace's equation. Using Green's Function, the potential at distance r from a central point mass m (i.e., the fundamental solution) isyields Poisson's equation for gravity,Substituting into Gauss's lawSince the gravitational field is conservative (and irrotational), it can be expressed in terms of a scalar potential \u03a6,In the case of a gravitational field g due to an attracting massive object of density \u03c1, Gauss's law for gravity in differential form can be used to obtain the corresponding Poisson equation for gravity,Poisson's equation may be solved using a Green's function; a general exposition of the Green's function for Poisson's equation is given in the article on the screened Poisson equation. There are various methods for numerical solution. The relaxation method, an iterative algorithm, is one example.In three-dimensional Cartesian coordinates, it takes the formPoisson's equation isIn mathematics, Poisson's equation is a partial differential equation of elliptic type with broad utility in mechanical engineering and theoretical physics. It arises, for instance, to describe the potential field caused by a given charge or mass density distribution; with the potential field known, one can then calculate gravitational or electrostatic field. It is a generalization of Laplace's equation, which is also frequently seen in physics. The equation is named after the French mathematician, geometer, and physicist Sim\u00e9on Denis Poisson.[1]",
            "title": "Poisson's equation",
            "url": "https://en.wikipedia.org/wiki/Poisson%27s_equation"
        },
        {
            "desc_links": [
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French",
                "/wiki/French_people",
                "/wiki/Mathematician",
                "/wiki/Physicist",
                "/wiki/Auxerre",
                "/wiki/Fourier_series",
                "/wiki/Heat_transfer",
                "/wiki/Vibration",
                "/wiki/Fourier_transform",
                "/wiki/Thermal_conduction#Fourier.27s_law",
                "/wiki/Greenhouse_effect"
            ],
            "links": [
                "/wiki/Horace-B%C3%A9n%C3%A9dict_de_Saussure",
                "/wiki/Convection",
                "/wiki/Greenhouse_effect",
                "/wiki/Claude-Louis_Navier",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Fran%C3%A7ois_Budan_de_Boislaurent",
                "/wiki/Budan%27s_theorem",
                "/wiki/Jacques_Charles_Fran%C3%A7ois_Sturm",
                "/wiki/Dimensional_analysis",
                "/wiki/Heat_equation",
                "/wiki/Sine_wave",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Peter_Gustav_Lejeune_Dirichlet",
                "/wiki/Fourier_transform",
                "/wiki/Heat_flow",
                "/wiki/Newton%27s_law_of_cooling",
                "/wiki/Jean_Gaston_Darboux",
                "/wiki/Joseph_Fourier_University",
                "/wiki/P%C3%A8re_Lachaise_Cemetery",
                "/wiki/The_72_names_on_the_Eiffel_Tower",
                "/wiki/Jean_Baptiste_Joseph_Delambre",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Royal_Swedish_Academy_of_Sciences",
                "/wiki/Description_de_l%27%C3%89gypte",
                "/wiki/Napoleon",
                "/wiki/Prefect_(France)",
                "/wiki/Is%C3%A8re",
                "/wiki/Grenoble",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Napoleon",
                "/wiki/Napoleon_Bonaparte",
                "/wiki/French_campaign_in_Egypt_and_Syria",
                "/wiki/Institut_d%27%C3%89gypte",
                "/wiki/Cairo",
                "/wiki/Abdullah_Jacques-Fran%C3%A7ois_de_Boussay,_baron_de_Menou",
                "/wiki/Auxerre",
                "/wiki/Yonne",
                "/wiki/D%C3%A9partement",
                "/wiki/Tailor",
                "/wiki/Orphaned",
                "/wiki/Bishop_of_Auxerre",
                "/wiki/Benedictine_Order",
                "/wiki/French_Revolution",
                "/wiki/Reign_of_Terror",
                "/wiki/%C3%89cole_Normale_Sup%C3%A9rieure",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French",
                "/wiki/French_people",
                "/wiki/Mathematician",
                "/wiki/Physicist",
                "/wiki/Auxerre",
                "/wiki/Fourier_series",
                "/wiki/Heat_transfer",
                "/wiki/Vibration",
                "/wiki/Fourier_transform",
                "/wiki/Thermal_conduction#Fourier.27s_law",
                "/wiki/Greenhouse_effect"
            ],
            "text": "In his articles, Fourier referred to an experiment by de Saussure, who lined a vase with blackened cork. Into the cork, he inserted several panes of transparent glass, separated by intervals of air. Midday sunlight was allowed to enter at the top of the vase through the glass panes. The temperature became more elevated in the more interior compartments of this device. Fourier concluded that gases in the atmosphere could form a stable barrier like the glass panes.[19] This conclusion may have contributed to the later use of the metaphor of the 'greenhouse effect' to refer to the processes that determine atmospheric temperatures.[20] Fourier noted that the actual mechanisms that determine the temperatures of the atmosphere included convection, which was not present in de Saussure's experimental device.In the 1820s Fourier calculated that an object the size of the Earth, and at its distance from the Sun, should be considerably colder than the planet actually is if warmed by only the effects of incoming solar radiation. He examined various possible sources of the additional observed heat in articles published in 1824[14] and 1827.[15] While he ultimately suggested that interstellar radiation might be responsible for a large portion of the additional warmth, Fourier's consideration of the possibility that the Earth's atmosphere might act as an insulator of some kind is widely recognized as the first proposal of what is now known as the greenhouse effect,[16] although Fourier never called it that.[17][18]Fourier left an unfinished work on determinate equations which was edited by Claude-Louis Navier and published in 1831. This work contains much original matter \u2014 in particular, there is a demonstration of Fourier's theorem on the position of the roots of an algebraic equation. Joseph-Louis Lagrange had shown how the roots of an algebraic equation might be separated by means of another equation whose roots were the squares of the differences of the roots of the original equation. Fran\u00e7ois Budan, in 1807 and 1811, had enunciated the theorem generally known by the name of Fourier, but the demonstration was not altogether satisfactory. Fourier's proof[13] is the same as that usually given in textbooks on the theory of equations. The final solution of the problem was given in 1829 by Jacques Charles Fran\u00e7ois Sturm.One important physical contribution in the book was the concept of dimensional homogeneity in equations; i.e. an equation can be formally correct only if the dimensions match on either side of the equality; Fourier made important contributions to dimensional analysis.[12] The other physical contribution was Fourier's proposal of his partial differential equation for conductive diffusion of heat. This equation is now taught to every student of mathematical physics.There were three important contributions in this work, one purely mathematical, two essentially physical. In mathematics, Fourier claimed that any function of a variable, whether continuous or discontinuous, can be expanded in a series of sines of multiples of the variable. Though this result is not correct without additional conditions, Fourier's observation that some discontinuous functions are the sum of infinite series was a breakthrough. The question of determining when a Fourier series converges has been fundamental for centuries. Joseph-Louis Lagrange had given particular cases of this (false) theorem, and had implied that the method was general, but he had not pursued the subject. Peter Gustav Lejeune Dirichlet was the first to give a satisfactory demonstration of it with some restrictive conditions. This work provides the foundation for what is today known as the Fourier transform.In 1822 Fourier published his work on heat flow in Th\u00e9orie analytique de la chaleur (The Analytical Theory of Heat),[8] in which he based his reasoning on Newton's law of cooling, namely, that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures. This book was translated,[9] with editorial 'corrections',[10] into English 56 years later by Freeman (1878).[11] The book was also edited, with many editorial corrections, by Darboux and republished in French in 1888.[10]A bronze statue was erected in Auxerre in 1849, but it was melted down for armaments during World War II.[7] Joseph Fourier University in Grenoble is named after him.Fourier was buried in the P\u00e8re Lachaise Cemetery in Paris, a tomb decorated with an Egyptian motif to reflect his position as secretary of the Cairo Institute, and his collation of Description de l'\u00c9gypte. His name is one of the 72 names inscribed on the Eiffel Tower.Shortly after this event, he died in his bed on 16 May 1830.In 1830, his diminished health began to take its toll:In 1822, Fourier succeeded Jean Baptiste Joseph Delambre as Permanent Secretary of the French Academy of Sciences. In 1830, he was elected a foreign member of the Royal Swedish Academy of Sciences.Hence being faithful to Napoleon, he took the office of Prefect.[4] It was while at Grenoble that he began to experiment on the propagation of heat. He presented his paper On the Propagation of Heat in Solid Bodies to the Paris Institute on December 21, 1807. He also contributed to the monumental Description de l'\u00c9gypte.[5]In 1801,[4] Napoleon appointed Fourier Prefect (Governor) of the Department of Is\u00e8re in Grenoble, where he oversaw road construction and other projects. However, Fourier had previously returned home from the Napoleon expedition to Egypt to resume his academic post as professor at \u00c9cole Polytechnique when Napoleon decided otherwise in his remarkFourier accompanied Napoleon Bonaparte on his Egyptian expedition in 1798, as scientific adviser, and was appointed secretary of the Institut d'\u00c9gypte. Cut off from France by the British fleet, he organized the workshops on which the French army had to rely for their munitions of war. He also contributed several mathematical papers to the Egyptian Institute (also called the Cairo Institute) which Napoleon founded at Cairo, with a view of weakening British influence in the East. After the British victories and the capitulation of the French under General Menou in 1801, Fourier returned to France.Fourier was born at Auxerre (now in the Yonne d\u00e9partement of France), the son of a tailor. He was orphaned at age nine. Fourier was recommended to the Bishop of Auxerre and, through this introduction, he was educated by the Benedictine Order of the Convent of St. Mark. The commissions in the scientific corps of the army were reserved for those of good birth, and being thus ineligible, he accepted a military lectureship on mathematics. He took a prominent part in his own district in promoting the French Revolution, serving on the local Revolutionary Committee. He was imprisoned briefly during the Terror but, in 1795, was appointed to the \u00c9cole Normale and subsequently succeeded Joseph-Louis Lagrange at the \u00c9cole Polytechnique.Jean-Baptiste Joseph Fourier (/\u02c8f\u028a\u0259ri\u02cce\u026a, -i\u0259r/;[1] French:\u00a0[fu\u0281je]; 21 March 1768 \u2013 16 May 1830) was a French mathematician and physicist born in Auxerre and best known for initiating the investigation of Fourier series and their applications to problems of heat transfer and vibrations. The Fourier transform and Fourier's law are also named in his honour. Fourier is also generally credited with the discovery of the greenhouse effect.[2]",
            "title": "Joseph Fourier",
            "url": "https://en.wikipedia.org/wiki/Joseph_Fourier"
        },
        {
            "desc_links": [
                "/wiki/Parabolic_partial_differential_equation",
                "/wiki/Heat",
                "/wiki/Temperature"
            ],
            "links": [
                "/wiki/Manifold",
                "/wiki/Atiyah%E2%80%93Singer_index_theorem",
                "/wiki/Riemannian_geometry",
                "/wiki/Mathematical_model",
                "/wiki/Financial_mathematics",
                "/wiki/Option_(finance)",
                "/wiki/Black%E2%80%93Scholes",
                "/wiki/Differential_equation",
                "/wiki/Dirichlet_boundary_conditions",
                "/wiki/Neumann_boundary_conditions",
                "/wiki/Robin_boundary_condition",
                "/wiki/Scale_space",
                "/wiki/Graph_Laplacian",
                "/wiki/Crank%E2%80%93Nicolson_method",
                "/wiki/Fourier_theory",
                "/wiki/Thermal_diffusivity",
                "/wiki/Polymers",
                "/wiki/F._J._Duarte",
                "/wiki/Wave_function",
                "/wiki/Schr%C3%B6dinger%27s_equation",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Wave_function",
                "/wiki/Wave_function",
                "/wiki/Imaginary_unit",
                "/wiki/Reduced_Planck%27s_constant",
                "/wiki/Wave_function",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Mass",
                "/wiki/Diffusion_equation",
                "/wiki/Diffusion",
                "/wiki/Laplace%27s_equation",
                "/wiki/Electrostatics",
                "/wiki/Thermodynamic_temperature",
                "/wiki/Thermal_conductivity",
                "/wiki/Poisson%27s_equation",
                "/wiki/Harmonic_functions#The_mean_value_property",
                "/wiki/Harmonic_function",
                "/wiki/Linear_combination",
                "/wiki/Dirac%27s_delta_function",
                "/wiki/Mollifier",
                "/wiki/Convolution",
                "/wiki/Even_function",
                "/wiki/Green%27s_function_number",
                "/wiki/Odd_function",
                "/wiki/Green%27s_function_number",
                "/wiki/Mollifier",
                "/wiki/Green%27s_function_number",
                "/wiki/Convolution",
                "/wiki/Neumann_problem",
                "/wiki/Dirichlet_problem",
                "/wiki/Dirichlet_problem",
                "/wiki/Neumann_problem",
                "/wiki/Green%27s_function",
                "/wiki/Method_of_images",
                "/wiki/Separation_of_variables",
                "/wiki/Laplace_transforms",
                "/wiki/Convolution",
                "/wiki/Dirac_delta_function",
                "/wiki/Green%27s_function",
                "/wiki/Fundamental_solution",
                "/wiki/Heat_kernel",
                "/wiki/Energy",
                "/wiki/Diagonal_matrix",
                "/wiki/Orthonormal",
                "/wiki/Inner_product",
                "/wiki/Linear_operator",
                "/wiki/Eigenvector",
                "/wiki/Spectral_theory",
                "/wiki/Self-adjoint_operator",
                "/wiki/Separation_of_variables",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_flux",
                "/wiki/Special_relativity",
                "/wiki/Light_cone",
                "/wiki/Hyperbolic_partial_differential_equation",
                "/wiki/Particle_diffusion",
                "/wiki/Action_potential",
                "/wiki/Finance",
                "/wiki/Black%E2%80%93Scholes",
                "/wiki/Ornstein-Uhlenbeck_process",
                "/wiki/Laplace_operator",
                "/wiki/Parabolic_partial_differential_equation",
                "/wiki/Heat",
                "/wiki/Thermodynamic_equilibrium",
                "/wiki/Boundary_condition",
                "/wiki/Heat_conduction",
                "/wiki/Wave_propagation",
                "/wiki/Isotropic",
                "/wiki/Dimension",
                "/wiki/Thermal_diffusivity",
                "/wiki/Fundamental_lemma_of_the_calculus_of_variations",
                "/wiki/Conservation_of_energy",
                "/wiki/Fundamental_theorem_of_calculus",
                "/wiki/Specific_heat_capacity",
                "/wiki/Difference_operator",
                "/wiki/Thermal_conductivity",
                "/wiki/Conduction_(heat)#Fourier's_law",
                "/wiki/Conservation_of_energy",
                "/wiki/Riemannian_geometry",
                "/wiki/Topology",
                "/wiki/Richard_S._Hamilton",
                "/wiki/Ricci_flow",
                "/wiki/Grigori_Perelman",
                "/wiki/Poincar%C3%A9_conjecture",
                "/wiki/Random_walk",
                "/wiki/Financial_mathematics",
                "/wiki/Maximum_principle",
                "/wiki/Parabolic_partial_differential_equation",
                "/wiki/Mathematics",
                "/wiki/Parabolic_partial_differential_equation",
                "/wiki/Probability_theory",
                "/wiki/Brownian_motion",
                "/wiki/Fokker%E2%80%93Planck_equation",
                "/wiki/Financial_mathematics",
                "/wiki/Black%E2%80%93Scholes",
                "/wiki/Diffusion_equation",
                "/wiki/Laplace_operator",
                "/wiki/Thermal_diffusivity",
                "/wiki/Coordinate_system",
                "/wiki/Function_(mathematics)",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Time",
                "/wiki/Parabolic_partial_differential_equation",
                "/wiki/Heat",
                "/wiki/Temperature"
            ],
            "text": "An abstract form of heat equation on manifolds provides a major approach to the Atiyah\u2013Singer index theorem, and has led to much further work on heat equations in Riemannian geometry.The heat equation arises in the modeling of a number of phenomena and is often used in financial mathematics in the modeling of options. The famous Black\u2013Scholes option pricing model's differential equation can be transformed into the heat equation allowing relatively easy solutions from a familiar body of mathematics. Many of the extensions to the simple option models do not have closed form solutions and thus must be solved numerically to obtain a modeled option price. The equation describing pressure diffusion in a porous medium is identical in form with the heat equation. Diffusion problems dealing with Dirichlet, Neumann and Robin boundary conditions have closed form analytic solutions (Thambynayagam 2011). The heat equation is also widely used in image analysis (Perona & Malik 1990) and in machine-learning as the driving theory behind scale-space or graph Laplacian methods. The heat equation can be efficiently solved numerically using the implicit Crank\u2013Nicolson method of (Crank & Nicolson 1947). This method can be extended to many of the models with no closed form solution, see for instance (Wilmott, Howison & Dewynne 1995).where T0 is the initial temperature of the sphere and TS the temperature at the surface of the sphere, of radius L. This equation has also found applications in protein energy transfer and thermal modeling in biophysics.A direct practical application of the heat equation, in conjunction with Fourier theory, in spherical coordinates, is the prediction of thermal transfer profiles and the measurement of the thermal diffusivity in polymers (Unsworth and Duarte). This dual theoretical-experimental method is applicable to rubber, various other polymeric materials of practical interest, and microfluids. These authors derived an expression for the temperature at the center of a sphere TCRemark: this analogy between quantum mechanics and diffusion is a purely formal one. Physically, the evolution of the wave function satisfying Schr\u00f6dinger's equation might have an origin other than diffusion.withApplying this transformation to the expressions of the Green functions determined in the case of particle diffusion yields the Green functions of the Schr\u00f6dinger equation, which in turn can be used to obtain the wave function at any time through an integral on the wave function at t = 0:This equation is formally similar to the particle diffusion equation, which one obtains through the following transformation:where i is the imaginary unit, \u0127 is the reduced Planck's constant, and \u03c8 is the wave function of the particle.With a simple division, the Schr\u00f6dinger equation for a single particle of mass m in the absence of any applied force field can be rewritten in the following way:which is the solution of the initial value problemBoth c and P are functions of position and time. D is the diffusion coefficient that controls the speed of the diffusive process, and is typically expressed in meters squared over second. If the diffusion coefficient D is not constant, but depends on the concentration c (or P in the second case), then one gets the nonlinear diffusion equation.orIn either case, one uses the heat equationOne can model particle diffusion by an equation involving either:The steady-state heat equation without a heat source within the volume (the homogeneous case) is the equation in electrostatics for a volume of free space that does not contain a charge. It is described by Laplace's equation:In electrostatics, this is equivalent to the case where the space under consideration contains an electrical charge.where u is the temperature, k is the thermal conductivity and q the heat-flux density of the source.The steady-state heat equation for a volume that contains a heat source (the inhomogeneous case), is the Poisson's equation:Steady-state condition:The equation is much simpler and can help to understand better the physics of the materials without focusing on the dynamic of the heat transport process. It is widely used for simple engineering problems assuming there is equilibrium of the temperature fields and heat transport, with time.In the steady-state case, a spatial thermal gradient may (or may not) exist, but if it does, it does not change in time. This equation therefore describes the end result in all thermal problems in which a source is switched on (for example, an engine started in an automobile), and enough time has passed for all permanent temperature gradients to establish themselves in space, after which these spatial gradients no longer change in time (as again, with an automobile in which the engine has been running for long enough). The other (trivial) solution is for all spatial temperature gradients to disappear as well, in which case the temperature become uniform in space, as well.This condition depends on the time constant and the amount of time passed since boundary conditions have been imposed. Thus, the condition is fulfilled in situations in which the time equilibrium constant is fast enough that the more complex time-dependent heat equation can be approximated by the steady-state case. Equivalently, the steady-state condition exists for all cases in which enough time has passed that the thermal field u no longer evolves in time.The steady-state heat equation is by definition not dependent on time. In other words, it is assumed conditions exist such that:as \u03bb \u2192 \u221e so the above formula holds for any (x, t) in the (open) set dom(u) for \u03bb large enough. [8]. This can be shown by an argument similar to the analogous one for harmonic functions.Notice thatwhere E\u03bb is a \"heat-ball\", that is a super-level set of the fundamental solution of the heat equation:thenandthough a bit more complicated. Precisely, if u solvessatisfy a mean-value property analogous to the mean-value properties of harmonic functions, solutions ofSolutions of the heat equationslet u = w + v + r where w, v, and r solve the problemsSimilarly, to solvelet u = w + v where w and v solve the problemsFor example, to solveSince the heat equation is linear, solutions of other combinations of boundary conditions, inhomogeneous term, and initial conditions can be found by taking an appropriate linear combination of the above Green's function solutions.Comment. This solution is obtained from the first formula as applied to the data f(x, t) suitably extended to R \u00d7 [0,\u221e), so as to be an even function of the variable x, that is, letting f(\u2212x, t)\u00a0:= f(x, t) for all x and t. Correspondingly, the solution of the inhomogeneous problem on (\u2212\u221e,\u221e) is an even function with respect to the variable x for all values of t, and in particular, being a smooth function, it satisfies the homogeneous Neumann boundary conditions ux(0, t) = 0.Comment. This solution is obtained from the preceding formula as applied to the data f(x, t) suitably extended to R \u00d7 [0,\u221e), so as to be an odd function of the variable x, that is, letting f(\u2212x, t)\u00a0:= \u2212f(x, t) for all x and t. Correspondingly, the solution of the inhomogeneous problem on (\u2212\u221e,\u221e) is an odd function with respect to the variable x for all values of t, and in particular it satisfies the homogeneous Dirichlet boundary conditions u(0, t) = 0.where the distribution \u03b4 is the Dirac's delta function, that is the evaluation at 0.which expressed in the language of distributions becomesand the function f(x, t), both meant as defined on the whole R2 and identically 0 for all t \u2192 0. One verifies thatComment. This solution is the convolution in R2, that is with respect to both the variables x and t, of the fundamental solutionso that, by general facts about approximation to the identity, \u03c8(x, \u22c5) \u2217 h \u2192 h as x \u2192 0 in various senses, according to the specific h. For instance, if h is assumed continuous on R with support in [0, \u221e) then \u03c8(x, \u22c5) \u2217 h converges uniformly on compacta to h as x \u2192 0, meaning that u(x, t) is continuous on [0, \u221e) \u00d7 [0, \u221e) with u(0, t) = h(t).the function \u03c8(x, t) is also a solution of the same heat equation, and so is u\u00a0:= \u03c8 \u2217 h, thanks to general properties of the convolution with respect to differentiation. Moreover,and the function h(t). Since \u03a6(x, t) is the fundamental solution ofComment. This solution is the convolution with respect to the variable t ofComment. This solution is obtained from the first solution formula as applied to the data g(x) suitably extended to R so as to be an even function, that is, letting g(\u2212x)\u00a0:= g(x) for all x. Correspondingly, the solution of the initial value problem on R is an even function with respect to the variable x for all values of t > 0, and in particular, being smooth, it satisfies the homogeneous Neumann boundary conditions ux(0, t) = 0. The Green's function number of this solution is X20.Comment. This solution is obtained from the preceding formula as applied to the data g(x) suitably extended to R, so as to be an odd function, that is, letting g(\u2212x)\u00a0:= \u2212g(x) for all x. Correspondingly, the solution of the initial value problem on (\u2212\u221e,\u221e) is an odd function with respect to the variable x for all values of t, and in particular it satisfies the homogeneous Dirichlet boundary conditions u(0, t) = 0. The Green's function number of this solution is X10.so that, by general facts about approximation to the identity, \u03a6(\u22c5, t) \u2217 g \u2192 g as t \u2192 0 in various senses, according to the specific g. For instance, if g is assumed bounded and continuous on R then \u03a6(\u22c5, t) \u2217 g converges uniformly to g as t \u2192 0, meaning that u(x, t) is continuous on R \u00d7 [0, \u221e) with u(x, 0) = g(x).Moreover,\nTherefore, according to the general properties of the convolution with respect to differentiation, u = g \u2217 \u03a6 is a solution of the same heat equation, forand the function g(x). (The Green's function number of the fundamental solution is X00.)Comment. This solution is the convolution with respect to the variable x of the fundamental solutionwhere f is some given function of x and t.A variety of elementary Green's function solutions in one-dimension are recorded here; many others are available elsewhere.[7] In some of these, the spatial domain is (\u2212\u221e,\u221e). In others, it is the semi-infinite interval (0,\u221e) with either Neumann or Dirichlet boundary conditions. One further variation is that some of these solve the inhomogeneous equationwith either Dirichlet or Neumann boundary data. A Green's function always exists, but unless the domain \u03a9 can be readily decomposed into one-variable problems (see below), it may not be possible to write it down explicitly. Other methods for obtaining Green's functions include the method of images, separation of variables, and Laplace transforms (Cole, 2011).The general problem on a domain \u03a9 in Rn isThe general solution of the heat equation on Rn is then obtained by a convolution, so that to solve the initial value problem with u(x, 0) = g(x), one hasThe n-variable fundamental solution is the product of the fundamental solutions in each variable; i.e.,In several spatial variables, the fundamental solution solves the analogous problemOne can obtain the general solution of the one variable heat equation with initial condition u(x, 0) = g(x) for \u2212\u221e < x < \u221e and 0 < t < \u221e by applying a convolution:where \u03b4 is the Dirac delta function. The solution to this problem is the fundamental solutionIn one variable, the Green's function is a solution of the initial value problemA fundamental solution, also called a heat kernel, is a solution of the heat equation corresponding to the initial condition of an initial point source of heat at a known position. These can be used to find a general solution of the heat equation over certain domains; see, for instance, (Evans 1998) for an introductory treatment.Remarks.Putting these equations together gives the general equation of heat flow:In general, the study of heat conduction is based on several principles. Heat flow is a form of energy flow, and as such it is meaningful to speak of the time rate of flow of heat into a region of space.Finally, the sequence {en}n \u2208 N spans a dense linear subspace of L2((0, L)). This shows that in effect we have diagonalized the operator \u0394.Moreover, any eigenvector f of \u0394 with the boundary conditions f(0) = f(L) = 0 is of the form en for some n \u2265 1. The functions en for n \u2265 1 form an orthonormal sequence with respect to a certain inner product on the space of real-valued functions on [0, L]. This meansfor n \u2265 1 are eigenvectors of \u0394. Indeed,Consider the linear operator \u0394u = uxx. The infinite sequence of functionsThe solution technique used above can be greatly extended to many other types of equations. The idea is that the operator uxx with the zero boundary conditions can be represented in terms of its eigenvectors. This leads naturally to one of the basic ideas of the spectral theory of linear self-adjoint operators.whereIn general, the sum of solutions to (1) that satisfy the boundary conditions (3) also satisfies (1) and (3). We can show that the solution to (1), (2) and (3) is given byThis solves the heat equation in the special case that the dependence of u has the special form (4).We will now show that nontrivial solutions for (6) for values of \u03bb \u2264 0 cannot occur:andSince the right hand side depends only on x and the left hand side only on t, both sides are equal to some constant value \u2212\u03bb. Thus:This solution technique is called separation of variables. Substituting u back into equation (1),Let us attempt to find a solution of (1) that is not identically zero satisfying the boundary conditions (3) but with the following property: u is a product in which the dependence of u on x, t is separated, that is:where the function f is given, and the boundary conditionsWe assume the initial conditionwhere u = u(x, t) is a function of two variables x and t. HereThe following solution technique for the heat equation was proposed by Joseph Fourier in his treatise Th\u00e9orie analytique de la chaleur, published in 1822. Let us consider the heat equation for one space variable. This could be used to model heat conduction in a rod. The equation isFor example, a tungsten light bulb filament generates heat, so it would have a positive nonzero value for q when turned on. While the light is turned off, the value of q for the tungsten filament would be zero.Suppose that a body obeys the heat equation and, in addition, generates its own heat per unit volume (e.g., in watts/litre - W/L) at a rate given by a known function q varying in space and time.[6] Then the heat per unit volume u satisfies an equationThe function u above represents temperature of a body. Alternatively, it is sometimes convenient to change units and represent u as the heat density of a medium. Since heat density is proportional to temperature in a homogeneous medium, the heat equation is still obeyed in the new units.The heat equation is, technically, in violation of special relativity, because its solutions involve instantaneous propagation of a disturbance. The part of the disturbance outside the forward light cone can usually be safely neglected, but if it is necessary to develop a reasonable speed for the transmission of heat, a hyperbolic problem should be considered instead \u2013 like a partial differential equation involving a second-order time derivative. Some models of nonlinear heat conduction (which are also parabolic equations) have solutions with finite heat transmission speed.[4][5]The heat equation governs heat diffusion, as well as other diffusive processes, such as particle diffusion or the propagation of action potential in nerve cells. Although they are not diffusive in nature, some quantum mechanics problems are also governed by a mathematical analog of the heat equation (see below). It also can be used to model some phenomena arising in finance, like the Black\u2013Scholes or the Ornstein-Uhlenbeck processes. The equation, and various non-linear analogues, has also been used in image analysis.where the Laplace operator, \u0394 or \u22072, the divergence of the gradient, is taken in the spatial variables.Using the Laplace operator, the heat equation can be simplified, and generalized to similar equations over spaces of arbitrary number of dimensions, asThe heat equation is the prototypical example of a parabolic partial differential equation.Solutions of the heat equation are characterized by a gradual smoothing of the initial temperature distribution by the flow of heat from warmer to colder areas of an object. Generally, many different states and starting conditions will tend toward the same stable equilibrium. As a consequence, to reverse the solution and conclude something about earlier times or initial conditions from the present heat distribution is very inaccurate except over the shortest of time periods.If the medium is not the whole space, in order to solve the heat equation uniquely we also need to specify boundary conditions for u. To determine uniqueness of solutions in the whole space it is necessary to assume an exponential bound on the growth of solutions.[3]The heat equation is a consequence of Fourier's law of conduction (see heat conduction).where:In the special cases of wave propagation of heat in an isotropic and homogeneous medium in a 3-dimensional space, this equation isAn additional term may be introduced into the equation to account for radiative loss of heat, which depends upon the excess temperature u = T \u2212 Ts at a given point compared with the surroundings. At low excess temperatures, the radiative loss is approximately \u03bcu, giving a one-dimensional heat-transfer equation of the formis called the thermal diffusivity.which is the heat equation, where the coefficient (often denoted \u03b1)or:Which can be rewritten as:This is true for any rectangle [t\u00a0\u2212\u0394t, t\u00a0+\u00a0\u0394t]\u00a0\u00d7\u00a0[x\u00a0\u2212\u00a0\u0394x, x\u00a0+\u00a0\u0394x]. By the fundamental lemma of the calculus of variations, the integrand must vanish identically:again by the fundamental theorem of calculus.[2] By conservation of energy,where the fundamental theorem of calculus was used. If no work is done and there are neither heat sources nor sinks, the change in internal energy in the interval [x\u2212\u0394x, x+\u0394x] is accounted for entirely by the flux of heat across the boundaries. By Fourier's law, this isis given by[1]over the time periodThe increase in internal energy in a small spatial region of the materialwhere cp is the specific heat capacity and \u03c1 is the mass density of the material. Choosing zero energy at absolute zero temperature, this can be rewritten asIn the absence of work done, a change in internal energy per unit volume in the material, \u0394Q, is proportional to the change in temperature, \u0394u (in this section only, \u0394 is the ordinary difference operator with respect to time, not the Laplacian with respect to space). That is,where k is the thermal conductivity and u is the temperature. In one dimension, the gradient is an ordinary spatial derivative, and so Fourier's law isThe heat equation is derived from Fourier's law and conservation of energy (Cannon 1984). By Fourier's law, the rate of flow of heat energy per unit area through a surface is proportional to the negative temperature gradient across the surface,It is also important in Riemannian geometry and thus topology: it was adapted by Richard S. Hamilton when he defined the Ricci flow that was later used by Grigori Perelman to solve the topological Poincar\u00e9 conjecture.The heat equation is used in probability and describes random walks. It is also applied in financial mathematics for this reason.Another interesting property is that even if u has a discontinuity at an initial time t\u00a0=\u00a0t0, the temperature becomes smooth as soon as t\u00a0>\u00a0t0. For example, if a bar of metal has temperature 0 and another has temperature 100 and they are stuck together end to end, then very quickly the temperature at the point of connection will become 50 and the graph of the temperature will run smoothly from 0 to 50.The image to the right is animated and describes the way heat changes in time along a metal bar. One of the interesting properties of the heat equation is the maximum principle that says that the maximum value of u is either earlier in time than the region of concern or on the edge of the region of concern. This is essentially saying that temperature comes either from some source or from earlier in time because heat permeates but is not created from nothingness. This is a property of parabolic partial differential equations and is not difficult to prove mathematically (see below).Suppose one has a function u that describes the temperature at a given location (x, y, z). This function will change over time as heat spreads throughout space. The heat equation is used to determine the change in the function u over time. The rate of change of u is proportional to the \"curvature\" of u. Thus, the sharper the corner, the faster it is rounded off. Over time, the tendency is for peaks to be eroded, and valleys filled in. If u is linear in space (or has a constant gradient) at a given point, then u has reached steady-state and is unchanging at this point (assuming a constant thermal conductivity).The heat equation is of fundamental importance in diverse scientific fields. In mathematics, it is the prototypical parabolic partial differential equation. In probability theory, the heat equation is connected with the study of Brownian motion via the Fokker\u2013Planck equation. In financial mathematics it is used to solve the Black\u2013Scholes partial differential equation. The diffusion equation, a more general version of the heat equation, arises in connection with the study of chemical diffusion and other related processes.where \u03b1 is a positive constant, and \u0394 or \u22072 denotes the Laplace operator. In the physical problem of temperature variation, u(x,y,z,t) is the temperature and \u03b1 is the thermal diffusivity. For the mathematical treatment it is sufficient to consider the case \u03b1\u00a0=\u00a01.More generally in any coordinate system:For a function u(x,y,z,t) of three spatial variables (x,y,z) (see Cartesian coordinate system) and the time variable t, the heat equation isThe heat equation is a parabolic partial differential equation that describes the distribution of heat (or variation in temperature) in a given region over time.",
            "title": "Heat equation",
            "url": "https://en.wikipedia.org/wiki/Heat_equation"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Separation of variables",
            "url": "https://en.wikipedia.org/wiki/Separation_of_variables"
        },
        {
            "desc_links": [
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French",
                "/wiki/French_people",
                "/wiki/Mathematician",
                "/wiki/Physicist",
                "/wiki/Auxerre",
                "/wiki/Fourier_series",
                "/wiki/Heat_transfer",
                "/wiki/Vibration",
                "/wiki/Fourier_transform",
                "/wiki/Thermal_conduction#Fourier.27s_law",
                "/wiki/Greenhouse_effect"
            ],
            "links": [
                "/wiki/Horace-B%C3%A9n%C3%A9dict_de_Saussure",
                "/wiki/Convection",
                "/wiki/Greenhouse_effect",
                "/wiki/Claude-Louis_Navier",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Fran%C3%A7ois_Budan_de_Boislaurent",
                "/wiki/Budan%27s_theorem",
                "/wiki/Jacques_Charles_Fran%C3%A7ois_Sturm",
                "/wiki/Dimensional_analysis",
                "/wiki/Heat_equation",
                "/wiki/Sine_wave",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Peter_Gustav_Lejeune_Dirichlet",
                "/wiki/Fourier_transform",
                "/wiki/Heat_flow",
                "/wiki/Newton%27s_law_of_cooling",
                "/wiki/Jean_Gaston_Darboux",
                "/wiki/Joseph_Fourier_University",
                "/wiki/P%C3%A8re_Lachaise_Cemetery",
                "/wiki/The_72_names_on_the_Eiffel_Tower",
                "/wiki/Jean_Baptiste_Joseph_Delambre",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Royal_Swedish_Academy_of_Sciences",
                "/wiki/Description_de_l%27%C3%89gypte",
                "/wiki/Napoleon",
                "/wiki/Prefect_(France)",
                "/wiki/Is%C3%A8re",
                "/wiki/Grenoble",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Napoleon",
                "/wiki/Napoleon_Bonaparte",
                "/wiki/French_campaign_in_Egypt_and_Syria",
                "/wiki/Institut_d%27%C3%89gypte",
                "/wiki/Cairo",
                "/wiki/Abdullah_Jacques-Fran%C3%A7ois_de_Boussay,_baron_de_Menou",
                "/wiki/Auxerre",
                "/wiki/Yonne",
                "/wiki/D%C3%A9partement",
                "/wiki/Tailor",
                "/wiki/Orphaned",
                "/wiki/Bishop_of_Auxerre",
                "/wiki/Benedictine_Order",
                "/wiki/French_Revolution",
                "/wiki/Reign_of_Terror",
                "/wiki/%C3%89cole_Normale_Sup%C3%A9rieure",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French",
                "/wiki/French_people",
                "/wiki/Mathematician",
                "/wiki/Physicist",
                "/wiki/Auxerre",
                "/wiki/Fourier_series",
                "/wiki/Heat_transfer",
                "/wiki/Vibration",
                "/wiki/Fourier_transform",
                "/wiki/Thermal_conduction#Fourier.27s_law",
                "/wiki/Greenhouse_effect"
            ],
            "text": "In his articles, Fourier referred to an experiment by de Saussure, who lined a vase with blackened cork. Into the cork, he inserted several panes of transparent glass, separated by intervals of air. Midday sunlight was allowed to enter at the top of the vase through the glass panes. The temperature became more elevated in the more interior compartments of this device. Fourier concluded that gases in the atmosphere could form a stable barrier like the glass panes.[19] This conclusion may have contributed to the later use of the metaphor of the 'greenhouse effect' to refer to the processes that determine atmospheric temperatures.[20] Fourier noted that the actual mechanisms that determine the temperatures of the atmosphere included convection, which was not present in de Saussure's experimental device.In the 1820s Fourier calculated that an object the size of the Earth, and at its distance from the Sun, should be considerably colder than the planet actually is if warmed by only the effects of incoming solar radiation. He examined various possible sources of the additional observed heat in articles published in 1824[14] and 1827.[15] While he ultimately suggested that interstellar radiation might be responsible for a large portion of the additional warmth, Fourier's consideration of the possibility that the Earth's atmosphere might act as an insulator of some kind is widely recognized as the first proposal of what is now known as the greenhouse effect,[16] although Fourier never called it that.[17][18]Fourier left an unfinished work on determinate equations which was edited by Claude-Louis Navier and published in 1831. This work contains much original matter \u2014 in particular, there is a demonstration of Fourier's theorem on the position of the roots of an algebraic equation. Joseph-Louis Lagrange had shown how the roots of an algebraic equation might be separated by means of another equation whose roots were the squares of the differences of the roots of the original equation. Fran\u00e7ois Budan, in 1807 and 1811, had enunciated the theorem generally known by the name of Fourier, but the demonstration was not altogether satisfactory. Fourier's proof[13] is the same as that usually given in textbooks on the theory of equations. The final solution of the problem was given in 1829 by Jacques Charles Fran\u00e7ois Sturm.One important physical contribution in the book was the concept of dimensional homogeneity in equations; i.e. an equation can be formally correct only if the dimensions match on either side of the equality; Fourier made important contributions to dimensional analysis.[12] The other physical contribution was Fourier's proposal of his partial differential equation for conductive diffusion of heat. This equation is now taught to every student of mathematical physics.There were three important contributions in this work, one purely mathematical, two essentially physical. In mathematics, Fourier claimed that any function of a variable, whether continuous or discontinuous, can be expanded in a series of sines of multiples of the variable. Though this result is not correct without additional conditions, Fourier's observation that some discontinuous functions are the sum of infinite series was a breakthrough. The question of determining when a Fourier series converges has been fundamental for centuries. Joseph-Louis Lagrange had given particular cases of this (false) theorem, and had implied that the method was general, but he had not pursued the subject. Peter Gustav Lejeune Dirichlet was the first to give a satisfactory demonstration of it with some restrictive conditions. This work provides the foundation for what is today known as the Fourier transform.In 1822 Fourier published his work on heat flow in Th\u00e9orie analytique de la chaleur (The Analytical Theory of Heat),[8] in which he based his reasoning on Newton's law of cooling, namely, that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures. This book was translated,[9] with editorial 'corrections',[10] into English 56 years later by Freeman (1878).[11] The book was also edited, with many editorial corrections, by Darboux and republished in French in 1888.[10]A bronze statue was erected in Auxerre in 1849, but it was melted down for armaments during World War II.[7] Joseph Fourier University in Grenoble is named after him.Fourier was buried in the P\u00e8re Lachaise Cemetery in Paris, a tomb decorated with an Egyptian motif to reflect his position as secretary of the Cairo Institute, and his collation of Description de l'\u00c9gypte. His name is one of the 72 names inscribed on the Eiffel Tower.Shortly after this event, he died in his bed on 16 May 1830.In 1830, his diminished health began to take its toll:In 1822, Fourier succeeded Jean Baptiste Joseph Delambre as Permanent Secretary of the French Academy of Sciences. In 1830, he was elected a foreign member of the Royal Swedish Academy of Sciences.Hence being faithful to Napoleon, he took the office of Prefect.[4] It was while at Grenoble that he began to experiment on the propagation of heat. He presented his paper On the Propagation of Heat in Solid Bodies to the Paris Institute on December 21, 1807. He also contributed to the monumental Description de l'\u00c9gypte.[5]In 1801,[4] Napoleon appointed Fourier Prefect (Governor) of the Department of Is\u00e8re in Grenoble, where he oversaw road construction and other projects. However, Fourier had previously returned home from the Napoleon expedition to Egypt to resume his academic post as professor at \u00c9cole Polytechnique when Napoleon decided otherwise in his remarkFourier accompanied Napoleon Bonaparte on his Egyptian expedition in 1798, as scientific adviser, and was appointed secretary of the Institut d'\u00c9gypte. Cut off from France by the British fleet, he organized the workshops on which the French army had to rely for their munitions of war. He also contributed several mathematical papers to the Egyptian Institute (also called the Cairo Institute) which Napoleon founded at Cairo, with a view of weakening British influence in the East. After the British victories and the capitulation of the French under General Menou in 1801, Fourier returned to France.Fourier was born at Auxerre (now in the Yonne d\u00e9partement of France), the son of a tailor. He was orphaned at age nine. Fourier was recommended to the Bishop of Auxerre and, through this introduction, he was educated by the Benedictine Order of the Convent of St. Mark. The commissions in the scientific corps of the army were reserved for those of good birth, and being thus ineligible, he accepted a military lectureship on mathematics. He took a prominent part in his own district in promoting the French Revolution, serving on the local Revolutionary Committee. He was imprisoned briefly during the Terror but, in 1795, was appointed to the \u00c9cole Normale and subsequently succeeded Joseph-Louis Lagrange at the \u00c9cole Polytechnique.Jean-Baptiste Joseph Fourier (/\u02c8f\u028a\u0259ri\u02cce\u026a, -i\u0259r/;[1] French:\u00a0[fu\u0281je]; 21 March 1768 \u2013 16 May 1830) was a French mathematician and physicist born in Auxerre and best known for initiating the investigation of Fourier series and their applications to problems of heat transfer and vibrations. The Fourier transform and Fourier's law are also named in his honour. Fourier is also generally credited with the discovery of the greenhouse effect.[2]",
            "title": "Joseph Fourier",
            "url": "https://en.wikipedia.org/wiki/Th%C3%A9orie_analytique_de_la_chaleur"
        },
        {
            "desc_links": [
                "/wiki/Fellow_of_the_Royal_Society",
                "/wiki/Mathematician"
            ],
            "links": [
                "/wiki/31043_Sturm",
                "/wiki/The_72_names_on_the_Eiffel_Tower",
                "/wiki/Eiffel_Tower",
                "/wiki/Jean-Daniel_Colladon",
                "/wiki/Speed_of_sound",
                "/wiki/Eponym",
                "/wiki/Sturm%E2%80%93Liouville_theory",
                "/wiki/Joseph_Liouville",
                "/wiki/Sturm%27s_theorem",
                "/wiki/Real_zero",
                "/wiki/Polynomial",
                "/wiki/Acad%C3%A9mie_des_Sciences",
                "/wiki/Andr%C3%A9-Marie_Amp%C3%A8re",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Sim%C3%A9on_Denis_Poisson",
                "/wiki/Classical_mechanics",
                "/wiki/Coll%C3%A8ge_Rollin",
                "/wiki/Jean-Daniel_Colladon",
                "/wiki/Sturm%27s_theorem",
                "/wiki/Root_of_a_polynomial",
                "/wiki/Algebraic_equation",
                "/wiki/Geneva",
                "/wiki/French_First_Republic",
                "/wiki/Strasbourg",
                "/wiki/Madame_de_Sta%C3%ABl",
                "/wiki/Fellow_of_the_Royal_Society",
                "/wiki/Mathematician"
            ],
            "text": "The asteroid 31043 Sturm is named for him.[3] Sturm's name is one of the 72 names engraved at the Eiffel Tower.In 1851 his health began to fail. He was able to return to teaching for a while during his long illness, but in 1855 he died.[2]In 1826, with his colleague Jean-Daniel Colladon, Sturm helped make the first experimental determination of the speed of sound in water.[2]He was the co-eponym of the Sturm\u2013Liouville theory with Joseph Liouville. Sturm's theorem is a basic result for counting and finding the real zeroes of polynomials.He was chosen a member of the Acad\u00e9mie des Sciences in 1836, filling the seat of Andr\u00e9-Marie Amp\u00e8re. Sturm became r\u00e9p\u00e9titeur in 1838, and in 1840 professor in the \u00c9cole Polytechnique. The same year, after the death of SD Poisson, Sturm was appointed as mechanics professor of the Facult\u00e9 des sciences de Paris\u00a0(fr). His works, Cours d'analyse de l'\u00e9cole polytechnique (1857\u20131863) and Cours de m\u00e9canique de l'\u00e9cole polytechnique (1861), were published after his death in Paris, and were regularly republished.Sturm benefited from the 1830 revolution, as his Protestant faith ceased to be an obstacle to employment in public high schools. At the end of 1830, he was thus appointed as a professor of Math\u00e9matiques Sp\u00e9ciales at the coll\u00e8ge Rollin.At the end of 1823, Sturm stayed in Paris for a short time following the family of his student. He resolved, with his school-fellow Colladon, to try his fortune in Paris, and obtained employment on the Bulletin universel. In 1829, he discovered the theorem which bears his name and which concerns the determination of the number and the localization of the real roots of a polynomial equation included between given limits.Sturm was born in Geneva (then part of France) in 1803. The family of his father, Jewn-Henri Sturm, had emigrated from Strasbourg around 1760 - about 50 years before Charles-Fran\u00e7ois's birth. His mother's name was Jeanne-Louise-Henriette Gremay[1]. In 1818, he started to follow the lectures of the academy of Geneva. In 1819, the death of his father forced Sturm to give lessons to children of the rich in order to support his own family. In 1823, he became tutor to the son of Madame de Sta\u00ebl.Jacques Charles Fran\u00e7ois Sturm ForMemRS (29 September 1803 \u2013 15 December 1855) was a French mathematician. ",
            "title": "Jacques Charles Fran\u00e7ois Sturm",
            "url": "https://en.wikipedia.org/wiki/Jacques_Charles_Fran%C3%A7ois_Sturm"
        },
        {
            "desc_links": [
                "/wiki/Hermite_(crater)",
                "/wiki/Moon",
                "/wiki/Thomas_Joannes_Stieltjes",
                "/wiki/Weierstrass_function",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Natural_logarithm",
                "/wiki/Transcendental_number",
                "/wiki/Ferdinand_von_Lindemann",
                "/wiki/Pi",
                "/wiki/Hermite_polynomials",
                "/wiki/Hermite_interpolation",
                "/wiki/Hermite_normal_form",
                "/wiki/Hermitian_operator",
                "/wiki/Cubic_Hermite_spline",
                "/wiki/Henri_Poincar%C3%A9",
                "/wiki/Help:IPA/French",
                "/wiki/Royal_Society_of_London",
                "/wiki/FRSE",
                "/wiki/France",
                "/wiki/Mathematician",
                "/wiki/Number_theory",
                "/wiki/Quadratic_form",
                "/wiki/Invariant_theory",
                "/wiki/Orthogonal_polynomials",
                "/wiki/Elliptic_function",
                "/wiki/Algebra"
            ],
            "links": [
                "/wiki/Thomas_Joannes_Stieltjes",
                "/wiki/Pure_mathematics",
                "/wiki/Abelian_function",
                "/wiki/Elliptic_function",
                "/wiki/Number_theory",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Natural_logarithm",
                "/wiki/Transcendental_number",
                "/wiki/Ferdinand_von_Lindemann",
                "/wiki/Pi",
                "/wiki/Augustin-Louis_Cauchy",
                "/wiki/Catholic",
                "/wiki/Jacques_Philippe_Marie_Binet",
                "/wiki/Acad%C3%A9mie_des_Sciences",
                "/wiki/Jean-Marie_Duhamel",
                "/wiki/%C3%89cole_Normale_Sup%C3%A9rieure",
                "/wiki/L%C3%A9gion_d%27honneur",
                "/wiki/Paris",
                "/wiki/Joseph_Bertrand",
                "/wiki/Joseph_Liouville",
                "/wiki/Baccalaur%C3%A9at",
                "/wiki/Carl_Gustav_Jacob_Jacobi",
                "/wiki/Abelian_variety",
                "/wiki/Elliptic_functions",
                "/wiki/Niels_Henrik_Abel",
                "/wiki/Nouvelles_Annales_de_Math%C3%A9matiques",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Eug%C3%A8ne_Charles_Catalan",
                "/wiki/Joseph_Louis_Lagrange",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/Nancy-Universit%C3%A9",
                "/wiki/Coll%C3%A8ge_Henri_IV",
                "/wiki/Lyc%C3%A9e_Louis-le-Grand",
                "/wiki/Dieuze",
                "/wiki/The_Moselle",
                "/wiki/Hermite_(crater)",
                "/wiki/Moon",
                "/wiki/Thomas_Joannes_Stieltjes",
                "/wiki/Weierstrass_function",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Natural_logarithm",
                "/wiki/Transcendental_number",
                "/wiki/Ferdinand_von_Lindemann",
                "/wiki/Pi",
                "/wiki/Hermite_polynomials",
                "/wiki/Hermite_interpolation",
                "/wiki/Hermite_normal_form",
                "/wiki/Hermitian_operator",
                "/wiki/Cubic_Hermite_spline",
                "/wiki/Henri_Poincar%C3%A9",
                "/wiki/Help:IPA/French",
                "/wiki/Royal_Society_of_London",
                "/wiki/FRSE",
                "/wiki/France",
                "/wiki/Mathematician",
                "/wiki/Number_theory",
                "/wiki/Quadratic_form",
                "/wiki/Invariant_theory",
                "/wiki/Orthogonal_polynomials",
                "/wiki/Elliptic_function",
                "/wiki/Algebra"
            ],
            "text": "The following is a list of his works.:[1]An inspiring teacher, Hermite strove to cultivate admiration for simple beauty and discourage rigorous minutiae. His correspondence with Thomas Stieltjes testifies to the great aid he gave those beginning scientific life. His published courses of lectures have exercised a great influence. His important original contributions to pure mathematics, published in the major mathematical journals of the world, dealt chiefly with Abelian and elliptic functions and the theory of numbers. During 1858 he solved the equation of the fifth degree by elliptic functions; and during 1873 he proved e, the base of the natural system of logarithms, to be transcendental. This last was used by Ferdinand von Lindemann to prove during 1882 the same for \u03c0.[1]During 1848, Hermite returned to the \u00c9cole Polytechnique as r\u00e9p\u00e9titeur and examinateur d'admission. During 1856 he contracted smallpox. Through the influence of Augustin-Louis Cauchy and of a nun who nursed him, he resumed the practice of his Catholic faith.[3] On 14 July, of that year, he was elected for the vacancy created by the death of Jacques Binet in the Acad\u00e9mie des Sciences. During 1869, he succeeded Jean-Marie Duhamel as professor of mathematics, both at the \u00c9cole Polytechnique, where he remained until 1876, and in the Faculty of Sciences of Paris,[4] which was a post he occupied until his death. From 1862 to 1873 he was lecturer at the \u00c9cole Normale Sup\u00e9rieure. Upon his seventieth birthday, on the occasion of his jubilee which was celebrated at the Sorbonne under the auspices of an international committee, he was promoted grand officer of the L\u00e9gion d'honneur.[1] He died in Paris, 14 January 1901,[1] aged 78.After spending five years working privately towards his degree, in which he befriended eminent mathematicians Joseph Bertrand, Carl Gustav Jacob Jacobi, and Joseph Liouville, he took and passed the examinations for the baccalaur\u00e9at, which he was awarded during 1847. He married Joseph Bertrand's sister, Louise Bertrand, during 1848.[2]A correspondence with Carl Jacobi, begun during 1843 and continued the next year, resulted in the insertion, in the complete edition of Jacobi's works, of two articles by Hermite, one concerning the extension to Abelian functions of one of the theorems of Abel on elliptic functions, and the other concerning the transformation of elliptic functions.[1]During 1842, his first original contribution to mathematics, in which he gave a simple proof of the proposition of Niels Abel concerning the impossibility of obtaining an algebraic solution for the equation of the fifth degree, was published in the \"Nouvelles Annales de Math\u00e9matiques\".[1]Hermite wanted to study at the \u00c9cole Polytechnique and during 1841 he took a year preparing for the examinations and was tutored by Eug\u00e8ne Charles Catalan.[2] During 1842 he was admitted to the school.[1] However, after one year Hermite was refused the right to continue his studies because of his disability (\u00c9cole Polytechnique is to this day a military academy). He had to struggle to regain his admission which he won but with strict conditions imposed. Hermite found this unacceptable and decided to quit the \u00c9cole Polytechnique without graduating.[2]As a boy he read some of the writings of Joseph Louis Lagrange on the solution of numerical equations, and of Carl Friedrich Gauss on the theory of numbers.He studied at the Coll\u00e8ge de Nancy and then, in Paris, at the Coll\u00e8ge Henri IV and at the Lyc\u00e9e Louis-le-Grand.[1]Hermite was born in Dieuze, The Moselle on 24 December 1822, [1] with a deformity in his right foot which would affect his gait for the rest of his life. He was the sixth of seven children of Ferdinand Hermite, and his wife Madeleine Lallemand. His father worked in his mother's family's drapery business, and also pursued a career as an artist. The drapery business relocated to Nancy during 1828 and so did the family.[2]A crater near the north pole of the moon has been named in his honor.In a letter to Thomas Joannes Stieltjes during 1893, Hermite remarked: \"I turn with terror and horror from this lamentable scourge of continuous functions with no derivatives.\"He was the first to prove that e, the base of natural logarithms, is a transcendental number. His methods were used later by Ferdinand von Lindemann to prove that \u03c0 is transcendental.Hermite polynomials, Hermite interpolation, Hermite normal form, Hermitian operators, and cubic Hermite splines are named in his honor. One of his students was Henri Poincar\u00e9.Prof Charles Hermite (French pronunciation:\u00a0\u200b[\u0283a\u0281l \u025b\u0281\u02c8mit]) FRS FRSE MIAS (December 24, 1822 \u2013 January 14, 1901) was a French mathematician who did research concerning number theory, quadratic forms, invariant theory, orthogonal polynomials, elliptic functions, and algebra.",
            "title": "Charles Hermite",
            "url": "https://en.wikipedia.org/wiki/Charles_Hermite"
        },
        {
            "desc_links": [
                "/wiki/Charles_Hermite",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Conjugate_transpose",
                "/wiki/Symmetric_matrix",
                "/wiki/Complex_number",
                "/wiki/Square_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Complex_conjugate"
            ],
            "links": [
                "/wiki/Pauli_matrices",
                "/wiki/Gell-Mann_matrices",
                "/wiki/Theoretical_physics",
                "/wiki/Imaginary_number",
                "/wiki/Real_number",
                "/wiki/Matrix_mechanics",
                "/wiki/Werner_Heisenberg",
                "/wiki/Max_Born",
                "/wiki/Pascual_Jordan",
                "/wiki/Charles_Hermite",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Symmetric_matrix",
                "/wiki/Complex_number",
                "/wiki/Square_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Complex_conjugate"
            ],
            "text": "The Rayleigh quotient is used in the min-max theorem to get exact values of all eigenvalues. It is also used in eigenvalue algorithms to obtain an eigenvalue approximation from an eigenvector approximation. Specifically, this is the basis for Rayleigh quotient iteration.Additional facts related to Hermitian matrices include:Well-known families of Pauli matrices, Gell-Mann matrices and their generalizations are Hermitian. In theoretical physics such Hermitian matrices are often multiplied by imaginary coefficients,[1][2] which results in skew-Hermitian matrices (see below).The diagonal elements must be real, as they must be their own complex conjugate.See the following example:Hermitian matrices are fundamental to the quantum theory of matrix mechanics created by Werner Heisenberg, Max Born, and Pascual Jordan in 1925.Hermitian matrices are named after Charles Hermite, who demonstrated in 1855 that matrices of this form share a property with real symmetric matrices of always having real eigenvalues.Hermitian matrices can be understood as the complex extension of real symmetric matrices.In mathematics, a Hermitian matrix (or self-adjoint matrix) is a complex square matrix that is equal to its own conjugate transpose\u2014that is, the element in the i-th row and j-th column is equal to the complex conjugate of the element in the j-th row and i-th column, for all indices i and j:",
            "title": "Hermitian matrix",
            "url": "https://en.wikipedia.org/wiki/Hermitian_matrix"
        },
        {
            "desc_links": [
                "/wiki/Italians",
                "/wiki/Mathematician"
            ],
            "links": [
                "/wiki/Quintic_equation",
                "/wiki/Sextic_equation",
                "/wiki/Elliptic_function",
                "/wiki/Eugenio_Beltrami",
                "/wiki/Luigi_Cremona",
                "/wiki/Felice_Casorati_(mathematician)",
                "/wiki/Analytical_mechanics",
                "/wiki/University_of_Pavia",
                "/wiki/Italian_unification",
                "/wiki/Parliament_of_Italy",
                "/wiki/Polytechnic_University_of_Milan",
                "/wiki/Hydraulics",
                "/wiki/Construction_engineering",
                "/wiki/Lincei_National_Academy",
                "/wiki/Quintino_Sella",
                "/wiki/Milan",
                "/wiki/Collegio_Borromeo",
                "/wiki/Italians",
                "/wiki/Mathematician"
            ],
            "text": "As mathematician, Brioschi published in Italy various algebraic theories and studied the problem of solving fifth and sixth degree equations using elliptic functions. Brioschi is also remembered as a distinguished teacher: among his students in the University of Pavia there were Eugenio Beltrami, Luigi Cremona and Felice Casorati.From 1850 he taught analytical mechanics in the University of Pavia. After the Italian unification in 1861, he was elected deputy in the Parliament of Italy and then appointed twice secretary of the Italian Education Ministry. In 1863 he founded the Polytechnic University of Milan, where he worked until his death, lecturing in hydraulics, analytical mechanics and construction engineering. In 1865 he entered in the Senate of the Kingdom. In 1870 he became a member of the Accademia dei lincei and in 1884 he succeeded Quintino Sella as president of the National Academy of the Lincei. He directed the Il Politecnico (The Polytechnic) review and, between 1867 and 1877, the Annali di Matematica Pura ed Applicata (Annals of pure and applied mathematics). He died in Milan in 1897.Brioschi was born in Milan in 1824. He graduated from the Collegio Borromeo in 1847.Francesco Brioschi (22 December 1824 \u2013 13 December 1897) was an Italian mathematician.",
            "title": "Francesco Brioschi",
            "url": "https://en.wikipedia.org/wiki/Francesco_Brioschi"
        },
        {
            "desc_links": [
                "/wiki/Group_(mathematics)",
                "/wiki/Orthogonal_group",
                "/wiki/Subgroup",
                "/wiki/Determinant",
                "/wiki/Special_orthogonal_group",
                "/wiki/Invertible_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Determinant",
                "/wiki/Linear_transformation",
                "/wiki/Dot_product",
                "/wiki/Isometry",
                "/wiki/Euclidean_space",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Reflection_(mathematics)",
                "/wiki/Unitary_transformation",
                "/wiki/Transpose",
                "/wiki/Inverse_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Real_number",
                "/wiki/Orthogonal",
                "/wiki/Unit_vector",
                "/wiki/Orthonormality"
            ],
            "links": [
                "/wiki/Clifford_algebra",
                "/wiki/Connected_space",
                "/wiki/Simply_connected_space",
                "/wiki/Covering_map",
                "/wiki/Spinor_group",
                "/wiki/Pin_group",
                "/wiki/Quaternion",
                "/wiki/Condition_number",
                "/wiki/Orthogonal_Procrustes_problem",
                "/wiki/Singular_value_decomposition",
                "/wiki/Matrix_square_root",
                "/wiki/Monte_Carlo_method",
                "/wiki/Uniform_distribution_(continuous)",
                "/wiki/Haar_measure",
                "/wiki/Statistical_independence",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/QR_decomposition",
                "/wiki/Normal_distribution",
                "/wiki/Householder_transformation",
                "/wiki/Gram-Schmidt_process",
                "/wiki/Orthogonalization",
                "/wiki/Polar_decomposition",
                "/wiki/Matrix_norm",
                "/wiki/Newton%27s_method",
                "/wiki/Invertible_matrix",
                "/wiki/Pseudoinverse",
                "/wiki/Linear_least_squares_(mathematics)",
                "/wiki/Gaussian_elimination",
                "/wiki/Cholesky_decomposition",
                "/wiki/Overdetermined_system_of_linear_equations",
                "/wiki/Matrix_decomposition",
                "/wiki/Matrix_multiplication",
                "/wiki/Gaussian_elimination",
                "/wiki/Pivot_element#Partial_and_complete_pivoting",
                "/wiki/Numerical_analysis",
                "/wiki/Linear_algebra",
                "/wiki/Numeric_stability",
                "/wiki/Condition_number",
                "/wiki/Householder_reflection",
                "/wiki/Givens_rotation",
                "/wiki/Lie_algebra",
                "/wiki/Skew-symmetric_matrix",
                "/wiki/Matrix_exponential",
                "/wiki/Complex_number",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Absolute_value",
                "/wiki/Factorial",
                "/wiki/Symmetric_group",
                "/wiki/Alternating_group",
                "/wiki/Givens_rotation",
                "/wiki/Householder_matrix",
                "/wiki/Reflection_group",
                "/wiki/Fiber_bundle",
                "/wiki/Connected_space",
                "/wiki/Normal_subgroup",
                "/wiki/Index_of_a_subgroup",
                "/wiki/Special_orthogonal_group",
                "/wiki/Rotation",
                "/wiki/Quotient_group",
                "/wiki/Coset",
                "/wiki/Exact_sequence",
                "/wiki/Semidirect_product",
                "/wiki/Direct_product_of_groups",
                "/wiki/Group_(mathematics)",
                "/wiki/Compact_space",
                "/wiki/Lie_group",
                "/wiki/Orthogonal_group",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Complex_number",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Absolute_value",
                "/wiki/Even_and_odd_permutations",
                "/wiki/Determinant",
                "/wiki/If_and_only_if",
                "/wiki/Orthonormal_basis",
                "/wiki/Euclidean_space",
                "/wiki/Dot_product",
                "/wiki/Diagonal_matrix",
                "/wiki/Jacobi_rotation",
                "/wiki/Givens_rotation",
                "/wiki/Euler_angles",
                "/wiki/Householder_reflection",
                "/wiki/Axis_and_angle",
                "/wiki/Plane_of_rotation",
                "/wiki/Inversion_in_a_point",
                "/wiki/Improper_rotation",
                "/wiki/Involutory_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Permutation_matrix",
                "/wiki/Rotation_matrix",
                "/wiki/Group_(mathematics)",
                "/wiki/Orthogonal_group",
                "/wiki/Point_group",
                "/wiki/Linear_algebra",
                "/wiki/QR_decomposition",
                "/wiki/Discrete_cosine_transform",
                "/wiki/MP3",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Isometry",
                "/wiki/Linear_algebra",
                "/wiki/Euclidean_space",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Field_(mathematics)",
                "/wiki/Dot_product",
                "/wiki/Euclidean_space",
                "/wiki/Group_(mathematics)",
                "/wiki/Orthogonal_group",
                "/wiki/Subgroup",
                "/wiki/Determinant",
                "/wiki/Special_orthogonal_group",
                "/wiki/Invertible_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Determinant",
                "/wiki/Linear_transformation",
                "/wiki/Dot_product",
                "/wiki/Isometry",
                "/wiki/Euclidean_space",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Reflection_(mathematics)",
                "/wiki/Unitary_transformation",
                "/wiki/Transpose",
                "/wiki/Inverse_matrix",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Real_number",
                "/wiki/Orthogonal",
                "/wiki/Unit_vector",
                "/wiki/Orthonormality"
            ],
            "text": "There is no standard terminology for these matrices. They are sometimes called \"orthonormal matrices\", sometimes \"orthogonal matrices\", and sometimes simply \"matrices with orthonormal rows/columns\".If Q is not a square matrix, then the conditions QTQ = I and QQT = I are not equivalent. The condition QTQ = I says that the columns of Q are orthonormal. This can only happen if Q is an m \u00d7 n matrix with n \u2264 m (due to linear dependence). Similarly, QQT = I says that the rows of Q are orthonormal, which requires n \u2265 m.The Pin and Spin groups are found within Clifford algebras, which themselves can be built from orthogonal matrices.A subtle technical problem afflicts some uses of orthogonal matrices. Not only are the group components with determinant +1 and \u22121 not connected to each other, even the +1 component, SO(n), is not simply connected (except for SO(1), which is trivial). Thus it is sometimes advantageous, or even necessary, to work with a covering group of SO(n), the spin group, Spin(n). Likewise, O(n) has covering groups, the pin groups, Pin(n). For n > 2, Spin(n) is simply connected and thus the universal covering group for SO(n). By far the most famous example of a spin group is Spin(3), which is nothing but SU(2), or the group of unit quaternions.Using a first-order approximation of the inverse and the same initialization results in the modified iteration:These iterations are stable provided the condition number of M is less than three.[3]where Q0 = M.This may be combined with the Babylonian method for extracting the square root of a matrix to give a recurrence which converges to an orthogonal matrix quadratically:The problem of finding the orthogonal matrix Q nearest a given matrix M is related to the Orthogonal Procrustes problem. There are several different ways to get the unique solution, the simplest of which is taking the singular value decomposition of M and replacing the singular values with ones. Another method expresses the R explicitly but requires the use of a matrix square root:[2]Some numerical applications, such as Monte Carlo methods and exploration of high-dimensional data spaces, require generation of uniformly distributed random orthogonal matrices. In this context, \"uniform\" is defined in terms of Haar measure, which essentially requires that the distribution not change if multiplied by any freely chosen orthogonal matrix. Orthogonalizing matrices with independent uniformly distributed random entries does not result in uniformly distributed orthogonal matrices[citation needed], but the QR decomposition of independent normally distributed random entries does, as long as the diagonal of R contains only positive entries. Stewart (1980) replaced this with a more efficient idea that Diaconis & Shahshahani (1987) later generalized as the \"subgroup algorithm\" (in which form it works just as well for permutations and rotations). To generate an (n + 1) \u00d7 (n + 1) orthogonal matrix, take an n \u00d7 n one and a uniformly distributed unit vector of dimension n + 1. Construct a Householder reflection from the vector, then apply it to the smaller matrix (embedded in the larger size with a 1 at the bottom right corner).Gram-Schmidt yields an inferior solution, shown by a Frobenius distance of 8.28659 instead of the minimum 8.12404.and which acceleration trims to two steps (with \u03b3 = 0.353553, 0.565685).For example, consider a non-orthogonal matrix for which the simple averaging algorithm takes seven stepsThe case of a square invertible matrix also holds interest. Suppose, for example, that A is a 3 \u00d7 3 rotation matrix which has been computed as the composition of numerous twists and turns. Floating point does not match the mathematical ideal of real numbers, so A has gradually lost its true orthogonality. A Gram-Schmidt process could orthogonalize the columns, but it is not the most reliable, nor the most efficient, nor the most invariant method. The polar decomposition factors a matrix into a pair, one of which is the unique closest orthogonal matrix to the given matrix, or one of the closest if the given matrix is singular. (Closeness can be measured by any matrix norm invariant under an orthogonal change of basis, such as the spectral norm or the Frobenius norm.) For a near-orthogonal matrix, rapid convergence to the orthogonal factor can be achieved by a \"Newton's method\" approach due to Higham (1986) (1990), repeatedly averaging the matrix with its inverse transpose. Dubrulle (1994) has published an accelerated method with a convenient convergence test.In the case of a linear system which is underdetermined, or an otherwise non-invertible matrix, singular value decomposition (SVD) is equally useful. With A factored as U\u03a3VT, a satisfactory solution uses the Moore-Penrose pseudoinverse, V\u03a3+UT, where \u03a3+ merely replaces each non-zero diagonal entry with its reciprocal. Set x to V\u03a3+UTb.The linear least squares problem is to find the x that minimizes ||Ax \u2212 b||, which is equivalent to projecting b to the subspace spanned by the columns of A. Assuming the columns of A (and hence R) are independent, the projection solution is found from ATAx = ATb. Now ATA is square (n \u00d7 n) and invertible, and also equal to RTR. But the lower rows of zeros in R are superfluous in the product, which is thus already in lower-triangular upper-triangular factored form, as in Gaussian elimination (Cholesky decomposition). Here orthogonality is important not only for reducing ATA = (RTQT)QR to RTR, but also for allowing solution without magnifying numerical problems.Consider an overdetermined system of linear equations, as might occur with repeated measurements of a physical phenomenon to compensate for experimental errors. Write Ax = b, where A is m \u00d7 n, m > n. A QR decomposition reduces A to upper triangular R. For example, if A is 5 \u00d7 3 then R has the formA number of important matrix decompositions (Golub & Van Loan 1996) involve orthogonal matrices, including especially:Likewise, algorithms using Householder and Givens matrices typically use specialized methods of multiplication and storage. For example, a Givens rotation affects only two rows of a matrix it multiplies, changing a full multiplication of order n3 to a much more efficient order n. When uses of these reflections and rotations introduce zeros in a matrix, the space vacated is enough to store sufficient data to reproduce the transform, and to do so robustly. (Following Stewart (1976), we do not store a rotation angle, which is both expensive and badly behaved.)Permutations are essential to the success of many algorithms, including the workhorse Gaussian elimination with partial pivoting (where permutations do the pivoting). However, they rarely appear explicitly as matrices; their special form allows more efficient representation, such as a list of n indices.Numerical analysis takes advantage of many of the properties of orthogonal matrices for numerical linear algebra, and they arise naturally. For example, it is often desirable to compute an orthonormal basis for a space, or an orthogonal change of bases; both take the form of orthogonal matrices. Having determinant \u00b11 and all eigenvalues of magnitude 1 is of great benefit for numeric stability. One implication is that the condition number is 1 (which is the minimum), so errors are not magnified when multiplying with an orthogonal matrix. Many algorithms use orthogonal matrices like Householder reflections and Givens rotations for this reason. It is also helpful that, not only is an orthogonal matrix invertible, but its inverse is available essentially free, by exchanging indices.The exponential of this is the orthogonal matrix for rotation around axis v by angle \u03b8; setting c = cos \u03b8/2, s = sin \u03b8/2,In Lie group terms, this means that the Lie algebra of an orthogonal matrix group consists of skew-symmetric matrices. Going the other direction, the matrix exponential of any skew-symmetric matrix is an orthogonal matrix (in fact, special orthogonal).Evaluation at t = 0 (Q = I) then impliesyieldsSuppose the entries of Q are differentiable functions of t, and that t = 0 gives Q = I. Differentiating the orthogonality conditionThe matrices R1, ..., Rk give conjugate pairs of eigenvalues lying on the unit circle in the complex plane; so this decomposition confirms that all eigenvalues have absolute value 1. If n is odd, there is at least one real eigenvalue, +1 or \u22121; for a 3 \u00d7 3 rotation, the eigenvector associated with +1 is the rotation axis.where the matrices R1, ..., Rk are 2 \u00d7 2 rotation matrices, and with the remaining entries zero. Exceptionally, a rotation block may be diagonal, \u00b1I. Thus, negating one column if necessary, and noting that a 2 \u00d7 2 reflection diagonalizes to a +1 and \u22121, any orthogonal matrix can be brought to the formMore broadly, the effect of any orthogonal matrix separates into independent actions on orthogonal two-dimensional subspaces. That is, if Q is special orthogonal then one can always find an orthogonal matrix P, a (rotational) change of basis, that brings Q into block diagonal form:Permutation matrices are simpler still; they form, not a Lie group, but only a finite group, the order n! symmetric group Sn. By the same kind of argument, Sn is a subgroup of Sn + 1. The even permutations produce the subgroup of permutation matrices of determinant +1, the order n!/2 alternating group.degrees of freedom, and so does O(n).Similarly, SO(n) is a subgroup of SO(n + 1); and any special orthogonal matrix can be generated by Givens plane rotations using an analogous procedure. The bundle structure persists: SO(n) \u21aa SO(n + 1) \u2192 Sn. A single rotation can produce a zero in the first row of the last column, and series of n \u2212 1 rotations will zero all but the last row of the last column of an n \u00d7 n rotation matrix. Since the planes are fixed, each rotation has only one degree of freedom, its angle. By induction, SO(n) therefore hasSince an elementary reflection in the form of a Householder matrix can reduce any orthogonal matrix to this constrained form, a series of such reflections can bring any orthogonal matrix to the identity; thus an orthogonal group is a reflection group. The last column can be fixed to any unit vector, and each choice gives a different copy of O(n) in O(n + 1); in this way O(n + 1) is a bundle over the unit sphere Sn with fiber O(n).Now consider (n + 1) \u00d7 (n + 1) orthogonal matrices with bottom right entry equal to 1. The remainder of the last column (and last row) must be zeros, and the product of any two such matrices has the same form. The rest of the matrix is an n \u00d7 n orthogonal matrix; thus O(n) is a subgroup of O(n + 1) (and of all higher groups).The orthogonal matrices whose determinant is +1 form a path-connected normal subgroup of O(n) of index 2, the special orthogonal group SO(n) of rotations. The quotient group O(n)/SO(n) is isomorphic to O(1), with the projection map choosing [+1] or [\u22121] according to the determinant. Orthogonal matrices with determinant \u22121 do not include the identity, and so do not form a subgroup but only a coset; it is also (separately) connected. Thus each orthogonal group falls into two pieces; and because the projection map splits, O(n) is a semidirect product of SO(n) by O(1). In practical terms, a comparable statement is that any orthogonal matrix can be produced by taking a rotation matrix and possibly negating one of its columns, as we saw with 2 \u00d7 2 matrices. If n is odd, then the semidirect product is in fact a direct product, and any orthogonal matrix can be produced by taking a rotation matrix and possibly negating all of its columns. This follows from the property of determinants that negating a column negates the determinant, and thus negating an odd (but not even) number of columns negates the determinant.The inverse of every orthogonal matrix is again orthogonal, as is the matrix product of two orthogonal matrices. In fact, the set of all n \u00d7 n orthogonal matrices satisfies all the axioms of a group. It is a compact Lie group of dimension n(n \u2212 1)/2, called the orthogonal group and denoted by O(n).Stronger than the determinant restriction is the fact that an orthogonal matrix can always be diagonalized over the complex numbers to exhibit a full set of eigenvalues, all of which must have (complex) modulus\u00a01.With permutation matrices the determinant matches the signature, being +1 or \u22121 as the parity of the permutation is even or odd, for the determinant is an alternating function of the rows.The converse is not true; having a determinant of \u00b11 is no guarantee of orthogonality, even with orthogonal columns, as shown by the following counterexample.The determinant of any orthogonal matrix is +1 or \u22121. This follows from basic facts about determinants, as follows:A real square matrix is orthogonal if and only if its columns form an orthonormal basis of the Euclidean space \u211dn with the ordinary Euclidean dot product, which is the case if and only if its rows form an orthonormal basis of \u211dn. It might be tempting to suppose a matrix with orthogonal (not orthonormal) columns would be called an orthogonal matrix, but such matrices have no special interest and no special name; they only satisfy MTM = D, with D a diagonal matrix.A Jacobi rotation has the same form as a Givens rotation, but is used to zero both off-diagonal entries of a 2 \u00d7 2 symmetric submatrix.A Givens rotation acts on a two-dimensional (planar) subspace spanned by two coordinate axes, rotating by a chosen angle. It is typically used to zero a single subdiagonal entry. Any rotation matrix of size n \u00d7 n can be constructed as a product of at most n(n \u2212 1)/2 such rotations. In the case of 3 \u00d7 3 matrices, three such rotations suffice; and by fixing the sequence we can thus describe all 3 \u00d7 3 rotation matrices (though not uniquely) in terms of the three angles used, often called Euler angles.Here the numerator is a symmetric matrix while the denominator is a number, the squared magnitude of v. This is a reflection in the hyperplane perpendicular to v (negating any vector component parallel to v). If v is a unit vector, then Q = I \u2212 2vvT suffices. A Householder reflection is typically used to simultaneously zero the lower part of a column. Any orthogonal matrix of size n \u00d7 n can be constructed as a product of at most n such reflections.A Householder reflection is constructed from a non-null vector v asThe most elementary permutation is a transposition, obtained from the identity matrix by exchanging two rows. Any n \u00d7 n permutation matrix can be constructed as a product of no more than n \u2212 1 transpositions.However, we have elementary building blocks for permutations, reflections, and rotations that apply in general.Rotations become more complicated in higher dimensions; they can no longer be completely characterized by one angle, and may affect more than one planar subspace. It is common to describe a 3 \u00d7 3 rotation matrix in terms of an axis and angle, but this only works in three dimensions. Above three dimensions two or more angles are needed, each associated with a plane of rotation.represent an inversion through the origin and a rotoinversion, respectively, about the z-axis.Regardless of the dimension, it is always possible to classify orthogonal matrices as purely rotational or not, but for 3 \u00d7 3 matrices and larger the non-rotational matrices can be more complicated than reflections. For example,A reflection is its own inverse, which implies that a reflection matrix is symmetric (equal to its transpose) as well as orthogonal. The product of two rotation matrices is a rotation matrix, and the product of two reflection matrices is also a rotation matrix.The identity is also a permutation matrix.The special case of the reflection matrix with \u03b8 = 90\u00b0 generates a reflection about the line at 45\u00b0 given by y = x and therefore exchanges x and y; it is a permutation matrix, with a single 1 in each column and row (and otherwise 0):In consideration of the first equation, without loss of generality let p = cos \u03b8, q = sin \u03b8; then either t = \u2212q, u = p or t = q, u = \u2212p. We can interpret the first case as a rotation by \u03b8 (where \u03b8 = 0 is the identity), and the second as a reflection across a line at an angle of \u03b8/2.which orthogonality demands satisfy the three equationsThe 2 \u00d7 2 matrices have the formThe simplest orthogonal matrices are the 1 \u00d7 1 matrices [1] and [\u22121] which we can interpret as the identity and a reflection of the real line across the origin.An instance of a 2 \u00d7 2 rotation matrix:Below are a few examples of small orthogonal matrices and possible interpretations.Orthogonal matrices are important for a number of reasons, both theoretical and practical. The n \u00d7 n orthogonal matrices form a group under matrix multiplication, the orthogonal group denoted by O(n), which\u2014with its subgroups\u2014is widely used in mathematics and the physical sciences. For example, the point group of a molecule is a subgroup of O(3). Because floating point versions of orthogonal matrices have advantageous properties, they are key to many algorithms in numerical linear algebra, such as QR decomposition. As another example, with appropriate normalization the discrete cosine transform (used in MP3 compression) is represented by an orthogonal matrix.Thus finite-dimensional linear isometries\u2014rotations, reflections, and their combinations\u2014produce orthogonal matrices. The converse is also true: orthogonal matrices imply orthogonal transformations. However, linear algebra includes orthogonal transformations between spaces which may be neither finite-dimensional nor of the same dimension, and these have no orthogonal matrix equivalent.where Q is an orthogonal matrix. To see the inner product connection, consider a vector v in an n-dimensional real Euclidean space. Written with respect to an orthonormal basis, the squared length of v is vTv. If a linear transformation, in matrix form Qv, preserves vector lengths, thenAn orthogonal matrix is the real specialization of a unitary matrix, and thus always a normal matrix. Although we consider only real matrices here, the definition can be used for matrices with entries from any field. However, orthogonal matrices arise naturally from dot products, and for matrices of complex numbers that leads instead to the unitary requirement. Orthogonal matrices preserve the dot product,[1] so, for vectors u and v in an n-dimensional real Euclidean spaceThe set of n \u00d7 n orthogonal matrices forms a group O(n), known as the orthogonal group. The subgroup SO(n) consisting of orthogonal matrices with determinant +1 is called the special orthogonal group, and each of its elements is a special orthogonal matrix. As a linear transformation, every special orthogonal matrix acts as a rotation.An orthogonal matrix Q is necessarily invertible (with inverse Q\u22121 = QT), unitary (Q\u22121 = Q\u2217) and therefore normal (Q\u2217Q = QQ\u2217) in the reals. The determinant of any orthogonal matrix is either +1 or \u22121. As a linear transformation, an orthogonal matrix preserves the dot product of vectors, and therefore acts as an isometry of Euclidean space, such as a rotation or reflection. In other words, it is a unitary transformation.This leads to the equivalent characterization: a matrix Q is orthogonal if its transpose is equal to its inverse:In linear algebra, an orthogonal matrix or real orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors), i.e.",
            "title": "Orthogonal matrix",
            "url": "https://en.wikipedia.org/wiki/Orthogonal_matrix"
        },
        {
            "desc_links": [
                "/wiki/Riemannian_circle",
                "/wiki/Norm_(mathematics)",
                "/wiki/Unit_disk",
                "/wiki/Right_triangle",
                "/wiki/Pythagorean_theorem",
                "/wiki/Mathematics",
                "/wiki/Circle",
                "/wiki/Radius",
                "/wiki/1_(number)",
                "/wiki/Trigonometry",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Euclidean_plane",
                "/wiki/Unit_sphere"
            ],
            "links": [
                "/wiki/Julia_set",
                "/wiki/Dynamical_system_(definition)",
                "/wiki/Evolution_function",
                "/wiki/Complex_number",
                "/wiki/Euclidean_plane",
                "/wiki/Group_(mathematics)",
                "/wiki/Wikipedia:AUDIENCE",
                "/wiki/Trigonometric_identity#Angle_sum_and_difference_identities",
                "/wiki/Real_number",
                "/wiki/Versine",
                "/wiki/Exsecant",
                "/wiki/Integer",
                "/wiki/Sine",
                "/wiki/Cosine",
                "/wiki/Periodic_function",
                "/wiki/Trigonometric_function",
                "/wiki/Angle",
                "/wiki/Cis_(mathematics)",
                "/wiki/Euler%27s_formula",
                "/wiki/Quantum_mechanics",
                "/wiki/Phase_factor",
                "/wiki/Unit_complex_numbers",
                "/wiki/Complex_numbers",
                "/wiki/Riemannian_circle",
                "/wiki/Norm_(mathematics)",
                "/wiki/Unit_disk",
                "/wiki/Right_triangle",
                "/wiki/Pythagorean_theorem",
                "/wiki/Mathematics",
                "/wiki/Circle",
                "/wiki/Radius",
                "/wiki/1_(number)",
                "/wiki/Trigonometry",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Euclidean_plane",
                "/wiki/Unit_sphere"
            ],
            "text": "is a unit circle. It is a simplest case so it is widely used in study of dynamical systems.Julia set of discrete nonlinear dynamical system with evolution function:Complex numbers can be identified with points in the Euclidean plane, namely the number a + bi is identified with the point (a, b). Under this identification, the unit circle is a group under multiplication, called the circle group; it is usually denoted \ud835\udd4b. On the plane, multiplication by cos \u03b8 + i sin \u03b8 gives a counterclockwise rotation by \u03b8. This group has important applications in mathematics and science.[example needed]Using the unit circle, the values of any trigonometric function for many angles other than those labeled can be calculated without the use of a calculator by using the angle sum and difference formulas.When working with right triangles, sine, cosine, and other trigonometric functions only make sense for angle measures more than zero and less than \u03c0/2. However, when defined with the unit circle, these functions produce meaningful values for any real-valued angle measure\u00a0\u2013 even those greater than 2\u03c0. In fact, all six standard trigonometric functions\u00a0\u2013 sine, cosine, tangent, cotangent, secant, and cosecant, as well as archaic functions like versine and exsecant\u00a0\u2013 can be defined geometrically in terms of a unit circle, as shown at right.Triangles constructed on the unit circle can also be used to illustrate the periodicity of the trigonometric functions. First, construct a radius OA from the origin to a point P(x1,y1) on the unit circle such that an angle t with 0 < t < \u03c0/2 is formed with the positive arm of the x-axis. Now consider a point Q(x1,0) and line segments PQ \u22a5 OQ. The result is a right triangle \u25b3OPQ with \u2220QOP = t. Because PQ has length y1, OQ length x1, and OA length 1, sin(t) = y1 and cos(t) = x1. Having established these equivalences, take another radius OR from the origin to a point R(\u2212x1,y1) on the circle such that the same angle t is formed with the negative arm of the x-axis. Now consider a point S(\u2212x1,0) and line segments RS \u22a5 OS. The result is a right triangle \u25b3ORS with \u2220SOR = t. It can hence be seen that, because \u2220ROQ = \u03c0 \u2212 t, R is at (cos(\u03c0 \u2212 t),sin(\u03c0 \u2212 t)) in the same way that P is at (cos(t),sin(t)). The conclusion is that, since (\u2212x1,y1) is the same as (cos(\u03c0 \u2212 t),sin(\u03c0 \u2212 t)) and (x1,y1) is the same as (cos(t),sin(t)), it is true that sin(t) = sin(\u03c0 \u2212 t) and \u2212cos(t) = cos(\u03c0 \u2212 t). It may be inferred in a similar manner that tan(\u03c0 \u2212 t) = \u2212tan(t), since tan(t) = y1/x1 and tan(\u03c0 \u2212 t) = y1/\u2212x1. A simple demonstration of the above can be seen in the equality sin(\u03c0/4) = sin(3\u03c0/4) = 1/\u221a2.for any integer k.The unit circle also demonstrates that sine and cosine are periodic functions, with the identitiesThe equation x2 + y2 = 1 gives the relationThe trigonometric functions cosine and sine of angle \u03b8 may be defined on the unit circle as follows: If (x, y) is a point on the unit circle, and if the ray from the origin (0, 0) to (x, y) makes an angle \u03b8 from the positive x-axis, (where counterclockwise turning is positive), thenfor all t (see also: cis). This relation represents Euler's formula. In quantum mechanics, this is referred to as phase factor.The unit circle can be considered as the unit complex numbers, i.e., the set of complex numbers z of the formOne may also use other notions of \"distance\" to define other \"unit circles\", such as the Riemannian circle; see the article on mathematical norms for additional examples.The interior of the unit circle is called the open unit disk, while the interior of the unit circle combined with the unit circle itself is called the closed unit disk.Since x2 = (\u2212x)2 for all x, and since the reflection of any point on the unit circle about the x- or y-axis is also on the unit circle, the above equation holds for all points (x, y) on the unit circle, not only those in the first quadrant.If (x, y) is a point on the unit circle's circumference, then |x| and |y| are the lengths of the legs of a right triangle whose hypotenuse has length 1. Thus, by the Pythagorean theorem, x and y satisfy the equationIn mathematics, a unit circle is a circle with a radius of one. Frequently, especially in trigonometry, the unit circle is the circle of radius one centered at the origin (0, 0) in the Cartesian coordinate system in the Euclidean plane. The unit circle is often denoted S1; the generalization to higher dimensions is the unit sphere.",
            "title": "Unit circle",
            "url": "https://en.wikipedia.org/wiki/Unit_circle"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Alfred Clebsch",
            "url": "https://en.wikipedia.org/wiki/Alfred_Clebsch"
        },
        {
            "desc_links": [
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Transpose"
            ],
            "links": [
                "/wiki/Diagonal_matrix",
                "/wiki/Exponential_map_(Lie_theory)",
                "/wiki/Connected_space",
                "/wiki/Special_orthogonal_group",
                "/wiki/Complex_number#Polar_form",
                "/wiki/Matrix_exponential",
                "/wiki/Orthogonal_matrix",
                "/wiki/Lie_algebra",
                "/wiki/Lie_group",
                "/wiki/Commutator",
                "/wiki/Tangent_space",
                "/wiki/Orthogonal_group",
                "/wiki/Special_orthogonal_Lie_algebra",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Vector_space",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Vector_space",
                "/wiki/Field_(mathematics)",
                "/wiki/Bilinear_form",
                "/wiki/Normal_matrix",
                "/wiki/Adjoint_matrix",
                "/wiki/Spectral_theorem",
                "/wiki/Unitary_matrix",
                "/wiki/Block_matrix",
                "/wiki/Special_orthogonal_matrix",
                "/wiki/Matrix_similarity",
                "/wiki/Eigenvalue",
                "/wiki/Spectral_theorem",
                "/wiki/Imaginary_number",
                "/wiki/On-Line_Encyclopedia_of_Integer_Sequences",
                "/wiki/Exponential_generating_function",
                "/wiki/On-Line_Encyclopedia_of_Integer_Sequences",
                "/wiki/Pfaffian",
                "/wiki/Polynomial",
                "/wiki/Carl_Gustav_Jacobi",
                "/wiki/Determinant",
                "/wiki/Cross_product",
                "/wiki/Main_diagonal",
                "/wiki/Trace_of_a_matrix",
                "/wiki/Direct_sum_of_modules",
                "/wiki/Square_matrix",
                "/wiki/Field_(mathematics)",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Main_diagonal",
                "/wiki/Symmetric_matrix",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Transpose"
            ],
            "text": "An n-by-n matrix A is said to be skew-symmetrizable if there exists an invertible diagonal matrix D and skew-symmetric matrix S such that S = DA. For real n-by-n matrices, sometimes the condition for D to have positive entries is added.[6]which corresponds exactly to the polar form cos\u00a0\u03b8\u00a0+\u00a0isin\u00a0\u03b8\u00a0=\u00a0ei\u03b8 of a complex number of unit modulus.with a2\u00a0+\u00a0b2\u00a0=\u00a01. Therefore, putting a\u00a0=\u00a0cos\u03b8 and b\u00a0=\u00a0sin\u00a0\u03b8, it can be writtenThe image of the exponential map of a Lie algebra always lies in the connected component of the Lie group that contains the identity element. In the case of the Lie group O(n), this connected component is the special orthogonal group SO(n), consisting of all orthogonal matrices with determinant 1. So R\u00a0=\u00a0exp(A) will have determinant\u00a0+1. Moreover, since the exponential map of a connected compact Lie group is always surjective, it turns out that every orthogonal matrix with unit determinant can be written as the exponential of some skew-symmetric matrix. In the particular important case of dimension n\u00a0=\u00a02, the exponential representation for an orthogonal matrix reduces to the well-known polar form of a complex number of unit modulus. Indeed, if n=2, a special orthogonal matrix has the formThe matrix exponential of a skew-symmetric matrix A is then an orthogonal matrix R:It is easy to check that the commutator of two skew-symmetric matrices is again skew-symmetric:Another way of saying this is that the space of skew-symmetric matrices forms the Lie algebra o(n) of the Lie group O(n). The Lie bracket on this space is given by the commutator:Skew-symmetric matrices over the field of real numbers form the tangent space to the real orthogonal group O(n) at the identity matrix; formally, the special orthogonal Lie algebra. In this sense, then, skew-symmetric matrices can be thought of as infinitesimal rotations.A bilinear form \u03c6 will be represented by a matrix A such that \u03c6(v, w) = vTAw, once a basis of V is chosen, and conversely an n \u00d7 n matrix A on Kn gives rise to a form sending (v, w) to vTAw. For each of symmetric, skew-symmetric and alternating forms, the representing matrices are symmetric, skew-symmetric and alternating respectively.whence,This is equivalent to a skew-symmetric form when the field is not of characteristic 2 as seen fromWhere the vector space V is over a field of arbitrary characteristic including characteristic 2, we may define an alternating form as a bilinear form \u03c6 such that for all vectors v in VThis defines a form with desirable properties for vector spaces over fields of characteristic not equal to 2, but in a vector space over a field of characteristic 2, the definition is equivalent to that of a symmetric form, as every element is its own additive inverse.such that for all v, w in V,A skew-symmetric form \u03c6 on a vector space V over a field K of arbitrary characteristic is defined to be a bilinear formMore generally, every complex skew-symmetric matrix can be written in the form A = U \u03a3 UT where U is special unitary (det(U) = 1) and \u03a3 has the block-diagonal form given above with complex \u03bbk. This is an example of the Youla decomposition of a complex square matrix.[5]for real \u03bbk. The nonzero eigenvalues of this matrix are \u00b1i\u03bbk. In the odd-dimensional case \u03a3 always has at least one row and column of zeros.Real skew-symmetric matrices are normal matrices (they commute with their adjoints) and are thus subject to the spectral theorem, which states that any real skew-symmetric matrix can be diagonalized by a unitary matrix. Since the eigenvalues of a real skew-symmetric matrix are imaginary, it is not possible to diagonalize one by a real matrix. However, it is possible to bring every skew-symmetric matrix to a block diagonal form by a special orthogonal transformation.[3][4] Specifically, every 2n\u00a0\u00d7\u00a02n real skew-symmetric matrix can be written in the form A = Q\u2009\u03a3\u2009QT where Q is orthogonal andSince a matrix is similar to its own transpose, they must have the same eigenvalues. It follows that the eigenvalues of a skew-symmetric matrix always come in pairs \u00b1\u03bb (except in the odd-dimensional case where there is an additional unpaired 0 eigenvalue). From the spectral theorem, for a real skew-symmetric matrix the nonzero eigenvalues are all pure imaginary and thus are of the form i\u03bb1, \u2212i\u03bb1, i\u03bb2, \u2212i\u03bb2, \u2026 where each of the \u03bbk are real.One actually hasThis can be immediately verified by computing both sides of the previous equation and comparing each corresponding element of the results.the cross product can be written asThe number of positive and negative terms are approximatively a half of the total, although their difference takes larger and larger positive and negative values as n increases (sequence A167029 in the OEIS).The latter yields to the asymptotics (for n even)and it is encoded in the exponential generating functionThe number of distinct terms s(n) in the expansion of the determinant of a skew-symmetric matrix of order n has been considered already by Cayley, Sylvester, and Pfaff. Due to cancellations, this number is quite small as compared the number of terms of a generic matrix of order n, which is n!. The sequence s(n) (sequence A002370 in the OEIS) isThis polynomial is called the Pfaffian of A and is denoted Pf(A). Thus the determinant of a real skew-symmetric matrix is always non-negative. However this last fact can be proved in an elementary way as follows: the eigenvalues of a real skew-symmetric matrix are purely imaginary (see below) and to every eigenvalue there corresponds the conjugate eigenvalue with the same multiplicity; therefore, as the determinant is the product of the eigenvalues, each one repeated according to its multiplicity, it follows at once that the determinant, if it is not 0, is a positive real number.The even-dimensional case is more interesting. It turns out that the determinant of A for n even can be written as the square of a polynomial in the entries of A, which was first proved by Cayley:[2]In particular, if n is odd, and since the underlying field is not of characteristic 2, the determinant vanishes. Hence, all odd dimension skew symmetric matrices are singular as their determinants are always zero. This result is called Jacobi's theorem, after Carl Gustav Jacobi (Eves, 1980).Let A be a n\u00d7n skew-symmetric matrix. The determinant of A satisfies3\u00d73 skew symmetric matrices can be used to represent cross products as matrix multiplications.All main diagonal entries of a skew-symmetric matrix must be zero, so the trace is zero. If A = (aij) is skew-symmetric, aij = \u2212aji; hence aii = 0.where \u2295 denotes the direct sum.Notice that \u00bd(A \u2212 AT) \u2208 Skewn and \u00bd(A + AT) \u2208 Symn. This is true for every square matrix A with entries from any field whose characteristic is different from 2. Then, since Matn = Skewn + Symn and Skewn \u2229 Symn = {0},Let Matn denote the space of n \u00d7 n matrices. A skew-symmetric matrix is determined byn(n\u00a0\u2212\u00a01)/2 scalars (the number of entries above the main diagonal); a symmetric matrix is determined by n(n\u00a0+\u00a01)/2 scalars (the number of entries on or above the main diagonal). Let Skewn denote the space of n \u00d7 n skew-symmetric matrices and Symn denote the space of n \u00d7 n symmetric matrices. If A \u2208 Matn thenIn terms of the entries of the matrix, if aij denotes the entry in the i\u2009th row and j\u2009th column; i.e., A = (aij), then the skew-symmetric condition is aji = \u2212aij. For example, the following matrix is skew-symmetric:In mathematics, particularly in linear algebra, a skew-symmetric (or antisymmetric or antimetric[1]) matrix is a square matrix whose transpose equals its negative; that is, it satisfies the condition",
            "title": "Skew-symmetric matrix",
            "url": "https://en.wikipedia.org/wiki/Skew-symmetric_matrix"
        },
        {
            "desc_links": [
                "/wiki/Continuous_function",
                "/wiki/Intermediate_value_theorem",
                "/wiki/Bolzano%E2%80%93Weierstrass_theorem",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/Mathematics",
                "/wiki/Mathematical_analysis"
            ],
            "links": [
                "/wiki/Impact_crater",
                "/wiki/Weierstrass_(crater)",
                "/wiki/Asteroid",
                "/wiki/14100_Weierstrass",
                "/wiki/Weierstrass_Institute_for_Applied_Analysis_and_Stochastics",
                "/wiki/Calculus_of_variations",
                "/wiki/Strong_extrema",
                "/wiki/Weierstrass%E2%80%93Erdmann_condition",
                "/wiki/Intermediate_value_theorem",
                "/wiki/Bolzano%E2%80%93Weierstrass_theorem",
                "/wiki/(%CE%B5,_%CE%B4)-definition_of_limit",
                "/wiki/Augustin-Louis_Cauchy",
                "/wiki/Uniform_limit",
                "/wiki/Uniform_convergence",
                "/wiki/Christoph_Gudermann",
                "/wiki/Soundness",
                "/wiki/Bernard_Bolzano",
                "/wiki/Limit_of_a_function",
                "/wiki/Limit_of_a_function",
                "/wiki/Continuous_function",
                "/wiki/Sofia_Kovalevskaya",
                "/wiki/Pneumonia",
                "/wiki/University_of_K%C3%B6nigsberg",
                "/wiki/Honorary_doctor%27s_degree",
                "/wiki/Technical_University_of_Berlin",
                "/wiki/Humboldt_University_of_Berlin",
                "/wiki/Carl_Wilhelm_Borchardt",
                "/wiki/Wa%C5%82cz",
                "/wiki/West_Prussia",
                "/wiki/Collegium_Hosianum",
                "/wiki/Braunsberg",
                "/wiki/Gymnasium_(Germany)",
                "/wiki/Paderborn",
                "/wiki/University_of_Bonn",
                "/wiki/University_of_M%C3%BCnster",
                "/wiki/M%C3%BCnster",
                "/wiki/Christoph_Gudermann",
                "/wiki/Elliptic_function",
                "/wiki/Ennigerloh",
                "/wiki/Province_of_Westphalia",
                "/wiki/Continuous_function",
                "/wiki/Intermediate_value_theorem",
                "/wiki/Bolzano%E2%80%93Weierstrass_theorem",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/Mathematics",
                "/wiki/Mathematical_analysis"
            ],
            "text": "The lunar crater Weierstrass and the asteroid 14100 Weierstrass are named after him. Also, there is the Weierstrass Institute for Applied Analysis and Stochastics in Berlin.Weierstrass also made significant advancements in the field of calculus of variations. Using the apparatus of analysis that he helped to develop, Weierstrass was able to give a complete reformulation of the theory which paved the way for the modern study of the calculus of variations. Among the several significant axioms, Weierstrass established a necessary condition for the existence of strong extrema of variational problems. He also helped devise the Weierstrass\u2013Erdmann condition, which gives sufficient conditions for an extremal to have a corner along a given extrema, and allows one to find a minimizing curve for a given integral.Using this definition, he proved the Intermediate Value Theorem. He also proved the Bolzano\u2013Weierstrass theorem and used it to study the properties of continuous functions on closed and bounded intervals.The formal definition of continuity of a function, as formulated by Weierstrass, is as follows:Delta-epsilon proofs are first found in the works of Cauchy in the 1820s.[4][5] Cauchy did not clearly distinguish between continuity and uniform continuity on an interval. Notably, in his 1821 Cours d'analyse, Cauchy argued that the (pointwise) limit of (pointwise) continuous functions was itself (pointwise) continuous, a statement interpreted as being incorrect by many scholars. The correct statement is rather that the uniform limit of continuous functions is continuous (also, the uniform limit of uniformly continuous functions is uniformly continuous). This required the concept of uniform convergence, which was first observed by Weierstrass's advisor, Christoph Gudermann, in an 1838 paper, where Gudermann noted the phenomenon but did not define it or elaborate on it. Weierstrass saw the importance of the concept, and both formalized it and applied it widely throughout the foundations of calculus.Weierstrass was interested in the soundness of calculus, and at the time, there were somewhat ambiguous definitions regarding the foundations of calculus, and hence important theorems could not be proven with sufficient rigour. While Bolzano had developed a reasonably rigorous definition of a limit as early as 1817 (and possibly even earlier) his work remained unknown to most of the mathematical community until years later, and many mathematicians had only vague definitions of limits and continuity of functions.At the age of fifty-five, Weierstrass met Sonya Kovalevsky whom he tutored privately after failing to secure her admission to the University. They had a fruitful intellectual, but troubled personal relationship that \"far transcended the usual teacher-student relationship\". The misinterpretation of this relationship and Kovalevsky's early death in 1891 was said to have contributed to Weierstrass' later ill-health. He was immobile for the last three years of his life, and died in Berlin from pneumonia.[3]After 1850 Weierstrass suffered from a long period of illness, but was able to publish papers that brought him fame and distinction. The University of K\u00f6nigsberg conferred an honorary doctor's degree on him on 31 March 1854. In 1856 he took a chair at the Gewerbeinstitut, which later became the Technical University of Berlin. In 1864 he became professor at the Friedrich-Wilhelms-Universit\u00e4t Berlin, which later became the Humboldt Universit\u00e4t zu Berlin.Weierstrass may have had an illegitimate child named Franz with the widow of his friend Carl Wilhelm Borchardt.[2]In 1843 he taught in Deutsch Krone in West Prussia and since 1848 he taught at the Lyceum Hosianum in Braunsberg. Besides mathematics he also taught physics, botany, and gymnastics.[1]Weierstrass was the son of Wilhelm Weierstrass, a government official, and Theodora Vonderforst. His interest in mathematics began while he was a gymnasium student at the Theodorianum\u00a0(de) in Paderborn. He was sent to the University of Bonn upon graduation to prepare for a government position. Because his studies were to be in the fields of law, economics, and finance, he was immediately in conflict with his hopes to study mathematics. He resolved the conflict by paying little heed to his planned course of study, but continued private study in mathematics. The outcome was to leave the university without a degree. After that he studied mathematics at the M\u00fcnster Academy (which was even at this time very famous for mathematics) and his father was able to obtain a place for him in a teacher training school in M\u00fcnster. Later he was certified as a teacher in that city. During this period of study, Weierstrass attended the lectures of Christoph Gudermann and became interested in elliptic functions.Weierstrass was born in Ostenfelde, part of Ennigerloh, Province of Westphalia.[1]Weierstrass formalized the definition of the continuity of a function, proved the intermediate value theorem and the Bolzano\u2013Weierstrass theorem, and used the latter to study the properties of continuous functions on closed bounded intervals.Karl Theodor Wilhelm Weierstrass (German: Weierstra\u00df [\u02c8va\u026a\u0250\u0283t\u0281as]; 31 October 1815 \u2013 19 February 1897) was a German mathematician often cited as the \"father of modern analysis\". Despite leaving university without a degree, he studied mathematics and trained as a teacher, eventually teaching mathematics, physics, botany and gymnastics.",
            "title": "Karl Weierstrass",
            "url": "https://en.wikipedia.org/wiki/Karl_Weierstrass"
        },
        {
            "desc_links": [
                "/wiki/Orbit_(dynamics)",
                "/wiki/Lyapunov_stability",
                "/wiki/Eigenvalue",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Lyapunov_function",
                "/wiki/Stability_criterion",
                "/wiki/Mathematics",
                "/wiki/Differential_equation",
                "/wiki/Dynamical_system",
                "/wiki/Heat_equation",
                "/wiki/Maximum_principle",
                "/wiki/Lp_space",
                "/wiki/Gromov%E2%80%93Hausdorff_convergence"
            ],
            "links": [
                "/wiki/Lyapunov_stability",
                "/wiki/Lyapunov_function",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Routh%E2%80%93Hurwitz_stability_criterion",
                "/wiki/Vector_field",
                "/wiki/Hartman%E2%80%93Grobman_theorem",
                "/wiki/Routh%E2%80%93Hurwitz_stability_criterion",
                "/wiki/Characteristic_polynomial",
                "/wiki/Hurwitz_polynomial",
                "/wiki/Routh%E2%80%93Hurwitz_theorem",
                "/wiki/Real_part",
                "/wiki/Autonomous_system_(mathematics)",
                "/wiki/Linear_differential_equation",
                "/wiki/Eigenvalue",
                "/wiki/Jacobian_matrix",
                "/wiki/Eigenvalues",
                "/wiki/Diffeomorphism",
                "/wiki/Smooth_manifold",
                "/wiki/Absolute_value",
                "/wiki/Derivative",
                "/wiki/Linear_approximation",
                "/wiki/Continuously_differentiable_function",
                "/wiki/Linearization",
                "/wiki/Oscillation",
                "/wiki/Pendulum",
                "/wiki/Damping",
                "/wiki/Linearization",
                "/wiki/Phase_space",
                "/wiki/Square_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Hartman%E2%80%93Grobman_theorem",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Exponential_decay",
                "/wiki/Lyapunov_stability",
                "/wiki/Exponential_stability",
                "/wiki/Qualitative_theory_of_differential_equations",
                "/wiki/Equilibrium_point",
                "/wiki/Periodic_orbit",
                "/wiki/Orbit_(dynamics)",
                "/wiki/Lyapunov_stability",
                "/wiki/Eigenvalue",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Lyapunov_function",
                "/wiki/Stability_criterion",
                "/wiki/Mathematics",
                "/wiki/Differential_equation",
                "/wiki/Dynamical_system",
                "/wiki/Heat_equation",
                "/wiki/Maximum_principle",
                "/wiki/Lp_space",
                "/wiki/Gromov%E2%80%93Hausdorff_convergence"
            ],
            "text": "A general way to establish Lyapunov stability or asymptotic stability of a dynamical system is by means of Lyapunov functions.Let Jp(v) be the n\u00d7n Jacobian matrix of the vector field v at the point p. If all eigenvalues of J have strictly negative real part then the solution is asymptotically stable. This condition can be tested using the Routh\u2013Hurwitz criterion.has a constant solutionSuppose that v is a C1-vector field in Rn which vanishes at a point p, v(p) = 0. Then the corresponding autonomous systemAsymptotic stability of fixed points of a non-linear system can often be established using the Hartman\u2013Grobman theorem.Application of this result in practice, in order to decide the stability of the origin for a linear system, is facilitated by the Routh\u2013Hurwitz stability criterion. The eigenvalues of a matrix are the roots of its characteristic polynomial. A polynomial in one variable with real coefficients is called a Hurwitz polynomial if the real parts of all roots are strictly negative. The Routh\u2013Hurwitz theorem implies a characterization of Hurwitz polynomials by means of an algorithm that avoids computing the roots.(In a different language, the origin 0 \u2208 Rn is an equilibrium point of the corresponding dynamical system.) This solution is asymptotically stable as t \u2192 \u221e (\"in the future\") if and only if for all eigenvalues \u03bb of A, Re(\u03bb) < 0. Similarly, it is asymptotically stable as t \u2192 \u2212\u221e (\"in the past\") if and only if for all eigenvalues \u03bb of A, Re(\u03bb) > 0. If there exists an eigenvalue \u03bb of A with Re(\u03bb) > 0 then the solution is unstable for t \u2192 \u221e.where x(t) \u2208 Rn and A is an n\u00d7n matrix with real entries, has a constant solutionAn autonomous systemThe stability of fixed points of a system of constant coefficient linear differential equations of first order can be analyzed using the eigenvalues of the corresponding matrix.There is an analogous criterion for a continuously differentiable map f: Rn \u2192 Rn with a fixed point a, expressed in terms of its Jacobian matrix at a, Ja(f). If all eigenvalues of J are real or complex numbers with absolute value strictly less than 1 then a is a stable fixed point; if at least one of them has absolute value strictly greater than 1 then a is unstable. Just as for n=1, the case of the largest absolute value being 1 needs to be investigated further\u00a0\u2014 the Jacobian matrix test is inconclusive. The same criterion holds more generally for diffeomorphisms of a smooth manifold.which means that the derivative measures the rate at which the successive iterates approach the fixed point a or diverge from it. If the derivative at a is exactly 1 or \u22121, then more information is needed in order to decide stability.ThusThe fixed point a is stable if the absolute value of the derivative of f at a is strictly less than 1, and unstable if it is strictly greater than 1. This is because near the point a, the function f has a linear approximation with slope f'(a):Let f: R \u2192 R be a continuously differentiable function with a fixed point a, f(a) = a. Consider the dynamical system obtained by iterating the function f:There are useful tests of stability for the case of a linear system. Stability of a nonlinear system can often be inferred from the stability of its linearization.The simplest kind of an orbit is a fixed point, or an equilibrium. If a mechanical system is in a stable equilibrium state then a small push will result in a localized motion, for example, small oscillations as in the case of a pendulum. In a system with damping, a stable equilibrium state is moreover asymptotically stable. On the other hand, for an unstable equilibrium, such as a ball resting on a top of a hill, certain small pushes will result in a motion with a large amplitude that may or may not converge to the original state.One of the key ideas in stability theory is that the qualitative behavior of an orbit under perturbations can be analyzed using the linearization of the system near the orbit. In particular, at each equilibrium of a smooth dynamical system with an n-dimensional phase space, there is a certain n\u00d7n matrix A whose eigenvalues characterize the behavior of the nearby points (Hartman\u2013Grobman theorem). More precisely, if all eigenvalues are negative real numbers or complex numbers with negative real parts then the point is a stable attracting fixed point, and the nearby points converge to it at an exponential rate, cf Lyapunov stability and exponential stability. If none of the eigenvalues are purely imaginary (or zero) then the attracting and repelling directions are related to the eigenspaces of the matrix A with eigenvalues whose real part is negative and, respectively, positive. Analogous statements are known for perturbations of more complicated orbits.Stability means that the trajectories do not change too much under small perturbations. The opposite situation, where a nearby orbit is getting repelled from the given orbit, is also of interest. In general, perturbing the initial state in some directions results in the trajectory asymptotically approaching the given one and in other directions to the trajectory getting away from it. There may also be directions for which the behavior of the perturbed orbit is more complicated (neither converging nor escaping completely), and then stability theory does not give sufficient information about the dynamics.Many parts of the qualitative theory of differential equations and dynamical systems deal with asymptotic properties of solutions and the trajectories\u2014what happens with the system after a long period of time. The simplest kind of behavior is exhibited by equilibrium points, or fixed points, and by periodic orbits. If a particular orbit is well understood, it is natural to ask next whether a small change in the initial condition will lead to similar behavior. Stability theory addresses the following questions: Will a nearby orbit indefinitely stay close to a given orbit? Will it converge to the given orbit? (The latter is a stronger property.) In the former case, the orbit is called stable; in the latter case, it is called asymptotically stable and the given orbit is said to be attracting.In dynamical systems, an orbit is called Lyapunov stable if the forward orbit of any point is in a small enough neighborhood or it stays in a small (but perhaps, larger) neighborhood. Various criteria have been developed to prove stability or instability of an orbit. Under favorable circumstances, the question may be reduced to a well-studied problem involving eigenvalues of matrices. A more general method involves Lyapunov functions. In practice, any one of a number of different stability criteria are applied.In mathematics, stability theory addresses the stability of solutions of differential equations and of trajectories of dynamical systems under small perturbations of initial conditions. The heat equation, for example, is a stable partial differential equation because small perturbations of initial data lead to small variations in temperature at a later time as a result of the maximum principle. In partial differential equations one may measure the distances between functions using Lp norms or the sup norm, while in differential geometry one may measure the distance between spaces using the Gromov\u2013Hausdorff distance.",
            "title": "Stability theory",
            "url": "https://en.wikipedia.org/wiki/Stability_theory"
        },
        {
            "desc_links": [
                "/wiki/Hermitian_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Characteristic_polynomial",
                "/wiki/Geometric_multiplicity",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Basis_function",
                "/wiki/Eigenvector",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Linearly_independent",
                "/wiki/Generalized_eigenvector",
                "/wiki/Ordinary_differential_equation"
            ],
            "links": [
                "/wiki/Eigenvalue",
                "/wiki/Jordan_normal_form",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Jordan_matrix",
                "/wiki/Hermitian_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Characteristic_polynomial",
                "/wiki/Geometric_multiplicity",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Basis_function",
                "/wiki/Eigenvector",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Linearly_independent",
                "/wiki/Generalized_eigenvector",
                "/wiki/Ordinary_differential_equation"
            ],
            "text": "(and constant multiples thereof).\nwhich has a double eigenvalue of 3 but only one distinct eigenvector\nA simple example of a defective matrix is:\nIn fact, any defective matrix has a nontrivial Jordan normal form, which is as close as one can come to diagonalization of such a matrix.\nhas an eigenvalue, \u03bb, with algebraic multiplicity n, but only one distinct eigenvector,\nAny nontrivial Jordan block of size 2\u00d72 or larger (that is, not completely diagonal) is defective.  (A diagonal matrix is a special case of the Jordan normal form and is not defective.)  For example, the n \u00d7 n Jordan block,\nA Hermitian matrix (or the special case of a real symmetric matrix) or a unitary matrix is never defective; more generally, a normal matrix (which includes Hermitian and unitary as special cases) is never defective.\nA defective matrix always has fewer than n distinct eigenvalues, since distinct eigenvalues always have linearly independent eigenvectors.  In particular, a defective matrix has one or more eigenvalues \u03bb with algebraic multiplicity m > 1 (that is, they are multiple roots of the characteristic polynomial), but fewer than m linearly independent eigenvectors associated with \u03bb. If the algebraic multiplicity of \u03bb exceeds its geometric multiplicity (that is, the number of linearly independent eigenvectors associated with \u03bb), then \u03bb is said to be a defective eigenvalue.[1]  However, every eigenvalue with algebraic multiplicity m always has m linearly independent generalized eigenvectors.\nIn linear algebra, a defective matrix is a square matrix that does not have a complete basis of eigenvectors, and is therefore not diagonalizable.  In particular, an n\u00a0\u00d7\u00a0n matrix is defective if and only if it does not have n linearly independent eigenvectors.[1]  A complete basis is formed by augmenting the eigenvectors with generalized eigenvectors, which are necessary for solving defective systems of ordinary differential equations and other problems.\n",
            "title": "Defective matrix",
            "url": "https://en.wikipedia.org/wiki/Defective_matrix"
        },
        {
            "desc_links": [
                "/wiki/Pierre-Simon_Laplace",
                "/wiki/Quarto_(text)",
                "/wiki/Saint_Petersburg",
                "/wiki/Russian_Empire",
                "/wiki/Berlin",
                "/wiki/Kingdom_of_Prussia",
                "/wiki/Help:IPA/English",
                "/wiki/Help:Pronunciation_respelling_key",
                "/wiki/Swiss_Standard_German",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/File:LeonhardEulerByDrsDotChRadio.ogg",
                "/wiki/German_Standard_German",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/File:De-Leonhard_Euler.flac",
                "/wiki/Mathematician",
                "/wiki/Physicist",
                "/wiki/Astronomer",
                "/wiki/Logician",
                "/wiki/Engineer",
                "/wiki/Infinitesimal_calculus",
                "/wiki/Graph_theory",
                "/wiki/Topology",
                "/wiki/Analytic_number_theory",
                "/wiki/Mathematical_notation",
                "/wiki/Mathematical_analysis",
                "/wiki/Function_(mathematics)",
                "/wiki/Mechanics",
                "/wiki/Fluid_dynamics",
                "/wiki/Optics",
                "/wiki/Astronomy",
                "/wiki/Music_theory"
            ],
            "links": [
                "/wiki/Euler_Commission",
                "/wiki/Swiss_Academies_of_Arts_and_Sciences",
                "/wiki/Contributions_of_Leonhard_Euler_to_mathematics#Works",
                "/wiki/Swiss_franc",
                "/wiki/Asteroid",
                "/wiki/2002_Euler",
                "/wiki/Lutheran_Church",
                "/wiki/Calendar_of_Saints_(Lutheran)",
                "/wiki/Biblical_inerrancy",
                "/wiki/Apologetics",
                "/wiki/Denis_Diderot",
                "/wiki/Atheism",
                "/wiki/Existence_of_God",
                "/wiki/Non_sequitur_(literary_device)",
                "/wiki/Augustus_De_Morgan",
                "/wiki/Biblical_inspiration",
                "/wiki/Daniel_Bernoulli",
                "/wiki/Gottfried_Leibniz",
                "/wiki/Monadism",
                "/wiki/Christian_Wolff_(philosopher)",
                "/wiki/Leonhard_Euler#Graph_theory",
                "/wiki/Tonnetz",
                "/wiki/Lattice_(music)",
                "/wiki/Diagram",
                "/wiki/Set_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Element_(mathematics)",
                "/wiki/Intersection_(set_theory)",
                "/wiki/Subset",
                "/wiki/Disjoint_sets",
                "/wiki/Disjoint_sets",
                "/wiki/Intersection_(set_theory)",
                "/wiki/Subset",
                "/wiki/Set_theory",
                "/wiki/New_math",
                "/wiki/Closed_curve",
                "/wiki/Syllogism",
                "/wiki/Euler_diagram",
                "/wiki/Buckling",
                "/wiki/Inviscid_flow",
                "/wiki/Euler_equations_(fluid_dynamics)",
                "/wiki/Optics",
                "/wiki/Corpuscular_theory_of_light",
                "/wiki/Opticks",
                "/wiki/Wave_theory_of_light",
                "/wiki/Christiaan_Huygens",
                "/wiki/Wave-particle_duality",
                "/wiki/Euler%E2%80%93Bernoulli_beam_equation",
                "/wiki/Classical_mechanics",
                "/wiki/Solar_parallax",
                "/wiki/History_of_longitude",
                "/wiki/Musical_theory",
                "/wiki/Bernoulli_numbers",
                "/wiki/Fourier_series",
                "/wiki/Euler_number",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Pi",
                "/wiki/Gottfried_Leibniz",
                "/wiki/Differential_calculus",
                "/wiki/Method_of_Fluxions",
                "/wiki/Numerical_approximation",
                "/wiki/Euler_approximations",
                "/wiki/Euler%27s_method",
                "/wiki/Euler%E2%80%93Maclaurin_formula",
                "/wiki/Differential_equations",
                "/wiki/Euler%E2%80%93Mascheroni_constant",
                "/wiki/Seven_Bridges_of_K%C3%B6nigsberg",
                "/wiki/K%C3%B6nigsberg",
                "/wiki/Kingdom_of_Prussia",
                "/wiki/Pregel",
                "/wiki/Eulerian_path",
                "/wiki/Graph_theory",
                "/wiki/Planar_graph",
                "/wiki/Newton%27s_identities",
                "/wiki/Fermat%27s_little_theorem",
                "/wiki/Fermat%27s_theorem_on_sums_of_two_squares",
                "/wiki/Lagrange%27s_four-square_theorem",
                "/wiki/Totient_function",
                "/wiki/Coprime",
                "/wiki/Euler%27s_theorem",
                "/wiki/Perfect_number",
                "/wiki/Euclid",
                "/wiki/Mersenne_prime",
                "/wiki/Euclid%E2%80%93Euler_theorem",
                "/wiki/Quadratic_reciprocity",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/2147483647",
                "/wiki/Largest_known_prime",
                "/wiki/Proof_that_the_sum_of_the_reciprocals_of_the_primes_diverges",
                "/wiki/Riemann_zeta_function",
                "/wiki/Proof_of_the_Euler_product_formula_for_the_Riemann_zeta_function",
                "/wiki/Christian_Goldbach",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Analytic_number_theory",
                "/wiki/Generalized_hypergeometric_series",
                "/wiki/Q-series",
                "/wiki/Hyperbolic_functions",
                "/wiki/Generalized_continued_fraction",
                "/wiki/Infinitude_of_primes",
                "/wiki/Harmonic_series_(mathematics)",
                "/wiki/Prime_numbers",
                "/wiki/Prime_number_theorem",
                "/wiki/Transcendental_function",
                "/wiki/Gamma_function",
                "/wiki/Quartic_equation",
                "/wiki/Complex_analysis",
                "/wiki/Calculus_of_variations",
                "/wiki/Euler%E2%80%93Lagrange_equation",
                "/wiki/De_Moivre%27s_formula",
                "/wiki/Euler%27s_formula",
                "/wiki/Richard_P._Feynman",
                "/wiki/Mathematical_Intelligencer",
                "/wiki/Euler%27s_identity",
                "/wiki/Exponential_function",
                "/wiki/Logarithms",
                "/wiki/Complex_number",
                "/wiki/Trigonometric_function",
                "/wiki/Real_number",
                "/wiki/%CE%A6",
                "/wiki/Euler%27s_formula",
                "/wiki/Exponential_function#On_the_complex_plane",
                "/wiki/Inverse_tangent",
                "/wiki/Isaac_Newton",
                "/wiki/Gottfried_Wilhelm_Leibniz",
                "/wiki/Basel_problem",
                "/wiki/Infinitesimal_calculus",
                "/wiki/Bernoulli_family",
                "/wiki/Mathematical_rigor",
                "/wiki/Generality_of_algebra",
                "/wiki/Mathematical_analysis",
                "/wiki/Power_series",
                "/wiki/Function_(mathematics)",
                "/wiki/Trigonometric_functions",
                "/wiki/Natural_logarithm",
                "/wiki/Euler%27s_number",
                "/wiki/Sigma",
                "/wiki/Imaginary_unit",
                "/wiki/Pi_(letter)",
                "/wiki/Pi",
                "/wiki/Welsh_people",
                "/wiki/William_Jones_(mathematician)",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Calculus",
                "/wiki/Euler%E2%80%93Mascheroni_constant",
                "/wiki/Gamma",
                "/wiki/Rational_number",
                "/wiki/Irrational_number",
                "/wiki/Geometry",
                "/wiki/Infinitesimal",
                "/wiki/Calculus",
                "/wiki/Trigonometry",
                "/wiki/Algebra",
                "/wiki/Number_theory",
                "/wiki/Continuum_physics",
                "/wiki/Lunar_theory",
                "/wiki/Physics",
                "/wiki/Quarto_(text)",
                "/wiki/List_of_topics_named_after_Leonhard_Euler",
                "/wiki/Smolensk_Lutheran_Cemetery",
                "/wiki/Goloday_Island",
                "/wiki/Russian_Academy_of_Sciences",
                "/wiki/Alexander_Nevsky_Lavra",
                "/wiki/Uranus",
                "/wiki/Orbit",
                "/wiki/Academician",
                "/wiki/Anders_Johan_Lexell",
                "/wiki/Brain_hemorrhage",
                "/wiki/Russian_Academy_of_Sciences",
                "/wiki/Nicolas_Fuss",
                "/wiki/Marquis_de_Condorcet",
                "/wiki/American_Academy_of_Arts_and_Sciences",
                "/wiki/Seven_Years%27_War",
                "/wiki/Ivan_Petrovich_Saltykov",
                "/wiki/Empress_Elizabeth",
                "/wiki/Catherine_II_of_Russia",
                "/wiki/Eyesight",
                "/wiki/Cartography",
                "/wiki/Cyclops",
                "/wiki/Cataract",
                "/wiki/Aeneid",
                "/wiki/Virgil",
                "/wiki/Frederick_II_of_Prussia",
                "/wiki/Voltaire",
                "/wiki/Friederike_Charlotte_of_Brandenburg-Schwedt",
                "/wiki/Anhalt-Dessau",
                "/wiki/Letters_to_a_German_Princess",
                "/wiki/Prussian_Academy_of_Sciences",
                "/wiki/Frederick_the_Great_of_Prussia",
                "/wiki/Berlin",
                "/wiki/Introductio_in_analysin_infinitorum",
                "/wiki/Institutiones_calculi_differentialis",
                "/wiki/Differential_calculus",
                "/wiki/Royal_Swedish_Academy_of_Sciences",
                "/wiki/Georg_Gsell",
                "/wiki/Neva_River",
                "/wiki/Catherine_I_of_Russia",
                "/wiki/Peter_II_of_Russia",
                "/wiki/Peter_I_of_Russia",
                "/wiki/Russian_Navy",
                "/wiki/Daniel_Bernoulli",
                "/wiki/Nicolaus_II_Bernoulli",
                "/wiki/Russian_Academy_of_Sciences",
                "/wiki/Saint_Petersburg",
                "/wiki/Speed_of_sound",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Mast_(sailing)",
                "/wiki/Pierre_Bouguer",
                "/wiki/University_of_Basel",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/Isaac_Newton",
                "/wiki/Johann_Bernoulli",
                "/wiki/Greek_language",
                "/wiki/Hebrew_language",
                "/wiki/Basel",
                "/wiki/Reformed_Church",
                "/wiki/Given_name#Name_at_birth",
                "/wiki/Riehen",
                "/wiki/Bernoulli_family",
                "/wiki/Johann_Bernoulli",
                "/wiki/Pierre-Simon_Laplace",
                "/wiki/Quarto_(text)",
                "/wiki/Saint_Petersburg",
                "/wiki/Russian_Empire",
                "/wiki/Berlin",
                "/wiki/Kingdom_of_Prussia"
            ],
            "text": "A definitive collection of Euler's works, entitled Opera Omnia, has been published since 1911 by the Euler Commission of the Swiss Academy of Sciences. A complete chronological list of Euler's works is available at the following page: The Enestr\u00f6m Index (PDF).Euler has an extensive bibliography. His best-known books include:Euler was featured on the sixth series of the Swiss 10-franc banknote and on numerous Swiss, German, and Russian postage stamps. The asteroid 2002 Euler was named in his honor. He is also commemorated by the Lutheran Church on their Calendar of Saints on 24 May\u2014he was a devout Christian (and believer in biblical inerrancy) who wrote apologetics and argued forcefully against the prominent atheists of his time.[63]There is a famous legend[64] inspired by Euler's arguments with secular philosophers over religion, which is set during Euler's second stint at the St. Petersburg Academy. The French philosopher Denis Diderot was visiting Russia on Catherine the Great's invitation. However, the Empress was alarmed that the philosopher's arguments for atheism were influencing members of her court, and so Euler was asked to confront the Frenchman. Diderot was informed that a learned mathematician had produced a proof of the existence of God: he agreed to view the proof as it was presented in court. Euler appeared, advanced toward Diderot, and in a tone of perfect conviction announced this non-sequitur: \"Sir, a+bn/n=x, hence God exists\u2014reply!\" Diderot, to whom (says the story) all mathematics was gibberish, stood dumbstruck as peals of laughter erupted from the court. Embarrassed, he asked to leave Russia, a request that was graciously granted by the Empress. However amusing the anecdote may be, it is apocryphal, given that Diderot himself did research in mathematics.[65] The legend was apparently first told by Dieudonn\u00e9 Thi\u00e9bault[66] with significant embellishment by Augustus De Morgan.[67][68]Much of what is known of Euler's religious beliefs can be deduced from his Letters to a German Princess and an earlier work, Rettung der G\u00f6ttlichen Offenbahrung Gegen die Einw\u00fcrfe der Freygeister (Defense of the Divine Revelation against the Objections of the Freethinkers). These works show that Euler was a devout Christian who believed the Bible to be inspired; the Rettung was primarily an argument for the divine inspiration of scripture.[63]Euler and his friend Daniel Bernoulli were opponents of Leibniz's monadism and the philosophy of Christian Wolff. Euler insisted that knowledge is founded in part on the basis of precise quantitative laws, something that monadism and Wolffian science were unable to provide. Euler's religious leanings might also have had a bearing on his dislike of the doctrine; he went so far as to label Wolff's ideas as \"heathen and atheistic\".[62]where pi are prime numbers and ki their exponents.[61]Euler further used the principle of the \"exponent\" to propose a derivation of the gradus suavitatis (degree of suavity, of agreeableness) of intervals and chords from their prime factors \u2013 one must keep in mind that he considered just intonation, i.e. 1 and the prime numbers 3 and 5 only.[60] Formulas have been proposed extending this system to any number of prime numbers, e.g. in the formEuler devised a specific graph, the Speculum musicum,[58] to illustrate the diatonico-chromatic genre, and discussed paths in this graph for specific intervals, reminding his interest for the Seven Bridges of K\u00f6nigsberg (see above). The device knew a renewed interest as the Tonnetz in neo-Riemannian theory (see also Lattice (music)).[59]A first point of Euler\u2019s musical theory is the definition of \"genres\", i.e. of possible divisions of the octave using the prime numbers 3 and 5. Euler describes 18 such genres, with the general definition 2mA, where A is the \"exponent\" of the genre (i.e. the sum of the exponents of 3 and 5) and 2m (where \"m is an indefinite number, small or large, so long as the sounds are perceptible\"[54]), expresses that the relation holds independently of the number of octaves concerned. The first genre, with A = 1, is the octave itself (or its duplicates); the second genre, 2m.3, is the octave divided by the fifth (fifth + fourth, C\u2013G\u2013C); the third genre is 2m.5, major third + minor sixth (C\u2013E\u2013C); the fourth is 2m.32, two fourths and a tone (C\u2013F\u2013B\u266d\u2013C); the fifth is 2m.3.5 (C\u2013E\u2013G\u2013B\u2013C); etc. Genres 12 (2m.33.5), 13 (2m.32.52) and 14 (2m.3.53) are corrected versions of the diatonic, chromatic and enharmonic, respectively, of the Ancients. Genre 18 (2m.33.52) is the \"diatonico-chromatic\", \"used generally in all compositions\",[55] and which turns out to be identical with the system described by Johann Mattheson.[56] Euler later envisaged the possibility of describing genres including the prime number 7.[57]Even when dealing with music, Euler\u2019s approach is mainly mathematical. His writings on music are not particularly numerous (a few hundred pages, in his total production of about thirty thousand pages), but they reflect an early preoccupation and one that did not leave him throughout his life.[53]An Euler diagram is a diagrammatic means of representing sets and their relationships. Euler diagrams consist of simple closed curves (usually circles) in the plane that depict sets. Each Euler curve divides the plane into two regions or \"zones\": the interior, which symbolically represents the elements of the set, and the exterior, which represents all elements that are not members of the set. The sizes or shapes of the curves are not important; the significance of the diagram is in how they overlap. The spatial relationships between the regions bounded by each curve (overlap, containment or neither) corresponds to set-theoretic relationships (intersection, subset and disjointness). Curves whose interior zones do not intersect represent disjoint sets. Two curves whose interior zones intersect represent sets that have common elements; the zone inside both curves represents the set of elements common to both sets (the intersection of the sets). A curve that is contained completely within the interior zone of another represents a subset of it. Euler diagrams were incorporated as part of instruction in set theory as part of the new math movement in the 1960s. Since then, they have also been adopted by other curriculum fields such as reading.[52]Euler is also credited with using closed curves to illustrate syllogistic reasoning (1768). These diagrams have become known as Euler diagrams.[51]whereEuler is also well known in structural engineering for his formula giving the critical buckling load of an ideal strut, which depends only on its length and flexural stiffness:[50]whereIn 1757 he published an important set of equations for inviscid flow, that are now known as the Euler equations.[49] In differential form, the equations are:In addition, Euler made important contributions in optics. He disagreed with Newton's corpuscular theory of light in the Opticks, which was then the prevailing theory. His 1740s papers on optics helped ensure that the wave theory of light proposed by Christiaan Huygens would become the dominant mode of thought, at least until the development of the quantum theory of light.[48]Euler helped develop the Euler\u2013Bernoulli beam equation, which became a cornerstone of engineering. Aside from successfully applying his analytic tools to problems in classical mechanics, Euler also applied these techniques to celestial problems. His work in astronomy was recognized by a number of Paris Academy Prizes over the course of his career. His accomplishments include determining with great accuracy the orbits of comets and other celestial bodies, understanding the nature of comets, and calculating the parallax of the sun. His calculations also contributed to the development of accurate longitude tables.[47]One of Euler's more unusual interests was the application of mathematical ideas in music. In 1739 he wrote the Tentamen novae theoriae musicae, hoping to eventually incorporate musical theory as part of mathematics. This part of his work, however, did not receive wide attention and was once described as too mathematical for musicians and too musical for mathematicians.[46]Some of Euler's greatest successes were in solving real-world problems analytically, and in describing numerous applications of the Bernoulli numbers, Fourier series, Euler numbers, the constants e and \u03c0, continued fractions and integrals. He integrated Leibniz's differential calculus with Newton's Method of Fluxions, and developed tools that made it easier to apply calculus to physical problems. He made great strides in improving the numerical approximation of integrals, inventing what are now known as the Euler approximations. The most notable of these approximations are Euler's method and the Euler\u2013Maclaurin formula. He also facilitated the use of differential equations, in particular introducing the Euler\u2013Mascheroni constant:In 1735, Euler presented a solution to the problem known as the Seven Bridges of K\u00f6nigsberg.[41] The city of K\u00f6nigsberg, Prussia was set on the Pregel River, and included two large islands that were connected to each other and the mainland by seven bridges. The problem is to decide whether it is possible to follow a path that crosses each bridge exactly once and returns to the starting point. It is not possible: there is no Eulerian circuit. This solution is considered to be the first theorem of graph theory, specifically of planar graph theory.[41]Euler proved Newton's identities, Fermat's little theorem, Fermat's theorem on sums of two squares, and he made distinct contributions to Lagrange's four-square theorem. He also invented the totient function \u03c6(n), the number of positive integers less than or equal to the integer n that are coprime to n. Using properties of this function, he generalized Fermat's little theorem to what is now known as Euler's theorem. He contributed significantly to the theory of perfect numbers, which had fascinated mathematicians since Euclid. He proved that the relationship shown between perfect numbers and Mersenne primes earlier proved by Euclid was one-to-one, a result otherwise known as the Euclid\u2013Euler theorem. Euler also conjectured the law of quadratic reciprocity. The concept is regarded as a fundamental theorem of number theory, and his ideas paved the way for the work of Carl Friedrich Gauss.[39] By 1772 Euler had proved that 231\u00a0\u2212\u00a01 = 2,147,483,647 is a Mersenne prime. It may have remained the largest known prime until 1867.[40]Euler linked the nature of prime distribution with ideas in analysis. He proved that the sum of the reciprocals of the primes diverges. In doing so, he discovered the connection between the Riemann zeta function and the prime numbers; this is known as the Euler product formula for the Riemann zeta function.Euler's interest in number theory can be traced to the influence of Christian Goldbach, his friend in the St. Petersburg Academy. A lot of Euler's early work on number theory was based on the works of Pierre de Fermat. Euler developed some of Fermat's ideas and disproved some of his conjectures.Euler also pioneered the use of analytic methods to solve number theory problems. In doing so, he united two disparate branches of mathematics and introduced a new field of study, analytic number theory. In breaking ground for this new field, Euler created the theory of hypergeometric series, q-series, hyperbolic trigonometric functions and the analytic theory of continued fractions. For example, he proved the infinitude of primes using the divergence of the harmonic series, and he used analytic methods to gain some understanding of the way prime numbers are distributed. Euler's work in this area led to the development of the prime number theorem.[38]In addition, Euler elaborated the theory of higher transcendental functions by introducing the gamma function and introduced a new method for solving quartic equations. He also found a way to calculate integrals with complex limits, foreshadowing the development of modern complex analysis. He also invented the calculus of variations including its best-known result, the Euler\u2013Lagrange equation.De Moivre's formula is a direct consequence of Euler's formula.called \"the most remarkable formula in mathematics\" by Richard P. Feynman, for its single uses of the notions of addition, multiplication, exponentiation, and equality, and the single uses of the important constants 0, 1, e, i and \u03c0.[36] In 1988, readers of the Mathematical Intelligencer voted it \"the Most Beautiful Mathematical Formula Ever\".[37] In total, Euler was responsible for three of the top five formulae in that poll.[37]A special case of the above formula is known as Euler's identity,Euler introduced the use of the exponential function and logarithms in analytic proofs. He discovered ways to express various logarithmic functions using power series, and he successfully defined logarithms for negative and complex numbers, thus greatly expanding the scope of mathematical applications of logarithms.[33] He also defined the exponential function for complex numbers, and discovered its relation to the trigonometric functions. For any real number \u03c6 (taken to be radians), Euler's formula states that the complex exponential function satisfiesNotably, Euler directly proved the power series expansions for e and the inverse tangent function. (Indirect proof via the inverse power series technique was given by Newton and Leibniz between 1670 and 1680.) His daring use of power series enabled him to solve the famous Basel problem in 1735 (he provided a more elaborate argument in 1741):[35]The development of infinitesimal calculus was at the forefront of 18th-century mathematical research, and the Bernoullis\u2014family friends of Euler\u2014were responsible for much of the early progress in the field. Thanks to their influence, studying calculus became the major focus of Euler's work. While some of Euler's proofs are not acceptable by modern standards of mathematical rigour[35] (in particular his reliance on the principle of the generality of algebra), his ideas led to many great advances. Euler is well known in analysis for his frequent use and development of power series, the expression of functions as sums of infinitely many terms, such asEuler introduced and popularized several notational conventions through his numerous and widely circulated textbooks. Most notably, he introduced the concept of a function[3] and was the first to write f(x) to denote the function f applied to the argument x. He also introduced the modern notation for the trigonometric functions, the letter e for the base of the natural logarithm (now also known as Euler's number), the Greek letter \u03a3 for summations and the letter i to denote the imaginary unit.[33] The use of the Greek letter \u03c0 to denote the ratio of a circle's circumference to its diameter was also popularized by Euler, although it originated with Welsh mathematician William Jones.[34]Euler is the only mathematician to have two numbers named after him: the important Euler's number in calculus, e, approximately equal to 2.71828, and the Euler\u2013Mascheroni constant \u03b3 (gamma) sometimes referred to as just \"Euler's constant\", approximately equal to 0.57721. It is not known whether \u03b3 is rational or irrational.[32]Euler worked in almost all areas of mathematics, such as geometry, infinitesimal calculus, trigonometry, algebra, and number theory, as well as continuum physics, lunar theory and other areas of physics. He is a seminal figure in the history of mathematics; if printed, his works, many of which are of fundamental interest, would occupy between 60 and 80 quarto volumes.[5] Euler's name is associated with a large number of topics.Euler was buried next to Katharina at the Smolensk Lutheran Cemetery on Goloday Island. In 1785, the Russian Academy of Sciences put a marble bust of Leonhard Euler on a pedestal next to the Director's seat and, in 1837, placed a headstone on Euler's grave. To commemorate the 250th anniversary of Euler's birth, the headstone was moved in 1956, together with his remains, to the 18th-century necropolis at the Alexander Nevsky Monastery.In St. Petersburg on 18 September 1783, after a lunch with his family, Euler was discussing the newly discovered planet Uranus and its orbit with a fellow academician Anders Johan Lexell, when he collapsed from a brain hemorrhage. He died a few hours later.[29] Jacob von Staehlin-Storcksburg wrote a short obituary for the Russian Academy of Sciences and Russian mathematician Nicolas Fuss, one of Euler's disciples, wrote a more detailed eulogy,[30] which he delivered at a memorial meeting. In his eulogy for the French Academy, French mathematician and philosopher Marquis de Condorcet, wrote:Three years after his wife's death, Euler married her half-sister, Salome Abigail Gsell (1723\u20131794).[27] This marriage lasted until his death. In 1782 he was elected a Foreign Honorary Member of the American Academy of Arts and Sciences.[28]In 1760, with the Seven Years' War raging, Euler's farm in Charlottenburg was ransacked by advancing Russian troops. Upon learning of this event, General Ivan Petrovich Saltykov paid compensation for the damage caused to Euler's estate, later Empress Elizabeth of Russia added a further payment of 4000 roubles \u2013 an exorbitant amount at the time.[26] The political situation in Russia stabilized after Catherine the Great's accession to the throne, so in 1766 Euler accepted an invitation to return to the St. Petersburg Academy. His conditions were quite exorbitant \u2013 a 3000 ruble annual salary, a pension for his wife, and the promise of high-ranking appointments for his sons. All of these requests were granted. He spent the rest of his life in Russia. However, his second stay in the country was marred by tragedy. A fire in St. Petersburg in 1771 cost him his home, and almost his life. In 1773, he lost his wife Katharina after 40 years of marriage.Euler's eyesight worsened throughout his mathematical career. In 1738, three years after nearly expiring from fever, he became almost blind in his right eye, but Euler rather blamed the painstaking work on cartography he performed for the St. Petersburg Academy for his condition. Euler's vision in that eye worsened throughout his stay in Germany, to the extent that Frederick referred to him as \"Cyclops\". Euler later developed a cataract in his left eye, which was discovered in 1766. Just a few weeks after its discovery, he was rendered almost totally blind. However, his condition appeared to have little effect on his productivity, as he compensated for it with his mental calculation skills and exceptional memory. Upon losing the sight in both eyes, Euler remarked, \"Now I will have fewer distractions\".[24] For example, Euler could repeat the Aeneid of Virgil from beginning to end without hesitation, and for every page in the edition he could indicate which line was the first and which the last. With the aid of his scribes, Euler's productivity on many areas of study actually increased. He produced, on average, one mathematical paper every week in the year 1775.[5] The Eulers bore a double name, Euler-Sch\u00f6lpi, the latter of which derives from schelb and schief, signifying squint-eyed, cross-eyed, or crooked. This suggests that the Eulers may have had a susceptibility to eye problems.[25]Despite Euler's immense contribution to the Academy's prestige, he eventually incurred the ire of Frederick and ended up having to leave Berlin. The Prussian king had a large circle of intellectuals in his court, and he found the mathematician unsophisticated and ill-informed on matters beyond numbers and figures. Euler was a simple, devoutly religious man who never questioned the existing social order or conventional beliefs, in many ways the polar opposite of Voltaire, who enjoyed a high place of prestige at Frederick's court. Euler was not a skilled debater and often made it a point to argue subjects that he knew little about, making him the frequent target of Voltaire's wit.[20] Frederick also expressed disappointment with Euler's practical engineering abilities:In addition, Euler was asked to tutor Friederike Charlotte of Brandenburg-Schwedt, the Princess of Anhalt-Dessau and Frederick's niece. Euler wrote over 200 letters to her in the early 1760s, which were later compiled into a best-selling volume entitled Letters of Euler on different Subjects in Natural Philosophy Addressed to a German Princess.[21] This work contained Euler's exposition on various subjects pertaining to physics and mathematics, as well as offering valuable insights into Euler's personality and religious beliefs. This book became more widely read than any of his mathematical works and was published across Europe and in the United States. The popularity of the \"Letters\" testifies to Euler's ability to communicate scientific matters effectively to a lay audience, a rare ability for a dedicated research scientist.[20]Concerned about the continuing turmoil in Russia, Euler left St. Petersburg on 19 June 1741 to take up a post at the Berlin Academy, which he had been offered by Frederick the Great of Prussia. He lived for 25 years in Berlin, where he wrote over 380 articles. In Berlin, he published the two works for which he would become most renowned: the Introductio in analysin infinitorum, a text on functions published in 1748, and the Institutiones calculi differentialis,[19] published in 1755 on differential calculus.[20] In 1755, he was elected a foreign member of the Royal Swedish Academy of Sciences.On 7 January 1734, he married Katharina Gsell (1707\u20131773), a daughter of Georg Gsell, a painter from the Academy Gymnasium.[17] The young couple bought a house by the Neva River. Of their thirteen children, only five survived childhood.[18]Conditions improved slightly after the death of Peter II, and Euler swiftly rose through the ranks in the academy and was made a professor of physics in 1731. Two years later, Daniel Bernoulli, who was fed up with the censorship and hostility he faced at Saint Petersburg, left for Basel. Euler succeeded him as the head of the mathematics department.[16]The Academy's benefactress, Catherine I, who had continued the progressive policies of her late husband, died on the day of Euler's arrival. The Russian nobility then gained power upon the ascension of the twelve-year-old Peter II. The nobility was suspicious of the academy's foreign scientists, and thus cut funding and caused other difficulties for Euler and his colleagues.The Academy at Saint Petersburg, established by Peter the Great, was intended to improve education in Russia and to close the scientific gap with Western Europe. As a result, it was made especially attractive to foreign scholars like Euler. The academy possessed ample financial resources and a comprehensive library drawn from the private libraries of Peter himself and of the nobility. Very few students were enrolled in the academy in order to lessen the faculty's teaching burden, and the academy emphasized research and offered to its faculty both the time and the freedom to pursue scientific questions.[11]Euler arrived in Saint Petersburg on 17 May 1727. He was promoted from his junior post in the medical department of the academy to a position in the mathematics department. He lodged with Daniel Bernoulli with whom he often worked in close collaboration. Euler mastered Russian and settled into life in Saint Petersburg. He also took on an additional job as a medic in the Russian Navy.[15]Around this time Johann Bernoulli's two sons, Daniel and Nicolaus, were working at the Imperial Russian Academy of Sciences in Saint Petersburg. On 31 July 1726, Nicolaus died of appendicitis after spending less than a year in Russia,[12][13] and when Daniel assumed his brother's position in the mathematics/physics division, he recommended that the post in physiology that he had vacated be filled by his friend Euler. In November 1726 Euler eagerly accepted the offer, but delayed making the trip to Saint Petersburg while he unsuccessfully applied for a physics professorship at the University of Basel.[14]In 1726, Euler completed a dissertation on the propagation of sound with the title De Sono.[10] At that time, he was unsuccessfully attempting to obtain a position at the University of Basel. In 1727, he first entered the Paris Academy Prize Problem competition; the problem that year was to find the best way to place the masts on a ship. Pierre Bouguer, who became known as \"the father of naval architecture\", won and Euler took second place. Euler later won this annual prize twelve times.[11]Euler's formal education started in Basel, where he was sent to live with his maternal grandmother. In 1720, aged thirteen, he enrolled at the University of Basel, and in 1723, he received a Master of Philosophy with a dissertation that compared the philosophies of Descartes and Newton. During that time, he was receiving Saturday afternoon lessons from Johann Bernoulli, who quickly discovered his new pupil's incredible talent for mathematics.[9] At that time Euler's main studies included theology, Greek, and Hebrew at his father's urging in order to become a pastor, but Bernoulli convinced his father that Leonhard was destined to become a great mathematician.Leonhard Euler was born on 15 April 1707, in Basel, Switzerland to Paul III Euler, a pastor of the Reformed Church, and Marguerite n\u00e9e Brucker, a pastor's daughter. He had two younger sisters: Anna Maria and Maria Magdalena, and a younger brother Johann Heinrich.[8] Soon after the birth of Leonhard, the Eulers moved from Basel to the town of Riehen, where Euler spent most of his childhood. Paul Euler was a friend of the Bernoulli family; Johann Bernoulli was then regarded as Europe's foremost mathematician, and would eventually be the most important influence on young Leonhard.A statement attributed to Pierre-Simon Laplace expresses Euler's influence on mathematics: \"Read Euler, read Euler, he is the master of us all.\"[6][7]Euler was one of the most eminent mathematicians of the 18th century and is held to be one of the greatest in history. He is also widely considered to be the most prolific mathematician of all time. His collected works fill 60 to 80 quarto volumes,[5] more than anybody in the field. He spent most of his adult life in Saint Petersburg, Russia, and in Berlin, then the capital of Prussia.",
            "title": "Leonhard Euler",
            "url": "https://en.wikipedia.org/wiki/Leonhard_Euler"
        },
        {
            "desc_links": [
                "/wiki/Special_relativity",
                "/wiki/Speed_of_light",
                "/wiki/Quantum_mechanics",
                "/wiki/Molecules",
                "/wiki/Rotational_spectroscopy#Classification_of_molecules_based_on_rotational_behavior",
                "/wiki/Physics",
                "/wiki/Physical_body",
                "/wiki/Deformation_(engineering)",
                "/wiki/Distance",
                "/wiki/Point_(geometry)",
                "/wiki/Force"
            ],
            "links": [
                "/wiki/Configuration_space_(physics)",
                "/wiki/Manifold",
                "/wiki/Rotation_group_SO(3)",
                "/wiki/Euclidean_group#Direct_and_indirect_isometries",
                "/wiki/Euclidean_group#Direct_and_indirect_isometries",
                "/wiki/Euclidean_group",
                "/wiki/Translation_(geometry)",
                "/wiki/Rotation",
                "/wiki/Through_and_through",
                "/wiki/Equality_(objects)",
                "/wiki/Proper_rotation",
                "/wiki/Chirality_(mathematics)",
                "/wiki/Mirror_image",
                "/wiki/Symmetry",
                "/wiki/Symmetry_group",
                "/wiki/Point_groups_in_three_dimensions",
                "/wiki/Vehicle",
                "/wiki/Winding_number",
                "/wiki/Polygon#Angles",
                "/wiki/Coordinate_system",
                "/wiki/Time_derivative",
                "/wiki/Time_derivative",
                "/wiki/Angular_velocity",
                "/wiki/Vector_(geometry)",
                "/wiki/Angular_speed",
                "/wiki/Axis_of_rotation",
                "/wiki/Euler%27s_rotation_theorem",
                "/wiki/Angular_velocity",
                "/wiki/Axis_of_rotation",
                "/wiki/Time_derivative",
                "/wiki/Derivative",
                "/wiki/Velocity",
                "/wiki/Vector_(geometry)",
                "/wiki/Time_derivative",
                "/wiki/Velocity",
                "/wiki/Motion_(physics)",
                "/wiki/Axis_of_rotation",
                "/wiki/Velocity",
                "/wiki/Angular_velocity",
                "/wiki/Frame_of_reference",
                "/wiki/Translation_(physics)",
                "/wiki/Rotation",
                "/wiki/Coordinate_system#Examples",
                "/wiki/Position_vector",
                "/wiki/Space#Classical_mechanics",
                "/wiki/Coordinate_system",
                "/wiki/Center_of_mass",
                "/wiki/Centroid",
                "/wiki/Coordinate_system",
                "/wiki/Kinematics",
                "/wiki/Dynamics_(mechanics)",
                "/wiki/Velocity",
                "/wiki/Acceleration",
                "/wiki/Momentum",
                "/wiki/Impulse_(physics)",
                "/wiki/Kinetic_energy",
                "/wiki/Coordinate_system#Examples",
                "/wiki/Collinear",
                "/wiki/Time-invariant",
                "/wiki/Special_relativity",
                "/wiki/Speed_of_light",
                "/wiki/Quantum_mechanics",
                "/wiki/Molecules",
                "/wiki/Rotational_spectroscopy#Classification_of_molecules_based_on_rotational_behavior",
                "/wiki/Physics",
                "/wiki/Physical_body",
                "/wiki/Deformation_(engineering)",
                "/wiki/Distance",
                "/wiki/Point_(geometry)",
                "/wiki/Force"
            ],
            "text": "The configuration space of a rigid body with one point fixed (i.e., a body with zero translational motion) is given by the underlying manifold of the rotation group SO(3). The configuration space of a nonfixed (with non-zero translational motion) rigid body is E+(3), the subgroup of direct isometries of the Euclidean group in three dimensions (combinations of translations and rotations).A sheet with a through and through image is achiral. We can distinguish again two cases:For a (rigid) rectangular transparent sheet, inversion symmetry corresponds to having on one side an image without rotational symmetry and on the other side an image such that what shines through is the image at the top side, upside down. We can distinguish two cases:Two rigid bodies are said to be different (not copies) if there is no proper rotation from one to the other. A rigid body is called chiral if its mirror image is different in that sense, i.e., if it has either no symmetry or its symmetry group contains only proper rotations. In the opposite case an object is called achiral: the mirror image is a copy, not a different object. Such an object may have a symmetry plane, but not necessarily: there may also be a plane of reflection with respect to which the image of the object is a rotated version. The latter applies for S2n, of which the case n = 1 is inversion symmetry.When the center of mass is used as reference point:However, depending on the application, a convenient choice may be:Any point that is rigidly connected to the body can be used as reference point (origin of coordinate system L) to describe the linear motion of the body (the linear position, velocity and acceleration vectors depend on the choice).Vehicles, walking people, etc., usually rotate according to changes in the direction of the velocity: they move forward with respect to their own orientation. Then, if the body follows a closed orbit in a plane, the angular velocity integrated over a time interval in which the orbit is completed once, is an integer times 360\u00b0. This integer is the winding number with respect to the origin of the velocity. Compare the amount of rotation associated with the vertices of a polygon.In 2D, the angular velocity is a scalar, and matrix A(t) simply represents a rotation in the xy-plane by an angle which is the integral of the angular velocity over time.whereIf C is the origin of a local coordinate system L, attached to the body,where Q is the point fixed in B that instantaneously coincident with R at the instant of interest.[7] This equation is often combined with Acceleration of two points fixed on a rigid body.The acceleration in reference frame N of the point R moving in body B while B is moving in frame N is given bywhere Q is the point fixed in B that is instantaneously coincident with R at the instant of interest.[7] This relation is often combined with the relation for the Velocity of two points fixed on a rigid body.If the point R is moving in rigid body B while B moves in reference frame N, then the velocity of R in N isBy differentiating the equation for the Velocity of two points fixed on a rigid body in N with respect to time, the acceleration in reference frame N of a point Q fixed on a rigid body B can be expressed asThe acceleration of point P in reference frame N is defined as the time derivative in N of its velocity:[5]where O is any arbitrary point fixed in reference frame N, and the N to the left of the d/dt operator indicates that the derivative is taken in reference frame N. The result is independent of the selection of O so long as O is fixed in N.The velocity of point P in reference frame N is defined as the time derivative in N of the position vector from O to P:[5]For any set of three points P, Q, and R, the position vector from P to R is the sum of the position vector from P to Q and the position vector from Q to R:In this case, rigid bodies and reference frames are indistinguishable and completely interchangeable.The angular velocity of a rigid body B in a reference frame N is equal to the sum of the angular velocity of a rigid body D in N and the angular velocity of B with respect to D:[4]Angular velocity is a vector quantity that describes the angular speed at which the orientation of the rigid body is changing and the instantaneous axis about which it is rotating (the existence of this instantaneous axis is guaranteed by the Euler's rotation theorem). All points on a rigid body experience the same angular velocity at all times. During purely rotational motion, all points on the body change position except for those lying on the instantaneous axis of rotation. The relationship between orientation and angular velocity is not directly analogous to the relationship between position and velocity. Angular velocity is not the time rate of change of orientation, because there is no such concept as an orientation vector that can be differentiated to obtain the angular velocity.The linear velocity of a rigid body is a vector quantity, equal to the time rate of change of its linear position. Thus, it is the velocity of a reference point fixed to the body. During purely translational motion (motion with no rotation), all points on a rigid body move with the same velocity. However, when motion involves rotation, the instantaneous velocity of any two points on the body will generally not be the same. Two points of a rotating body will have the same instantaneous velocity only if they happen to lie on an axis parallel to the instantaneous axis of rotation.Velocity (also called linear velocity) and angular velocity are measured with respect to a frame of reference.In general, when a rigid body moves, both its position and orientation vary with time. In the kinematic sense, these changes are referred to as translation and rotation, respectively. Indeed, the position of a rigid body can be viewed as a hypothetic translation and rotation (roto-translation) of the body starting from a hypothetic reference position (not necessarily coinciding with a position actually taken by the body during its motion).The linear position can be represented by a vector with its tail at an arbitrary reference point in space (the origin of a chosen coordinate system) and its tip at an arbitrary point of interest on the rigid body, typically coinciding with its center of mass or centroid. This reference point may define the origin of a coordinate system fixed to the body.Thus, the position of a rigid body has two components: linear and angular, respectively.[2] The same is true for other kinematic and kinetic quantities describing the motion of a rigid body, such as linear and angular velocity, acceleration, momentum, impulse, and kinetic energy.[3]The position of a rigid body is the position of all the particles of which it is composed. To simplify the description of this position, we exploit the property that the body is rigid, namely that all its particles maintain the same distance relative to each other. If the body is rigid, it is sufficient to describe the position of at least three non-collinear particles. This makes it possible to reconstruct the position of all the other particles, provided that their time-invariant position relative to the three selected particles is known. However, typically a different, mathematically more convenient, but equivalent approach is used. The position of the whole body is represented by:In the study of special relativity, a perfectly rigid body does not exist; and objects can only be assumed to be rigid if they are not moving near the speed of light. In quantum mechanics a rigid body is usually thought of as a collection of point masses. For instance, in quantum mechanics molecules (consisting of the point masses: electrons and nuclei) are often seen as rigid bodies (see classification of molecules as rigid rotors).In physics, a rigid body is a solid body in which deformation is zero or so small it can be neglected. The distance between any two given points on a rigid body remains constant in time regardless of external forces exerted on it. A rigid body is usually considered as a continuous distribution of mass.",
            "title": "Rigid body",
            "url": "https://en.wikipedia.org/wiki/Rigid_body"
        },
        {
            "desc_links": [
                "/wiki/Diagonal_matrix",
                "/wiki/Rigid_body",
                "/wiki/Tensor",
                "/wiki/Torque",
                "/wiki/Angular_acceleration",
                "/wiki/Intensive_and_extensive_properties"
            ],
            "links": [
                "/wiki/Ellipsoid",
                "/wiki/Poinsot%27s_ellipsoid",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Sylvester%27s_law_of_inertia",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Unit_vector",
                "/wiki/Jacobi_identity",
                "/wiki/Cross_product",
                "/wiki/Kinematics",
                "/wiki/Centre_of_mass",
                "/wiki/Centre_of_mass",
                "/wiki/Cross_product#Conversion_to_matrix_multiplication",
                "/wiki/Centre_of_mass",
                "/wiki/Kinematics",
                "/wiki/Resultant_force",
                "/wiki/Moment_(physics)",
                "/wiki/Centre_of_mass",
                "/wiki/Mechanical_system",
                "/wiki/List_of_moments_of_inertia",
                "/wiki/Parallel_axis_theorem",
                "/wiki/Second_moment_of_area",
                "/wiki/Beam_(structure)",
                "/wiki/Multiple_integral",
                "/wiki/Kater%27s_pendulum",
                "/wiki/Gravimeter",
                "/wiki/Angular_momentum",
                "/wiki/Angular_velocity",
                "/wiki/Radius_of_gyration",
                "/wiki/Simple_pendulum",
                "/wiki/Rotation_around_a_fixed_axis",
                "/wiki/Torque",
                "/wiki/Angular_acceleration",
                "/wiki/Figure_skating_spins",
                "/wiki/Diving",
                "/wiki/Diving#Dive_positions",
                "/wiki/Angular_momentum",
                "/wiki/Angular_velocity",
                "/wiki/Angular_momentum",
                "/wiki/Kinetic_energy",
                "/wiki/Rigid_body_dynamics",
                "/wiki/Christiaan_Huygens",
                "/wiki/Compound_pendulum",
                "/wiki/Leonhard_Euler",
                "/wiki/Euler%27s_laws#Euler's_second_law",
                "/wiki/Torque",
                "/wiki/Angular_momentum",
                "/wiki/Angular_acceleration",
                "/wiki/Angular_velocity",
                "/wiki/SI",
                "/wiki/Imperial_units",
                "/wiki/United_States_customary_units",
                "/wiki/Diagonal_matrix"
            ],
            "text": "Thus, the magnitude of a point x in the direction n on the inertia ellipsoid isLet a point x on this ellipsoid be defined in terms of its magnitude and direction, x = |x|n, where n is a unit vector. Then the relationship presented above, between the inertia matrix and the scalar moment of inertia In around an axis in the direction n, yieldsto see that the semi-principal diameters of this ellipsoid are given bydefines an ellipsoid in the body frame. Write this equation in the form,orThe moment of inertia matrix in body-frame coordinates is a quadratic form that defines a surface in the body called Poinsot's ellipsoid.[28] Let \u039b be the inertia matrix relative to the centre of mass aligned with the principal axes, then the surfaceFor bodies with constant density an axis of rotational symmetry is a principal axis.The columns of the rotation matrix Q define the directions of the principal axes of the body, and the constants I1, I2, and I3 are called the principal moments of inertia. This result was first shown by J. J. Sylvester (1852), and is a form of Sylvester's law of inertia.[26][27]whereMeasured in the body frame the inertia matrix is a constant real symmetric matrix. A real symmetric matrix has the eigendecomposition into the product of a rotation matrix Q and a diagonal matrix \u039b, given byNotice that A changes as the body moves, while IB\nC remains constant.where vectors y in the body fixed coordinate frame have coordinates x in the inertial frame. Then, the inertia matrix of the body measured in the inertial frame is given byLet the body frame inertia matrix relative to the centre of mass be denoted IB\nC, and define the orientation of the body frame relative to the inertial frame by the rotation matrix A, such that,The use of the inertia matrix in Newton's second law assumes its components are computed relative to axes parallel to the inertial frame and not relative to a body-fixed reference frame.[6][23] This means that as the body moves the components of the inertia matrix change with time. In contrast, the components of the inertia matrix measured in a body-fixed frame are constant.It is common in rigid body mechanics to use notation that explicitly identifies the x, y, and z axes, such as Ixx and Ixy, for the components of the inertia tensor.The components of tensors of degree two can be assembled into a matrix. For the inertia tensor this matrix is given by,and can be interpreted as the moment of inertia around the x-axis when the object rotates around the y-axis.where the dot product is taken with the corresponding elements in the component tensors. A product of inertia term such as I12 is obtained by the computationThe inertia tensor can be used in the same way as the inertia matrix to compute the scalar moment of inertia about an arbitrary axis in the direction n,where r defines the coordinates of a point in the body and \u03c1(r) is the mass density at that point. The integral is taken over the volume V of the body. The inertia tensor is symmetric because Iij =\u00a0Iji.The inertia tensor for a continuous body is given byIn this case, the components of the inertia tensor are given bywhere E is the identity tensorFor a rigid system of particles Pk, k = 1, \u2026, N each of mass mk with position coordinates rk\u00a0=\u00a0(xk, yk, zk), the inertia tensor is given byThis tensor is of degree two because the component tensors are each constructed from two basis vectors. In this form the inertia tensor is also called the inertia binor.where ei, i\u00a0=\u00a01,\u00a02,\u00a03 are the three orthogonal unit vectors defining the inertial frame in which the body moves. Using this basis the inertia tensor is given byThis shows that the inertia matrix can be used to calculate the moment of inertia of a body around any specified rotation axis in the body.where IR is the moment of inertia matrix of the system relative to the reference point R.Thus, the moment of inertia around the line L through R in the direction k\u0302 is obtained from the calculationwhere the dot and the cross products have been interchanged. Exchanging products, and simplifying by noting that \u0394ri and k\u0302 are orthogonal:The simplification of this equation uses the triple scalar product identityThe magnitude squared of the perpendicular vector isnoting that k\u0302 is a unit vector.To relate this scalar moment of inertia to the inertia matrix of the body, introduce the skew-symmetric matrix [k\u0302] such that [k\u0302]y = k\u0302 \u00d7 y, then we have the identitywhere E is the identity matrix, so as to avoid confusion with the inertia matrix, and k\u0302\u2009k\u0302T is the outer product matrix formed from the unit vector k\u0302 along the line\u00a0L.This is derived as follows. Let a rigid assembly of N particles, Pi, i = 1, \u2026, N, have coordinates ri. Choose R as a reference point and compute the moment of inertia around a line L defined by the unit vector k\u0302 through the reference point R, L(t) = R + tk\u0302. The perpendicular vector from this line to the particle Pi is obtained from \u0394ri by removing the component that projects onto k\u0302.where IR is the moment of inertia matrix of the system relative to the reference point R, and [\u0394ri] is the skew symmetric matrix obtained from the vector \u0394ri = ri \u2212 R.The scalar moment of inertia, IL, of a body about a specified axis whose direction is specified by the unit vector k\u0302 and passes through the body at a point R is as follows:[6]Note on the minus sign: By using the skew symmetric matrix of position vectors relative to the reference point, the inertia matrix of each particle has the form \u2212m[r]2, which is similar to the mr2 that appears in planar movement. However, to make this to work out correctly a minus sign is needed. This minus sign can be absorbed into the term m[r]T[r], if desired, by using the skew-symmetry property of [r].where d is the vector from the centre of mass C to the reference point R.The result is the parallel axis theorem,The first term is the inertia matrix IC relative to the centre of mass. The second and third terms are zero by definition of the centre of mass C. And the last term is the total mass of the system multiplied by the square of the skew-symmetric matrix [d] constructed from d.Distribute over the cross product to obtainwhere d is the vector from the centre of mass C to the reference point R. Use this equation to compute the inertia matrix,Let C be the centre of mass of the rigid system, thenConsider the inertia matrix IR obtained for a rigid system of particles measured relative to a reference point R, given byThe inertia matrix of a body depends on the choice of the reference point. There is a useful relationship between the inertia matrix relative to the centre of mass C and the inertia matrix relative to another point R. This relationship is called the parallel axis theorem.[3][6]where IC is the inertia matrix relative to the centre of mass.Thus, the resultant torque on the rigid system of particles is given byobtained from the Jacobi identity for the triple cross product as shown in the proof below:The calculation uses the identityUse the centre of mass C as the reference point, and introduce the skew-symmetric matrix [\u0394ri] = [ri \u2212 C] to represent the cross product (ri \u2212 C) \u00d7, to obtainwhere ai is the acceleration of the particle Pi. The kinematics of a rigid body yields the formula for the acceleration of the particle Pi in terms of the position R and acceleration Ar of the reference point, as well as the angular velocity vector \u03c9 and angular acceleration vector \u03b1 of the rigid system as,The inertia matrix appears in the application of Newton's second law to a rigid assembly of particles. The resultant torque on this system is,[3][6]where IC is the inertia matrix relative to the centre of mass and M is the total mass.Thus, the kinetic energy of the rigid system of particles is given byThe second term in this equation is zero because C is the centre of mass. Introduce the skew-symmetric matrix [\u0394ri] so the kinetic energy becomesThis equation expands to yield three termswhere \u0394ri = ri\u00a0\u2212\u00a0C is the position vector of a particle relative to the centre of mass.The kinetic energy of a rigid system of particles can be formulated in terms of the centre of mass and a matrix of mass moments of inertia of the system. Let the system of particles Pi, i = 1, \u2026,n be located at the coordinates ri with velocities vi, then the kinetic energy is[3][6]is the symmetric inertia matrix of the rigid system of particles measured relative to the centre of mass C.where IC defined byThen, the skew-symmetric matrix [\u0394ri] obtained from the relative position vector \u0394ri = ri \u2212 C, can be used to define,where the terms containing VR (= C) sum to zero by the definition of centre of mass.The inertia matrix is constructed by considering the angular momentum, with the reference point R of the body chosen to be the centre of mass C:[3][6]Note that the cross product can be equivalently written as matrix multiplication by combining the first operand and the operator into a, skew-symmetric, matrix, [b], constructed from the components of b = (bx, by, bz):where \u03c9 is the angular velocity of the system, and VR is the velocity of R.and the (absolute) velocities areLet the system of particles Pi, i = 1, \u2026, n be located at the coordinates ri with velocities vi relative to a fixed reference frame. For a (possibly moving) reference point R, the relative positions areThe scalar moments of inertia appear as elements in a matrix when a system of particles is assembled into a rigid body that moves in three-dimensional space. This inertia matrix appears in the calculation of the angular momentum, kinetic energy and resultant torque of the rigid system of particles.[3][4][5][6][25]Use the centre of mass C as the reference point and define the moment of inertia relative to the centre of mass IC, then the equation for the resultant torque simplifies to[18]:1029where \u00eai \u00d7 \u00eai = 0, and \u00eai \u00d7 t\u0302i = k\u0302 is the unit vector perpendicular to the plane for all of the particles Pi.This yields the resultant torque on the system asFor systems that are constrained to planar movement, the angular velocity and angular acceleration vectors are directed along k\u0302 perpendicular to the plane of movement, which simplifies this acceleration equation. In this case, the acceleration vectors can be simplified by introducing the unit vectors \u00eai from the reference point R to a point ri and the unit vectors t\u0302i = k\u0302 \u00d7 \u00eai , soThe kinematics of a rigid body yields the formula for the acceleration of the particle Pi in terms of the position R and acceleration A of the reference particle as well as the angular velocity vector \u03c9 and angular acceleration vector \u03b1 of the rigid system of particles as,where ri denotes the trajectory of each particle.Newton's laws for a rigid system of N particles, Pi, i = 1, \u2026 N, can be written in terms of a resultant force and torque at a reference point R, to yield[14][17]The moment of inertia IC is the polar moment of inertia of the body.Let the reference point be the centre of mass C of the system so the second term becomes zero, and introduce the moment of inertia IC so the kinetic energy is given by[18]:1084The kinetic energy of a rigid system of particles moving in the plane is given by[14][17]For a given amount of angular momentum, a decrease in the moment of inertia results in an increase in the angular velocity. Figure skaters can change their moment of inertia by pulling in their arms. Thus, the angular velocity achieved by a skater with outstretched arms results in a greater angular velocity when the arms are pulled in, because of the reduced moment of inertia. A figure skater is not, however, a rigid body.The moment of inertia IC about an axis perpendicular to the movement of the rigid system and through the centre of mass is known as the polar moment of inertia. Specifically, it is the second moment of mass with respect to the orthogonal distance from an axis (or pole).then the equation for angular momentum simplifies to[18]:1028and define the moment of inertia relative to the centre of mass IC asUse the centre of mass C as the reference point soThe angular momentum vector for the planar movement of a rigid system of particles is given by[14][17]Note on the cross product: When a body moves parallel to a ground plane, the trajectories of all the points in the body lie in planes parallel to this ground plane. This means that any rotation that the body undergoes must be around an axis perpendicular to this plane. Planar movement is often presented as projected onto this ground plane so that the axis of rotation appears as a point. In this case, the angular velocity and angular acceleration of the body are scalars and the fact that they are vectors along the rotation axis is ignored. This is usually preferred for introductions to the topic. But in the case of moment of inertia, the combination of mass and geometry benefits from the geometric properties of the cross product. For this reason, in this section on planar movement the angular velocity and accelerations of the body are vectors perpendicular to the ground plane, and the cross product operations are the same as used for the study of spatial rigid body movement.This defines the relative position vector and the velocity vector for the rigid system of the particles moving in a plane.For planar movement the angular velocity vector is directed along the unit vector k which is perpendicular to the plane of movement. Introduce the unit vectors ei from the reference point R to a point ri , and the unit vector t\u0302i = k\u0302 \u00d7 \u00eai sowhere \u03c9 is the angular velocity of the system and V is the velocity of R.If a system of n particles, Pi, i = 1, \u2026, n, are assembled into a rigid body, then the momentum of the system can be written in terms of positions relative to a reference point R, and absolute velocities viIf a mechanical system is constrained to move parallel to a fixed plane, then the rotation of a body in the system occurs around an axis k\u0302 perpendicular to this plane. In this case, the moment of inertia of the mass in this system is a scalar known as the polar moment of inertia. The definition of the polar moment of inertia can be obtained by considering momentum, kinetic energy and Newton's laws for the planar movement of a rigid system of particles.[14][17][23][24]where m = 4/3\u03c0R3\u03c1 is the mass of the sphere.Therefore, the moment of inertia of the ball is the sum of the moments of inertia of the discs along the z-axis,then the radius r of the disc at the cross-section z along the z-axis isAs one more example, consider the moment of inertia of a solid sphere of constant density about an axis through its centre of mass. This is determined by summing the moments of inertia of the thin discs that form the sphere. If the surface of the ball is defined by the equation[18]:1301A list of moments of inertia formulas for standard body shapes provides a way to obtain the moment of inertia of a complex body as an assembly of simpler shaped bodies. The parallel axis theorem is used to shift the reference point of the individual bodies to the reference point of the assembly.The moment of inertia of a compound pendulum constructed from a thin disc mounted at the end of a thin rod that oscillates around a pivot at the other end of the rod, begins with the calculation of the moment of inertia of the thin rod and thin disc about their respective centres of mass.[18]Note on second moment of area: The moment of inertia of a body moving in a plane and the second moment of area of a beam's cross-section are often confused. The moment of inertia of body with the shape of the cross-section is the second moment of this area about the z-axis perpendicular to the cross-section, weighted by its density. This is also called the polar moment of the area, and is the sum of the second moments about the x- and y-axes.[22] The stresses in a beam are calculated using the second moment of the cross-sectional area around either the x-axis or y-axis depending on the load.Here, the function \u03c1 gives the mass density at each point (x, y, z), r is a vector perpendicular to the axis of rotation and extending from a point on the rotation axis to a point (x, y, z) in the solid, and the integration is evaluated over the volume\u00a0V of the body\u00a0Q. The moment of inertia of a flat surface is similar with the mass density being replaced by its areal mass density with the integral evaluated over its area.Another expression replaces the summation with an integral,The moment of inertia of a continuous body rotating about a specified axis is calculated in the same way, except with infinitely many point particles. Thus the limits of summation are removed, and the sum is written as follows:Thus, moment of inertia is a physical property that combines the mass and distribution of the particles around the rotation axis. Notice that rotation about different axes of the same body yield different moments of inertia.This shows that the moment of inertia of the body is the sum of each of the mr2 terms, that isConsider the kinetic energy of an assembly of N masses mi that lie at the distances ri from the pivot point P, which is the nearest point on the axis of rotation. It is the sum of the kinetic energy of the individual masses,[17]:516\u2013517[18]:1084\u20131085 [18]:1296\u20131300The moment of inertia about an axis of a body is calculated by summing mr2 for every particle in the body, where r is the perpendicular distance to the specified axis. To see how moment of inertia arises in the study of the movement of an extended body, it is convenient to consider a rigid assembly of point masses. (This equation can be used for axes that are not principal axes provided that it is understood that this does not fully describe the moment of inertia.[21])The moment of inertia of a complex system such as a vehicle or airplane around its vertical axis can be measured by suspending the system from three points to form a trifilar pendulum. A trifilar pendulum is a platform supported by three wires designed to oscillate in torsion around its vertical centroidal axis.[19] The period of oscillation of the trifilar pendulum yields the moment of inertia of the system.[20]Notice that the distance to the center of oscillation of the seconds pendulum must be adjusted to accommodate different values for the local acceleration of gravity. Kater's pendulum is a compound pendulum that uses this property to measure the local acceleration of gravity, and is called a gravimeter.orThis shows that the quantity I = mr2 is how mass combines with the shape of a body to define rotational inertia. The moment of inertia of an arbitrarily shaped body is the sum of the values mr2 for all of the elements of mass in the body.Similarly, the kinetic energy of the pendulum mass is defined by the velocity of the pendulum around the pivot to yieldusing a similar derivation the previous equation.The quantity I = mr2 also appears in the angular momentum of a simple pendulum, which is calculated from the velocity v = \u03c9 \u00d7 r of the pendulum mass around the pivot, where \u03c9 is the angular velocity of the mass about the pivot point. This angular momentum is given byMoment of inertia can be measured using a simple pendulum, because it is the resistance to the rotation caused by gravity. Mathematically, the moment of inertia of the pendulum is the ratio of the torque due to gravity about the pivot of a pendulum to its angular acceleration about that pivot point. For a simple pendulum this is found to be the product of the mass of the particle m with the square of its distance r to the pivot, that iswhere k is known as the radius of gyration.In general, given an object of mass m, an effective radius k can be defined for an axis through its center of mass, with such a value that its moment of inertia isThis simple formula generalizes to define moment of inertia for an arbitrarily shaped body as the sum of all the elemental point masses dm each multiplied by the square of its perpendicular distance r to an axis k\u0302.Thus, moment of inertia depends on both the mass m of a body and its geometry, or shape, as defined by the distance r to the axis of rotation.For a simple pendulum, this definition yields a formula for the moment of inertia I in terms of the mass m of the pendulum and its distance r from the pivot point as,If the shape of the body does not change, then its moment of inertia appears in Newton's law of motion as the ratio of an applied torque \u03c4 on a body to the angular acceleration \u03b1 around a principal axis, that isIf the angular momentum of a system is constant, then as the moment of inertia gets smaller, the angular velocity must increase. This occurs when spinning figure skaters pull in their outstretched arms or divers curl their bodies into a tuck position during a dive, to spin faster.[7][8][9][10][11][12][13]Moment of inertia I is defined as the ratio of the net angular momentum L of a system to its angular velocity \u03c9 around a principal axis,[7][8] that isThe moment of inertia of a rotating flywheel is used in a machine to resist variations in applied torque to smooth its rotational output. The moment of inertia of an airplane about its longitudinal, horizontal and vertical axis determines how steering forces on the control surfaces of its wings, elevators and tail affect the plane in roll, pitch and yaw.Moment of inertia also appears in momentum, kinetic energy, and in Newton's laws of motion for a rigid body as a physical parameter that combines its shape and mass. There is an interesting difference in the way moment of inertia appears in planar and spatial movement. Planar movement has a single scalar that defines the moment of inertia, while for spatial movement the same calculations yield a 3\u00a0\u00d7\u00a03 matrix of moments of inertia, called the inertia matrix or inertia tensor.[5][6]The natural frequency of oscillation of a compound pendulum is obtained from the ratio of the torque imposed by gravity on the mass of the pendulum to the resistance to acceleration defined by the moment of inertia. Comparison of this natural frequency to that of a simple pendulum consisting of a single point of mass provides a mathematical formulation for moment of inertia of an extended body.[3][4]In 1673 Christiaan Huygens introduced this parameter in his study of the oscillation of a body hanging from a pivot, known as a compound pendulum.[1] The term moment of inertia was introduced by Leonhard Euler in his book Theoria motus corporum solidorum seu rigidorum in 1765,[1][2] and it is incorporated into Euler's second law.When a body is rotating, or free to rotate, around an axis, a torque must be applied to change its angular momentum. The amount of torque needed to cause any given angular acceleration (the rate of change in angular velocity) is proportional to the moment of inertia of the body. Moment of inertia may be expressed in units of kilogram metre squared (kg\u00b7m2) in SI units and pound-foot-second squared (lb\u00b7ft\u00b7s2) in imperial or US units.For bodies constrained to rotate in a plane, it is sufficient to consider their moment of inertia about an axis perpendicular to the plane. For bodies free to rotate in three dimensions, their moments can be described by a symmetric 3\u00a0\u00d7\u00a03 matrix; each body has a set of mutually perpendicular principal axes for which this matrix is diagonal and torques around the axes act independently of each other.",
            "title": "Moment of inertia",
            "url": "https://en.wikipedia.org/wiki/Principal_axis_(mechanics)"
        },
        {
            "desc_links": [
                "/wiki/History_of_the_metric_system",
                "/wiki/French_First_Republic",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Bureau_des_Longitudes",
                "/wiki/S%C3%A9nat_conservateur",
                "/wiki/Leonhard_Euler",
                "/wiki/Jean_le_Rond_d%27Alembert",
                "/wiki/Prussian_Academy_of_Sciences",
                "/wiki/Prussia",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Analytical_mechanics",
                "/wiki/M%C3%A9canique_analytique",
                "/wiki/Isaac_Newton",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French",
                "/wiki/Wikipedia:Verifiability",
                "/wiki/Enlightenment_Era",
                "/wiki/Mathematician",
                "/wiki/Astronomer",
                "/wiki/Mathematical_analysis",
                "/wiki/Number_theory",
                "/wiki/Classical_mechanics",
                "/wiki/Celestial_mechanics"
            ],
            "links": [
                "/wiki/List_of_the_72_names_on_the_Eiffel_Tower",
                "/wiki/Eiffel_Tower",
                "/wiki/Lunar_crater",
                "/wiki/Lagrange_(crater)",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Libration",
                "/wiki/Satellites_of_Jupiter",
                "/wiki/Royal_Society_of_Edinburgh",
                "/wiki/Royal_Society",
                "/wiki/Royal_Swedish_Academy_of_Sciences",
                "/wiki/Napoleon",
                "/wiki/Legion_of_Honour",
                "/wiki/Count_of_the_Empire",
                "/wiki/Planetary_motion",
                "/wiki/Sim%C3%A9on_Poisson",
                "/wiki/Continued_fraction",
                "/wiki/Fermat%27s_little_theorem",
                "/wiki/Infinitesimal",
                "/wiki/Lagrange_multipliers",
                "/wiki/Augustin_Louis_Cauchy",
                "/wiki/Carl_Gustav_Jakob_Jacobi",
                "/wiki/Karl_Weierstrass",
                "/wiki/John_Landen",
                "/wiki/Taylor%27s_theorem",
                "/wiki/Differential_calculus",
                "/wiki/Generality_of_algebra",
                "/wiki/Lagrange_multipliers",
                "/wiki/Principle_of_least_action",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Pure_mathematics",
                "/wiki/Adrien-Marie_Legendre",
                "/wiki/Mechanics",
                "/wiki/Astronomy",
                "/wiki/Partial_differential_equation",
                "/wiki/Analytical_geometry",
                "/wiki/Quadric",
                "/wiki/Canonical_form",
                "/wiki/Prussian_Academy_of_Sciences",
                "/wiki/Algebra",
                "/wiki/Lagrangian_mechanics",
                "/wiki/Jupiter",
                "/wiki/Secular_equation",
                "/wiki/Acad%C3%A9mie_fran%C3%A7aise",
                "/wiki/Astronomy",
                "/wiki/Infinite_series",
                "/wiki/Joseph_Fourier",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/%C3%89cole_Normale_Sup%C3%A9rieure",
                "/wiki/Bureau_des_Longitudes",
                "/wiki/Napoleon_I_of_France",
                "/wiki/Piedmont",
                "/wiki/S%C3%A9nat_conservateur",
                "/wiki/S%C3%A9natus-consulte",
                "/wiki/Reign_of_Terror",
                "/wiki/Lavoisier",
                "/wiki/Tax_farming",
                "/wiki/Pierre_Charles_Le_Monnier",
                "/wiki/Naples",
                "/wiki/Louis_XVI_of_France",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/French_Institute",
                "/wiki/French_revolution",
                "/wiki/Saint_Petersburg",
                "/wiki/Prussia",
                "/wiki/Euler",
                "/wiki/Maupertuis",
                "/wiki/Jean_le_Rond_d%27Alembert",
                "/wiki/Frederick_the_Great",
                "/wiki/Libration",
                "/wiki/Moon",
                "/wiki/Virtual_work",
                "/wiki/Integral_calculus",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Square_number",
                "/wiki/N-body_problem",
                "/wiki/Principle_of_least_action",
                "/wiki/Dynamics_(mechanics)",
                "/wiki/Calculus_of_variations",
                "/wiki/Tautochrone",
                "/wiki/Leonhard_Euler",
                "/wiki/Euler%E2%80%93Lagrange_equation",
                "/wiki/Pierre_Louis_Maupertuis",
                "/wiki/Edmond_Halley",
                "/wiki/Charles_Emmanuel_III",
                "/wiki/Benjamin_Robins",
                "/wiki/Leonhard_Euler",
                "/wiki/Fran%C3%A7ois_Daviet_de_Foncenex",
                "/wiki/Charles_Emmanuel_III_of_Sardinia",
                "/wiki/University_of_Turin",
                "/wiki/Kingdom_of_France",
                "/wiki/Kingdom_of_Sardinia",
                "/wiki/Turin",
                "/wiki/Roman_Catholic",
                "/wiki/Calculus_of_variations",
                "/wiki/Euler%E2%80%93Lagrange_equation",
                "/wiki/Functional_(mathematics)",
                "/wiki/Lagrange_multipliers",
                "/wiki/Differential_equation",
                "/wiki/Method_of_variation_of_parameters",
                "/wiki/Differential_calculus",
                "/wiki/Probability_theory",
                "/wiki/Algebraic_equation",
                "/wiki/Lagrange%27s_four-square_theorem",
                "/wiki/Group_theory",
                "/wiki/%C3%89variste_Galois",
                "/wiki/Calculus",
                "/wiki/Lagrange_interpolation",
                "/wiki/Taylor_series",
                "/wiki/Three-body_problem",
                "/wiki/Lagrangian_point",
                "/wiki/Newtonian_mechanics",
                "/wiki/Lagrangian_mechanics",
                "/wiki/History_of_the_metric_system",
                "/wiki/French_First_Republic",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Bureau_des_Longitudes",
                "/wiki/S%C3%A9nat_conservateur",
                "/wiki/Leonhard_Euler",
                "/wiki/Jean_le_Rond_d%27Alembert",
                "/wiki/Prussian_Academy_of_Sciences",
                "/wiki/Prussia",
                "/wiki/French_Academy_of_Sciences",
                "/wiki/Analytical_mechanics",
                "/wiki/M%C3%A9canique_analytique",
                "/wiki/Isaac_Newton",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French",
                "/wiki/Wikipedia:Verifiability",
                "/wiki/Enlightenment_Era",
                "/wiki/Mathematician",
                "/wiki/Astronomer",
                "/wiki/Mathematical_analysis",
                "/wiki/Number_theory",
                "/wiki/Classical_mechanics",
                "/wiki/Celestial_mechanics"
            ],
            "text": "Lagrange is one of the 72 prominent French scientists who were commemorated on plaques at the first stage of the Eiffel Tower when it first opened. Rue Lagrange in the 5th Arrondissement in Paris is named after him. In Turin, the street where the house of his birth still stands is named via Lagrange. The lunar crater Lagrange also bears his name.Lagrange was awarded the 1764 prize of the French Academy of Sciences for his memoir on the libration of the Moon. In 1766 the Academy proposed a problem of the motion of the satellites of Jupiter, and the prize again was awarded to Lagrange. He also shared or won the prizes of 1772, 1774, and 1778.Euler proposed Lagrange for election to the Berlin Academy and he was elected on 2 September 1756. He was elected a Fellow of the Royal Society of Edinburgh in 1790, a Fellow of the Royal Society and a foreign member of the Royal Swedish Academy of Sciences in 1806. In 1808, Napoleon made Lagrange a Grand Officer of the Legion of Honour and a Count of the Empire. He was awarded the Grand Croix of the Ordre Imp\u00e9rial de la R\u00e9union in 1813, a week before his death in Paris.The theory of the planetary motions had formed the subject of some of the most remarkable of Lagrange's Berlin papers. In 1806 the subject was reopened by Poisson, who, in a paper read before the French Academy, showed that Lagrange's formulae led to certain limits for the stability of the orbits. Lagrange, who was present, now discussed the whole subject afresh, and in a letter communicated to the Academy in 1808 explained how, by the variation of arbitrary constants, the periodical and secular inequalities of any system of mutually interacting bodies could be determined.where p is a prime and a is prime to p, may be applied to give the complete algebraic solution of any binomial equation. He also here explains how the equation whose roots are the squares of the differences of the roots of the original equation may be used so as to give considerable information as to the position and nature of those roots.His R\u00e9solution des \u00e9quations num\u00e9riques, published in 1798, was also the fruit of his lectures at \u00c9cole Polytechnique. There he gives the method of approximating to the real roots of an equation by means of continued fractions, and enunciates several other theorems. In a note at the end he shows how Fermat's little theorem, that isAt a later period Lagrange fully embraced the use of infinitesimals in preference to founding the differential calculus on the study of algebraic forms; and in the preface to the second edition of the M\u00e9canique Analytique, which was issued in 1811, he justifies the employment of infinitesimals, and concludes by saying that:Another treatise on the same lines was his Le\u00e7ons sur le calcul des fonctions, issued in 1804, with the second edition in 1806. It is in this book that Lagrange formulated his celebrated method of Lagrange multipliers, in the context of problems of variational calculus with integral constraints. These works devoted to differential calculus and calculus of variations may be considered as the starting point for the researches of Cauchy, Jacobi, and Weierstrass.A somewhat similar method had been previously used by John Landen in the Residual Analysis, published in London in 1758. Lagrange believed that he could thus get rid of those difficulties, connected with the use of infinitely large and infinitely small quantities, to which philosophers objected in the usual treatment of the differential calculus. The book is divided into three parts: of these, the first treats of the general theory of functions, and gives an algebraic proof of Taylor's theorem, the validity of which is, however, open to question; the second deals with applications to geometry; and the third with applications to mechanics.Lagrange's lectures on the differential calculus at \u00c9cole Polytechnique form the basis of his treatise Th\u00e9orie des fonctions analytiques, which was published in 1797. This work is the extension of an idea contained in a paper he had sent to the Berlin papers in 1772, and its object is to substitute for the differential calculus a group of theorems based on the development of algebraic functions in series, relying in particular on the principle of the generality of algebra.where T represents the kinetic energy and V represents the potential energy of the system. He then presented what we now know as the method of Lagrange multipliers\u2014though this is not the first time that method was published\u2014as a means to solve this equation.[18] Amongst other minor theorems here given it may suffice to mention the proposition that the kinetic energy imparted by the given impulses to a material system under given constraints is a maximum, and the principle of least action. All the analysis is so elegant that Sir William Rowan Hamilton said the work could be described only as a scientific poem. Lagrange remarked that mechanics was really a branch of pure mathematics analogous to a geometry of four dimensions, namely, the time and the three coordinates of the point in space; and it is said that he prided himself that from the beginning to the end of the work there was not a single diagram. At first no printer could be found who would publish the book; but Legendre at last persuaded a Paris firm to undertake it, and it was issued under the supervision of Laplace, Cousin, Legendre (editor) and Condorcet in 1788.[9]The object of the book is to show that the subject is implicitly included in a single principle, and to give general formulae from which any particular result can be obtained. The method of generalised co-ordinates by which he obtained this result is perhaps the most brilliant result of his analysis. Instead of following the motion of each individual part of a material system, as D'Alembert and Euler had done, he showed that, if we determine its configuration by a sufficient number of variables whose number is the same as that of the degrees of freedom possessed by the system, then the kinetic and potential energies of the system can be expressed in terms of those variables, and the differential equations of motion thence deduced by simple differentiation. For example, in dynamics of a rigid system he replaces the consideration of the particular problem by the general equation, which is now usually written in the formOver and above these various papers he composed his great treatise, the M\u00e9canique analytique. In this he lays down the law of virtual work, and from that one fundamental principle, by the aid of the calculus of variations, deduces the whole of mechanics, both of solids and fluids.Lastly, there are numerous papers on problems in astronomy. Of these the most important are the following:During the years from 1772 to 1785, he contributed a long series of papers which created the science of partial differential equations. A large part of these results were collected in the second edition of Euler's integral calculus which was published in 1794.There are also numerous articles on various points of analytical geometry. In two of them, written rather later, in 1792 and 1793, he reduced the equations of the quadrics (or conicoids) to their canonical forms.Several of his early papers also deal with questions of number theory.The greater number of his papers during this time were, however, contributed to the Prussian Academy of Sciences. Several of them deal with questions in algebra.Between 1772 and 1788, Lagrange re-formulated Classical/Newtonian mechanics to simplify formulas and ease calculations. These mechanics are called Lagrangian mechanics.Most of the papers sent to Paris were on astronomical questions, and among these one ought to particularly mention his paper on the Jovian system in 1766, his essay on the problem of three bodies in 1772, his work on the secular equation of the Moon in 1773, and his treatise on cometary perturbations in 1778. These were all written on subjects proposed by the Acad\u00e9mie fran\u00e7aise, and in each case the prize was awarded to him.First, his contributions to the fourth and fifth volumes, 1766\u20131773, of the Miscellanea Taurinensia; of which the most important was the one in 1771, in which he discussed how numerous astronomical observations should be combined so as to give the most probable result. And later, his contributions to the first two volumes, 1784\u20131785, of the transactions of the Turin Academy; to the first of which he contributed a paper on the pressure exerted by fluids in motion, and to the second an article on integration by infinite series, and the kind of problems for which it is suitable.Lagrange was extremely active scientifically during twenty years he spent in Berlin. Not only did he produce his M\u00e9canique analytique, but he contributed between one and two hundred papers to the Academy of Turin, the Berlin Academy, and the French Academy. Some of these are really treatises, and all without exception are of a high order of excellence. Except for a short time when he was ill he produced on average about one paper a month. Of these, note the following as amongst the most important.In 1810, Lagrange commenced a thorough revision of the M\u00e9canique analytique, but he was able to complete only about two-thirds of it before his death at Paris in 1813, in 128 Rue du Faubourg Saint Honor\u00e9. Napoleon honoured him with the Grand Croix of the Ordre Imp\u00e9rial de la R\u00e9union just two days before he died. He was buried that same year in the Panth\u00e9on in Paris. The inscription on his tomb reads in translation:But Lagrange does not seem to have been a successful teacher. Fourier, who attended his lectures in 1795, wrote:In 1794, Lagrange was appointed professor of the \u00c9cole Polytechnique; and his lectures there, described by mathematicians who had the good fortune to be able to attend them, were almost perfect both in form and matter.[citation needed] Beginning with the merest elements, he led his hearers on until, almost unknown to themselves, they were themselves extending the bounds of the subject: above all he impressed on his pupils the advantage of always using general methods expressed in a symmetrical notation.In 1795, Lagrange was appointed to a mathematical chair at the newly established \u00c9cole Normale, which enjoyed only a brief existence of four months. His lectures there were quite elementary, and contain nothing of any special importance, but they were published because the professors had to \"pledge themselves to the representatives of the people and to each other neither to read nor to repeat from memory,\" and the discourses were ordered to be taken down in shorthand to enable the deputies to see how the professors acquitted themselves.Lagrange was considerably involved in the process of making new standard units of measurement in the 1790s. He was offered the presidency of the Commission for the reform of weights and measures (la Commission des Poids et Mesures) when he was preparing to escape. And after Lavoisier's death in 1794, it was largely owing to Lagrange's influence that the final choice of the unit system of metre and kilogram was settled and the decimal subdivision was finally accepted by the commission of 1799. Lagrange was also one of the founding members of the Bureau des Longitudes in 1795.Though Lagrange had been preparing to escape from France while there was yet time, he was never in any danger; different revolutionary governments (and at a later time, Napoleon) loaded him with honours and distinctions. This luckiness or safety may to some extent be due to his life attitude he expressed many years before: \"I believe that, in general, one of the first principles of every wise man is to conform strictly to the laws of the country in which he is living, even when they are unreasonable\".[9] A striking testimony to the respect in which he was held was shown in 1796 when the French commissary in Italy was ordered to attend in full state on Lagrange's father, and tender the congratulations of the republic on the achievements of his son, who \"had done honor to all mankind by his genius, and whom it was the special glory of Piedmont to have produced.\" It may be added that Napoleon, when he attained power, warmly encouraged scientific studies in France, and was a liberal benefactor of them. Appointed senator in 1799, he was the first signer of the S\u00e9natus-consulte which in 1802 annexed his fatherland Piedmont to France.[6] He acquired French citizenship in consequence.[6] The French claimed he was a French mathematician, but the Italians continued to claim him as Italian.[9]In September 1793, the Reign of Terror began. Under intervention of Antoine Lavoisier, who himself was by then already thrown out of the Academy along with many other scholars, Lagrange was specifically exempted by name in the decree of October 1793 that ordered all foreigners to leave France. On 4 May 1794, Lavoisier and 27 other tax farmers were arrested and sentenced to death and guillotined on the afternoon after the trial. Lagrange said on the death of Lavoisier:It was about the same time, 1792, that the unaccountable sadness of his life and his timidity moved the compassion of 24-year-old Ren\u00e9e-Fran\u00e7oise-Ad\u00e9la\u00efde Le Monnier, daughter of his friend, the astronomer Pierre Charles Le Monnier. She insisted on marrying him, and proved a devoted wife to whom he became warmly attached.In 1786, following Frederick's death, Lagrange received similar invitations from states including Spain and Naples, and he accepted the offer of Louis XVI to move to Paris. In France he was received with every mark of distinction and special apartments in the Louvre were prepared for his reception, and he became a member of the French Academy of Sciences, which later became part of the Institut de France (1795). At the beginning of his residence in Paris he was seized with an attack of melancholy, and even the printed copy of his M\u00e9canique on which he had worked for a quarter of a century lay for more than two years unopened on his desk. Curiosity as to the results of the French revolution first stirred him out of his lethargy, a curiosity which soon turned to alarm as the revolution developed.Nonetheless, during his years in Berlin, Lagrange's health was rather poor on many occasions, and that of his wife Vittoria was even worse. She died in 1783 after years of illness and Lagrange was very depressed. In 1786, Frederick II died, and the climate of Berlin became rather trying for Lagrange.[9]Lagrange was a favourite of the king, who used frequently to discourse to him on the advantages of perfect regularity of life. The lesson went home, and thenceforth Lagrange studied his mind and body as though they were machines, and found by experiment the exact amount of work which he was able to do without breaking down. Every night he set himself a definite task for the next day, and on completing any branch of a subject he wrote a short analysis to see what points in the demonstrations or in the subject-matter were capable of improvement. He always thought out the subject of his papers before he began to compose them, and usually wrote them straight off without a single erasure or correction.In 1766, Euler left Berlin for Saint Petersburg, and Frederick himself wrote to Lagrange expressing the wish of \"the greatest king in Europe\" to have \"the greatest mathematician in Europe\" resident at his court. Lagrange was finally persuaded and he spent the next twenty years in Prussia, where he produced not only the long series of papers published in the Berlin and Turin transactions, but also his monumental work, the M\u00e9canique analytique. In 1767, he married his cousin Vittoria Conti.Already in 1756, Euler and Maupertuis, seeing his mathematical talent, tried to persuade him to come to Berlin, but Lagrange had no such intention and shyly refused the offer. In 1765, d'Alembert interceded on Lagrange's behalf with Frederick of Prussia and by letter, asked him to leave Turin for a considerably more prestigious position in Berlin. Lagrange again turned down the offer, responding that[15]:361The next work he produced was in 1764 on the libration of the Moon, and an explanation as to why the same face was always turned to the earth, a problem which he treated by the aid of virtual work. His solution is especially interesting as containing the germ of the idea of generalised equations of motion, equations which he first formally proved in 1780.The third volume includes the solution of several dynamical problems by means of the calculus of variations; some papers on the integral calculus; a solution of Fermat's problem mentioned above: given an integer n which is not a perfect square, to find a number x such that x2n\u00a0+\u00a01 is a perfect square; and the general differential equations of motion for three bodies moving under their mutual attractions.The second volume contains a long paper embodying the results of several papers in the first volume on the theory and notation of the calculus of variations; and he illustrates its use by deducing the principle of least action, and by solutions of various problems in dynamics.Euler was very impressed with Lagrange's results. It has been stated that \"with characteristic courtesy he withheld a paper he had previously written, which covered some of the same ground, in order that the young Italian might have time to complete his work, and claim the undisputed invention of the new calculus\"; however, this chivalric view has been disputed.[14] Lagrange published his method in two memoirs of the Turin Society in 1762 and 1773.Lagrange is one of the founders of the calculus of variations. Starting in 1754, he worked on the problem of tautochrone, discovering a method of maximising and minimising functionals in a way similar to finding extrema of functions. Lagrange wrote several letters to Leonhard Euler between 1754 and 1756 describing his results. He outlined his \"\u03b4-algorithm\", leading to the Euler\u2013Lagrange equations of variational calculus and considerably simplifying Euler's earlier analysis.[13] Lagrange also applied his ideas to problems of classical mechanics, generalising the results of Euler and Maupertuis.It was not until he was seventeen that he showed any taste for mathematics \u2013 his interest in the subject being first excited by a paper by Edmond Halley which he came across by accident. Alone and unaided he threw himself into mathematical studies; at the end of a year's incessant toil he was already an accomplished mathematician. Charles Emmanuel III appointed Lagrange to serve as the \"Sostituto del Maestro di Matematica\" (mathematics assistant professor) at the Royal Military Academy of the Theory and Practice of Artillery in 1755, where he taught courses in calculus and mechanics to support the Piedmontese army's early adoption of the ballistics theories of Benjamin Robins and Leonhard Euler. In that capacity, Lagrange was the first to teach calculus in an engineering school. According to Alessandro Papacino D'Antoni, the academy's military commander and famous artillery theorist, Lagrange unfortunately proved to be a problematic professor with his oblivious teaching style, abstract reasoning, and impatience with artillery and fortification-engineering applications.[11] In this Academy one of his students was Fran\u00e7ois Daviet de Foncenex.[12]His father, who had charge of the king's military chest and was Treasurer of the Office of Public Works and Fortifications in Turin, should have maintained a good social position and wealth, but before his son grew up he had lost most of his property in speculations. A career as a lawyer was planned out for Lagrange by his father, and certainly Lagrange seems to have accepted this willingly. He studied at the University of Turin and his favourite subject was classical Latin. At first he had no great enthusiasm for mathematics, finding Greek geometry rather dull.Born as Giuseppe Lodovico Lagrangia, Lagrange was of Italian and French descent. His paternal great-grandfather was a French army officer who had moved to Turin, the de facto capital of the kingdom of Piedmont-Sardinia at Lagrange's time, and married an Italian; so did his grandfather and his father. His mother was from the countryside of Turin.[9] He was raised as a Roman Catholic (but later on became an agnostic).[10]Lagrange was one of the creators of the calculus of variations, deriving the Euler\u2013Lagrange equations for extrema of functionals. He also extended the method to take into account possible constraints, arriving at the method of Lagrange multipliers. Lagrange invented the method of solving differential equations known as variation of parameters, applied differential calculus to the theory of probabilities and attained notable work on the solution of equations. He proved that every natural number is a sum of four squares. His treatise Theorie des fonctions analytiques laid some of the foundations of group theory, anticipating Galois. In calculus, Lagrange developed a novel approach to interpolation and Taylor series. He studied the three-body problem for the Earth, Sun and Moon (1764) and the movement of Jupiter's satellites (1766), and in 1772 found the special-case solutions to this problem that yield what are now known as Lagrangian points. But above all, he is best known for his work on mechanics, where he transformed Newtonian mechanics into a branch of analysis, Lagrangian mechanics as it is now called, and presented the so-called mechanical \"principles\" as simple results of the variational calculus.In 1787, at age 51, he moved from Berlin to Paris and became a member of the French Academy of Sciences. He remained in France until the end of his life. He was significantly involved in the decimalisation in Revolutionary France, became the first professor of analysis at the \u00c9cole Polytechnique upon its opening in 1794, was a founding member of the Bureau des Longitudes, and became Senator in 1799.In 1766, on the recommendation of Euler and d'Alembert, Lagrange succeeded Euler as the director of mathematics at the Prussian Academy of Sciences in Berlin, Prussia, where he stayed for over twenty years, producing volumes of work and winning several prizes of the French Academy of Sciences. Lagrange's treatise on analytical mechanics (M\u00e9canique analytique, 4. ed., 2 vols. Paris: Gauthier-Villars et fils, 1788\u201389), written in Berlin and first published in 1788, offered the most comprehensive treatment of classical mechanics since Newton and formed a basis for the development of mathematical physics in the nineteenth century.Joseph-Louis Lagrange (/l\u0259\u02c8\u0261r\u0251\u02d0nd\u0292/[1] or /l\u0259\u02c8\u0261re\u026and\u0292/;[2] French:\u00a0[lagr\u0251\u0303\u0292]; born Giuseppe Lodovico Lagrangia[3][need quotation to verify][4] or Giuseppe Ludovico De la Grange Tournier,[5] Turin, 25 January 1736 \u2013 Paris, 10 April 1813; also reported as Giuseppe Luigi Lagrange[6] or Lagrangia[7]) was an Italian Enlightenment Era mathematician and astronomer. He made significant contributions to the fields of analysis, number theory, and both classical and celestial mechanics.",
            "title": "Joseph-Louis Lagrange",
            "url": "https://en.wikipedia.org/wiki/Lagrange"
        },
        {
            "desc_links": [
                "/wiki/Hans_Freudenthal",
                "/wiki/Elasticity_(physics)",
                "/wiki/Mathematical_physics",
                "/wiki/Baron",
                "/wiki/Royal_Society",
                "/wiki/FRSE",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French",
                "/wiki/Mathematician",
                "/wiki/Physicist",
                "/wiki/Mathematical_analysis",
                "/wiki/Calculus",
                "/wiki/Generality_of_algebra",
                "/wiki/Complex_analysis",
                "/wiki/Permutation_group",
                "/wiki/Abstract_algebra"
            ],
            "links": [
                "/wiki/Niels_Henrik_Abel",
                "/wiki/Guglielmo_Libri_Carucci_dalla_Sommaja",
                "/wiki/Joseph_Liouville",
                "/wiki/Jean_Marie_Constant_Duhamel",
                "/wiki/Jean-Victor_Poncelet",
                "/wiki/Society_of_Saint_Vincent_de_Paul",
                "/wiki/Society_of_Jesus",
                "/wiki/Charles_Hermite",
                "/wiki/Great_Irish_Famine",
                "/wiki/Arcueil",
                "/wiki/French_Revolution",
                "/wiki/Rouen",
                "/wiki/Leonhard_Euler",
                "/wiki/Argument_principle",
                "/wiki/Cauchy_argument_principle",
                "/wiki/Nyquist_stability_criterion",
                "/wiki/Feedback_amplifier",
                "/wiki/Feedback",
                "/wiki/Taylor%27s_theorem",
                "/wiki/Limit_of_a_function",
                "/wiki/Absolute_convergence",
                "/wiki/Cauchy_condensation_test",
                "/wiki/Niels_Henrik_Abel",
                "/wiki/Uniform_continuity",
                "/wiki/Non-standard_analysis",
                "/wiki/Pierre_Alphonse_Laurent",
                "/wiki/Laurent_series",
                "/wiki/Residue_theorem",
                "/wiki/Cauchy%27s_integral_formula",
                "/wiki/Residue_(mathematics)",
                "/wiki/Pole_(complex_analysis)",
                "/wiki/Neighborhood",
                "/wiki/Holomorphic_function",
                "/wiki/Complex_plane",
                "/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions",
                "/wiki/Complex_function_theory",
                "/wiki/Cauchy%27s_integral_theorem",
                "/wiki/Fermat_polygonal_number_theorem",
                "/wiki/Augustin-Jean_Fresnel",
                "/wiki/Dispersion_(optics)",
                "/wiki/Polarization_(waves)",
                "/wiki/Mechanics",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Cauchy_stress_tensor",
                "/wiki/Elasticity_(physics)",
                "/wiki/Stress_(physics)",
                "/wiki/Sim%C3%A9on_Poisson",
                "/wiki/Problem_of_Apollonius",
                "/wiki/Circle",
                "/wiki/Euler_characteristic",
                "/wiki/Polyhedra",
                "/wiki/Wave",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Q-series",
                "/wiki/List_of_the_72_names_on_the_Eiffel_Tower",
                "/wiki/Last_Rites",
                "/wiki/Napoleon_III_of_France",
                "/wiki/Napoleon_III",
                "/wiki/Institut_Catholique_de_Paris",
                "/wiki/The_Enlightenment",
                "/wiki/Celestial_mechanics",
                "/wiki/Signed-digit_representation",
                "/wiki/John_Colson",
                "/wiki/Bureau_des_Longitudes",
                "/wiki/Longitude",
                "/wiki/Latitude",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Baron",
                "/wiki/Prague",
                "/wiki/Henri_d%27Artois",
                "/wiki/Fribourg",
                "/wiki/King_of_Sardinia",
                "/wiki/Royal_Swedish_Academy_of_Sciences",
                "/wiki/American_Academy_of_Arts_and_Sciences",
                "/wiki/July_Revolution",
                "/wiki/Louis-Philippe",
                "/wiki/House_of_Orl%C3%A9ans",
                "/wiki/Charles_X",
                "/wiki/Coll%C3%A8ge_de_France",
                "/wiki/Louis_Poinsot",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Polygonal_number_theorem",
                "/wiki/Symmetric_functions",
                "/wiki/Symmetric_group",
                "/wiki/Louis_XVIII",
                "/wiki/Acad%C3%A9mie_des_Sciences",
                "/wiki/Lazare_Carnot",
                "/wiki/Gaspard_Monge",
                "/wiki/Canal_de_l%27Ourcq",
                "/wiki/Pont_de_Saint-Cloud",
                "/wiki/Institut_de_France",
                "/wiki/Regular_polyhedron",
                "/wiki/Conic_sections",
                "/wiki/%C3%89cole_des_Ponts_et_Chauss%C3%A9es",
                "/wiki/%C3%89cole_Centrale_du_Panth%C3%A9on",
                "/wiki/%C3%89cole_Polytechnique",
                "/wiki/Louis_Fran%C3%A7ois_Cauchy",
                "/wiki/French_Revolution",
                "/wiki/Reign_of_Terror",
                "/wiki/Robespierre",
                "/wiki/Napoleon",
                "/wiki/Pierre-Simon_Laplace",
                "/wiki/Joseph_Louis_Lagrange",
                "/wiki/Louis_Fran%C3%A7ois_Cauchy",
                "/wiki/Hans_Freudenthal",
                "/wiki/Elasticity_(physics)",
                "/wiki/Mathematical_physics",
                "/wiki/Baron",
                "/wiki/Royal_Society",
                "/wiki/FRSE",
                "/wiki/Help:IPA/English",
                "/wiki/Help:IPA/French",
                "/wiki/Mathematician",
                "/wiki/Physicist",
                "/wiki/Mathematical_analysis",
                "/wiki/Calculus",
                "/wiki/Generality_of_algebra",
                "/wiki/Complex_analysis",
                "/wiki/Permutation_group",
                "/wiki/Abstract_algebra"
            ],
            "text": "His royalism and religious zeal also made him contentious, which caused difficulties with his colleagues. He felt that he was mistreated for his beliefs, but his opponents felt he intentionally provoked people by berating them over religious matters or by defending the Jesuits after they had been suppressed. Niels Henrik Abel called him a \"bigoted Catholic\"[26] and added he was \"mad and there is nothing that can be done about him\", but at the same time praised him as a mathematician. Cauchy's views were widely unpopular among mathematicians and when Guglielmo Libri Carucci dalla Sommaja was made chair in mathematics before him he, and many others, felt his views were the cause. When Libri was accused of stealing books he was replaced by Joseph Liouville rather than Cauchy, which caused a rift between Liouville and Cauchy. Another dispute with political overtones concerned Jean Marie Constant Duhamel and a claim on inelastic shocks. Cauchy was later shown, by Jean-Victor Poncelet, to be wrong.He was an equally staunch Catholic and a member of the Society of Saint Vincent de Paul.[25] He also had links to the Society of Jesus and defended them at the Academy when it was politically unwise to do so. His zeal for his faith may have led to his caring for Charles Hermite during his illness and leading Hermite to become a faithful Catholic. It also inspired Cauchy to plead on behalf of the Irish during the Potato Famine.In any event, he inherited his father's staunch royalism and hence refused to take oaths to any government after the overthrow of Charles X.Augustin-Louis Cauchy grew up in the house of a staunch royalist. This made his father flee with the family to Arcueil during the French Revolution. Their life there during that time was apparently hard; Augustin-Louis's father, Louis Fran\u00e7ois, spoke of living on rice, bread, and crackers during the period. A paragraph from an undated letter from Louis Fran\u00e7ois to his mother in Rouen says:[23]His other works include:His greatest contributions to mathematical science are enveloped in the rigorous methods which he introduced; these are mainly embodied in his three great treatises:Cauchy was very productive, in number of papers second only to Leonhard Euler. It took almost a century to collect all his writings into 27 large volumes:In a paper published in 1855, two years before Cauchy's death, he discussed some theorems, one of which is similar to the \"Argument Principle\" in many modern textbooks on complex analysis. In modern control theory textbooks, the Cauchy argument principle is quite frequently used to derive the Nyquist stability criterion, which can be used to predict the stability of negative feedback amplifier and negative feedback control systems. Thus Cauchy's work has a strong impact on both pure mathematics and practical engineering.He was the first to prove Taylor's theorem rigorously, establishing his well-known form of the remainder.[2] He wrote a textbook[20] (see the illustration) for his students at the \u00c9cole Polytechnique in which he developed the basic theorems of mathematical analysis as rigorously as possible. In this book he gave the necessary and sufficient condition for the existence of a limit in the form that is still taught. Also Cauchy's well-known test for absolute convergence stems from this book: Cauchy condensation test. In 1829 he defined for the first time a complex function of a complex variable in another textbook.[21] In spite of these, Cauchy's own research papers often used intuitive, not rigorous, methods;[22] thus one of his theorems was exposed to a \"counter-example\" by Abel, later fixed by the introduction of the notion of uniform continuity.Cauchy gave an explicit definition of an infinitesimal in terms of a sequence tending to zero. There has been a vast body of literature written about Cauchy's notion of \"infinitesimally small quantities\", arguing they lead from everything from the usual \"epsilontic\" definitions or to the notions of non-standard analysis. The consensus is that Cauchy omitted or left implicit the important ideas to make clear the precise meaning of the infinitely small quantities he used. (Barany 2013)M. Barany claims that the \u00c9cole mandated the inclusion of infinitesimal methods against Cauchy's better judgement (Barany 2011). Gilain notes that when the portion of the curriculum devoted to Analyse Alg\u00e9brique was reduced in 1825, Cauchy insisted on placing the topic of continuous functions (and therefore also infinitesimals) at the beginning of the Differential Calculus (Gilain 1989). Laugwitz (1989) and Benis-Sinaceur (1973) point out that Cauchy continued to use infinitesimals in his own research as late as 1853.where the sum is over all the n poles of f(z) on and within the contour C. These results of Cauchy's still form the core of complex function theory as it is taught today to physicists and electrical engineers. For quite some time, contemporaries of Cauchy ignored his theory, believing it to be too complicated. Only in the 1840s the theory started to get response, with Pierre Alphonse Laurent being the first mathematician, besides Cauchy, making a substantial contribution (his Laurent series published in 1843).where f(z) is analytic on C and within the region bounded by the contour C and the complex number a is somewhere in this region. The contour integral is taken counter-clockwise. Clearly, the integrand has a simple pole at z = a. In the second paper[18] he presented the residue theorem,In 1831, while in Turin, Cauchy submitted two papers to the Academy of Sciences of Turin. In the first[17] he proposed the formula now known as Cauchy's integral formula,where we replaced B1 by the modern notation of the residue.where \u03c6(z) is analytic (i.e., well-behaved without singularities), then f is said to have a pole of order n in the point a. If n = 1, the pole is called simple. The coefficient B1 is called by Cauchy the residue of function f at a. If f is non-singular at a then the residue of f is zero at a. Clearly the residue is in the case of a simple pole equal to,In 1826 Cauchy gave a formal definition of a residue of a function.[16] This concept regards functions that have poles\u2014isolated singularities, i.e., points where a function goes to positive or negative infinity. If the complex-valued function f(z) can be expanded in the neighborhood of a singularity a aswhere f(z) is a complex-valued function holomorphic on and within the non-self-intersecting closed curve C (contour) lying in the complex plane. The contour integral is taken along the contour C. The rudiments of this theorem can already be found in a paper that the 24-year-old Cauchy presented to the Acad\u00e9mie des Sciences (then still called \"First Class of the Institute\") on August 11, 1814. In full form the theorem was given in 1825.[15] The 1825 paper is seen by many[by whom?] as Cauchy's most important contribution to mathematics.Cauchy is most famous for his single-handed development of complex function theory. The first pivotal theorem proved by Cauchy, now known as Cauchy's integral theorem, was the following:Other significant contributions include being the first to prove the Fermat polygonal number theorem.In the theory of light he worked on Fresnel's wave theory and on the dispersion and polarization of light. He also contributed significant research in mechanics, substituting the notion of the continuity of geometrical displacements for the principle of the continuity of matter. He wrote on the equilibrium of rods and elastic membranes and on waves in elastic media. He introduced[14] a 3 \u00d7 3 symmetric matrix of numbers that is now known as the Cauchy stress tensor. In elasticity, he originated the theory of stress, and his results are nearly as valuable as those of Sim\u00e9on Poisson.[2]The genius of Cauchy was illustrated in his simple solution of the problem of Apollonius\u2014describing a circle touching three given circles\u2014which he discovered in 1805, his generalization of Euler's formula on polyhedra in 1811, and in several other elegant problems. More important is his memoir on wave propagation, which obtained the Grand Prix of the French Academy of Sciences in 1816. Cauchy's writings covered notable topics including: the theory of series, where he developed the notion of convergence and discovered many of the basic formulas for q-series. In the theory of numbers and complex quantities, he was the first to define complex numbers as pairs of real numbers. He also wrote on the theory of groups and substitutions, the theory of functions, differential equations and determinants.[2]His name is one of the 72 names inscribed on the Eiffel Tower.Not unexpectedly, the idea came up in bureaucratic circles that it would be useful to again require a loyalty oath from all state functionaries, including university professors. Not always does history repeat itself, however, because this time a cabinet minister was able to convince the Emperor to exempt Cauchy from the oath. Cauchy remained a professor at the University until his death at the age of 67. He received the Last Rites and died of a bronchial condition at 4\u00a0a.m. on May 23, 1857.[13]The year 1848 was the year of revolution all over Europe; revolutions broke out in numerous countries, beginning in France. King Louis-Philippe, fearful of sharing the fate of Louis XVI, fled to England. The oath of allegiance was abolished, and the road to an academic appointment was finally clear for Cauchy. On March 1, 1849, he was reinstated at the Facult\u00e9 de Sciences, as a professor of mathematical astronomy. After political turmoil all through 1848, France chose to become a Republic, under the Presidency of Louis Napoleon Bonaparte, nephew of Napoleon Bonaparte, and son of Napoleon's brother, who had been installed as the first king of Holland. Soon (early 1852) the President made himself Emperor of France, and took the name Napoleon III.Throughout the nineteenth century the French educational system struggled over the separation of Church and State. After losing control of the public education system, the Catholic Church sought to establish its own branch of education and found in Cauchy a staunch and illustrious ally. He lent his prestige and knowledge to the \u00c9cole Normale \u00c9ccl\u00e9siastique, a school in Paris run by Jesuits, for training teachers for their colleges. He also took part in the founding of the Institut Catholique. The purpose of this institute was to counter the effects of the absence of Catholic university education in France. These activities did not make Cauchy popular with his colleagues who, on the whole, supported the Enlightenment ideals of the French Revolution. When a chair of mathematics became vacant at the Coll\u00e8ge de France in 1843, Cauchy applied for it, but got just three out of 45 votes.In November 1839 Cauchy was elected to the Bureau, and discovered immediately that the matter of the oath was not so easily dispensed with. Without his oath, the king refused to approve his election. For four years Cauchy was in the absurd position of being elected, but not being approved; hence, he was not a formal member of the Bureau, did not receive payment, could not participate in meetings, and could not submit papers. Still Cauchy refused to take any oaths; however, he did feel loyal enough to direct his research to celestial mechanics. In 1840, he presented a dozen papers on this topic to the Academy. He also described and illustrated the signed-digit representation of numbers, an innovation presented in England in 1727 by John Colson. The confounded membership of the Bureau lasted until the end of 1843, when Cauchy was finally replaced by Poinsot.In August 1839 a vacancy appeared in the Bureau des Longitudes. This Bureau had some resemblance to the Academy; for instance, it had the right to co-opt its members. Further, it was believed that members of the Bureau could \"forget\" about the oath of allegiance, although formally, unlike the Academicians, they were obliged to take it. The Bureau des Longitudes was an organization founded in 1795 to solve the problem of determining position on sea \u2013 mainly the longitudinal coordinate, since latitude is easily determined from the position of the sun. Since it was thought that position on sea was best determined by astronomical observations, the Bureau had developed into an organization resembling an academy of astronomical sciences.Cauchy returned to Paris and his position at the Academy of Sciences late in 1838.[13] He could not regain his teaching positions, because he still refused to swear an oath of allegiance. However, he desperately wanted to regain a formal position[citation needed] in Parisian science.During his civil engineering days, Cauchy once had been briefly in charge of repairing a few of the Parisian sewers, and he made the mistake of mentioning this to his pupil; with great malice, the young Duke went about saying Mister Cauchy started his career in the sewers of Paris. His role as tutor lasted until the Duke became eighteen years old, in September 1838.[13] Cauchy did hardly any research during those five years, while the Duke acquired a lifelong dislike of mathematics. The only good that came out of this episode was Cauchy's promotion to baron, a title by which Cauchy set great store. In 1834, his wife and two daughters moved to Prague, and Cauchy was finally reunited with his family after four years in exile.In August 1833 Cauchy left Turin for Prague, to become the science tutor of the thirteen-year-old Duke of Bordeaux Henri d'Artois (1820\u20131883), the exiled Crown Prince and grandson of Charles X.[13] As a professor of the \u00c9cole Polytechnique, Cauchy had been a notoriously bad lecturer, assuming levels of understanding that only a few of his best students could reach, and cramming his allotted time with too much material. The young Duke had neither taste nor talent for either mathematics or science, so student and teacher were a perfect mismatch. Although Cauchy took his mission very seriously, he did this with great clumsiness, and with surprising lack of authority over the Duke.These events marked a turning point in Cauchy's life, and a break in his mathematical productivity. Cauchy, shaken by the fall of the government, and moved by a deep hatred of the liberals who were taking power, left Paris to go abroad, leaving his family behind.[11] He spent a short time at Fribourg in Switzerland, where he had to decide whether he would swear a required oath of allegiance to the new regime. He refused to do this, and consequently lost all his positions in Paris, except his membership of the Academy, for which an oath was not required. In 1831 Cauchy went to the Italian city of Turin, and after some time there, he accepted an offer from the King of Sardinia (who ruled Turin and the surrounding Piedmont region) for a chair of theoretical physics, which was created especially for him. He taught in Turin during 1832\u20131833. In 1831, he was elected a foreign member of the Royal Swedish Academy of Sciences, and the following year a Foreign Honorary Member of the American Academy of Arts and Sciences.[12]In July 1830, the July Revolution occurred in France. Charles X fled the country, and was succeeded by the non-Bourbon king Louis-Philippe (of the House of Orl\u00e9ans). Riots, in which uniformed students of the \u00c9cole Polytechnique took an active part, raged close to Cauchy's home in Paris.The conservative political climate that lasted until 1830 suited Cauchy perfectly. In 1824 Louis XVIII died, and was succeeded by his even more reactionary brother Charles X. During these years Cauchy was highly productive, and published one important mathematical treatise after another. He received cross appointments at the Coll\u00e8ge de France, and the Facult\u00e9 des sciences de Paris\u00a0(fr).When Cauchy was 28 years old, he was still living with his parents. His father found it high time for his son to marry; he found him a suitable bride, Alo\u00efse de Bure, five years his junior. The de Bure family were printers and booksellers, and published most of Cauchy's works.[9] Alo\u00efse and Augustin were married on April 4, 1818, with great Roman Catholic pomp and ceremony, in the Church of Saint-Sulpice. In 1819 the couple's first daughter, Marie Fran\u00e7oise Alicia, was born, and in 1823 the second and last daughter, Marie Mathilde.[10] Cauchy had two brothers: Alexandre Laurent Cauchy, who became a president of a division of the court of appeal in 1847, and a judge of the court of cassation in 1849; and Eug\u00e8ne Fran\u00e7ois Cauchy, a publicist who also wrote several mathematical works.In November 1815, Louis Poinsot, who was an associate professor at the \u00c9cole Polytechnique, asked to be exempted from his teaching duties for health reasons. Cauchy was by then a rising mathematical star, who certainly merited a professorship. One of his great successes at that time was the proof of Fermat's polygonal number theorem. However, the fact that Cauchy was known to be very loyal to the Bourbons, doubtless also helped him in becoming the successor of Poinsot. He finally quit his engineering job, and received a one-year contract for teaching mathematics to second-year students of the \u00c9cole Polytechnique. In 1816, this Bonapartist, non-religious school was reorganized, and several liberal professors were fired; the reactionary Cauchy was promoted to full professor.In September 1812, now 23 years old, after becoming ill from overwork, Cauchy returned to Paris.[6] Another reason for his return to the capital was that he was losing his interest in his engineering job, being more and more attracted to the abstract beauty of mathematics; in Paris, he would have a much better chance to find a mathematics related position. Therefore, when his health improved in 1813, Cauchy chose to not return to Cherbourg.[6] Although he formally kept his engineering position, he was transferred from the payroll of the Ministry of the Marine to the Ministry of the Interior. The next three years Augustin-Louis was mainly on unpaid sick leave, and spent his time quite fruitfully, working on mathematics (on the related topics of symmetric functions, the symmetric group and the theory of higher-order algebraic equations). He attempted admission to the First Class of the Institut de France but failed on three different occasions between 1813 and 1815. In 1815 Napoleon was defeated at Waterloo, and the newly installed Bourbon king Louis XVIII took the restoration in hand. The Acad\u00e9mie des Sciences was re-established in March 1816; Lazare Carnot and Gaspard Monge were removed from this Academy for political reasons, and the king appointed Cauchy to take the place of one of them. The reaction by Cauchy's peers was harsh; they considered his acceptance of membership of the Academy an outrage, and Cauchy thereby created many enemies in scientific circles.After finishing school in 1810, Cauchy accepted a job as a junior engineer in Cherbourg, where Napoleon intended to build a naval base. Here Augustin-Louis stayed for three years, and was assigned the Ourcq Canal project and the Saint-Cloud Bridge project, and worked at the Harbor of Cherbourg.[6] Although he had an extremely busy managerial job, he still found time to prepare three mathematical manuscripts, which he submitted to the Premi\u00e8re Classe (First Class) of the Institut de France.[8] Cauchy's first two manuscripts (on polyhedra) were accepted; the third one (on directrices of conic sections) was rejected.In 1805, he placed second out of 293 applicants on this exam, and he was admitted.[6] One of the main purposes of this school was to give future civil and military engineers a high-level scientific and mathematical education. The school functioned under military discipline, which caused the young and pious Cauchy some problems in adapting. Nevertheless, he finished the Polytechnique in 1807, at the age of 18, and went on to the \u00c9cole des Ponts et Chauss\u00e9es (School for Bridges and Roads). He graduated in civil engineering, with the highest honors.On Lagrange's advice, Augustin-Louis was enrolled in the \u00c9cole Centrale du Panth\u00e9on, the best secondary school of Paris at that time, in the fall of 1802.[6] Most of the curriculum consisted of classical languages; the young and ambitious Cauchy, being a brilliant student, won many prizes in Latin and Humanities. In spite of these successes, Augustin-Louis chose an engineering career, and prepared himself for the entrance examination to the \u00c9cole Polytechnique.Cauchy's father (Louis Fran\u00e7ois Cauchy) was a high official in the Parisian Police of the New R\u00e9gime. He lost his position because of the French Revolution (July 14, 1789) that broke out one month before Augustin-Louis was born.[4] The Cauchy family survived the revolution and the following Reign of Terror (1793-4) by escaping to Arcueil, where Cauchy received his first education, from his father.[5] After the execution of Robespierre (1794), it was safe for the family to return to Paris. There Louis-Fran\u00e7ois Cauchy found himself a new bureaucratic job in 1800,[6] and quickly moved up the ranks. When Napoleon Bonaparte came to power (1799), Louis-Fran\u00e7ois Cauchy was further promoted, and became Secretary-General of the Senate, working directly under Laplace (who is now better known for his work on mathematical physics). The famous mathematician Lagrange was also a friend of the Cauchy family.[7]Cauchy married Aloise de Bure in 1818. She was a close relative of the publisher who published most of Cauchy's works. By her he had two daughters, Marie Fran\u00e7oise Alicia (1819) and Marie Mathilde (1823).Cauchy was the son of Louis Fran\u00e7ois Cauchy (1760\u20131848) and Marie-Madeleine Desestre. Cauchy had two brothers, Alexandre Laurent Cauchy (1792\u20131857), who became a president of a division of the court of appeal in 1847, and a judge of the court of cassation in 1849; and Eugene Fran\u00e7ois Cauchy (1802\u20131877), a publicist who also wrote several mathematical works.A profound mathematician, Cauchy had a great influence over his contemporaries and successors;[2] Hans Freudenthal stated: \"More concepts and theorems have been named for Cauchy than for any other mathematician (in elasticity alone there are sixteen concepts and theorems named for Cauchy).\"[3] Cauchy was a prolific writer; he wrote approximately eight hundred research articles and five complete textbooks, on a variety of topics in the fields of mathematics and mathematical physics.Baron Augustin-Louis Cauchy FRS FRSE (/ko\u028a\u02c8\u0283i\u02d0/;[1] French:\u00a0[o\u0261yst\u025b\u0303 lwi ko\u0283i]; 21 August 1789\u00a0\u2013 23 May 1857) was a French mathematician, engineer and physicist who made pioneering contributions to several branches of mathematics, including: mathematical analysis and continuum mechanics. He was one of the first to state and prove theorems of calculus rigorously, rejecting the heuristic principle of the generality of algebra of earlier authors. He almost singlehandedly founded complex analysis and the study of permutation groups in abstract algebra.",
            "title": "Augustin-Louis Cauchy",
            "url": "https://en.wikipedia.org/wiki/Augustin_Louis_Cauchy"
        },
        {
            "desc_links": [
                "/wiki/Affine_algebraic_variety",
                "/wiki/Affine_algebraic_set",
                "/wiki/Projective_space",
                "/wiki/Vector_(geometry)",
                "/wiki/Transpose",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Algebraic_equation",
                "/wiki/Dimension",
                "/wiki/Generalization",
                "/wiki/Conic_section",
                "/wiki/Ellipse",
                "/wiki/Parabola",
                "/wiki/Hyperbola",
                "/wiki/Hypersurface",
                "/wiki/Zero_set",
                "/wiki/Irreducible_polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Absolutely_irreducible"
            ],
            "links": [
                "/wiki/Quadratic_set",
                "/wiki/Module_over_a_ring",
                "/wiki/Line_(geometry)",
                "/wiki/Complex_projective_space",
                "/wiki/Ruled_surface",
                "/wiki/Gaussian_curvature",
                "/wiki/Projective_transformation",
                "/wiki/Real_projective_space",
                "/wiki/Sylvester%27s_law_of_inertia",
                "/wiki/Quadratic_form",
                "/wiki/Projective_space",
                "/wiki/Algebraic_variety",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Invertible_matrix",
                "/wiki/Homogeneous_coordinates",
                "/wiki/Projective_space",
                "/wiki/Surface_of_revolution",
                "/wiki/Ellipsoid",
                "/wiki/Paraboloid",
                "/wiki/Hyperboloid",
                "/wiki/Principal_axis_theorem",
                "/wiki/Euclidean_transformation",
                "/wiki/Cartesian_coordinates",
                "/wiki/Quadratic_equation",
                "/wiki/Euclidean_space",
                "/wiki/Orbit_(group_theory)",
                "/wiki/Affine_transformation",
                "/wiki/Euclidean_plane",
                "/wiki/Plane_curve",
                "/wiki/Affine_algebraic_variety",
                "/wiki/Affine_algebraic_set",
                "/wiki/Projective_space",
                "/wiki/Vector_(geometry)",
                "/wiki/Transpose",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Algebraic_equation",
                "/wiki/Dimension",
                "/wiki/Generalization",
                "/wiki/Conic_section",
                "/wiki/Ellipse",
                "/wiki/Parabola",
                "/wiki/Hyperbola",
                "/wiki/Hypersurface",
                "/wiki/Zero_set",
                "/wiki/Irreducible_polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Absolutely_irreducible"
            ],
            "text": "There are generalizations of quadrics: quadratic sets. A quadratic set is a set of points of a projective plane/space, which bears the same geometric properties as a quadric: any line intersects a quadratic set in no or 1 or two lines or is contained in the set.Remark: It is not reasonable to define formally quadrics for \"vector spaces\" (strictly speaking, modules) over genuine skew fields (division rings). Because one would get secants bearing more than 2 points of the quadric which is totally different from usual quadrics. The reason is the following statement.Example:Remark:The linear mappingA quadric is a rather homogeneous object:Lemma:the set of lines.For example:In complex projective space all of the nondegenerate quadrics become indistinguishable from each other.We see that projective transformations don't mix Gaussian curvatures of different sign. This is true for general surfaces. [4]generates the elliptic cylinder, the parabolic cylinder, the hyperbolic cylinder, or the cone, depending on whether the plane at infinity cuts it in a point, a line, two lines, or a nondegenerate conic respectively. These are singly ruled surfaces of zero Gaussian curvature.The degenerate formThe third case generates the hyperbolic paraboloid or the hyperboloid of one sheet, depending on whether the plane at infinity cuts it in two lines, or in a nondegenerate conic respectively. These are doubly ruled surfaces of negative Gaussian curvature.The second case generates the ellipsoid, the elliptic paraboloid or the hyperboloid of two sheets, depending on whether the chosen plane at infinity cuts the quadric in the empty set, in a point, or in a nondegenerate conic respectively. These all have positive Gaussian curvature.The first case is the empty set.by means of a suitable projective transformation (normal forms for singular quadrics can have zeros as well as \u00b11 as coefficients). For surfaces in space (dimension D\u00a0=\u00a02) there are exactly three nondegenerate cases:In real projective space, by Sylvester's law of inertia, a non-singular quadratic form Q(X) may be put into the normal formwhere the coefficients aij are symmetric in i and j. Regarding Q(X)\u00a0=\u00a00 as an equation in projective space exhibits the quadric as a projective algebraic variety. The quadric is said to be non-degenerate if the quadratic form is non-singular; equivalently, if the matrix (aij) is invertible.one introduces new coordinates on RD+2The quadrics can be treated in a uniform manner by introducing homogeneous coordinates on a Euclidean space, thus effectively regarding it as a projective space. Thus if the original (affine) coordinates on RD+1 areWhen two or more of the parameters of the canonical equation are equal, one gets a quadric of revolution, which remains invariant when rotated around an axis (or infinitely many axes, in the case of the sphere).Thus, among the 17 normal forms, there are nine true quadrics: a cone, three cylinders (often called degenerate quadrics) and five non-degenerate quadrics (ellipsoid, paraboloids and hyperboloids), which are detailed in the following tables. The eight remaining quadrics are the imaginary ellipsoid (no real point), the imaginary cylinder (no real point), the imaginary cone (a single real point), and the reducible quadrics, which are decomposed in two planes; there are five such decomposed quadrics, depending whether the planes are distinct or not, parallel or not, real or complex conjugate.The principal axis theorem shows that for any (possibly reducible) quadric, a suitable Euclidean transformation or a change of Cartesian coordinates allows putting the quadratic equation of the quadric into one of the following normal forms:In three-dimensional Euclidean space, quadrics have dimension D\u00a0=\u00a02, and are known as quadric surfaces. They are classified and named by their orbits under affine transformations. More precisely, if an affine transformation maps a quadric onto another one, they belong to the same class, and share the same name and many properties.Quadrics in the Euclidean plane are those of dimension D\u00a0=\u00a01, which is to say that they are plane curves. In this case, one talks of conic sections, or conics.A quadric is an affine algebraic variety, or, if it is reducible, an affine algebraic set. Quadrics may also be defined in projective spaces; see \u00a7\u00a0Projective geometry, below.where x = (x1, x2, ..., xD+1) is a row vector, xT is the transpose of x (a column vector), Q is a (D + 1)\u2009\u00d7\u2009(D + 1) matrix and P is a (D + 1)-dimensional row vector and R a scalar constant. The values Q, P and R are often taken to be over real numbers or complex numbers, but a quadric may be defined over any field.which may be compactly written in vector and matrix notation as:In coordinates x1, x2, ..., xD+1, the general quadric is thus defined by the algebraic equation[1]In mathematics, a quadric or quadric surface (quadric hypersurface in higher dimensions), is a generalization of conic sections (ellipses, parabolas, and hyperbolas). It is a hypersurface (of dimension D) in a (D + 1)-dimensional space, and it is defined as the zero set of an irreducible polynomial of degree two in D + 1 variables (D=1 in the case of conic sections). When the defining polynomial is not absolutely irreducible, the zero set is generally not considered a quadric, although it is often called a degenerate quadric or a reducible quadric.",
            "title": "Quadric",
            "url": "https://en.wikipedia.org/wiki/Quadric_surface"
        },
        {
            "desc_links": [
                "/wiki/Graph_(discrete_mathematics)",
                "/wiki/Adjacency_matrix",
                "/wiki/Graph_invariant",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Polynomial",
                "/wiki/Matrix_similarity",
                "/wiki/Eigenvalues",
                "/wiki/Root_of_a_polynomial",
                "/wiki/Determinant",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Endomorphism",
                "/wiki/Vector_space"
            ],
            "links": [
                "/wiki/Secular_phenomena",
                "/wiki/Joseph_Louis_Lagrange",
                "/wiki/Open_subset",
                "/wiki/Topological_space",
                "/wiki/Zariski_topology",
                "/wiki/Non-singular_matrix",
                "/wiki/Similar_matrices",
                "/wiki/Transpose",
                "/wiki/Triangular_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Jordan_normal_form",
                "/wiki/Similar_matrices",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Exterior_algebra",
                "/wiki/Root_of_a_function",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Polynomial_expression",
                "/wiki/Trace_(matrix)",
                "/wiki/Hyperbolic_function",
                "/wiki/Hyperbolic_angle",
                "/wiki/Determinant",
                "/wiki/Monic_polynomial",
                "/wiki/Identity_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Singular_matrix",
                "/wiki/Determinant",
                "/wiki/Eigenvector",
                "/wiki/Diagonal_matrix",
                "/wiki/Graph_(discrete_mathematics)",
                "/wiki/Adjacency_matrix",
                "/wiki/Graph_invariant",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Polynomial",
                "/wiki/Matrix_similarity",
                "/wiki/Eigenvalues",
                "/wiki/Root_of_a_polynomial",
                "/wiki/Determinant",
                "/wiki/Trace_(linear_algebra)",
                "/wiki/Endomorphism",
                "/wiki/Vector_space"
            ],
            "text": "Secular equation may have several meanings.The term secular function has been used for what is now called characteristic polynomial (in some literature the term secular function is still used). The term comes from the fact that the characteristic polynomial was used to calculate secular perturbations (on a time scale of a century, i.e. slow compared to annual motion) of planetary orbits, according to Lagrange's theory of oscillations.To prove this, one may suppose n > m, by exchanging, if needed, A and B. Then, by bordering A on the bottom by n \u2013 m rows of zeros, and B on the right, by, n \u2013 m columns of zeros, one gets two n\u00d7n matrices A' and B' such that B'A' = BA, and A'B' is equal to AB bordered by n \u2013 m rows and columns of zeros. The result follows from the case of square matrices, by comparing the characteristic polynomials of A'B' and AB.More generally, if A is a matrix of order m\u00d7n and B is a matrix of order n\u00d7m, then AB is m\u00d7m and BA is n\u00d7n matrix, and one hasFor the case where both A and B are singular, one may remark that the desired identity is an equality between polynomials in t and the coefficients of the matrices. Thus, to prove this equality, it suffices to prove that it is verified on a non-empty open subset (for the usual topology, or, more generally, for the Zariski topology) of the space of all the coefficients. As the non-singular matrices form such an open subset of the space of all matrices, this proves the result.When A is non-singular this result follows from the fact that AB and BA are similar:If A and B are two square n\u00d7n matrices then characteristic polynomials of AB and BA coincide:The matrix A and its transpose have the same characteristic polynomial. A is similar to a triangular matrix if and only if its characteristic polynomial can be completely factored into linear factors over K (the same is true with the minimal polynomial instead of the characteristic polynomial). In this case A is similar to a matrix in Jordan normal form.Two similar matrices have the same characteristic polynomial. The converse however is not true in general: two matrices with the same characteristic polynomial need not be similar.The Cayley\u2013Hamilton theorem states that replacing t by A in the characteristic polynomial (interpreting the resulting powers as matrix powers, and the constant term c as c times the identity matrix) yields the zero matrix. Informally speaking, every matrix satisfies its own characteristic equation. This statement is equivalent to saying that the minimal polynomial of A divides the characteristic polynomial of A.When the characteristic is 0 it may alternatively be computed as a single determinant, that of the k\u00d7k matrix,Using the language of exterior algebra, one may compactly express the characteristic polynomial of an n\u00d7n matrix A asFor a 2\u00d72 matrix A, the characteristic polynomial is thus given byThe polynomial pA(t) is monic (its leading coefficient is 1) and its degree is n. The most important fact about the characteristic polynomial was already mentioned in the motivational paragraph: the eigenvalues of A are precisely the roots of pA(t) (this also holds for the minimal polynomial of A, but its degree may be less than n). The coefficients of the characteristic polynomial are all polynomial expressions in the entries of the matrix. In particular its constant coefficient pA\u00a0(0)\u00a0 is det(\u2212A) = (\u22121)n det(A), the coefficient of tn is one, and the coefficient of tn\u22121 is tr(\u2212A) = \u2212tr(A), where tr(A) is the matrix trace of\u00a0A. (The signs given here correspond to the formal definition given in the previous section;[2] for the alternative definition these would instead be det(A) and (\u22121)n\u00a0\u2212\u00a01 tr(A) respectively.[3])Its characteristic polynomial isAnother example uses hyperbolic functions of a hyperbolic angle \u03c6. For the matrix takeWe now compute the determinant ofSuppose we want to compute the characteristic polynomial of the matrixSome authors define the characteristic polynomial to be det(A\u00a0-\u00a0t\u00a0I). That polynomial differs from the one defined here by a sign (\u22121)n, so it makes no difference for properties like having as roots the eigenvalues of A; however the current definition always gives a monic polynomial, whereas the alternative definition always has constant term det(A).where I denotes the n-by-n identity matrix.We consider an n\u00d7n matrix A. The characteristic polynomial of A, denoted by pA(t), is the polynomial defined by(where I is the identity matrix). Since v is non-zero, this means that the matrix \u03bb I\u00a0\u2212\u00a0A is singular (non-invertible), which in turn means that its determinant is 0. Thus the roots of the function det(\u03bb\u00a0I\u00a0\u2212\u00a0A) are the eigenvalues of A, and it is clear that this determinant is a polynomial in \u03bb.orFor a general matrix A, one can proceed as follows. A scalar \u03bb is an eigenvalue of A if and only if there is an eigenvector v \u2260 0 such thatThis works because the diagonal entries are also the eigenvalues of this matrix.Given a square matrix A, we want to find a polynomial whose zeros are the eigenvalues of A. For a diagonal matrix A, the characteristic polynomial is easy to define: if the diagonal entries are a1,\u00a0a2,\u00a0a3,\u00a0etc. then the characteristic polynomial will be:The characteristic polynomial of a graph is the characteristic polynomial of its adjacency matrix. It is a graph invariant, though it is not complete: the smallest pair of non-isomorphic graphs with the same characteristic polynomial have five nodes.[1]In linear algebra, the characteristic polynomial of a square matrix is a polynomial which is invariant under matrix similarity and has the eigenvalues as roots. It has the determinant and the trace of the matrix as coefficients. The characteristic polynomial of an endomorphism of vector spaces of finite dimension is the characteristic polynomial of the matrix of the endomorphism over any base; it does not depend on the choice of a basis. The characteristic equation is the equation obtained by equating to zero the characteristic polynomial.",
            "title": "Characteristic polynomial",
            "url": "https://en.wikipedia.org/wiki/Secular_equation"
        },
        {
            "desc_links": [
                "/wiki/Geometry",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Functional_analysis",
                "/wiki/Engineering",
                "/wiki/Mathematical_model",
                "/wiki/Nonlinear_system",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Mathematics",
                "/wiki/Linear_equation"
            ],
            "links": [
                "/wiki/Algebraic_geometry",
                "/wiki/Systems_of_polynomial_equations",
                "/wiki/Representation_theory",
                "/wiki/Functional_analysis",
                "/wiki/Mathematical_analysis",
                "/wiki/Lp_space",
                "/wiki/Algebra_over_a_field",
                "/wiki/Multilinear_algebra",
                "/wiki/Dual_space",
                "/wiki/Tensor_product",
                "/wiki/Module_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Free_module",
                "/wiki/Linear_map",
                "/wiki/Linear_functional",
                "/wiki/Cramer%27s_rule",
                "/wiki/System_of_linear_equations",
                "/wiki/Homogeneous_coordinates",
                "/wiki/Linear_equation",
                "/wiki/Inner_product_space",
                "/wiki/Partial_differential_equation",
                "/wiki/Dirichlet_conditions",
                "/wiki/Fourier_series",
                "/wiki/Linear_least_squares_(mathematics)",
                "/wiki/Triangular_form",
                "/wiki/Normal_matrix",
                "/wiki/Hermitian_conjugate",
                "/wiki/Cauchy%E2%80%93Schwarz_inequality",
                "/wiki/Axiom",
                "/wiki/Inner_product",
                "/wiki/Bilinear_form",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Polynomial",
                "/wiki/Algebraically_closed_field",
                "/wiki/Complex_number",
                "/wiki/Invariant_(mathematics)#Invariant_set",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Characteristic_value",
                "/wiki/Determinant",
                "/wiki/Invertible_matrix",
                "/wiki/Inverse_element",
                "/wiki/Nullspace",
                "/wiki/Cramer%27s_rule",
                "/wiki/Gaussian_elimination",
                "/wiki/Standard_basis",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Coordinate_system",
                "/wiki/Linear_combination",
                "/wiki/Cardinality",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Well-defined",
                "/wiki/Dimension_theorem_for_vector_spaces",
                "/wiki/Linearly_independent",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Axiom_of_choice",
                "/wiki/Rationals",
                "/wiki/Linear_span",
                "/wiki/Linear_subspace",
                "/wiki/Nullspace",
                "/wiki/Linear_combination",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Origin_(mathematics)",
                "/wiki/Bijective",
                "/wiki/Isomorphic",
                "/wiki/Determinant",
                "/wiki/Range_(mathematics)",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Linear_transformation",
                "/wiki/Map_(mathematics)",
                "/wiki/Abelian_group",
                "/wiki/Sequence",
                "/wiki/Function_(mathematics)",
                "/wiki/Polynomial_ring",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Set_(mathematics)",
                "/wiki/Binary_operation",
                "/wiki/Element_(mathematics)",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Axiom",
                "/wiki/School_Mathematics_Study_Group",
                "/wiki/Secondary_school",
                "/wiki/Singular-value_decomposition",
                "/wiki/Determinants",
                "/wiki/Gaussian_elimination",
                "/wiki/H%C3%BCseyin_Tevfik_Pasha",
                "/wiki/Peano",
                "/wiki/Abstract_algebra",
                "/wiki/Quantum_mechanics",
                "/wiki/Special_relativity",
                "/wiki/Statistics",
                "/wiki/Algorithm",
                "/wiki/Hermann_Grassmann",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Arthur_Cayley",
                "/wiki/Determinant",
                "/wiki/Systems_of_linear_equations",
                "/wiki/Gottfried_Wilhelm_Leibniz",
                "/wiki/Gabriel_Cramer",
                "/wiki/Cramer%27s_Rule",
                "/wiki/Gauss",
                "/wiki/Gaussian_elimination",
                "/wiki/Geodesy",
                "/wiki/Geometry",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Functional_analysis",
                "/wiki/Engineering",
                "/wiki/Mathematical_model",
                "/wiki/Nonlinear_system",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Mathematics",
                "/wiki/Linear_equation"
            ],
            "text": "There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to.Algebraic geometry considers the solutions of systems of polynomial equations.Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces.If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \u00d7 V \u2192 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V\u2217 consisting of linear maps f: V \u2192 F where F is the field of scalars. Multilinear maps T: Vn \u2192 F can be described via tensor products of elements of V\u2217.Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.The set of points of a linear functional that map to zero define the kernel of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.[25][27]Notice that if h is a solution to this homogeneous equation, then t h is also a solution.The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,For convenience the free parameter x has been relabeled t.Solve for y and obtain the inverse image as the set of points,In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equationNotice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c.The set of points in the plane E that map to the same image in R under the linear functional \u03bb define a line in E. This line is the image of the inverse map, \u03bb\u22121: R\u2192E. This inverse image is the set of the points x=(x, y) that solve the equation,Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.[25][27]These equations can be assembled into the single matrix equation,These coordinate functionals have the properties,which can be written in matrix form asThe functionals \u03c3 and \u03c4 compute the components of x along the basis vectors v and w, respectively, that is,To solve this equation for \u03b1, \u03b2, we compute the linear coordinate functionals \u03c3 and \u03c4 for the basis v, w, which are given by,[26]This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E. Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is to find the real numbers \u03b1, \u03b2, so that x=\u03b1v+\u03b2w, that iswhere Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form asThis is true for any pair of vectors used to define coordinates in E. Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E. This means a vector x has coordinates (\u03b1,\u03b2), such that x=\u03b1v+\u03b2w. Then, we have the linear functionalThus, the columns of the matrix A are the image of the basis vectors of E in R.Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj. It is now possible to see thatThis shows that the sum of vectors in E map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation.[25] For this case, where the image space is a real number the map is called a linear functional.[27]This transformation has the important property that if Ay=d, thenorAnother way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in E and consider the linear function \u03bb: E\u2192R, given byClearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E. For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.which in homogeneous form yields,It is interesting to consider the case of three lines, \u03bb1, \u03bb2 and \u03bb3, which yield the matrix equation,if the rows of B are linearly independent (i.e., \u03bb1 and \u03bb2 represent distinct lines). Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns.[27] Notice that this yields a point in the z = 1 plane only when the 2\u2009\u00d7\u20092 submatrix associated with x3 has a non-zero determinant.The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates, the solutions are multiples of the following solution:[26]or using homogeneous coordinates,which forms a system of linear equations. The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,Now consider the equations of the two lines \u03bb1 and \u03bb2,The linear equation, \u03bb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \u03b1x1 + \u03b2x2 is also on the line, for any real \u03b1 and \u03b2.Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space. The x\u2212y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1).where x = (x, y, 1) is the 3\u2009\u00d7\u20091 set of homogeneous coordinates associated with the point (x, y).[26]orwhere a, b and c are not all zero. Then,Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E. When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n \u2265 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient ak, we take the inner product with hk:The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the \"same\" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner productThis series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.Fourier series are a representation of a function f: [\u2212\u03c0, \u03c0] \u2192 R as a trigonometric series:The least squares method is used to determine the best-fit line for a set of data.[24] This line will minimize the sum of the squares of the residuals.We can, in general, write any system of linear equations as a matrix equation:The system is solved.Next, z and y can be substituted into L1, which can be solved to obtainThen, z can be substituted into L2, which can then be solved to obtainThe last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen thatThis result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.The result is:Now y is eliminated from L3 by adding \u22124L2 to L3:The result is:In the example, x is eliminated from L2 by adding (3/2)L1 to L2. x is then eliminated from L3 by adding L1 to L3. Formally:The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.If T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.The inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfyingand so we can call this quantity the cosine of the angle between the two vectors.In particular, the quantityand we can prove the Cauchy\u2013Schwarz inequality:We can define the length of a vector v in V byNote that in R, it is symmetric.that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a mapSuch a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).where I is the identity matrix. For there to be nontrivial solutions to that equation, det(T \u2212 \u03bb I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ..., vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \u03bb1, \u03bb2, ..., \u03bbn, and if v = a1v1 + ... + an vn, then,To find an eigenvector or an eigenvalue, we note thatIn general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T. If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector. The scalar \u03bb such that Tv = \u03bbv is called a characteristic value or eigenvalue of T.One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20]). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V. While Rn has a standard basis {e1, e2, ..., en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V).The condition that v1, v2, ..., vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ..., vn assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to v). In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn. Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn. Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \u2192 W may be encoded by an m \u00d7 n matrix A with entries in the field F, called the matrix of T with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.A particular basis {v1, v2, ..., vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ..., an) is the linear combinationOne often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism.Any two bases of a vector space V have the same cardinality, which is called the dimension of V. The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of V has finite number of elements, V is called a finite-dimensional vector space. If V is finite-dimensional and U is a subspace of V, then dim U \u2264 dim V. If U1 and U2 are subspaces of V, thenA linear combination of any system of vectors with all zero coefficients is the zero vector of V. If this is the only way to express the zero vector as a linear combination of v1, v2, ..., vk then these vectors are linearly independent. Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V. Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis.[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed.where a1, a2, ..., ak are scalars. The set of all linear combinations of vectors v1, v2, ..., vk is called their span, which forms a subspace.Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ..., vk:Linear transformations have geometric significance. For example, 2 \u00d7 2 real matrices denote standard planar mappings that preserve the origin.When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.Additionally for any vectors u, v \u2208 V and scalars a, b \u2208 F:for any vectors u,v \u2208 V and a scalar a \u2208 F.that is compatible with addition and scalar multiplication:Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a mapThe first four axioms are those of V being an abelian group under vector addition. Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.The main structures of linear algebra are vector spaces. A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms. Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The operations of addition and multiplication in a vector space must satisfy the following axioms.[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s.[7] Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s.[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum.[9] In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation.[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular-value decomposition (SVD), as 'so many other disciplines use it'.[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination.[13][14]The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.In 1882, H\u00fcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\".[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.[4]The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".[4]The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations. Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's Rule. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.[4]Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation.and their representations through matrices and vector spaces.[1][2][3]linear functions such asLinear algebra is the branch of mathematics concerning linear equations such as",
            "title": "Linear algebra",
            "url": "https://en.wikipedia.org/wiki/Linear_algebra"
        },
        {
            "desc_links": [
                "/wiki/Numerical_analysis",
                "/wiki/Matrix_(mathematics)#Decomposition",
                "/wiki/Sparse_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Finite_element_method",
                "/wiki/Derivative_(calculus)",
                "/wiki/Taylor_series",
                "/wiki/Physics",
                "/wiki/Classical_mechanics",
                "/wiki/Optics",
                "/wiki/Electromagnetism",
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum_electrodynamics",
                "/wiki/Rigid-body_dynamics",
                "/wiki/Computer_graphics",
                "/wiki/3D_models",
                "/wiki/Glossary_of_computer_graphics#screen_space",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Stochastic_matrix",
                "/wiki/PageRank",
                "/wiki/Matrix_calculus",
                "/wiki/Mathematical_analysis",
                "/wiki/Derivative",
                "/wiki/Exponentials",
                "/wiki/Economics",
                "/wiki/Matrix_addition",
                "/wiki/Conformable_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_multiplication#Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Linear_transformation",
                "/wiki/Linear_functions",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Euclidean_vector",
                "/wiki/Dimension",
                "/wiki/Rotation_matrix",
                "/wiki/Column_vector",
                "/wiki/Position_(vector)",
                "/wiki/Transformation_matrices",
                "/wiki/Function_composition",
                "/wiki/Transformation_(function)",
                "/wiki/System_of_linear_equations",
                "/wiki/Determinant",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Zero",
                "/wiki/Geometry",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Mathematics",
                "/wiki/Rectangle",
                "/wiki/Number",
                "/wiki/Symbol_(formal)",
                "/wiki/Expression_(mathematics)"
            ],
            "links": [
                "/wiki/Alfred_Tarski",
                "/wiki/Truth_table",
                "/wiki/Bertrand_Russell",
                "/wiki/Alfred_North_Whitehead",
                "/wiki/Axiom_of_reducibility",
                "/wiki/Extension_(predicate_logic)",
                "/wiki/Matrix_mechanics",
                "/wiki/Werner_Heisenberg",
                "/wiki/Max_Born",
                "/wiki/Pascual_Jordan",
                "/wiki/John_von_Neumann",
                "/wiki/Mathematical_formulation_of_quantum_mechanics",
                "/wiki/Functional_analysis",
                "/wiki/Linear_operator",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_space",
                "/wiki/Hamel_dimension",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Georg_Frobenius",
                "/wiki/Bilinear_form",
                "/wiki/Gauss%E2%80%93Jordan_elimination",
                "/wiki/Gauss_elimination",
                "/wiki/Wilhelm_Jordan_(geodesist)",
                "/wiki/Hypercomplex_number",
                "/wiki/Multiplication",
                "/wiki/Eigenvalue",
                "/wiki/Carl_Gustav_Jacob_Jacobi",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Infinitesimal",
                "/wiki/Leopold_Kronecker",
                "/wiki/Karl_Weierstrass",
                "/wiki/Axiom",
                "/wiki/Number_theory",
                "/wiki/Gauss",
                "/wiki/Quadratic_form",
                "/wiki/Linear_map",
                "/wiki/Gotthold_Eisenstein",
                "/wiki/Matrix_product",
                "/wiki/Non-commutative",
                "/wiki/Augustin-Louis_Cauchy",
                "/wiki/Polynomial",
                "/wiki/Arthur_Cayley",
                "/wiki/Arthur_Cayley",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Linear_equations",
                "/wiki/Chinese_mathematics",
                "/wiki/The_Nine_Chapters_on_the_Mathematical_Art",
                "/wiki/System_of_linear_equations",
                "/wiki/Determinant",
                "/wiki/Gerolamo_Cardano",
                "/wiki/Japanese_mathematics",
                "/wiki/Seki_Kowa",
                "/wiki/Jan_de_Witt",
                "/wiki/Gottfried_Wilhelm_Leibniz",
                "/wiki/Gabriel_Cramer",
                "/wiki/Cramer%27s_rule",
                "/wiki/Electronics",
                "/wiki/Electrical_impedance",
                "/wiki/Admittance",
                "/wiki/Dimensionless_quantity",
                "/wiki/Mesh_analysis",
                "/wiki/Nodal_analysis",
                "/wiki/Geometrical_optics",
                "/wiki/Light_wave",
                "/wiki/Ray_(optics)",
                "/wiki/Ray_(geometry)",
                "/wiki/Lens_(optics)",
                "/wiki/Ray_transfer_matrix",
                "/wiki/Equation_of_motion",
                "/wiki/Eigenvector",
                "/wiki/Normal_mode",
                "/wiki/Molecules",
                "/wiki/Particle_accelerator",
                "/wiki/S-matrix",
                "/wiki/Quantum_mechanics",
                "/wiki/Werner_Heisenberg",
                "/wiki/Matrix_mechanics",
                "/wiki/Density_matrix",
                "/wiki/Eigenstates",
                "/wiki/Symmetry",
                "/wiki/Elementary_particle",
                "/wiki/Quantum_field_theory",
                "/wiki/Lorentz_group",
                "/wiki/Spin_group",
                "/wiki/Pauli_matrices",
                "/wiki/Gamma_matrices",
                "/wiki/Fermion",
                "/wiki/Spinor",
                "/wiki/Quark",
                "/wiki/Special_unitary_group",
                "/wiki/Gell-Mann_matrices",
                "/wiki/Gauge_group",
                "/wiki/Quantum_chromodynamics",
                "/wiki/Cabibbo%E2%80%93Kobayashi%E2%80%93Maskawa_matrix",
                "/wiki/Weak_interaction",
                "/wiki/Mass",
                "/wiki/Random_matrix",
                "/wiki/Probability_distribution",
                "/wiki/Matrix_normal_distribution",
                "/wiki/Number_theory",
                "/wiki/Physics",
                "/wiki/Singular_value_decomposition",
                "/wiki/Descriptive_statistics",
                "/wiki/Data_matrix_(multivariate_statistics)",
                "/wiki/Dimensionality_reduction",
                "/wiki/Covariance_matrix",
                "/wiki/Variance",
                "/wiki/Random_variable",
                "/wiki/Linear_least_squares_(mathematics)",
                "/wiki/Stochastic_matrix",
                "/wiki/Probability_vector",
                "/wiki/Markov_chain",
                "/wiki/Absorbing_state",
                "/wiki/Finite_element_method",
                "/wiki/Partial_differential_equation",
                "/wiki/Elliptic_partial_differential_equation",
                "/wiki/Implicit_function_theorem",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Hessian_matrix",
                "/wiki/Differentiable_function",
                "/wiki/Second_derivative",
                "/wiki/Adjacency_matrix",
                "/wiki/Finite_graph",
                "/wiki/Graph_theory",
                "/wiki/Logical_matrix",
                "/wiki/Distance_matrix",
                "/wiki/Website",
                "/wiki/Hyperlink",
                "/wiki/Sparse_matrix",
                "/wiki/Network_theory",
                "/wiki/Chemistry",
                "/wiki/Quantum_mechanics",
                "/wiki/Chemical_bond",
                "/wiki/Spectroscopy",
                "/wiki/Overlap_matrix",
                "/wiki/Fock_matrix",
                "/wiki/Roothaan_equations",
                "/wiki/Molecular_orbital",
                "/wiki/Hartree%E2%80%93Fock",
                "/wiki/Encryption",
                "/wiki/Hill_cipher",
                "/wiki/Computer_graphics",
                "/wiki/Rotation_matrix",
                "/wiki/Polynomial_ring",
                "/wiki/Control_theory",
                "/wiki/Absolute_value",
                "/wiki/Quaternion",
                "/wiki/Clifford_algebra",
                "/wiki/Game_theory",
                "/wiki/Economics",
                "/wiki/Payoff_matrix",
                "/wiki/Text_mining",
                "/wiki/Thesaurus",
                "/wiki/Document-term_matrix",
                "/wiki/Tf-idf",
                "/wiki/Zero_vector_space",
                "/wiki/Computer_algebra_system",
                "/wiki/Empty_product",
                "/wiki/Hilbert_space#Operators_on_Hilbert_spaces",
                "/wiki/Continuous_function",
                "/wiki/Functional_analysis",
                "/wiki/Absolutely_convergent_series",
                "/wiki/Linear_combination",
                "/wiki/Finite_group",
                "/wiki/Isomorphic",
                "/wiki/Regular_representation",
                "/wiki/Symmetric_group",
                "/wiki/Representation_theory",
                "/wiki/Orthogonal_group",
                "/wiki/Determinant",
                "/wiki/Subgroup",
                "/wiki/Special_linear_group",
                "/wiki/Orthogonal_matrix",
                "/wiki/Group_(mathematics)",
                "/wiki/Binary_operation",
                "/wiki/General_linear_group",
                "/wiki/Matrix_ring",
                "/wiki/Endomorphism_ring",
                "/wiki/Matrix_equivalence",
                "/wiki/Transpose_of_a_linear_map",
                "/wiki/Dual_space",
                "/wiki/Hamel_dimension",
                "/wiki/Vector_space",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Block_matrix",
                "/wiki/Ring_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Matrix_ring",
                "/wiki/Endomorphism_ring",
                "/wiki/Module_(mathematics)",
                "/wiki/Commutative_ring",
                "/wiki/Associative_algebra",
                "/wiki/Determinant",
                "/wiki/Leibniz_formula_(determinant)",
                "/wiki/Invertible",
                "/wiki/Superring",
                "/wiki/Supermatrix",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Addition",
                "/wiki/Subtraction",
                "/wiki/Multiplication",
                "/wiki/Division_(mathematics)",
                "/wiki/Rational_number",
                "/wiki/Finite_field",
                "/wiki/Coding_theory",
                "/wiki/Eigenvalue",
                "/wiki/Algebraically_closed_field",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Tensors",
                "/wiki/Group_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Matrix_ring",
                "/wiki/Field_(mathematics)",
                "/wiki/Matrix_field",
                "/wiki/Matrix_exponential",
                "/wiki/Linear_differential_equation",
                "/wiki/Matrix_logarithm",
                "/wiki/Square_root_of_a_matrix",
                "/wiki/Condition_number",
                "/wiki/Schur_decomposition",
                "/wiki/Eigendecomposition",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Jordan_normal_form",
                "/wiki/LU_decomposition",
                "/wiki/Triangular_matrix",
                "/wiki/Forward_substitution",
                "/wiki/Row_echelon_form",
                "/wiki/Elementary_matrix",
                "/wiki/Permutation_matrix",
                "/wiki/Singular_value_decomposition",
                "/wiki/Unitary_matrix",
                "/wiki/Computer_language",
                "/wiki/HP_9830",
                "/wiki/APL_(programming_language)",
                "/wiki/List_of_numerical_analysis_software",
                "/wiki/Matrix_norm",
                "/wiki/Condition_number",
                "/wiki/Adjugate_matrix",
                "/wiki/Sparse_matrix",
                "/wiki/Conjugate_gradient_method",
                "/wiki/Upper_bound",
                "/wiki/Matrix_multiplication_algorithm",
                "/wiki/Strassen_algorithm",
                "/wiki/Numerical_linear_algebra",
                "/wiki/Complexity_analysis",
                "/wiki/Numerical_stability",
                "/wiki/Sequence_(mathematics)",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Infinity",
                "/wiki/Indeterminate_(variable)",
                "/wiki/Characteristic_polynomial",
                "/wiki/Monic_polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Cayley%E2%80%93Hamilton_theorem",
                "/wiki/Zero_matrix",
                "/wiki/Logical_equivalence",
                "/wiki/Laplace_expansion",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Linear_system",
                "/wiki/Cramer%27s_rule",
                "/wiki/Rule_of_Sarrus",
                "/wiki/Leibniz_formula_for_determinants",
                "/wiki/If_and_only_if",
                "/wiki/Absolute_value",
                "/wiki/Trace_of_a_matrix",
                "/wiki/Complex_number",
                "/wiki/Unitary_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Determinant",
                "/wiki/Determinant",
                "/wiki/Linear_transformation",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Reflection_(mathematics)",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_(mathematics)#Square_matrices",
                "/wiki/Real_number",
                "/wiki/Orthogonal",
                "/wiki/Unit_vector",
                "/wiki/Orthonormality",
                "/wiki/Transpose",
                "/wiki/Invertible_matrix",
                "/wiki/Bilinear_form",
                "/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices",
                "/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices",
                "/wiki/Positive-definite_matrix",
                "/wiki/Quadratic_form",
                "/wiki/Identity_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Invertible_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Spectral_theorem",
                "/wiki/Eigenbasis",
                "/wiki/Linear_combination",
                "/wiki/Symmetric_matrix",
                "/wiki/Skew-symmetric_matrix",
                "/wiki/Hermitian_matrix",
                "/wiki/Asterisk",
                "/wiki/Conjugate_transpose",
                "/wiki/Complex_conjugate",
                "/wiki/Diagonal_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Triangular_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Square_matrix",
                "/wiki/Main_diagonal",
                "/wiki/Rank_of_a_matrix",
                "/wiki/Linear_independence",
                "/wiki/Hamel_dimension",
                "/wiki/Image_(mathematics)",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Kernel_(matrix)",
                "/wiki/Bijection",
                "/wiki/Function_composition",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Unit_vector",
                "/wiki/Inverse_matrix",
                "/wiki/Generalized_inverse",
                "/wiki/Independent_equation",
                "/wiki/Minor_(linear_algebra)",
                "/wiki/Determinant",
                "/wiki/Linear_equation",
                "/wiki/Matrix_inverse",
                "/wiki/Hadamard_product_(matrices)",
                "/wiki/Kronecker_product",
                "/wiki/Sylvester_equation",
                "/wiki/Commutativity",
                "/wiki/Associativity",
                "/wiki/Distributivity",
                "/wiki/Dot_product",
                "/wiki/Commutative",
                "/wiki/Set_(mathematics)",
                "/wiki/Box_bracket",
                "/wiki/Parens",
                "/wiki/Row_vector",
                "/wiki/Column_vectors",
                "/wiki/Square_matrix",
                "/wiki/Number",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_numbers",
                "/wiki/Complex_numbers",
                "/wiki/Numerical_analysis",
                "/wiki/Matrix_(mathematics)#Decomposition",
                "/wiki/Sparse_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Finite_element_method",
                "/wiki/Derivative_(calculus)",
                "/wiki/Taylor_series",
                "/wiki/Physics",
                "/wiki/Classical_mechanics",
                "/wiki/Optics",
                "/wiki/Electromagnetism",
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum_electrodynamics",
                "/wiki/Rigid-body_dynamics",
                "/wiki/Computer_graphics",
                "/wiki/3D_models",
                "/wiki/Glossary_of_computer_graphics#screen_space",
                "/wiki/Probability_theory",
                "/wiki/Statistics",
                "/wiki/Stochastic_matrix",
                "/wiki/PageRank",
                "/wiki/Matrix_calculus",
                "/wiki/Mathematical_analysis",
                "/wiki/Derivative",
                "/wiki/Exponentials",
                "/wiki/Economics",
                "/wiki/Matrix_addition",
                "/wiki/Conformable_matrix",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_multiplication#Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Linear_transformation",
                "/wiki/Linear_functions",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Euclidean_vector",
                "/wiki/Dimension",
                "/wiki/Rotation_matrix",
                "/wiki/Column_vector",
                "/wiki/Position_(vector)",
                "/wiki/Transformation_matrices",
                "/wiki/Function_composition",
                "/wiki/Transformation_(function)",
                "/wiki/System_of_linear_equations",
                "/wiki/Determinant",
                "/wiki/Invertible_matrix",
                "/wiki/If_and_only_if",
                "/wiki/Zero",
                "/wiki/Geometry",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Mathematics",
                "/wiki/Rectangle",
                "/wiki/Number",
                "/wiki/Symbol_(formal)",
                "/wiki/Expression_(mathematics)"
            ],
            "text": "Alfred Tarski in his 1946 Introduction to Logic used the word \"matrix\" synonymously with the notion of truth table as used in mathematical logic.[118]For example, a function \u03a6(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by \"considering\" the function for all possible values of \"individuals\" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, \u2200ai: \u03a6(ai, y), can be reduced to a \"matrix\" of values by \"considering\" the function for all possible values of \"individuals\" bi substituted in place of variable y:Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910\u20131913) use the word \"matrix\" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the \"bottom\" (0 order) the function is identical to its extension:The word has been used in unusual ways by at least two authors of historical importance.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.Many theorems were first established for small matrices only, for example the Cayley\u2013Hamilton theorem was proved for 2\u00d72 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4\u00d74 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss\u2013Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra.[115] partially due to their use in classification of the hypercomplex number systems of the previous century.where \u03a0 denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied \"functional determinants\"\u2014later called Jacobi determinants by Sylvester\u2014which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen \u00fcber die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy \u2212 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialAn English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley\u2013Hamilton theorem.[103]The term \"matrix\" (Latin for \"womb\", derived from mater\u2014mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th\u20132nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104] The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H \u00b7 A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the \"mixed\" state of a quantum system as a linear combination of elementary, \"pure\" eigenstates.[97]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo\u2013Kobayashi\u2013Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear functionStochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn \u2192 Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]The Hessian matrix of a differentiable function \u0192: Rn \u2192 R consists of the second derivatives of \u0192 with respect to the several coordinate directions, that is,[81]The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example \"yes\" and \"no\", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree\u2013Fock method.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Complex numbers can be represented by particular real 2-by-2 matrices viaThere are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant\u00a01, a fact that is often used as a part of the characterization of determinants.In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V\u2192W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector\u00a0v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A\u00b7v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]form the orthogonal group.[67] Every orthogonal matrix has determinant 1 or \u22121. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionA group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.More generally, the set of m\u00d7n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n\u00a0=\u00a0m composition of these maps is possible, and this gives rise to the matrix ring of n\u00d7n matrices representing the endomorphism ring of Rn.In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]Linear maps Rn \u2192 Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V \u2192 W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatMatrices do not always have all their entries in the same ring\u00a0\u2013 or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.More generally, abstract algebra makes great use of matrices with entries in a ring R.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]The eigendecomposition or diagonalization expresses A as a product VDV\u22121, where D is a diagonal matrix and V is a suitable invertible matrix.[52] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues \u03bb1 to \u03bbn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaThe LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV\u2217, where U and V are unitary matrices and D is a diagonal matrix.There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace expansion (Adj (A) denotes the adjugate matrix of A)In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this \"naive\" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn\u2212A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(\u03bb)\u00a0=\u00a00 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley\u2013Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number \u03bb is an eigenvalue of an n\u00d7n-matrix A if and only if A\u2212\u03bbIn is not invertible, which is equivalent toA number \u03bb and a non-zero vector v satisfyingAdding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by \u22121.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]The determinant of a product of square matrices equals the product of their determinants:The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of 2-by-2 matrices is given byThe determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.Also, the trace of a matrix is equal to that of its transpose, that is,This is immediate from the definition of matrix multiplication:The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:The complex analogue of an orthogonal matrix is a unitary matrix.An orthogonal matrix A is necessarily invertible (with inverse A\u22121 = AT), unitary (A\u22121 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or \u22121. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.where I is the identity matrix of size n.which entailsAn orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:Allowing as input two different vectors instead yields the bilinear form associated to A:A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.produces only positive values for any input vector x. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric n\u00d7n-matrix A is called positive-definite if for all nonzero vectors x\u00a0\u2208\u00a0Rn the associated quadratic form given bywhere In is the n\u00d7n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A\u22121.A square matrix A is called invertible or non-singular if there exists a matrix B such thatBy the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = \u2212AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A\u2217 = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank\u2013nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]The last equality follows from the above-mentioned associativity of matrix multiplication.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g\u00a0: Rm \u2192 Rk, then the composition g \u2218 f is represented by BA sinceThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.For example, the 2\u00d72 matrixMatrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn \u2192 Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn \u2192 Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.where A\u22121 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingis equivalent to the system of linear equationsMatrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n\u00d71-matrix) of n variables x1, x2, ..., xn, and b is an m\u00d71-column vector, then the matrix equationA principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix to be one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.There are three types of row operations:Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.whereasthat is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m \u2260 k. Even if both products are defined, they need not be equal, that is, generallywhere 1 \u2264 i \u2264 m and 1 \u2264 j \u2264 p.[13] For example, the underlined entry 2340 in the product is calculated as (2 \u00d7 1000) + (3 \u00d7 100) + (4 \u00d7 10) = 2340:Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A\u00a0+\u00a0B\u00a0=\u00a0B\u00a0+\u00a0A.[12] The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A\u00a0+\u00a0B)T\u00a0=\u00a0AT\u00a0+\u00a0BT. Finally, (AT)T\u00a0=\u00a0A.There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,\u2217 refers to the ith row of A, and a\u2217,j refers to the jth column of A. The set of all m-by-n matrices is denoted \ud835\udd44(m, n).Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-\u00d7-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 \u2264 i \u2264 m \u2212 1 and 0 \u2264 j \u2264 n \u2212 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m \u00d7 n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m \u00d7 n as a subscript. For instance, the matrix A above is 3 \u00d7 4 and can be defined as A = [i \u2212 j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i \u2212 j]3\u00d74.Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i \u2212 j.The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Matrices are commonly written in box brackets or parentheses:Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m\u00a0\u00d7\u00a0n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3\u00a0\u00d7\u00a02 matrix.The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6] Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.The individual items in an m \u00d7 n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n \u00d7 Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 \u00d7 3 (read \"two by three\"), because there are two rows and three columns:",
            "title": "Matrix (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Matrix_(mathematics)"
        },
        {
            "desc_links": [
                "/wiki/Number_theory",
                "/wiki/Linear_algebra",
                "/wiki/Group_theory",
                "/wiki/Orthogonal_group",
                "/wiki/Differential_geometry",
                "/wiki/Riemannian_metric",
                "/wiki/Second_fundamental_form",
                "/wiki/Differential_topology",
                "/wiki/Intersection_form_(4-manifold)",
                "/wiki/Four-manifold",
                "/wiki/Lie_theory",
                "/wiki/Killing_form",
                "/wiki/Mathematics",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Variable_(mathematics)"
            ],
            "links": [
                "/wiki/15_and_290_theorems",
                "/wiki/Gauss",
                "/wiki/Disquisitiones_Arithmeticae",
                "/wiki/Lattice_(group)",
                "/wiki/Number_theory",
                "/wiki/Topology",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Algebra_over_a_field",
                "/wiki/Orthogonal_group",
                "/wiki/Orthogonal",
                "/wiki/Isotropic_quadratic_form",
                "/wiki/Commutative_ring",
                "/wiki/Module_(mathematics)",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Homogeneous_function",
                "/wiki/Symmetric_bilinear_form",
                "/wiki/Symmetric_matrix",
                "/wiki/General_linear_group",
                "/wiki/Column_vector",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Isometry_group",
                "/wiki/Compact_space",
                "/wiki/Orthogonal_group",
                "/wiki/Indefinite_orthogonal_group",
                "/wiki/Clifford_algebra",
                "/wiki/Pin_group",
                "/wiki/Invertible_matrix",
                "/wiki/Real_number",
                "/wiki/Diagonal_matrix",
                "/wiki/Orthogonal_matrix",
                "/wiki/Sylvester%27s_law_of_inertia",
                "/wiki/Invariant_(mathematics)",
                "/wiki/Positive_definite_form",
                "/wiki/Nondegenerate_form",
                "/wiki/Spacetime",
                "/wiki/Carl_Gustav_Jacobi",
                "/wiki/Symmetric_matrix",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/Disquisitiones_Arithmeticae",
                "/wiki/Binary_quadratic_form",
                "/wiki/Integer",
                "/wiki/Quadratic_number_field",
                "/wiki/Modular_group",
                "/wiki/Brahmagupta",
                "/wiki/Br%C4%81hmasphu%E1%B9%ADasiddh%C4%81nta",
                "/wiki/Pell%27s_equation",
                "/wiki/William_Brouncker,_2nd_Viscount_Brouncker",
                "/wiki/Leonhard_Euler",
                "/wiki/Joseph_Louis_Lagrange",
                "/wiki/Fermat%27s_theorem_on_sums_of_two_squares",
                "/wiki/Pythagorean_triple",
                "/wiki/Vector_space",
                "/wiki/Euclidean_space",
                "/wiki/Euclidean_norm",
                "/wiki/Distance",
                "/wiki/Homogeneous_coordinates",
                "/wiki/Quadric_(projective_geometry)",
                "/wiki/Projective_space",
                "/wiki/Projective_geometry",
                "/wiki/Conic_sections",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Integer",
                "/wiki/Linear_algebra",
                "/wiki/Analytic_geometry",
                "/wiki/Field_(algebra)",
                "/wiki/Commutative_ring",
                "/wiki/P-adic_integer",
                "/wiki/Number_theory",
                "/wiki/Quadratic_field",
                "/wiki/Continued_fraction",
                "/wiki/Modular_forms",
                "/wiki/Algebraic_topology",
                "/wiki/Quadratic_function",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Binary_quadratic_form",
                "/wiki/Number_theory",
                "/wiki/Linear_algebra",
                "/wiki/Group_theory",
                "/wiki/Orthogonal_group",
                "/wiki/Differential_geometry",
                "/wiki/Riemannian_metric",
                "/wiki/Second_fundamental_form",
                "/wiki/Differential_topology",
                "/wiki/Intersection_form_(4-manifold)",
                "/wiki/Four-manifold",
                "/wiki/Lie_theory",
                "/wiki/Killing_form",
                "/wiki/Mathematics",
                "/wiki/Homogeneous_polynomial",
                "/wiki/Degree_of_a_polynomial",
                "/wiki/Variable_(mathematics)"
            ],
            "text": "There are also forms whose image consists of all but one of the positive integers. For example, {1,2,5,5} has 15 as the exception. Recently, the 15 and 290 theorems have completely characterized universal integral quadratic forms: if all coefficients are integers, then it represents all positive integers if and only if it represents all integers up through 290; if it has an integral matrix, it represents all positive integers if and only if it represents all integers up through 15.Several points of view mean that twos out has been adopted as the standard convention. Those include:this is the convention Gauss uses in Disquisitiones Arithmeticae.This debate was due to the confusion of quadratic forms (represented by polynomials) and symmetric bilinear forms (represented by matrices), and \"twos out\" is now the accepted convention; \"twos in\" is instead the theory of integral symmetric bilinear forms (integral symmetric matrices).Historically there was some confusion and controversy over whether the notion of integral quadratic form should mean:This is the current use of the term; in the past it was sometimes used differently, as detailed below.An integral quadratic form has integer coefficients, such as x2 + xy + y2; equivalently, given a lattice \u039b in a vector space V (over a field with characteristic 0, such as Q or R), a quadratic form Q is integral with respect to \u039b if and only if it is integer-valued on \u039b, meaning Q(x, y) \u2208 Z if x, y \u2208 \u039b.Quadratic forms over the ring of integers are called integral quadratic forms, whereas the corresponding modules are quadratic lattices (sometimes, simply lattices). They play an important role in number theory and topology.Classification of all quadratic forms up to equivalence can thus be reduced to the case of diagonal forms.Such a diagonal form is often denoted byEvery quadratic form q in n variables over a field of characteristic not equal to 2 is equivalent[clarification needed] to a diagonal formIf a quadratic space (A, Q) has a product so that A is an algebra over a field, and satisfiesThe orthogonal group of a non-singular quadratic form Q is the group of the linear automorphisms of V that preserve Q, i.e. the group of isometries of (V, Q) into itself.Two elements v and w of V are called orthogonal if B(v,w) = 0. The kernel of a bilinear form B consists of the elements that are orthogonal to every element of V. Q is non-singular if the kernel of its associated bilinear form is 0. If there exists a non-zero v in V such that Q(v) = 0, the quadratic form Q is isotropic, otherwise it is anisotropic. This terminology also applies to vectors and subspaces of a quadratic space. If the restriction of Q to a subspace U of V is identically zero, U is totally singular.Let R be a commutative ring and M be an R-module. A mapping Q\u00a0: M \u2192 R is a quadratic form if there exists a bilinear form[9] B\u00a0: M \u00d7 M \u2192 R such that Q is the mapping v \u21a6 B(v, v). Commonly found in the literature is the equivalent definition of a quadratic form as a mapping Q\u00a0: M \u2192 R with the properties that Q(av) = a2Q(v) for all a \u2208 R and v \u2208 M and that the form B\u2032\u00a0: M \u00d7 M \u2192 R\u00a0: (u, v) \u21a6 Q(u + v) \u2212 Q(u) \u2212 Q(v) is bilinear.The isometry classes of n-dimensional quadratic spaces over K correspond to the equivalence classes of n-ary quadratic forms over K.Two n-dimensional quadratic spaces (V, Q) and (V\u2032, Q\u2032) are isometric if there exists an invertible linear transformation T\u00a0: V \u2192 V\u2032 (isometry) such thatThe pair (V, Q) consisting of a finite-dimensional vector space V over K and a quadratic map from V to K is called a quadratic space, and B as defined here is the associated symmetric bilinear form of Q. The notion of a quadratic space is a coordinate-free version of the notion of quadratic form. Sometimes, Q is also called a quadratic form.When the characteristic of K is 2, so that 2 is not a unit, it is still possible to use a quadratic form to define a symmetric bilinear form B\u2032(x, y) = Q(x + y) \u2212 Q(x) \u2212 Q(y). However, Q(x) can no longer be recovered from this B\u2032 in the same way, since B\u2032(x, x) = 0 for all x (and is thus alternating[8]). Alternately, there always exists a bilinear form B\u2033 (not in general either unique or symmetric) such that B\u2033(x, x) = Q(x).This bilinear form B is symmetric, i.e. B(x, y) = B(y, x) for all x, y in V, and it determines Q: Q(x) = B(x, x) for all x in V.When the characteristic of K is not 2, the map B\u00a0: V \u00d7 V \u2192 K defined below is bilinear over K:The map Q is a homogeneous function of degree 2, which means that it has the property that, for all a in K and v in V:A quadratic form q in n variables over K induces a map from the n-dimensional coordinate space Kn into K:and these two processes are the inverses of one another. As a consequence, over a field of characteristic not equal to 2, the theories of symmetric bilinear forms and of quadratic forms in n variables are essentially the same.Thus, bq is a symmetric bilinear form over K with matrix A. Conversely, any symmetric bilinear form b defines a quadratic formThe associated bilinear form of a quadratic form q is defined byLet the characteristic of K be different from 2.[7] The coefficient matrix A of q may be replaced by the symmetric matrix (A + AT)/2 with the same quadratic form, so it may be assumed from the outset that A is symmetric. Moreover, a symmetric matrix A is uniquely determined by the corresponding quadratic form. Under an equivalence C, the symmetric matrix A of \u03c6 and the symmetric matrix B of \u03c8 are related as follows:Two n-ary quadratic forms \u03c6 and \u03c8 over K are equivalent if there exists a nonsingular linear transformation C \u2208 GL(n, K) such thatThis formula may be rewritten using matrices: let x be the column vector with components x1, ..., xn and A = (aij) be the n\u00d7n matrix over K whose entries are the coefficients of q. ThenAn n-ary quadratic form over a field K is a homogeneous polynomial of degree 2 in n variables with coefficients in K:The quadratic form q is positive definite (resp., negative definite) if q(v) > 0 (resp., q(v) < 0) for every nonzero vector v.[6] When q(v) assumes both positive and negative values, q is an indefinite quadratic form. The theorems of Jacobi and Sylvester show that any positive definite quadratic form in n variables can be brought to the sum of n squares by a suitable invertible linear transformation: geometrically, there is only one positive definite real quadratic form of every dimension. Its isometry group is a compact orthogonal group O(n). This stands in contrast with the case of indefinite forms, when the corresponding group, the indefinite orthogonal group O(p, q), is non-compact. Further, the isometry groups of Q and \u2212Q are the same (O(p, q) \u2248 O(q, p)), but the associated Clifford algebras (and hence pin groups) are different.by a suitable choice of an orthogonal matrix S, and the diagonal entries of B are uniquely determined \u2014 this is Jacobi's theorem. If S is allowed to be any invertible matrix then B can be made to have only 0,1, and \u22121 on the diagonal, and the number of the entries of each type (n0 for 0, n+ for 1, and n\u2212 for \u22121) depends only on A. This is one of the formulations of Sylvester's law of inertia and the numbers n+ and n\u2212 are called the positive and negative indices of inertia. Although their definition involved a choice of basis and consideration of the corresponding real symmetric matrix A, Sylvester's law of inertia means that they are invariants of the quadratic form q.Any symmetric matrix A can be transformed into a diagonal matrixwhere x is the column vector of coordinates of v in the chosen basis. Under a change of basis, the column x is multiplied on the left by an n \u00d7 n invertible matrix S, and the symmetric square matrix A is transformed into another symmetric square matrix B of the same size according to the formulaLet q be a quadratic form defined on an n-dimensional real vector space. Let A be the matrix of the quadratic form q in a given basis. This means that A is a symmetric n \u00d7 n matrix such thatThese results are reformulated in a different way below.so that the corresponding symmetric matrix is diagonal, and this is even possible to accomplish with a change of variables given by an orthogonal matrix \u2013 in this case the coefficients \u03bb1, \u03bb2, ..., \u03bbn are determined uniquely up to a permutation. If the change of variables is given by an invertible matrix, not necessarily orthogonal, then the coefficients \u03bbi can be made to be 0, 1, and \u22121. Sylvester's law of inertia states that the numbers of each 1 and \u22121 are invariants of the quadratic form, in the sense that any other diagonalization will contain the same number of each. The signature of the quadratic form is the triple (n0, n+, n\u2212), where n0 is the number of 0s and n\u00b1 is the number of \u00b11s. Sylvester's law of inertia shows that this is a well-defined quantity attached to the quadratic form. The case when all \u03bbi have the same sign is especially important: in this case the quadratic form is called positive definite (all 1) or negative definite (all \u22121). If none of the terms are 0, then the form is called nondegenerate; this includes positive definite, negative definite, and indefinite (a mix of 1 and \u22121); equivalently, a nondegenerate quadratic form is one whose associated symmetric form is a nondegenerate bilinear form. A real vector space with an indefinite nondegenerate quadratic form of index (p, q) (denoting p 1s and q \u22121s) is often denoted as Rp,q particularly in the physical theory of spacetime.One of the most important questions in the theory of quadratic forms is how much can one simplify a quadratic form q by a homogeneous linear change of variables. A fundamental theorem due to Jacobi asserts that q can be brought to a diagonal form [5]Conversely, given a quadratic form in n variables, its coefficients can be arranged into an n \u00d7 n symmetric matrix.Any n\u00d7n real symmetric matrix A determines a quadratic form qA in n variables by the formulaIn 1801 Gauss published Disquisitiones Arithmeticae, a major portion of which was devoted to a complete theory of binary quadratic forms over the integers. Since then, the concept has been generalized, and the connections with quadratic number fields, the modular group, and other areas of mathematics have been further elucidated.In 628, the Indian mathematician Brahmagupta wrote Br\u0101hmasphu\u1e6dasiddh\u0101nta which includes, among many other things, a study of equations of the form x2 \u2212 ny2 = c. In particular he considered what is now called Pell's equation, x2 \u2212 ny2 = 1, and found a method for its solution.[4] In Europe this problem was studied by Brouncker, Euler and Lagrange.The study of particular quadratic forms, in particular the question of whether a given integer can be the value of a quadratic form over the integers, dates back many centuries. One such case is Fermat's theorem on sums of two squares, which determines when an integer may be expressed in the form x2 + y2, where x, y are integers. This problem is related to the problem of finding Pythagorean triples, which appeared in the second millennium B.C.[3]A closely related notion with geometric overtones is a quadratic space, which is a pair (V,q), with V a vector space over a field K, and q: V \u2192 K a quadratic form on V. An example is given by the three-dimensional Euclidean space and the square of the Euclidean norm expressing the distance between a point with coordinates (x,y,z) and the origin:Using homogeneous coordinates, a non-zero quadratic form in n variables defines an (n\u22122)-dimensional quadric in the (n\u22121)-dimensional projective space. This is a basic construction in projective geometry. In this way one may visualize 3-dimensional real quadratic forms as conic sections.The theory of quadratic forms and methods used in their study depend in a large measure on the nature of the coefficients, which may be real or complex numbers, rational numbers, or integers. In linear algebra, analytic geometry, and in the majority of applications of quadratic forms, the coefficients are real or complex numbers. In the algebraic theory of quadratic forms, the coefficients are elements of a certain field. In the arithmetic theory of quadratic forms, the coefficients belong to a fixed commutative ring, frequently the integers Z or the p-adic integers Zp.[2] Binary quadratic forms have been extensively studied in number theory, in particular, in the theory of quadratic fields, continued fractions, and modular forms. The theory of integral quadratic forms in n variables has important applications to algebraic topology.where a, ..., f are the coefficients.[1] Note that quadratic functions, such as ax2 + bx + c in the one variable case, are not quadratic forms, as they are typically not homogeneous (unless b and c are both 0).Quadratic forms are homogeneous quadratic polynomials in n variables. In the cases of one, two, and three variables they are called unary, binary, and ternary and have the following explicit form:Quadratic forms occupy a central place in various branches of mathematics, including number theory, linear algebra, group theory (orthogonal group), differential geometry (Riemannian metric, second fundamental form), differential topology (intersection forms of four-manifolds), and Lie theory (the Killing form).is a quadratic form in the variables x and y.In mathematics, a quadratic form is a homogeneous polynomial of degree two in a number of variables. For example,",
            "title": "Quadratic form",
            "url": "https://en.wikipedia.org/wiki/Quadratic_form"
        },
        {
            "desc_links": [
                "/wiki/Dynamical_systems",
                "/wiki/Numerical_methods",
                "/wiki/Pure_mathematics",
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Function_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Engineering",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Biology"
            ],
            "links": [
                "/wiki/Rate_equation",
                "/wiki/Chemical_reaction",
                "/wiki/Reaction_rate",
                "/wiki/Reaction_order",
                "/wiki/Mass_balance",
                "/wiki/Thermodynamics",
                "/wiki/Quantum_mechanics",
                "/wiki/Lotka%E2%80%93Volterra_equations",
                "/wiki/Non-linear",
                "/wiki/Population_dynamics",
                "/wiki/Schr%C3%B6dinger%27s_equation",
                "/wiki/Linear_differential_equation",
                "/wiki/Partial_differential_equation",
                "/wiki/Wave_function",
                "/wiki/Einstein_field_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Albert_Einstein",
                "/wiki/General_relativity",
                "/wiki/Fundamental_interaction",
                "/wiki/Gravitation",
                "/wiki/Spacetime",
                "/wiki/Curvature",
                "/wiki/Matter",
                "/wiki/Energy",
                "/wiki/Tensor_equation",
                "/wiki/Curvature",
                "/wiki/Einstein_tensor",
                "/wiki/Momentum",
                "/wiki/Stress%E2%80%93energy_tensor",
                "/wiki/Maxwell%27s_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Lorentz_force",
                "/wiki/Classical_electrodynamics",
                "/wiki/Optics",
                "/wiki/Electric_circuit",
                "/wiki/Electric_field",
                "/wiki/Magnetic_field",
                "/wiki/Electric_charge",
                "/wiki/Electric_current",
                "/wiki/James_Clerk_Maxwell",
                "/wiki/Newton%27s_second_law",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Physics",
                "/wiki/Chemistry",
                "/wiki/Biology",
                "/wiki/Economics",
                "/wiki/Mathematical_modelling",
                "/wiki/Partial_differential_equation",
                "/wiki/Wave_equation",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_equation",
                "/wiki/Diffusion",
                "/wiki/Black%E2%80%93Scholes",
                "/wiki/Pure_mathematics",
                "/wiki/Applied_mathematics",
                "/wiki/Physics",
                "/wiki/Engineering",
                "/wiki/Closed-form_expression",
                "/wiki/Numerical_ordinary_differential_equations",
                "/wiki/Difference_equations",
                "/wiki/Algebraic_equations",
                "/wiki/Derivative#Higher_derivatives",
                "/wiki/Second_derivative",
                "/wiki/Thin-film_equation",
                "/wiki/Linearization",
                "/wiki/Symmetry",
                "/wiki/Chaos_theory",
                "/wiki/Navier%E2%80%93Stokes_existence_and_smoothness",
                "/wiki/Sound",
                "/wiki/Heat",
                "/wiki/Electrostatics",
                "/wiki/Electrodynamics",
                "/wiki/Fluid_flow",
                "/wiki/Elasticity_(physics)",
                "/wiki/Quantum_mechanics",
                "/wiki/Dynamical_systems",
                "/wiki/Multidimensional_systems",
                "/wiki/Stochastic_partial_differential_equations",
                "/wiki/Partial_differential_equation",
                "/wiki/Multivariable_calculus",
                "/wiki/Partial_derivatives",
                "/wiki/Ordinary_differential_equations",
                "/wiki/Computer_model",
                "/wiki/Closed-form_expression",
                "/wiki/Numerical_ordinary_differential_equations",
                "/wiki/Physics",
                "/wiki/Special_functions",
                "/wiki/Holonomic_function",
                "/wiki/Linear_differential_equation",
                "/wiki/Linear_equation",
                "/wiki/Antiderivative",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Function_of_a_real_variable",
                "/wiki/Variable_(mathematics)",
                "/wiki/Independent_variable",
                "/wiki/Partial_differential_equation",
                "/wiki/Equations_of_motion",
                "/wiki/Classical_mechanics",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_flow",
                "/wiki/Newton%27s_law_of_cooling",
                "/wiki/Heat_equation",
                "/wiki/Mechanics",
                "/wiki/Lagrangian_mechanics",
                "/wiki/Euler%E2%80%93Lagrange_equation",
                "/wiki/Tautochrone",
                "/wiki/Musical_instrument",
                "/wiki/Jean_le_Rond_d%27Alembert",
                "/wiki/Leonhard_Euler",
                "/wiki/Daniel_Bernoulli",
                "/wiki/Joseph-Louis_Lagrange",
                "/wiki/Jacob_Bernoulli",
                "/wiki/Bernoulli_differential_equation",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Calculus",
                "/wiki/Isaac_Newton",
                "/wiki/Leibniz",
                "/wiki/Method_of_Fluxions",
                "/wiki/Dynamical_systems",
                "/wiki/Numerical_methods",
                "/wiki/Pure_mathematics",
                "/wiki/Mathematics",
                "/wiki/Equation",
                "/wiki/Function_(mathematics)",
                "/wiki/Derivative",
                "/wiki/Engineering",
                "/wiki/Physics",
                "/wiki/Economics",
                "/wiki/Biology"
            ],
            "text": "The rate law or rate equation for a chemical reaction is a differential equation that links the reaction rate with concentrations or pressures of reactants and constant parameters (normally rate coefficients and partial reaction orders).[18] To determine the rate equation for a particular system one combines the reaction rate with a mass balance for the system.[19] In addition, a range of differential equations are present in the study of thermodynamics and quantum mechanics.The Lotka\u2013Volterra equations, also known as the predator\u2013prey equations, are a pair of first-order, non-linear, differential equations frequently used to describe the population dynamics of two species that interact, one as a predator and the other as prey.In quantum mechanics, the analogue of Newton's law is Schr\u00f6dinger's equation (a partial differential equation) for a quantum system (usually atoms, molecules, and subatomic particles whether free, bound, or localized). It is not a simple algebraic equation, but in general a linear partial differential equation, describing the time-evolution of the system's wave function (also called a \"state function\").[17]The Einstein field equations (EFE; also known as \"Einstein's equations\") are a set of ten partial differential equations in Albert Einstein's general theory of relativity which describe the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy.[14] First published by Einstein in 1915[15] as a tensor equation, the EFE equate local spacetime curvature (expressed by the Einstein tensor) with the local energy and momentum within that spacetime (expressed by the stress\u2013energy tensor).[16]Maxwell's equations are a set of partial differential equations that, together with the Lorentz force law, form the foundation of classical electrodynamics, classical optics, and electric circuits. These fields in turn underlie modern electrical and communications technologies. Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. They are named after the Scottish physicist and mathematician James Clerk Maxwell, who published an early form of those equations between 1861 and 1862.So long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the equation of motion.Many fundamental laws of physics and chemistry can be formulated as differential equations. In biology and economics, differential equations are used to model the behavior of complex systems. The mathematical theory of differential equations first developed together with the sciences where the equations had originated and where the results found application. However, diverse problems, sometimes originating in quite distinct scientific fields, may give rise to identical differential equations. Whenever this happens, mathematical theory behind the equations can be viewed as a unifying principle behind diverse phenomena. As an example, consider the propagation of light and sound in the atmosphere, and of waves on the surface of a pond. All of them may be described by the same second-order partial differential equation, the wave equation, which allows us to think of light and sound as forms of waves, much like familiar waves in the water. Conduction of heat, the theory of which was developed by Joseph Fourier, is governed by another second-order partial differential equation, the heat equation. It turns out that many diffusion processes, while seemingly different, are described by the same equation; the Black\u2013Scholes equation in finance is, for instance, related to the heat equation.The study of differential equations is a wide field in pure and applied mathematics, physics, and engineering. All of these disciplines are concerned with the properties of differential equations of various types. Pure mathematics focuses on the existence and uniqueness of solutions, while applied mathematics emphasizes the rigorous justification of the methods for approximating solutions. Differential equations play an important role in modelling virtually every physical, technical, or biological process, from celestial motion, to bridge design, to interactions between neurons. Differential equations such as those used to solve real-life problems may not necessarily be directly solvable, i.e. do not have closed form solutions. Instead, solutions can be approximated using numerical methods.The theory of differential equations is closely related to the theory of difference equations, in which the coordinates assume only discrete values, and the relationship involves values of the unknown function or functions and values at nearby coordinates. Many methods to compute numerical solutions of differential equations or study the properties of differential equations involve the approximation of the solution of a differential equation by the solution of a corresponding difference equation.such thatHowever, this only helps us with first order initial value problems. Suppose we had a linear initial value problem of the nth order:Solving differential equations is not like solving algebraic equations. Not only are their solutions often unclear, but whether solutions are unique or exist at all are also notable subjects of interest.In the next group of examples, the unknown function u depends on two variables x and t or x and y.In the first group of examples, let u be an unknown function of x, and let c & \u03c9 be known constants. Note both ordinary and partial differential equations are broadly classified as linear and nonlinear.Differential equations are described by their order, determined by the term with the highest derivatives. An equation containing only first derivatives is a first-order differential equation, an equation containing the second derivative is a second-order differential equation, and so on.[11][12] Differential equations that describe natural phenomena almost always have only first and second order derivatives in them, but there are some exceptions, such as the thin film equation, which is a fourth order partial differential equation.Linear differential equations frequently appear as approximations to nonlinear equations. These approximations are only valid under restricted conditions. For example, the harmonic oscillator equation is an approximation to the nonlinear pendulum equation that is valid for small amplitude oscillations (see below).Non-linear differential equations are formed by the products of the unknown function and its derivatives are allowed and its degree is > 1. There are very few methods of solving nonlinear differential equations exactly; those that are known typically depend on the equation having particular symmetries. Nonlinear differential equations can exhibit very complicated behavior over extended time intervals, characteristic of chaos. Even the fundamental questions of existence, uniqueness, and extendability of solutions for nonlinear differential equations, and well-posedness of initial and boundary value problems for nonlinear PDEs are hard problems and their resolution in special cases is considered to be a significant advance in the mathematical theory (cf. Navier\u2013Stokes existence and smoothness). However, if the differential equation is a correctly formulated representation of a meaningful physical process, then one expects it to have a solution.[10]PDEs can be used to describe a wide variety of phenomena in nature such as sound, heat, electrostatics, electrodynamics, fluid flow, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.A partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. (This is in contrast to ordinary differential equations, which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved in closed form, or used to create a relevant computer model.As, in general, the solutions of a differential equation cannot be expressed by a closed-form expression, numerical methods are commonly used for solving differential equations on a computer.Most ODEs that are encountered in physics are linear, and, therefore, most special functions may be defined as solutions of linear differential equations (see Holonomic function).Linear differential equations are the differential equations that are linear in the unknown function and its derivatives. Their theory is well developed, and, in many cases, one may express their solutions in terms of integrals.An ordinary differential equation (ODE) is an equation containing an unknown function of one real or complex variable x, its derivatives, and some given functions of x. The unknown function is generally represented by a variable (often denoted y), which, therefore, depends on x. Thus x is often called the independent variable of the equation. The term \"ordinary\" is used in contrast with the term partial differential equation, which may be with respect to more than one independent variable.Differential equations can be divided into several types. Apart from describing the properties of the equation itself, these classes of differential equations can help inform the choice of approach to a solution. Commonly used distinctions include whether the equation is: Ordinary/Partial, Linear/Non-linear, and Homogeneous/Inhomogeneous. This list is far from exhaustive; there are many other properties and subclasses of differential equations which can be very useful in specific contexts.An example of modelling a real world problem using differential equations is the determination of the velocity of a ball falling through the air, considering only gravity and air resistance. The ball's acceleration towards the ground is the acceleration due to gravity minus the acceleration due to air resistance. Gravity is considered constant, and air resistance may be modeled as proportional to the ball's velocity. This means that the ball's acceleration, which is a derivative of its velocity, depends on the velocity (and the velocity depends on time). Finding the velocity as a function of time involves solving a differential equation and verifying its validity.In some cases, this differential equation (called an equation of motion) may be solved explicitly.For example, in classical mechanics, the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow (given the position, velocity, acceleration and various forces acting on the body) one to express these variables dynamically as a differential equation for the unknown position of the body as a function of time.Fourier published his work on heat flow in Th\u00e9orie analytique de la chaleur (The Analytic Theory of Heat),[9] in which he based his reasoning on Newton's law of cooling, namely, that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures. Contained in this book was Fourier's proposal of his heat equation for conductive diffusion of heat. This partial differential equation is now taught to every student of mathematical physics.Lagrange solved this problem in 1755 and sent the solution to Euler. Both further developed Lagrange's method and applied it to mechanics, which led to the formulation of Lagrangian mechanics.The Euler\u2013Lagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the tautochrone problem. This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point.Historically, the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond d'Alembert, Leonhard Euler, Daniel Bernoulli, and Joseph-Louis Lagrange.[4][5][6][7] In 1746, d\u2019Alembert discovered the one-dimensional wave equation, and within ten years Euler discovered the three-dimensional wave equation.[8]for which the following year Leibniz obtained solutions by simplifying it.[3]Jacob Bernoulli proposed the Bernoulli differential equation in 1695.[2] This is an ordinary differential equation of the formHe solves these examples and others using infinite series and discusses the non-uniqueness of solutions.Differential equations first came into existence with the invention of calculus by Newton and Leibniz. In Chapter 2 of his 1671 work \"Methodus fluxionum et Serierum Infinitarum\",[1] Isaac Newton listed three kinds of differential equations:If a self-contained formula for the solution is not available, the solution may be numerically approximated using computers. The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy.In pure mathematics, differential equations are studied from several different perspectives, mostly concerned with their solutions\u2014the set of functions that satisfy the equation. Only the simplest differential equations are solvable by explicit formulas; however, some properties of solutions of a given differential equation may be determined without finding their exact form.A differential equation is a mathematical equation that relates some function with its derivatives. In applications, the functions usually represent physical quantities, the derivatives represent their rates of change, and the equation defines a relationship between the two. Because such relations are extremely common, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology.",
            "title": "Differential equation",
            "url": "https://en.wikipedia.org/wiki/Differential_equation"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)"
            ],
            "links": [
                "/wiki/Integral_operator",
                "/wiki/Jordan_normal_form",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/QR_decomposition",
                "/wiki/Orthogonal_matrix",
                "/wiki/Triangular_matrix#Forward_and_back_substitution",
                "/wiki/Numerically_stable",
                "/wiki/Numerical_analysis",
                "/wiki/Algorithm",
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)"
            ],
            "text": "These factorizations are based on early work by Fredholm (1903), Hilbert (1904) and Schmidt (1907). For an account, and a translation to English of the seminal papers, see Stewart (2011).There exist analogues of the SVD, QR, LU and Cholesky factorizations for quasimatrices and cmatrices or continuous matrices.[11] A \u2018quasimatrix\u2019 is, like a matrix, a rectangular scheme whose elements are indexed, but one discrete index is replaced by a continuous index. Likewise, a \u2018cmatrix\u2019, is continuous in both indices. As an example of a cmatrix, one can think of the kernel of an integral operator.The Jordan normal form and the Jordan\u2013Chevalley decompositionSimilarly, the QR decomposition expresses A as QR with Q an orthogonal matrix and R an upper triangular matrix. The system Q(Rx) = b is solved by Rx = QTb = c, and the system Rx = c is solved by 'back substitution'. The number of additions and multiplications required is about twice that of using the LU solver, but no more digits are required in inexact arithmetic because the QR decomposition is numerically stable.In numerical analysis, different decompositions are used to implement efficient matrix algorithms.In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.",
            "title": "Matrix decomposition",
            "url": "https://en.wikipedia.org/wiki/Matrix_decomposition"
        },
        {
            "desc_links": [
                "/wiki/Transformation_matrix#Eigenbasis_and_diagonal_matrix",
                "/wiki/Determinant",
                "/wiki/Inhomogeneous_dilation",
                "/wiki/Scaling_(geometry)",
                "/wiki/Homogeneous_dilation",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Basis_(linear_algebra)#Ordered_bases_and_coordinates",
                "/wiki/Diagonal_matrix",
                "/wiki/Defective_matrix"
            ],
            "links": [
                "/wiki/Quantum_mechanics",
                "/wiki/Quantum_chemistry",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hilbert_space",
                "/wiki/Variational_principle",
                "/wiki/Linear_recursive_sequences",
                "/wiki/Fibonacci_number",
                "/wiki/Matrix_exponential",
                "/wiki/Matrix_function",
                "/wiki/Invertible_matrix#Methods_of_matrix_inversion",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvalues",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvectors",
                "/wiki/Eigenvalues",
                "/wiki/Nilpotent_matrix",
                "/wiki/Eigenvalue,_eigenvector_and_eigenspace#Algebraic_and_geometric_multiplicities",
                "/wiki/Rotation_matrix",
                "/wiki/Rotation_matrix#Independent_planes",
                "/wiki/Jordan_Normal_Form",
                "/wiki/Lie_theory",
                "/wiki/Toral_Lie_algebra",
                "/wiki/Normal_matrix",
                "/wiki/Unitary_matrix",
                "/wiki/Commuting_matrices",
                "/wiki/Eigenvalue_algorithm",
                "/wiki/Hermitian_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Orthonormal_basis",
                "/wiki/Unitary_matrix",
                "/wiki/Orthogonal_matrix",
                "/wiki/Conjugate_transpose",
                "/wiki/Transpose",
                "/wiki/Right_eigenvector",
                "/wiki/Eigenvalue",
                "/wiki/Linearly_independent",
                "/wiki/Row_vector",
                "/wiki/Left_eigenvector",
                "/wiki/Jordan%E2%80%93Chevalley_decomposition",
                "/wiki/Nilpotent",
                "/wiki/Jordan_form",
                "/wiki/Subset",
                "/wiki/Lebesgue_measure",
                "/wiki/Zariski_topology",
                "/wiki/Discriminant",
                "/wiki/Hypersurface",
                "/wiki/Norm_(mathematics)",
                "/wiki/Minimal_polynomial_(linear_algebra)",
                "/wiki/Elementary_divisor",
                "/wiki/Transformation_matrix#Eigenbasis_and_diagonal_matrix",
                "/wiki/Determinant",
                "/wiki/Inhomogeneous_dilation",
                "/wiki/Scaling_(geometry)",
                "/wiki/Homogeneous_dilation",
                "/wiki/Linear_algebra",
                "/wiki/Square_matrix",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Diagonal_matrix",
                "/wiki/Invertible_matrix",
                "/wiki/Dimension_(linear_algebra)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Basis_(linear_algebra)#Ordered_bases_and_coordinates",
                "/wiki/Diagonal_matrix",
                "/wiki/Defective_matrix"
            ],
            "text": "In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schr\u00f6dinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space). A very common approximation is to truncate Hilbert space to finite dimension, after which the Schr\u00f6dinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle, valid for Hamiltonians that are bounded from below. But also first-order perturbation theory for degenerate states leads to a matrix eigenvalue problem.thereby explaining the above phenomenon.The preceding relations, expressed in matrix form, areSwitching back to the standard basis, we haveThus, a and b are the eigenvalues corresponding to u and v, respectively. By linearity of matrix multiplication, we have thatStraightforward calculations show thatwhere ei denotes the standard basis of Rn. The reverse change of basis is given byThe above phenomenon can be explained by diagonalizing M. To accomplish this, we need a basis of R2 consisting of eigenvectors of M. One such eigenvector basis is given byCalculating the various powers of M reveals a surprising pattern:For example, consider the following matrix:This is particularly useful in finding closed form expressions for terms of linear recursive sequences, such as the Fibonacci numbers.and the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions that can be defined as power series.Diagonalization can be used to compute the powers of a matrix A efficiently, provided the matrix is diagonalizable. Suppose we have found thatThen P diagonalizes A, as a simple computation confirms, having calculated P \u22121 using any suitable method:Note that there is no preferred order of the eigenvectors in P; changing the order of the eigenvectors in P just changes the order of the eigenvalues in the diagonalized form of A.[3]Now, let P be the matrix with these eigenvectors as its columns:The eigenvectors of A areThese eigenvalues are the values that will appear in the diagonalized form of matrix A, so by finding the eigenvalues of A we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize A.A is a 3\u00d73 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly n distinct eigenvalues in an n\u00d7n matrix then this matrix is diagonalizable.This matrix has eigenvaluesConsider a matrixNote that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.then Q\u22121BQ is diagonal. It is easy to find that B is the rotation matrix which rotates counterclockwise by angle \u03b8=3\u03c0/2The matrix B does not have any real eigenvalues, so there is no real matrix Q such that Q\u22121BQ is a diagonal matrix. However, we can diagonalize B if we allow complex numbers. Indeed, if we takeSome real matrices are not diagonalizable over the reals. Consider for instance the matrixThis matrix is not diagonalizable: there is no matrix U such that U\u22121CU is a diagonal matrix. Indeed, C has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices. This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, considerIn general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to \"do the best one can\", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal - known as Jordan normal form.In the language of Lie theory, a set of simultaneously diagonalisable matrices generate a toral Lie algebra.A set consists of commuting normal matrices if and only if it is simultaneously diagonalisable by a unitary matrix; that is, there exists a unitary matrix U such that U*AU is diagonal for every A in the set.are diagonalizable but not simultaneously diagonalizable because they do not commute.The set of all n\u00d7n diagonalisable matrices (over C) with n > 1 is not simultaneously diagonalisable. For instance, the matricesA set of matrices is said to be simultaneously diagonalizable if there exists a single invertible matrix P such that P\u22121AP is a diagonal matrix for every A in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalisable.[2]In practice, matrices are diagonalized numerically using computers. Many algorithms exist to accomplish this.When the matrix A is a Hermitian matrix (resp. symmetric matrix), eigenvectors of A can be chosen to form an orthonormal basis of Cn (resp. Rn). Under such circumstance P will be a unitary matrix (resp. orthogonal matrix) and P\u22121 equals the conjugate transpose (resp. transpose) of P.So the column vectors of P are right eigenvectors of A, and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of P also suggests that the eigenvectors are linearly independent and form a basis of Fn. This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of P\u22121 are the left eigenvectors of A.the above equation can be rewritten asthen:If a matrix A can be diagonalized, that is,The Jordan\u2013Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each \"block\" is a one-by-one matrix.As a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex n\u00d7n matrices that are not diagonalizable over C, considered as a subset of Cn\u00d7n, has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology: the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface. From that follows also density in the usual (strong) topology given by a norm. The same is not true over R.The following sufficient (but not necessary) condition is often useful.Another characterization: A matrix or linear map is diagonalizable over the field F if and only if its minimal polynomial is a product of distinct linear factors over F. (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)The fundamental fact about diagonalizable maps and matrices is expressed by the following:Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle; once their eigenvalues and eigenvectors are known, one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power, and the determinant of a diagonal matrix is simply the product of all diagonal entries. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or anisotropic scaling)\u00a0\u2014 it scales the space, as does a homogeneous dilation, but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).In linear algebra, a square matrix A is called diagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix P such that P\u22121AP is a diagonal matrix. If V is a finite-dimensional vector space, then a linear map T\u00a0: V \u2192 V is called diagonalizable if there exists an ordered basis of V with respect to which T is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map.[1] A square matrix that is not diagonalizable is called defective.",
            "title": "Diagonalizable matrix",
            "url": "https://en.wikipedia.org/wiki/Diagonalizable_matrix"
        },
        {
            "desc_links": [
                "/wiki/Lisa_del_Giocondo",
                "/wiki/Lombardy_Poplar",
                "/wiki/Panel_painting",
                "/wiki/Francis_I_of_France",
                "/wiki/French_Republic",
                "/wiki/Louvre",
                "/wiki/Paris",
                "/wiki/Help:IPA/English",
                "/wiki/Italian_language",
                "/wiki/Help:IPA/Italian",
                "/wiki/Help:IPA/Italian",
                "/wiki/French_language",
                "/wiki/Help:IPA/French",
                "/wiki/Portrait_painting",
                "/wiki/Italian_Renaissance",
                "/wiki/Leonardo_da_Vinci",
                "/wiki/Guinness_World_Records"
            ],
            "links": [
                "/wiki/Isleworth_Mona_Lisa",
                "/wiki/Hugh_Blaker",
                "/wiki/Frank_Z%C3%B6llner",
                "/wiki/Martin_Kemp_(art_historian)",
                "/wiki/Speculations_about_Mona_Lisa",
                "/wiki/Stereoscopic",
                "/wiki/Museo_del_Prado",
                "/wiki/Leonardeschi",
                "/wiki/Sala%C3%AC",
                "/wiki/Francesco_Melzi",
                "/wiki/Salvador_Dal%C3%AD",
                "/wiki/Andy_Warhol",
                "/wiki/Serigraphy",
                "/wiki/Marilyn_Monroe",
                "/wiki/Elvis_Presley",
                "/wiki/Campbell%27s_Soup_Cans",
                "/wiki/Invader_(artist)",
                "/wiki/Avant-garde",
                "/wiki/Dada",
                "/wiki/Surrealism",
                "/wiki/Caricature",
                "/wiki/Incoherents",
                "/wiki/Marcel_Duchamp",
                "/wiki/L.H.O.O.Q.",
                "/wiki/Humor_in_Freud",
                "/wiki/Rhonda_Roland_Shearer",
                "/wiki/Vasari",
                "/wiki/Andr%C3%A9_F%C3%A9libien",
                "/wiki/Realism_(arts)",
                "/wiki/Romanticism",
                "/wiki/Th%C3%A9ophile_Gautier",
                "/wiki/Walter_Pater",
                "/wiki/Bernard_Berenson",
                "/wiki/Florentine_painting",
                "/wiki/Raphael",
                "/wiki/Young_Woman_with_Unicorn",
                "/wiki/Portrait_of_Maddalena_Doni",
                "/wiki/La_velata",
                "/wiki/Portrait_of_Baldassare_Castiglione",
                "/wiki/France_24",
                "/wiki/List_of_most_expensive_paintings#List_of_highest_prices_paid",
                "/wiki/Henri_Loyrette",
                "/wiki/SS_France_(1961)",
                "/wiki/Metropolitan_Museum_of_Art",
                "/wiki/King_Francis_I_of_France",
                "/wiki/Femme_fatale",
                "/wiki/Baedeker",
                "/wiki/Intelligentsia",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/LED_lamp",
                "/wiki/Colour_Rendering_Index",
                "/wiki/Infrared",
                "/wiki/Ultraviolet",
                "/wiki/Nippon_TV",
                "/wiki/Carbon_tetrachloride",
                "/wiki/Ethylene_oxide",
                "/wiki/Mus%C3%A9e_Napol%C3%A9on",
                "/wiki/Solvent",
                "/wiki/Cross_brace",
                "/wiki/Beechwood",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Silica_gel",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Varnish",
                "/wiki/Urbino_University",
                "/wiki/Montefeltro",
                "/wiki/Pesaro",
                "/wiki/Urbino",
                "/wiki/Rimini",
                "/wiki/Margaret_Livingstone",
                "/wiki/Harvard_University",
                "/wiki/Foveal",
                "/wiki/Peripheral_vision",
                "/wiki/Yukio_Yashiro",
                "/wiki/Chinese_paintings",
                "/wiki/Aerial_perspective",
                "/wiki/Ginevra_de%27_Benci",
                "/wiki/Sfumato",
                "/wiki/Lorenzo_di_Credi",
                "/wiki/Agnolo_di_Domenico_del_Mazziere",
                "/wiki/Frank_Z%C3%B6llner",
                "/wiki/Flemish_painting",
                "/wiki/Hans_Memling",
                "/wiki/Loggia",
                "/wiki/Ginevra_de%27_Benci",
                "/wiki/Virgin_Mary",
                "/wiki/Bulletproof_glass",
                "/wiki/Tokyo_National_Museum",
                "/wiki/Eduardo_de_Valfierno",
                "/wiki/Yves_Chaudron",
                "/wiki/United_States",
                "/wiki/Uffizi_Gallery",
                "/wiki/Florence",
                "/wiki/Guillaume_Apollinaire",
                "/wiki/Pablo_Picasso",
                "/wiki/Exoneration",
                "/wiki/Vincenzo_Peruggia",
                "/wiki/Art_theft",
                "/wiki/Louis_B%C3%A9roud",
                "/wiki/Gian_Paolo_Lomazzo",
                "/wiki/Franco-Prussian_War",
                "/wiki/Brest_Arsenal",
                "/wiki/World_War_II",
                "/wiki/Ch%C3%A2teau_d%27Amboise",
                "/wiki/Loc-Dieu_Abbey",
                "/wiki/Ch%C3%A2teau_de_Chambord",
                "/wiki/Mus%C3%A9e_Ingres",
                "/wiki/Montauban",
                "/wiki/Palace_of_Fontainebleau",
                "/wiki/Louis_XIV_of_France",
                "/wiki/Palace_of_Versailles",
                "/wiki/French_Revolution",
                "/wiki/Louvre",
                "/wiki/Napoleon",
                "/wiki/Tuileries_Palace",
                "/wiki/Martin_Kemp_(art_historian)",
                "/wiki/Sala%C3%AC",
                "/wiki/The_Virgin_and_Child_with_St._Anne",
                "/wiki/Francis_I_of_France",
                "/wiki/Clos_Luc%C3%A9",
                "/wiki/Ch%C3%A2teau_d%27Amboise",
                "/wiki/Ch%C3%A2teau_d%27Amboise",
                "/wiki/Giuliano_di_Lorenzo_de%27_Medici",
                "/wiki/Raphael",
                "/wiki/National_Museum_of_Art,_Architecture_and_Design",
                "/wiki/The_Walters_Art_Museum",
                "/wiki/Frank_Z%C3%B6llner",
                "/wiki/Frank_Z%C3%B6llner",
                "/wiki/Florence",
                "/wiki/Martin_Kemp_(art_historian)",
                "/wiki/Carlo_Pedretti",
                "/wiki/Alessandro_Vezzosi",
                "/wiki/Giorgio_Vasari",
                "/wiki/Speculations_about_Mona_Lisa",
                "/wiki/Isabella_of_Aragon,_Duchess_of_Milan",
                "/wiki/Cecilia_Gallerani",
                "/wiki/Costanza_d%27Avalos,_Duchess_of_Francavilla",
                "/wiki/Isabella_d%27Este",
                "/wiki/Caterina_Sforza",
                "/wiki/Sala%C3%AC",
                "/wiki/Model_(art)",
                "/wiki/Lisa_del_Giocondo",
                "/wiki/Florence",
                "/wiki/Tuscany",
                "/wiki/Heidelberg_University",
                "/wiki/Ancient_Rome",
                "/wiki/Cicero",
                "/wiki/Agostino_Vespucci",
                "/wiki/Apelles",
                "/wiki/Provenance",
                "/wiki/Sala%C3%AC",
                "/wiki/Renaissance",
                "/wiki/Giorgio_Vasari",
                "/wiki/My_lady",
                "/wiki/Madonna_(art)",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Lisa_del_Giocondo",
                "/wiki/Lombardy_Poplar",
                "/wiki/Panel_painting",
                "/wiki/Francis_I_of_France",
                "/wiki/French_Republic",
                "/wiki/Louvre",
                "/wiki/Paris",
                "/wiki/Help:IPA/English",
                "/wiki/Italian_language",
                "/wiki/Help:IPA/Italian",
                "/wiki/Help:IPA/Italian",
                "/wiki/French_language",
                "/wiki/Help:IPA/French",
                "/wiki/Portrait_painting",
                "/wiki/Italian_Renaissance",
                "/wiki/Leonardo_da_Vinci",
                "/wiki/Guinness_World_Records"
            ],
            "text": "A version of the Mona Lisa known as the Isleworth Mona Lisa and also known as the Earlier Mona Lisa was first bought by an English nobleman in 1778 and was rediscovered in 1913 by Hugh Blaker, an art connoisseur. The painting was presented to the media in 2012 by the Mona Lisa Foundation.[114] It is a painting of the same subject as Leonardo da Vinci's Mona Lisa. The painting is claimed by a majority of experts to be mostly an original work of Leonardo dating from the early 16th century.[115][29][116][117][118][119][120][31][121][6][7][45] Other experts, including Z\u00f6llner and Kemp, deny the attribution.[122][119]The restored painting is from a slightly different perspective than the original Mona Lisa, leading to the speculation that it is part of the world's first stereoscopic pair.[110][111][112] However, a more recent report has demonstrated that this stereoscopic pair in fact gives no reliable stereoscopic depth.[113]A version of Mona Lisa known as Mujer de mano de Leonardo Abince (\"Leonardo da Vinci's handy-woman\") held in Madrid's Museo del Prado was for centuries considered to be a work by Leonardo. However, since its restoration in 2012 it is considered to have been executed by one of Leonardo's pupils in his studio at the same time as Mona Lisa was being painted.[108] Their conclusion, based on analysis obtained after the picture underwent extensive restoration, that the painting is probably by Sala\u00ec (1480\u20131524) or by Melzi (1493\u20131572). This has been called into question by others.[109]Salvador Dal\u00ed, famous for his surrealist work, painted Self portrait as Mona Lisa in 1954.[104] In 1963 following the painting's visit to the United States, Andy Warhol created serigraph prints of multiple Mona Lisas called Thirty are Better than One, like his works of Marilyn Monroe (Twenty-five Coloured Marilyns, 1962), Elvis Presley (1964) and Campbell's soup (1961\u201362).[105] The Mona Lisa continues to inspire artists around the world. A French urban artist known pseudonymously as Invader has created versions on city walls in Paris and Tokyo using his trademark mosaic style.[106] A collection of Mona Lisa parodies may be found on YouTube.[107] A 2014 New Yorker magazine cartoon parodies the supposed enigma of the Mona Lisa smile in an animation showing progressively maniacal smiles.The avant-garde art world has made note of the undeniable fact of the Mona Lisa's popularity. Because of the painting's overwhelming stature, Dadaists and Surrealists often produce modifications and caricatures. Already in 1883, Le rire, an image of a Mona Lisa smoking a pipe, by Sapeck (Eug\u00e8ne Bataille), was shown at the \"Incoherents\" show in Paris. In 1919, Marcel Duchamp, one of the most influential modern artists, created L.H.O.O.Q., a Mona Lisa parody made by adorning a cheap reproduction with a moustache and goatee. Duchamp added an inscription, which when read out loud in French sounds like \"Elle a chaud au cul\" meaning: \"she has a hot ass\", implying the woman in the painting is in a state of sexual excitement and intended as a Freudian joke.[102] According to Rhonda R. Shearer, the apparent reproduction is in fact a copy partly modelled on Duchamp's own face.[103]Early commentators such as Vasari and Andr\u00e9 F\u00e9libien praised the picture for its realism, but by the Victorian era writers began to regard the Mona Lisa as imbued with a sense of mystery and romance. In 1859 Th\u00e9ophile Gautier wrote that the Mona Lisa was a \"sphinx of beauty who smiles so mysteriously\" and that \"Beneath the form expressed one feels a thought that is vague, infinite, inexpressible. One is moved, troubled ... repressed desires, hopes that drive one to despair, stir painfully.\" Walter Pater's famous essay of 1869 described the sitter as \"older than the rocks among which she sits; like the vampire, she has been dead many times, and learned the secrets of the grave; and has been a diver in the deep seas, and keeps their fallen day about her.\"[99] By the early 20th\u00a0century some critics started to feel the painting had become a repository for subjective exegeses and theories,[100] and upon the painting's theft in 1911, Renaissance historian Bernard Berenson admitted that it had \"simply become an incubus, and I was glad to be rid of her.\"[100][101]Before its completion the Mona Lisa had already begun to influence contemporary Florentine painting. Raphael, who had been to Leonardo's workshop several times, promptly used elements of the portrait's composition and format in several of his works, such as Young Woman with Unicorn (c.\u00a01506[97]), and Portrait of Maddalena Doni (c.\u00a01506). Celebrated later paintings by Raphael, La velata (1515\u201316) and Portrait of Baldassare Castiglione (c.\u00a01514\u201315), continued to borrow from Leonardo's painting. Zollner states that \"None of Leonardo's works would exert more influence upon the evolution of the genre than the Mona Lisa. It became the definitive example of the Renaissance portrait and perhaps for this reason is seen not just as the likeness of a real person, but also as the embodiment of an ideal.\"[98]In 2014 a France 24 article suggested that the painting could be sold to help ease the national debt, although it was noted that the Mona Lisa and other such art works were prohibited from being sold due to French heritage law, which states that \"Collections held in museums that belong to public bodies are considered public property and cannot be otherwise.\"[96]Before the 1962\u201363 tour, the painting was assessed for insurance at $100\u00a0million. The insurance was not bought. Instead, more was spent on security.[94] Adjusted for inflation using the US Consumer Price Index, $100\u00a0million in 1962 is around $782 million in 2015[95] making it, in practice, by far the most valued painting in the world.In 2014, 9.3 million people visited the Louvre.[92] Former director Henri Loyrette reckoned that \"80\u00a0percent of the people only want to see the Mona Lisa.\"[93]In 1974, the painting was exhibited in Tokyo and Moscow.[91]From December 1962 to March 1963, the French government lent it to the United States to be displayed in New York City and Washington, D.C.[87][88] It was shipped on the new liner SS France.[89] In New York an estimated 1.7 million people queued \"in order to cast a glance at the Mona Lisa for 20 seconds or so.\"[85] While exhibited in the Metropolitan Museum of Art, the painting was almost drenched in water because of a faulty sprinkler, but the bullet-proof glass case which encased the painting protected it.[90]The 1911 theft of the Mona Lisa and its subsequent return, however, was reported worldwide, leading to a massive increase in public recognition of the painting. During the 20th century it was an object for mass reproduction, merchandising, lampooning and speculation, and was claimed to have been reproduced in \"300 paintings and 2,000 advertisements\".[85] It has been said that the Mona Lisa was regarded as \"just another Leonardo until early last century, when the scandal of the painting's theft from the Louvre and subsequent return kept a spotlight on it over several years.\"[86]Today the Mona Lisa is considered the most famous painting in the world, but until the 20th century it was simply one among many highly regarded artworks.[83] Once part of King Francis I of France's collection, the Mona Lisa was among the very first artworks to be exhibited in Louvre, which became a national museum after the French Revolution. From the 19th century Leonardo began to be revered as a genius and the painting's popularity grew from the mid-19th century when French intelligentsia developed a theme that it was somehow mysterious and a representation of the femme fatale.[84] The Baedeker guide in 1878 called it \"the most celebrated work of Leonardo in the Louvre\",[85] but the painting was known more by the intelligentsia than the general public.[citation needed]On 6 April 2005\u2014following a period of curatorial maintenance, recording, and analysis\u2014the painting was moved to a new location within the museum's Salle des \u00c9tats. It is displayed in a purpose-built, climate-controlled enclosure behind bulletproof glass.[80] Since 2005 the painting has been illuminated by an LED lamp, and in 2013 a new 20 watt LED lamp was installed, specially designed for this painting. The lamp has a colour rendering index up to 98, and minimizes infrared and ultraviolet radiation which could otherwise degrade the painting.[81] The renovation of the gallery where the painting now resides was financed by the Japanese broadcaster Nippon Television.[82] About 6\u00a0million people view the painting at the Louvre each year.[25]In 1977, a new insect infestation was discovered in the back of the panel as a result of crosspieces installed to keep the painting from warping. This was treated on the spot with carbon tetrachloride, and later with an ethylene oxide treatment. In 1985, the spot was again treated with carbon tetrachloride as a preventive measure.[37]The first and most extensive recorded cleaning, revarnishing, and touch-up of the Mona Lisa was an 1809 wash and revarnishing undertaken by Jean-Marie Hooghstoel, who was responsible for restoration of paintings for the galleries of the Mus\u00e9e Napol\u00e9on. The work involved cleaning with spirits, touch-up of colour, and revarnishing the painting. In 1906, Louvre restorer Eug\u00e8ne Denizard performed watercolour retouches on areas of the paint layer disturbed by the crack in the panel. Denizard also retouched the edges of the picture with varnish, to mask areas that had been covered initially by an older frame. In 1913, when the painting was recovered after its theft, Denizard was again called upon to work on the Mona Lisa. Denizard was directed to clean the picture without solvent, and to lightly touch up several scratches to the painting with watercolour. In 1952, the varnish layer over the background in the painting was evened out. After the second 1956 attack, restorer Jean-Gabriel Goulinat was directed to touch up the damage to Mona Lisa's left elbow with watercolour.[37]The Mona Lisa has had many different decorative frames in its history, owing to changes in taste over the centuries. In 1909, the Comtesse de B\u00e9hague gave the portrait its current frame,[79] a Renaissance-era work consistent with the historical period of the Mona Lisa. The edges of the painting have been trimmed at least once in its history to fit the picture into various frames, but no part of the original paint layer has been trimmed.[37]Because the Mona Lisa's poplar support expands and contracts with changes in humidity, the picture has experienced some warping. In response to warping and swelling experienced during its storage during World War II, and to prepare the picture for an exhibit to honour the anniversary of Leonardo's 500th birthday, the Mona Lisa was fitted in 1951 with a flexible oak frame with beech crosspieces. This flexible frame, which is used in addition to the decorative frame described below, exerts pressure on the panel to keep it from warping further. In 1970, the beech crosspieces were switched to maple after it was found that the beechwood had been infested with insects. In 2004\u201305, a conservation and study team replaced the maple crosspieces with sycamore ones, and an additional metal crosspiece was added for scientific measurement of the panel's warp.[citation needed]The picture is kept under strict, climate-controlled conditions in its bulletproof glass case. The humidity is maintained at 50%\u00a0\u00b110%, and the temperature is maintained between 18 and 21\u00a0\u00b0C. To compensate for fluctuations in relative humidity, the case is supplemented with a bed of silica gel treated to provide 55% relative humidity.[37]At some point, the Mona Lisa was removed from its original frame. The unconstrained poplar panel warped freely with changes in humidity, and as a result, a crack developed near the top of the panel, extending down to the hairline of the figure. In the mid-18th century to early 19th century, two butterfly-shaped walnut braces were inserted into the back of the panel to a depth of about one third the thickness of the panel. This intervention was skilfully executed, and successfully stabilized the crack. Sometime between 1888 and 1905, or perhaps during the picture's theft, the upper brace fell out. A later restorer glued and lined the resulting socket and crack with cloth.[citation needed]The Mona Lisa has survived for more than 500 years, and an international commission convened in 1952 noted that \"the picture is in a remarkable state of preservation.\"[37] This is partly due to a variety of conservation treatments the painting has undergone. A detailed analysis in 1933 by Madame de Gironde revealed that earlier restorers had \"acted with a great deal of restraint.\"[37] Nevertheless, applications of varnish made to the painting had darkened even by the end of the 16th century, and an aggressive 1809 cleaning and revarnishing removed some of the uppermost portion of the paint layer, resulting in a washed-out appearance to the face of the figure. Despite the treatments, the Mona Lisa has been well cared for throughout its history, and although the panel's warping caused the curators \"some worry\",[78] the 2004\u201305 conservation team was optimistic about the future of the work.[37]Research in 2008 by a geomorphology professor at Urbino University and an artist-photographer revealed likenesses of Mona Lisa's landscapes to some views in the Montefeltro region in the Italian provinces of Pesaro, Urbino and Rimini.[76][77]Research in 2003 by Professor Margaret Livingstone of Harvard University said that Mona Lisa's smile disappears when observed with direct vision, known as foveal. Because of the way the human eye processes visual information, it is less suited to pick up shadows directly; however, peripheral vision can pick up shadows well.[75]There has been much speculation regarding the painting's model and landscape. For example, Leonardo probably painted his model faithfully since her beauty is not seen as being among the best, \"even when measured by late quattrocento (15th century) or even twenty-first century standards.\"[73] Some art historians in Eastern art, such as Yukio Yashiro, argue that the landscape in the background of the picture was influenced by Chinese paintings,[74] but this thesis has been contested for lack of clear evidence.[74]Mona Lisa has no clearly visible eyebrows or eyelashes. Some researchers claim that it was common at this time for genteel women to pluck these hairs, as they were considered unsightly.[69][70] In 2007, French engineer Pascal Cotte announced that his ultra-high resolution scans of the painting provide evidence that Mona Lisa was originally painted with eyelashes and with visible eyebrows, but that these had gradually disappeared over time, perhaps as a result of overcleaning.[71] Cotte discovered the painting had been reworked several times, with changes made to the size of the Mona Lisa's face and the direction of her gaze. He also found that in one layer the subject was depicted wearing numerous hairpins and a headdress adorned with pearls which was later scrubbed out and overpainted.[72]The painting was one of the first portraits to depict the sitter in front of an imaginary landscape, and Leonardo was one of the first painters to use aerial perspective.[68] The enigmatic woman is portrayed seated in what appears to be an open loggia with dark pillar bases on either side. Behind her, a vast landscape recedes to icy mountains. Winding paths and a distant bridge give only the slightest indications of human presence. Leonardo has chosen to place the horizon line not at the neck, as he did with Ginevra de' Benci, but on a level with the eyes, thus linking the figure with the landscape and emphasizing the mysterious nature of the painting.[65]The woman sits markedly upright in a \"pozzetto\" armchair with her arms folded, a sign of her reserved posture. Her gaze is fixed on the observer. The woman appears alive to an unusual extent, which Leonardo achieved by his method of not drawing outlines (sfumato). The soft blending creates an ambiguous mood \"mainly in two features: the corners of the mouth, and the corners of the eyes\".[66]The depiction of the sitter in three-quarter profile is similar to late 15th-century works by Lorenzo di Credi and Agnolo di Domenico del Mazziere.[63] Z\u00f6llner notes that the sitter's general position can be traced back to Flemish models and that \"in particular the vertical slices of columns at both sides of the panel had precedents in Flemish portraiture.\"[64] Woods-Marsden cites Hans Memling's portrait of Benededetto Portinari (1487) or Italian imitations such as Sebastiano Mainardi's pendant portraits for the use of a loggia, which has the effect of mediating between the sitter and the distant landscape, a feature missing from Leonardo's earlier portrait of Ginevra de' Benci.[65]The Mona Lisa bears a strong resemblance to many Renaissance depictions of the Virgin Mary, who was at that time seen as an ideal for womanhood.[63]The use of bulletproof glass has shielded the Mona Lisa from subsequent attacks. In April 1974, while the painting was on display at the Tokyo National Museum, a woman sprayed it with red paint as a protest against that museum's failure to provide access for disabled people.[60] On 2 August 2009, a Russian woman, distraught over being denied French citizenship, threw a ceramic teacup purchased at the Louvre; the vessel shattered against the glass enclosure.[61][62] In both cases, the painting was undamaged.In 1956, part of the painting was damaged when a vandal threw acid at it.[58] On 30 December of that year, a rock was thrown at the painting, dislodging a speck of pigment near the left elbow, later restored.[59]Peruggia may have been motivated by an associate whose copies of the original would significantly rise in value after the painting's theft. A later account suggested Eduardo de Valfierno had been the mastermind of the theft and had commissioned forger Yves Chaudron to create six copies of the painting to sell in the U.S. while the location of the original was unclear.[55] However, the original painting remained in Europe. After having kept the Mona Lisa in his apartment for two years, Peruggia grew impatient and was caught when he attempted to sell it to directors of the Uffizi Gallery in Florence. It was exhibited in the Uffizi Gallery for over two weeks and returned to the Louvre on 4 January 1914.[56] Peruggia served six months in prison for the crime and was hailed for his patriotism in Italy.[54] Before its theft, the Mona Lisa was not widely known outside the art world. It was not until the 1860s that some critics, a thin slice of the French intelligentsia, began to hail it as a masterwork of Renaissance painting.[57]French poet Guillaume Apollinaire, who had once called for the Louvre to be \"burnt down\", came under suspicion and was arrested and imprisoned. Apollinaire implicated his friend Pablo Picasso, who was brought in for questioning. Both were later exonerated.[53][54] Two years later the thief revealed himself. Louvre employee Vincenzo Peruggia had stolen the Mona Lisa by entering the building during regular hours, hiding in a broom closet, and walking out with it hidden under his coat after the museum had closed.[19] Peruggia was an Italian patriot who believed Leonardo's painting should have been returned for display in an Italian museum.On 21 August 1911, the painting was stolen from the Louvre.[52] The theft was not discovered until the next day, when painter Louis B\u00e9roud walked into the museum and went to the Salon Carr\u00e9 where the Mona Lisa had been on display for five years, only to find four iron pegs on the wall. B\u00e9roud contacted the head of the guards, who thought the painting was being photographed for promotional purposes. A few hours later, B\u00e9roud checked back with the Section Chief of the Louvre who confirmed that the Mona Lisa was not with the photographers. The Louvre was closed for an entire week during the investigation.However, this portrait does not fit with the description of the painting in the historical records: Both Vasari[11] and Gian Paolo Lomazzo[50] describe the subject as smiling; the subject in Cotte\u2019s portrait displays no smile. In addition, the portrait lacks the flanking columns drawn by Raphael in his c.1504 sketch of Mona Lisa. Moreover, Cotte admits that his reconstitution had been carried out only in support of his hypotheses and should not be considered a real painting; he stresses that the images never existed.[51] Kemp is also adamant that Cotte\u2019s images in no way establish the existence of a separate underlying portrait.[42]In December 2015, it was reported that French scientist Pascal Cotte had found a hidden portrait underneath the surface of the painting using reflective light technology.[47] The portrait is an underlying image of a model looking off to the side.[48] Having been given access to the painting by Louvre in 2004, Cotte spent ten years using layer amplification methods to study the painting.[47] According to Cotte, the underlying image is Leonardo's original Mona Lisa.[47][49]During the Franco-Prussian War (1870\u201371) it was moved from the Louvre to the Brest Arsenal.[46] During World War II, Mona Lisa was again removed from the Louvre and taken safely, first to Ch\u00e2teau d'Amboise, then to the Loc-Dieu Abbey and Ch\u00e2teau de Chambord, then finally to the Ingres Museum in Montauban.The painting was kept at the Palace of Fontainebleau, where it remained until Louis XIV moved the painting to the Palace of Versailles. After the French Revolution, it was moved to the Louvre, but spent a brief period in the bedroom of Napoleon in the Tuileries Palace.Given the issue surrounding the dating of the painting, the presence of the flanking columns in the Raphael sketch, the uncertainty concerning the person who commissioned it and its fate around the time of Leonardo\u2019s death, a number of experts have argued that Leonardo painted two versions of the Mona Lisa.[31][6][45] The first would have been commissioned by Francesco del Giocondo circa 1503, had flanking columns, have been left unfinished and have been in Salai\u2019s possession in 1525. The second, commissioned by Giuliano de Medici circa 1513, without the flanking columns, would have been sold by Salai to Francis I in 1518 and be the one in the Louvre today.[31][6][45]The fate of the painting around Leonardo\u2019s death and just after it has divided academic opinion. Some, such as Kemp, believe that upon Leonardo\u2019s death, the painting was inherited with other works by his pupil and assistant Sala\u00ec and was still in the latter\u2019s possession in 1525.[17][42] Others believe that the painting was sold to Francis I by Sala\u00ec, together with The Virgin and Child with St. Anne and the St. John the Baptist in 1518.[43] The Louvre Museum lists the painting as having entered the Royal collection in 1518.[44]In 1516, Leonardo was invited by King Fran\u00e7ois I to work at the Clos Luc\u00e9 near the king's castle in Amboise. It is believed that he took the Mona Lisa with him and continued to work after he moved to France.[25] Art historian Carmen C. Bambach has concluded that Leonardo probably continued refining the work until 1516 or 1517.[41]It is unclear as to who commissioned the painting. Vasari states that the work was painted for Francesco del Giocondo, the husband of Lisa del Giocondo.[39] However, Antonio de Beatis, following a visit with Leonardo in 1517, records that the painting was executed at the instance of Giuliano di Lorenzo de' Medici.[40]Circa 1504, Raphael executed a pen and ink sketch, today in the Louvre museum, in which the subject is flanked by large columns,. Experts universally agree it is based on Leonardo\u2019s portrait of Mona Lisa.[29][30][6][31] Other later copies of the Mona Lisa, such as those in the National Museum of Art, Architecture and Design in Oslo and The Walters Art Museum in Baltimore, also display large flanking columns. As a result, it was originally thought that the Mona Lisa in the Louvre had side columns and had been cut.[32][33][4][34][35] However, as early as 1993, Z\u00f6llner observed that the painting surface had never been trimmed.[36] This was confirmed through a series of tests conducted in 2004.[37] In view of this, Vincent Delieuvin, curator of 16th-century Italian painting at the Louvre museum states that the sketch and these other copies must have been inspired by another version,[38] while Frank Z\u00f6llner states that the sketch brings up the possibility that Leonardo executed another work on the subject of Mona Lisa.[36]Leonardo da Vinci is thought by some to have begun painting the Mona Lisa in 1503 or 1504 in Florence, Italy.[27] Although the Louvre states that it was \"doubtless painted between 1503 and 1506\",[10] the art historian Martin Kemp says there are some difficulties in confirming the actual dates with certainty.[17] In addition, many Leonardo experts, such as Carlo Pedretti [4] and Alessandro Vezzosi,[5] are of the opinion that the painting is characteristic of Leonardo\u2019s style in the final years of his life, post-1513. Other academics argue that, given the historical documentation, Leonardo would have painted the work from 1513.[7] According to Leonardo's contemporary, Giorgio Vasari, \"after he had lingered over it four years, [he] left it unfinished\".[12] Leonardo, later in his life, is said to have regretted \"never having completed a single work\".[28]Before that discovery, scholars had developed several alternative views as to the subject of the painting. Some argued that Lisa del Giocondo was the subject of a different portrait, identifying at least four other paintings as the Mona Lisa referred to by Vasari.[20][21] Several other women have been proposed as the subject of the painting.[22] Isabella of Aragon,[23] Cecilia Gallerani,[24] Costanza d'Avalos, Duchess of Francavilla,[22] Isabella d'Este, Pacifica Brandano or Brandino, Isabela Gualanda, Caterina Sforza\u2014even Sala\u00ec and Leonardo himself\u2014are all among the list of posited models portrayed in the painting.[25][26] The consensus of art historians in the 21st century maintains the long-held traditional opinion, that the painting depicts Lisa del Giocondo.[13]The model, Lisa del Giocondo,[15][16] was a member of the Gherardini family of Florence and Tuscany, and the wife of wealthy Florentine silk merchant Francesco del Giocondo.[17] The painting is thought to have been commissioned for their new home, and to celebrate the birth of their second son, Andrea.[18] The Italian name for the painting, La Gioconda, means \"jocund\" (\"happy\" or \"jovial\") or, literally, \"the jocund one\", a pun on the feminine form of Lisa's married name, \"Giocondo\".[17][19] In French, the title La Joconde has the same meaning.In response to the announcement of the discovery of this document, Vincent Delieuvin, the Louvre representative, stated \"Leonardo da Vinci was painting, in 1503, the portrait of a Florentine lady by the name of Lisa del Giocondo. About this we are now certain. Unfortunately, we cannot be absolutely certain that this portrait of Lisa del Giocondo is the painting of the Louvre.\"[14]That Leonardo painted such a work, and its date, were confirmed in 2005 when a scholar at Heidelberg University discovered a marginal note in a 1477 printing of a volume written by the ancient Roman philosopher Cicero. Dated October 1503, the note was written by Leonardo's contemporary Agostino Vespucci. This note likens Leonardo to renowned Greek painter Apelles, who is mentioned in the text, and states that Leonardo was at that time working on a painting of Lisa del Giocondo.[13]Vasari's account of the Mona Lisa comes from his biography of Leonardo published in 1550, 31 years after the artist's death. It has long been the best-known source of information on the provenance of the work and identity of the sitter. Leonardo's assistant Sala\u00ec, at his death in 1524, owned a portrait which in his personal papers was named la Gioconda, a painting bequeathed to him by Leonardo.The title of the painting, which is known in English as Mona Lisa, comes from a description by Renaissance art historian Giorgio Vasari, who wrote \"Leonardo undertook to paint, for Francesco del Giocondo, the portrait of Mona Lisa, his wife.\"[11][12] Mona in Italian is a polite form of address originating as \"ma donna\" \u2013 similar to \"Ma\u2019am\", \"Madam\", or \"my lady\" in English. This became \"madonna\", and its contraction \"mona\". The title of the painting, though traditionally spelled \"Mona\" (as used by Vasari[11]), is also commonly spelled in modern Italian as Monna Lisa (\"mona\" being a vulgarity in some Italian dialects) but this is rare in English.[citation needed]The subject's expression, which is frequently described as enigmatic,[9] the monumentality of the composition, the subtle modelling of forms, and the atmospheric illusionism were novel qualities that have contributed to the continuing fascination and study of the work.[10]The painting is thought to be a portrait of Lisa Gherardini, the wife of Francesco del Giocondo, and is in oil on a white Lombardy poplar panel. It had been believed to have been painted between 1503 and 1506; however, Leonardo may have continued working on it as late as 1517. Recent academic work suggests that it would not have been started before 1513.[4][5][6][7] It was acquired by King Francis I of France and is now the property of the French Republic, on permanent display at the Louvre Museum in Paris since 1797.[8]The Mona Lisa (/\u02ccmo\u028an\u0259 \u02c8li\u02d0s\u0259/; Italian: Monna Lisa [\u02c8m\u0254nna \u02c8li\u02d0za] or La Gioconda [la d\u0292o\u02c8konda], French: La Joconde [la \u0292\u0254k\u0254\u0303d]) is a half-length portrait painting by the Italian Renaissance artist Leonardo da Vinci that has been described as \"the best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world\".[1] The Mona Lisa is also one of the most valuable paintings in the world. It holds the Guinness World Record for the highest known insurance valuation in history at $100 million in 1962,[2] which is worth nearly $800 million in 2017.[3]",
            "title": "Mona Lisa",
            "url": "https://en.wikipedia.org/wiki/Mona_Lisa"
        },
        {
            "desc_links": [
                "/wiki/Cartesian_geometry",
                "/wiki/Hyperplane",
                "/wiki/Linear_transformation",
                "/wiki/Measure_(mathematics)",
                "/wiki/Three-dimensional_geometry",
                "/wiki/Laminar_flow",
                "/wiki/Rotation_(geometry)",
                "/wiki/Angle",
                "/wiki/Straight_angle",
                "/wiki/Line_segment",
                "/wiki/Parallelogram",
                "/wiki/Circle",
                "/wiki/Ellipse",
                "/wiki/Area",
                "/wiki/Collinear",
                "/wiki/Italic_font",
                "/wiki/Latin_alphabet",
                "/wiki/Cartesian_coordinates",
                "/wiki/Plane_geometry",
                "/wiki/Linear_map",
                "/wiki/Signed_distance_function",
                "/wiki/Straight_line",
                "/wiki/Parallel_(geometry)"
            ],
            "links": [
                "/wiki/Oblique_type",
                "/wiki/Digital_image",
                "/wiki/Pixel",
                "/wiki/Pythagorean_theorem",
                "/wiki/Geometric_mean_theorem#Based_on_shear_mappings",
                "/wiki/William_Kingdon_Clifford",
                "/wiki/Block_matrix",
                "/wiki/Direct_sum_of_vector_spaces",
                "/wiki/Vector_space",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Matrix_product",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Three-dimensional_geometry",
                "/wiki/Laminar_flow",
                "/wiki/Rotation_(geometry)",
                "/wiki/Angle",
                "/wiki/Straight_angle",
                "/wiki/Line_segment",
                "/wiki/Parallelogram",
                "/wiki/Circle",
                "/wiki/Ellipse",
                "/wiki/Area",
                "/wiki/Collinear",
                "/wiki/Italic_font",
                "/wiki/Latin_alphabet",
                "/wiki/Plane_geometry",
                "/wiki/Linear_map",
                "/wiki/Signed_distance_function",
                "/wiki/Straight_line",
                "/wiki/Parallel_(geometry)"
            ],
            "text": "The oblique type can be thought of as normal text under a shear.An algorithm due to Alan W. Paeth uses a sequence of three shear mappings (horizontal, vertical, then horizontal again) to rotate a digital image by an arbitrary angle. The algorithm is very simple to implement, and very efficient, since each step processes only one column or one row of pixels at a time.[4]The area-preserving property of a shear mapping can be used for results involving area. For instance, the Pythagorean theorem has been illustrated with shear mapping[3] as well as the related geometric mean theorem.The following applications of shear mapping were noted by William Kingdon Clifford:where M is a linear mapping from W\u2032 into W. Therefore in block matrix terms L can be represented ascorrespondingly, the typical shear fixing W is L whereTo be more precise, if V is the direct sum of W and W\u2032, and we write vectors asFor a vector space V and subspace W, a shear fixing W translates all vectors parallel to W.If the coordinates of a point are written as a column vector (a 2\u00d71 matrix), the shear mapping can be written as multiplication by a 2\u00d72 matrix:The same definition is used in three-dimensional geometry, except that the distance is measured from a fixed plane. A three-dimensional shearing transformation preserves the volume of solid figures, but changes areas of plane figures (except those that are parallel to the displacement). This transformation is used to describe laminar flow of a fluid between plates, one moving in a plane above and parallel to the first.Shear mappings must not be confused with rotations. Applying a shear map to a set of points of the plane will change all angles between them (except straight angles), and the length of any line segment that is not parallel to the direction of displacement. Therefore it will usually distort the shape of a geometric figure, for example turning squares into non-square parallelograms, and circles into ellipses. However a shearing does preserve the area of geometric figures and the alignment and relative distances of collinear points. A shear mapping is the main difference between the upright and slanted (or italic) styles of letters.In plane geometry, a shear mapping is a linear map that displaces each point in fixed direction, by an amount proportional to its signed distance from a line that is parallel to that direction.[1] This type of mapping is also called shear transformation, transvection, or just shearing.",
            "title": "Shear mapping",
            "url": "https://en.wikipedia.org/wiki/Shear_mapping"
        },
        {
            "desc_links": [
                "/wiki/Scalar_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Tensor",
                "/wiki/Quaternion",
                "/wiki/Inner_product",
                "/wiki/Scalar_multiplication",
                "/wiki/Inner_product_space",
                "/wiki/Linear_algebra",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Vector_(mathematics_and_physics)"
            ],
            "links": [
                "/wiki/Scaling_(geometry)",
                "/wiki/Linear_transformation",
                "/wiki/Manifold",
                "/wiki/Section_(fiber_bundle)",
                "/wiki/Tangent_bundle",
                "/wiki/Algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Commutative",
                "/wiki/Module_(mathematics)",
                "/wiki/Norm_(mathematics)",
                "/wiki/Normed_vector_space",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Isomorphism",
                "/wiki/Coordinate_vector_space",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Rational_number",
                "/wiki/Algebraic_number",
                "/wiki/Finite_field",
                "/wiki/Oxford_English_Dictionary",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Quaternion",
                "/wiki/Latin_language",
                "/wiki/Fran%C3%A7ois_Vi%C3%A8te",
                "/wiki/Wikipedia:Citing_sources",
                "/wiki/Scalar_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Tensor",
                "/wiki/Quaternion",
                "/wiki/Inner_product",
                "/wiki/Scalar_multiplication",
                "/wiki/Inner_product_space",
                "/wiki/Linear_algebra",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Vector_(mathematics_and_physics)"
            ],
            "text": "Operations that apply to a single value at a time.The scalar multiplication of vector spaces and modules is a special case of scaling, a kind of linear transformation.In this case the \"scalars\" may be complicated objects. For instance, if R is a ring, the vectors of the product space Rn can be made into a module with the n\u00d7n matrices with entries from R as the scalars. Another example comes from manifold theory, where the space of sections of the tangent bundle forms a module over the algebra of real functions on the manifold.When the requirement that the set of scalars form a field is relaxed so that it need only form a ring (so that, for example, the division of scalars need not be defined, or the scalars need not be commutative), the resulting more general algebraic structure is called a module.The norm is usually defined to be an element of V's scalar field K, which restricts the latter to fields that support the notion of sign. Moreover, if V has dimension 2 or more, K must be closed under square root, as well as the four arithmetic operations; thus the rational numbers Q are excluded, but the surd field is acceptable. For this reason, not every scalar product space is a normed vector space.Alternatively, a vector space V can be equipped with a norm function that assigns to every vector v in V a scalar ||v||. By definition, multiplying v by a scalar k also multiplies its norm by |k|. If ||v|| is interpreted as the length of v, this operation can be described as scaling the length of v by k. A vector space equipped with a norm is called a normed vector space (or normed linear space).According to a fundamental theorem of linear algebra, every vector space has a basis. It follows that every vector space over a scalar field K is isomorphic to a coordinate vector space where the coordinates are elements of K. For example, every real vector space of dimension n is isomorphic to n-dimensional real space Rn.The scalars can be taken from any field, including the rational, algebraic, real, and complex numbers, as well as finite fields.According to a citation in the Oxford English Dictionary the first recorded usage of the term \"scalar\" in English came with W. R. Hamilton in 1846, referring to the real part of a quaternion:The word scalar derives from the Latin word scalaris, an adjectival form of scala (Latin for \"ladder\"), from which the English word scale also comes. The first recorded usage of the word \"scalar\" in mathematics occurs in Fran\u00e7ois Vi\u00e8te's Analytic Art (In artem analyticem isagoge) (1591):[5][page\u00a0needed][6]The term scalar matrix is used to denote a matrix of the form kI where k is a scalar and I is the identity matrix.The term is also sometimes used informally to mean a vector, matrix, tensor, or other usually \"compound\" value that is actually reduced to a single component. Thus, for example, the product of a 1\u00d7n matrix and an n\u00d71 matrix, which is formally a 1\u00d71 matrix, is often said to be a scalar.The real component of a quaternion is also called its scalar part.A scalar product operation \u2013\u00a0not to be confused with scalar multiplication\u00a0\u2013 may be defined on a vector space, allowing two vectors to be multiplied to produce a scalar. A vector space equipped with a scalar product is called an inner product space.In linear algebra, real numbers or other elements of a field are called scalars and relate to vectors in a vector space through the operation of scalar multiplication, in which a vector can be multiplied by a number to produce another vector.[2][3][4] More generally, a vector space may be defined by using any field instead of real numbers, such as complex numbers. Then the scalars of that vector space will be the elements of the associated field.A scalar is an element of a field which is used to define a vector space. A quantity described by multiple scalars, such as having both direction and magnitude, is called a vector.[1]",
            "title": "Scalar (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Scalar_(mathematics)"
        },
        {
            "desc_links": [
                "/wiki/One-dimensional",
                "/wiki/Number_line",
                "/wiki/Two-dimensional",
                "/wiki/Complex_plane",
                "/wiki/Horizontal_axis",
                "/wiki/Vertical_axis",
                "/wiki/Imaginary_number",
                "/wiki/Argument_(complex_analysis)",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Polynomial",
                "/wiki/Gerolamo_Cardano",
                "/wiki/Cubic_equations",
                "/wiki/Field_extension",
                "/wiki/Field_(mathematics)",
                "/wiki/Number",
                "/wiki/Imaginary_number",
                "/wiki/Real_number"
            ],
            "links": [
                "/wiki/Local_field",
                "/wiki/Hypercomplex_number",
                "/wiki/Split-complex_number",
                "/wiki/Linear_complex_structure",
                "/wiki/Linear_representation",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Regular_representation",
                "/wiki/Algebra_(ring_theory)",
                "/wiki/Normed_division_algebra",
                "/wiki/Hurwitz%27s_theorem_(normed_division_algebras)",
                "/wiki/Sedenion",
                "/wiki/Ordered_field",
                "/wiki/Quaternion",
                "/wiki/Skew_field",
                "/wiki/Octonion",
                "/wiki/Cayley%E2%80%93Dickson_construction",
                "/wiki/Quaternion",
                "/wiki/Octonion",
                "/wiki/Richard_Dedekind",
                "/wiki/Otto_H%C3%B6lder",
                "/wiki/Felix_Klein",
                "/wiki/Henri_Poincar%C3%A9",
                "/wiki/Hermann_Schwarz",
                "/wiki/Karl_Weierstrass",
                "/wiki/G._H._Hardy",
                "/wiki/Niels_Henrik_Abel",
                "/wiki/Carl_Gustav_Jacob_Jacobi",
                "/wiki/Augustin_Louis_Cauchy",
                "/wiki/Bernhard_Riemann",
                "/wiki/Copenhagen_Academy",
                "/wiki/Jean-Robert_Argand",
                "/wiki/Fundamental_theorem_of_algebra#History",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/Topology",
                "/wiki/C._V._Mourey",
                "/wiki/Giusto_Bellavitis",
                "/wiki/Caspar_Wessel",
                "/wiki/John_Wallis",
                "/wiki/Power_series",
                "/wiki/Leonhard_Euler",
                "/wiki/Euler%27s_formula",
                "/wiki/Complex_analysis",
                "/wiki/Abraham_de_Moivre",
                "/wiki/De_Moivre%27s_formula",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/Riemann_zeta_function",
                "/wiki/Prime_number",
                "/wiki/Gaussian_integer",
                "/wiki/Fermat%27s_theorem_on_sums_of_two_squares",
                "/wiki/Algebraic_number",
                "/wiki/Algebraic_number_theory",
                "/wiki/Field_theory_(mathematics)",
                "/wiki/Number_field",
                "/wiki/Root_of_unity",
                "/wiki/Nonagon",
                "/wiki/Compass_and_straightedge_constructions",
                "/wiki/Fractal",
                "/wiki/Mandelbrot_set",
                "/wiki/Julia_set",
                "/wiki/Special_relativity",
                "/wiki/General_relativity",
                "/wiki/Spacetime",
                "/wiki/Wick_rotation",
                "/wiki/Quantum_field_theory",
                "/wiki/Spinor",
                "/wiki/Tensor",
                "/wiki/Mathematical_formulations_of_quantum_mechanics",
                "/wiki/Hilbert_space",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Matrix_mechanics",
                "/wiki/Amplitude_modulation",
                "/wiki/Digital_signal_processing",
                "/wiki/Digital_image_processing",
                "/wiki/Wavelet",
                "/wiki/Data_compression",
                "/wiki/Digital_data",
                "/wiki/Sound",
                "/wiki/Video",
                "/wiki/Angular_frequency",
                "/wiki/Fourier_analysis",
                "/wiki/Signal_analysis",
                "/wiki/Sine_wave",
                "/wiki/Frequency",
                "/wiki/Amplitude",
                "/wiki/Argument_(complex_analysis)",
                "/wiki/Phase_(waves)",
                "/wiki/Voltage",
                "/wiki/Electric_circuit",
                "/wiki/Electric_current",
                "/wiki/Electrical_engineering",
                "/wiki/Fourier_transform",
                "/wiki/Voltage",
                "/wiki/Electric_current",
                "/wiki/Resistor",
                "/wiki/Capacitor",
                "/wiki/Inductor",
                "/wiki/Electrical_impedance",
                "/wiki/Phasor",
                "/wiki/Differential_equation",
                "/wiki/Linear_differential_equation#Homogeneous_equations_with_constant_coefficients",
                "/wiki/Linear_differential_equation",
                "/wiki/Difference_equations",
                "/wiki/Fluid_dynamics",
                "/wiki/Potential_flow_in_two_dimensions",
                "/wiki/Improper_integral",
                "/wiki/Methods_of_contour_integration",
                "/wiki/Nonminimum_phase",
                "/wiki/Control_theory",
                "/wiki/Time_domain",
                "/wiki/Frequency_domain",
                "/wiki/Laplace_transform",
                "/wiki/Zeros_and_poles",
                "/wiki/Root_locus",
                "/wiki/Nyquist_plot",
                "/wiki/Nichols_plot",
                "/wiki/Signal_processing",
                "/wiki/Control_theory",
                "/wiki/Electromagnetism",
                "/wiki/Fluid_dynamics",
                "/wiki/Quantum_mechanics",
                "/wiki/Cartography",
                "/wiki/Vibration#Vibration_analysis",
                "/wiki/Open_subset",
                "/wiki/Meromorphic_function",
                "/wiki/Essential_singularity",
                "/wiki/Holomorphic_function",
                "/wiki/Cauchy%E2%80%93Riemann_equations",
                "/wiki/Linear_transformation#Definition_and_first_consequences",
                "/wiki/Exponentiation#Failure_of_power_and_logarithm_identities",
                "/wiki/Exponentiation",
                "/wiki/Arg_(mathematics)",
                "/wiki/Natural_logarithm",
                "/wiki/Multivalued_function",
                "/wiki/Principal_value",
                "/wiki/Interval_(mathematics)",
                "/wiki/Complex_logarithm",
                "/wiki/Infinite_set",
                "/wiki/Euler%27s_formula",
                "/wiki/Sine",
                "/wiki/Cosine",
                "/wiki/Hyperbolic_functions",
                "/wiki/Tangent_(function)",
                "/wiki/Analytic_continuation",
                "/wiki/Elementary_function",
                "/wiki/Exponential_function",
                "/wiki/Infinite_series",
                "/wiki/Metric_space",
                "/wiki/Triangle_inequality",
                "/wiki/Convergent_series",
                "/wiki/Continuous_function",
                "/wiki/Convergent_sequence",
                "/wiki/(%CE%B5,_%CE%B4)-definition_of_limit",
                "/wiki/Metric_(mathematics)",
                "/wiki/Complex_analysis",
                "/wiki/Applied_mathematics",
                "/wiki/Real_analysis",
                "/wiki/Number_theory",
                "/wiki/Prime_number_theorem",
                "/wiki/Complex_function",
                "/wiki/Graph_of_a_function_of_two_variables",
                "/wiki/Rotation_matrix",
                "/wiki/Determinant",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Algebraic_extension",
                "/wiki/Algebraic_closure",
                "/wiki/Isomorphism",
                "/wiki/Quotient_ring",
                "/wiki/Coset",
                "/wiki/Vector_space",
                "/wiki/Linear_combination",
                "/wiki/Prime_ideal",
                "/wiki/Principal_ideal_domain",
                "/wiki/Maximal_ideal",
                "/wiki/Ring_(mathematics)",
                "/wiki/Polynomial_ring",
                "/wiki/Coefficient",
                "/wiki/Distributive_law",
                "/wiki/Ordered_pair",
                "/wiki/Connected_space",
                "/wiki/Locally_compact",
                "/wiki/Topological_ring",
                "/wiki/Connected_space",
                "/wiki/Base_(topology)",
                "/wiki/Involution_(mathematics)",
                "/wiki/Automorphism",
                "/wiki/Neighborhood_(topology)",
                "/wiki/Continuity_(topology)",
                "/wiki/Mathematical_analysis",
                "/wiki/Topology",
                "/wiki/Topological_ring",
                "/wiki/Topological_space",
                "/wiki/Characteristic_(algebra)",
                "/wiki/Transcendence_degree",
                "/wiki/Prime_field",
                "/wiki/Cardinality_of_the_continuum",
                "/wiki/Algebraically_closed",
                "/wiki/Isomorphic",
                "/wiki/Algebraic_closure",
                "/wiki/P-adic_numbers",
                "/wiki/Puiseux_series",
                "/wiki/Axiom_of_choice",
                "/wiki/Square_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Liouville%27s_theorem_(complex_analysis)",
                "/wiki/Topology",
                "/wiki/Winding_number",
                "/wiki/Galois_theory",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Carl_Friedrich_Gauss",
                "/wiki/Jean_le_Rond_d%27Alembert",
                "/wiki/Algebraically_closed_field",
                "/wiki/Rational_number",
                "/wiki/Square_root_of_2",
                "/wiki/Coefficient",
                "/wiki/Complex_analysis",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Polynomial",
                "/wiki/Lie_algebra",
                "/wiki/Ordered_field",
                "/wiki/Total_order",
                "/wiki/Field_(mathematics)",
                "/wiki/Additive_inverse",
                "/wiki/Multiplicative_inverse",
                "/wiki/Commutativity",
                "/wiki/Multivalued_function",
                "/wiki/Nth_root",
                "/wiki/De_Moivre%27s_formula",
                "/wiki/Branch_cut",
                "/wiki/Complex_logarithm",
                "/wiki/Absolute_convergence",
                "/wiki/Taylor_series",
                "/wiki/E_(mathematical_constant)",
                "/wiki/Euler%27s_formula",
                "/wiki/Arctan",
                "/wiki/Machin-like_formulas",
                "/wiki/Pi",
                "/wiki/Radian",
                "/wiki/Arctan",
                "/wiki/Turn_(geometry)",
                "/wiki/Angle_notation",
                "/wiki/Electronics",
                "/wiki/Phasor_(sine_waves)",
                "/wiki/Cis_(mathematics)",
                "/wiki/Euler%27s_formula",
                "/wiki/Atan2",
                "/wiki/Principal_value",
                "/wiki/Radian",
                "/wiki/Multivalued_function",
                "/wiki/Pythagoras%27_theorem",
                "/wiki/Complex_plane",
                "/wiki/Absolute_value",
                "/wiki/Origin_(mathematics)",
                "/wiki/Positive_real_axis",
                "/wiki/Inversive_geometry",
                "/wiki/Network_analysis_(electrical_circuits)",
                "/wiki/Maximum_power_transfer_theorem",
                "/wiki/Multiplicative_inverse",
                "/wiki/Denominator",
                "/wiki/Rationalisation_(mathematics)",
                "/wiki/Square_(algebra)",
                "/wiki/Parallelogram",
                "/wiki/Triangle",
                "/wiki/Congruence_(geometry)",
                "/wiki/Subtraction",
                "/wiki/Addition",
                "/wiki/Linear_ordering",
                "/wiki/Linear_ordering",
                "/wiki/Ordered_field",
                "/wiki/Electromagnetism",
                "/wiki/Electrical_engineering",
                "/wiki/Electric_current",
                "/wiki/Rafael_Bombelli",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Quaternions",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Polynomial",
                "/wiki/Algebraically_closed_field",
                "/wiki/Polynomial",
                "/wiki/Root_of_a_function",
                "/wiki/Trigonometric_functions",
                "/wiki/Negative_numbers",
                "/wiki/Rational_root_test",
                "/wiki/Irreducible_polynomial",
                "/wiki/Casus_irreducibilis",
                "/wiki/Gerolamo_Cardano",
                "/wiki/Euclidean_vector#Addition_and_subtraction",
                "/wiki/Orientation_(geometry)",
                "/wiki/Turn_(geometry)",
                "/wiki/Right_angle",
                "/wiki/Vector_(geometric)",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Argand_diagram",
                "/wiki/Jean-Robert_Argand",
                "/wiki/Ordered_pair",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Imaginary_number",
                "/wiki/The_fundamental_theorem_of_algebra",
                "/wiki/Polynomial_equation",
                "/wiki/Field_extension",
                "/wiki/Indeterminate_(variable)",
                "/wiki/Imaginary_unit",
                "/wiki/Real_number",
                "/wiki/One-dimensional",
                "/wiki/Number_line",
                "/wiki/Two-dimensional",
                "/wiki/Complex_plane",
                "/wiki/Horizontal_axis",
                "/wiki/Vertical_axis",
                "/wiki/Imaginary_number",
                "/wiki/Argument_(complex_analysis)",
                "/wiki/Fundamental_theorem_of_algebra",
                "/wiki/Polynomial",
                "/wiki/Gerolamo_Cardano",
                "/wiki/Cubic_equations",
                "/wiki/Field_extension",
                "/wiki/Field_(mathematics)",
                "/wiki/Number",
                "/wiki/Imaginary_number",
                "/wiki/Real_number"
            ],
            "text": "The fields R and Qp and their finite field extensions, including C, are local fields.Hypercomplex numbers also generalize R, C, H, and O. For example, this notion contains the split-complex numbers, which are elements of the ring R[x]/(x2 \u2212 1) (as opposed to R[x]/(x2 + 1)). In this ring, the equation a2 = 1 has four solutions.is also isomorphic to the field C, and gives an alternative complex structure on R2. This is generalized by the notion of a linear complex structure.has the property that its square is the negative of the identity matrix: J2 = \u2212I. Theni.e., the one mentioned in the section on matrix representation of complex numbers above. While this is a linear representation of C in the 2 \u00d7 2 real matrices, it is not the only one. Any matrixfor some fixed complex number w can be represented by a 2\u2009\u00d7\u20092 matrix (once a basis has been chosen). With respect to the basis (1,\u2009i), this matrix isThe Cayley\u2013Dickson construction is closely related to the regular representation of C, thought of as an R-algebra (an R-vector space with a multiplication), with respect to the basis (1,\u2009i). This means the following: the R-linear mapReals, complex numbers, quaternions and octonions are all normed division algebras over R. However, by Hurwitz's theorem they are the only ones. The next step in the Cayley\u2013Dickson construction, the sedenions, in fact fails to have this structure.However, just as applying the construction to reals loses the property of ordering, more properties familiar from real and complex numbers vanish with increasing dimension. The quaternions are only a skew field, i.e. for some x,\u2009y: x\u00b7y \u2260 y\u00b7x for two quaternions, the multiplication of octonions fails (in addition to not being commutative) to be associative: for some x,\u2009y,\u2009z: (x\u00b7y)\u00b7z \u2260 x\u00b7(y\u00b7z).The process of extending the field R of reals to C is known as the Cayley\u2013Dickson construction. It can be carried further to higher dimensions, yielding the quaternions H and octonions O which (as a real vector space) are of dimension\u00a04 and 8, respectively. In this context the complex numbers have been called the binarions.[37]Later classical writers on the general theory include Richard Dedekind, Otto H\u00f6lder, Felix Klein, Henri Poincar\u00e9, Hermann Schwarz, Karl Weierstrass and many others.The English mathematician G. H. Hardy remarked that Gauss was the first mathematician to use complex numbers in 'a really confident and scientific way' although mathematicians such as Niels Henrik Abel and Carl Gustav Jacob Jacobi were necessarily using them routinely before Gauss published his 1831 treatise.[36] Augustin Louis Cauchy and Bernhard Riemann together brought the fundamental ideas of complex analysis to a high state of completion, commencing around 1825 in Cauchy's case.Wessel's memoir appeared in the Proceedings of the Copenhagen Academy but went largely unnoticed. In 1806 Jean-Robert Argand independently issued a pamphlet on complex numbers and provided a rigorous proof of the fundamental theorem of algebra. Carl Friedrich Gauss had earlier published an essentially topological proof of the theorem in 1797 but expressed his doubts at the time about \"the true metaphysics of the square root of \u22121\". It was not until 1831 that he overcame these doubts and published his treatise on complex numbers as points in the plane, largely establishing modern notation and terminology. In the beginning of the 19th century, other mathematicians discovered independently the geometrical representation of the complex numbers: Bu\u00e9e, Mourey, Warren, Fran\u00e7ais and his brother, Bellavitis.[35]The idea of a complex number as a point in the complex plane (above) was first described by Caspar Wessel in 1799, although it had been anticipated as early as 1685 in Wallis's De Algebra tractatus.by formally manipulating complex power series and observed that this formula could be used to reduce any trigonometric identity to much simpler exponential identities.In 1748 Leonhard Euler went further and obtained Euler's formula of complex analysis:In the 18th century complex numbers gained wider use, as it was noticed that formal manipulation of complex expressions could be used to simplify calculations involving trigonometric functions. For instance, in 1730 Abraham de Moivre noted that the complicated identities relating trigonometric functions of an integer multiple of an angle to powers of trigonometric functions of that angle could be simply re-expressed by the following well-known formula which bears his name, de Moivre's formula:The term \"imaginary\" for these quantities was coined by Ren\u00e9 Descartes in 1637, although he was at pains to stress their imaginary nature[34]Analytic number theory studies numbers, often integers or rationals, by taking advantage of the fact that they can be regarded as complex numbers, in which analytic methods can be used. This is done by encoding number-theoretic information in complex-valued functions. For example, the Riemann zeta function \u03b6(s) is related to the distribution of prime numbers.Another example are Gaussian integers, that is, numbers of the form x + iy, where x and y are integers, which can be used to classify sums of squares.As mentioned above, any nonconstant polynomial equation (in complex coefficients) has a solution in C. A fortiori, the same is true if the equation has rational coefficients. The roots of such equations are called algebraic numbers \u2013 they are a principal object of study in algebraic number theory. Compared to Q, the algebraic closure of Q, which also contains all algebraic numbers, C has the advantage of being easily understandable in geometric terms. In this way, algebraic methods can be used to study geometric questions and vice versa. With algebraic methods, more specifically applying the machinery of field theory to the number field containing roots of unity, it can be shown that it is not possible to construct a regular nonagon using only compass and straightedge \u2013 a purely geometric problem.Certain fractals are plotted in the complex plane, e.g. the Mandelbrot set and Julia sets.In special and general relativity, some formulas for the metric on spacetime become simpler if one takes the time component of the spacetime continuum to be imaginary. (This approach is no longer standard in classical relativity, but is used in an essential way in quantum field theory.) Complex numbers are essential to spinors, which are a generalization of the tensors used in relativity.The complex number field is intrinsic to the mathematical formulations of quantum mechanics, where complex Hilbert spaces provide the context for one such formulation that is convenient and perhaps most standard. The original foundation formulas of quantum mechanics\u2014the Schr\u00f6dinger equation and Heisenberg's matrix mechanics\u2014make use of complex numbers.Another example, relevant to the two side bands of amplitude modulation of AM radio, is:This use is also extended into digital signal processing and digital image processing, which utilize digital versions of Fourier analysis (and wavelet analysis) to transmit, compress, restore, and otherwise process digital audio signals, still images, and video signals.where \u03c9 represents the angular frequency and the complex number A encodes the phase and amplitude as explained above.andIf Fourier analysis is employed to write a given real-valued signal as a sum of periodic functions, these periodic functions are often written as complex valued functions of the formComplex numbers are used in signal analysis and other fields for a convenient description for periodically varying signals. For given real functions representing actual physical quantities, often in terms of sines and cosines, corresponding complex functions are considered of which the real parts are the original quantities. For a sine wave of a given frequency, the absolute value |\u2009z\u2009| of the corresponding z is the amplitude and the argument arg(z) is the phase.To obtain the measurable quantity, the real part is taken:Since the voltage in an AC circuit is oscillating, it can be represented asIn electrical engineering, the imaginary unit is denoted by j, to avoid confusion with I, which is generally in use to denote electric current, or, more particularly, i, which is generally in use to denote instantaneous electric current.In electrical engineering, the Fourier transform is used to analyze varying voltages and currents. The treatment of resistors, capacitors, and inductors can then be unified by introducing imaginary, frequency-dependent resistances for the latter two and combining all three in a single complex number called the impedance. This approach is called phasor calculus.In differential equations, it is common to first find all complex roots r of the characteristic equation of a linear differential equation or equation system and then attempt to solve the system in terms of base functions of the form f(t) = ert. Likewise, in difference equations, the complex roots r of the characteristic equation of the difference equation system are used, to attempt to solve the system in terms of base functions of the form f(t) = rt.In fluid dynamics, complex functions are used to describe potential flow in two dimensions.In applied fields, complex numbers are often used to compute certain real-valued improper integrals, by means of complex-valued functions. Several methods exist to do this; see methods of contour integration.If a system has zeros in the right half plane, it is a nonminimum phase system.In the root locus method, it is important whether zeros and poles are in the left or right half planes, i.e. have real part greater than or less than zero. If a linear, time-invariant (LTI) system has poles that areIn control theory, systems are often transformed from the time domain to the frequency domain using the Laplace transform. The system's zeros and poles are then analyzed in the complex plane. The root locus, Nyquist plot, and Nichols plot techniques all make use of the complex plane.Complex numbers have essential concrete applications in a variety of scientific and related areas such as signal processing, control theory, electromagnetism, fluid dynamics, quantum mechanics, cartography, and vibration analysis. Some applications of complex numbers are:Complex analysis shows some features not apparent in real analysis. For example, any two holomorphic functions f and g that agree on an arbitrarily small open subset of C necessarily agree everywhere. Meromorphic functions, functions that can locally be written as f(z)/(z \u2212 z0)n with a holomorphic function f, still share some of the features of holomorphic functions. Other functions have essential singularities, such as sin(1/z) at z = 0.A function f\u2009: C \u2192 C is called holomorphic if it satisfies the Cauchy\u2013Riemann equations. For example, any R-linear map C \u2192 C can be written in the formBoth sides of the equation are multivalued by the definition of complex exponentiation given here, and the values on the left are a subset of those on the right.Complex numbers, unlike real numbers, do not in general satisfy the unmodified power and logarithm identities, particularly when na\u00efvely treated as single-valued functions; see failure of power and logarithm identities. For example, they do not satisfyComplex exponentiation z\u03c9 is defined aswhere arg is the argument defined above, and ln the (real) natural logarithm. As arg is a multivalued function, unique only up to a multiple of 2\u03c0, log is also multivalued. The principal value of log is often taken by restricting the imaginary part to the interval (\u2212\u03c0,\u03c0].for any complex number w \u2260 0. It can be shown that any such solution z\u2014called complex logarithm of w\u2014satisfiesUnlike in the situation of real numbers, there is an infinitude of complex solutions z of the equationfor any real number \u03c6, in particularEuler's formula states:The series defining the real trigonometric functions sine and cosine, as well as the hyperbolic functions sinh and cosh, also carry over to complex arguments without change. For the other trigonometric and hyperbolic functions, such as tangent, things are slightly more complicated, as the defining series do not converge for all complex values. Therefore, one must define them either in terms of sine, cosine and exponential, or, equivalently, by using the method of analytic continuation.Like in real analysis, this notion of convergence is used to construct a number of elementary functions: the exponential function exp(z), also written ez, is defined as the infinite seriesfor any two complex numbers z1 and z2.is a complete metric space, which notably includes the triangle inequalityThe notions of convergent series and continuous functions in (real) analysis have natural analogs in complex analysis. A sequence of complex numbers is said to converge if and only if its real and imaginary parts do. This is equivalent to the (\u03b5, \u03b4)-definition of limits, where the absolute value of real numbers is replaced by the one of complex numbers. From a more abstract point of view, C, endowed with the metricThe study of functions of a complex variable is known as complex analysis and has enormous practical use in applied mathematics as well as in other branches of mathematics. Often, the most natural proofs for statements in real analysis or even number theory employ techniques from complex analysis (see prime number theorem for an example). Unlike real functions, which are commonly represented as two-dimensional graphs, complex functions have four-dimensional graphs and may usefully be illustrated by color-coding a three-dimensional graph to suggest four dimensions, or by animating the complex function's dynamic transformation of the complex plane.The geometric description of the multiplication of complex numbers can also be expressed in terms of rotation matrices by using this correspondence between complex numbers and such matrices. Moreover, the square of the absolute value of a complex number expressed as a matrix is equal to the determinant of that matrix:Here the entries a and b are real numbers. The sum and product of two such matrices is again of this form, and the sum and product of complex numbers corresponds to the sum and product of such matrices, the product being:Complex numbers a + bi can also be represented by 2\u2009\u00d7\u20092 matrices that have the following form:Accepting that C is algebraically closed, since it is an algebraic extension of R in this approach, C is therefore the algebraic closure of R.The formulas for addition and multiplication in the ring R[X], modulo the relation (X2 = 1 correspond to the formulas for addition and multiplication of complex numbers defined as ordered pairs. So the two definitions of the field C are isomorphic (as fields).The set of complex numbers is defined as the quotient ring R[X]/(X 2 + 1).[28] This extension field contains two square roots of \u22121, namely (the cosets of) X and \u2212X, respectively. (The cosets of) 1 and X form a basis of R[X]/(X 2 + 1) as a real vector space, which means that each element of the extension field can be uniquely written as a linear combination in these two elements. Equivalently, elements of the extension field can be written as ordered pairs (a,\u2009b) of real numbers. The quotient ring is a field, because the (X2 + 1) is a prime ideal in R[X], a principal ideal domain, and therefore is a maximal ideal.where the a0, ...,\u2009an are real numbers. The usual addition and multiplication of polynomials endows the set R[X] of all such polynomials with a ring structure. This ring is called the polynomial ring over the real numbers.must hold for any three elements x, y and z of a field. The set R of real numbers does form a field. A polynomial p(X) with real coefficients is an expression of the formThough this low-level construction does accurately describe the structure of the complex numbers, the following equivalent definition reveals the algebraic nature of C more immediately. This characterization relies on the notion of fields and polynomials. A field is a set endowed with addition, subtraction, multiplication and division operations that behave as is familiar from, say, rational numbers. For example, the distributive lawIt is then just a matter of notation to express (a,\u2009b) as a + bi.The set C of complex numbers can be defined as the set R2 of ordered pairs (a,\u2009b) of real numbers, in which the following rules for addition and multiplication are imposed:[27]The only connected locally compact topological fields are R and C. This gives another characterization of C as a topological field, since C can be distinguished from R because the nonzero complex numbers are connected, while the nonzero real numbers are not.[26]Any field F with these properties can be endowed with a topology by taking the sets B(x,\u2009p) = {\u2009y | p \u2212 (y \u2212 x)(y \u2212 x)* \u2208 P\u2009}\u2009 as a base, where x ranges over the field and p ranges over P. With this topology F is isomorphic as a topological field to C.Moreover, C has a nontrivial involutive automorphism x \u21a6 x* (namely the complex conjugation), such that x\u2009x* is in P for any nonzero x in C.The preceding characterization of C describes only the algebraic aspects of C. That is to say, the properties of nearness and continuity, which matter in areas such as analysis and topology, are not dealt with. The following description of C as a topological field (that is, a field that is equipped with a topology, which allows the notion of convergence) does take into account the topological properties. C contains a subset P (namely the set of positive real numbers) of nonzero elements satisfying the following three conditions:The field C has the following three properties: first, it has characteristic 0. This means that 1 + 1 + \u22ef + 1 \u2260 0 for any number of summands (all of which equal one). Second, its transcendence degree over Q, the prime field of C, is the cardinality of the continuum. Third, it is algebraically closed (see above). It can be shown that any field having these properties is isomorphic (as a field) to C. For example, the algebraic closure of Qp also satisfies these three properties, so these two fields are isomorphic (as fields, but not as topological fields).[25] Also, C is isomorphic to the field of complex Puiseux series. However, specifying an isomorphism requires the axiom of choice. Another consequence of this algebraic characterization is that C contains many proper subfields that are isomorphic to C.Because of this fact, theorems that hold for any algebraically closed field, apply to C. For example, any non-empty complex square matrix has at least one (complex) eigenvalue.There are various proofs of this theorem, either by analytic methods such as Liouville's theorem, or topological ones such as the winding number, or a proof combining Galois theory and the fact that any real polynomial of odd degree has at least one real root.has at least one complex solution z, provided that at least one of the higher coefficients a1,\u2009\u2026,\u2009an is nonzero.[24] This is the statement of the fundamental theorem of algebra, of Carl Friedrich Gauss and Jean le Rond d'Alembert. Because of this fact, C is called an algebraically closed field. This property does not hold for the field of rational numbers Q (the polynomial x2 \u2212 2 does not have a rational root, since \u221a2 is not a rational number) nor the real numbers R (the polynomial x2 + a does not have a real root for a > 0, since the square of x is positive for any real number x).Given any complex numbers (called coefficients) a0,\u2009\u2026,\u2009an, the equationWhen the underlying field for a mathematical topic or construct is the field of complex numbers, the topic's name is usually modified to reflect that fact. For example: complex analysis, complex matrix, complex polynomial, and complex Lie algebra.Unlike the reals, C is not an ordered field, that is to say, it is not possible to define a relation z1 < z2 that is compatible with the addition and multiplication. In fact, in any ordered field, the square of any element is necessarily positive, so i2 = \u22121 precludes the existence of an ordering on C.[23]These two laws and the other requirements on a field can be proven by the formulas given above, using the fact that the real numbers themselves form a field.The set C of complex numbers is a field.[22] Briefly, this means that the following facts hold: first, any two complex numbers can be added and multiplied to yield another complex number. Second, for any complex number z, its additive inverse \u2212z is also a complex number; and third, every nonzero complex number has a reciprocal complex number. Moreover, these operations satisfy a number of laws, for example the law of commutativity of addition and multiplication for any two complex numbers z1 and z2:(which holds for positive real numbers), do in general not hold for complex numbers.for any integer k satisfying 0 \u2264 k \u2264 n \u2212 1. Here n\u221ar is the usual (positive) nth root of the positive real number r. While the nth root of a positive real number r is chosen to be the positive real number c satisfying cn = r there is no natural way of distinguishing one particular complex nth root of a complex number. Therefore, the nth root of z is considered as a multivalued function (in z), as opposed to a usual function f, for which f(z) is a uniquely defined number. Formulas such asThe nth roots of z are given byWhen n is an integer, this simplifies to de Moivre's formula:to define complex exponentiation, which is likewise multi-valued:We may use the identityAlternatively, a branch cut can be used to define a single-valued \"branch\" of the complex logarithm.To deal with the existence of more than one possible value for a given input, the complex logarithm may be considered a multi-valued function, withwhere r is a non-negative real number, one possible value for the complex logarithm of z isIt follows from Euler's formula that, for any complex number z written in polar form,The rearrangement of terms is justified because each series is absolutely convergent.and so on, and by considering the Taylor series expansions of eix, cos x and sin x:where e is the base of the natural logarithm. This can be proved through induction by observing thatEuler's formula states that, for any real number x,Similarly, division is given byholds. As the arctan function can be approximated highly efficiently, formulas like this\u2014known as Machin-like formulas\u2014are used for high-precision approximations of \u03c0.Since the real and imaginary part of 5 + 5i are equal, the argument of that number is 45 degrees, or \u03c0/4 (in radian). On the other hand, it is also the sum of the angles at the origin of the red and blue triangles are arctan(1/3) and arctan(1/2), respectively. Thus, the formulaIn other words, the absolute values are multiplied and the arguments are added to yield the polar form of the product. For example, multiplying by i corresponds to a quarter-turn counter-clockwise, which gives back i2 = \u22121. The picture at the right illustrates the multiplication ofwe may deriveFormulas for multiplication, division and exponentiation are simpler in polar form than the corresponding formulas in Cartesian coordinates. Given two complex numbers z1 = r1(cos\u200a\u03c61 + i\u2009sin\u200a\u03c61) and z2 = r2(cos\u200a\u03c62 + i\u2009sin\u200a\u03c62), because of the well-known trigonometric identitiesIn angle notation, often used in electronics to represent a phasor with amplitude r and phase \u03c6, it is written as[21]Using the cis function, this is sometimes abbreviated toUsing Euler's formula this can be written asTogether, r and \u03c6 give another way of representing complex numbers, the polar form, as the combination of modulus and argument fully specify the position of a point on the plane. Recovering the original rectangular co-ordinates from the polar form is done by the formula called trigonometric formThe value of \u03c6 equals the result of atan2:Normally, as given above, the principal value in the interval (\u2212\u03c0,\u03c0] is chosen. Values in the range [0,2\u03c0) are obtained by adding 2\u03c0 if the value is negative. The value of \u03c6 is expressed in radians in this article. It can increase by any integer multiple of 2\u03c0 and still give the same angle. Hence, the arg function is sometimes considered as multivalued. The polar angle for the complex number 0 is indeterminate, but arbitrary choice of the angle\u00a00 is common.The square of the absolute value isBy Pythagoras' theorem, the absolute value of complex number is the distance to the origin of the point representing the complex number in the complex plane.If z is a real number (that is, if y = 0), then r = |\u2009x\u2009|. That is, the absolute value of a real number equals its absolute value as a complex number.The absolute value (or modulus or magnitude) of a complex number z = x + yi is[19]An alternative way of defining a point P in the complex plane, other than using the x- and y-coordinates, is to use the distance of the point from O, the point whose coordinates are (0,\u20090) (the origin), together with the angle subtended between the positive real axis and the line segment OP in a counterclockwise direction. This idea leads to the polar form of complex numbers.andThis formula can be used to compute the multiplicative inverse of a complex number if it is given in rectangular coordinates. Inversive geometry, a branch of geometry studying reflections more general than ones about a line, can also be expressed in terms of complex numbers. In the network analysis of electrical circuits, the complex conjugate is used in finding the equivalent impedance when the maximum power transfer theorem is used.The reciprocal of a nonzero complex number z = x + yi is given byAs shown earlier, c \u2212 di is the complex conjugate of the denominator c + di. At least one of the real part c and the imaginary part d of the denominator must be nonzero for division to be defined. This is called \"rationalization\" of the denominator (although the denominator in the final expression might be an irrational real number).Division can be defined in this way because of the following observation:The division of two complex numbers is defined in terms of complex multiplication, which is described above, and real division. When at least one of c and d is non-zero, we haveThe preceding definition of multiplication of general complex numbers follows naturally from this fundamental property of i. Indeed, if i is treated as a number so that di means d times i, the above multiplication rule is identical to the usual rule for multiplying two sums of two terms.In particular, the square of i is \u22121:The multiplication of two complex numbers is defined by the following formula:Using the visualization of complex numbers in the complex plane, the addition has the following geometric interpretation: the sum of two complex numbers A and B, interpreted as points of the complex plane, is the point X obtained by building a parallelogram, three of whose vertices are O, A and B. Equivalently, X is the point such that the triangles with vertices O, A, B, and X, B, A, are congruent.Similarly, subtraction is defined byComplex numbers are added by separately adding the real and imaginary parts of the summands. That is to say:Conjugation distributes over the standard arithmetic operations:Moreover, a complex number is real if and only if it equals its own conjugate.The real and imaginary parts of a complex number z can be extracted using the conjugate:Because complex numbers are naturally thought of as existing on a two-dimensional plane, there is no natural linear ordering on the set of complex numbers. Furthermore, there is no linear ordering on the complex numbers that is compatible with addition and multiplication \u2013 the complex numbers cannot have the structure of an ordered field. This is because any square in an ordered field is at least 0, but i2 = \u22121.Because it is a polynomial in the indeterminate i, a + ib may be written instead of a + bi, which is often expedient when b is a radical.[13] In some disciplines, in particular electromagnetism and electrical engineering, j is used instead of i,[14] since i is frequently used for electric current. In these cases complex numbers are written as a + bj or a + jb.Many mathematicians contributed to the full development of complex numbers. The rules for addition, subtraction, multiplication, and division of complex numbers were developed by the Italian mathematician Rafael Bombelli.[12] A more abstract formalism for the complex numbers was further developed by the Irish mathematician William Rowan Hamilton, who extended this abstraction to the theory of quaternions.Work on the problem of general polynomials ultimately led to the fundamental theorem of algebra, which shows that with complex numbers, a solution exists to every polynomial equation of degree one or higher. Complex numbers thus form an algebraically closed field, where any polynomial equation has a root.The solution in radicals (without trigonometric functions) of a general cubic equation contains the square roots of negative numbers when all three roots are real numbers, a situation that cannot be rectified by factoring aided by the rational root test if the cubic is irreducible (the so-called casus irreducibilis). This conundrum led Italian mathematician Gerolamo Cardano to conceive of complex numbers in around 1545,[11] though his understanding was rudimentary.A position vector may also be defined in terms of its magnitude and direction relative to the origin. These are emphasized in a complex number's polar form. Using the polar form of the complex number in calculations may lead to a more intuitive interpretation of mathematical results. Notably, the operations of addition and multiplication take on a very natural geometric character when complex numbers are viewed as position vectors: addition corresponds to vector addition while multiplication corresponds to multiplying their magnitudes and adding their arguments (i.e. the angles they make with the x axis). Viewed in this way the multiplication of a complex number by i corresponds to rotating the position vector counterclockwise by a quarter turn (90\u00b0) about the origin: (a+bi)i = ai+bi2 = -b+ai.A complex number can be viewed as a point or position vector in a two-dimensional Cartesian coordinate system called the complex plane or Argand diagram (see Pedoe 1988 and Solomentsev 2001), named after Jean-Robert Argand. The numbers are conventionally plotted using the real part as the horizontal component, and imaginary part as vertical (see Figure 1). These two values used to identify a given complex number are therefore called its Cartesian, rectangular, or algebraic form.A complex number can thus be identified with an ordered pair (Re(z),Im(z)) in the Cartesian plane, an identification sometimes known as the Cartesian form of z. In fact, a complex number can be defined as an ordered pair (a,b), but then rules for addition and multiplication must also be included as part of the definition (see below).[9] William Rowan Hamilton introduced this approach to define the complex number system.[10]A real number a can be regarded as a complex number a + 0i whose imaginary part is 0. A purely imaginary number bi is a complex number 0 + bi whose real part is zero. It is common to write a for a + 0i and bi for 0 + bi. Moreover, when the imaginary part is negative, it is common to write a \u2212 bi with b > 0 instead of a + (\u2212b)i, for example 3 \u2212 4i instead of 3 + (\u22124)i.The real number a is called the real part of the complex number a + bi; the real number b is called the imaginary part of a + bi. By this convention, the imaginary part does not include a factor of i: hence b, not bi, is the imaginary part.[7][8] The real part of a complex number z is denoted by Re(z) or \u211c(z); the imaginary part of a complex number z is denoted by Im(z) or \u2111(z). For example,A complex number is a number of the form a + bi, where a and b are real numbers and i is an indeterminate satisfying i2 = \u22121. For example, 2 + 3i is a complex number.[5]According to the fundamental theorem of algebra, all polynomial equations with real or complex coefficients in a single variable have a solution in complex numbers.has no real solution, since the square of a real number cannot be negative. Complex numbers provide a solution to this problem. The idea is to extend the real numbers with an indeterminate i (sometimes called the imaginary unit) that is taken to satisfy the relation i2 = \u22121, so that solutions to equations like the preceding one can be found. In this case the solutions are \u22121 + 3i and \u22121 \u2212 3i, as can be verified using the fact that i2 = \u22121:Complex numbers allow solutions to certain equations that have no solutions in real numbers. For example, the equationGeometrically, complex numbers extend the concept of the one-dimensional number line to the two-dimensional complex plane by using the horizontal axis for the real part and the vertical axis for the imaginary part. The complex number a + bi can be identified with the point (a,\u2009b) in the complex plane. A complex number whose real part is zero is said to be purely imaginary; the points for these numbers lie on the vertical axis of the complex plane. A complex number whose imaginary part is zero can be viewed as a real number; its point lies on the horizontal axis of the complex plane. Complex numbers can also be represented in polar form, which associates each complex number with its distance from the origin (its magnitude) and with a particular angle known as the argument of this complex number.Most importantly the complex numbers give rise to the fundamental theorem of algebra: every non-constant polynomial equation with complex coefficients has a complex solution. This property is true of the complex numbers, but not the reals. The 16th century Italian mathematician Gerolamo Cardano is credited with introducing complex numbers in his attempts to find solutions to cubic equations.[4]The complex number system can be defined as the algebraic extension of the ordinary real numbers by an imaginary number i.[3] This means that complex numbers can be added, subtracted, and multiplied, as polynomials in the variable i, with the rule i2 = \u22121 imposed. Furthermore, complex numbers can also be divided by nonzero complex numbers. Overall, the complex number system is a field.A complex number is a number that can be expressed in the form a + bi, where a and b are real numbers, and i is a solution of the equation x2 = \u22121, which is called an imaginary number because there is no real number that satisfies this equation. For the complex number a + bi, a is called the real part, and b is called the imaginary part. Despite the historical nomenclature \"imaginary\", complex numbers are regarded in the mathematical sciences as just as \"real\" as the real numbers, and are fundamental in many aspects of the scientific description of the natural world.[1][2]",
            "title": "Complex number",
            "url": "https://en.wikipedia.org/wiki/Complex_number"
        },
        {
            "desc_links": [
                "/wiki/Inflected_language",
                "/wiki/Grammatical_case",
                "/wiki/Germanic_strong_verb",
                "/wiki/Indo-European_languages",
                "/wiki/Latin",
                "/wiki/Greek_language",
                "/wiki/French_language",
                "/wiki/English_language",
                "/wiki/German_Standard_German",
                "/wiki/Austrian_Standard_German",
                "/wiki/Swiss_Standard_German",
                "/wiki/Pluricentric_language",
                "/wiki/German_dialects",
                "/wiki/Standard_German",
                "/wiki/Low_German",
                "/wiki/Plautdietsch_language",
                "/wiki/World_language",
                "/wiki/European_Union",
                "/wiki/French_language",
                "/wiki/Spanish_language_in_the_United_States",
                "/wiki/French_language_in_the_United_States#Language_study",
                "/wiki/American_Sign_Language",
                "/wiki/English_language",
                "/wiki/Russian_language",
                "/wiki/List_of_territorial_entities_where_German_is_an_official_language",
                "/wiki/United_Kingdom",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/File:De-Deutsch.ogg",
                "/wiki/West_Germanic_languages",
                "/wiki/Central_Europe",
                "/wiki/Germany",
                "/wiki/Austria",
                "/wiki/Switzerland",
                "/wiki/South_Tyrol",
                "/wiki/Italy",
                "/wiki/German-speaking_Community_of_Belgium",
                "/wiki/Liechtenstein",
                "/wiki/Luxembourg",
                "/wiki/Afrikaans_language",
                "/wiki/Dutch_language",
                "/wiki/English_language",
                "/wiki/Frisian_languages",
                "/wiki/Low_German",
                "/wiki/Luxembourgish_language",
                "/wiki/Yiddish",
                "/wiki/Germanic_languages"
            ],
            "links": [
                "/wiki/BBC_World_Service",
                "/wiki/Johann_Wolfgang_von_Goethe",
                "/wiki/Loanword",
                "/wiki/Martin_Luther",
                "/wiki/Gotthold_Ephraim_Lessing",
                "/wiki/Johann_Wolfgang_von_Goethe",
                "/wiki/Friedrich_Schiller",
                "/wiki/Heinrich_von_Kleist",
                "/wiki/E.T.A._Hoffmann",
                "/wiki/Bertolt_Brecht",
                "/wiki/Heinrich_Heine",
                "/wiki/Franz_Kafka",
                "/wiki/Nobel_Prize_in_literature",
                "/wiki/Theodor_Mommsen",
                "/wiki/Rudolf_Christoph_Eucken",
                "/wiki/Paul_von_Heyse",
                "/wiki/Gerhart_Hauptmann",
                "/wiki/Carl_Spitteler",
                "/wiki/Thomas_Mann",
                "/wiki/Nelly_Sachs",
                "/wiki/Hermann_Hesse",
                "/wiki/Heinrich_B%C3%B6ll",
                "/wiki/Elias_Canetti",
                "/wiki/G%C3%BCnter_Grass",
                "/wiki/Elfriede_Jelinek",
                "/wiki/Herta_M%C3%BCller",
                "/wiki/Middle_Ages",
                "/wiki/Walther_von_der_Vogelweide",
                "/wiki/Wolfram_von_Eschenbach",
                "/wiki/Nibelungenlied",
                "/wiki/Brothers_Grimm",
                "/wiki/Germanic_languages",
                "/wiki/Dental_fricative",
                "/wiki/Thou",
                "/wiki/Affricate",
                "/wiki/Glottal_stop",
                "/wiki/Moon",
                "/wiki/Hessen",
                "/wiki/Noun#Proper_nouns_and_common_nouns",
                "/wiki/Mecklenburg",
                "/wiki/Jan_Hofer",
                "/wiki/Das_Erste",
                "/wiki/Marietta_Slomka",
                "/wiki/ZDF",
                "/wiki/Secondary_stress",
                "/wiki/Captain_Bluebear",
                "/wiki/North_Rhine-Westphalia",
                "/wiki/Bavaria",
                "/wiki/Comma",
                "/wiki/Long_s",
                "/wiki/Long_s",
                "/wiki/Fraktur",
                "/wiki/Antiqua_(typeface_class)",
                "/wiki/Long_s",
                "/wiki/Lower_case",
                "/wiki/Blackletter",
                "/wiki/Typeface",
                "/wiki/Fraktur",
                "/wiki/Schwabacher",
                "/wiki/Penmanship",
                "/wiki/Kurrent",
                "/wiki/S%C3%BCtterlin",
                "/wiki/Sans-serif",
                "/wiki/Antiqua_(typeface_class)",
                "/wiki/Germanic_languages",
                "/wiki/Nazism",
                "/wiki/Aryan",
                "/wiki/World_War_II",
                "/wiki/Quotation_mark",
                "/wiki/Telephone_directory",
                "/wiki/Operating_systems",
                "/wiki/Microsoft_Windows",
                "/wiki/Alt_codes",
                "/wiki/Help:IPA/Standard_German",
                "/wiki/Proper_noun",
                "/wiki/Capital_%C3%9F",
                "/wiki/German_orthography_reform_of_1996",
                "/wiki/Vowel_length",
                "/wiki/Diphthong",
                "/wiki/Germanic_umlaut",
                "/wiki/German_orthography",
                "/wiki/Donaudampfschiffahrtselektrizit%C3%A4tenhauptbetriebswerkbauunterbeamtengesellschaft",
                "/wiki/Umlaut_(diacritic)",
                "/wiki/Scharfes_s",
                "/wiki/%C3%9F",
                "/wiki/Cognate",
                "/wiki/%C3%96sterreichisches_W%C3%B6rterbuch",
                "/wiki/%C3%96WB",
                "/wiki/Dictionary",
                "/wiki/Republic_of_Austria",
                "/wiki/Federal_Ministry_of_Education,_Arts_and_Culture",
                "/wiki/Duden",
                "/wiki/Austrian_German",
                "/wiki/Southern_Germany",
                "/wiki/Bavaria",
                "/wiki/Switzerland",
                "/wiki/German_spelling_reform_of_1996",
                "/wiki/Italy",
                "/wiki/South_Tyrol",
                "/wiki/Duden",
                "/wiki/Dictionary",
                "/wiki/Konrad_Duden",
                "/wiki/Loanword",
                "/wiki/Etymology",
                "/wiki/Pronunciation",
                "/wiki/Synonyms",
                "/wiki/Usage_dictionary",
                "/wiki/Deutsches_W%C3%B6rterbuch",
                "/wiki/Jacob_Grimm",
                "/wiki/Wilhelm_Grimm",
                "/wiki/Text_corpus",
                "/wiki/Synonym",
                "/wiki/Loanword",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Notker_Labeo",
                "/wiki/Joachim_Heinrich_Campe",
                "/wiki/Ersatz",
                "/wiki/Roman_Empire",
                "/wiki/Renaissance_humanism",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Latin_language",
                "/wiki/Greek_language",
                "/wiki/Italian_language",
                "/wiki/Joachim_Heinrich_Campe",
                "/wiki/Future",
                "/wiki/Passive_voice",
                "/wiki/Modality_(semiotics)",
                "/wiki/Perfect_(grammar)",
                "/wiki/Auxiliary_verb",
                "/wiki/Auxiliary_verb",
                "/wiki/Perfect_(grammar)",
                "/wiki/Poetic_meter",
                "/wiki/Figures_of_speech",
                "/wiki/Sentence_(linguistics)",
                "/wiki/Auxiliary_verb",
                "/wiki/V2_word_order",
                "/wiki/Topic%E2%80%93comment",
                "/wiki/V2_word_order",
                "/wiki/SOV_word_order",
                "/wiki/Clause",
                "/wiki/Finite_verb",
                "/wiki/Parenthetical_referencing",
                "/wiki/German_verbs",
                "/wiki/Finite_verb",
                "/wiki/Compound_(linguistics)",
                "/wiki/English_compounds",
                "/wiki/Rinderkennzeichnungs-_und_Rindfleischetikettierungs%C3%BCberwachungsaufgaben%C3%BCbertragungsgesetz",
                "/wiki/Luxembourgish_language",
                "/wiki/North_Frisian_language",
                "/wiki/Old_High_German",
                "/wiki/Indo-European_languages",
                "/wiki/Latin",
                "/wiki/Ancient_Greek",
                "/wiki/Sanskrit",
                "/wiki/Old_English",
                "/wiki/Icelandic_language",
                "/wiki/Russian_language",
                "/wiki/Article_(grammar)",
                "/wiki/Natural_language",
                "/wiki/German_nouns",
                "/wiki/Fusional_language",
                "/wiki/Inflection",
                "/wiki/Grammatical_gender",
                "/wiki/Austria",
                "/wiki/Vienna",
                "/wiki/Lower_Austria",
                "/wiki/Upper_Austria",
                "/wiki/Styria",
                "/wiki/Carinthia",
                "/wiki/Salzburg_(state)",
                "/wiki/Burgenland",
                "/wiki/Tyrol_(state)",
                "/wiki/Bavaria",
                "/wiki/Upper_Bavaria",
                "/wiki/Lower_Bavaria",
                "/wiki/Upper_Palatinate",
                "/wiki/South_Tyrol",
                "/wiki/Saxony",
                "/wiki/Vogtl%C3%A4ndisch_dialect",
                "/wiki/Samnaun",
                "/wiki/Vienna",
                "/wiki/Munich",
                "/wiki/Switzerland",
                "/wiki/High_Alemannic_German",
                "/wiki/Swiss_Plateau",
                "/wiki/Highest_Alemannic_German",
                "/wiki/Low_Alemannic_German",
                "/wiki/Basel",
                "/wiki/Swabian_German",
                "/wiki/Swabia_(Bavaria)",
                "/wiki/Vorarlberg",
                "/wiki/Alsace",
                "/wiki/Liechtenstein",
                "/wiki/Tyrol_(state)",
                "/wiki/Reutte_District",
                "/wiki/Alsatian_dialect",
                "/wiki/Stuttgart",
                "/wiki/Z%C3%BCrich",
                "/wiki/Upper_German",
                "/wiki/Alemannic_German",
                "/wiki/Bavarian_language",
                "/wiki/Alsace",
                "/wiki/Alsatian_dialect",
                "/wiki/Low_Alemannic_German",
                "/wiki/Karlsruhe",
                "/wiki/Heilbronn",
                "/wiki/Franconia",
                "/wiki/Saxony",
                "/wiki/Vogtland",
                "/wiki/Bavaria",
                "/wiki/Upper_Franconia",
                "/wiki/Middle_Franconia",
                "/wiki/Lower_Franconia",
                "/wiki/South_Thuringia",
                "/wiki/Thuringia",
                "/wiki/Heilbronn-Franken",
                "/wiki/Tauber_Franconia",
                "/wiki/Baden-W%C3%BCrttemberg",
                "/wiki/Nuremberg",
                "/wiki/W%C3%BCrzburg",
                "/wiki/High_Franconian_German",
                "/wiki/East_Franconian_German",
                "/wiki/South_Franconian_German",
                "/wiki/Thuringian_dialect",
                "/wiki/Upper_Saxon_German",
                "/wiki/Erzgebirgisch",
                "/wiki/Lausitzisch-neum%C3%A4rkisch_dialects",
                "/wiki/Silesia",
                "/wiki/Silesian_German",
                "/wiki/East_Prussia",
                "/wiki/High_Prussian_dialect",
                "/wiki/Berlin",
                "/wiki/Leipzig",
                "/wiki/Transylvanian_Saxon_dialect",
                "/wiki/Transylvania",
                "/wiki/Cologne",
                "/wiki/Frankfurt",
                "/wiki/Central_Franconian_dialects",
                "/wiki/Ripuarian_language",
                "/wiki/Moselle_Franconian_dialects",
                "/wiki/Rhine_Franconian_dialects",
                "/wiki/Hessian_dialects",
                "/wiki/Palatine_German_language",
                "/wiki/Central_German",
                "/wiki/Aachen",
                "/wiki/G%C3%B6rlitz",
                "/wiki/Franconian_languages",
                "/wiki/West_Central_German",
                "/wiki/East_Central_German",
                "/wiki/Central_German",
                "/wiki/High_Franconian_German",
                "/wiki/Upper_German_language",
                "/wiki/Ashkenazi_Jew",
                "/wiki/Yiddish",
                "/wiki/Hebrew_alphabet",
                "/wiki/Low_Franconian_languages",
                "/wiki/Dutch_language",
                "/wiki/Low_German",
                "/wiki/Netherlands",
                "/wiki/Belgium",
                "/wiki/North_Rhine-Westphalia",
                "/wiki/Lower_Rhine",
                "/wiki/Meuse-Rhenish",
                "/wiki/South_Guelderish",
                "/wiki/Limburgish_language",
                "/wiki/Rhine",
                "/wiki/Low_Bergish",
                "/wiki/Ripuarian_language",
                "/wiki/Central_Franconian_dialects",
                "/wiki/East_Bergish",
                "/wiki/Westphalian_language",
                "/wiki/D%C3%BCsseldorf",
                "/wiki/Duisburg",
                "/wiki/Education",
                "/wiki/Standard_German",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Missingsch",
                "/wiki/Low_Franconian",
                "/wiki/World_War_II",
                "/wiki/Hamburg",
                "/wiki/Dortmund",
                "/wiki/Middle_Low_German",
                "/wiki/Lingua_franca",
                "/wiki/Hanseatic_League",
                "/wiki/Luther_Bible",
                "/wiki/Early_New_High_German",
                "/wiki/Central_German",
                "/wiki/Upper_German",
                "/wiki/Low_German",
                "/wiki/Asia",
                "/wiki/Americas",
                "/wiki/High_German_languages",
                "/wiki/Low_German",
                "/wiki/Lower_Saxony",
                "/wiki/West_Germanic_languages",
                "/wiki/Germanic_languages",
                "/wiki/Language_family",
                "/wiki/Indo-European_language_family",
                "/wiki/Lexicon",
                "/wiki/Phonology",
                "/wiki/Syntax",
                "/wiki/Language",
                "/wiki/Mutual_intelligibility",
                "/wiki/Ethnologue",
                "/wiki/Switzerland",
                "/wiki/Diglossia",
                "/wiki/Swiss_Standard_German",
                "/wiki/Austrian_Standard_German",
                "/wiki/Linguistics",
                "/wiki/Dialect",
                "/wiki/Variety_(linguistics)",
                "/wiki/Standard_German",
                "/wiki/Pluricentric_language",
                "/wiki/Vocabulary",
                "/wiki/Pronunciation",
                "/wiki/Grammar",
                "/wiki/Orthography",
                "/wiki/Dialects",
                "/wiki/Pluricentric_language",
                "/wiki/Written_language",
                "/wiki/Northern_Germany",
                "/wiki/French_language",
                "/wiki/Spanish_language",
                "/wiki/English_language",
                "/wiki/Spanish_language",
                "/wiki/Russia",
                "/wiki/Eastern_Europe",
                "/wiki/Northern_Europe",
                "/wiki/Czech_Republic",
                "/wiki/Croatia",
                "/wiki/Denmark",
                "/wiki/Netherlands",
                "/wiki/Slovakia",
                "/wiki/Hungary",
                "/wiki/Slovenia",
                "/wiki/Sweden",
                "/wiki/Poland",
                "/wiki/Lingua_franca",
                "/wiki/German-based_creole_languages",
                "/wiki/Unserdeutsch",
                "/wiki/German_New_Guinea",
                "/wiki/Micronesia",
                "/wiki/Queensland",
                "/wiki/Western_Australia",
                "/wiki/Puhoi",
                "/wiki/Nelson,_New_Zealand",
                "/wiki/Gore,_New_Zealand",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Australia",
                "/wiki/South_Australia",
                "/wiki/Silesia",
                "/wiki/Australian_English",
                "/wiki/Barossa_German",
                "/wiki/Barossa_Valley",
                "/wiki/Adelaide",
                "/wiki/World_War_I",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Rio_Grande_do_Sul",
                "/wiki/Riograndenser_Hunsr%C3%BCckisch",
                "/wiki/Santa_Catarina_(state)",
                "/wiki/Paran%C3%A1_(state)",
                "/wiki/S%C3%A3o_Paulo",
                "/wiki/Esp%C3%ADrito_Santo",
                "/wiki/Argentina",
                "/wiki/Chile",
                "/wiki/Paraguay",
                "/wiki/Venezuela",
                "/wiki/Peru",
                "/wiki/Bolivia",
                "/wiki/German_Mexican",
                "/wiki/Mexico_City",
                "/wiki/Puebla",
                "/wiki/Mazatl%C3%A1n",
                "/wiki/Tapachula",
                "/wiki/Ecatepec_de_Morelos",
                "/wiki/Chihuahua_(state)",
                "/wiki/Durango",
                "/wiki/Zacatecas",
                "/wiki/Canada",
                "/wiki/German_Canadians",
                "/wiki/British_Columbia",
                "/wiki/Ontario",
                "/wiki/Kitchener,_Ontario",
                "/wiki/Montreal",
                "/wiki/Toronto",
                "/wiki/Vancouver",
                "/wiki/Second_World_War",
                "/wiki/German-Canadian",
                "/wiki/French_language",
                "/wiki/English_language",
                "/wiki/Hutterite_German",
                "/wiki/Austro-Bavarian",
                "/wiki/Washington_(state)",
                "/wiki/Montana",
                "/wiki/North_Dakota",
                "/wiki/South_Dakota",
                "/wiki/Minnesota",
                "/wiki/Alberta",
                "/wiki/Saskatchewan",
                "/wiki/Manitoba",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Carinthia_(state)",
                "/wiki/Texas_German",
                "/wiki/Adelsverein",
                "/wiki/Amana_Colonies",
                "/wiki/Amana_German",
                "/wiki/Plautdietsch",
                "/wiki/Minority_language",
                "/wiki/Mennonite",
                "/wiki/Pennsylvania_German_language",
                "/wiki/West_Central_German",
                "/wiki/Amish",
                "/wiki/Palatine_German_language",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/United_States_Brewers%27_Association",
                "/wiki/World_War_I",
                "/wiki/Pennsylvania",
                "/wiki/Amish",
                "/wiki/Hutterites",
                "/wiki/Dunkard_Township,_Greene_County,_Pennsylvania",
                "/wiki/Mennonites",
                "/wiki/Hutterite_German",
                "/wiki/West_Central_German",
                "/wiki/Pennsylvania_German_language",
                "/wiki/Kansas",
                "/wiki/Volga_German",
                "/wiki/History_of_Germans_in_Russia_and_the_Soviet_Union",
                "/wiki/Baltic_Germans",
                "/wiki/South_Dakota",
                "/wiki/Montana",
                "/wiki/Texas",
                "/wiki/Texas_German",
                "/wiki/Wisconsin",
                "/wiki/Indiana",
                "/wiki/Oregon",
                "/wiki/Oklahoma",
                "/wiki/Ohio",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Pietists",
                "/wiki/Iowa",
                "/wiki/Amana_Colonies",
                "/wiki/St._Louis,_Missouri",
                "/wiki/Chicago",
                "/wiki/New_York_City",
                "/wiki/Milwaukee",
                "/wiki/Pittsburgh",
                "/wiki/Cincinnati",
                "/wiki/North_Dakota",
                "/wiki/South_Dakota",
                "/wiki/New_Ulm,_Minnesota",
                "/wiki/Minnesota",
                "/wiki/Bismarck,_North_Dakota",
                "/wiki/Munich,_North_Dakota",
                "/wiki/Karlsruhe,_North_Dakota",
                "/wiki/Strasburg,_North_Dakota",
                "/wiki/New_Braunfels",
                "/wiki/Fredericksburg,_Texas",
                "/wiki/Muenster,_Texas",
                "/wiki/Kiel,_Wisconsin",
                "/wiki/Berlin,_Wisconsin",
                "/wiki/Germantown,_Wisconsin",
                "/wiki/South_Africa",
                "/wiki/Low_German",
                "/wiki/Wartburg,_KwaZulu-Natal",
                "/wiki/Kroondal",
                "/wiki/North_West_(South_African_province)",
                "/wiki/Pan_South_African_Language_Board",
                "/wiki/Deutsche_Schule_Pretoria",
                "/wiki/Afrikaans",
                "/wiki/South_Africa",
                "/wiki/Apartheid",
                "/wiki/Evangelical_Lutheran_Church_in_Namibia_(GELK)",
                "/wiki/Deutsche_H%C3%B6here_Privatschule_Windhoek",
                "/wiki/Namibian_Broadcasting_Corporation",
                "/wiki/EES_(rapper)",
                "/wiki/Allgemeine_Zeitung_(Namibia)",
                "/wiki/German_South-West_Africa",
                "/wiki/German_Empire",
                "/wiki/Pidgin",
                "/wiki/Namibian_Black_German",
                "/wiki/France",
                "/wiki/High_German",
                "/wiki/Alsatian_dialect",
                "/wiki/Moselle_Franconian",
                "/wiki/Regional_language",
                "/wiki/European_Charter_for_Regional_and_Minority_Languages",
                "/wiki/Netherlands",
                "/wiki/Limburgish_language",
                "/wiki/Frisian_languages",
                "/wiki/Low_German",
                "/wiki/Flight_and_expulsion_of_Germans",
                "/wiki/Organised_persecution_of_ethnic_Germans",
                "/wiki/World_War",
                "/wiki/German_Sprachraum",
                "/wiki/German_diaspora",
                "/wiki/Foreign_language",
                "/wiki/Geographical_distribution_of_German_speakers",
                "/wiki/Alemannic_German",
                "/wiki/Alsatian_dialect",
                "/wiki/Low_German",
                "/wiki/First_language",
                "/wiki/Second_language",
                "/wiki/Foreign_language",
                "/wiki/German_orthography_reform_of_1996",
                "/wiki/Standardization",
                "/wiki/Stage_(theatre)",
                "/wiki/B%C3%BChnendeutsch",
                "/wiki/Deutsches_W%C3%B6rterbuch",
                "/wiki/Dictionary",
                "/wiki/Brothers_Grimm",
                "/wiki/Duden_Handbook",
                "/wiki/Banat",
                "/wiki/Transylvania",
                "/wiki/Timi%C8%99oara",
                "/wiki/Sibiu",
                "/wiki/Bra%C8%99ov",
                "/wiki/Prague",
                "/wiki/Budapest",
                "/wiki/Buda",
                "/wiki/Germanization",
                "/wiki/Pozsony",
                "/wiki/Bratislava",
                "/wiki/Zagreb",
                "/wiki/Ljubljana",
                "/wiki/Habsburg_Empire",
                "/wiki/Merchant",
                "/wiki/New_Testament",
                "/wiki/Old_Testament",
                "/wiki/Saxony",
                "/wiki/Gloss_(annotation)",
                "/wiki/Chancery_(medieval_office)",
                "/wiki/Holy_Roman_Emperor",
                "/wiki/Maximilian_I,_Holy_Roman_Emperor",
                "/wiki/Electorate_of_Saxony",
                "/wiki/Duchy_of_Saxe-Wittenberg",
                "/wiki/Early_New_High_German",
                "/wiki/Philology",
                "/wiki/Wilhelm_Scherer",
                "/wiki/Thirty_Years%27_War",
                "/wiki/List_of_states_in_the_Holy_Roman_Empire",
                "/wiki/Holy_Roman_Empire",
                "/wiki/Principality",
                "/wiki/German_dialects",
                "/wiki/Printing_press",
                "/wiki/Luther_Bible",
                "/wiki/Europe",
                "/wiki/Black_Death",
                "/wiki/Nibelungenlied",
                "/wiki/Epic_poetry",
                "/wiki/Dragon",
                "/wiki/Sigurd",
                "/wiki/Iwein",
                "/wiki/King_Arthur",
                "/wiki/Hartmann_von_Aue",
                "/wiki/Lyric_poetry",
                "/wiki/Parzival",
                "/wiki/Tristan",
                "/wiki/Sachsenspiegel",
                "/wiki/Middle_Low_German",
                "/wiki/Vowel_breaking",
                "/wiki/Schwa",
                "/wiki/Middle_High_German",
                "/wiki/Elbe",
                "/wiki/Saale",
                "/wiki/Slavs",
                "/wiki/Ostsiedlung",
                "/wiki/Hohenstaufen",
                "/wiki/Swabia",
                "/wiki/Oral_literature",
                "/wiki/Phonetics",
                "/wiki/Phonology",
                "/wiki/Morphology_(linguistics)",
                "/wiki/Syntax",
                "/wiki/Standard_language",
                "/wiki/Dialect",
                "/wiki/Monastery",
                "/wiki/Scriptorium",
                "/wiki/Old_High_German",
                "/wiki/Elder_Futhark",
                "/wiki/Pforzen_buckle",
                "/wiki/Abrogans",
                "/wiki/Glossary",
                "/wiki/Latin",
                "/wiki/Muspilli",
                "/wiki/Merseburg_Incantations",
                "/wiki/Hildebrandslied",
                "/wiki/Georgslied",
                "/wiki/Ludwigslied",
                "/wiki/Bavarian_dialects",
                "/wiki/Last_Judgment",
                "/wiki/Paganism",
                "/wiki/Epic_poetry",
                "/wiki/Old_Saxon",
                "/wiki/Alemanni",
                "/wiki/Bavarian_dialects",
                "/wiki/Thuringian_dialect",
                "/wiki/Irminones",
                "/wiki/Germany",
                "/wiki/Austria",
                "/wiki/History_of_German",
                "/wiki/High_German_consonant_shift",
                "/wiki/Migration_period",
                "/wiki/Old_High_German",
                "/wiki/Old_Saxon",
                "/wiki/Sound_change",
                "/wiki/Voice_(phonetics)",
                "/wiki/Stop_consonant",
                "/wiki/Germany",
                "/wiki/Denmark",
                "/wiki/North_Frisian_language",
                "/wiki/Schleswig-Holstein",
                "/wiki/Saterland_Frisian_language",
                "/wiki/Lower_Saxony",
                "/wiki/West_Frisian_language",
                "/wiki/Netherlands",
                "/wiki/English_language",
                "/wiki/Scots_language",
                "/wiki/Anglo-Frisian_languages",
                "/wiki/Standard_German",
                "/wiki/Thuringian_dialect",
                "/wiki/Upper_Saxon_German",
                "/wiki/Central_German",
                "/wiki/High_German_languages",
                "/wiki/Luxembourgish_language",
                "/wiki/Central_Franconian_dialects",
                "/wiki/Yiddish",
                "/wiki/Upper_German",
                "/wiki/List_of_territorial_entities_where_German_is_an_official_language",
                "/wiki/Swiss_German",
                "/wiki/Alemannic_German",
                "/wiki/France",
                "/wiki/Regions_of_France",
                "/wiki/Grand_Est",
                "/wiki/Alsatian_dialect",
                "/wiki/High_Franconian_German",
                "/wiki/Lorraine_Franconian",
                "/wiki/Benrath_line",
                "/wiki/Uerdingen_line",
                "/wiki/D%C3%BCsseldorf",
                "/wiki/D%C3%BCsseldorf-Benrath",
                "/wiki/Krefeld",
                "/wiki/Uerdingen",
                "/wiki/High_German_consonant_shift",
                "/wiki/High_German_languages",
                "/wiki/Low_German",
                "/wiki/Low_Franconian_languages",
                "/wiki/Irminones",
                "/wiki/Ingvaeonic_languages",
                "/wiki/Istvaeones",
                "/wiki/Standard_German",
                "/wiki/West_Germanic_languages",
                "/wiki/Germanic_languages",
                "/wiki/Indo-European_languages",
                "/wiki/North_Germanic_languages",
                "/wiki/East_Germanic_languages",
                "/wiki/West_Germanic_languages",
                "/wiki/Danish_language",
                "/wiki/Swedish_language",
                "/wiki/Norwegian_language",
                "/wiki/Faroese_language",
                "/wiki/Icelandic_language",
                "/wiki/Old_Norse",
                "/wiki/Gothic_language",
                "/wiki/English_language",
                "/wiki/Dutch_language",
                "/wiki/Yiddish",
                "/wiki/Afrikaans",
                "/wiki/Inflected_language",
                "/wiki/Grammatical_case",
                "/wiki/Germanic_strong_verb",
                "/wiki/Indo-European_languages",
                "/wiki/Latin",
                "/wiki/Greek_language",
                "/wiki/French_language",
                "/wiki/English_language",
                "/wiki/German_Standard_German",
                "/wiki/Austrian_Standard_German",
                "/wiki/Swiss_Standard_German",
                "/wiki/Pluricentric_language",
                "/wiki/German_dialects",
                "/wiki/Standard_German",
                "/wiki/Low_German",
                "/wiki/Plautdietsch_language",
                "/wiki/World_language",
                "/wiki/European_Union",
                "/wiki/French_language",
                "/wiki/Spanish_language_in_the_United_States",
                "/wiki/French_language_in_the_United_States#Language_study",
                "/wiki/American_Sign_Language",
                "/wiki/English_language",
                "/wiki/Russian_language",
                "/wiki/List_of_territorial_entities_where_German_is_an_official_language",
                "/wiki/United_Kingdom"
            ],
            "text": "The German state broadcaster Deutsche Welle is the equivalent of the British BBC World Service and provides radio and television broadcasts in German and 30 other languages across the globe.[95] Its German language services are tailored for German language learners by being spoken at slow speed. Deutsche Welle also provides an e-learning website to learn German.The Dortmund-based Verein Deutsche Sprache (VDS), which was founded in 1997, supports the German language and is the largest language association of citizens in the world. The VDS has more than thirty-five thousand members in over seventy countries. Its founder, statistics professor Dr. Walter Kr\u00e4mer, has remained chairperson of the association from its beginnings.[94]The government-backed Goethe-Institut[93] (named after the famous German author Johann Wolfgang von Goethe) aims to enhance the knowledge of German culture and language within Europe and the rest of the world. This is done by holding exhibitions and conferences with German-related themes, and providing training and guidance in the learning and use of the German language. For example, the Goethe-Institut teaches the Goethe-Zertifikat German language qualification.The use and learning of the German language are promoted by a number of organisations.English has taken many loanwords from German, often without any change of spelling (aside from, often, the elimination of umlauts and not capitalizing nouns):Reformer and theologian Martin Luther, who was the first to translate the Bible into German, is widely credited for having set the basis for the modern \"High German\" language. Among the most well known poets and authors in German are Lessing, Goethe, Schiller, Kleist, Hoffmann, Brecht, Heine, and Kafka. Thirteen German-speaking people have won the Nobel Prize in literature: Theodor Mommsen, Rudolf Christoph Eucken, Paul von Heyse, Gerhart Hauptmann, Carl Spitteler, Thomas Mann, Nelly Sachs, Hermann Hesse, Heinrich B\u00f6ll, Elias Canetti, G\u00fcnter Grass, Elfriede Jelinek and Herta M\u00fcller.The German language is used in German literature and can be traced back to the Middle Ages, with the most notable authors of the period being Walther von der Vogelweide and Wolfram von Eschenbach. The Nibelungenlied, whose author remains unknown, is also an important work of the epoch. The fairy tales collections collected and published by Jacob and Wilhelm Grimm in the 19th century became famous throughout the world.Likewise, the gh in Germanic English words, pronounced in several different ways in modern English (as an f, or not at all), can often be linked to German ch: \"to laugh\" \u2192 lachen, \"through\" \u2192 durch, \"high\" \u2192 hoch, \"naught\" \u2192 nichts, \"light\" \u2192 leicht or Licht, \"sight\" \u2192 Sicht, \"daughter\" \u2192 Tochter, \"neighbour\" \u2192 Nachbar.German does not have any dental fricatives (as English th). The th sounds, which the English language still has, disappeared on the continent in German with the consonant shifts between the 8th and the 10th centuries.[91] It is sometimes possible to find parallels between English and German by replacing the English th with d in German: \"Thank\" \u2192 in German Dank, \"this\" and \"that\" \u2192 dies and das, \"thou\" (old 2nd person singular pronoun) \u2192 du, \"think\" \u2192 denken, \"thirsty\" \u2192 durstig and many other examples.With approximately 25 phonemes, the German consonant system exhibits an average number of consonants in comparison with other languages. One of the more noteworthy ones is the unusual affricate /p\u0361f/. The consonant inventory of the standard language is shown below.In most varieties of standard German, syllables that begin with a vowel are preceded by a glottal stop [\u0294].Additionally, the digraph ie generally represents the phoneme /i\u02d0/, which is not a diphthong. In many varieties, an /r/ at the end of a syllable is vocalised. However, a sequence of a vowel followed by such a vocalised /r/ is not a phonemic diphthong: B\u00e4r [b\u025b\u02d0\u0250\u032f] \"bear\", er [e\u02d0\u0250\u032f] \"he\", wir [vi\u02d0\u0250\u032f] \"we\", Tor [to\u02d0\u0250\u032f] \"gate\", kurz [k\u028a\u0250\u032fts] \"short\", W\u00f6rter [v\u0153\u0250\u032ft\u0250] \"words\".German vowels can form the following digraphs (in writing) and diphthongs (in pronunciation); note that the pronunciation of some of them (ei, \u00e4u, eu) is very different from what one would expect when considering the component letters:Both of these rules have exceptions (e.g. hat [hat] \"has\" is short despite the first rule; Mond [mo\u02d0nt], \"moon\" is long despite the second rule). For an i that is neither in the combination ie (making it long) nor followed by a double consonant or cluster (making it short), there is no general rule. In some cases, there are regional differences: In central Germany (Hessen), the o in the proper name \"Hoffmann\" is pronounced long, whereas most other Germans would pronounce it short; the same applies to the e in the geographical name \"Mecklenburg\" for people in that region. The word St\u00e4dte \"cities\", is pronounced with a short vowel [\u02c8\u0283t\u025bt\u0259] by some (Jan Hofer, ARD Television) and with a long vowel [\u02c8\u0283t\u025b\u02d0t\u0259] by others (Marietta Slomka, ZDF Television). Finally, a vowel followed by ch can be short (Fach [fax] \"compartment\", K\u00fcche [\u02c8k\u028f\u00e7\u0259] \"kitchen\") or long (Suche [\u02c8zu\u02d0x\u0259] \"search\", B\u00fccher [\u02c8by\u02d0\u00e7\u0250] \"books\") almost at random. Thus, Lache is homographous between [la\u02d0x\u0259] Lache \"puddle\" and [lax\u0259] Lache \"manner of laughing\" (colloquial) or lache! \"laugh!\" (imperative).Whether any particular vowel letter represents the long or short phoneme is not completely predictable, although the following regularities exist:In many varieties of standard German, an unstressed /\u025br/ is not pronounced [\u0259r], but vocalised to [\u0250].Short /\u025b/ is realized as [\u025b] in stressed syllables (including secondary stress), but as [\u0259] in unstressed syllables. Note that stressed short /\u025b/ can be spelled either with e or with \u00e4 (for instance, h\u00e4tte \"would have\" and Kette \"chain\" rhyme). In general, the short vowels are open and the long vowels are close. The one exception is the open /\u025b\u02d0/ sound of long \u00c4; in some varieties of standard German, /\u025b\u02d0/ and /e\u02d0/ have merged into [e\u02d0], removing this anomaly. In that case, pairs like B\u00e4ren/Beeren 'bears/berries' or \u00c4hre/Ehre 'spike (of wheat)/honour' become homophonous (see: Captain Bluebear).In German, vowels (excluding diphthongs; see below) are either short or long, as follows:Thus F\u00fc\u00dfe, pa\u00dft, and da\u00df. Currently only the first rule is in effect, thus F\u00fc\u00dfe, passt, and dass. The word Fu\u00df 'foot' has the letter \u00df because it contains a long vowel, even though that letter occurs at the end of a syllable. The logic of this change is that an '\u00df' is a single letter whereas 'ss' obviously are two letters, so the same distinction applies as for instance between the words den and denn.The most noticeable change was probably in the use of the letter \u00df, called scharfes s (Sharp S) or ess-zett (pronounced ess-tsett). Traditionally, this letter was used in three situations:The orthography reform of 1996 led to public controversy and considerable dispute. The states (Bundesl\u00e4nder) of North Rhine-Westphalia and Bavaria would not accept it. The dispute landed at one point in the highest court, which made a short issue of it, claiming that the states had to decide for themselves and that only in schools could the reform be made the official rule\u00a0\u2013 everybody else could continue writing as they had learned it. After 10 years, without any intervention by the federal parliament, a major revision was installed in 2006, just in time for the coming school year. In 2007, some traditional spellings were finally invalidated, whereas in 2008, on the other hand, many of the old comma rules were again put in force.A proper use of the long s, (langes s), \u017f, is essential for writing German text in Fraktur typefaces. Many Antiqua typefaces include the long s also. A specific set of rules applies for the use of long s in German text, but nowadays it is rarely used in Antiqua typesetting. Any lower case \"s\" at the beginning of a syllable would be a long s, as opposed to a terminal s or short s (the more common variation of the letter s), which marks the end of a syllable; for example, in differentiating between the words Wach\u017ftube (guard-house) and Wachstube (tube of polish/wax). One can easily decide which \"s\" to use by appropriate hyphenation, (Wach-\u017ftube vs. Wachs-tube). The long s only appears in lower case.The Fraktur script however remains present in everyday life in pub signs, beer brands and other forms of advertisement, where it is used to convey a certain rusticality and antiquity.Until the early 20th century, German was mostly printed in blackletter typefaces (mostly in Fraktur, but also in Schwabacher) and written in corresponding handwriting (for example Kurrent and S\u00fctterlin). These variants of the Latin alphabet are very different from the serif or sans-serif Antiqua typefaces used today, and the handwritten forms in particular are difficult for the untrained to read. The printed forms, however, were claimed by some to be more readable when used for Germanic languages.[87] (Often, foreign names in a text were printed in an Antiqua typeface even though the rest of the text was in Fraktur.) The Nazis initially promoted Fraktur and Schwabacher because they were considered Aryan, but they abolished them in 1941, claiming that these letters were Jewish.[88] It is also believed that the Nazi r\u00e9gime had banned this script as they realized that Fraktur would inhibit communication in the territories occupied during World War II.[89]Written German also typically uses an alternative opening inverted comma (quotation mark) as in \u201eGuten Morgen!\u201c.There is no general agreement on where letters with umlauts occur in the sorting sequence. Telephone directories treat them by replacing them with the base vowel followed by an e. Some dictionaries sort each umlauted vowel as a separate letter after the base vowel, but more commonly words with umlauts are ordered immediately after the same word without umlauts. As an example in a telephone book \u00c4rzte occurs after Adressenverlage but before Anlagenbauer (because \u00c4 is replaced by Ae). In a dictionary \u00c4rzte comes after Arzt, but in some dictionaries \u00c4rzte and all other words starting with \u00c4 may occur after all words starting with A. In some older dictionaries or indexes, initial Sch and St are treated as separate letters and are listed as separate entries after S, but they are usually treated as S+C+H and S+T.Umlaut vowels (\u00e4, \u00f6, \u00fc) are commonly transcribed with ae, oe, and ue if the umlauts are not available on the keyboard or other medium used. In the same manner \u00df can be transcribed as ss. Some operating systems use key sequences to extend the set of possible characters to include, amongst other things, umlauts; in Microsoft Windows this is done using Alt codes. German readers understand these transcriptions (although they appear unusual), but they are avoided if the regular umlauts are available because they are a makeshift, not proper spelling. (In Westphalia and Schleswig-Holstein, city and family names exist where the extra e has a vowel lengthening effect, e.g. Raesfeld [\u02c8ra\u02d0sf\u025blt], Coesfeld [\u02c8ko\u02d0sf\u025blt] and Itzehoe [\u026ats\u0259\u02c8ho\u02d0], but this use of the letter e after a/o/u does not occur in the present-day spelling of words other than proper nouns.)Since there is no traditional capital form of \u00df, it was replaced by SS when capitalization was required. For example, Ma\u00dfband (tape measure) became MASSBAND in capitals. An exception was the use of \u00df in legal documents and forms when capitalizing names. To avoid confusion with similar names, lower case \u00df was maintained (so, \"KRE\u00dfLEIN\" instead of \"KRESSLEIN\"). Capital \u00df (\u1e9e) was ultimately adopted into German orthography in 2017, ending a long orthographic debate.[86]Before the German orthography reform of 1996, \u00df replaced ss after long vowels and diphthongs and before consonants, word-, or partial-word-endings. In reformed spelling, \u00df replaces ss only after long vowels and diphthongs.Written texts in German are easily recognisable as such by distinguishing features such as umlauts and certain orthographical features\u00a0\u2013 German is the only major language that capitalizes all nouns, a relic of a widespread practice in Northern Europe in the early modern era (including English for a while, in the 1700s)\u00a0\u2013 and the frequent occurrence of long compounds. The longest German word that has been published is Donaudampfschiffahrtselektrizit\u00e4tenhauptbetriebswerkbauunterbeamtengesellschaft made of 79 characters. Because legibility and convenience set certain boundaries, compounds consisting of more than three or four nouns are almost exclusively found in humorous contexts. (In contrast, although English can also string nouns together, it usually separates the nouns with spaces. For example, \"toilet bowl cleaner\".)German is written in the Latin alphabet. In addition to the 26 standard letters, German has three vowels with Umlaut, namely \u00e4, \u00f6 and \u00fc, as well as the eszett or scharfes s (sharp s): \u00df. In Switzerland and Liechtenstein, ss is used instead of \u00df. Since \u00df can never occur at the beginning of a word, it has no traditional uppercase form.This is a selection of cognates in both English and German. Instead of the usual infinitive ending -en German verbs are indicated by a hyphen \"-\" after their stems. Words that are written with capital letters in German are nouns.The \u00d6sterreichisches W\u00f6rterbuch (\"Austrian Dictionary\"), abbreviated \u00d6WB, is the official dictionary of the German language in the Republic of Austria. It is edited by a group of linguists under the authority of the Austrian Federal Ministry of Education, Arts and Culture (German: Bundesministerium f\u00fcr Unterricht, Kunst und Kultur). It is the Austrian counterpart to the German Duden and contains a number of terms unique to Austrian German or more frequently used or differently pronounced there.[85] A considerable amount of this \"Austrian\" vocabulary is also common in Southern Germany, especially Bavaria, and some of it is used in Switzerland as well. The most recent edition is the 42nd from 2012. Since the 39th edition from 2001 the orthography of the \u00d6WB was adjusted to the German spelling reform of 1996. The dictionary is also officially used in the Italian province of South Tyrol.The Duden is the de facto official dictionary of the German language, first published by Konrad Duden in 1880. The Duden is updated regularly, with new editions appearing every four or five years. As of August 2013[update] it is in its 26th edition and in 12 volumes, each covering different aspects such as loanwords, etymology, pronunciation, synonyms, and so forth.\nThe first of these volumes, Die deutsche Rechtschreibung (German Orthography), has long been the prescriptive source for the spelling of German. The Duden has become the bible of the German language, being the definitive set of rules regarding grammar, spelling and usage of German.[84]The size of the vocabulary of German is difficult to estimate. The Deutsches W\u00f6rterbuch (The German Dictionary) initiated by Jacob and Wilhelm Grimm already contained over 330,000 headwords in its first edition. The modern German scientific vocabulary is estimated at nine million words and word groups (based on the analysis of 35 million sentences of a corpus in Leipzig, which as of July 2003 included 500 million words in total).[83]As in English, there are many pairs of synonyms due to the enrichment of the Germanic vocabulary with loanwords from Latin and Latinized Greek. These words often have different connotations from their Germanic counterparts and are usually perceived as more scholarly.At the same time, the effectiveness of the German language in forming equivalents for foreign words from its inherited Germanic stem repertory is great.[citation needed] Thus, Notker Labeo was able to translate Aristotelian treatises in pure (Old High) German in the decades after the year 1000. The tradition of loan translation was revitalized in the 18th century, with linguists like Joachim Heinrich Campe, who introduced close to 300 words that are still used in modern German. Even today, there are movements that try to promote the Ersatz (substitution) of foreign words deemed unnecessary with German alternatives.[81] It is claimed that this would also help in spreading modern or scientific notions among the less educated and as well democratise public life.Latin words were already imported into the predecessor of the German language during the Roman Empire and underwent all the characteristic phonetic changes in German. Their origin is thus no longer recognizable for most speakers (e.g. Pforte, Tafel, Mauer, K\u00e4se, K\u00f6ln from Latin porta, tabula, murus, caseus, Colonia). Borrowing from Latin continued after the fall of the Roman Empire during Christianization, mediated by the church and monasteries. Another important influx of Latin words can be observed during Renaissance humanism. In a scholarly context, the borrowings from Latin have continued until today, in the last few decades often indirectly through borrowings from English. During the 15th to 17th centuries, the influence of Italian was great, leading to many Italian loanwords in the fields of architecture, finance, and music. The influence of the French language in the 17th to 19th centuries resulted in an even greater import of French words. The English influence was already present in the 19th century, but it did not become dominant until the second half of the 20th century.Most German vocabulary is derived from the Germanic branch of the European language family.[citation needed] However, there is a significant amount of loanwords from other languages, in particular from Latin, Greek, Italian, French[78] and most recently English.[79] In the early 19th century, Joachim Heinrich Campe estimated that one fifth of the total German vocabulary was of French or Latin origin.[80]The order at the end of such strings is subject to variation, but the latter version is unusual.German subordinate clauses have all verbs clustered at the end. Given that auxiliaries encode future, passive, modality, and the perfect, very long chains of verbs at the end of the sentence can occur. In these constructions, the past participle in ge- is often replaced by the infinitive.Sentences using modal verbs place the infinitive at the end. For example, the English sentence \"Should he go home?\" would be rearranged in German to say \"Should he (to) home go?\" (Soll er nach Hause gehen?). Thus, in sentences with several subordinate or relative clauses, the infinitives are clustered at the end. Compare the similar clustering of prepositions in the following (highly contrived) English sentence: \"What did you bring that book that I do not like to be read to out of up for?\"The main verb may appear in first position to put stress on the action itself. The auxiliary verb is still in second position.When an auxiliary verb is present, it appears in second position, and the main verb appears at the end. This occurs notably in the creation of the perfect tense. Many word orders are still possible:The flexible word order also allows one to use language \"tools\" (such as poetic meter and figures of speech) more freely.Swapped object:Swapped adverbs:Another possibility:Both time expressions in front:Adverb of time in front:Object in front:Normal word order:However, German's flexibile word order allows one to emphasise specific words:The position of a noun in a German sentence has no bearing on its being a subject, an object or another argument. In a declarative sentence in English, if the subject does not occur before the predicate, the sentence could well be misunderstood.German requires for a verbal element (main verb or auxiliary verb) to appear second in the sentence. The verb is preceded by the topic of the sentence. The element in focus appears at the end of the sentence. For a sentence without an auxiliary, these are some possibilities:German word order is generally with the V2 word order restriction and also with the SOV word order restriction for main clauses. For polar questions, exclamations and wishes, the finite verb always has the first position. In subordinate clauses, the verb occurs at the very end.A selectively literal translation of this example to illustrate the point might look like this:Indeed, several parenthetical clauses may occur between the prefix of a finite verb and its complement (ankommen = to arrive, er kam an = he arrived, er ist angekommen = he has arrived):Many German verbs have a separable prefix, often with an adverbial function. In finite verb forms, it is split off and moved to the end of the clause and is hence considered by some to be a \"resultative particle\". For example, mitgehen, meaning \"to go along\", would be split, giving Gehen Sie mit? (Literal: \"Go you with?\"; Idiomatic: \"Are you going along?\").Other examples include the following: haften (to stick), verhaften (to detain); kaufen (to buy), verkaufen (to sell); h\u00f6ren (to hear), aufh\u00f6ren (to cease); fahren (to drive), erfahren (to experience).The meaning of basic verbs can be expanded and sometimes radically changed through the use of a number of prefixes. Some prefixes have a specific meaning; the prefix zer- refers to destruction, as in zerrei\u00dfen (to tear apart), zerbrechen (to break apart), zerschneiden (to cut apart). Other prefixes have only the vaguest meaning in themselves; ver- is found in a number of verbs with a large variety of meanings, as in versuchen (to try) from suchen (to seek), vernehmen (to interrogate) from nehmen (to take), verteilen (to distribute) from teilen (to share), verstehen (to understand) from stehen (to stand).The inflection of standard German verbs includes:Like the other Germanic languages, German forms noun compounds in which the first noun modifies the category given by the second,: Hundeh\u00fctte (\"dog hut\"; specifically: \"dog kennel\"). Unlike English, whose newer compounds or combinations of longer nouns are often written in \"open\" with separating spaces, German (like some other Germanic languages) nearly always uses the \"closed\" form without spaces, for example: Baumhaus (\"tree house\"). Like English, German allows arbitrarily long compounds in theory (see also English compounds). The longest German word verified to be actually in (albeit very limited) use is Rindfleischetikettierungs\u00fcberwachungsaufgaben\u00fcbertragungsgesetz, which, literally translated, is \"beef labelling supervision duty assignment law\" [from Rind (cattle), Fleisch (meat), Etikettierung(s) (labelling), \u00dcberwachung(s) (supervision), Aufgaben (duties), \u00dcbertragung(s) (assignment), Gesetz (law)]. However, examples like this are perceived by native speakers as excessively bureaucratic, stylistically awkward or even satirical.In German orthography, nouns and most words with the syntactical function of nouns are capitalised to make it easier for readers to determine the function of a word within a sentence (Am Freitag ging ich einkaufen.\u00a0\u2013 \"On Friday I went shopping.\"; Eines Tages kreuzte er endlich auf.\u00a0\u2013 \"One day he finally showed up.\") This convention is almost unique to German today (shared perhaps only by the closely related Luxembourgish language and several insular dialects of the North Frisian language), but it was historically common in other languages such as Danish (which abolished the capitalization of nouns in 1948) and English.This degree of inflection is considerably less than in Old High German and other old Indo-European languages such as Latin, Ancient Greek, and Sanskrit, and it is also somewhat less than, for instance, Old English, modern Icelandic, or Russian. The three genders have collapsed in the plural. With four cases and three genders plus plural, there are 16 permutations of case and gender/number, but there are only six forms of the definite article, which together cover all 16 permutations. In nouns, inflection for case is required in the singular for strong masculine and neuter nouns, in the genitive and sometimes in the dative. Both of these cases are losing ground to substitutes in informal speech.[77] The dative noun ending is considered somewhat old-fashioned in many contexts and is often dropped, but it is still used in proverbs and the like, in formal speech and in written language. Weak masculine nouns share a common case ending for genitive, dative and accusative in the singular. Feminine nouns are not declined in the singular. The plural has an inflection for the dative. In total, seven inflectional endings (not counting plural markers) exist in German: -s, -es, -n, -ns, -en, -ens, -e.German nouns inflect by case, gender and number:German is a fusional language with a moderate degree of inflection, with three grammatical genders; as such, there can be a large number of words derived from the same root.Bavarian dialects are spoken in Austria (Vienna, Lower- and Upper Austria, Styria, Carinthia, Salzburg, Burgenland, and in most parts of Tyrol), Bavaria (Upper- and Lower Bavaria as well as Upper Palatinate), South Tyrol, southwesternmost Saxony (Southern Vogtlandian), and in the Swiss village of Samnaun. The largest cities in the Bavarian area are Vienna and Munich.Alemannic dialects are spoken in Switzerland (High Alemannic in the densely populated Swiss Plateau, in the south also Highest Alemannic, and Low Alemannic in Basel), Baden-W\u00fcrttemberg (Swabian and Low Alemannic, in the southwest also High Alemannic), Bavarian Swabia (Swabian, in the southwesternmost part also Low Alemannic), Vorarlberg (Low-, High-, and Highest Alemannic), Alsace (Low Alemannic, in the southernmost part also High Alemannic), Liechtenstein (High- and Highest Alemannic), and in the Tyrolean district of Reutte (Swabian). The Alemannic dialects are considered as Alsatian in Alsace. The largest cities in the Alemannic area are Stuttgart and Z\u00fcrich.The Upper German dialects are the Alemannic dialects in the west, and the Bavarian dialects in the east.South Franconian is mainly spoken in northern Baden-W\u00fcrttemberg in Germany, but also in the northeasternmost part of the region of Alsace in France. While these dialects are considered as dialects of German in Baden-W\u00fcrttemberg, they are considered as dialects of Alsatian in Alsace (most Alsatian dialects are however Low Alemannic). The largest cities in the South Franconian area are Karlsruhe and Heilbronn.The East Franconian dialect branch is one of the most spoken dialect branches in Germany. These dialects are spoken in the region of Franconia and in the central parts of Saxon Vogtland. Franconia consists of the Bavarian districts of Upper-, Middle-, and Lower Franconia, the region of South Thuringia (Thuringia), and the eastern parts of the region of Heilbronn-Franken (Tauber Franconia and Hohenlohe) in Baden-W\u00fcrttemberg. The largest cities in the East Franconian area are Nuremberg and W\u00fcrzburg.The High Franconian dialects are transitional dialects between Central- and Upper German. They consist of the East- and South Franconian dialects.Further east, the non-Franconian, East Central German dialects are spoken (Thuringian, Upper Saxon, Ore Mountainian, and Lusatian-New Markish, and earlier, in the then German-speaking parts of Silesia also Silesian, and in then German southern East Prussia also High Prussian). The largest cities in the East Central German area are Berlin and Leipzig.Luxembourgish as well as the Transylvanian Saxon dialect spoken in Transylvania are based on Moselle Franconian dialects. The largest cities in the Franconian Central German area are Cologne and Frankfurt.The Franconian, West Central German dialects are the Central Franconian dialects (Ripuarian and Moselle Franconian), and the Rhine Franconian dialects (Hessian and Palatine). These dialects are considered asThe Central German dialects are spoken in Central Germany, from Aachen in the west to G\u00f6rlitz in the east. They consist of Franconian dialects in the west (West Central German), and non-Franconian dialects in the east (East Central German). Modern Standard German is mostly based on Central German dialects.The High German dialects consist of the Central German, High Franconian, and Upper German dialects. The High Franconian dialects are transitional dialects between Central- and Upper German. The High German varieties spoken by the Ashkenazi Jews have several unique features, and are considered as a separate language, Yiddish, written with the Hebrew alphabet.The Low Franconian dialects are the dialects that are more closely related to Dutch than to Low German. Most of the Low Franconian dialects are spoken in the Netherlands and in Belgium, where they are considered as dialects of Dutch, which is itself a Low Franconian language. In Germany, Low Franconian dialects are spoken in the northwest of North Rhine-Westphalia, along the Lower Rhine. The Low Franconian dialects spoken in Germany are referred to as Meuse-Rhenish or Low Rhenish. In the north of the German Low Franconian language area, North Low Franconian dialects (also referred to as Cleverlands or as dialects of South Guelderish) are spoken. These dialects are more closely related to Dutch (also North Low Franconian) than the South Low Franconian dialects (also referred to as East Limburgish and, east of the Rhine, Low Bergish), which are spoken in the south of the German Low Franconian language area. The South Low Franconian dialects are more closely related to Limburgish than to Dutch, and are transitional dialects between Low Franconian and Ripuarian (Central Franconian). The East Bergish dialects are the easternmost Low Franconian dialects, and are transitional dialects between North- and South Low Franconian, and Westphalian (Low German), with most of its features however being North Low Franconian. The largest cities in the German Low Franconian area are D\u00fcsseldorf and Duisburg.The 18th and 19th centuries were marked by mass education in Standard German in schools. Gradually, Low German came to be politically viewed as a mere dialect spoken by the uneducated. Today, Low Saxon can be divided in two groups: Low Saxon varieties with a reasonable Standard German influx[clarification needed] and varieties of Standard German with a Low Saxon influence known as Missingsch. Sometimes, Low Saxon and Low Franconian varieties are grouped together because both are unaffected by the High German consonant shift. However, the proportion of the population who can understand and speak it has decreased continuously since World War II. The largest cities in the Low German area are Hamburg and Dortmund.Middle Low German was the lingua franca of the Hanseatic League. It was the predominant language in Northern Germany until the 16th century. In 1534, the Luther Bible was published. The translation is considered to be an important step towards the evolution of the Early New High German. It aimed to be understandable to a broad audience and was based mainly on Central and Upper German varieties. The Early New High German language gained more prestige than Low German and became the language of science and literature. Around the same time, the Hanseatic League, based around northern ports, lost its importance as new trade routes to Asia and the Americas were established, and the most powerful German states of that period were located in Middle and Southern Germany.The variation among the German dialects is considerable, with often only neighbouring dialects being mutually intelligible. Some dialects are not intelligible to people who know only Standard German. However, all German dialects belong to the dialect continuum of High German and Low Saxon.The German dialect continuum is traditionally divided most broadly into High German and Low German, also called Low Saxon. However, historically, High German dialects and Low Saxon/Low German dialects do not belong to the same language. Nevertheless, in today's Germany, Low Saxon/Low German is often perceived as a dialectal variation of Standard German on a functional level even by many native speakers. The same phenomenon is found in the eastern Netherlands, as the traditional dialects are not always identified with their Low Saxon/Low German origins, but with Dutch.[76]German is a member of the West Germanic language of the Germanic family of languages, which in turn is part of the Indo-European language family. The German dialects are the traditional local varieties; many of them are hardly understandable to someone who knows only standard German, and they have great differences in lexicon, phonology and syntax. If a narrow definition of language based on mutual intelligibility is used, many German dialects are considered to be separate languages (for instance in the Ethnologue). However, such a point of view is unusual in German linguistics.[2]A mixture of dialect and standard does not normally occur in Northern Germany either. The traditional varieties there are Low German, whereas Standard German is a High German \"variety\". Because their linguistic distance to it is greater, they do not mesh with Standard German the way that High German dialects (such as Bavarian, Swabian, Hessian) can.In the German-speaking parts of Switzerland, mixtures of dialect and standard are very seldom used, and the use of Standard German is largely restricted to the written language, though about 10% of the Swiss residents speak High German (aka Standard German) at home, but mainly due to German immigrants.[75] This situation has been called a medial diglossia. Swiss Standard German is used in the Swiss education system, whereas Austrian Standard German is officially used in the Austrian education system.In German linguistics, German dialects are distinguished from varieties of standard German. The varieties of standard German refer to the different local varieties of the pluricentric standard German. They differ only slightly in lexicon and phonology. In certain regions, they have replaced the traditional German dialects, especially in Northern Germany.In most regions, the speakers use a continuum from more dialectal varieties to more standard varieties according to circumstances.Standard German differs regionally between German-speaking countries in vocabulary and some instances of pronunciation and even grammar and orthography. This variation must not be confused with the variation of local dialects. Even though the regional varieties of standard German are only somewhat influenced by the local dialects, they are very distinct. German is thus considered a pluricentric language.Standard German originated not as a traditional dialect of a specific region but as a written language. However, there are places where the traditional regional dialects have been replaced by new vernaculars based on standard German; that is the case in large stretches of Northern Germany but also in major cities in other parts of the country. It is important to note, however, that the colloquial standard German differs greatly from the formal written language, especially in grammar and syntax, in which it has been influenced by dialectal speech.Like French and Spanish, German has become a classic second foreign language in the western world, as English (Spanish in the US) is well established as the first foreign language.[3][70] German ranks second (after English) among the best known foreign languages in the EU (on a par with French)[3] as well as in Russia.[71] In terms of student numbers across all levels of education, German ranks third in the EU (after English and French)[38] as well as in the United States (after Spanish and French).[11][72] In 2015, approximately 15.4 million people were in the process of learning German across all levels of education worldwide.[70] As this number remained relatively stable since 2005 (\u00b1 1 million), roughly 75\u2013100 million people able to communicate in German as foreign language can be inferred assuming an average course duration of three years and other estimated parameters.[2] According to a 2012 survey, 47 million people within the EU (i.e., up to two thirds of the 75\u2013100 million worldwide) claimed to have sufficient German skills to have a conversation. Within the EU, not counting countries where it is an official language, German as a foreign language is most popular in Eastern and Northern Europe, namely the Czech Republic, Croatia, Denmark, the Netherlands, Slovakia, Hungary, Slovenia, Sweden and Poland.[3][38] German was once and, to some extent, is still, a lingua franca in those parts of Europe.[73]There is also an important German creole being studied and recovered, named Unserdeutsch, spoken in the former German colony of German New Guinea, across Micronesia and in northern Australia (i.e. coastal parts of Queensland and Western Australia), by a few elderly people. The risk of its extinction is serious and efforts to revive interest in the language are being implemented by scholars.[69]German migration to New Zealand in the 19th century was less pronounced than migration from Britain, Ireland, and perhaps even Scandinavia. Despite this there were significant pockets of German-speaking communities which lasted until the first decades of the 20th century. German-speakers settled principally in Puhoi, Nelson, and Gore. At the last census (2006), 37,500 people in New Zealand spoke German, making it the third most spoken European language after English and French and overall the ninth most spoken language.[citation needed]In Australia, the state of South Australia experienced a pronounced wave of immigration in the 1840s from Prussia (particularly the Silesia region). With the prolonged isolation from other German speakers and contact with Australian English, a unique dialect known as Barossa German has developed and is spoken predominantly in the Barossa Valley near Adelaide. Usage of German sharply declined with the advent of World War I, due to the prevailing anti-German sentiment in the population and related government action. It continued to be used as a first language into the twentieth century but now its use is limited to a few older speakers.[citation needed]In Brazil, the largest concentrations of German speakers are in the states of Rio Grande do Sul (where Riograndenser Hunsr\u00fcckisch developed), Santa Catarina, Paran\u00e1, S\u00e3o Paulo and Esp\u00edrito Santo.[65] There are also important concentrations of German-speaking descendants in Argentina, Chile, Paraguay, Venezuela, Peru and Bolivia.[52]In Mexico there are also large populations of German ancestry, mainly in the cities of: Mexico City,[62] Puebla, Mazatl\u00e1n, Tapachula, Ecatepec de Morelos, and larger populations scattered in the states of Chihuahua, Durango, and Zacatecas.[63][64]In Canada, there are 622,650 speakers of German according to the most recent census in 2006,[61] with people of German ancestry (German Canadians) found throughout the country. German-speaking communities are particularly found in British Columbia (118,035) and Ontario (230,330).[61] There is a large and vibrant community in the city of Kitchener, Ontario, which was at one point named Berlin. German immigrants were instrumental in the country's three largest urban areas: Montreal, Toronto, and Vancouver; post-Second World War immigrants managed to preserve a fluency in the German language in their respective neighborhoods and sections. In the first half of the 20th century, over a million German-Canadians made the language Canada's third most spoken after French and English.Hutterite German is an Upper German dialect of the Austro-Bavarian variety of the German language, which is spoken by Hutterite communities in Canada and the United States. Hutterite is spoken in the U.S. states of Washington, Montana, North Dakota, South Dakota, and Minnesota; and in the Canadian provinces of Alberta, Saskatchewan and Manitoba. Its speakers belong to some Schmiedleit, Lehrerleit, and Dariusleit Hutterite groups, but there are also speakers among the older generations of Prairieleit (the descendants of those Hutterites who chose not to settle in colonies). Hutterite children who grow up in the colonies learn to speak Hutterite German before learning English, the standard language of the surrounding areas, in school. Many of these children, though, continue with German Grammar School, in addition to public school, throughout a student's elementary education.[citation needed]The dialects of German which are or were primarily spoken in colonies or communities founded by German-speaking people resemble the dialects of the regions the founders came from. For example, Hutterite German resembles dialects of Carinthia. Texas German is a dialect spoken in the areas of Texas settled by the Adelsverein, such as New Braunfels and Fredericksburg. In the Amana Colonies in the state of Iowa, Amana German is spoken. Plautdietsch is a large minority language spoken in Northern Mexico by the Mennonite communities, and is spoken by more than 200,000 people in Mexico. Pennsylvania German is a West Central German dialect spoken by most of the Amish population of Pennsylvania, Ohio, and Indiana and resembles Palatinate German dialects.[citation needed]Between 1843 and 1910, more than 5 million Germans emigrated overseas,[57] mostly to the United States.[58] German remained an important language in churches, schools, newspapers, and even the administration of the United States Brewers' Association[59] through the early 20th century, but was severely repressed during World War I. Over the course of the 20th century, many of the descendants of 18th century and 19th century immigrants ceased speaking German at home, but small populations of speakers are still found in Pennsylvania (Amish, Hutterites, Dunkards and some Mennonites historically spoke Hutterite German and a West Central German variety of German known as Pennsylvania German or Pennsylvania Dutch), Kansas (Mennonites and Volga Germans), North Dakota (Hutterite Germans, Mennonites, Russian Germans, Volga Germans, and Baltic Germans), South Dakota, Montana, Texas (Texas German), Wisconsin, Indiana, Oregon, Oklahoma, and Ohio (72,570).[60][citation needed] A significant group of German Pietists in Iowa formed the Amana Colonies and continue to practice speaking their heritage language. Early twentieth century immigration was often to St. Louis, Chicago, New York, Milwaukee, Pittsburgh and Cincinnati.In the United States, the states of North Dakota and South Dakota are the only states where German is the most common language spoken at home after English.[55] German geographical names can be found throughout the Midwest region of the country, such as New Ulm and many other towns in Minnesota; Bismarck (North Dakota's state capital), Munich, Karlsruhe, and Strasburg (named after a town near Odessa in Ukraine)[56] in North Dakota; New Braunfels, Fredericksburg, Weimar, and Muenster in Texas; Corn (formerly Korn), Kiefer and Berlin in Oklahoma; and Kiel, Berlin, and Germantown in Wisconsin.Mostly originating from different waves of immigration during the 19th and 20th centuries, an estimated 12,000 people speak German or a German variety as a first language in South Africa.[52] One of the largest communities consists of the speakers of \"Nataler Deutsch\",[53] a variety of Low German, concentrated in and around Wartburg. The small town of Kroondal in the North-West Province also has a mostly German-speaking population. The South African constitution identifies German as a \"commonly used\" language and the Pan South African Language Board is obligated to promote and ensure respect for it.[54] The community is strong enough that several German International schools are supported such as the Deutsche Schule Pretoria.German, along with English and Afrikaans was a co-official language of Namibia from 1984 until its independence from South Africa in 1990. At this point, the Namibian government perceived Afrikaans and German as symbols of apartheid and colonialism, and decided English would be the sole official language, claiming that it was a \"neutral\" language as there were virtually no English native speakers in Namibia at that time.[50] German, Afrikaans and several indigenous languages became \"national languages\" by law, identifying them as elements of the cultural heritage of the nation and ensuring that the state acknowledged and supported their presence in the country.[2] Today, German is used in a wide variety of spheres, especially business and tourism, as well as the churches (most notably the German-speaking Evangelical Lutheran Church in Namibia (GELK)), schools (e.g. the Deutsche H\u00f6here Privatschule Windhoek), literature (German-Namibian authors include Giselher W. Hoffmann), radio (the Namibian Broadcasting Corporation produces radio programs in German), and music (e.g. artist EES). The Allgemeine Zeitung is one of the three biggest newspapers in Namibia and the only German-language daily in Africa.[50]Namibia was a colony of the German Empire from 1884 to 1919. Mostly descending from German settlers who immigrated during this time, 25\u201330,000 people still speak German as a native tongue today.[50] The period of German colonialism in Namibia also led to the evolution of a Standard German-based pidgin language called \"Namibian Black German\", which became a second language for parts of the indigenous population. Although it is nearly extinct today, some older Namibians still have some knowledge of it.[51]In France, the High German varieties of Alsatian and Moselle Franconian are identified as \"regional languages\", but the European Charter for Regional and Minority Languages of 1998 has not yet been ratified by the government.[49] In the Netherlands, the Limburgish, Frisian, and Low German languages are protected regional languages according to the European Charter for Regional and Minority Languages;[43] however, they are widely considered separate languages and neither German nor Dutch dialects.Within Europe and Asia, German is a recognized minority language in the following countries:Although expulsions and (forced) assimilation after the two World Wars greatly diminished them, minority communities of mostly bilingual German native speakers exist in areas both adjacent to and detached from the Sprachraum.[2]It is a co-official language of theIn Europe, German is the second most widely spoken mother tongue (after Russian) and the second biggest language in terms of overall speakers (after English). The area in central Europe where the majority of the population speaks German as a first language and has German as a (co-)official language is called the \"German Sprachraum\". It comprises an estimated 88 million native speakers and 10 million who speak German as a second language (e.g. immigrants).[2][17] Excluding regional minority languages, German is the only official language ofDue to the German diaspora as well as German being the second most widely spoken language in Europe and the third most widely taught foreign language in the US[11] and the EU (in upper secondary education)[38] amongst others, the geographical distribution of German speakers (or \"Germanophones\") spans all inhabited continents. As for the number of speakers of any language worldwide, an assessment is always compromised by the lack of sufficient, reliable data. For an exact, global number of native German speakers, this is further complicated by the existence of several varieties whose status as separate \"languages\" or \"dialects\" is disputed for political and/or linguistic reasons, including quantitatively strong varieties like certain forms of Alemannic (e.g., Alsatian)[2] and Low German/Plautdietsch.[5] Mostly depending on the inclusion or exclusion of certain varieties, it is estimated that approximately 90\u201395 million people speak German as a first language,[2][17][39] 10-25 million as a second language,[2][17] and 75\u2013100 million as a foreign language.[2][3] This would imply approximately 175\u2013220 million German speakers worldwide.[40] It is estimated that also including all persons who are or were taking German classes, i.e., regardless of their actual proficiency, would amount to about 280 million people worldwide with at least some knowledge of German.[2]Official revisions of some of the rules from 1901 were not issued until the controversial German orthography reform of 1996 was made the official standard by governments of all German-speaking countries.[37] Media and written works are now almost all produced in Standard German (often called Hochdeutsch, \"High German\") which is understood in all areas where German is spoken.In 1901, the 2nd Orthographical Conference ended with a complete standardization of the German language in its written form and the Duden Handbook was declared its standard definition.[35] The Deutsche B\u00fchnensprache (literally, German stage language) had established conventions for German pronunciation in theatre (B\u00fchnendeutsch[36]) three years earlier; however, this was an artificial standard that did not correspond to any traditional spoken dialect. Rather, it was based on the pronunciation of Standard German in Northern Germany, although it was subsequently regarded often as a general prescriptive norm, despite differing pronunciation traditions especially in the Upper-German-speaking regions that still characterize the dialect of the area today\u00a0\u2013 especially the pronunciation of the ending -ig as [\u026ak] instead of [\u026a\u00e7]. In Northern Germany, Standard German was a foreign language to most inhabitants, whose native dialects were subsets of Low German. It was usually encountered only in writing or formal speech; in fact, most of Standard German was a written language, not identical to any spoken dialect, throughout the German-speaking area until well into the 19th century.The most comprehensive guide to the vocabulary of the German language is found within the Deutsches W\u00f6rterbuch. This dictionary was created by the Brothers Grimm and is composed of 16 parts which were issued between 1852 and 1860.[33] In 1872, grammatical and orthographic rules first appeared in the Duden Handbook.[34]In the eastern provinces of Banat and Transylvania (German: Siebenb\u00fcrgen), German was the predominant language not only in the larger towns\u00a0\u2013 such as Temeswar (Timi\u0219oara), Hermannstadt (Sibiu) and Kronstadt (Bra\u0219ov)\u00a0\u2013 but also in many smaller localities in the surrounding areas.[31][32]Some cities, such as Prague (German: Prag) and Budapest (Buda, German: Ofen), were gradually Germanized in the years after their incorporation into the Habsburg domain. Others, such as Pozsony (German: Pressburg, now Bratislava), were originally settled during the Habsburg period, and were primarily German at that time. Prague, Budapest and Bratislava as well as cities like Zagreb (German: Agram), and Ljubljana (German: Laibach), contained significant German minorities.German was the language of commerce and government in the Habsburg Empire, which encompassed a large area of Central and Eastern Europe. Until the mid-19th century, it was essentially the language of townspeople throughout most of the Empire. Its use indicated that the speaker was a merchant or someone from an urban area, regardless of nationality.With Luther's rendering of the Bible in the vernacular German asserted itself against the dominance of Latin as a legitimate language for courtly, literary, and now ecclesiastical subject-matter. Further, his Bible was ubiquitous in the German states with nearly every household possessing a copy.[29] Nevertheless, even with the influence of Luther's Bible as an unofficial written standard, it was not until the middle of the 18th century after the ENHG period that a widely accepted standard for written German appeared.[30]One of the central events in the development of ENHG was the publication of Luther's translation of the Bible into German (the New Testament in 1522 and the Old Testament, published in parts and completed in 1534). Luther based his translation primarily on the Mei\u00dfner Deutsch of Saxony,[27] spending much time among the population of Saxony researching the dialect so as to make the work as natural and accessible to German speakers as possible. Copies of Luther's Bible featured a long list of glosses for each region that translated words which were unknown in the region into the regional dialect. Concerning his translation method Luther says the following:The ENHG period saw the rise of several important cross-regional forms of chancery German, one being gemeine tiutsch, used in the court of the Holy Roman Emperor Maximilian I, and the other being Mei\u00dfner Deutsch, used in the Electorate of Saxony in the Duchy of Saxe-Wittenberg.[25] Alongside these courtly written standards, the invention of the printing press led to the development of a number of printers' languages (Druckersprachen) aimed at making printed material readable and understandable across as many diverse dialects of German as possible.[26] The greater ease of production and increased availability of written texts brought about increased standardization in the written form of the German language.Modern German begins with the Early New High German (ENHG) period, which the influential German philologist Wilhelm Scherer dates 1350\u20131650, terminating with the end of the Thirty Years' War.[21] This period saw the further displacement of Latin by German as the primary language of courtly proceedings and, increasingly, of literature in the German states. While these states were still under the control of the Holy Roman Empire and far from any form of unification, the desire for a cohesive written language that would be understandable across the many German-speaking principalities and kingdoms was stronger than ever. As a spoken language German remained highly fractured through this period with a vast number of often mutually-incomprehensible regional dialects being spoken throughout the German states; the invention of the printing press c.1440 and the publication of Luther's vernacular translation of the Bible in 1534, however, had an immense effect on standardizing German as a supra-dialectal written language.The Middle High German period is generally seen as ending with the decimation of the population of Europe in the Black Death of 1346\u20131353.[21]A great wealth of texts survives from the MHG period. Significantly, among this repertoire are a number of impressive secular works, such as the Nibelungenlied, an epic poem telling the story of the dragon-slayer Siegfried (c. 13th century), and the Iwein, an Arthurian verse poem by Hartmann von Aue (c. 1203), as well as several lyric poems and courtly romances such as Parzival and Tristan. (Also noteworthy is the Sachsenspiegel, the first book of laws written in Middle Low German (c. 1220)). The abundance and especially the secular character of the literature of the MHG period demonstrate the beginnings of a standardized written form of German, as well as the desire of poets and authors to be understood by individuals on supra-dialectal terms.While the major changes of the MHG period were socio-cultural, German was still undergoing significant linguistic changes in syntax, phonetics, and morphology as well (e.g. diphthongization of certain vowel sounds: hus (OHG \"house\")\u2192haus (MHG), and weakening of unstressed short vowels to schwa [\u0259]: taga (OHG \"days\")\u2192tage (MHG)).[24]While there is no complete agreement over the dates of the Middle High German (MHG) period, it is generally seen as lasting from 1050 to 1350.[21][22] This was a period of significant expansion of the geographical territory occupied by Germanic tribes, and consequently of the number of German speakers. Whereas during the Old High German period the Germanic tribes extended only as far east as the Elbe and Saale rivers, the MHG period saw a number of these tribes expanding beyond this eastern boundary into Slavic territory (this is known as the Ostsiedlung). Along with the increasing wealth and geographic extent of the Germanic groups came greater use of German in the courts of nobles as the standard language of official proceedings and literature.[22][23] A clear example of this is the mittelhochdeutsche Dichtersprache employed in the Hohenstaufen court in Swabia as a standardized supra-dialectal written language. While these efforts were still regionally bound, German began to be used in place of Latin for certain official purposes, leading to a greater need for regularity in written conventions.The German language through the OHG period was still predominantly a spoken language, with a wide range of dialects and a much more extensive oral tradition than a written one. Having just emerged from the High German consonant shift, OHG was also a relatively new and volatile language still undergoing a number of phonetic, phonological, morphological, and syntactic changes. The scarcity of written work, instability of the language, and widespread illiteracy of the time thus account for the lack of standardization up to the end of the OHG period in 1050.In general, the surviving texts of OHG show a wide range of dialectal diversity with very little written uniformity. The early written tradition of OHG survived mostly through monasteries and scriptoria as local translations of Latin originals; as a result, the surviving texts are written in highly disparate regional dialects and exhibit significant Latin influence, particularly in vocabulary.[19] At this point monasteries, where most written works were produced, were dominated by Latin, and German saw only occasional use in official and ecclesiastical writing.While there is written evidence of the Old High German language in several Elder Futhark inscriptions from as early as the 6th century AD (such as the Pforzen buckle), the Old High German period is generally seen as beginning with the Abrogans (written c.765\u2013775), a Latin-German glossary supplying over 3,000 OHG words with their Latin equivalents. Following the Abrogans the first coherent works written in OHG appear in the 9th century, chief among them being the Muspilli, the Merseberg Incantations, and the Hildebrandslied, as well as a number of other religious texts (the Georgslied, the Ludwigslied, the Evangelienbuch, and translated hymns and prayers).[19][20] The Muspilli is a Christian poem written in a Bavarian dialect offering an account of the soul after the Last Judgment, and the Merseberg Incantations are transcriptions of spells and charms from the pagan Germanic tradition. Of particular interest to scholars, however, has been the Hildebrandslied, a secular epic poem telling the tale of an estranged father and son unknowingly meeting each other in battle. Linguistically this text is highly interesting due to the mixed use of Old Saxon and Old High German dialects in its composition. The written works of this period stem mainly from the Alamanni, Bavarian, and Thuringian groups, all belonging to the Elbe Germanic group (Irminones), which had settled in what is now southern-central Germany and Austria between the 2nd and 6th centuries during the great migration.[19]The history of the German language begins with the High German consonant shift during the migration period, which separated Old High German (OHG) dialects from Old Saxon. This sound shift involved a drastic change in the pronunciation of both voiced and voiceless stop consonants (b, d, g, and p, t, k, respectively). The primary effects of the shift were the following:After these High German dialects, standard German is (somewhat less closely) related to languages based on Low Franconian dialects (e.g. Dutch and Afrikaans) or Low German/Low Saxon dialects (spoken in northern Germany and southern Denmark), neither of which underwent the High German consonant shift. As has been noted, the former of these dialect types is Istvaeonic and the latter Ingvaeonic, whereas the High German dialects are all Irminonic; the differences between these languages and standard German are therefore considerable. Also related to German are the Frisian languages\u2014North Frisian (spoken in Schleswig-Holstein no. 28), Saterland Frisian (spoken in Lower Saxony \u2013 no. 27), and West Frisian (spoken in the Netherlands \u2013 no. 26)\u2014as well as the Anglic languages of English and Scots. These Anglo-Frisian dialects are all members of the Ingvaeonic family of West Germanic languages which did not take part in the High German consonant shift.Standard German is based on Thuringian-Upper Saxon dialects (no. 30 on the map), which are Central German dialects (nos. 29\u201331), belonging to the Irminonic High German dialect group. German is therefore most closely related to the other languages based on High German dialects, such as Luxembourgish (based on Central Franconian dialects \u2013 no. 29), and Yiddish. Also closely related to Standard German are the Upper German dialects spoken in the southern German-speaking countries, such as Swiss German (Alemannic dialects \u2013 no. 34), and the various dialects spoken in the French region of Grand Est, such as Alsatian (mainly Alemannic, but also Central- and\u00a0Upper Franconian\u00a0(no. 32)\u00a0dialects) and Lorraine Franconian (Central Franconian \u2013 no. 29).Within the West Germanic language dialect continuum, the Benrath and Uerdingen lines (running through D\u00fcsseldorf-Benrath and Krefeld-Uerdingen, respectively) serve to distinguish the Germanic dialects that were affected by the High German consonant shift (south of Benrath) from those that were not (north of Uerdingen). The various regional dialects spoken south of these lines are grouped as High German dialects (nos. 29\u201334 on the map), while those spoken to the north comprise the Low German/Low Saxon (nos. 19\u201324) and Low Franconian (no. 25) dialects. As members of the West Germanic language family, High German, Low German, and Low Franconian can be further distinguished historically as Irminonic, Ingvaeonic, and Istvaeonic, respectively. This classification indicates their historical descent from dialects spoken by the Irminones (also known as the Elbe group), Ingvaeones (or North Sea Germanic group), and Istvaeones (or Weser-Rhine group).[18]Modern Standard German is a West Germanic language descended from the Germanic branch of the Indo-European languages. The Germanic languages are traditionally subdivided into three branches: North Germanic, East Germanic, and West Germanic. The first of these branches survives in modern Danish, Swedish, Norwegian, Faroese, and Icelandic, all of which are descended from Old Norse. The East Germanic languages are now extinct, and the only historical member of this branch from which written texts survive is Gothic. The West Germanic languages, however, have undergone extensive dialectal subdivision and are now represented in modern languages such as English, German, Dutch, Yiddish, Afrikaans, and others.[18]German is an inflected language with four cases for nouns, pronouns, and adjectives (nominative, accusative, genitive, dative), three genders (masculine, feminine, neuter), and strong and weak verbs. German derives the majority of its vocabulary from the ancient Germanic branch of the Indo-European language family. A portion of German words are derived from Latin and Greek, and fewer are borrowed from French and English. With slightly different standardized variants (German, Austrian, and Swiss Standard German), German is a pluricentric language. Like English, German is also notable for its broad spectrum of dialects, with many unique varieties existing in Europe and also other parts of the world.[2][16] Due to the limited intelligibility between certain varieties and Standard German, as well as the lack of an undisputed, scientific difference between a \"dialect\" and a \"language\",[2] some German varieties or dialect groups (e.g. Low German or Plautdietsch[5]) are alternatively referred to as \"languages\" and \"dialects\".[17]One of the major languages of the world, German is the first language of almost 100\u00a0million people worldwide and the most widely spoken native language in the European Union.[2][8] Together with French, German is the second most commonly spoken foreign language in the EU after English, making it the second biggest language in the EU in terms of overall speakers.[9] German is also the second most widely taught foreign language in the EU after English at primary school level (but third after English and French at lower secondary level),[10] the fourth most widely taught non-English language in the US[11] (after Spanish, French and American Sign Language), and the second most commonly used scientific language[12] as well as the third most widely used language on websites (after English and Russian).[13] The German-speaking countries are ranked fifth in terms of annual publication of new books, with one tenth of all books (including e-books) in the world being published in the German language.[14] In the United Kingdom, German and French are the most-sought after foreign languages for businesses (with 49% and 50% of businesses identifying these two languages as the most useful, respectively).[15]",
            "title": "German language",
            "url": "https://en.wikipedia.org/wiki/German_language"
        },
        {
            "desc_links": [
                "/wiki/Diagonal_matrix",
                "/wiki/Rigid_body",
                "/wiki/Tensor",
                "/wiki/Torque",
                "/wiki/Angular_acceleration",
                "/wiki/Intensive_and_extensive_properties"
            ],
            "links": [
                "/wiki/Ellipsoid",
                "/wiki/Poinsot%27s_ellipsoid",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Sylvester%27s_law_of_inertia",
                "/wiki/Eigendecomposition_of_a_matrix",
                "/wiki/Unit_vector",
                "/wiki/Jacobi_identity",
                "/wiki/Cross_product",
                "/wiki/Kinematics",
                "/wiki/Centre_of_mass",
                "/wiki/Centre_of_mass",
                "/wiki/Cross_product#Conversion_to_matrix_multiplication",
                "/wiki/Centre_of_mass",
                "/wiki/Kinematics",
                "/wiki/Resultant_force",
                "/wiki/Moment_(physics)",
                "/wiki/Centre_of_mass",
                "/wiki/Mechanical_system",
                "/wiki/List_of_moments_of_inertia",
                "/wiki/Parallel_axis_theorem",
                "/wiki/Second_moment_of_area",
                "/wiki/Beam_(structure)",
                "/wiki/Multiple_integral",
                "/wiki/Kater%27s_pendulum",
                "/wiki/Gravimeter",
                "/wiki/Angular_momentum",
                "/wiki/Angular_velocity",
                "/wiki/Radius_of_gyration",
                "/wiki/Simple_pendulum",
                "/wiki/Rotation_around_a_fixed_axis",
                "/wiki/Torque",
                "/wiki/Angular_acceleration",
                "/wiki/Figure_skating_spins",
                "/wiki/Diving",
                "/wiki/Diving#Dive_positions",
                "/wiki/Angular_momentum",
                "/wiki/Angular_velocity",
                "/wiki/Angular_momentum",
                "/wiki/Kinetic_energy",
                "/wiki/Rigid_body_dynamics",
                "/wiki/Christiaan_Huygens",
                "/wiki/Compound_pendulum",
                "/wiki/Leonhard_Euler",
                "/wiki/Euler%27s_laws#Euler's_second_law",
                "/wiki/Torque",
                "/wiki/Angular_momentum",
                "/wiki/Angular_acceleration",
                "/wiki/Angular_velocity",
                "/wiki/SI",
                "/wiki/Imperial_units",
                "/wiki/United_States_customary_units",
                "/wiki/Diagonal_matrix"
            ],
            "text": "Thus, the magnitude of a point x in the direction n on the inertia ellipsoid isLet a point x on this ellipsoid be defined in terms of its magnitude and direction, x = |x|n, where n is a unit vector. Then the relationship presented above, between the inertia matrix and the scalar moment of inertia In around an axis in the direction n, yieldsto see that the semi-principal diameters of this ellipsoid are given bydefines an ellipsoid in the body frame. Write this equation in the form,orThe moment of inertia matrix in body-frame coordinates is a quadratic form that defines a surface in the body called Poinsot's ellipsoid.[28] Let \u039b be the inertia matrix relative to the centre of mass aligned with the principal axes, then the surfaceFor bodies with constant density an axis of rotational symmetry is a principal axis.The columns of the rotation matrix Q define the directions of the principal axes of the body, and the constants I1, I2, and I3 are called the principal moments of inertia. This result was first shown by J. J. Sylvester (1852), and is a form of Sylvester's law of inertia.[26][27]whereMeasured in the body frame the inertia matrix is a constant real symmetric matrix. A real symmetric matrix has the eigendecomposition into the product of a rotation matrix Q and a diagonal matrix \u039b, given byNotice that A changes as the body moves, while IB\nC remains constant.where vectors y in the body fixed coordinate frame have coordinates x in the inertial frame. Then, the inertia matrix of the body measured in the inertial frame is given byLet the body frame inertia matrix relative to the centre of mass be denoted IB\nC, and define the orientation of the body frame relative to the inertial frame by the rotation matrix A, such that,The use of the inertia matrix in Newton's second law assumes its components are computed relative to axes parallel to the inertial frame and not relative to a body-fixed reference frame.[6][23] This means that as the body moves the components of the inertia matrix change with time. In contrast, the components of the inertia matrix measured in a body-fixed frame are constant.It is common in rigid body mechanics to use notation that explicitly identifies the x, y, and z axes, such as Ixx and Ixy, for the components of the inertia tensor.The components of tensors of degree two can be assembled into a matrix. For the inertia tensor this matrix is given by,and can be interpreted as the moment of inertia around the x-axis when the object rotates around the y-axis.where the dot product is taken with the corresponding elements in the component tensors. A product of inertia term such as I12 is obtained by the computationThe inertia tensor can be used in the same way as the inertia matrix to compute the scalar moment of inertia about an arbitrary axis in the direction n,where r defines the coordinates of a point in the body and \u03c1(r) is the mass density at that point. The integral is taken over the volume V of the body. The inertia tensor is symmetric because Iij =\u00a0Iji.The inertia tensor for a continuous body is given byIn this case, the components of the inertia tensor are given bywhere E is the identity tensorFor a rigid system of particles Pk, k = 1, \u2026, N each of mass mk with position coordinates rk\u00a0=\u00a0(xk, yk, zk), the inertia tensor is given byThis tensor is of degree two because the component tensors are each constructed from two basis vectors. In this form the inertia tensor is also called the inertia binor.where ei, i\u00a0=\u00a01,\u00a02,\u00a03 are the three orthogonal unit vectors defining the inertial frame in which the body moves. Using this basis the inertia tensor is given byThis shows that the inertia matrix can be used to calculate the moment of inertia of a body around any specified rotation axis in the body.where IR is the moment of inertia matrix of the system relative to the reference point R.Thus, the moment of inertia around the line L through R in the direction k\u0302 is obtained from the calculationwhere the dot and the cross products have been interchanged. Exchanging products, and simplifying by noting that \u0394ri and k\u0302 are orthogonal:The simplification of this equation uses the triple scalar product identityThe magnitude squared of the perpendicular vector isnoting that k\u0302 is a unit vector.To relate this scalar moment of inertia to the inertia matrix of the body, introduce the skew-symmetric matrix [k\u0302] such that [k\u0302]y = k\u0302 \u00d7 y, then we have the identitywhere E is the identity matrix, so as to avoid confusion with the inertia matrix, and k\u0302\u2009k\u0302T is the outer product matrix formed from the unit vector k\u0302 along the line\u00a0L.This is derived as follows. Let a rigid assembly of N particles, Pi, i = 1, \u2026, N, have coordinates ri. Choose R as a reference point and compute the moment of inertia around a line L defined by the unit vector k\u0302 through the reference point R, L(t) = R + tk\u0302. The perpendicular vector from this line to the particle Pi is obtained from \u0394ri by removing the component that projects onto k\u0302.where IR is the moment of inertia matrix of the system relative to the reference point R, and [\u0394ri] is the skew symmetric matrix obtained from the vector \u0394ri = ri \u2212 R.The scalar moment of inertia, IL, of a body about a specified axis whose direction is specified by the unit vector k\u0302 and passes through the body at a point R is as follows:[6]Note on the minus sign: By using the skew symmetric matrix of position vectors relative to the reference point, the inertia matrix of each particle has the form \u2212m[r]2, which is similar to the mr2 that appears in planar movement. However, to make this to work out correctly a minus sign is needed. This minus sign can be absorbed into the term m[r]T[r], if desired, by using the skew-symmetry property of [r].where d is the vector from the centre of mass C to the reference point R.The result is the parallel axis theorem,The first term is the inertia matrix IC relative to the centre of mass. The second and third terms are zero by definition of the centre of mass C. And the last term is the total mass of the system multiplied by the square of the skew-symmetric matrix [d] constructed from d.Distribute over the cross product to obtainwhere d is the vector from the centre of mass C to the reference point R. Use this equation to compute the inertia matrix,Let C be the centre of mass of the rigid system, thenConsider the inertia matrix IR obtained for a rigid system of particles measured relative to a reference point R, given byThe inertia matrix of a body depends on the choice of the reference point. There is a useful relationship between the inertia matrix relative to the centre of mass C and the inertia matrix relative to another point R. This relationship is called the parallel axis theorem.[3][6]where IC is the inertia matrix relative to the centre of mass.Thus, the resultant torque on the rigid system of particles is given byobtained from the Jacobi identity for the triple cross product as shown in the proof below:The calculation uses the identityUse the centre of mass C as the reference point, and introduce the skew-symmetric matrix [\u0394ri] = [ri \u2212 C] to represent the cross product (ri \u2212 C) \u00d7, to obtainwhere ai is the acceleration of the particle Pi. The kinematics of a rigid body yields the formula for the acceleration of the particle Pi in terms of the position R and acceleration Ar of the reference point, as well as the angular velocity vector \u03c9 and angular acceleration vector \u03b1 of the rigid system as,The inertia matrix appears in the application of Newton's second law to a rigid assembly of particles. The resultant torque on this system is,[3][6]where IC is the inertia matrix relative to the centre of mass and M is the total mass.Thus, the kinetic energy of the rigid system of particles is given byThe second term in this equation is zero because C is the centre of mass. Introduce the skew-symmetric matrix [\u0394ri] so the kinetic energy becomesThis equation expands to yield three termswhere \u0394ri = ri\u00a0\u2212\u00a0C is the position vector of a particle relative to the centre of mass.The kinetic energy of a rigid system of particles can be formulated in terms of the centre of mass and a matrix of mass moments of inertia of the system. Let the system of particles Pi, i = 1, \u2026,n be located at the coordinates ri with velocities vi, then the kinetic energy is[3][6]is the symmetric inertia matrix of the rigid system of particles measured relative to the centre of mass C.where IC defined byThen, the skew-symmetric matrix [\u0394ri] obtained from the relative position vector \u0394ri = ri \u2212 C, can be used to define,where the terms containing VR (= C) sum to zero by the definition of centre of mass.The inertia matrix is constructed by considering the angular momentum, with the reference point R of the body chosen to be the centre of mass C:[3][6]Note that the cross product can be equivalently written as matrix multiplication by combining the first operand and the operator into a, skew-symmetric, matrix, [b], constructed from the components of b = (bx, by, bz):where \u03c9 is the angular velocity of the system, and VR is the velocity of R.and the (absolute) velocities areLet the system of particles Pi, i = 1, \u2026, n be located at the coordinates ri with velocities vi relative to a fixed reference frame. For a (possibly moving) reference point R, the relative positions areThe scalar moments of inertia appear as elements in a matrix when a system of particles is assembled into a rigid body that moves in three-dimensional space. This inertia matrix appears in the calculation of the angular momentum, kinetic energy and resultant torque of the rigid system of particles.[3][4][5][6][25]Use the centre of mass C as the reference point and define the moment of inertia relative to the centre of mass IC, then the equation for the resultant torque simplifies to[18]:1029where \u00eai \u00d7 \u00eai = 0, and \u00eai \u00d7 t\u0302i = k\u0302 is the unit vector perpendicular to the plane for all of the particles Pi.This yields the resultant torque on the system asFor systems that are constrained to planar movement, the angular velocity and angular acceleration vectors are directed along k\u0302 perpendicular to the plane of movement, which simplifies this acceleration equation. In this case, the acceleration vectors can be simplified by introducing the unit vectors \u00eai from the reference point R to a point ri and the unit vectors t\u0302i = k\u0302 \u00d7 \u00eai , soThe kinematics of a rigid body yields the formula for the acceleration of the particle Pi in terms of the position R and acceleration A of the reference particle as well as the angular velocity vector \u03c9 and angular acceleration vector \u03b1 of the rigid system of particles as,where ri denotes the trajectory of each particle.Newton's laws for a rigid system of N particles, Pi, i = 1, \u2026 N, can be written in terms of a resultant force and torque at a reference point R, to yield[14][17]The moment of inertia IC is the polar moment of inertia of the body.Let the reference point be the centre of mass C of the system so the second term becomes zero, and introduce the moment of inertia IC so the kinetic energy is given by[18]:1084The kinetic energy of a rigid system of particles moving in the plane is given by[14][17]For a given amount of angular momentum, a decrease in the moment of inertia results in an increase in the angular velocity. Figure skaters can change their moment of inertia by pulling in their arms. Thus, the angular velocity achieved by a skater with outstretched arms results in a greater angular velocity when the arms are pulled in, because of the reduced moment of inertia. A figure skater is not, however, a rigid body.The moment of inertia IC about an axis perpendicular to the movement of the rigid system and through the centre of mass is known as the polar moment of inertia. Specifically, it is the second moment of mass with respect to the orthogonal distance from an axis (or pole).then the equation for angular momentum simplifies to[18]:1028and define the moment of inertia relative to the centre of mass IC asUse the centre of mass C as the reference point soThe angular momentum vector for the planar movement of a rigid system of particles is given by[14][17]Note on the cross product: When a body moves parallel to a ground plane, the trajectories of all the points in the body lie in planes parallel to this ground plane. This means that any rotation that the body undergoes must be around an axis perpendicular to this plane. Planar movement is often presented as projected onto this ground plane so that the axis of rotation appears as a point. In this case, the angular velocity and angular acceleration of the body are scalars and the fact that they are vectors along the rotation axis is ignored. This is usually preferred for introductions to the topic. But in the case of moment of inertia, the combination of mass and geometry benefits from the geometric properties of the cross product. For this reason, in this section on planar movement the angular velocity and accelerations of the body are vectors perpendicular to the ground plane, and the cross product operations are the same as used for the study of spatial rigid body movement.This defines the relative position vector and the velocity vector for the rigid system of the particles moving in a plane.For planar movement the angular velocity vector is directed along the unit vector k which is perpendicular to the plane of movement. Introduce the unit vectors ei from the reference point R to a point ri , and the unit vector t\u0302i = k\u0302 \u00d7 \u00eai sowhere \u03c9 is the angular velocity of the system and V is the velocity of R.If a system of n particles, Pi, i = 1, \u2026, n, are assembled into a rigid body, then the momentum of the system can be written in terms of positions relative to a reference point R, and absolute velocities viIf a mechanical system is constrained to move parallel to a fixed plane, then the rotation of a body in the system occurs around an axis k\u0302 perpendicular to this plane. In this case, the moment of inertia of the mass in this system is a scalar known as the polar moment of inertia. The definition of the polar moment of inertia can be obtained by considering momentum, kinetic energy and Newton's laws for the planar movement of a rigid system of particles.[14][17][23][24]where m = 4/3\u03c0R3\u03c1 is the mass of the sphere.Therefore, the moment of inertia of the ball is the sum of the moments of inertia of the discs along the z-axis,then the radius r of the disc at the cross-section z along the z-axis isAs one more example, consider the moment of inertia of a solid sphere of constant density about an axis through its centre of mass. This is determined by summing the moments of inertia of the thin discs that form the sphere. If the surface of the ball is defined by the equation[18]:1301A list of moments of inertia formulas for standard body shapes provides a way to obtain the moment of inertia of a complex body as an assembly of simpler shaped bodies. The parallel axis theorem is used to shift the reference point of the individual bodies to the reference point of the assembly.The moment of inertia of a compound pendulum constructed from a thin disc mounted at the end of a thin rod that oscillates around a pivot at the other end of the rod, begins with the calculation of the moment of inertia of the thin rod and thin disc about their respective centres of mass.[18]Note on second moment of area: The moment of inertia of a body moving in a plane and the second moment of area of a beam's cross-section are often confused. The moment of inertia of body with the shape of the cross-section is the second moment of this area about the z-axis perpendicular to the cross-section, weighted by its density. This is also called the polar moment of the area, and is the sum of the second moments about the x- and y-axes.[22] The stresses in a beam are calculated using the second moment of the cross-sectional area around either the x-axis or y-axis depending on the load.Here, the function \u03c1 gives the mass density at each point (x, y, z), r is a vector perpendicular to the axis of rotation and extending from a point on the rotation axis to a point (x, y, z) in the solid, and the integration is evaluated over the volume\u00a0V of the body\u00a0Q. The moment of inertia of a flat surface is similar with the mass density being replaced by its areal mass density with the integral evaluated over its area.Another expression replaces the summation with an integral,The moment of inertia of a continuous body rotating about a specified axis is calculated in the same way, except with infinitely many point particles. Thus the limits of summation are removed, and the sum is written as follows:Thus, moment of inertia is a physical property that combines the mass and distribution of the particles around the rotation axis. Notice that rotation about different axes of the same body yield different moments of inertia.This shows that the moment of inertia of the body is the sum of each of the mr2 terms, that isConsider the kinetic energy of an assembly of N masses mi that lie at the distances ri from the pivot point P, which is the nearest point on the axis of rotation. It is the sum of the kinetic energy of the individual masses,[17]:516\u2013517[18]:1084\u20131085 [18]:1296\u20131300The moment of inertia about an axis of a body is calculated by summing mr2 for every particle in the body, where r is the perpendicular distance to the specified axis. To see how moment of inertia arises in the study of the movement of an extended body, it is convenient to consider a rigid assembly of point masses. (This equation can be used for axes that are not principal axes provided that it is understood that this does not fully describe the moment of inertia.[21])The moment of inertia of a complex system such as a vehicle or airplane around its vertical axis can be measured by suspending the system from three points to form a trifilar pendulum. A trifilar pendulum is a platform supported by three wires designed to oscillate in torsion around its vertical centroidal axis.[19] The period of oscillation of the trifilar pendulum yields the moment of inertia of the system.[20]Notice that the distance to the center of oscillation of the seconds pendulum must be adjusted to accommodate different values for the local acceleration of gravity. Kater's pendulum is a compound pendulum that uses this property to measure the local acceleration of gravity, and is called a gravimeter.orThis shows that the quantity I = mr2 is how mass combines with the shape of a body to define rotational inertia. The moment of inertia of an arbitrarily shaped body is the sum of the values mr2 for all of the elements of mass in the body.Similarly, the kinetic energy of the pendulum mass is defined by the velocity of the pendulum around the pivot to yieldusing a similar derivation the previous equation.The quantity I = mr2 also appears in the angular momentum of a simple pendulum, which is calculated from the velocity v = \u03c9 \u00d7 r of the pendulum mass around the pivot, where \u03c9 is the angular velocity of the mass about the pivot point. This angular momentum is given byMoment of inertia can be measured using a simple pendulum, because it is the resistance to the rotation caused by gravity. Mathematically, the moment of inertia of the pendulum is the ratio of the torque due to gravity about the pivot of a pendulum to its angular acceleration about that pivot point. For a simple pendulum this is found to be the product of the mass of the particle m with the square of its distance r to the pivot, that iswhere k is known as the radius of gyration.In general, given an object of mass m, an effective radius k can be defined for an axis through its center of mass, with such a value that its moment of inertia isThis simple formula generalizes to define moment of inertia for an arbitrarily shaped body as the sum of all the elemental point masses dm each multiplied by the square of its perpendicular distance r to an axis k\u0302.Thus, moment of inertia depends on both the mass m of a body and its geometry, or shape, as defined by the distance r to the axis of rotation.For a simple pendulum, this definition yields a formula for the moment of inertia I in terms of the mass m of the pendulum and its distance r from the pivot point as,If the shape of the body does not change, then its moment of inertia appears in Newton's law of motion as the ratio of an applied torque \u03c4 on a body to the angular acceleration \u03b1 around a principal axis, that isIf the angular momentum of a system is constant, then as the moment of inertia gets smaller, the angular velocity must increase. This occurs when spinning figure skaters pull in their outstretched arms or divers curl their bodies into a tuck position during a dive, to spin faster.[7][8][9][10][11][12][13]Moment of inertia I is defined as the ratio of the net angular momentum L of a system to its angular velocity \u03c9 around a principal axis,[7][8] that isThe moment of inertia of a rotating flywheel is used in a machine to resist variations in applied torque to smooth its rotational output. The moment of inertia of an airplane about its longitudinal, horizontal and vertical axis determines how steering forces on the control surfaces of its wings, elevators and tail affect the plane in roll, pitch and yaw.Moment of inertia also appears in momentum, kinetic energy, and in Newton's laws of motion for a rigid body as a physical parameter that combines its shape and mass. There is an interesting difference in the way moment of inertia appears in planar and spatial movement. Planar movement has a single scalar that defines the moment of inertia, while for spatial movement the same calculations yield a 3\u00a0\u00d7\u00a03 matrix of moments of inertia, called the inertia matrix or inertia tensor.[5][6]The natural frequency of oscillation of a compound pendulum is obtained from the ratio of the torque imposed by gravity on the mass of the pendulum to the resistance to acceleration defined by the moment of inertia. Comparison of this natural frequency to that of a simple pendulum consisting of a single point of mass provides a mathematical formulation for moment of inertia of an extended body.[3][4]In 1673 Christiaan Huygens introduced this parameter in his study of the oscillation of a body hanging from a pivot, known as a compound pendulum.[1] The term moment of inertia was introduced by Leonhard Euler in his book Theoria motus corporum solidorum seu rigidorum in 1765,[1][2] and it is incorporated into Euler's second law.When a body is rotating, or free to rotate, around an axis, a torque must be applied to change its angular momentum. The amount of torque needed to cause any given angular acceleration (the rate of change in angular velocity) is proportional to the moment of inertia of the body. Moment of inertia may be expressed in units of kilogram metre squared (kg\u00b7m2) in SI units and pound-foot-second squared (lb\u00b7ft\u00b7s2) in imperial or US units.For bodies constrained to rotate in a plane, it is sufficient to consider their moment of inertia about an axis perpendicular to the plane. For bodies free to rotate in three dimensions, their moments can be described by a symmetric 3\u00a0\u00d7\u00a03 matrix; each body has a set of mutually perpendicular principal axes for which this matrix is diagonal and torques around the axes act independently of each other.",
            "title": "Moment of inertia",
            "url": "https://en.wikipedia.org/wiki/Principal_axis_(mechanics)"
        },
        {
            "desc_links": [
                "/wiki/Special_relativity",
                "/wiki/Speed_of_light",
                "/wiki/Quantum_mechanics",
                "/wiki/Molecules",
                "/wiki/Rotational_spectroscopy#Classification_of_molecules_based_on_rotational_behavior",
                "/wiki/Physics",
                "/wiki/Physical_body",
                "/wiki/Deformation_(engineering)",
                "/wiki/Distance",
                "/wiki/Point_(geometry)",
                "/wiki/Force"
            ],
            "links": [
                "/wiki/Configuration_space_(physics)",
                "/wiki/Manifold",
                "/wiki/Rotation_group_SO(3)",
                "/wiki/Euclidean_group#Direct_and_indirect_isometries",
                "/wiki/Euclidean_group#Direct_and_indirect_isometries",
                "/wiki/Euclidean_group",
                "/wiki/Translation_(geometry)",
                "/wiki/Rotation",
                "/wiki/Through_and_through",
                "/wiki/Equality_(objects)",
                "/wiki/Proper_rotation",
                "/wiki/Chirality_(mathematics)",
                "/wiki/Mirror_image",
                "/wiki/Symmetry",
                "/wiki/Symmetry_group",
                "/wiki/Point_groups_in_three_dimensions",
                "/wiki/Vehicle",
                "/wiki/Winding_number",
                "/wiki/Polygon#Angles",
                "/wiki/Coordinate_system",
                "/wiki/Time_derivative",
                "/wiki/Time_derivative",
                "/wiki/Angular_velocity",
                "/wiki/Vector_(geometry)",
                "/wiki/Angular_speed",
                "/wiki/Axis_of_rotation",
                "/wiki/Euler%27s_rotation_theorem",
                "/wiki/Angular_velocity",
                "/wiki/Axis_of_rotation",
                "/wiki/Time_derivative",
                "/wiki/Derivative",
                "/wiki/Velocity",
                "/wiki/Vector_(geometry)",
                "/wiki/Time_derivative",
                "/wiki/Velocity",
                "/wiki/Motion_(physics)",
                "/wiki/Axis_of_rotation",
                "/wiki/Velocity",
                "/wiki/Angular_velocity",
                "/wiki/Frame_of_reference",
                "/wiki/Translation_(physics)",
                "/wiki/Rotation",
                "/wiki/Coordinate_system#Examples",
                "/wiki/Position_vector",
                "/wiki/Space#Classical_mechanics",
                "/wiki/Coordinate_system",
                "/wiki/Center_of_mass",
                "/wiki/Centroid",
                "/wiki/Coordinate_system",
                "/wiki/Kinematics",
                "/wiki/Dynamics_(mechanics)",
                "/wiki/Velocity",
                "/wiki/Acceleration",
                "/wiki/Momentum",
                "/wiki/Impulse_(physics)",
                "/wiki/Kinetic_energy",
                "/wiki/Coordinate_system#Examples",
                "/wiki/Collinear",
                "/wiki/Time-invariant",
                "/wiki/Special_relativity",
                "/wiki/Speed_of_light",
                "/wiki/Quantum_mechanics",
                "/wiki/Molecules",
                "/wiki/Rotational_spectroscopy#Classification_of_molecules_based_on_rotational_behavior",
                "/wiki/Physics",
                "/wiki/Physical_body",
                "/wiki/Deformation_(engineering)",
                "/wiki/Distance",
                "/wiki/Point_(geometry)",
                "/wiki/Force"
            ],
            "text": "The configuration space of a rigid body with one point fixed (i.e., a body with zero translational motion) is given by the underlying manifold of the rotation group SO(3). The configuration space of a nonfixed (with non-zero translational motion) rigid body is E+(3), the subgroup of direct isometries of the Euclidean group in three dimensions (combinations of translations and rotations).A sheet with a through and through image is achiral. We can distinguish again two cases:For a (rigid) rectangular transparent sheet, inversion symmetry corresponds to having on one side an image without rotational symmetry and on the other side an image such that what shines through is the image at the top side, upside down. We can distinguish two cases:Two rigid bodies are said to be different (not copies) if there is no proper rotation from one to the other. A rigid body is called chiral if its mirror image is different in that sense, i.e., if it has either no symmetry or its symmetry group contains only proper rotations. In the opposite case an object is called achiral: the mirror image is a copy, not a different object. Such an object may have a symmetry plane, but not necessarily: there may also be a plane of reflection with respect to which the image of the object is a rotated version. The latter applies for S2n, of which the case n = 1 is inversion symmetry.When the center of mass is used as reference point:However, depending on the application, a convenient choice may be:Any point that is rigidly connected to the body can be used as reference point (origin of coordinate system L) to describe the linear motion of the body (the linear position, velocity and acceleration vectors depend on the choice).Vehicles, walking people, etc., usually rotate according to changes in the direction of the velocity: they move forward with respect to their own orientation. Then, if the body follows a closed orbit in a plane, the angular velocity integrated over a time interval in which the orbit is completed once, is an integer times 360\u00b0. This integer is the winding number with respect to the origin of the velocity. Compare the amount of rotation associated with the vertices of a polygon.In 2D, the angular velocity is a scalar, and matrix A(t) simply represents a rotation in the xy-plane by an angle which is the integral of the angular velocity over time.whereIf C is the origin of a local coordinate system L, attached to the body,where Q is the point fixed in B that instantaneously coincident with R at the instant of interest.[7] This equation is often combined with Acceleration of two points fixed on a rigid body.The acceleration in reference frame N of the point R moving in body B while B is moving in frame N is given bywhere Q is the point fixed in B that is instantaneously coincident with R at the instant of interest.[7] This relation is often combined with the relation for the Velocity of two points fixed on a rigid body.If the point R is moving in rigid body B while B moves in reference frame N, then the velocity of R in N isBy differentiating the equation for the Velocity of two points fixed on a rigid body in N with respect to time, the acceleration in reference frame N of a point Q fixed on a rigid body B can be expressed asThe acceleration of point P in reference frame N is defined as the time derivative in N of its velocity:[5]where O is any arbitrary point fixed in reference frame N, and the N to the left of the d/dt operator indicates that the derivative is taken in reference frame N. The result is independent of the selection of O so long as O is fixed in N.The velocity of point P in reference frame N is defined as the time derivative in N of the position vector from O to P:[5]For any set of three points P, Q, and R, the position vector from P to R is the sum of the position vector from P to Q and the position vector from Q to R:In this case, rigid bodies and reference frames are indistinguishable and completely interchangeable.The angular velocity of a rigid body B in a reference frame N is equal to the sum of the angular velocity of a rigid body D in N and the angular velocity of B with respect to D:[4]Angular velocity is a vector quantity that describes the angular speed at which the orientation of the rigid body is changing and the instantaneous axis about which it is rotating (the existence of this instantaneous axis is guaranteed by the Euler's rotation theorem). All points on a rigid body experience the same angular velocity at all times. During purely rotational motion, all points on the body change position except for those lying on the instantaneous axis of rotation. The relationship between orientation and angular velocity is not directly analogous to the relationship between position and velocity. Angular velocity is not the time rate of change of orientation, because there is no such concept as an orientation vector that can be differentiated to obtain the angular velocity.The linear velocity of a rigid body is a vector quantity, equal to the time rate of change of its linear position. Thus, it is the velocity of a reference point fixed to the body. During purely translational motion (motion with no rotation), all points on a rigid body move with the same velocity. However, when motion involves rotation, the instantaneous velocity of any two points on the body will generally not be the same. Two points of a rotating body will have the same instantaneous velocity only if they happen to lie on an axis parallel to the instantaneous axis of rotation.Velocity (also called linear velocity) and angular velocity are measured with respect to a frame of reference.In general, when a rigid body moves, both its position and orientation vary with time. In the kinematic sense, these changes are referred to as translation and rotation, respectively. Indeed, the position of a rigid body can be viewed as a hypothetic translation and rotation (roto-translation) of the body starting from a hypothetic reference position (not necessarily coinciding with a position actually taken by the body during its motion).The linear position can be represented by a vector with its tail at an arbitrary reference point in space (the origin of a chosen coordinate system) and its tip at an arbitrary point of interest on the rigid body, typically coinciding with its center of mass or centroid. This reference point may define the origin of a coordinate system fixed to the body.Thus, the position of a rigid body has two components: linear and angular, respectively.[2] The same is true for other kinematic and kinetic quantities describing the motion of a rigid body, such as linear and angular velocity, acceleration, momentum, impulse, and kinetic energy.[3]The position of a rigid body is the position of all the particles of which it is composed. To simplify the description of this position, we exploit the property that the body is rigid, namely that all its particles maintain the same distance relative to each other. If the body is rigid, it is sufficient to describe the position of at least three non-collinear particles. This makes it possible to reconstruct the position of all the other particles, provided that their time-invariant position relative to the three selected particles is known. However, typically a different, mathematically more convenient, but equivalent approach is used. The position of the whole body is represented by:In the study of special relativity, a perfectly rigid body does not exist; and objects can only be assumed to be rigid if they are not moving near the speed of light. In quantum mechanics a rigid body is usually thought of as a collection of point masses. For instance, in quantum mechanics molecules (consisting of the point masses: electrons and nuclei) are often seen as rigid bodies (see classification of molecules as rigid rotors).In physics, a rigid body is a solid body in which deformation is zero or so small it can be neglected. The distance between any two given points on a rigid body remains constant in time regardless of external forces exerted on it. A rigid body is usually considered as a continuous distribution of mass.",
            "title": "Rigid body",
            "url": "https://en.wikipedia.org/wiki/Rigid_body"
        },
        {
            "desc_links": [
                "/wiki/Orbit_(dynamics)",
                "/wiki/Lyapunov_stability",
                "/wiki/Eigenvalue",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Lyapunov_function",
                "/wiki/Stability_criterion",
                "/wiki/Mathematics",
                "/wiki/Differential_equation",
                "/wiki/Dynamical_system",
                "/wiki/Heat_equation",
                "/wiki/Maximum_principle",
                "/wiki/Lp_space",
                "/wiki/Gromov%E2%80%93Hausdorff_convergence"
            ],
            "links": [
                "/wiki/Lyapunov_stability",
                "/wiki/Lyapunov_function",
                "/wiki/Jacobian_matrix_and_determinant",
                "/wiki/Routh%E2%80%93Hurwitz_stability_criterion",
                "/wiki/Vector_field",
                "/wiki/Hartman%E2%80%93Grobman_theorem",
                "/wiki/Routh%E2%80%93Hurwitz_stability_criterion",
                "/wiki/Characteristic_polynomial",
                "/wiki/Hurwitz_polynomial",
                "/wiki/Routh%E2%80%93Hurwitz_theorem",
                "/wiki/Real_part",
                "/wiki/Autonomous_system_(mathematics)",
                "/wiki/Linear_differential_equation",
                "/wiki/Eigenvalue",
                "/wiki/Jacobian_matrix",
                "/wiki/Eigenvalues",
                "/wiki/Diffeomorphism",
                "/wiki/Smooth_manifold",
                "/wiki/Absolute_value",
                "/wiki/Derivative",
                "/wiki/Linear_approximation",
                "/wiki/Continuously_differentiable_function",
                "/wiki/Linearization",
                "/wiki/Oscillation",
                "/wiki/Pendulum",
                "/wiki/Damping",
                "/wiki/Linearization",
                "/wiki/Phase_space",
                "/wiki/Square_matrix",
                "/wiki/Eigenvalue",
                "/wiki/Hartman%E2%80%93Grobman_theorem",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Exponential_decay",
                "/wiki/Lyapunov_stability",
                "/wiki/Exponential_stability",
                "/wiki/Qualitative_theory_of_differential_equations",
                "/wiki/Equilibrium_point",
                "/wiki/Periodic_orbit",
                "/wiki/Orbit_(dynamics)",
                "/wiki/Lyapunov_stability",
                "/wiki/Eigenvalue",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Lyapunov_function",
                "/wiki/Stability_criterion",
                "/wiki/Mathematics",
                "/wiki/Differential_equation",
                "/wiki/Dynamical_system",
                "/wiki/Heat_equation",
                "/wiki/Maximum_principle",
                "/wiki/Lp_space",
                "/wiki/Gromov%E2%80%93Hausdorff_convergence"
            ],
            "text": "A general way to establish Lyapunov stability or asymptotic stability of a dynamical system is by means of Lyapunov functions.Let Jp(v) be the n\u00d7n Jacobian matrix of the vector field v at the point p. If all eigenvalues of J have strictly negative real part then the solution is asymptotically stable. This condition can be tested using the Routh\u2013Hurwitz criterion.has a constant solutionSuppose that v is a C1-vector field in Rn which vanishes at a point p, v(p) = 0. Then the corresponding autonomous systemAsymptotic stability of fixed points of a non-linear system can often be established using the Hartman\u2013Grobman theorem.Application of this result in practice, in order to decide the stability of the origin for a linear system, is facilitated by the Routh\u2013Hurwitz stability criterion. The eigenvalues of a matrix are the roots of its characteristic polynomial. A polynomial in one variable with real coefficients is called a Hurwitz polynomial if the real parts of all roots are strictly negative. The Routh\u2013Hurwitz theorem implies a characterization of Hurwitz polynomials by means of an algorithm that avoids computing the roots.(In a different language, the origin 0 \u2208 Rn is an equilibrium point of the corresponding dynamical system.) This solution is asymptotically stable as t \u2192 \u221e (\"in the future\") if and only if for all eigenvalues \u03bb of A, Re(\u03bb) < 0. Similarly, it is asymptotically stable as t \u2192 \u2212\u221e (\"in the past\") if and only if for all eigenvalues \u03bb of A, Re(\u03bb) > 0. If there exists an eigenvalue \u03bb of A with Re(\u03bb) > 0 then the solution is unstable for t \u2192 \u221e.where x(t) \u2208 Rn and A is an n\u00d7n matrix with real entries, has a constant solutionAn autonomous systemThe stability of fixed points of a system of constant coefficient linear differential equations of first order can be analyzed using the eigenvalues of the corresponding matrix.There is an analogous criterion for a continuously differentiable map f: Rn \u2192 Rn with a fixed point a, expressed in terms of its Jacobian matrix at a, Ja(f). If all eigenvalues of J are real or complex numbers with absolute value strictly less than 1 then a is a stable fixed point; if at least one of them has absolute value strictly greater than 1 then a is unstable. Just as for n=1, the case of the largest absolute value being 1 needs to be investigated further\u00a0\u2014 the Jacobian matrix test is inconclusive. The same criterion holds more generally for diffeomorphisms of a smooth manifold.which means that the derivative measures the rate at which the successive iterates approach the fixed point a or diverge from it. If the derivative at a is exactly 1 or \u22121, then more information is needed in order to decide stability.ThusThe fixed point a is stable if the absolute value of the derivative of f at a is strictly less than 1, and unstable if it is strictly greater than 1. This is because near the point a, the function f has a linear approximation with slope f'(a):Let f: R \u2192 R be a continuously differentiable function with a fixed point a, f(a) = a. Consider the dynamical system obtained by iterating the function f:There are useful tests of stability for the case of a linear system. Stability of a nonlinear system can often be inferred from the stability of its linearization.The simplest kind of an orbit is a fixed point, or an equilibrium. If a mechanical system is in a stable equilibrium state then a small push will result in a localized motion, for example, small oscillations as in the case of a pendulum. In a system with damping, a stable equilibrium state is moreover asymptotically stable. On the other hand, for an unstable equilibrium, such as a ball resting on a top of a hill, certain small pushes will result in a motion with a large amplitude that may or may not converge to the original state.One of the key ideas in stability theory is that the qualitative behavior of an orbit under perturbations can be analyzed using the linearization of the system near the orbit. In particular, at each equilibrium of a smooth dynamical system with an n-dimensional phase space, there is a certain n\u00d7n matrix A whose eigenvalues characterize the behavior of the nearby points (Hartman\u2013Grobman theorem). More precisely, if all eigenvalues are negative real numbers or complex numbers with negative real parts then the point is a stable attracting fixed point, and the nearby points converge to it at an exponential rate, cf Lyapunov stability and exponential stability. If none of the eigenvalues are purely imaginary (or zero) then the attracting and repelling directions are related to the eigenspaces of the matrix A with eigenvalues whose real part is negative and, respectively, positive. Analogous statements are known for perturbations of more complicated orbits.Stability means that the trajectories do not change too much under small perturbations. The opposite situation, where a nearby orbit is getting repelled from the given orbit, is also of interest. In general, perturbing the initial state in some directions results in the trajectory asymptotically approaching the given one and in other directions to the trajectory getting away from it. There may also be directions for which the behavior of the perturbed orbit is more complicated (neither converging nor escaping completely), and then stability theory does not give sufficient information about the dynamics.Many parts of the qualitative theory of differential equations and dynamical systems deal with asymptotic properties of solutions and the trajectories\u2014what happens with the system after a long period of time. The simplest kind of behavior is exhibited by equilibrium points, or fixed points, and by periodic orbits. If a particular orbit is well understood, it is natural to ask next whether a small change in the initial condition will lead to similar behavior. Stability theory addresses the following questions: Will a nearby orbit indefinitely stay close to a given orbit? Will it converge to the given orbit? (The latter is a stronger property.) In the former case, the orbit is called stable; in the latter case, it is called asymptotically stable and the given orbit is said to be attracting.In dynamical systems, an orbit is called Lyapunov stable if the forward orbit of any point is in a small enough neighborhood or it stays in a small (but perhaps, larger) neighborhood. Various criteria have been developed to prove stability or instability of an orbit. Under favorable circumstances, the question may be reduced to a well-studied problem involving eigenvalues of matrices. A more general method involves Lyapunov functions. In practice, any one of a number of different stability criteria are applied.In mathematics, stability theory addresses the stability of solutions of differential equations and of trajectories of dynamical systems under small perturbations of initial conditions. The heat equation, for example, is a stable partial differential equation because small perturbations of initial data lead to small variations in temperature at a later time as a result of the maximum principle. In partial differential equations one may measure the distances between functions using Lp norms or the sup norm, while in differential geometry one may measure the distance between spaces using the Gromov\u2013Hausdorff distance.",
            "title": "Stability theory",
            "url": "https://en.wikipedia.org/wiki/Stability_theory"
        },
        {
            "desc_links": [
                "/wiki/Wave",
                "/wiki/Vocal_cords",
                "/wiki/Ear_drum",
                "/wiki/Energy",
                "/wiki/Sound",
                "/wiki/Engine",
                "/wiki/Electric_motor",
                "/wiki/Machine",
                "/wiki/Engine_balance",
                "/wiki/Friction",
                "/wiki/Gear",
                "/wiki/Tuning_fork",
                "/wiki/Reed_(music)",
                "/wiki/Woodwind_instrument",
                "/wiki/Harmonica",
                "/wiki/Mobile_phone",
                "/wiki/Loudspeaker",
                "/wiki/Oscillation",
                "/wiki/Equilibrium_point",
                "/wiki/Periodic_function",
                "/wiki/Random"
            ],
            "links": [
                "/wiki/Femap",
                "/wiki/ANSYS",
                "/wiki/Cantilever",
                "/wiki/I-beam",
                "/wiki/Modal_analysis",
                "/wiki/Finite_element_method",
                "/wiki/Eigenvalues_and_eigenvectors#Vibration_analysis",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Degrees_of_freedom_(engineering)",
                "/wiki/Modal_analysis",
                "/wiki/Superposition_principle",
                "/wiki/Linear_system",
                "/wiki/Frequency_spectrum",
                "/wiki/Periodic_function",
                "/wiki/Fast_Fourier_Transform",
                "/wiki/Window_function",
                "/wiki/Fourier_transform",
                "/wiki/Time_domain",
                "/wiki/Frequency_domain",
                "/wiki/Newton_(unit)",
                "/wiki/Square_wave",
                "/wiki/Steady_state",
                "/wiki/Critical_damping",
                "/wiki/Overdamped",
                "/wiki/Mass-spring-damper_model",
                "/wiki/Angular_frequency",
                "/wiki/Ordinary_frequency",
                "/wiki/Hertz",
                "/wiki/Simple_harmonic_motion",
                "/wiki/Amplitude",
                "/wiki/Newton%27s_laws_of_motion",
                "/wiki/Mass-spring-damper",
                "/wiki/Simple_harmonic_oscillator",
                "/wiki/RLC_circuit",
                "/wiki/Waveform",
                "/wiki/Fast_Fourier_Transform",
                "/wiki/Vibroscope",
                "/wiki/Noise,_vibration,_and_harshness",
                "/wiki/Vehicle_suspension",
                "/wiki/Shock_absorber",
                "/wiki/Resonance",
                "/wiki/Damping",
                "/wiki/Wave",
                "/wiki/Vocal_cords",
                "/wiki/Ear_drum",
                "/wiki/Energy",
                "/wiki/Sound",
                "/wiki/Engine",
                "/wiki/Electric_motor",
                "/wiki/Machine",
                "/wiki/Engine_balance",
                "/wiki/Friction",
                "/wiki/Gear",
                "/wiki/Tuning_fork",
                "/wiki/Reed_(music)",
                "/wiki/Woodwind_instrument",
                "/wiki/Harmonica",
                "/wiki/Mobile_phone",
                "/wiki/Loudspeaker",
                "/wiki/Oscillation",
                "/wiki/Equilibrium_point",
                "/wiki/Periodic_function",
                "/wiki/Random"
            ],
            "text": "Written in this form it can be seen that the vibration at each of the degrees of freedom is just a linear sum of the mode shapes. Furthermore, how much each mode \"participates\" in the final vibration is defined by q, its modal participation factor.Solving for x is replaced by solving for q, referred to as the modal coordinates or modal participation factors.This equation is the foundation of vibration analysis for multiple degree of freedom systems. A similar type of result can be derived for damped systems.[7] The key is that the modal mass and stiffness matrices are diagonal matrices and therefore the equations have been \"decoupled\". In other words, the problem has been transformed from a large unwieldy multiple degree of freedom problem into many single degree of freedom problems that can be solved using the same methods outlined above.The orthogonality properties then simplify this equation to:Using this coordinate transformation in the original free vibration differential equation results in the following equation.These properties can be used to greatly simplify the solution of multi-degree of freedom models by making the following coordinate transformation.The eigenvectors have very important properties called orthogonality properties. These properties can be used to greatly simplify the solution of multi-degree of freedom models. It can be shown that the eigenvectors have the following properties:^ Note that when performing a numerical approximation of any mathematical model, convergence of the parameters of interest must be ascertained.When there are many degrees of freedom, one method of visualizing the mode shapes is by animating them using structural analysis software such as Femap or ANSYS. An example of animating mode shapes is shown in the figure below for a cantilevered I-beam as demonstrated using modal analysis on ANSYS. In this case, the finite element method was used to generate an approximation of the mass and stiffness matrices by meshing the object of interest in order to solve a discrete eigenvalue problem. Note that, in this case, the finite element method provides an approximation of the meshed surface (for which there exists an infinite number of vibration modes and frequencies). Therefore, this relatively simple model that has over 100 degrees of freedom and hence as many natural frequencies and mode shapes, provides a good approximation for the first natural frequencies and modes\u2020. Generally, only the first few modes are important for practical applications.Since the system is a 2 DOF system, there are two modes with their respective natural frequencies and shapes. The mode shape vectors are not the absolute motion, but just describe relative motion of the degrees of freedom. In our case the first mode shape vector is saying that the masses are moving together in phase since they have the same value and sign. In the case of the second mode shape vector, each mass is moving in opposite direction at the same rate.The two mode shapes for the respective natural frequencies are given as:The eigenvalues for this problem given by an eigenvalue routine is:A simple example using the 2 DOF model can help illustrate the concepts. Let both masses have a mass of 1\u00a0kg and the stiffness of all three springs equal 1000 N/m. The mass and stiffness matrix for this problem are then:The eigenvalues and eigenvectors are often written in the following matrix format and describe the modal model of the system:The equation then becomes:This differential equation can be solved by assuming the following type of solution:In the following analysis involves the case where there is no damping and no applied forces (i.e. free vibration). The solution of a viscously damped system is somewhat more complicated.[7]A more compact form of this matrix equation can be written as:This can be rewritten in matrix format:The equations of motion of the 2DOF system are found to be:The simple mass\u2013spring damper model is the foundation of vibration analysis, but what about more complex systems? The mass\u2013spring\u2013damper model described above is called a single degree of freedom (SDOF) model since the mass is assumed to only move up and down. In more complex systems, the system must be discretized into more masses that move in more than one direction, adding degrees of freedom. The major concepts of multiple degrees of freedom (MDOF) can be understood by looking at just a 2 degree of freedom model as shown in the figure.The frequency response function (FRF) does not necessarily have to be calculated from the knowledge of the mass, damping, and stiffness of the system\u2014but can be measured experimentally. For example, if a known force over a range of frequencies is applied, and if the associated vibrations are measured, the frequency response function can be calculated, thereby characterizing the system. This technique is used in the field of experimental modal analysis to determine the vibration characteristics of a structure.The figure also shows the time domain representation of the resulting vibration. This is done by performing an inverse Fourier Transform that converts frequency domain data to time domain. In practice, this is rarely done because the frequency spectrum provides all the necessary information.For example, calculating the FRF for a mass\u2013spring\u2013damper system with a mass of 1\u00a0kg, spring stiffness of 1.93\u00a0N/mm and a damping ratio of 0.1. The values of the spring and mass give a natural frequency of 7\u00a0Hz for this specific system. Applying the 1\u00a0Hz square wave from earlier allows the calculation of the predicted vibration of the mass. The figure illustrates the resulting vibration. It happens in this example that the fourth harmonic of the square wave falls at 7\u00a0Hz. The frequency response of the mass\u2013spring\u2013damper therefore outputs a high 7\u00a0Hz vibration even though the input force had a relatively low 7\u00a0Hz harmonic. This example highlights that the resulting vibration is dependent on both the forcing function and the system that the force is applied to.The phase of the FRF was also presented earlier as:The solution of a vibration problem can be viewed as an input/output relation \u2013 where the force is the input and the output is the vibration. Representing the force and vibration in the frequency domain (magnitude and phase) allows the following relation:In the previous section, the vibration solution was given for a single harmonic force, but the Fourier transform in general gives multiple harmonic forces. The second mathematical tool, \"the principle of superposition\", allows the summation of the solutions from multiple forces if the system is linear. In the case of the spring\u2013mass\u2013damper model, the system is linear if the spring force is proportional to the displacement and the damping is proportional to the velocity over the range of motion of interest. Hence, the solution to the problem with a square wave is summing the predicted vibration from each one of the harmonic forces found in the frequency spectrum of the square wave.In the case of our square wave force, the first component is actually a constant force of 0.5 newton and is represented by a value at \"0\" Hz in the frequency spectrum. The next component is a 1\u00a0Hz sine wave with an amplitude of 0.64. This is shown by the line at 1\u00a0Hz. The remaining components are at odd frequencies and it takes an infinite amount of sine waves to generate the perfect square wave. Hence, the Fourier transform allows you to interpret the force as a sum of sinusoidal forces being applied instead of a more \"complex\" force (e.g. a square wave).The Fourier transform of the square wave generates a frequency spectrum that presents the magnitude of the harmonics that make up the square wave (the phase is also generated, but is typically of less concern and therefore is often not plotted). The Fourier transform can also be used to analyze non-periodic functions such as transients (e.g. impulses) and random functions. The Fourier transform is almost always computed using the Fast Fourier Transform (FFT) computer algorithm in combination with a window function.In a previous section only a simple harmonic force was applied to the model, but this can be extended considerably using two powerful mathematical tools. The first is the Fourier transform that takes a signal as a function of time (time domain) and breaks it down into its harmonic components as a function of frequency (frequency domain). For example, by applying a force to the mass\u2013spring\u2013damper model that repeats the following cycle \u2013 a force equal to 1\u00a0newton for 0.5\u00a0second and then no force for 0.5\u00a0second. This type of force has the shape of a 1\u00a0Hz square wave.The damper, instead of storing energy, dissipates energy. Since the damping force is proportional to the velocity, the more the motion, the more the damper dissipates the energy. Therefore, there is a point when the energy dissipated by the damper equals the energy added by the force. At this point, the system has reached its maximum amplitude and will continue to vibrate at this level as long as the force applied stays the same. If no damping exists, there is nothing to dissipate the energy and, theoretically, the motion will continue to grow into infinity.Resonance is simple to understand if the spring and mass are viewed as energy storage elements \u2013 with the mass storing kinetic energy and the spring storing potential energy. As discussed earlier, when the mass and spring have no external force acting on them they transfer energy back and forth at a rate equal to the natural frequency. In other words, to efficiently pump energy into both mass and spring requires that the energy source feed the energy in at a rate equal to the natural frequency. Applying a force to the mass and spring is similar to pushing a child on swing, a push is needed at the correct moment to make the swing get higher and higher. As in the case of the swing, the force applied need not be high to get large motions, but must just add energy to the system.The following are some other points in regards to the forced vibration shown in the frequency response plots.If resonance occurs in a mechanical system it can be very harmful \u2013 leading to eventual failure of the system. Consequently, one of the major reasons for vibration analysis is to predict when this type of resonance may occur and then to determine what steps to take to prevent it from occurring. As the amplitude plot shows, adding damping can significantly reduce the magnitude of the vibration. Also, the magnitude can be reduced if the natural frequency can be shifted away from the forcing frequency by changing the stiffness or mass of the system. If the system cannot be changed, perhaps the forcing frequency can be shifted (for example, changing the speed of the machine generating the force).Where \u201cr\u201d is defined as the ratio of the harmonic force frequency over the undamped natural frequency of the mass\u2013spring\u2013damper model.The amplitude of the vibration \u201cX\u201d is defined by the following formula.The steady state solution of this problem can be written as:Summing the forces on the mass results in the following ordinary differential equation:The behavior of the spring mass damper model varies with the addition of a harmonic force. A force of this type could, for example, be generated by a rotating imbalance.[6]The plots to the side present how 0.1 and 0.3 damping ratios effect how the system \u201crings\u201d down over time. What is often done in practice is to experimentally measure the free vibration after an impact (for example by a hammer) and then determine the natural frequency of the system by measuring the rate of oscillation, as well as the damping ratio by measuring the rate of decay. The natural frequency and damping ratio are not only important in free vibration, but also characterize how a system behaves under forced vibration.The damped natural frequency is less than the undamped natural frequency, but for many practical cases the damping ratio is relatively small and hence the difference is negligible. Therefore, the damped and undamped description are often dropped when stating the natural frequency (e.g. with 0.1 damping ratio, the damped natural frequency is only 1% less than the undamped).The major points to note from the solution are the exponential term and the cosine function. The exponential term defines how quickly the system \u201cdamps\u201d down \u2013 the larger the damping ratio, the quicker it damps to zero. The cosine function is the oscillating portion of the solution, but the frequency of the oscillations is different from the undamped case.For example, metal structures (e.g., airplane fuselages, engine crankshafts) have damping factors less than 0.05, while automotive suspensions are in the range of 0.2\u20130.3. The solution to the underdamped system for the mass-spring-damper model is the following:The solution to this equation depends on the amount of damping. If the damping is small enough, the system still vibrates\u2014but eventually, over time, stops vibrating. This case is called underdamping, which is important in vibration analysis. If damping is increased just to the point where the system no longer oscillates, the system has reached the point of critical damping. If the damping is increased past critical damping, the system is overdamped. The value that the damping coefficient must reach for critical damping in the mass-spring-damper model is:Summing the forces on the mass results in the following ordinary differential equation:When a \"viscous\" damper is added to the model this outputs a force that is proportional to the velocity of the mass. The damping is called viscous because it models the effects of a fluid within an object. The proportionality constant c is called the damping coefficient and has units of Force over velocity (lbf\u22c5s/in or N\u22c5s/m).Note: angular frequency \u03c9 (\u03c9=2 \u03c0 f) with the units of radians per second is often used in equations because it simplifies the equations, but is normally converted to ordinary frequency (units of Hz or equivalently cycles per second) when stating the frequency of a system. If the mass and stiffness of the system is known, the formula above can determine the frequency at which the system vibrates once set in motion by an initial disturbance. Every vibrating system has one or more natural frequencies that it vibrates at once disturbed. This simple relation can be used to understand in general what happens to a more complex system once we add mass or stiffness. For example, the above formula explains why, when a car or truck is fully loaded, the suspension feels \u2033softer\u2033 than unloaded\u2014the mass has increased, reducing the natural frequency of the system.This solution says that it will oscillate with simple harmonic motion that has an amplitude of A and a frequency of fn. The number fn is called the undamped natural frequency. For the simple mass\u2013spring system, fn is defined as:Assuming that the initiation of vibration begins by stretching the spring by the distance of A and releasing, the solution to the above equation that describes the motion of mass is:The force generated by the mass is proportional to the acceleration of the mass as given by Newton\u2019s second law of motion:To start the investigation of the mass\u2013spring\u2013damper assume the damping is negligible and that there is no external force applied to the mass (i.e. free vibration). The force applied to the mass by the spring is proportional to the amount the spring is stretched \"x\" (assuming the spring is already compressed due to the weight of the mass). The proportionality constant, k, is the stiffness of the spring and has units of force/distance (e.g. lbf/in or N/m). The negative sign indicates that the force is always opposing the motion of the mass attached to it:Note: This article does not include the step-by-step mathematical derivations, but focuses on major vibration analysis equations and concepts. Please refer to the references at the end of the article for detailed derivations.The fundamentals of vibration analysis can be understood by studying the simple Mass-spring-damper model. Indeed, even a complex structure such as an automobile body can be modeled as a \"summation\" of simple mass\u2013spring\u2013damper models. The mass\u2013spring\u2013damper model is an example of a simple harmonic oscillator. The mathematics used to describe its behavior is identical to other simple harmonic oscillators such as the RLC circuit.VA can use the units of Displacement, Velocity and Acceleration displayed as a Time Waveform (TWF), but most commonly the spectrum is used, derived from a Fast Fourier Transform of the TWF. The vibration spectrum provides important frequency information that can pinpoint the faulty component.Vibration Analysis (VA), applied in an industrial or maintenance environment aims to reduce maintenance costs and equipment downtime by detecting equipment faults.[3][4] VA is a key component of a Condition Monitoring (CM) program, and is often referred to as Predictive Maintenance (PdM).[5] Most commonly VA is used to detect faults in rotating equipment (Fans, Motors, Pumps, and Gearboxes etc.) such as Unbalance, Misalignment, rolling element bearing faults and resonance conditions.Most vibration testing is conducted in a 'single DUT axis' at a time, even though most real-world vibration occurs in various axes simultaneously. MIL-STD-810G, released in late 2008, Test Method 527, calls for multiple exciter testing. The vibration test fixture used to attach the DUT to the shaker table must be designed for the frequency range of the vibration test spectrum. Generally for smaller fixtures and lower frequency ranges, the designer targets a fixture design that is free of resonances in the test frequency range. This becomes more difficult as the DUT gets larger and as the test frequency increases. In these cases multi-point control strategies can mitigate some of the resonances that may be present in the future. Devices specifically designed to trace or record vibrations are called vibroscopes.The most common types of vibration testing services conducted by vibration test labs are Sinusoidal and Random. Sine (one-frequency-at-a-time) tests are performed to survey the structural response of the device under test (DUT). A random (all frequencies at once) test is generally considered to more closely replicate a real world environment, such as road inputs to a moving automobile.For relatively low frequency forcing, servohydraulic (electrohydraulic) shakers are used. For higher frequencies, electrodynamic shakers are used. Generally, one or more \"input\" or \"control\" points located on the DUT-side of a fixture is kept at a specified acceleration.[1] Other \"response\" points experience maximum vibration level (resonance) or minimum vibration level (anti-resonance). It is often desirable to achieve anti-resonance to keep a system from becoming too noisy, or to reduce strain on certain parts due to vibration modes caused by specific vibration frequencies .[2]Vibration testing is accomplished by introducing a forcing function into a structure, usually with some type of shaker. Alternately, a DUT (device under test) is attached to the \"table\" of a shaker. Vibration testing is performed to examine the response of a device under test (DUT) to a defined vibration environment. The measured response may be fatigue life, resonant frequencies or squeak and rattle sound output (NVH). Squeak and rattle testing is performed with a special type of quiet shaker that produces very low sound levels while under operation.Damped vibration: When the energy of a vibrating system is gradually dissipated by friction and other resistances, the vibrations are said to be damped. The vibrations gradually reduce or change in frequency or intensity or cease and the system rests in its equilibrium position. An example of this type of vibration is the vehicular suspension dampened by the shock absorber.Forced vibration is when a time-varying disturbance (load, displacement or velocity) is applied to a mechanical system. The disturbance can be a periodic and steady-state input, a transient input, or a random input. The periodic input can be a harmonic or a non-harmonic disturbance. Examples of these types of vibration include a washing machine shaking due to an imbalance, transportation vibration caused by an engine or uneven road, or the vibration of a building during an earthquake. For linear systems, the frequency of the steady-state vibration response resulting from the application of a periodic, harmonic input is equal to the frequency of the applied force or motion, with the response magnitude being dependent on the actual mechanical system.Free vibration occurs when a mechanical system is set in motion with an initial input and allowed to vibrate freely. Examples of this type of vibration are pulling a child back on a swing and letting it go, or hitting a tuning fork and letting it ring. The mechanical system vibrates at one or more of its natural frequencies and damps down to motionlessness.The studies of sound and vibration are closely related. Sound, or pressure waves, are generated by vibrating structures (e.g. vocal cords); these pressure waves can also induce the vibration of structures (e.g. ear drum). Hence, attempts to reduce noise are often related to issues of vibration.In many cases, however, vibration is undesirable, wasting energy and creating unwanted sound. For example, the vibrational motions of engines, electric motors, or any mechanical device in operation are typically unwanted. Such vibrations could be caused by imbalances in the rotating parts, uneven friction, or the meshing of gear teeth. Careful designs usually minimize unwanted vibrations.Vibration can be desirable: for example, the motion of a tuning fork, the reed in a woodwind instrument or harmonica, a mobile phone, or the cone of a loudspeaker.Vibration is a mechanical phenomenon whereby oscillations occur about an equilibrium point. The word comes from Latin vibrationem (\"shaking, brandishing\"). The oscillations may be periodic, such as the motion of a pendulum\u2014or random, such as the movement of a tire on a gravel road.",
            "title": "Vibration",
            "url": "https://en.wikipedia.org/wiki/Vibration_analysis#eigenvalue_problem"
        },
        {
            "desc_links": [
                "/wiki/Hydrogen_atom",
                "/wiki/Chemical_element",
                "/wiki/Periodic_table",
                "/wiki/Aufbau_principle",
                "/wiki/Quantum_number",
                "/wiki/Energy",
                "/wiki/Angular_momentum",
                "/wiki/Vector_component",
                "/wiki/Magnetic_quantum_number",
                "/wiki/Spin_quantum_number",
                "/wiki/Electron_configuration",
                "/wiki/Alkali_metal",
                "/wiki/Spectral_line",
                "/wiki/Quantum_mechanics",
                "/wiki/Function_(mathematics)",
                "/wiki/Electron",
                "/wiki/Atom",
                "/wiki/Probability",
                "/wiki/Atomic_nucleus"
            ],
            "links": [
                "/wiki/Mercury_(element)",
                "/wiki/Gold",
                "/wiki/Caesium",
                "/wiki/Atomic_number",
                "/wiki/Valence_electron",
                "/wiki/Electron_configuration#Atoms:_Aufbau_principle_and_Madelung_rule",
                "/wiki/Periodic_table_block",
                "/wiki/Lithium",
                "/wiki/Beryllium",
                "/wiki/Sodium",
                "/wiki/Magnesium",
                "/wiki/Periodic_table",
                "/wiki/Niels_Bohr",
                "/wiki/Periodic_table",
                "/wiki/Photon",
                "/wiki/Electron_configuration",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Spin_quantum_number",
                "/wiki/Spin_quantum_number",
                "/wiki/Keplerian_orbit",
                "/wiki/Orbital_eccentricity",
                "/wiki/Particle",
                "/wiki/Limit_(mathematics)",
                "/wiki/Antinode",
                "/wiki/Vibrations_of_a_circular_drum",
                "/wiki/Heisenberg_uncertainty_principle",
                "/wiki/Radium",
                "/wiki/Wave_function",
                "/wiki/Spherical_harmonics",
                "/wiki/Spherical_harmonics",
                "/wiki/Cubic_harmonic",
                "/wiki/Torus",
                "/wiki/Ellipsoid",
                "/wiki/Point_of_tangency",
                "/wiki/Atomic_nucleus",
                "/wiki/Dumbbell",
                "/wiki/Electron_shell",
                "/wiki/Complex_number",
                "/wiki/Standing_wave",
                "/wiki/Interference_(wave_propagation)",
                "/wiki/Travelling_wave",
                "/wiki/Orbital_eccentricity",
                "/wiki/Contour_line",
                "/wiki/Absolute_value",
                "/wiki/Wave_function",
                "/wiki/Spherical_harmonic",
                "/wiki/Stern%E2%80%93Gerlach_experiment",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Spin_quantum_number",
                "/wiki/Principal_quantum_number",
                "/wiki/Positive_integer",
                "/wiki/Electron_shells",
                "/wiki/Bohr_model",
                "/wiki/Electron_shell",
                "/wiki/Electron_subshell",
                "/wiki/Quantum_state",
                "/wiki/Wikipedia:Please_clarify",
                "/wiki/Linear_combination",
                "/wiki/Linear_combination_of_atomic_orbitals_molecular_orbital_method",
                "/wiki/Principal_quantum_number",
                "/wiki/Azimuthal_quantum_number",
                "/wiki/Magnetic_quantum_number",
                "/wiki/Periodic_table",
                "/wiki/Hydrogen_atom",
                "/wiki/Ion",
                "/wiki/Eigenstates",
                "/wiki/Hamiltonian_operator",
                "/wiki/Hydrogen_atom",
                "/wiki/X-ray_notation",
                "/wiki/Principal_quantum_number",
                "/wiki/Electron_shell#Subshells",
                "/wiki/Angular_quantum_number",
                "/wiki/Wikipedia:Citation_needed",
                "/wiki/Erwin_Schr%C3%B6dinger",
                "/wiki/Linus_Pauling",
                "/wiki/Robert_S._Mulliken",
                "/wiki/Max_Born",
                "/wiki/Probability_distribution",
                "/wiki/Werner_Heisenberg",
                "/wiki/Uncertainty_principle",
                "/wiki/Niels_Bohr",
                "/wiki/Wave_packet",
                "/wiki/Hydrogen",
                "/wiki/Helium",
                "/wiki/Argon",
                "/wiki/Quantum_mechanics",
                "/wiki/Electron_shell",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Louis_de_Broglie",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hydrogen-like_atom",
                "/wiki/Albert_Einstein",
                "/wiki/Photoelectric_effect",
                "/wiki/Emission_spectra",
                "/wiki/Absorption_spectra",
                "/wiki/Matter_waves",
                "/wiki/Quantum_mechanics",
                "/wiki/Ernest_Rutherford",
                "/wiki/Niels_Bohr",
                "/wiki/Reduced_Planck_constant",
                "/wiki/Bohr_model",
                "/wiki/Hantaro_Nagaoka",
                "/wiki/Saturnian_model",
                "/wiki/J._J._Thomson",
                "/wiki/Plum_pudding_model",
                "/wiki/Robert_Mulliken",
                "/wiki/Niels_Bohr",
                "/wiki/Hantaro_Nagaoka",
                "/wiki/Quantum_mechanics",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Hydrogen-like_atom",
                "/wiki/Coordinate_system",
                "/wiki/Spherical_coordinates",
                "/wiki/Cartesian_coordinate_system",
                "/wiki/Spherical_harmonics#Real_form",
                "/wiki/Spherical_harmonics",
                "/wiki/Hartree%E2%80%93Fock",
                "/wiki/Molecular_orbital_theory",
                "/wiki/Configuration_interaction",
                "/wiki/Atomic_electron_transition",
                "/wiki/Fermion",
                "/wiki/Pauli_exclusion_principle",
                "/wiki/Electron_correlation",
                "/wiki/Atomic_physics",
                "/wiki/Atomic_spectral_line",
                "/wiki/Atomic_electron_transition",
                "/wiki/Quantum_state",
                "/wiki/Quantum_number",
                "/wiki/Term_symbol",
                "/wiki/Neon",
                "/wiki/Quantum_mechanics",
                "/wiki/Eigenstate",
                "/wiki/Hamiltonian_(quantum_mechanics)",
                "/wiki/Configuration_interaction",
                "/wiki/Basis_set_(chemistry)",
                "/wiki/Linear_combination",
                "/wiki/Slater_determinant",
                "/wiki/Spin_(physics)",
                "/wiki/Nuclear_structure#The_independent-particle_model",
                "/wiki/London_dispersion_force",
                "/wiki/Uncertainty_principle",
                "/wiki/Quantum_mechanics",
                "/wiki/Wave%E2%80%93particle_duality",
                "/wiki/Hydrogen_atom",
                "/wiki/Chemical_element",
                "/wiki/Periodic_table",
                "/wiki/Aufbau_principle",
                "/wiki/Quantum_number",
                "/wiki/Energy",
                "/wiki/Angular_momentum",
                "/wiki/Vector_component",
                "/wiki/Magnetic_quantum_number",
                "/wiki/Spin_quantum_number",
                "/wiki/Electron_configuration",
                "/wiki/Alkali_metal",
                "/wiki/Spectral_line",
                "/wiki/Quantum_mechanics",
                "/wiki/Function_(mathematics)",
                "/wiki/Electron",
                "/wiki/Atom",
                "/wiki/Probability",
                "/wiki/Atomic_nucleus"
            ],
            "text": "The atomic orbital model is nevertheless an approximation to the full quantum theory, which only recognizes many electron states. The predictions of line spectra are qualitatively useful but are not quantitatively accurate for atoms and ions other than those containing only one electron.The atomic orbital model thus predicts line spectra, which are observed experimentally. This is one of the main validations of the atomic orbital model.By quantum theory, state\u00a01 has a fixed energy of E1, and state\u00a02 has a fixed energy of E2. Now, what would happen if an electron in state\u00a01 were to move to state\u00a02? For this to happen, the electron would need to gain an energy of exactly E2 \u2212 E1. If the electron receives energy that is less than or greater than this value, it cannot jump from state\u00a01 to state\u00a02. Now, suppose we irradiate the atom with a broad-spectrum of light. Photons that reach the atom that have an energy of exactly E2 \u2212 E1 will be absorbed by the electron in state\u00a01, and that electron will jump to state\u00a02. However, photons that are greater or lower in energy cannot be absorbed by the electron, because the electron can only jump to one of the orbitals, it cannot jump to a state between orbitals. The result is that only photons of a specific frequency will be absorbed by the atom. This creates a line in the spectrum, known as an absorption line, which corresponds to the energy difference between states 1 and 2.State 2) n = 2, \u2113 = 0, m\u2113 = 0 and s = +1/2State 1) n = 1, \u2113 = 0, m\u2113 = 0 and s = +1/2Consider two states of the hydrogen atom:Bound quantum states have discrete energy levels. When applied to atomic orbitals, this means that the energy differences between states are also discrete. A transition between these states (i.e., an electron absorbing or emitting a photon) can thus only happen if the photon has an energy corresponding with the exact energy difference between said states.There are no nodes in relativistic orbital densities, although individual components of the wave function will have nodes.[31]Examples of significant physical outcomes of this effect include the lowered melting temperature of mercury (which results from 6s electrons not being available for metal bonding) and the golden color of gold and caesium.[29]For elements with high atomic number Z, the effects of relativity become more pronounced, and especially so for s\u00a0electrons, which move at relativistic velocities as they penetrate the screening electrons near the core of high-Z atoms. This relativistic increase in momentum for high speed electrons causes a corresponding decrease in wavelength and contraction of 6s orbitals relative to 5d orbitals (by comparison to corresponding s and d electrons in lighter elements in the same column of the periodic table); this results in 6s valence electrons becoming lowered in energy.The number of electrons in an electrically neutral atom increases with the atomic number. The electrons in the outermost shell, or valence electrons, tend to be responsible for an element's chemical behavior. Elements that contain the same number of valence electrons can be grouped together and display similar chemical properties.Although this is the general order of orbital filling according to the Madelung rule, there are exceptions, and the actual electronic energies of each element are also dependent upon additional details of the atoms (see Electron configuration#Atoms: Aufbau principle and Madelung rule).The \"periodic\" nature of the filling of orbitals, as well as emergence of the s, p, d and f \"blocks\", is more obvious if this order of filling is given in matrix form, with increasing principal quantum numbers starting the new rows (\"periods\") in the matrix. Then, each subshell (composed of the first two quantum numbers) is repeated as many times as required for each pair of electrons it may contain. The result is a compressed periodic table, with each entry representing two successive elements:The following is the order for filling the \"subshell\" orbitals, which also gives the order of the \"blocks\" in the periodic table:The periodic table may also be divided into several numbered rectangular 'blocks'. The elements belonging to a given block have this common feature: their highest-energy electrons all belong to the same \u2113-state (but the n associated with that \u2113-state depends upon the period). For instance, the leftmost two columns constitute the 's-block'. The outermost electrons of Li and Be respectively belong to the 2s\u00a0subshell, and those of Na and Mg to the 3s\u00a0subshell.This behavior is responsible for the structure of the periodic table. The table may be divided into several rows (called 'periods'), numbered starting with 1 at the top. The presently known elements occupy seven periods. If a certain period has number i, it consists of elements whose outermost electrons fall in the ith shell. Niels Bohr was the first to propose (1923) that the periodicity in the properties of the elements might be explained by the periodic filling of the electron energy levels, resulting in the electronic structure of the atom.[28]Additionally, an electron always tends to fall to the lowest possible energy state. It is possible for it to occupy any orbital so long as it does not violate the Pauli exclusion principle, but if lower-energy orbitals are available, this condition is unstable. The electron will eventually lose energy (by releasing a photon) and drop into the lower orbital. Thus, electrons fill orbitals in the order specified by the energy sequence given above.Several rules govern the placement of electrons in orbitals (electron configuration). The first dictates that no two electrons in an atom may have the same set of values of quantum numbers (this is the Pauli exclusion principle). These quantum numbers include the three that define orbitals, as well as s, or spin quantum number. Thus, two electrons may occupy a single orbital, so long as they have different values of\u00a0s. However, only two electrons, because of their spin, can be associated with each orbital.Note: empty cells indicate non-existent sublevels, while numbers in italics indicate sublevels that could (potentially) exist, but which do not hold electrons in any element currently known.In addition, the drum modes analogous to p and d modes in an atom show spatial irregularity along the different radial directions from the center of the drum, whereas all of the modes analogous to s\u00a0modes are perfectly symmetrical in radial direction. The non radial-symmetry properties of non-s orbitals are necessary to localize a particle with angular momentum and a wave nature in an orbital where it must tend to stay away from the central attraction force, since any particle localized at the point of central attraction could have no angular momentum. For these modes, waves in the drum head tend to avoid the central point. Such features again emphasize that the shapes of atomic orbitals are a direct consequence of the wave nature of electrons.None of the other sets of modes in a drum membrane have a central antinode, and in all of them the center of the drum does not move. These correspond to a node at the nucleus for all non-s orbitals in an atom. These orbitals all have some angular momentum, and in the planetary model, they correspond to particles in orbit with eccentricity less than 1.0, so that they do not pass straight through the center of the primary body, but keep somewhat away from it.Below, a number of drum membrane vibration modes and the respective wave functions of the hydrogen atom are shown. A correspondence can be considered where the wave functions of a vibrating drum head are for a two-coordinate system \u03c8(r,\u2009\u03b8) and the wave functions for a vibrating sphere are three-coordinate \u03c8(r,\u2009\u03b8,\u2009\u03c6).A mental \"planetary orbit\" picture closest to the behavior of electrons in s\u00a0orbitals, all of which have no angular momentum, might perhaps be that of a Keplerian orbit with the orbital eccentricity of 1 but a finite major axis, not physically possible (because particles were to collide), but can be imagined as a limit of orbits with equal major axes but increasing eccentricity.This relationship means that certain key features can be observed in both drum membrane modes and atomic orbitals. For example, in all of the modes analogous to s\u00a0orbitals (the top row in the animated illustration below), it can be seen that the very center of the drum membrane vibrates most strongly, corresponding to the antinode in all s\u00a0orbitals in an atom. This antinode means the electron is most likely to be at the physical position of the nucleus (which it passes straight through without scattering or striking it), since it is moving (on average) most rapidly at that point, giving it maximal momentum.The shapes of atomic orbitals can be qualitatively understood by considering the analogous case of standing waves on a circular drum.[27] To see the analogy, the mean vibrational displacement of each bit of drum membrane from the equilibrium point over many cycles (a measure of average drum membrane velocity and momentum at that point) must be considered relative to that point's distance from the center of the drum head. If this displacement is taken as being analogous to the probability of finding an electron at a given distance from the nucleus, then it will be seen that the many modes of the vibrating disk form patterns that trace the various shapes of atomic orbitals. The basic reason for this correspondence lies in the fact that the distribution of kinetic energy and momentum in a matter-wave is predictive of where the particle associated with the wave will be. That is, the probability of finding an electron at a given place is also a function of the electron's average momentum at that point, since high electron momentum at a given position tends to \"localize\" the electron in that position, via the properties of electron wave-packets (see the Heisenberg uncertainty principle for details of the mechanism).This table shows all orbital configurations for the real hydrogen-like wave functions up to 7s, and therefore covers the simple electronic configuration for all elements in the periodic table up to radium. \"\u03c8\" graphs are shown with \u2212 and + wave function phases shown in two different colors (arbitrarily red and blue). The pz orbital is the same as the p0 orbital, but the px and py are formed by taking linear combinations of the p+1 and p\u22121 orbitals (which is why they are listed under the m = \u00b11 label). Also, the p+1 and p\u22121 are not the same shape as the p0, since they are pure spherical harmonics.Although individual orbitals are most often shown independent of each other, the orbitals coexist around the nucleus at the same time.The shapes of atomic orbitals in one-electron atom are related to 3-dimensional spherical harmonics. These shapes are not unique, and any linear combination is valid, like a transformation to cubic harmonics, in fact it is possible to generate sets where all the d's are the same shape, just like the px, py, and pz are the same shape.[25][26]Additionally, as is the case with the s orbitals, individual p, d, f and g orbitals with n values higher than the lowest possible value, exhibit an additional radial node structure which is reminiscent of harmonic waves of the same type, as compared with the lowest (or fundamental) mode of the wave. As with s orbitals, this phenomenon provides p, d, f, and g orbitals at the next higher possible value of n (for example, 3p orbitals vs. the fundamental 2p), an additional node in each lobe. Still higher values of n further increase the number of radial nodes, for each type of orbital.There are seven f-orbitals, each with shapes more complex than those of the d-orbitals.Four of the five d-orbitals for n = 3 look similar, each with four pear-shaped lobes, each lobe tangent at right angles to two others, and the centers of all four lying in one plane. Three of these planes are the xy-, xz-, and yz-planes\u2014the lobes are between the pairs of primary axes\u2014and the fourth has the centres along the x and y axes themselves. The fifth and final d-orbital consists of three regions of high probability density: a torus with two pear-shaped regions placed symmetrically on its z axis. The overall total of 18 directional lobes point in every primary axis direction and between every pair.The shapes of p, d and f-orbitals are described verbally here and shown graphically in the Orbitals table below. The three p-orbitals for n = 2 have the form of two ellipsoids with a point of tangency at the nucleus (the two-lobed shape is sometimes referred to as a \"dumbbell\"\u2014there are two lobes pointing in opposite directions from each other). The three p-orbitals in each shell are oriented at right angles to each other, as determined by their respective linear combination of values of\u00a0m\u2113. The overall result is a lobe pointing along each direction of the primary axes.Also in general terms, \u2113 determines an orbital's shape, and m\u2113 its orientation. However, since some orbitals are described by equations in complex numbers, the shape sometimes depends on m\u2113 also. Together, the whole set of orbitals for a given \u2113 and n fill space as symmetrically as possible, though with increasingly complex sets of lobes and nodes.Generally speaking, the number n determines the size and energy of the orbital for a given nucleus: as n increases, the size of the orbital increases. When comparing different elements, the higher nuclear charge Z of heavier elements causes their orbitals to contract by comparison to lighter ones, so that the overall size of the whole atom remains very roughly constant, even as the number of electrons in heavier elements (higher Z) increases.The lobes can be viewed as standing wave interference patterns between the two counter rotating, ring resonant travelling wave \"m\" and \"\u2212m\" modes, with the projection of the orbital onto the xy plane having a resonant \"m\" wavelengths around the circumference. Though rarely depicted the travelling wave solutions can be viewed as rotating banded tori, with the bands representing phase information. For each m there are two standing wave solutions \u27e8m\u27e9+\u27e8\u2212m\u27e9 and \u27e8m\u27e9\u2212\u27e8\u2212m\u27e9. For the case where m = 0 the orbital is vertical, counter rotating information is unknown, and the orbital is z-axis symmetric. For the case where \u2113 = 0 there are no counter rotating modes. There are only radial modes and the shape is spherically symmetric. For any given n, the smaller \u2113 is, the more radial nodes there are. Loosely speaking n is energy, \u2113 is analogous to eccentricity, and m is orientation. In the classical case, a ring resonant travelling wave, for example in a circular transmission line, unless actively forced, will spontaneously decay into a ring resonant standing wave because reflections will build up over time at even the smallest imperfection or discontinuity.Sometimes the \u03c8 function will be graphed to show its phases, rather than the |\u2009\u03c8(r,\u2009\u03b8,\u2009\u03c6)\u2009|2 which shows probability density but has no phases (which have been lost in the process of taking the absolute value, since \u03c8(r,\u2009\u03b8,\u2009\u03c6) is a complex number). |\u2009\u03c8(r,\u2009\u03b8,\u2009\u03c6)\u2009|2 orbital graphs tend to have less spherical, thinner lobes than \u03c8(r,\u2009\u03b8,\u2009\u03c6) graphs, but have the same number of lobes in the same places, and otherwise are recognizable. This article, in order to show wave function phases, shows mostly \u03c8(r,\u2009\u03b8,\u2009\u03c6) graphs.Simple pictures showing orbital shapes are intended to describe the angular forms of regions in space where the electrons occupying the orbital are likely to be found. The diagrams cannot show the entire region where an electron can be found, since according to quantum mechanics there is a non-zero probability of finding the electron (almost) anywhere in space. Instead the diagrams are approximate representations of boundary or contour surfaces where the probability density |\u2009\u03c8(r,\u2009\u03b8,\u2009\u03c6)\u2009|2 has a constant value, chosen so that there is a certain probability (for example 90%) of finding the electron within the contour. Although |\u2009\u03c8\u2009|2 as the square of an absolute value is everywhere non-negative, the sign of the wave function \u03c8(r,\u2009\u03b8,\u2009\u03c6) is often indicated in each subregion of the orbital picture.where p0 = Rn\u20091\u2009Y1\u20090, p1 = Rn\u20091\u2009Y1\u20091, and p\u22121 = Rn\u20091\u2009Y1\u2009\u22121, are the complex orbitals corresponding to \u2113 = 1.In the real hydrogen-like orbitals, for example, n and \u2113 have the same interpretation and significance as their complex counterparts, but m is no longer a good quantum number (though its absolute value is). The orbitals are given new names based on their shape with respect to a standardized Cartesian basis. The real hydrogen-like p\u00a0orbitals are given by the following[20][21]An atom that is embedded in a crystalline solid feels multiple preferred axes, but often no preferred direction. Instead of building atomic orbitals out of the product of radial functions and a single spherical harmonic, linear combinations of spherical harmonics are typically used, designed so that the imaginary part of the spherical harmonics cancel out. These real orbitals are the building blocks most commonly shown in orbital visualizations.The above conventions imply a preferred axis (for example, the z direction in Cartesian coordinates), and they also imply a preferred direction along this preferred axis. Otherwise there would be no sense in distinguishing m = +1 from m = \u22121. As such, the model is most useful when applied to physical systems that share these symmetries. The Stern\u2013Gerlach experiment \u2014 where an atom is exposed to a magnetic field \u2014 provides one such example.[19]The Pauli exclusion principle states that no two electrons in an atom can have the same values of all four quantum numbers. If there are two electrons in an orbital with given values for three quantum numbers, (n, l, m), these two electrons must differ in their spin.Each electron also has a spin quantum number, s, which describes the spin of each electron (spin up or spin down). The number s can be +1/2 or \u22121/2.The principal quantum number n describes the energy of the electron and is always a positive integer. In fact, it can be any positive integer, but for reasons discussed below, large numbers are seldom encountered. Each atom has, in general, many orbitals associated with each value of n; these orbitals together are sometimes called electron shells.In physics, the most common orbital descriptions are based on the solutions to the hydrogen atom, where orbitals are given by the product between a radial function and a pure spherical harmonic. The quantum numbers, together with the rules governing their possible values, are as follows:Because of the quantum mechanical nature of the electrons around a nucleus, atomic orbitals can be uniquely defined by a set of integers known as quantum numbers. These quantum numbers only occur in certain combinations of values, and their physical interpretation changes depending on whether real or complex versions of the atomic orbitals are employed.The quantum number n first appeared in the Bohr model where it determines the radius of each circular electron orbit. In modern quantum mechanics however, n determines the mean distance of the electron from the nucleus; all electrons with the same value of n lie at the same average distance. For this reason, orbitals with the same value of n are said to comprise a \"shell\". Orbitals with the same value of n and also the same value of\u00a0\u2113 are even more closely related, and are said to comprise a \"subshell\".The stationary states (quantum states) of the hydrogen-like atoms are its atomic orbitals.[clarification needed] However, in general, an electron's behavior is not fully described by a single orbital. Electron states are best represented by time-depending \"mixtures\" (linear combinations) of multiple orbitals. See Linear combination of atomic orbitals molecular orbital method.A given (hydrogen-like) atomic orbital is identified by unique values of three quantum numbers: n, \u2113, and m\u2113. The rules restricting the values of the quantum numbers, and their energies (see below), explain the electron configuration of the atoms and the periodic table.For atoms with two or more electrons, the governing equations can only be solved with the use of methods of iterative approximation. Orbitals of multi-electron atoms are qualitatively similar to those of hydrogen, and in the simplest models, they are taken to have the same form. For more rigorous and precise analysis, the numerical approximations must be used.The simplest atomic orbitals are those that are calculated for systems with a single electron, such as the hydrogen atom. An atom of any other element ionized down to a single electron is very similar to hydrogen, and the orbitals take the same form. In the Schr\u00f6dinger equation for this system of one negative and one positive particle, the atomic orbitals are the eigenstates of the Hamiltonian operator for the energy. They can be obtained analytically, meaning that the resulting orbitals are products of a polynomial series, and exponential and trigonometric functions. (see hydrogen atom).For example, the orbital 1s2 (pronounced as the individual numbers and letters: \"one ess two\") has two electrons and is the lowest energy level (n = 1) and has an angular quantum number of \u2113 = 0. In X-ray notation, the principal quantum number is given a letter associated with it. For n = 1,\u20092,\u20093,\u20094,\u20095,\u2009\u2026, the letters associated with those numbers are K, L, M, N, O, \u2026 respectively.where X is the energy level corresponding to the principal quantum number n, type is a lower-case letter denoting the shape or subshell of the orbital and it corresponds to the angular quantum number\u00a0\u2113, and y is the number of electrons in that orbital.Orbitals are given names in the form:In the quantum picture of Heisenberg, Schr\u00f6dinger and others, the Bohr atom number\u00a0n for each orbital became known as an n-sphere[citation needed] in a three dimensional atom and was pictured as the mean energy of the probability cloud of the electron's wave packet which surrounded the atom.In chemistry, Schr\u00f6dinger, Pauling, Mulliken and others noted that the consequence of Heisenberg's relation was that the electron, as a wave packet, could not be considered to have an exact location in its orbital. Max Born suggested that the electron's position needed to be described by a probability distribution which was connected with finding the electron at some point in the wave-function which described its associated wave packet. The new quantum mechanics did not give exact results, but only the probabilities for the occurrence of a variety of possible such results. Heisenberg held that the path of a moving particle has no meaning if we cannot observe it, as we cannot with electrons in an atom.Immediately after Heisenberg discovered his uncertainty principle,[17] Bohr noted that the existence of any sort of wave packet implies uncertainty in the wave frequency and wavelength, since a spread of frequencies is needed to create the packet itself.[18] In quantum mechanics, where all particle momenta are associated with waves, it is the formation of such a wave packet which localizes the wave, and thus the particle, in space. In states where a quantum mechanical particle is bound, it must be localized as a wave packet, and the existence of the packet and its minimum size implies a spread and minimal value in particle wavelength, and thus also momentum and energy. In quantum mechanics, as a particle is localized to a smaller region in space, the associated compressed wave packet requires a larger and larger range of momenta, and thus larger kinetic energy. Thus the binding energy to contain or trap a particle in a smaller region of space increases without bound as the region of space grows smaller. Particles cannot be restricted to a geometric point in space, since this would require an infinite particle momentum.The Bohr model was able to explain the emission and absorption spectra of hydrogen. The energies of electrons in the n = 1, 2, 3, etc. states in the Bohr model match those of current physics. However, this did not explain similarities between different atoms, as expressed by the periodic table, such as the fact that helium (two electrons), neon (10 electrons), and argon (18 electrons) exhibit similar chemical inertness. Modern quantum mechanics explains this in terms of electron shells and subshells which can each hold a number of electrons determined by the Pauli exclusion principle. Thus the n = 1 state can hold one or two electrons, while the n = 2 state can hold up to eight electrons in 2s and 2p subshells. In helium, all n = 1 states are fully occupied; the same for n = 1 and n = 2 in neon. In argon the 3s and 3p subshells are similarly fully occupied by eight electrons; quantum mechanics also allows a 3d subshell but this is at higher energy than the 3s and 3p in argon (contrary to the situation in the hydrogen atom) and remains empty.With de Broglie's suggestion of the existence of electron matter waves in 1924, and for a short time before the full 1926 Schr\u00f6dinger equation treatment of hydrogen-like atom, a Bohr electron \"wavelength\" could be seen to be a function of its momentum, and thus a Bohr orbiting electron was seen to orbit in a circle at a multiple of its half-wavelength (this physically incorrect Bohr model is still often taught to beginning students). The Bohr model for a short time could be seen as a classical model with an additional constraint provided by the 'wavelength' argument. However, this period was immediately superseded by the full three-dimensional wave mechanics of 1926. In our current understanding of physics, the Bohr model is called a semi-classical model because of its quantization of angular momentum, not primarily because of its relationship with electron wavelength, which appeared in hindsight a dozen years after the Bohr model was proposed.After Bohr's use of Einstein's explanation of the photoelectric effect to relate energy levels in atoms with the wavelength of emitted light, the connection between the structure of electrons in atoms and the emission and absorption spectra of atoms became an increasingly useful tool in the understanding of electrons in atoms. The most prominent feature of emission and absorption spectra (known experimentally since the middle of the 19th century), was that these atomic spectra contained discrete lines. The significance of the Bohr model was that it related the lines in emission and absorption spectra to the energy differences between the orbits that electrons could take around an atom. This was, however, not achieved by Bohr through giving the electrons some kind of wave-like properties, since the idea that electrons could behave as matter waves was not suggested until eleven years later. Still, the Bohr model's use of quantized angular momenta and therefore quantized energy levels was a significant step towards the understanding of electrons in atoms, and also a significant step towards the development of quantum mechanics in suggesting that quantized restraints must account for all discontinuous energy levels and spectra in atoms.In 1909, Ernest Rutherford discovered that the bulk of the atomic mass was tightly condensed into a nucleus, which was also found to be positively charged. It became clear from his analysis in 1911 that the plum pudding model could not explain atomic structure. In 1913 as Rutherford's post-doctoral student, Niels Bohr proposed a new model of the atom, wherein electrons orbited the nucleus with classical periods, but were only permitted to have discrete values of angular momentum, quantized in units h/2\u03c0.[10] This constraint automatically permitted only certain values of electron energies. The Bohr model of the atom fixed the problem of energy loss from radiation from a ground state (by declaring that there was no state below this), and more importantly explained the origin of spectral lines.Shortly after Thomson's discovery, Hantaro Nagaoka predicted a different model for electronic structure.[11] Unlike the plum pudding model, the positive charge in Nagaoka's \"Saturnian Model\" was concentrated into a central core, pulling the electrons into circular orbits reminiscent of Saturn's rings. Few people took notice of Nagaoka's work at the time,[15] and Nagaoka himself recognized a fundamental defect in the theory even at its conception, namely that a classical charged object cannot sustain orbital motion because it is accelerating and therefore loses energy due to electromagnetic radiation.[16] Nevertheless, the Saturnian model turned out to have more in common with modern theory than any of its contemporaries.With J. J. Thomson's discovery of the electron in 1897,[13] it became clear that atoms were not the smallest building blocks of nature, but were rather composite particles. The newly discovered structure within atoms tempted many to imagine how the atom's constituent parts might interact with each other. Thomson theorized that multiple electrons revolved in orbit-like rings within a positively charged jelly-like substance,[14] and between the electron's discovery and 1909, this \"plum pudding model\" was the most widely accepted explanation of atomic structure.The term \"orbital\" was coined by Robert Mulliken in 1932 as an abbreviation for one-electron orbital wave function.[9] However, the idea that electrons might revolve around a compact nucleus with definite angular momentum was convincingly argued at least 19 years earlier by Niels Bohr,[10] and the Japanese physicist Hantaro Nagaoka published an orbit-based hypothesis for electronic behavior as early as 1904.[11] Explaining the behavior of these electron \"orbits\" was one of the driving forces behind the development of quantum mechanics.[12]Although hydrogen-like orbitals are still used as pedagogical tools, the advent of computers has made STOs preferable for atoms and diatomic molecules since combinations of STOs can replace the nodes in hydrogen-like atomic orbital. Gaussians are typically used in molecules with three or more atoms. Although not as accurate by themselves as STOs, combinations of many Gaussians can attain the accuracy of hydrogen-like orbitals.Atomic orbitals can be the hydrogen-like \"orbitals\" which are exact solutions to the Schr\u00f6dinger equation for a hydrogen-like \"atom\" (i.e., an atom with one electron). Alternatively, atomic orbitals refer to functions that depend on the coordinates of one electron (i.e., orbitals) but are used as starting points for approximating wave functions that depend on the simultaneous coordinates of all the electrons in an atom or molecule. The coordinate systems chosen for atomic orbitals are usually spherical coordinates (r,\u2009\u03b8,\u2009\u03c6) in atoms and cartesians (x,\u2009y,\u2009z) in polyatomic molecules. The advantage of spherical coordinates (for atoms) is that an orbital wave function is a product of three factors each dependent on a single coordinate: \u03c8(r,\u2009\u03b8,\u2009\u03c6) = R(r)\u2009\u0398(\u03b8)\u2009\u03a6(\u03c6). The angular factors of atomic orbitals \u0398(\u03b8)\u2009\u03a6(\u03c6) generate s, p, d, etc. functions as real combinations of spherical harmonics Y\u2113m(\u03b8,\u2009\u03c6) (where \u2113 and m are quantum numbers). There are typically three mathematical forms for the radial functions\u00a0R(r) which can be chosen as a starting point for the calculation of the properties of atoms and molecules with many electrons:Fundamentally, an atomic orbital is a one-electron wave function, even though most electrons do not exist in one-electron atoms, and so the one-electron view is an approximation. When thinking about orbitals, we are often given an orbital visualization heavily influenced by the Hartree\u2013Fock approximation, which is one way to reduce the complexities of molecular orbital theory.This notation means that the corresponding Slater determinants have a clear higher weight in the configuration interaction expansion. The atomic orbital concept is therefore a key concept for visualizing the excitation process associated with a given transition. For example, one can say for a given transition that it corresponds to the excitation of an electron from an occupied orbital to a given unoccupied orbital. Nevertheless, one has to keep in mind that electrons are fermions ruled by the Pauli exclusion principle and cannot be distinguished from the other electrons in the atom. Moreover, it sometimes happens that the configuration interaction expansion converges very slowly and that one cannot speak about simple one-determinant wave function at all. This is the case when electron correlation is large.In atomic physics, the atomic spectral lines correspond to transitions (quantum leaps) between quantum states of an atom. These states are labeled by a set of quantum numbers summarized in the term symbol and usually associated with particular electron configurations, i.e., by occupation schemes of atomic orbitals (for example, 1s2\u00a02s2\u00a02p6 for the ground state of neon\u2014term symbol: 1S0).Atomic orbitals may be defined more precisely in formal quantum mechanical language. Specifically, in quantum mechanics, the state of an atom, i.e., an eigenstate of the atomic Hamiltonian, is approximated by an expansion (see configuration interaction expansion and basis set) into linear combinations of anti-symmetrized products (Slater determinants) of one-electron functions. The spatial components of these one-electron functions are called atomic orbitals. (When one considers also their spin component, one speaks of atomic spin orbitals.) A state is actually a function of the coordinates of all the electrons, so that their motion is correlated, but this is often approximated by this independent-particle model of products of single electron wave functions.[8] (The London dispersion force, for example, depends on the correlations of the motion of the electrons.)Thus, despite the popular analogy to planets revolving around the Sun, electrons cannot be described simply as solid particles. In addition, atomic orbitals do not closely resemble a planet's elliptical path in ordinary atoms. A more accurate analogy might be that of a large and often oddly shaped \"atmosphere\" (the electron), distributed around a relatively tiny planet (the atomic nucleus). Atomic orbitals exactly describe the shape of this \"atmosphere\" only when a single electron is present in an atom. When more electrons are added to a single atom, the additional electrons tend to more evenly fill in a volume of space around the nucleus so that the resulting collection (sometimes termed the atom's \"electron cloud\"[7]) tends toward a generally spherical zone of probability describing the electron's location, because of the uncertainty principle.Particle-like properties:Wave-like properties:With the development of quantum mechanics and experimental findings (such as the two slit diffraction of electrons), it was found that the orbiting electrons around a nucleus could not be fully described as particles, but needed to be explained by the wave-particle duality. In this sense, the electrons have the following properties:Atomic orbitals are the basic building blocks of the atomic orbital model (alternatively known as the electron cloud or wave mechanics model), a modern framework for visualizing the submicroscopic behavior of electrons in matter. In this model the electron cloud of a multi-electron atom may be seen as being built up (in approximation) in an electron configuration that is a product of simpler hydrogen-like atomic orbitals. The repeating periodicity of the blocks of 2, 6, 10, and 14 elements within sections of the periodic table arises naturally from the total number of electrons that occupy a complete set of s, p, d and f atomic orbitals, respectively, although for higher values of the quantum number n, particularly when the atom in question bears a positive charge, the energies of certain sub-shells become very similar and so the order in which they are said to be populated by electrons (e.g. Cr = [Ar]4s13d5 and Cr2+ = [Ar]3d4) can only be rationalized somewhat arbitrarily.Each orbital in an atom is characterized by a unique set of values of the three quantum numbers n, \u2113, and m, which respectively correspond to the electron's energy, angular momentum, and an angular momentum vector component (the magnetic quantum number). Each such orbital can be occupied by a maximum of two electrons, each with its own spin quantum number s. The simple names s orbital, p orbital, d orbital and f orbital refer to orbitals with angular momentum quantum number \u2113 = 0, 1, 2 and 3 respectively. These names, together with the value of\u00a0n, are used to describe the electron configurations of atoms. They are derived from the description by early spectroscopists of certain series of alkali metal spectroscopic lines as sharp, principal, diffuse, and fundamental. Orbitals for \u2113 > 3 continue alphabetically, omitting\u00a0j (g, h, i, k, \u2026)[3][4][5] because some languages do not distinguish between the letters \"i\" and \"j\".[6]In quantum mechanics, an atomic orbital is a mathematical function that describes the wave-like behavior of either one electron or a pair of electrons in an atom.[1] This function can be used to calculate the probability of finding any electron of an atom in any specific region around the atom's nucleus. The term atomic orbital may also refer to the physical region or space where the electron can be calculated to be present, as defined by the particular mathematical form of the orbital.[2]",
            "title": "Atomic orbital",
            "url": "https://en.wikipedia.org/wiki/Atomic_orbital"
        },
        {
            "desc_links": [
                "/wiki/Eigenvector",
                "/wiki/Computer_vision",
                "/wiki/Facial_recognition_system",
                "/wiki/Facial_recognition_system",
                "/wiki/Alex_Pentland",
                "/wiki/Covariance_matrix",
                "/wiki/Probability_distribution",
                "/wiki/Dimension",
                "/wiki/Vector_space"
            ],
            "links": [
                "/wiki/Active_appearance_model",
                "/wiki/Active_shape_model",
                "/wiki/Fisherface",
                "/wiki/Linear_discriminant_analysis",
                "/wiki/Euclidean_distance",
                "/wiki/Singular_value_decomposition",
                "/wiki/Rank_(linear_algebra)",
                "/wiki/Handwriting_recognition",
                "/wiki/Lip_reading",
                "/wiki/Speaker_recognition",
                "/wiki/Sign_language",
                "/wiki/Gestures",
                "/wiki/Medical_imaging",
                "/wiki/Symmetry",
                "/wiki/Principal_component_analysis",
                "/wiki/Statistical_analysis",
                "/wiki/Digital_photograph",
                "/wiki/Eigenvectors",
                "/wiki/Covariance_matrix",
                "/wiki/Principal_component_analysis",
                "/wiki/Eigenvector",
                "/wiki/Computer_vision",
                "/wiki/Facial_recognition_system",
                "/wiki/Facial_recognition_system",
                "/wiki/Alex_Pentland",
                "/wiki/Covariance_matrix",
                "/wiki/Probability_distribution",
                "/wiki/Dimension",
                "/wiki/Vector_space"
            ],
            "text": "To cope with illumination distraction in practice, the eigenface method usually discards the first three eigenfaces from the dataset. Since illumination is usually the cause behind the largest variations in face images, the first three eigenfaces will mainly capture the information of 3-dimensional lighting changes, which has little contribution to face recognition. By discarding those three eigenfaces, there will be a decent amount of boost in accuracy of face recognition, but other methods such as Fisherface and Linear space still have the advantage.However, the deficiencies of the eigenface method are also obvious:Eigenface provides an easy and cheap way to realize face recognition in that:Many modern approaches still use principal component analysis as a means of dimension reduction or to form basis images for different modes of variation.A further alternative to eigenfaces and fisherfaces is the active appearance model. This approach uses an active shape model to describe the outline of a face. By collecting many face outlines, principal component analysis can be used to form a basis set of models which, encapsulate the variation of different faces.Various extensions have been made to the eigenface method such eigenfeatures. This method combines facial metrics (measuring distance between facial features) with the eigenface representation. Another method similar to the eigenface technique is 'fisherfaces' which uses linear discriminant analysis.[9] This method for facial recognition is less sensitive to variation in lighting and pose of the face than using eigenfaces. Fisherface utilises labelled data to retain more of the class specific information during the dimension reduction stage.The weights of each gallery image only convey information describing that image, not that subject. An image of one subject under frontal lighting may have very different weights to those of the same subject under strong left lighting. This limits the application of such a system. Experiments in the original Eigenface paper presented the following results: an average of 96% with light variation, 85% with orientation variation, and 64% with size variation. (Turk & Pentland 1991, p.\u00a0590)Intuitively, recognition process with eigenface method is to project query images into the face-space spanned by eigenfaces we have calculated and in that face-space find the closest match to a face class.To recognise faces, gallery images, those seen by the system, are saved as collections of weights describing the contribution each eigenface has to that image. When a new face is presented to the system for classification, its own weights are found by projecting the image onto the collection of eigenfaces. This provides a set of weights describing the probe face. These weights are then classified against all weights in the gallery set to find the closest match. A nearest neighbour method is a simple approach for finding the Euclidean distance between two vectors, where the minimum can be classified as the closest subject.(Turk & Pentland 1991, p.\u00a0590)Facial recognition was the source of motivation behind the creation of eigenfaces. For this use, eigenfaces have advantages over other techniques available, such as the system's speed and efficiency. As eigenface is primarily a dimension reduction method, a system can represent many subjects with a relatively small set of data. As a face recognition system it is also fairly invariant to large reductions in image sizing, however it begins to fail considerably when the variation between the seen images and probe image is large.Using SVD on data matrix X, we don\u2019t need to calculate the actual covariance matrix to get eigenfaces.Thus we can see easily that:Let the singular value decomposition (SVD) of X be:Meaning that, if ui is an eigenvector of TTT, then vi\u00a0=\u00a0Tui is an eigenvector of S. If we have a training set of 300 images of 100\u00a0\u00d7\u00a0100 pixels, the matrix TTT is a 300\u00a0\u00d7\u00a0300 matrix, which is much more manageable than the 10,000\u00a0\u00d7 10,000 covariance matrix. Notice however that the resulting vectors vi are not normalised; if normalisation is required it should be applied as an extra step.then we notice that by pre-multiplying both sides of the equation with T, we obtainHowever TTT is a large matrix, and if instead we take the eigenvalue decomposition ofLet T be the matrix of preprocessed training examples, where each column contains one mean-subtracted image. The covariance matrix can then be computed as S = TTT and the eigenvector decomposition of S is given byPerforming PCA directly on the covariance matrix of the images is often computationally infeasible. If small images are used, say 100\u00a0\u00d7\u00a0100 pixels, each image is a point in a 10,000-dimensional space and the covariance matrix S is a matrix of 10,000\u00a0\u00d7 10,000\u00a0= 108 elements. However the rank of the covariance matrix is limited by the number of training examples: if there are N training examples, there will be at most N\u00a0\u2212\u00a01 eigenvectors with non-zero eigenvalues. If the number of training examples is smaller than the dimensionality of the images, the principal components can be computed more easily as follows.Note that although the covariance matrix S generates many eigenfaces, only a fraction of those are needed to represent the majority of the faces. For example, to represent 95% of the total variation of all face images, only the first 43 eigenfaces are needed. To calculate this result, implement the following code:Here is an example of calculating eigenfaces with Extended Yale Face Database B. To evade computational and storage bottleneck, the face images are sampled down by a factor 4x4=16.These eigenfaces can now be used to represent both existing and new faces: we can project a new (mean-subtracted) image on the eigenfaces and thereby record how that new face differs from the mean face. The eigenvalues associated with each eigenface represent how much the images in the training set vary from the mean image in that direction. We lose information by projecting the image on a subset of the eigenvectors, but we minimize this loss by keeping those eigenfaces with the largest eigenvalues. For instance, if we are working with a 100 x 100 image, then we will obtain 10,000 eigenvectors. In practical applications, most faces can typically be identified using a projection on between 100 and 150 eigenfaces, so that most of the 10,000 eigenvectors can be discarded.To create a set of eigenfaces, one must:The technique used in creating eigenfaces and using them for recognition is also used outside of face recognition. This technique is also used for Handwriting recognition, lip reading, voice recognition, sign language/hand gestures interpretation and medical imaging analysis. Therefore, some do not use the term eigenface, but prefer to use 'eigenimage'.The eigenfaces that are created will appear as light and dark areas that are arranged in a specific pattern. This pattern is how different features of a face are singled out to be evaluated and scored. There will be a pattern to evaluate symmetry, if there is any style of facial hair, where the hairline is, or evaluate the size of the nose or mouth. Other eigenfaces have patterns that are less simple to identify, and the image of the eigenface may look very little like a face.A set of eigenfaces can be generated by performing a mathematical process called principal component analysis (PCA) on a large set of images depicting different human faces. Informally, eigenfaces can be considered a set of \"standardized face ingredients\", derived from statistical analysis of many pictures of faces. Any human face can be considered to be a combination of these standard faces. For example, one's face might be composed of the average face plus 10% from eigenface 1, 55% from eigenface 2, and even -3% from eigenface 3. Remarkably, it does not take many eigenfaces combined together to achieve a fair approximation of most faces. Also, because a person's face is not recorded by a digital photograph, but instead as just a list of values (one value for each eigenface in the database used), much less space is taken for each person's face.Once established, the eigenface method was expanded to include methods of preprocessing to improve accuracy.[4] Multiple manifold approaches were also used to build sets of eigenfaces for different subjects[5][6] and different features, such as the eyes.[7]In 1991 M. Turk and A. Pentland expanded these results and presented the Eigenface method of face recognition.[3] In addition to designing a system for automated face recognition using eigenfaces, they showed a way of calculating the eigenvectors of a covariance matrix in such a way as to make it possible for computers at that time to perform eigen-decomposition on a large number of face images. Face images usually occupy a high-dimensional space and conventional principal component analysis was intractable on such data sets. Turk and Pentland's paper demonstrated ways to extract the eigenvectors based on matrices sized by the number of images rather than the number of pixels.The Eigenface approach began with a search for a low-dimensional representation of face images. Sirovich and Kirby (1987) showed that principal component analysis could be used on a collection of face images to form a set of basis features. These basis images, known as Eigenpictures, could be linearly combined to reconstruct images in the original training set. If the training set consists of M images, principal component analysis could form a basis set of N images, where N < M. The reconstruction error is reduced by increasing the number of eigenpictures, however the number needed is always chosen less than M. For example, if you need to generate a number of N eigenfaces for a training set of M face images, you can say that each face image can be made up of \"proportions\" of all this K \"features\" or eigenfaces\u00a0: Face image1 = (23% of E1) + (2% of E2) + (51% of E3) + ... + (1% En).Eigenfaces is the name given to a set of eigenvectors when they are used in the computer vision problem of human face recognition.[1] The approach of using eigenfaces for recognition was developed by Sirovich and Kirby (1987) and used by Matthew Turk and Alex Pentland in face classification.[2] The eigenvectors are derived from the covariance matrix of the probability distribution over the high-dimensional vector space of face images. The eigenfaces themselves form a basis set of all images used to construct the covariance matrix. This produces dimension reduction by allowing the smaller set of basis images to represent the original training images. Classification can be achieved by comparing how faces are represented by the basis set.",
            "title": "Eigenface",
            "url": "https://en.wikipedia.org/wiki/Eigenface"
        },
        {
            "desc_links": [
                "/wiki/Linear_algebra",
                "/wiki/Spectral_decomposition",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)",
                "/wiki/Canonical_form",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Diagonalizable_matrix"
            ],
            "links": [
                "/wiki/Matrix_pencil",
                "/wiki/Optics",
                "/wiki/Forward_Scattering_Alignment",
                "/wiki/Radar",
                "/wiki/Back_Scattering_Alignment",
                "/wiki/Generalized_eigenspace",
                "/wiki/Generalized_eigenvector",
                "/wiki/Power_iteration",
                "/wiki/Rayleigh_quotient",
                "/wiki/Hermitian_matrix",
                "/wiki/Normal_matrix",
                "/wiki/Schur_decomposition",
                "/wiki/Backsubstitution",
                "/wiki/Divide-and-conquer_eigenvalue_algorithm",
                "/wiki/Gaussian_elimination",
                "/wiki/System_of_linear_equations#Solving_a_linear_system",
                "/wiki/System_of_linear_equations",
                "/wiki/Sequence",
                "/wiki/Almost_always",
                "/wiki/Google",
                "/wiki/PageRank",
                "/wiki/Linear_span",
                "/wiki/Arnoldi_iteration",
                "/wiki/QR_algorithm",
                "/wiki/Newton%27s_method",
                "/wiki/Round-off_error",
                "/wiki/Ill-conditioned",
                "/wiki/Abel%E2%80%93Ruffini_theorem",
                "/wiki/Iterative_method",
                "/wiki/Characteristic_polynomial",
                "/wiki/Numerical_analysis",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Orthogonal_matrix",
                "/wiki/Symmetric_matrix",
                "/wiki/Holomorphic_functional_calculus",
                "/wiki/Diagonal_matrix",
                "/wiki/Power_series",
                "/wiki/Laplace_operator",
                "/wiki/Data",
                "/wiki/Inverse_function",
                "/wiki/Diagonal_matrix",
                "/wiki/Nonsingular",
                "/wiki/Linearly_independent",
                "/wiki/Geometric_multiplicity",
                "/wiki/Algebraic_multiplicity",
                "/wiki/Factorization",
                "/wiki/Characteristic_polynomial",
                "/wiki/Spectrum_of_a_matrix",
                "/wiki/Linear_algebra",
                "/wiki/Spectral_decomposition",
                "/wiki/Factorization",
                "/wiki/Matrix_(math)",
                "/wiki/Canonical_form",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Diagonalizable_matrix"
            ],
            "text": "The set of matrices of the form A \u2212 \u03bbB, where \u03bb is a complex number, is called a pencil; the term matrix pencil can also refer to the pair (A,B) of matrices.[9] If B is invertible, then the original problem can be written in the formAnd since P is invertible, we multiply the equation from the right by its inverse, finishing the proof.And the proof isThen the following equality holdswhere A and B are matrices. If v obeys this equation, with some \u03bb, then we call v the generalized eigenvector of A and B (in the 2nd sense), and \u03bb is called the generalized eigenvalue of A and B (in the 2nd sense) which corresponds to the generalized eigenvector v. The possible values of \u03bb must obey the following equationA generalized eigenvalue problem (2nd sense) is the problem of finding a vector v that obeysFor example, in coherent electromagnetic scattering theory, the linear transformation A represents the action performed by the scattering object, and the eigenvectors represent polarization states of the electromagnetic wave. In optics, the coordinate system is defined from the wave's viewpoint, known as the Forward Scattering Alignment (FSA), and gives rise to a regular eigenvalue equation, whereas in radar, the coordinate system is defined from the radar's viewpoint, known as the Back Scattering Alignment (BSA), and gives rise to a coneigenvalue equation.A conjugate eigenvector or coneigenvector is a vector sent after transformation to a scalar multiple of its conjugate, where the scalar is called the conjugate eigenvalue or coneigenvalue of the linear transformation. The coneigenvectors and coneigenvalues represent essentially the same information and meaning as the regular eigenvectors and eigenvalues, but arise when an alternative coordinate system is used. The corresponding equation isThis usage should not be confused with the generalized eigenvalue problem described below.Recall that the geometric multiplicity of an eigenvalue can be described as the dimension of the associated eigenspace, the nullspace of \u03bbI \u2212 A. The algebraic multiplicity can also be thought of as a dimension: it is the dimension of the associated generalized eigenspace (1st sense), which is the nullspace of the matrix (\u03bbI \u2212 A)k for any sufficiently large k. That is, it is the space of generalized eigenvectors (1st sense), where a generalized eigenvector is any vector which eventually becomes 0 if \u03bbI \u2212 A is applied to it enough times successively. Any eigenvector is a generalized eigenvector, and so each eigenspace is contained in the associated generalized eigenspace. This provides an easy proof that the geometric multiplicity is always less than or equal to the algebraic multiplicity.However, in practical large-scale eigenvalue methods, the eigenvectors are usually computed in other ways, as a byproduct of the eigenvalue computation. In power iteration, for example, the eigenvector is actually computed before the eigenvalue (which is typically computed by the Rayleigh quotient of the eigenvector).[6] In the QR algorithm for a Hermitian matrix (or any normal matrix), the orthonormal eigenvectors are obtained as a product of the Q matrices from the steps in the algorithm.[6] (For more general matrices, the QR algorithm yields the Schur decomposition first, from which the eigenvectors can be obtained by a backsubstitution procedure.[8]) For Hermitian matrices, the Divide-and-conquer eigenvalue algorithm is more efficient than the QR algorithm if both eigenvectors and eigenvalues are desired.[6]using Gaussian elimination or any other method for solving matrix equations.Once the eigenvalues are computed, the eigenvectors could be calculated by solving the equationThis sequence will almost always converge to an eigenvector corresponding to the eigenvalue of greatest magnitude, provided that v has a nonzero component of this eigenvector in the eigenvector basis (and also provided that there is only one eigenvalue of greatest magnitude). This simple algorithm is useful in some practical applications; for example, Google uses it to calculate the page rank of documents in their search engine.[7] Also, the power method is the starting point for many more sophisticated algorithms. For instance, by keeping not just the last vector in the sequence, but instead looking at the span of all the vectors in the sequence, one can get a better (faster converging) approximation for the eigenvector, and this idea is the basis of Arnoldi iteration.[6] Alternatively, the important QR algorithm is also based on a subtle transformation of a power method.[6]Iterative numerical algorithms for approximating roots of polynomials exist, such as Newton's method, but in general it is impractical to compute the characteristic polynomial and then apply these methods. One reason is that small round-off errors in the coefficients of the characteristic polynomial can lead to large errors in the eigenvalues and eigenvectors: the roots are an extremely ill-conditioned function of the coefficients.[6]In practice, eigenvalues of large matrices are not computed using the characteristic polynomial. Computing the polynomial becomes expensive in itself, and exact (symbolic) roots of a high-degree polynomial can be difficult to compute and express: the Abel\u2013Ruffini theorem implies that the roots of high-degree (5 or above) polynomials cannot in general be expressed simply using nth roots. Therefore, general algorithms to find eigenvectors and eigenvalues are iterative.Suppose that we want to compute the eigenvalues of a given matrix. If the matrix is small, we can compute them symbolically using the characteristic polynomial. However, this is often impossible for larger matrices, in which case we must use a numerical method.Note that each eigenvalue is multiplied by ni, the algebraic multiplicity.Note that each eigenvalue is raised to the power ni, the algebraic multiplicity.where Q is an orthogonal matrix whose columns are the eigenvectors of A, and \u039b is a diagonal matrix whose entries are the eigenvalues of A.As a special case, for every N\u00d7N real symmetric matrix, the eigenvalues are real and the eigenvectors can be chosen such that they are orthogonal to each other. Thus a real symmetric matrix A can be decomposed asfrom above. Once again, we find thatA similar technique works more generally with the holomorphic functional calculus, usingThe off-diagonal elements of f(\u039b) are zero; that is, f(\u039b) is also a diagonal matrix. Therefore, calculating f(A) reduces to just calculating the function on each of the eigenvalues.Because \u039b is a diagonal matrix, functions of \u039b are very easy to calculate:then we know thatThe eigendecomposition allows for much easier computation of power series of matrices. If f(x) is given bywhere the eigenvalues are subscripted with an 's' to denote being sorted. The position of the minimization is the lowest reliable eigenvalue. In measurement systems, the square root of this reliable eigenvalue is the average noise over the components of the system.If the eigenvalues are rank-sorted by value, then the reliable eigenvalue can be found by minimization of the Laplacian of the sorted eigenvalues:[5]The reliable eigenvalue can be found by assuming that eigenvalues of extremely similar and low value are a good representation of measurement noise (which is assumed low for most systems).The second mitigation extends the eigenvalue so that lower values have much less influence over inversion, but do still contribute, such that solutions near the noise will still be found.The first mitigation method is similar to a sparse sample of the original matrix, removing components that are not considered valuable. However, if the solution or detection process is near the noise level, truncating may remove components that influence the desired solution.Two mitigations have been proposed: 1) truncating small/zero eigenvalues, 2) extending the lowest reliable eigenvalue to those below it.When eigendecomposition is used on a matrix of measured, real data, the inverse may be less valid when all eigenvalues are used unmodified in the form above. This is because as eigenvalues become relatively small, their contribution to the inversion is large. Those near zero or at the \"noise\" of the measurement system will have undue influence and could hamper solutions (detection) using the inverse.Furthermore, because \u039b is a diagonal matrix, its inverse is easy to calculate:If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is nonsingular and its inverse is given byPutting the solutions back into the above simultaneous equationsThusAnd can be represented by a single vector equation involving 2 solutions as eigenvalues:The above equation can be decomposed into 2 simultaneous equations:ThenThe eigenvectors can be indexed by eigenvalues, i.e. using a double index, with vi,j being the jth eigenvector for the ith eigenvalue. The eigenvectors can also be indexed using the simpler notation of a single index vk, with k = 1, 2, ..., Nv.There will be 1 \u2264 mi \u2264 ni linearly independent solutions to each eigenvalue equation. The linear combinations of the mi solutions are the eigenvectors associated with the eigenvalue \u03bbi. The integer mi is termed the geometric multiplicity of \u03bbi. It is important to keep in mind that the algebraic multiplicity ni and geometric multiplicity mi may or may not be equal, but we always have mi \u2264 ni. The simplest case is of course when mi = ni = 1. The total number of linearly independent eigenvectors, Nv, can be calculated by summing the geometric multiplicitiesFor each eigenvalue, \u03bbi, we have a specific eigenvalue equationThe integer ni is termed the algebraic multiplicity of eigenvalue \u03bbi. The algebraic multiplicities sum to N:We can factor p asWe call p(\u03bb) the characteristic polynomial, and the equation, called the characteristic equation, is an Nth order polynomial equation in the unknown \u03bb. This equation will have N\u03bb distinct solutions, where 1 \u2264 N\u03bb \u2264 N . The set of solutions, that is, the eigenvalues, is called the spectrum of A.[1][2][3]This yields an equation for the eigenvaluesA (non-zero) vector v of dimension N is an eigenvector of a square (N\u00d7N) matrix A if it satisfies the linear equationIn linear algebra, eigendecomposition or sometimes spectral decomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way.",
            "title": "Eigendecomposition of a matrix",
            "url": "https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Square matrix",
            "url": "https://en.wikipedia.org/wiki/Square_matrix"
        },
        {
            "desc_links": [
                "/wiki/Dual_space",
                "/wiki/Inner_product",
                "/wiki/Vector_space",
                "/wiki/Row_and_column_spaces",
                "/wiki/Transpose",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)"
            ],
            "links": [
                "/wiki/Stochastic_matrix",
                "/wiki/Stochastic_vector",
                "/wiki/Ludwik_Silberstein",
                "/wiki/List_of_important_publications_in_physics#Special",
                "/wiki/McGraw-Hill",
                "/wiki/Heinrich_Guggenheimer",
                "/wiki/University_of_Minnesota",
                "/wiki/H._S._M._Coxeter",
                "/wiki/Rafael_Artzy",
                "/wiki/Galois_geometry",
                "/wiki/Transpose",
                "/wiki/Antihomomorphism",
                "/wiki/Dual_space",
                "/wiki/Dual_space#Transpose_of_a_linear_map",
                "/wiki/Matrix_product",
                "/wiki/Dyadic_product",
                "/wiki/Tensor_product",
                "/wiki/Dot_product",
                "/wiki/Matrix_multiplication",
                "/wiki/Comma",
                "/wiki/Semicolon",
                "/wiki/Dual_space",
                "/wiki/Inner_product",
                "/wiki/Vector_space",
                "/wiki/Row_and_column_spaces",
                "/wiki/Transpose",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)"
            ],
            "text": "In the study of stochastic processes with a stochastic matrix, it is conventional to use a row vector as the stochastic vector.[5]Ludwik Silberstein used row vectors for spacetime events; he applied Lorentz transformation matrices on the right in his Theory of Relativity in 1914 (see page 143). In 1963 when McGraw-Hill published Differential Geometry by Heinrich Guggenheimer of the University of Minnesota, he uses the row vector convention in chapter 5, \"Introduction to transformation groups\" (eqs. 7a,9b and 12 to 15). When H. S. M. Coxeter reviewed[3] Linear Geometry by Rafael Artzy, he wrote, \"[Artzy] is to be congratulated on his choice of the 'left-to-right' convention, which enables him to regard a point as a row matrix instead of the clumsy column that many authors prefer.\" J.W.P. Hirschfeld used right multiplication of row vectors by matrices in his description of projectivities on the Galois geometry PG(1,q).[4](The Greek letters represent row vectors).For an instance where this row vector input convention has been used to good effect see Raiz Usmani,[2] where on page 106 the convention allows the statement \"The product mapping ST of U into W [is given] by:Nevertheless, using the transpose operation these differences between inputs of a row or column nature are resolved by an antihomomorphism between the groups arising on the two sides. The technical construction uses the dual space associated with a vector space to develop the transpose of a linear map.leading to the algebraic expression QM v for the composed output from v input. The matrix transformations mount up to the left in this use of a column vector for input to matrix transformation.In contrast, when a column vector is transformed to become another column under an n \u00d7 n matrix action, the operation occurs to the left,Conveniently, one can write t = p Q = v MQ telling us that the matrix product transformation MQ can take v directly to t. Continuing with row vectors, matrix transformations further reconfiguring n-space can be applied to the right of previous outputs.Then p is also a row vector and may present to another n \u00d7 n matrix Q,Frequently a row vector presents itself for an operation within n-space expressed by an n \u00d7 n matrix M,In this case the two matrices are different: they are transposes of each other.which is not equivalent to the matrix product of the column vector representation of b and the row vector representation of a,The matrix product of a column and a row vector gives the dyadic product of two vectors a and b, an example of the more general tensor product. The matrix product of the column vector representation of a and the row vector representation of b gives the components of their dyadic product,which is also equivalent to the matrix product of the row vector representation of b and the column vector representation of a,The dot product of two vectors a and b is equivalent to the matrix product of the row vector representation of a and the column vector representation of b,Matrix multiplication involves the action of multiplying each row vector of one matrix by each column vector of another matrix.Some authors also use the convention of writing both column vectors and row vectors as rows, but separating row vector elements with commas and column vector elements with semicolons (see alternative notation 2 in the table below).orTo simplify writing column vectors in-line with other text, sometimes they are written as row vectors with the transpose operation applied to them.The column space can be viewed as the dual space to the row space, since any linear functional on the space of column vectors can be represented uniquely as an inner product with a specific row vector.The set of all row vectors forms a vector space called row space, similarly the set of all column vectors forms a vector space called column space. The dimensions of the row and column spaces equals the number of entries in the row or column vector.and the transpose of a column vector is a row vectorThroughout, boldface is used for the row and column vectors. The transpose (indicated by T) of a row vector is a column vectorSimilarly, a row vector or row matrix is a 1 \u00d7 m matrix, that is, a matrix consisting of a single row of m elements[1]In linear algebra, a column vector or column matrix is an m \u00d7 1 matrix, that is, a matrix consisting of a single column of m elements,",
            "title": "Row and column vectors",
            "url": "https://en.wikipedia.org/wiki/Row_and_column_vectors"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Matrix multiplication",
            "url": "https://en.wikipedia.org/wiki/Matrix-vector_multiplication"
        },
        {
            "desc_links": [
                "/wiki/Scalar_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Tensor",
                "/wiki/Quaternion",
                "/wiki/Inner_product",
                "/wiki/Scalar_multiplication",
                "/wiki/Inner_product_space",
                "/wiki/Linear_algebra",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Vector_(mathematics_and_physics)"
            ],
            "links": [
                "/wiki/Scaling_(geometry)",
                "/wiki/Linear_transformation",
                "/wiki/Manifold",
                "/wiki/Section_(fiber_bundle)",
                "/wiki/Tangent_bundle",
                "/wiki/Algebra",
                "/wiki/Ring_(mathematics)",
                "/wiki/Commutative",
                "/wiki/Module_(mathematics)",
                "/wiki/Norm_(mathematics)",
                "/wiki/Normed_vector_space",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Isomorphism",
                "/wiki/Coordinate_vector_space",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Rational_number",
                "/wiki/Algebraic_number",
                "/wiki/Finite_field",
                "/wiki/Oxford_English_Dictionary",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Quaternion",
                "/wiki/Latin_language",
                "/wiki/Fran%C3%A7ois_Vi%C3%A8te",
                "/wiki/Wikipedia:Citing_sources",
                "/wiki/Scalar_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Tensor",
                "/wiki/Quaternion",
                "/wiki/Inner_product",
                "/wiki/Scalar_multiplication",
                "/wiki/Inner_product_space",
                "/wiki/Linear_algebra",
                "/wiki/Scalar_multiplication",
                "/wiki/Field_(mathematics)",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Vector_(mathematics_and_physics)"
            ],
            "text": "Operations that apply to a single value at a time.The scalar multiplication of vector spaces and modules is a special case of scaling, a kind of linear transformation.In this case the \"scalars\" may be complicated objects. For instance, if R is a ring, the vectors of the product space Rn can be made into a module with the n\u00d7n matrices with entries from R as the scalars. Another example comes from manifold theory, where the space of sections of the tangent bundle forms a module over the algebra of real functions on the manifold.When the requirement that the set of scalars form a field is relaxed so that it need only form a ring (so that, for example, the division of scalars need not be defined, or the scalars need not be commutative), the resulting more general algebraic structure is called a module.The norm is usually defined to be an element of V's scalar field K, which restricts the latter to fields that support the notion of sign. Moreover, if V has dimension 2 or more, K must be closed under square root, as well as the four arithmetic operations; thus the rational numbers Q are excluded, but the surd field is acceptable. For this reason, not every scalar product space is a normed vector space.Alternatively, a vector space V can be equipped with a norm function that assigns to every vector v in V a scalar ||v||. By definition, multiplying v by a scalar k also multiplies its norm by |k|. If ||v|| is interpreted as the length of v, this operation can be described as scaling the length of v by k. A vector space equipped with a norm is called a normed vector space (or normed linear space).According to a fundamental theorem of linear algebra, every vector space has a basis. It follows that every vector space over a scalar field K is isomorphic to a coordinate vector space where the coordinates are elements of K. For example, every real vector space of dimension n is isomorphic to n-dimensional real space Rn.The scalars can be taken from any field, including the rational, algebraic, real, and complex numbers, as well as finite fields.According to a citation in the Oxford English Dictionary the first recorded usage of the term \"scalar\" in English came with W. R. Hamilton in 1846, referring to the real part of a quaternion:The word scalar derives from the Latin word scalaris, an adjectival form of scala (Latin for \"ladder\"), from which the English word scale also comes. The first recorded usage of the word \"scalar\" in mathematics occurs in Fran\u00e7ois Vi\u00e8te's Analytic Art (In artem analyticem isagoge) (1591):[5][page\u00a0needed][6]The term scalar matrix is used to denote a matrix of the form kI where k is a scalar and I is the identity matrix.The term is also sometimes used informally to mean a vector, matrix, tensor, or other usually \"compound\" value that is actually reduced to a single component. Thus, for example, the product of a 1\u00d7n matrix and an n\u00d71 matrix, which is formally a 1\u00d71 matrix, is often said to be a scalar.The real component of a quaternion is also called its scalar part.A scalar product operation \u2013\u00a0not to be confused with scalar multiplication\u00a0\u2013 may be defined on a vector space, allowing two vectors to be multiplied to produce a scalar. A vector space equipped with a scalar product is called an inner product space.In linear algebra, real numbers or other elements of a field are called scalars and relate to vectors in a vector space through the operation of scalar multiplication, in which a vector can be multiplied by a number to produce another vector.[2][3][4] More generally, a vector space may be defined by using any field instead of real numbers, such as complex numbers. Then the scalars of that vector space will be the elements of the associated field.A scalar is an element of a field which is used to define a vector space. A quantity described by multiple scalars, such as having both direction and magnitude, is called a vector.[1]",
            "title": "Scalar (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Scalar_(mathematics)"
        },
        {
            "desc_links": [
                "/wiki/Geometry",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Functional_analysis",
                "/wiki/Engineering",
                "/wiki/Mathematical_model",
                "/wiki/Nonlinear_system",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Mathematics",
                "/wiki/Linear_equation"
            ],
            "links": [
                "/wiki/Algebraic_geometry",
                "/wiki/Systems_of_polynomial_equations",
                "/wiki/Representation_theory",
                "/wiki/Functional_analysis",
                "/wiki/Mathematical_analysis",
                "/wiki/Lp_space",
                "/wiki/Algebra_over_a_field",
                "/wiki/Multilinear_algebra",
                "/wiki/Dual_space",
                "/wiki/Tensor_product",
                "/wiki/Module_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Free_module",
                "/wiki/Linear_map",
                "/wiki/Linear_functional",
                "/wiki/Cramer%27s_rule",
                "/wiki/System_of_linear_equations",
                "/wiki/Homogeneous_coordinates",
                "/wiki/Linear_equation",
                "/wiki/Inner_product_space",
                "/wiki/Partial_differential_equation",
                "/wiki/Dirichlet_conditions",
                "/wiki/Fourier_series",
                "/wiki/Linear_least_squares_(mathematics)",
                "/wiki/Triangular_form",
                "/wiki/Normal_matrix",
                "/wiki/Hermitian_conjugate",
                "/wiki/Cauchy%E2%80%93Schwarz_inequality",
                "/wiki/Axiom",
                "/wiki/Inner_product",
                "/wiki/Bilinear_form",
                "/wiki/Diagonalizable_matrix",
                "/wiki/Diagonal_matrix",
                "/wiki/Identity_matrix",
                "/wiki/Polynomial",
                "/wiki/Algebraically_closed_field",
                "/wiki/Complex_number",
                "/wiki/Invariant_(mathematics)#Invariant_set",
                "/wiki/Eigenvalues_and_eigenvectors",
                "/wiki/Characteristic_value",
                "/wiki/Determinant",
                "/wiki/Invertible_matrix",
                "/wiki/Inverse_element",
                "/wiki/Nullspace",
                "/wiki/Cramer%27s_rule",
                "/wiki/Gaussian_elimination",
                "/wiki/Standard_basis",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Similar_(linear_algebra)",
                "/wiki/Coordinate_system",
                "/wiki/Linear_combination",
                "/wiki/Cardinality",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Well-defined",
                "/wiki/Dimension_theorem_for_vector_spaces",
                "/wiki/Linearly_independent",
                "/wiki/Basis_(linear_algebra)",
                "/wiki/Axiom_of_choice",
                "/wiki/Rationals",
                "/wiki/Linear_span",
                "/wiki/Linear_subspace",
                "/wiki/Nullspace",
                "/wiki/Linear_combination",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Origin_(mathematics)",
                "/wiki/Bijective",
                "/wiki/Isomorphic",
                "/wiki/Determinant",
                "/wiki/Range_(mathematics)",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Linear_transformation",
                "/wiki/Map_(mathematics)",
                "/wiki/Abelian_group",
                "/wiki/Sequence",
                "/wiki/Function_(mathematics)",
                "/wiki/Polynomial_ring",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Set_(mathematics)",
                "/wiki/Binary_operation",
                "/wiki/Element_(mathematics)",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Axiom",
                "/wiki/School_Mathematics_Study_Group",
                "/wiki/Secondary_school",
                "/wiki/Singular-value_decomposition",
                "/wiki/Determinants",
                "/wiki/Gaussian_elimination",
                "/wiki/H%C3%BCseyin_Tevfik_Pasha",
                "/wiki/Peano",
                "/wiki/Abstract_algebra",
                "/wiki/Quantum_mechanics",
                "/wiki/Special_relativity",
                "/wiki/Statistics",
                "/wiki/Algorithm",
                "/wiki/Hermann_Grassmann",
                "/wiki/James_Joseph_Sylvester",
                "/wiki/Arthur_Cayley",
                "/wiki/Determinant",
                "/wiki/Systems_of_linear_equations",
                "/wiki/Gottfried_Wilhelm_Leibniz",
                "/wiki/Gabriel_Cramer",
                "/wiki/Cramer%27s_Rule",
                "/wiki/Gauss",
                "/wiki/Gaussian_elimination",
                "/wiki/Geodesy",
                "/wiki/Geometry",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Rotation_(mathematics)",
                "/wiki/Functional_analysis",
                "/wiki/Engineering",
                "/wiki/Mathematical_model",
                "/wiki/Nonlinear_system",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Linear_map",
                "/wiki/Mathematics",
                "/wiki/Linear_equation"
            ],
            "text": "There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to.Algebraic geometry considers the solutions of systems of polynomial equations.Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces.If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \u00d7 V \u2192 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V\u2217 consisting of linear maps f: V \u2192 F where F is the field of scalars. Multilinear maps T: Vn \u2192 F can be described via tensor products of elements of V\u2217.Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.The set of points of a linear functional that map to zero define the kernel of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.[25][27]Notice that if h is a solution to this homogeneous equation, then t h is also a solution.The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,For convenience the free parameter x has been relabeled t.Solve for y and obtain the inverse image as the set of points,In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equationNotice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c.The set of points in the plane E that map to the same image in R under the linear functional \u03bb define a line in E. This line is the image of the inverse map, \u03bb\u22121: R\u2192E. This inverse image is the set of the points x=(x, y) that solve the equation,Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.[25][27]These equations can be assembled into the single matrix equation,These coordinate functionals have the properties,which can be written in matrix form asThe functionals \u03c3 and \u03c4 compute the components of x along the basis vectors v and w, respectively, that is,To solve this equation for \u03b1, \u03b2, we compute the linear coordinate functionals \u03c3 and \u03c4 for the basis v, w, which are given by,[26]This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E. Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is to find the real numbers \u03b1, \u03b2, so that x=\u03b1v+\u03b2w, that iswhere Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form asThis is true for any pair of vectors used to define coordinates in E. Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E. This means a vector x has coordinates (\u03b1,\u03b2), such that x=\u03b1v+\u03b2w. Then, we have the linear functionalThus, the columns of the matrix A are the image of the basis vectors of E in R.Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj. It is now possible to see thatThis shows that the sum of vectors in E map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation.[25] For this case, where the image space is a real number the map is called a linear functional.[27]This transformation has the important property that if Ay=d, thenorAnother way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in E and consider the linear function \u03bb: E\u2192R, given byClearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E. For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.which in homogeneous form yields,It is interesting to consider the case of three lines, \u03bb1, \u03bb2 and \u03bb3, which yield the matrix equation,if the rows of B are linearly independent (i.e., \u03bb1 and \u03bb2 represent distinct lines). Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns.[27] Notice that this yields a point in the z = 1 plane only when the 2\u2009\u00d7\u20092 submatrix associated with x3 has a non-zero determinant.The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates, the solutions are multiples of the following solution:[26]or using homogeneous coordinates,which forms a system of linear equations. The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,Now consider the equations of the two lines \u03bb1 and \u03bb2,The linear equation, \u03bb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \u03b1x1 + \u03b2x2 is also on the line, for any real \u03b1 and \u03b2.Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space. The x\u2212y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1).where x = (x, y, 1) is the 3\u2009\u00d7\u20091 set of homogeneous coordinates associated with the point (x, y).[26]orwhere a, b and c are not all zero. Then,Point coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation[25]Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E. When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.The functions gn(x) = sin(nx) for n > 0 and hn(x) = cos(nx) for n \u2265 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient ak, we take the inner product with hk:The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the \"same\" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner productThis series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.Fourier series are a representation of a function f: [\u2212\u03c0, \u03c0] \u2192 R as a trigonometric series:The least squares method is used to determine the best-fit line for a set of data.[24] This line will minimize the sum of the squares of the residuals.We can, in general, write any system of linear equations as a matrix equation:The system is solved.Next, z and y can be substituted into L1, which can be solved to obtainThen, z can be substituted into L2, which can then be solved to obtainThe last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen thatThis result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.The result is:Now y is eliminated from L3 by adding \u22124L2 to L3:The result is:In the example, x is eliminated from L2 by adding (3/2)L1 to L2. x is then eliminated from L3 by adding L1 to L3. Formally:The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.If T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.The inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfyingand so we can call this quantity the cosine of the angle between the two vectors.In particular, the quantityand we can prove the Cauchy\u2013Schwarz inequality:We can define the length of a vector v in V byNote that in R, it is symmetric.that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[21][22]Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a mapSuch a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).where I is the identity matrix. For there to be nontrivial solutions to that equation, det(T \u2212 \u03bb I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ..., vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \u03bb1, \u03bb2, ..., \u03bbn, and if v = a1v1 + ... + an vn, then,To find an eigenvector or an eigenvalue, we note thatIn general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T. If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector. The scalar \u03bb such that Tv = \u03bbv is called a characteristic value or eigenvalue of T.One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse[20]). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.There is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V. While Rn has a standard basis {e1, e2, ..., en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V).The condition that v1, v2, ..., vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ..., vn assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to v). In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn. Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn. Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \u2192 W may be encoded by an m \u00d7 n matrix A with entries in the field F, called the matrix of T with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.A particular basis {v1, v2, ..., vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ..., an) is the linear combinationOne often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic,[19] giving an easy way of characterizing isomorphism.Any two bases of a vector space V have the same cardinality, which is called the dimension of V. The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of V has finite number of elements, V is called a finite-dimensional vector space. If V is finite-dimensional and U is a subspace of V, then dim U \u2264 dim V. If U1 and U2 are subspaces of V, thenA linear combination of any system of vectors with all zero coefficients is the zero vector of V. If this is the only way to express the zero vector as a linear combination of v1, v2, ..., vk then these vectors are linearly independent. Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V. Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis.[16] It turns out that if we accept the axiom of choice, every vector space has a basis;[17] nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed.where a1, a2, ..., ak are scalars. The set of all linear combinations of vectors v1, v2, ..., vk is called their span, which forms a subspace.Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ..., vk:Linear transformations have geometric significance. For example, 2 \u00d7 2 real matrices denote standard planar mappings that preserve the origin.When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.Additionally for any vectors u, v \u2208 V and scalars a, b \u2208 F:for any vectors u,v \u2208 V and a scalar a \u2208 F.that is compatible with addition and scalar multiplication:Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a mapThe first four axioms are those of V being an abelian group under vector addition. Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.The main structures of linear algebra are vector spaces. A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms. Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The operations of addition and multiplication in a vector space must satisfy the following axioms.[15] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.Linear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s.[7] Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s.[8] In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum.[9] In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation.[10] Reviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas,[11] and to include the jewel in the crown of linear algebra, the singular-value decomposition (SVD), as 'so many other disciplines use it'.[12] To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination.[13][14]The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.In 1882, H\u00fcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\".[5][6] The first modern and more precise definition of a vector space was introduced by Peano in 1888;[4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.[4]The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".[4]The study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations. Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's Rule. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.[4]Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first approximation.and their representations through matrices and vector spaces.[1][2][3]linear functions such asLinear algebra is the branch of mathematics concerning linear equations such as",
            "title": "Linear algebra",
            "url": "https://en.wikipedia.org/wiki/Linear_algebra"
        },
        {
            "desc_links": [
                "/wiki/Abstract_algebra",
                "/wiki/Module_homomorphism",
                "/wiki/Category_theory",
                "/wiki/Morphism",
                "/wiki/Category_of_modules",
                "/wiki/Ring_(mathematics)",
                "/wiki/Map_(mathematics)",
                "/wiki/Origin_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Straight_line",
                "/wiki/Point_(geometry)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rotation_and_reflection_linear_transformations",
                "/wiki/Endomorphism",
                "/wiki/Linear_function",
                "/wiki/Analytic_geometry",
                "/wiki/Mathematics",
                "/wiki/Transformation_(function)",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Scalar_(mathematics)"
            ],
            "links": [
                "/wiki/Compiler_optimizations",
                "/wiki/Parallelizing_compiler",
                "/wiki/Computer_graphics",
                "/wiki/Transformation_matrix",
                "/wiki/Topological_vector_space",
                "/wiki/Normed_space",
                "/wiki/Continuous_function_(topology)",
                "/wiki/Continuous_linear_operator",
                "/wiki/Bounded_operator",
                "/wiki/Discontinuous_linear_operator",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Tensor",
                "/wiki/Endomorphism",
                "/wiki/Covariance_and_contravariance_of_vectors",
                "/wiki/Euler_characteristic",
                "/wiki/Operator_theory",
                "/wiki/Fredholm",
                "/wiki/Atiyah%E2%80%93Singer_index_theorem",
                "/wiki/Quotient_space_(linear_algebra)",
                "/wiki/Exact_sequence",
                "/wiki/Rank_of_a_matrix",
                "/wiki/Kernel_(matrix)#Subspace_properties",
                "/wiki/Linear_subspace",
                "/wiki/Dimension",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Kernel_(linear_operator)",
                "/wiki/Image_(mathematics)",
                "/wiki/Range_(mathematics)",
                "/wiki/Isomorphism",
                "/wiki/Associative_algebra",
                "/wiki/Group_isomorphism",
                "/wiki/General_linear_group",
                "/wiki/Isomorphism",
                "/wiki/Automorphism",
                "/wiki/Group_(math)",
                "/wiki/Automorphism_group",
                "/wiki/Endomorphisms",
                "/wiki/Unit_(ring_theory)",
                "/wiki/Endomorphism",
                "/wiki/Associative_algebra",
                "/wiki/Ring_(algebra)",
                "/wiki/Identity_function",
                "/wiki/Matrix_multiplication",
                "/wiki/Matrix_addition",
                "/wiki/Associative_algebra",
                "/wiki/Composition_of_maps",
                "/wiki/Pointwise",
                "/wiki/Inverse_function",
                "/wiki/Relation_composition",
                "/wiki/Class_(set_theory)",
                "/wiki/Morphism",
                "/wiki/Category_(mathematics)",
                "/wiki/Dimension",
                "/wiki/2_%C3%97_2_real_matrices",
                "/wiki/Finite-dimensional",
                "/wiki/Basis_of_a_vector_space",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Euclidean_space",
                "/wiki/Abstract_algebra",
                "/wiki/Module_homomorphism",
                "/wiki/Category_theory",
                "/wiki/Morphism",
                "/wiki/Category_of_modules",
                "/wiki/Ring_(mathematics)",
                "/wiki/Map_(mathematics)",
                "/wiki/Origin_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Straight_line",
                "/wiki/Point_(geometry)",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Rotation_and_reflection_linear_transformations",
                "/wiki/Endomorphism",
                "/wiki/Linear_function",
                "/wiki/Analytic_geometry",
                "/wiki/Mathematics",
                "/wiki/Transformation_(function)",
                "/wiki/Map_(mathematics)",
                "/wiki/Module_(mathematics)",
                "/wiki/Vector_space",
                "/wiki/Scalar_(mathematics)"
            ],
            "text": "Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A linear transformation between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9] An infinite-dimensional domain may have discontinuous linear operators.Therefore, linear maps are said to be 1-co 1-contra -variant objects, or type (1, 1) tensors.Therefore, the matrix in the new basis is A\u2032 = B\u22121AB, being B the matrix of the given basis.henceSubstituting this in the first expressionGiven a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Let V and W denote vector spaces over a field, F. Let T: V \u2192 W be a linear map.No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 \u2192 V \u2192 W \u2192 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah\u2013Singer index theorem.[8]For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) \u2212 dim(W), by rank\u2013nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.namely the degrees of freedom minus the number of constraints.For a linear operator with finite-dimensional kernel and co-kernel, one may define index as:The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.These can be interpreted thus: given a linear equation f(v) = w to solve,This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target. Formally, one has the exact sequenceThe number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, \u03c1(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or \u03bd(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.ker(f) is a subspace of V and im(f) is a subspace of W. The following dimension formula is known as the rank\u2013nullity theorem:If f\u00a0: V \u2192 W is linear, we define the kernel and the image or range of f byIf V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n \u00d7 n matrices with entries in K. The automorphism group of V is isomorphic to the general linear group GL(n, K) of all n \u00d7 n invertible matrices with entries in K.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).A linear transformation f: V \u2192 V is an endomorphism of V; the set of all such endomorphisms End(V) together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V \u2192 V.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W). Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.If f\u00a0: V \u2192 W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.If f1\u00a0: V \u2192 W and f2\u00a0: V \u2192 W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).The inverse of a linear map, when defined, is again a linear map.The composition of linear maps is linear: if f\u00a0: V \u2192 W and g\u00a0: W \u2192 Z are linear, then so is their composition g \u2218 f\u00a0: V \u2192 Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.In two-dimensional space R2 linear maps are described by 2 \u00d7 2 real matrices. These are some examples:The matrices of a linear transformation can be represented visually:where M is the matrix of f. The symbol \u2217 denotes that there are other columns which together with column j make up a total of n columns of M. In other words, every column j = 1, ..., n has a corresponding vector f(vj) whose coordinates a1j, ..., amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.corresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),Thus, the function f is entirely determined by the values of aij. If we put these values into an m \u00d7 n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V. To get M, every column j of M is a vectorwhich implies that the function f is entirely determined by the vectors f(v1), ..., f(vn). Now let {w1, ..., wm} be a basis for W. Then we can represent each vector f(vj) asIf f\u00a0: V \u2192 W is a linear map,Let {v1, ..., vn} be a basis for V. Then every vector v in V is uniquely determined by the coefficients c1, ..., cn in the field R:If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m \u00d7 n matrix, then f(x) = Ax describes a linear map Rn \u2192 Rm (see Euclidean space).Thus, a linear map is said to be operation preserving. In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.In the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of\u00a0V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V \u2192 W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.",
            "title": "Linear map",
            "url": "https://en.wikipedia.org/wiki/Linear_map"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Science",
                "/wiki/Engineering",
                "/wiki/System_of_linear_equations",
                "/wiki/Fourier_series",
                "/wiki/Image_compression",
                "/wiki/Partial_differential_equation",
                "/wiki/Coordinate-free",
                "/wiki/Tensor",
                "/wiki/Manifold_(mathematics)",
                "/wiki/Abstract_algebra",
                "/wiki/Analytic_geometry",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Linear_equation",
                "/wiki/Giuseppe_Peano",
                "/wiki/Euclidean_space",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Linear_algebra",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Mathematical_analysis",
                "/wiki/Function_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Topology",
                "/wiki/Continuous_function",
                "/wiki/Norm_(mathematics)",
                "/wiki/Inner_product",
                "/wiki/Metric_(mathematics)",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_vector",
                "/wiki/Physics",
                "/wiki/Force",
                "/wiki/Force_vector",
                "/wiki/Geometry",
                "/wiki/Three-dimensional_space",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Axiom"
            ],
            "links": [
                "/wiki/Parallel_(geometry)",
                "/wiki/Grassmannian_manifold",
                "/wiki/Flag_manifold",
                "/wiki/Flag_(linear_algebra)",
                "/wiki/Nullspace",
                "/wiki/Coset",
                "/wiki/Transitive_group_action",
                "/wiki/Group_action",
                "/wiki/Ring_(mathematics)",
                "/wiki/Multiplicative_inverse",
                "/wiki/Abelian_group",
                "/wiki/Modular_arithmetic",
                "/wiki/Free_module",
                "/wiki/Module_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Division_ring",
                "/wiki/Spectrum_of_a_ring",
                "/wiki/Locally_free_module",
                "/wiki/Cotangent_bundle",
                "/wiki/Cotangent_space",
                "/wiki/Section_(fiber_bundle)",
                "/wiki/Differential_form",
                "/wiki/Tangent_bundle",
                "/wiki/Tangent_space",
                "/wiki/Vector_field",
                "/wiki/Hairy_ball_theorem",
                "/wiki/2-sphere",
                "/wiki/K-theory",
                "/wiki/Division_algebra",
                "/wiki/Quaternion",
                "/wiki/Octonion",
                "/wiki/Fiber_(mathematics)",
                "/wiki/Line_bundle",
                "/wiki/Trivial_bundle",
                "/wiki/Locally",
                "/wiki/Neighborhood_(topology)",
                "/wiki/M%C3%B6bius_strip",
                "/wiki/Homeomorphism#Examples",
                "/wiki/Cylinder_(geometry)",
                "/wiki/Orientable_manifold",
                "/wiki/Topological_space",
                "/wiki/Riemannian_manifold",
                "/wiki/Riemannian_metric",
                "/wiki/Riemann_curvature_tensor",
                "/wiki/Curvature_(mathematics)",
                "/wiki/General_relativity",
                "/wiki/Einstein_curvature_tensor",
                "/wiki/Space-time",
                "/wiki/Compact_Lie_group",
                "/wiki/Tangent_plane",
                "/wiki/Linear_approximation",
                "/wiki/Linearization",
                "/wiki/Differentiable_manifold",
                "/wiki/Fast_Fourier_transform",
                "/wiki/Convolution_theorem",
                "/wiki/Convolution",
                "/wiki/Digital_filter",
                "/wiki/Multiplication_algorithm",
                "/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm",
                "/wiki/Boundary_value_problem",
                "/wiki/Partial_differential_equation",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_equation",
                "/wiki/Sampling_(signal_processing)",
                "/wiki/Discrete_Fourier_transform",
                "/wiki/Digital_signal_processing",
                "/wiki/Radar",
                "/wiki/Speech_encoding",
                "/wiki/Image_compression",
                "/wiki/JPEG",
                "/wiki/Discrete_cosine_transform",
                "/wiki/Superposition_principle",
                "/wiki/Sine_waves",
                "/wiki/Frequency_spectrum",
                "/wiki/Duality_(mathematics)",
                "/wiki/Pontryagin_duality",
                "/wiki/Group_(mathematics)",
                "/wiki/Reciprocal_lattice",
                "/wiki/Lattice_(group)",
                "/wiki/Atom",
                "/wiki/Crystal",
                "/wiki/Fourier_coefficient",
                "/wiki/Periodic_function",
                "/wiki/Trigonometric_functions",
                "/wiki/Fourier_series",
                "/wiki/Hilbert_space",
                "/wiki/Fourier_expansion",
                "/wiki/Dirac_distribution",
                "/wiki/Green%27s_function",
                "/wiki/Fundamental_solution",
                "/wiki/Lax%E2%80%93Milgram_theorem",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Test_function",
                "/wiki/Smooth_function",
                "/wiki/Compact_support",
                "/wiki/Optimization_(mathematics)",
                "/wiki/Minimax_theorem",
                "/wiki/Game_theory",
                "/wiki/Representation_theory",
                "/wiki/Group_theory",
                "/wiki/Distributive_law",
                "/wiki/Symmetric_algebra",
                "/wiki/Exterior_algebra",
                "/wiki/Tensor_algebra",
                "/wiki/Tensor",
                "/wiki/Commutator",
                "/wiki/Cross_product",
                "/wiki/Commutative_algebra",
                "/wiki/Polynomial_ring",
                "/wiki/Commutative",
                "/wiki/Associative",
                "/wiki/Quotient_ring",
                "/wiki/Algebraic_geometry",
                "/wiki/Coordinate_ring",
                "/wiki/Bilinear_operator",
                "/wiki/Banach_algebra",
                "/wiki/Differential_equation",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Quantum_mechanics",
                "/wiki/Partial_differential_equation",
                "/wiki/Wavefunction",
                "/wiki/Eigenvalue",
                "/wiki/Differential_operator",
                "/wiki/Eigenstate",
                "/wiki/Spectral_theorem",
                "/wiki/Compact_operator",
                "/wiki/Taylor_approximation",
                "/wiki/Differentiable_function",
                "/wiki/Stone%E2%80%93Weierstrass_theorem",
                "/wiki/Trigonometric_function",
                "/wiki/Fourier_expansion",
                "/wiki/Closure_(topology)",
                "/wiki/Hilbert_space_dimension",
                "/wiki/Gram%E2%80%93Schmidt_process",
                "/wiki/Orthogonal_basis",
                "/wiki/Euclidean_space",
                "/wiki/David_Hilbert",
                "/wiki/Derivative",
                "/wiki/Sobolev_space",
                "/wiki/Integrable_function",
                "/wiki/Domain_(mathematics)",
                "/wiki/Lp_space",
                "/wiki/Riemann_integral",
                "/wiki/Lebesgue_integral",
                "/wiki/Zero_vector",
                "/wiki/Stefan_Banach",
                "/wiki/Lp_space",
                "/wiki/P-norm",
                "/wiki/Functional_(mathematics)",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Functional_analysis",
                "/wiki/Cauchy_sequence",
                "/wiki/Completeness_(topology)",
                "/wiki/Topology_of_uniform_convergence",
                "/wiki/Weierstrass_approximation_theorem",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Function_space",
                "/wiki/Function_series",
                "/wiki/Modes_of_convergence",
                "/wiki/Pointwise_convergence",
                "/wiki/Uniform_convergence",
                "/wiki/Series_(mathematics)",
                "/wiki/Infinite_sum",
                "/wiki/Topological_space",
                "/wiki/Neighborhood_(topology)",
                "/wiki/Continuous_map",
                "/wiki/Law_of_cosines",
                "/wiki/Dot_product",
                "/wiki/Partial_order",
                "/wiki/Ordered_vector_space",
                "/wiki/Riesz_space",
                "/wiki/Lebesgue_integration",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Infinite_series",
                "/wiki/Functional_analysis",
                "/wiki/Tuple",
                "/wiki/Function_composition",
                "/wiki/Universal_property",
                "/wiki/Tensor",
                "/wiki/Multilinear_algebra",
                "/wiki/Cartesian_product",
                "/wiki/Bilinear_map",
                "/wiki/Derivative",
                "/wiki/Linear_differential_operator",
                "/wiki/Group_(mathematics)",
                "/wiki/Kernel_(algebra)",
                "/wiki/Image_(mathematics)",
                "/wiki/Category_of_vector_spaces",
                "/wiki/Abelian_category",
                "/wiki/Category_(mathematics)",
                "/wiki/Category_of_abelian_groups",
                "/wiki/First_isomorphism_theorem",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Modular_arithmetic",
                "/wiki/If_and_only_if",
                "/wiki/Subset",
                "/wiki/Linear_span",
                "/wiki/Linear_combination",
                "/wiki/Universal_property",
                "/wiki/Characteristic_polynomial",
                "/wiki/Algebraically_closed_field",
                "/wiki/Eigenbasis",
                "/wiki/Jordan_canonical_form",
                "/wiki/Spectral_theorem",
                "/wiki/Endomorphism",
                "/wiki/Kernel_(linear_algebra)",
                "/wiki/Identity_function",
                "/wiki/Determinant",
                "/wiki/Square_matrix",
                "/wiki/Orientation_(mathematics)",
                "/wiki/Matrix_multiplication",
                "/wiki/Bijection",
                "/wiki/Up_to",
                "/wiki/Dual_vector_space",
                "/wiki/Natural_(category_theory)",
                "/wiki/Origin_(mathematics)",
                "/wiki/Coordinate_system",
                "/wiki/Isomorphism",
                "/wiki/Inverse_map",
                "/wiki/Function_composition",
                "/wiki/Identity_function",
                "/wiki/Injective",
                "/wiki/Surjective",
                "/wiki/Function_(mathematics)",
                "/wiki/Degree_of_a_field_extension",
                "/wiki/Countably_infinite",
                "/wiki/A_fortiori",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Zorn%27s_lemma",
                "/wiki/Axiom_of_Choice",
                "/wiki/Zermelo%E2%80%93Fraenkel_set_theory",
                "/wiki/Ultrafilter_lemma",
                "/wiki/Cardinality",
                "/wiki/Dimension_theorem_for_vector_spaces",
                "/wiki/Cartesian_coordinates",
                "/wiki/Coordinate_vector",
                "/wiki/Standard_basis",
                "/wiki/Sequence",
                "/wiki/Index_set",
                "/wiki/Linearly_independent",
                "/wiki/Linear_combination",
                "/wiki/Natural_exponential_function",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Homogeneous_linear_equation",
                "/wiki/Real_line",
                "/wiki/Interval_(mathematics)",
                "/wiki/Subset",
                "/wiki/Continuous_function",
                "/wiki/Integral",
                "/wiki/Differentiability",
                "/wiki/Functional_analysis",
                "/wiki/Polynomial_ring",
                "/wiki/Polynomial_function",
                "/wiki/Complex_plane",
                "/wiki/Complex_numbers",
                "/wiki/Real_numbers",
                "/wiki/Imaginary_unit",
                "/wiki/Coordinate_space",
                "/wiki/Tuple",
                "/wiki/Function_space",
                "/wiki/Henri_Lebesgue",
                "/wiki/Stefan_Banach",
                "/wiki/David_Hilbert",
                "/wiki/Algebra",
                "/wiki/Functional_analysis",
                "/wiki/Lp_space",
                "/wiki/Hilbert_space",
                "/wiki/Giusto_Bellavitis",
                "/wiki/Complex_number",
                "/wiki/Jean-Robert_Argand",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Quaternion",
                "/wiki/Biquaternion",
                "/wiki/Linear_combination",
                "/wiki/Edmond_Laguerre",
                "/wiki/System_of_linear_equations",
                "/wiki/Affine_geometry",
                "/wiki/Coordinate",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Analytic_geometry",
                "/wiki/Curve",
                "/wiki/Bernard_Bolzano",
                "/wiki/Barycentric_coordinate_system_(mathematics)",
                "/wiki/August_Ferdinand_M%C3%B6bius",
                "/wiki/C._V._Mourey",
                "/wiki/Elementary_group_theory",
                "/wiki/Abstract_algebra",
                "/wiki/Abelian_group",
                "/wiki/Module_(mathematics)",
                "/wiki/Ring_homomorphism",
                "/wiki/Endomorphism_ring",
                "/wiki/Closure_(mathematics)",
                "/wiki/Neighborhood_(topology)",
                "/wiki/Angle",
                "/wiki/Distance",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Addition",
                "/wiki/Subtraction",
                "/wiki/Multiplication",
                "/wiki/Division_(mathematics)",
                "/wiki/Rational_number",
                "/wiki/Axiom",
                "/wiki/Field_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Cartesian_coordinates",
                "/wiki/Ordered_pair",
                "/wiki/Arrow",
                "/wiki/Plane_(geometry)",
                "/wiki/Force",
                "/wiki/Velocity",
                "/wiki/Parallelogram",
                "/wiki/Real_number",
                "/wiki/Mathematics",
                "/wiki/Science",
                "/wiki/Engineering",
                "/wiki/System_of_linear_equations",
                "/wiki/Fourier_series",
                "/wiki/Image_compression",
                "/wiki/Partial_differential_equation",
                "/wiki/Coordinate-free",
                "/wiki/Tensor",
                "/wiki/Manifold_(mathematics)",
                "/wiki/Abstract_algebra",
                "/wiki/Analytic_geometry",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Linear_equation",
                "/wiki/Giuseppe_Peano",
                "/wiki/Euclidean_space",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Linear_algebra",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Mathematical_analysis",
                "/wiki/Function_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Topology",
                "/wiki/Continuous_function",
                "/wiki/Norm_(mathematics)",
                "/wiki/Inner_product",
                "/wiki/Metric_(mathematics)",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_vector",
                "/wiki/Physics",
                "/wiki/Force",
                "/wiki/Force_vector",
                "/wiki/Geometry",
                "/wiki/Three-dimensional_space",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Axiom"
            ],
            "text": "The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[106] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.generalizing the homogeneous case b = 0 above.[105] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).If W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x \u2208 W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v \u2208 V. An important example is the space of solutions of a system of inhomogeneous linear equationsRoughly, affine spaces are vector spaces whose origins are not specified.[104] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the mapModules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[102] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term vector space to mean modules over a division ring.[103] The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 \u00d7 R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[100] K-theory studies the isomorphism classes of all vector bundles over some topological space.[101] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.such that for every x in X, the fiber \u03c0\u22121(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X \u00d7 V \u2192 X makes the product X \u00d7 V into a \"trivial\" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of \u03c0 to \u03c0\u22121(U) is isomorphic[nb 16] to the trivial bundle U \u00d7 V \u2192 U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be \"twisted\" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X \u00d7 V). For example, the M\u00f6bius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 \u00d7 R, because the latter is orientable whereas the former is not.[99]A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[94] More precisely, a vector bundle over X is a topological space E equipped with a continuous mapRiemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[95] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[96][97] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[98]The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[94]The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[89] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[90] They in turn are applied in digital filters[91] and as a rapid multiplication algorithm for polynomials and large integers (Sch\u00f6nhage\u2013Strassen algorithm).[92][93]Fourier series are used to solve boundary value problems in partial differential equations.[84] In 1822, Fourier first used this technique to solve the heat equation.[85] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[86] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[87] The JPEG image format is an application of the closely related discrete cosine transform.[88]In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[81] A complex-number form of Fourier series is also commonly used.[80] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[82] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[83]The coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[80]Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][78] The underlying vector space is usually the Hilbert space L2(0, 2\u03c0), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[79] The Fourier expansion of an L2 function f isWhen \u03a9 = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by \u03b4, which associates to a test function f its value at the p: \u03b4(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax\u2013Milgram theorem, a consequence of the Riesz representation theorem).[77]A distribution (or generalized function) is a linear map assigning a number to each \"test\" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[76] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain \u03a9:Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[74] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[75]When a field, F is explicitly stated, a common term used is F-algebra.The multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product \u2297, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 \u2297 v2 and v2 \u2297 v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 \u2297 v2 = \u2212 v2 \u2297 v1 yields the exterior algebra.[73]The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[72] As a vector space, it is spanned by symbols, called simple tensorsExamples include the vector space of n-by-n matrices, with [x, y] = xy \u2212 yx, the commutator of two matrices, and R3, endowed with the cross product.Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[70]General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[69] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone\u2013Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[66] As an example from physics, the time-dependent Schr\u00f6dinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[67] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[68]By definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[63] By the Stone\u2013Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[64] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what \"basic functions\", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram\u2013Schmidt process, it enables one to construct a basis of orthogonal vectors.[65] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[61] The Hilbert space L2(\u03a9), with inner product given byImposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[60]there exists a function f(x) belonging to the vector space Lp(\u03a9) such thatThe space of integrable functions on a given domain \u03a9 (for example an interval) satisfying |f|p < \u221e, and equipped with this norm are called Lebesgue spaces, denoted Lp(\u03a9).[nb 10] These spaces are complete.[59] (If one uses the Riemann integral instead, the space is not complete, which may be seen as a justification for Lebesgue's integration theory.[nb 11]) Concretely this means that for any sequence of Lebesgue-integrable functions f1, f2, ... with |fn|p < \u221e, satisfying the conditionMore generally than sequences of real numbers, functions f: \u03a9 \u2192 R are endowed with a norm that replaces the above sum by the Lebesgue integralis finite. The topologies on the infinite-dimensional space \u2113\u2009p are inequivalent for different p. E.g. the sequence of vectors xn = (2\u2212n, 2\u2212n, ..., 2\u2212n, 0, 0, ...), i.e. the first 2n components are 2\u2212n, the following ones are 0, converges to the zero vector for p = \u221e, but does not for p = 1:Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[58] A first example is the vector space \u2113\u2009p consisting of infinite vectors with real entries x = (x1, x2, ...) whose p-norm (1 \u2264 p \u2264 \u221e) given byFrom a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V \u2192 W, maps between topological vector spaces are required to be continuous.[56] In particular, the (topological) dual space V\u2217 consists of continuous functionals V \u2192 R (or to C). The fundamental Hahn\u2013Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[57]Banach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study\u2014a key piece of functional analysis\u2014focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[55] The image at the right shows the equivalence of the 1-norm and \u221e-norm on R2: as the unit \"balls\" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[53] In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[54] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only ifdenotes the limit of the corresponding finite partial sums of the sequence (fi)i\u2208N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.In such topological vector spaces one can consider series of vectors. The infinite sumConvergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[51][52] Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:Coordinate space Fn can be equipped with the standard dot product:where f+ denotes the positive part of f and f\u2212 the negative part.[48]A vector space may be given a partial order \u2264, under which some vectors can be compared.[47] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functionsFrom the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question\u2014crucial to analysis\u2014whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.These rules ensure that the map f from the V \u00d7 W to V \u2297 W that maps a tuple (v, w) to v \u2297 w is bilinear. The universality states that given any vector space X and any bilinear map g\u00a0: V \u00d7 W \u2192 X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v \u2297 w) = g(v, w).[46] This is called the universal property of the tensor product, an instance of the method\u2014much used in advanced abstract algebra\u2014to indirectly define objects by specifying maps from or to this object.subject to the rulesThe tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensorsThe tensor product V \u2297F W, or simply V \u2297 W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g\u00a0: V \u00d7 W \u2192 X is called bilinear if g is linear in both variables v and w. That is to say, for fixed w the map v \u21a6 g(v, w) is linear in the sense above and likewise for fixed v.The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.the derivatives of the function f appear linearly (as opposed to f\u2032\u2032(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)\u2032 = f\u2032 + g\u2009\u2032 and (c\u00b7f)\u2032 = c\u00b7f\u2032 for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).In the corresponding mapAn important example is the kernel of a linear map x \u21a6 Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equationsand the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.The kernel ker(f) of a linear map f\u00a0: V \u2192 W consists of vectors v that are mapped to 0 in W.[41] Both kernel and image im(f) = {f(v)\u00a0: v \u2208 V} are subspaces of V and W, respectively.[42] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[43] Because of this, many statements such as the first isomorphism theorem (also called rank\u2013nullity theorem in matrix-related terms)The counterpart to subspaces are quotient vector spaces.[40] Given any subspace W \u2282 V, the quotient space V/W (\"V modulo W\") is defined as follows: as a set, it consists of v + W = {v + w\u00a0: w \u2208 W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a \u00b7 (v + W) = (a \u00b7 v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space \"forgets\" information that is contained in the subspace W.A linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n \u2013 1.A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[38][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[39]In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.By spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in \u03bb, called the characteristic polynomial of f.[36] If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[37][nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.Endomorphisms, linear maps f\u00a0: V \u2192 V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying \u03bbv = f(v), where \u03bb is a scalar, is called an eigenvector of f with eigenvalue \u03bb.[nb 5][35] Equivalently, v is an element of the kernel of the difference f \u2212 \u03bb \u00b7 Id (where Id is the identity map V \u2192 V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue \u03bb is equivalent toThe determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.Moreover, after choosing bases of V and W, any linear map f\u00a0: V \u2192 W is uniquely represented by a matrix via this assignment.[33]or, using the matrix multiplication of the matrix A with the coordinate vector x:Matrices are a useful notion to encode linear maps.[32] They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the followingOnce a basis of V is chosen, linear maps f\u00a0: V \u2192 W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no \"canonical\" or preferred isomorphism; actually an isomorphism \u03c6\u00a0: Fn \u2192 V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via \u03c6. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context, see below.Linear maps V \u2192 W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V\u2217.[28] Via the injective natural map V \u2192 V\u2217\u2217, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]For example, the \"arrows in the plane\" and \"ordered pairs of numbers\" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.An isomorphism is a linear map f\u00a0: V \u2192 W such that there exists an inverse map g\u00a0: W \u2192 V, which is a map such that the two possible compositions f \u2218 g\u00a0: W \u2192 W and g \u2218 f\u00a0: V \u2192 V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26] If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.The relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure\u2014i.e., they preserve sums and scalar multiplication:A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(\u03b1) over Q depends on \u03b1. If \u03b1 satisfies some polynomial equation The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e\u2212x and xe\u2212x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo\u2013Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i \u2208 I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. \"Spanning the whole space\" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:yields f(x) = a\u2009e\u2212x + bx\u2009e\u2212x, where a and b are arbitrary constants, and ex is the natural exponential function.are given by triples with arbitrary a, b = a/2, and c = \u22125a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namelySystems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions ofand similarly for multiplication. Such function spaces occur in many geometric situations, when \u03a9 is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15] Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:Functions from any fixed set \u03a9 to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given byIn fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c \u22c5 (x + iy) = (c \u22c5 x) + i(c \u22c5 y) for real numbers x, y, a, b and c. The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.A vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of n-tuples (sequences of length n) of elements of F, such asAn important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by M\u00f6bius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example the zero vector 0 of V and the additive inverse \u2212v of any vector v are unique. Other properties follow from the distributive law, for example av equals 0 if and only if a equals 0 or v equals 0.In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]Vector addition and scalar multiplication are operations, satisfying the closure property: u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.When the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.Subtraction of two vectors and division by a (non-zero) scalar can be defined asLikewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:To qualify as a vector space, the set\u00a0V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.Elements of V are commonly called vectors. Elements of\u00a0F are commonly called scalars.A vector space over a field F is a set\u00a0V together with two operations that satisfy the eight axioms listed below.In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.andA second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (\u22121)v = \u2212v has the opposite direction and the same length as v (blue vector pointing down in the right image).The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.The concept of vector space will first be explained by describing two particular examples:Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied (\"scaled\") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.",
            "title": "Vector space",
            "url": "https://en.wikipedia.org/wiki/Vector_space"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/Science",
                "/wiki/Engineering",
                "/wiki/System_of_linear_equations",
                "/wiki/Fourier_series",
                "/wiki/Image_compression",
                "/wiki/Partial_differential_equation",
                "/wiki/Coordinate-free",
                "/wiki/Tensor",
                "/wiki/Manifold_(mathematics)",
                "/wiki/Abstract_algebra",
                "/wiki/Analytic_geometry",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Linear_equation",
                "/wiki/Giuseppe_Peano",
                "/wiki/Euclidean_space",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Linear_algebra",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Mathematical_analysis",
                "/wiki/Function_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Topology",
                "/wiki/Continuous_function",
                "/wiki/Norm_(mathematics)",
                "/wiki/Inner_product",
                "/wiki/Metric_(mathematics)",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_vector",
                "/wiki/Physics",
                "/wiki/Force",
                "/wiki/Force_vector",
                "/wiki/Geometry",
                "/wiki/Three-dimensional_space",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Axiom"
            ],
            "links": [
                "/wiki/Parallel_(geometry)",
                "/wiki/Grassmannian_manifold",
                "/wiki/Flag_manifold",
                "/wiki/Flag_(linear_algebra)",
                "/wiki/Nullspace",
                "/wiki/Coset",
                "/wiki/Transitive_group_action",
                "/wiki/Group_action",
                "/wiki/Ring_(mathematics)",
                "/wiki/Multiplicative_inverse",
                "/wiki/Abelian_group",
                "/wiki/Modular_arithmetic",
                "/wiki/Free_module",
                "/wiki/Module_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Field_(mathematics)",
                "/wiki/Division_ring",
                "/wiki/Spectrum_of_a_ring",
                "/wiki/Locally_free_module",
                "/wiki/Cotangent_bundle",
                "/wiki/Cotangent_space",
                "/wiki/Section_(fiber_bundle)",
                "/wiki/Differential_form",
                "/wiki/Tangent_bundle",
                "/wiki/Tangent_space",
                "/wiki/Vector_field",
                "/wiki/Hairy_ball_theorem",
                "/wiki/2-sphere",
                "/wiki/K-theory",
                "/wiki/Division_algebra",
                "/wiki/Quaternion",
                "/wiki/Octonion",
                "/wiki/Fiber_(mathematics)",
                "/wiki/Line_bundle",
                "/wiki/Trivial_bundle",
                "/wiki/Locally",
                "/wiki/Neighborhood_(topology)",
                "/wiki/M%C3%B6bius_strip",
                "/wiki/Homeomorphism#Examples",
                "/wiki/Cylinder_(geometry)",
                "/wiki/Orientable_manifold",
                "/wiki/Topological_space",
                "/wiki/Riemannian_manifold",
                "/wiki/Riemannian_metric",
                "/wiki/Riemann_curvature_tensor",
                "/wiki/Curvature_(mathematics)",
                "/wiki/General_relativity",
                "/wiki/Einstein_curvature_tensor",
                "/wiki/Space-time",
                "/wiki/Compact_Lie_group",
                "/wiki/Tangent_plane",
                "/wiki/Linear_approximation",
                "/wiki/Linearization",
                "/wiki/Differentiable_manifold",
                "/wiki/Fast_Fourier_transform",
                "/wiki/Convolution_theorem",
                "/wiki/Convolution",
                "/wiki/Digital_filter",
                "/wiki/Multiplication_algorithm",
                "/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm",
                "/wiki/Boundary_value_problem",
                "/wiki/Partial_differential_equation",
                "/wiki/Joseph_Fourier",
                "/wiki/Heat_equation",
                "/wiki/Sampling_(signal_processing)",
                "/wiki/Discrete_Fourier_transform",
                "/wiki/Digital_signal_processing",
                "/wiki/Radar",
                "/wiki/Speech_encoding",
                "/wiki/Image_compression",
                "/wiki/JPEG",
                "/wiki/Discrete_cosine_transform",
                "/wiki/Superposition_principle",
                "/wiki/Sine_waves",
                "/wiki/Frequency_spectrum",
                "/wiki/Duality_(mathematics)",
                "/wiki/Pontryagin_duality",
                "/wiki/Group_(mathematics)",
                "/wiki/Reciprocal_lattice",
                "/wiki/Lattice_(group)",
                "/wiki/Atom",
                "/wiki/Crystal",
                "/wiki/Fourier_coefficient",
                "/wiki/Periodic_function",
                "/wiki/Trigonometric_functions",
                "/wiki/Fourier_series",
                "/wiki/Hilbert_space",
                "/wiki/Fourier_expansion",
                "/wiki/Dirac_distribution",
                "/wiki/Green%27s_function",
                "/wiki/Fundamental_solution",
                "/wiki/Lax%E2%80%93Milgram_theorem",
                "/wiki/Riesz_representation_theorem",
                "/wiki/Test_function",
                "/wiki/Smooth_function",
                "/wiki/Compact_support",
                "/wiki/Optimization_(mathematics)",
                "/wiki/Minimax_theorem",
                "/wiki/Game_theory",
                "/wiki/Representation_theory",
                "/wiki/Group_theory",
                "/wiki/Distributive_law",
                "/wiki/Symmetric_algebra",
                "/wiki/Exterior_algebra",
                "/wiki/Tensor_algebra",
                "/wiki/Tensor",
                "/wiki/Commutator",
                "/wiki/Cross_product",
                "/wiki/Commutative_algebra",
                "/wiki/Polynomial_ring",
                "/wiki/Commutative",
                "/wiki/Associative",
                "/wiki/Quotient_ring",
                "/wiki/Algebraic_geometry",
                "/wiki/Coordinate_ring",
                "/wiki/Bilinear_operator",
                "/wiki/Banach_algebra",
                "/wiki/Differential_equation",
                "/wiki/Schr%C3%B6dinger_equation",
                "/wiki/Quantum_mechanics",
                "/wiki/Partial_differential_equation",
                "/wiki/Wavefunction",
                "/wiki/Eigenvalue",
                "/wiki/Differential_operator",
                "/wiki/Eigenstate",
                "/wiki/Spectral_theorem",
                "/wiki/Compact_operator",
                "/wiki/Taylor_approximation",
                "/wiki/Differentiable_function",
                "/wiki/Stone%E2%80%93Weierstrass_theorem",
                "/wiki/Trigonometric_function",
                "/wiki/Fourier_expansion",
                "/wiki/Closure_(topology)",
                "/wiki/Hilbert_space_dimension",
                "/wiki/Gram%E2%80%93Schmidt_process",
                "/wiki/Orthogonal_basis",
                "/wiki/Euclidean_space",
                "/wiki/David_Hilbert",
                "/wiki/Derivative",
                "/wiki/Sobolev_space",
                "/wiki/Integrable_function",
                "/wiki/Domain_(mathematics)",
                "/wiki/Lp_space",
                "/wiki/Riemann_integral",
                "/wiki/Lebesgue_integral",
                "/wiki/Zero_vector",
                "/wiki/Stefan_Banach",
                "/wiki/Lp_space",
                "/wiki/P-norm",
                "/wiki/Functional_(mathematics)",
                "/wiki/Hahn%E2%80%93Banach_theorem",
                "/wiki/Functional_analysis",
                "/wiki/Cauchy_sequence",
                "/wiki/Completeness_(topology)",
                "/wiki/Topology_of_uniform_convergence",
                "/wiki/Weierstrass_approximation_theorem",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Function_space",
                "/wiki/Function_series",
                "/wiki/Modes_of_convergence",
                "/wiki/Pointwise_convergence",
                "/wiki/Uniform_convergence",
                "/wiki/Series_(mathematics)",
                "/wiki/Infinite_sum",
                "/wiki/Topological_space",
                "/wiki/Neighborhood_(topology)",
                "/wiki/Continuous_map",
                "/wiki/Law_of_cosines",
                "/wiki/Dot_product",
                "/wiki/Partial_order",
                "/wiki/Ordered_vector_space",
                "/wiki/Riesz_space",
                "/wiki/Lebesgue_integration",
                "/wiki/Limit_of_a_sequence",
                "/wiki/Infinite_series",
                "/wiki/Functional_analysis",
                "/wiki/Tuple",
                "/wiki/Function_composition",
                "/wiki/Universal_property",
                "/wiki/Tensor",
                "/wiki/Multilinear_algebra",
                "/wiki/Cartesian_product",
                "/wiki/Bilinear_map",
                "/wiki/Derivative",
                "/wiki/Linear_differential_operator",
                "/wiki/Group_(mathematics)",
                "/wiki/Kernel_(algebra)",
                "/wiki/Image_(mathematics)",
                "/wiki/Category_of_vector_spaces",
                "/wiki/Abelian_category",
                "/wiki/Category_(mathematics)",
                "/wiki/Category_of_abelian_groups",
                "/wiki/First_isomorphism_theorem",
                "/wiki/Rank%E2%80%93nullity_theorem",
                "/wiki/Modular_arithmetic",
                "/wiki/If_and_only_if",
                "/wiki/Subset",
                "/wiki/Linear_span",
                "/wiki/Linear_combination",
                "/wiki/Universal_property",
                "/wiki/Characteristic_polynomial",
                "/wiki/Algebraically_closed_field",
                "/wiki/Eigenbasis",
                "/wiki/Jordan_canonical_form",
                "/wiki/Spectral_theorem",
                "/wiki/Endomorphism",
                "/wiki/Kernel_(linear_algebra)",
                "/wiki/Identity_function",
                "/wiki/Determinant",
                "/wiki/Square_matrix",
                "/wiki/Orientation_(mathematics)",
                "/wiki/Matrix_multiplication",
                "/wiki/Bijection",
                "/wiki/Up_to",
                "/wiki/Dual_vector_space",
                "/wiki/Natural_(category_theory)",
                "/wiki/Origin_(mathematics)",
                "/wiki/Coordinate_system",
                "/wiki/Isomorphism",
                "/wiki/Inverse_map",
                "/wiki/Function_composition",
                "/wiki/Identity_function",
                "/wiki/Injective",
                "/wiki/Surjective",
                "/wiki/Function_(mathematics)",
                "/wiki/Degree_of_a_field_extension",
                "/wiki/Countably_infinite",
                "/wiki/A_fortiori",
                "/wiki/Ordinary_differential_equation",
                "/wiki/Zorn%27s_lemma",
                "/wiki/Axiom_of_Choice",
                "/wiki/Zermelo%E2%80%93Fraenkel_set_theory",
                "/wiki/Ultrafilter_lemma",
                "/wiki/Cardinality",
                "/wiki/Dimension_theorem_for_vector_spaces",
                "/wiki/Cartesian_coordinates",
                "/wiki/Coordinate_vector",
                "/wiki/Standard_basis",
                "/wiki/Sequence",
                "/wiki/Index_set",
                "/wiki/Linearly_independent",
                "/wiki/Linear_combination",
                "/wiki/Natural_exponential_function",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Homogeneous_linear_equation",
                "/wiki/Real_line",
                "/wiki/Interval_(mathematics)",
                "/wiki/Subset",
                "/wiki/Continuous_function",
                "/wiki/Integral",
                "/wiki/Differentiability",
                "/wiki/Functional_analysis",
                "/wiki/Polynomial_ring",
                "/wiki/Polynomial_function",
                "/wiki/Complex_plane",
                "/wiki/Complex_numbers",
                "/wiki/Real_numbers",
                "/wiki/Imaginary_unit",
                "/wiki/Coordinate_space",
                "/wiki/Tuple",
                "/wiki/Function_space",
                "/wiki/Henri_Lebesgue",
                "/wiki/Stefan_Banach",
                "/wiki/David_Hilbert",
                "/wiki/Algebra",
                "/wiki/Functional_analysis",
                "/wiki/Lp_space",
                "/wiki/Hilbert_space",
                "/wiki/Giusto_Bellavitis",
                "/wiki/Complex_number",
                "/wiki/Jean-Robert_Argand",
                "/wiki/William_Rowan_Hamilton",
                "/wiki/Quaternion",
                "/wiki/Biquaternion",
                "/wiki/Linear_combination",
                "/wiki/Edmond_Laguerre",
                "/wiki/System_of_linear_equations",
                "/wiki/Affine_geometry",
                "/wiki/Coordinate",
                "/wiki/Ren%C3%A9_Descartes",
                "/wiki/Pierre_de_Fermat",
                "/wiki/Analytic_geometry",
                "/wiki/Curve",
                "/wiki/Bernard_Bolzano",
                "/wiki/Barycentric_coordinate_system_(mathematics)",
                "/wiki/August_Ferdinand_M%C3%B6bius",
                "/wiki/C._V._Mourey",
                "/wiki/Elementary_group_theory",
                "/wiki/Abstract_algebra",
                "/wiki/Abelian_group",
                "/wiki/Module_(mathematics)",
                "/wiki/Ring_homomorphism",
                "/wiki/Endomorphism_ring",
                "/wiki/Closure_(mathematics)",
                "/wiki/Neighborhood_(topology)",
                "/wiki/Angle",
                "/wiki/Distance",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Addition",
                "/wiki/Subtraction",
                "/wiki/Multiplication",
                "/wiki/Division_(mathematics)",
                "/wiki/Rational_number",
                "/wiki/Axiom",
                "/wiki/Field_(mathematics)",
                "/wiki/Set_(mathematics)",
                "/wiki/Cartesian_coordinates",
                "/wiki/Ordered_pair",
                "/wiki/Arrow",
                "/wiki/Plane_(geometry)",
                "/wiki/Force",
                "/wiki/Velocity",
                "/wiki/Parallelogram",
                "/wiki/Real_number",
                "/wiki/Mathematics",
                "/wiki/Science",
                "/wiki/Engineering",
                "/wiki/System_of_linear_equations",
                "/wiki/Fourier_series",
                "/wiki/Image_compression",
                "/wiki/Partial_differential_equation",
                "/wiki/Coordinate-free",
                "/wiki/Tensor",
                "/wiki/Manifold_(mathematics)",
                "/wiki/Abstract_algebra",
                "/wiki/Analytic_geometry",
                "/wiki/Matrix_(mathematics)",
                "/wiki/Linear_equation",
                "/wiki/Giuseppe_Peano",
                "/wiki/Euclidean_space",
                "/wiki/Line_(geometry)",
                "/wiki/Plane_(geometry)",
                "/wiki/Linear_algebra",
                "/wiki/Dimension_(vector_space)",
                "/wiki/Mathematical_analysis",
                "/wiki/Function_space",
                "/wiki/Function_(mathematics)",
                "/wiki/Topology",
                "/wiki/Continuous_function",
                "/wiki/Norm_(mathematics)",
                "/wiki/Inner_product",
                "/wiki/Metric_(mathematics)",
                "/wiki/Banach_space",
                "/wiki/Hilbert_space",
                "/wiki/Euclidean_vector",
                "/wiki/Physics",
                "/wiki/Force",
                "/wiki/Force_vector",
                "/wiki/Geometry",
                "/wiki/Three-dimensional_space",
                "/wiki/Vector_addition",
                "/wiki/Scalar_multiplication",
                "/wiki/Scalar_(mathematics)",
                "/wiki/Real_number",
                "/wiki/Complex_number",
                "/wiki/Rational_number",
                "/wiki/Field_(mathematics)",
                "/wiki/Axiom"
            ],
            "text": "The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[106] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.generalizing the homogeneous case b = 0 above.[105] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).If W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x \u2208 W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v \u2208 V. An important example is the space of solutions of a system of inhomogeneous linear equationsRoughly, affine spaces are vector spaces whose origins are not specified.[104] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the mapModules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[102] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term vector space to mean modules over a division ring.[103] The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 \u00d7 R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[100] K-theory studies the isomorphism classes of all vector bundles over some topological space.[101] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.such that for every x in X, the fiber \u03c0\u22121(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X \u00d7 V \u2192 X makes the product X \u00d7 V into a \"trivial\" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of \u03c0 to \u03c0\u22121(U) is isomorphic[nb 16] to the trivial bundle U \u00d7 V \u2192 U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be \"twisted\" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X \u00d7 V). For example, the M\u00f6bius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 \u00d7 R, because the latter is orientable whereas the former is not.[99]A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[94] More precisely, a vector bundle over X is a topological space E equipped with a continuous mapRiemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[95] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[96][97] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[98]The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[94]The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[89] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[90] They in turn are applied in digital filters[91] and as a rapid multiplication algorithm for polynomials and large integers (Sch\u00f6nhage\u2013Strassen algorithm).[92][93]Fourier series are used to solve boundary value problems in partial differential equations.[84] In 1822, Fourier first used this technique to solve the heat equation.[85] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[86] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[87] The JPEG image format is an application of the closely related discrete cosine transform.[88]In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[81] A complex-number form of Fourier series is also commonly used.[80] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[82] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[83]The coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[80]Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][78] The underlying vector space is usually the Hilbert space L2(0, 2\u03c0), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[79] The Fourier expansion of an L2 function f isWhen \u03a9 = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by \u03b4, which associates to a test function f its value at the p: \u03b4(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax\u2013Milgram theorem, a consequence of the Riesz representation theorem).[77]A distribution (or generalized function) is a linear map assigning a number to each \"test\" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[76] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain \u03a9:Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[74] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[75]When a field, F is explicitly stated, a common term used is F-algebra.The multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product \u2297, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 \u2297 v2 and v2 \u2297 v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 \u2297 v2 = \u2212 v2 \u2297 v1 yields the exterior algebra.[73]The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[72] As a vector space, it is spanned by symbols, called simple tensorsExamples include the vector space of n-by-n matrices, with [x, y] = xy \u2212 yx, the commutator of two matrices, and R3, endowed with the cross product.Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[70]General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[69] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone\u2013Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[66] As an example from physics, the time-dependent Schr\u00f6dinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[67] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[68]By definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[63] By the Stone\u2013Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[64] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what \"basic functions\", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram\u2013Schmidt process, it enables one to construct a basis of orthogonal vectors.[65] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[61] The Hilbert space L2(\u03a9), with inner product given byImposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[60]there exists a function f(x) belonging to the vector space Lp(\u03a9) such thatThe space of integrable functions on a given domain \u03a9 (for example an interval) satisfying |f|p < \u221e, and equipped with this norm are called Lebesgue spaces, denoted Lp(\u03a9).[nb 10] These spaces are complete.[59] (If one uses the Riemann integral instead, the space is not complete, which may be seen as a justification for Lebesgue's integration theory.[nb 11]) Concretely this means that for any sequence of Lebesgue-integrable functions f1, f2, ... with |fn|p < \u221e, satisfying the conditionMore generally than sequences of real numbers, functions f: \u03a9 \u2192 R are endowed with a norm that replaces the above sum by the Lebesgue integralis finite. The topologies on the infinite-dimensional space \u2113\u2009p are inequivalent for different p. E.g. the sequence of vectors xn = (2\u2212n, 2\u2212n, ..., 2\u2212n, 0, 0, ...), i.e. the first 2n components are 2\u2212n, the following ones are 0, converges to the zero vector for p = \u221e, but does not for p = 1:Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[58] A first example is the vector space \u2113\u2009p consisting of infinite vectors with real entries x = (x1, x2, ...) whose p-norm (1 \u2264 p \u2264 \u221e) given byFrom a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V \u2192 W, maps between topological vector spaces are required to be continuous.[56] In particular, the (topological) dual space V\u2217 consists of continuous functionals V \u2192 R (or to C). The fundamental Hahn\u2013Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[57]Banach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study\u2014a key piece of functional analysis\u2014focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[55] The image at the right shows the equivalence of the 1-norm and \u221e-norm on R2: as the unit \"balls\" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[53] In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[54] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only ifdenotes the limit of the corresponding finite partial sums of the sequence (fi)i\u2208N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.In such topological vector spaces one can consider series of vectors. The infinite sumConvergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[51][52] Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:Coordinate space Fn can be equipped with the standard dot product:where f+ denotes the positive part of f and f\u2212 the negative part.[48]A vector space may be given a partial order \u2264, under which some vectors can be compared.[47] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functionsFrom the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question\u2014crucial to analysis\u2014whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.These rules ensure that the map f from the V \u00d7 W to V \u2297 W that maps a tuple (v, w) to v \u2297 w is bilinear. The universality states that given any vector space X and any bilinear map g\u00a0: V \u00d7 W \u2192 X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v \u2297 w) = g(v, w).[46] This is called the universal property of the tensor product, an instance of the method\u2014much used in advanced abstract algebra\u2014to indirectly define objects by specifying maps from or to this object.subject to the rulesThe tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensorsThe tensor product V \u2297F W, or simply V \u2297 W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g\u00a0: V \u00d7 W \u2192 X is called bilinear if g is linear in both variables v and w. That is to say, for fixed w the map v \u21a6 g(v, w) is linear in the sense above and likewise for fixed v.The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.the derivatives of the function f appear linearly (as opposed to f\u2032\u2032(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)\u2032 = f\u2032 + g\u2009\u2032 and (c\u00b7f)\u2032 = c\u00b7f\u2032 for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).In the corresponding mapAn important example is the kernel of a linear map x \u21a6 Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equationsand the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.The kernel ker(f) of a linear map f\u00a0: V \u2192 W consists of vectors v that are mapped to 0 in W.[41] Both kernel and image im(f) = {f(v)\u00a0: v \u2208 V} are subspaces of V and W, respectively.[42] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[43] Because of this, many statements such as the first isomorphism theorem (also called rank\u2013nullity theorem in matrix-related terms)The counterpart to subspaces are quotient vector spaces.[40] Given any subspace W \u2282 V, the quotient space V/W (\"V modulo W\") is defined as follows: as a set, it consists of v + W = {v + w\u00a0: w \u2208 W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a \u00b7 (v + W) = (a \u00b7 v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space \"forgets\" information that is contained in the subspace W.A linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n \u2013 1.A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[38][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[39]In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.By spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in \u03bb, called the characteristic polynomial of f.[36] If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[37][nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.Endomorphisms, linear maps f\u00a0: V \u2192 V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying \u03bbv = f(v), where \u03bb is a scalar, is called an eigenvector of f with eigenvalue \u03bb.[nb 5][35] Equivalently, v is an element of the kernel of the difference f \u2212 \u03bb \u00b7 Id (where Id is the identity map V \u2192 V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue \u03bb is equivalent toThe determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.Moreover, after choosing bases of V and W, any linear map f\u00a0: V \u2192 W is uniquely represented by a matrix via this assignment.[33]or, using the matrix multiplication of the matrix A with the coordinate vector x:Matrices are a useful notion to encode linear maps.[32] They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the followingOnce a basis of V is chosen, linear maps f\u00a0: V \u2192 W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no \"canonical\" or preferred isomorphism; actually an isomorphism \u03c6\u00a0: Fn \u2192 V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via \u03c6. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context, see below.Linear maps V \u2192 W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V\u2217.[28] Via the injective natural map V \u2192 V\u2217\u2217, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]For example, the \"arrows in the plane\" and \"ordered pairs of numbers\" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.An isomorphism is a linear map f\u00a0: V \u2192 W such that there exists an inverse map g\u00a0: W \u2192 V, which is a map such that the two possible compositions f \u2218 g\u00a0: W \u2192 W and g \u2218 f\u00a0: V \u2192 V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26] If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.The relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure\u2014i.e., they preserve sums and scalar multiplication:A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(\u03b1) over Q depends on \u03b1. If \u03b1 satisfies some polynomial equation The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e\u2212x and xe\u2212x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo\u2013Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i \u2208 I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. \"Spanning the whole space\" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:yields f(x) = a\u2009e\u2212x + bx\u2009e\u2212x, where a and b are arbitrary constants, and ex is the natural exponential function.are given by triples with arbitrary a, b = a/2, and c = \u22125a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namelySystems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions ofand similarly for multiplication. Such function spaces occur in many geometric situations, when \u03a9 is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15] Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:Functions from any fixed set \u03a9 to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given byIn fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c \u22c5 (x + iy) = (c \u22c5 x) + i(c \u22c5 y) for real numbers x, y, a, b and c. The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.A vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of n-tuples (sequences of length n) of elements of F, such asAn important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by M\u00f6bius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example the zero vector 0 of V and the additive inverse \u2212v of any vector v are unique. Other properties follow from the distributive law, for example av equals 0 if and only if a equals 0 or v equals 0.In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]Vector addition and scalar multiplication are operations, satisfying the closure property: u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.When the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.Subtraction of two vectors and division by a (non-zero) scalar can be defined asLikewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:To qualify as a vector space, the set\u00a0V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.Elements of V are commonly called vectors. Elements of\u00a0F are commonly called scalars.A vector space over a field F is a set\u00a0V together with two operations that satisfy the eight axioms listed below.In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.andA second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (\u22121)v = \u2212v has the opposite direction and the same length as v (blue vector pointing down in the right image).The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.The concept of vector space will first be explained by describing two particular examples:Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.Euclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied (\"scaled\") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.",
            "title": "Vector space",
            "url": "https://en.wikipedia.org/wiki/Vector_space"
        },
        {
            "desc_links": [],
            "links": [],
            "text": "",
            "title": "Field (mathematics)",
            "url": "https://en.wikipedia.org/wiki/Field_(mathematics)"
        },
        {
            "desc_links": [
                "/wiki/Mathematics",
                "/wiki/0_(number)",
                "/wiki/Algebraic_structure"
            ],
            "links": [
                "/wiki/Tensor_product",
                "/wiki/Mathematics",
                "/wiki/Tensor",
                "/wiki/0_(number)",
                "/wiki/Zero_vector",
                "/wiki/Linear_transformation",
                "/wiki/Zero_vector",
                "/wiki/Mathematics",
                "/wiki/Linear_algebra",
                "/wiki/Matrix_(mathematics)",
                "/wiki/0_(number)",
                "/wiki/Mathematics",
                "/wiki/Module_(mathematics)",
                "/wiki/Identity_element",
                "/wiki/Addition",
                "/wiki/Integer",
                "/wiki/0_(number)",
                "/wiki/Multiplication",
                "/wiki/Least_element",
                "/wiki/Partially_ordered_set",
                "/wiki/Lattice_(order)",
                "/wiki/Category_of_groups",
                "/wiki/Zero_morphism",
                "/wiki/Category_(mathematics)",
                "/wiki/Function_composition",
                "/wiki/Category_(mathematics)",
                "/wiki/Initial_and_terminal_objects",
                "/wiki/Coproduct",
                "/wiki/Product_(category_theory)",
                "/wiki/Field_(mathematics)",
                "/wiki/Ring_(mathematics)",
                "/wiki/Principal_ideal",
                "/wiki/Absorbing_element",
                "/wiki/Semigroup",
                "/wiki/Semiring",
                "/wiki/Additive_identity",
                "/wiki/Identity_element",
                "/wiki/Abelian_group",
                "/wiki/Mathematics",
                "/wiki/0_(number)",
                "/wiki/Algebraic_structure"
            ],
            "text": "Taking a tensor product of any tensor with any zero tensor results in another zero tensor. Adding the zero tensor is equivalent to the identity operation.In mathematics, the zero tensor is a tensor, of any order, all of whose components are zero. The zero tensor of order 1 is sometimes known as the zero vector.The zero matrix represents the linear transformation sending all vectors to the zero vector.There is exactly one zero matrix of any given size m\u2009\u00d7\u2009n having entries in a given ring, so when the context is clear one often refers to the zero matrix. In general the zero element of a ring is unique and typically denoted as 0 without any subscript to indicate the parent ring. Hence the examples above represent zero matrices over any ring.In mathematics, particularly linear algebra, a zero matrix is a matrix with all its entries being zero. Some examples of zero matrices areIn mathematics, the zero module is the module consisting of only the additive identity for the module's addition function. In the integers, this identity is zero, which gives the name zero module. That the zero module is in fact a module is simple to show; it is closed under addition and multiplication trivially.A least element in a partially ordered set or lattice may sometimes be called a zero element, and written either as 0 or \u22a5.If a category has a zero object 0, then there are canonical morphisms X \u2192 0 and 0 \u2192 Y, and composing them gives a zero morphism 0XY\u00a0: X \u2192 Y. In the category of groups, for example, zero morphisms are morphisms which always return group identities, thus generalising the function z(x) = 0.A zero morphism in a category is a generalised absorbing element under function composition: any morphism composed with a zero morphism gives a zero morphism. Specifically, if 0XY\u00a0: X \u2192 Y is the zero morphism among morphisms from X to Y, and f\u00a0: A \u2192 X and g\u00a0: Y \u2192 B are arbitrary morphisms, then g \u2218 0XY = 0XB and 0XY \u2218 f = 0AY.A zero object in a category is both an initial and terminal object (and so an identity under both coproducts and products). For example, the trivial structure (containing only the identity) is a zero object in categories where morphisms must map identities to identities. Specific examples include:Many absorbing elements are also additive identities, including the empty set and the zero function. Another important example is the distinguished element 0 in a field or ring, which is both the additive identity and the multiplicative absorbing element, and whose principal ideal is the smallest ideal.An absorbing element in a multiplicative semigroup or semiring generalises the property 0 \u22c5 x = 0. Examples include:An additive identity is the identity element in an additive group. It generalises the property 0 + x = x. Examples include:In mathematics, a zero element is one of several generalizations of the number zero to other algebraic structures. These alternate meanings may or may not reduce to the same thing, depending on the context.",
            "title": "Zero element",
            "url": "https://en.wikipedia.org/wiki/Zero_vector"
        }
    ]
}