Tree
In botany, a tree is a perennial plant with an elongated stem, or trunk, supporting branches and leaves in most species. In some usages, the definition of a tree may be narrower, including only woody plants with secondary growth, plants that are usable as lumber or plants above a specified height. Trees are not a taxonomic group but include a variety of plant species that have independently evolved a woody trunk and branches as a way to tower above other plants to compete for sunlight. Trees tend to be long-lived, some reaching several thousand years old. In wider definitions, the taller palms, tree ferns, bananas, and bamboos are also trees. Trees have been in existence for 370 million years. It is estimated that there are just over 3 trillion mature trees in the world.[1]A tree typically has many secondary branches supported clear of the ground by the trunk. This trunk typically contains woody tissue for strength, and vascular tissue to carry materials from one part of the tree to another. For most trees it is surrounded by a layer of bark which serves as a protective barrier. Below the ground, the roots branch and spread out widely; they serve to anchor the tree and extract moisture and nutrients from the soil. Above ground, the branches divide into smaller branches and shoots. The shoots typically bear leaves, which capture light energy and convert it into sugars by photosynthesis, providing the food for the tree's growth and development.Trees usually reproduce using seeds. Flowers and fruit may be present, but some trees, such as conifers, instead have pollen cones and seed cones. Palms, bananas, and bamboos also produce seeds, but tree ferns produce spores instead.Trees play a significant role in reducing erosion and moderating the climate. They remove carbon dioxide from the atmosphere and store large quantities of carbon in their tissues. Trees and forests provide a habitat for many species of animals and plants. Tropical rainforests are among the most biodiverse habitats in the world. Trees provide shade and shelter, timber for construction, fuel for cooking and heating, and fruit for food as well as having many other uses. In parts of the world, forests are shrinking as trees are cleared to increase the amount of land available for agriculture. Because of their longevity and usefulness, trees have always been revered, with sacred groves in various cultures, and they play a role in many of the world's mythologies.Although "tree" is a term of common parlance, there is no universally recognised precise definition of what a tree is, either botanically or in common language.[2] In its broadest sense, a tree is any plant with the general form of an elongated stem, or trunk, which supports the photosynthetic leaves or branches at some distance above the ground.[3] Trees are also typically defined by height,[4] with smaller plants from 0.5 to 10 m (1.6 to 32.8 ft) being called shrubs,[5] so the minimum height of a tree is only loosely defined.[4] Large herbaceous plants such as papaya and bananas are trees in this broad sense.[2][6]A commonly applied narrower definition is that a tree has a woody trunk formed by secondary growth, meaning that the trunk thickens each year by growing outwards, in addition to the primary upwards growth from the growing tip.[4][7] Under such a definition, herbaceous plants such as palms, bananas and papayas are not considered trees regardless of their height, growth form or stem girth. Certain monocots may be considered trees under a slightly looser definition;[8] while the Joshua tree, bamboos and palms do not have secondary growth and never produce true wood with growth rings,[9][10] they may produce "pseudo-wood" by lignifying cells formed by primary growth.[11]Aside from structural definitions, trees are commonly defined by use; for instance, as those plants which yield lumber.[12]The tree growth habit is an evolutionary adaptation found in different groups of plants: by growing taller, trees are able to compete better for sunlight.[13] Trees tend to be tall and long-lived,[14] some reaching several thousand years old.[15] Several trees are among the oldest organisms now living.[16] Trees have modified structures such as thicker stems composed of specialised cells that add structural strength and durability, allowing them to grow taller than many other plants and to spread out their foliage. They differ from shrubs, which have a similar growth form, by usually growing larger and having a single main stem;[5] but there is no consistent distinction between a tree and a shrub,[17] made more confusing by the fact that trees may be reduced in size under harsher environmental conditions such as on mountains and subarctic areas. The tree form has evolved separately in unrelated classes of plants in response to similar environmental challenges, making it a classic example of parallel evolution. With an estimated 60,000-100,000 species, the number of trees worldwide might total twenty-five per cent of all living plant species.[18][19] The greatest number of these grow in tropical regions and many of these areas have not yet been fully surveyed by botanists, making tree diversity and ranges poorly known.[20]The majority of tree species are angiosperms. There are about 1000 species of gymnosperm trees,[21] including conifers, cycads, ginkgophytes and gnetales; they produce seeds which are not enclosed in fruits, but in open structures such as pine cones, and many have tough waxy leaves, such as pine needles.[22] Most angiosperm trees are eudicots, the "true dicotyledons", so named because the seeds contain two cotyledons or seed leaves. There are also some trees among the old lineages of flowering plants called  basal angiosperms or paleodicots; these include Amborella, Magnolia, nutmeg and avocado,[23] while trees such as bamboo, palms and bananas are monocots.Wood gives structural strength to the trunk of most types of tree; this supports the plant as it grows larger. The vascular system of trees allows water, nutrients and other chemicals to be distributed around the plant, and without it trees would not be able to grow as large as they do. Trees, as relatively tall plants, need to draw water up the stem through the xylem from the roots by the suction produced as water evaporates from the leaves. If insufficient water is available the leaves will die.[24] The three main parts of trees include the root, stem, and leaves; they are integral parts of the vascular system which interconnects all the living cells. In trees and other plants that develop wood, the vascular cambium allows the expansion of vascular tissue that produces woody growth. Because this growth ruptures the epidermis of the stem, woody plants also have a cork cambium that develops among the phloem. The cork cambium gives rise to thickened cork cells to protect the surface of the plant and reduce water loss. Both the production of wood and the production of cork are forms of secondary growth.[25]Trees are either evergreen, having foliage that persists and remains green throughout the year,[26] or deciduous, shedding their leaves at the end of the growing season and then having a dormant period without foliage.[27] Most conifers are evergreens, but larches (Larix and Pseudolarix) are deciduous, dropping their needles each autumn, and some species of cypress (Glyptostrobus, Metasequoia and Taxodium) shed small leafy shoots annually in a process known as cladoptosis.[5] The crown is a name for the spreading top of a tree including the branches and leaves,[28] while the uppermost layer in a forest, formed by the crowns of the trees, is known as the canopy.[29] A sapling is a young tree.[30]Many tall palms are herbaceous[31] monocots; these do not undergo secondary growth and never produce wood.[9][10] In many tall palms, the terminal bud on the main stem is the only one to develop, so they have unbranched trunks with large spirally arranged leaves. Some of the tree ferns, order Cyatheales, have tall straight trunks, growing up to 20 metres (66 ft), but these are composed not of wood but of rhizomes which grow vertically and are covered by numerous adventitious roots.[32]The number of trees in the world, according to a 2015 estimate, is 3.04 trillion, of which 1.39 trillion (46%) are in the tropics or sub-tropics, 0.61 trillion (20%) in the temperate zones, and 0.74 trillion (24%) in the coniferous boreal forests. The estimate is about eight times higher than previous estimates, and is based on tree densities measured on over 400,000 plots. It remains subject to a wide margin of error, not least because the samples are mainly from Europe and North America. The estimate suggests that about 15 billion trees are cut down annually and about 5 billion are planted. In the 12,000 years since the start of human agriculture, the number of trees worldwide has decreased by 46%.[1][33][34][35]In suitable environments, such as the Daintree Rainforest in Queensland, or the mixed podocarp and broadleaf forest of Ulva Island, New Zealand, forest is the more-or-less stable climatic climax community at the end of a plant succession, where open areas such as grassland are colonised by taller plants, which in turn give way to trees that eventually form a forest canopy.[36][37]In cool temperate regions, conifers often predominate; a widely distributed climax community in the far north of the northern hemisphere is moist taiga or northern coniferous forest (also called boreal forest).[38][39] Taiga is the world's largest land biome, forming 29% of the world's forest cover.[40] The long cold winter of the far north is unsuitable for plant growth and trees must grow rapidly in the short summer season when the temperature rises and the days are long. Light is very limited under their dense cover and there may be little plant life on the forest floor, although fungi may abound.[41] Similar woodland is found on mountains where the altitude causes the average temperature to be lower thus reducing the length of the growing season.[42]Where rainfall is relatively evenly spread across the seasons in temperate regions, temperate broadleaf and mixed forest typified by species like oak, beech, birch and maple is found.[43] Temperate forest is also found in the southern hemisphere, as for example in the Eastern Australia temperate forest, characterised by Eucalyptus forest and open acacia woodland.[44]In tropical regions with a monsoon or monsoon-like climate, where a drier part of the year alternates with a wet period as in the Amazon rainforest, different species of broad-leaved trees dominate the forest, some of them being deciduous.[45] In tropical regions with a drier savanna climate and insufficient rainfall to support dense forests, the canopy is not closed, and plenty of sunshine reaches the ground which is covered with grass and scrub. Acacia and baobab are well adapted to living in such areas.[46]The roots of a tree serve to anchor it to the ground and gather water and nutrients to transfer to all parts of the tree. They are also used for reproduction, defence, survival, energy storage and many other purposes. The radicle or embryonic root is the first part of a seedling to emerge from the seed during the process of germination. This develops into a taproot which goes straight downwards. Within a few weeks lateral roots branch out of the side of this and grow horizontally through the upper layers of the soil. In most trees, the taproot eventually withers away and the wide-spreading laterals remain. Near the tip of the finer roots are single cell root hairs. These are in immediate contact with the soil particles and can absorb water and nutrients such as potassium in solution. The roots require oxygen to respire and only a few species such as the mangrove and the pond cypress (Taxodium ascendens) can live in permanently waterlogged soil.[47]In the soil, the roots encounter the hyphae of fungi. Many of these are known as mycorrhiza and form a mutualistic relationship with the tree roots. Some are specific to a single tree species, which will not flourish in the absence of its mycorrhizal associate. Others are generalists and associate with many species. The tree acquires minerals such as phosphorus from the fungus while it obtains the carbohydrate products of photosynthesis from the tree.[48] The hyphae of the fungus can link different trees and a network is formed, transferring nutrients from one place to another. The fungus promotes growth of the roots and helps protect the trees against predators and pathogens. It can also limit damage done to a tree by pollution as the fungus accumulate heavy metals within its tissues.[49] Fossil evidence shows that roots have been associated with mycorrhizal fungi since the early Paleozoic, four hundred million years ago, when the first vascular plants colonised dry land.[50]Some trees such as the alders (Alnus species) have a symbiotic relationship with Frankia species, a filamentous bacterium that can fix nitrogen from the air, converting it into ammonia. They have actinorhizal root nodules on their roots in which the bacteria live. This process enables the tree to live in low nitrogen habitats where they would otherwise be unable to thrive.[51] The plant hormones called cytokinins initiate root nodule formation, in a process closely related to mycorrhizal association.[52]It has been demonstrated that some trees are interconnected through their root system, forming a colony. The interconnections are made by the inosculation process, a kind of natural grafting or welding of vegetal tissues. The tests to demonstrate this networking are performed by injecting chemicals, sometimes radioactive, into a tree, and then checking for its presence in neighbouring trees.[53]The roots are, generally, an underground part of the tree, but some tree species have evolved roots that are aerial. The common purposes for aerial roots may be of two kinds, to contribute to the mechanical stability of the tree, and to obtain oxygen from air. An instance of mechanical stability enhancement is the red mangrove that develops prop roots that loop out of the trunk and branches and descend vertically into the mud.[54] A similar structure is developed by the Indian banyan.[55] Many large trees have buttress roots which flare out from the lower part of the trunk. These brace the tree rather like angle brackets and provide stability, reducing sway in high winds. They are particularly prevalent in tropical rainforests where the soil is poor and the roots are close to the surface.[56]Some tree species have developed root extensions that pop out of soil, in order to get oxygen, when it is not available in the soil because of excess water. These root extensions are called pneumatophores, and are present, among others, in black mangrove and pond cypress.[54]The main purpose of the trunk is to raise the leaves above the ground, enabling the tree to overtop other plants and outcompete them for light.[57] It also transports water and nutrients from the roots to the aerial parts of the tree, and distributes the food produced by the leaves to all other parts, including the roots.[58]In the case of angiosperms and gymnosperms, the outermost layer of the trunk is the bark, mostly composed of dead cells of phellem (cork).[59] It provides a thick, waterproof covering to the living inner tissue. It protects the trunk against the elements, disease, animal attack and fire. It is perforated by a large number of fine breathing pores called lenticels, through which oxygen diffuses. Bark is continually replaced by a living layer of cells called the cork cambium or phellogen.[59] The London plane (Platanus × acerifolia) periodically sheds its bark in large flakes. Similarly, the bark of the silver birch (Betula pendula) peels off in strips. As the tree's girth expands, newer layers of bark are larger in circumference, and the older layers develop fissures in many species. In some trees such as the pine (Pinus species) the bark exudes sticky resin which deters attackers whereas in rubber trees (Hevea brasiliensis) it is a milky latex that oozes out. The quinine bark tree (Cinchona officinalis) contains bitter substances to make the bark unpalatable.[58] Large tree-like plants with lignified trunks in the Pteridophyta, Arecales, Cycadophyta and Poales such as the tree ferns, palms, cycads and bamboos have different structures and outer coverings.[60]Although the bark functions as a protective barrier, it is itself attacked by boring insects such as beetles. These lay their eggs in crevices and the larvae chew their way through the cellulose tissues leaving a gallery of tunnels. This may allow fungal spores to gain admittance and attack the tree. Dutch elm disease is caused by a fungus (Ophiostoma species) carried from one elm tree to another by various beetles. The tree reacts to the growth of the fungus by blocking off the xylem tissue carrying sap upwards and the branch above, and eventually the whole tree, is deprived of nourishment and dies. In Britain in the 1990s, 25 million elm trees were killed by this disease.[61]The innermost layer of bark is known as the phloem and this is involved in the transport of the sap containing the sugars made by photosynthesis to other parts of the tree. It is a soft spongy layer of living cells, some of which are arranged end to end to form tubes. These are supported by parenchyma cells which provide padding and include fibres for strengthening the tissue.[62] Inside the phloem is a layer of undifferentiated cells one cell thick called the vascular cambium layer. The cells are continually dividing, creating phloem cells on the outside and wood cells known as xylem on the inside.[63]The newly created xylem is the sapwood. It is composed of water-conducting cells and associated cells which are often living, and is usually pale in colour. It transports water and minerals from the roots to the upper parts of the tree. The oldest, inner part of the sapwood is progressively converted into heartwood as new sapwood is formed at the cambium. The conductive cells of the heartwood are blocked in some species, and the surrounding cells are more often dead. Heartwood is usually darker in colour than the sapwood. It is the dense central core of the trunk giving it rigidity. Three quarters of the dry mass of the xylem is cellulose, a polysaccharide, and most of the remainder is lignin, a complex polymer. A transverse section through a tree trunk or a horizontal core will show concentric circles or lighter or darker wood - tree rings. These rings are the annual growth rings[64] There may also be rays running at right angles to growth rings. These are vascular rays which are thin sheets of living tissue permeating the wood.[65] Many older trees may become hollow but may still stand upright for many years.[66]Trees do not usually grow continuously throughout the year but mostly have spurts of active expansion followed by periods of rest. This pattern of growth is related to climatic conditions; growth normally ceases when conditions are either too cold or too dry. In readiness for the inactive period, trees form buds to protect the meristem, the zone of active growth. Before the period of dormancy, the last few leaves produced at the tip of a twig form scales. These are thick, small and closely wrapped and enclose the growing point in a waterproof sheath. Inside this bud there is a rudimentary stalk and neatly folded miniature leaves, ready to expand when the next growing season arrives. Buds also form in the axils of the leaves ready to produce new side shoots. A few trees, such as the eucalyptus, have "naked buds" with no protective scales and some conifers, such as the Lawson's cypress, have no buds but instead have little pockets of meristem concealed among the scale-like leaves.[67]When growing conditions improve, such as the arrival of warmer weather and the longer days associated with spring in temperate regions, growth starts again. The expanding shoot pushes its way out, shedding the scales in the process. These leave behind scars on the surface of the twig. The whole year's growth may take place in just a few weeks. The new stem is unlignified at first and may be green and downy. The Arecaceae (palms) have their leaves spirally arranged on an unbranched trunk.[67] In some tree species in temperate climates, a second spurt of growth, a Lammas growth may occur which is believed to be a strategy to compensate for loss of early foliage to insect predators.[68]Primary growth is the elongation of the stems and roots. Secondary growth consists of a progressive thickening and strengthening of the tissues as the outer layer of the epidermis is converted into bark and the cambium layer creates new phloem and xylem cells. The bark is inelastic.[69] Eventually the growth of a tree slows down and stops and it gets no taller. If damage occurs the tree may in time become hollow.[70]Leaves are structures specialised for photosynthesis and are arranged on the tree in such a way as to maximise their exposure to light without shading each other.[71] They are an important investment by the tree and may be thorny or contain phytoliths, lignins, tannins or poisons to discourage herbivory. Trees have evolved leaves in a wide range of shapes and sizes, in response to environmental pressures including climate and predation. They can be broad or needle-like, simple or compound, lobed or entire, smooth or hairy, delicate or tough, deciduous or evergreen. The needles of coniferous trees are compact but are structurally similar to those of broad-leaved trees. They are adapted for life in environments where resources are low or water is scarce. Frozen ground may limit water availability and conifers are often found in colder places at higher altitudes and higher latitudes than broad leaved trees. In conifers such as fir trees, the branches hang down at an angle to the trunk, enabling them to shed snow. In contrast, broad leaved trees in temperate regions deal with winter weather by shedding their leaves. When the days get shorter and the temperature begins to decrease, the leaves no longer make new chlorophyll and the red and yellow pigments already present in the blades become apparent.[71] Synthesis in the leaf of a plant hormone called auxin also ceases. This causes the cells at the junction of the petiole and the twig to weaken until the joint breaks and the leaf floats to the ground. In tropical and subtropical regions, many trees keep their leaves all year round. Individual leaves may fall intermittently and be replaced by new growth but most leaves remain intact for some time. Other tropical species and those in arid regions may shed all their leaves annually, such as at the start of the dry season.[72] Many deciduous trees flower before the new leaves emerge.[73] A few trees do not have true leaves but instead have structures with similar external appearance such as Phylloclades – modified stem structures[74] – as seen in the genus Phyllocladus.[75]Trees can be pollinated either by wind or by animals, mostly insects. Many angiosperm trees are insect pollinated. Wind pollination may take advantage of increased wind speeds high above the ground.[76] Trees use a variety of methods of seed dispersal. Some rely on wind, with winged or plumed seeds. Others rely on animals, for example with edible fruits. Others again eject their seeds (ballistic dispersal), or use gravity so that seeds fall and sometimes roll.[77]Seeds are the primary way that trees reproduce and their seeds vary greatly in size and shape. Some of the largest seeds come from trees, but the largest tree, Sequoiadendron giganteum, produces one of the smallest tree seeds.[78] The great diversity in tree fruits and seeds reflects the many different ways that tree species have evolved to disperse their offspring.For a tree seedling to grow into an adult tree it needs light. If seeds only fell straight to the ground, competition among the concentrated saplings and the shade of the parent would likely prevent it from flourishing. Many seeds such as birch are small and have papery wings to aid dispersal by the wind. Ash trees and maples have larger seeds with blade shaped wings which spiral down to the ground when released. The kapok tree has cottony threads to catch the breeze.[79]The seeds of conifers, the largest group of gymnosperms, are enclosed in a cone and most species have seeds that are light and papery that can be blown considerable distances once free from the cone.[80] Sometimes the seed remains in the cone for years waiting for a trigger event to liberate it. Fire stimulates release and germination of seeds of the jack pine, and also enriches the forest floor with wood ash and removes competing vegetation.[81] Similarly, a number of angiosperms including Acacia cyclops and Acacia mangium have seeds that germinate better after exposure to high temperatures.[82]The flame tree Delonix regia does not rely on fire but shoots its seeds through the air when the two sides of its long pods crack apart explosively on drying.[79] The miniature cone-like catkins of alder trees produce seeds that contain small droplets of oil that help disperse the seeds on the surface of water. Mangroves often grow in water and some species have propagules, which are buoyant fruits with seeds that start germinating before becoming detached from the parent tree.[83][84] These float on the water and may become lodged on emerging mudbanks and successfully take root.[79]Other seeds, such as apple pips and plum stones, have fleshy receptacles and smaller fruits like hawthorns have seeds enclosed in edible tissue; animals including mammals and birds eat the fruits and either discard the seeds, or swallow them so they pass through the gut to be deposited in the animal's droppings well away from the parent tree. The germination of some seeds is improved when they are processed in this way.[85] Nuts may be gathered by animals such as squirrels that cache any not immediately consumed.[86] Many of these caches are never revisited, the nut-casing softens with rain and frost, and the seed germinates in the spring.[87] Pine cones may similarly be hoarded by red squirrels, and grizzly bears may help to disperse the seed by raiding squirrel caches.[88]The single extant species of Ginkgophyta (Ginkgo biloba) has fleshy seeds produced at the ends of short branches on female trees,[89] and Gnetum, a tropical and subtropical group of gymnosperms produce seeds at the tip of a shoot axis.[90]The earliest trees were tree ferns, horsetails and lycophytes, which grew in forests in the Carboniferous period. The first tree may have been Wattieza, fossils of which have been found in New York State in 2007 dating back to the Middle Devonian (about 385 million years ago). Prior to this discovery, Archaeopteris was the earliest known tree.[91] Both of these reproduced by spores rather than seeds and are considered to be links between ferns and the gymnosperms which evolved in the Triassic period. The gymnosperms include conifers, cycads, gnetales and ginkgos and these may have appeared as a result of a whole genome duplication event which took place about 319 million years ago.[92] Ginkgophyta was once a widespread diverse group[93] of which the only survivor is the maidenhair tree Ginkgo biloba. This is considered to be a living fossil because it is virtually unchanged from the fossilised specimens found in Triassic deposits.[94]During the Mesozoic (245 to 66 million years ago) the conifers flourished and became adapted to live in all the major terrestrial habitats. Subsequently, the tree forms of flowering plants evolved during the Cretaceous period. These began to displace the conifers during the Tertiary era (66 to 2 million years ago) when forests covered the globe.[95] When the climate cooled 1.5 million years ago and the first of four ice ages occurred, the forests retreated as the ice advanced. In the interglacials, trees recolonised the land that had been covered by ice, only to be driven back again in the next ice age.[95]Trees are an important part of the terrestrial ecosystem,[96] providing essential habitats including many kinds of forest for communities of organisms. Epiphytic plants such as ferns, some mosses, liverworts, orchids and some species of parasitic plants (e.g., mistletoe) hang from branches;[97] these along with arboreal lichens, algae, and fungi provide micro-habitats for themselves and for other organisms, including animals. Leaves, flowers and fruits are seasonally available. On the ground underneath trees there is shade, and often there is undergrowth, leaf litter, and decaying wood that provide other habitat.[98][99] Trees stabilise the soil, prevent rapid run-off of rain water, help prevent desertification, have a role in climate control and help in the maintenance of biodiversity and ecosystem balance.[100]Many species of tree support their own specialised invertebrates. In their natural habitats, 284 different species of insect have been found on the English oak (Quercus robur)[101] and 306 species of invertebrate on the Tasmanian oak (Eucalyptus obliqua).[102] Non-native tree species provide a less biodiverse community, for example in the United Kingdom the sycamore (Acer pseudoplatanus), which originates from southern Europe, has few associated invertebrate species, though its bark supports a wide range of lichens, bryophytes and other epiphytes.[103]In ecosystems such as mangrove swamps, trees play a role in developing the habitat, since the roots of the mangrove trees reduce the speed of flow of tidal currents and trap water-borne sediment, reducing the water depth and creating suitable conditions for further mangrove colonisation. Thus mangrove swamps tend to extend seawards in suitable locations.[104] Mangrove swamps also provide an effective buffer against the more damaging effects of cyclones and tsunamis.[105]Silviculture is the practice of controlling the establishment, growth, composition, health, and quality of forests, which are areas that have a high density of trees. Cultivated trees are planted and tended by humans, usually because they provide food (fruits or nuts), ornamental beauty, or some type of wood product that benefits people. An area of land planted with fruit or nut trees is an orchard.[106] A small wooded area, usually with no undergrowth, is called a grove[107] and a small wood or thicket of trees and bushes is called a coppice or copse.[108] A large area of land covered with trees and undergrowth is called woodland or forest.[109] An area of woodland composed primarily of trees established by planting or artificial seeding is known as a plantation.[110]Trees are the source of many of the world's best known fleshy fruits. Apples, pears, plums, cherries and citrus are all grown commercially in temperate climates and a wide range of edible fruits are found in the tropics. Other commercially important fruit include dates, figs and olives. Palm oil is obtained from the fruits of the oil palm (Elaeis guineensis). The fruits of the cocoa tree (Theobroma cacao) are used to make cocoa and chocolate and the berries of coffee trees, Coffea arabica and Coffea canephora, are processed to extract the coffee beans. In many rural areas of the world, fruit is gathered from forest trees for consumption.[111] Many trees bear edible nuts which can loosely be described as being large, oily kernels found inside a hard shell. These include coconuts (Cocos nucifera), Brazil nuts (Bertholletia excelsa), pecans (Carya illinoinensis), hazel nuts (Corylus), almonds (Prunus dulcis), walnuts (Juglans regia), pistachios (Pistacia vera) and many others. They are high in nutritive value and contain high-quality protein, vitamins and minerals as well as dietary fibre. Walnuts are particularly beneficial to health and contain a higher level of antioxidants than do other nuts.[112] A variety of nut oils are extracted by pressing for culinary use; some such as walnut, pistachio and hazelnut oils are prized for their distinctive flavours, but they tend to spoil quickly.[113]In temperate climates there is a sudden movement of sap at the end of the winter as trees prepare to burst into growth. In North America, the sap of the sugar maple (Acer saccharum) is most often used in the production of a sweet liquid, maple syrup. About 90% of the sap is water, the remaining 10% being a mixture of various sugars and certain minerals.[114] The sap is harvested by drilling holes in the trunks of the trees and collecting the liquid that flows out of the inserted spigots. It is piped to a sugarhouse where it is heated to concentrate it and improve its flavour. One litre of maple syrup is obtained from every forty litres of sap and has a sugar content of exactly 66%.[114] Similarly in northern Europe the spring rise in the sap of the silver birch (Betula pendula) is tapped and collected, either to be drunk fresh or fermented into an alcoholic drink. In Alaska, the sap of the sweet birch (Betula lenta) is made into a syrup with a sugar content of 67%. Sweet birch sap is more dilute than maple sap; a hundred litres are required to make one litre of birch syrup.[115]Various parts of trees are used as spices. These include cinnamon, made from the bark of the cinnamon tree (Cinnamomum zeylanicum) and allspice, the dried small fruits of the pimento tree (Pimenta dioica). Nutmeg is a seed found in the fleshy fruit of the nutmeg tree (Myristica fragrans) and cloves are the unopened flower buds of the clove tree (Syzygium aromaticum).[116]Many trees have flowers rich in nectar which are attractive to bees. The production of forest honey is an important industry in rural areas of the developing world where it is undertaken by small-scale beekeepers using traditional methods.[117] The flowers of the elder (Sambucus) are used to make elderflower cordial and petals of the plum (Prunus spp.) can be candied.[118] Sassafras oil is a flavouring obtained from distilling bark from the roots of the white sassafras tree (Sassafras albidum).The leaves of trees are widely gathered as fodder for livestock and some can be eaten by humans but they tend to be high in tannins which makes them bitter. Leaves of the curry tree (Murraya koenigii) are eaten, those of kaffir lime (Citrus × hystrix) (in Thai food)[119] and Ailanthus (in Korean dishes such as bugak) and those of the European bay tree (Laurus nobilis) and the California bay tree (Umbellularia californica) are used for flavouring food.[116] Camellia sinensis, the source of tea, is a small tree but seldom reaches its full height, being heavily pruned to make picking the leaves easier.[120]Wood has traditionally been used for fuel, especially in rural areas. In less developed nations it may be the only fuel available and collecting firewood is often a time consuming task as it becomes necessary to travel further and further afield in the search for fuel.[121] It is often burned inefficiently on an open fire. In more developed countries other fuels are available and burning wood is a choice rather than a necessity. Modern wood-burning stoves are very fuel efficient and new products such as wood pellets are available to burn.[122]Charcoal can be made by slow pyrolysis of wood by heating it in the absence of air in a kiln. The carefully stacked branches, often oak, are burned with a very limited amount of air. The process of converting them into charcoal takes about fifteen hours. Charcoal is used as a fuel in barbecues and by blacksmiths and has many industrial and other uses.[123]Wood smoke can be used to preserve food. In the hot smoking process the food is exposed to smoke and heat in a controlled environment. The food is ready to eat when the process is complete, having been tenderised and flavoured by the smoke it has absorbed. In the cold process, the temperature is not allowed to rise above 100 °F (38 °C). The flavour of the food is enhanced but raw food requires further cooking. If it is to be preserved, meat should be cured before cold smoking.[124]Timber, "trees that are grown in order to produce wood"[125] is cut into lumber (sawn wood) for use in construction. Wood has been an important, easily available material for construction since humans started building shelters. Engineered wood products are available which bind the particles, fibres or veneers of wood together with adhesives to form composite materials. Plastics have taken over from wood for some traditional uses.[126]Wood is used in the construction of buildings, bridges, trackways, piles, poles for power lines, masts for boats, pit props, railway sleepers, fencing, hurdles, shuttering for concrete, pipes, scaffolding and pallets. In housebuilding it is used in joinery, for making joists, roof trusses, roofing shingles, thatching, staircases, doors, window frames, floor boards, parquet flooring, panelling and cladding.[127]Wood is used to construct carts, farm implements, boats, dugout canoes and in shipbuilding. It is used for making furniture, tool handles, boxes, ladders, musical instruments, bows, weapons, matches, clothes pegs, brooms, shoes, baskets, turnery, carving, toys, pencils, rollers, cogs, wooden screws, barrels, coffins, skittles, veneers, artificial limbs, oars, skis, wooden spoons, sports equipment and wooden balls.[127]Wood is pulped for paper and used in the manufacture of cardboard and made into engineered wood products for use in construction such as fibreboard, hardboard, chipboard and plywood.[127] The wood of conifers is known as softwood while that of broad-leaved trees is hardwood.[128]Besides inspiring artists down the centuries, trees have been used to create art. Living trees have been used in bonsai and in tree shaping, and both living and dead specimens have been sculpted into sometimes fantastic shapes.[129]Bonsai (盆栽,  lit. The art of growing a miniature tree or trees in a low-sided pot or tray) is the practice of hòn non bộ originated in China and spread to Japan more than a thousand years ago, there are similar practices in other cultures like the living miniature landscapes of Vietnam hòn non bộ. The word bonsai is often used in English as an umbrella term for all miniature trees in containers or pots.[130]The purposes of bonsai are primarily contemplation (for the viewer) and the pleasant exercise of effort and ingenuity (for the grower).[131] Bonsai practice focuses on long-term cultivation and shaping of one or more small trees growing in a container, beginning with a cutting, seedling, or small tree of a species suitable for bonsai development. Bonsai can be created from nearly any perennial woody-stemmed tree or shrub species[132] that produces true branches and can be cultivated to remain small through pot confinement with crown and root pruning. Some species are popular as bonsai material because they have characteristics, such as small leaves or needles, that make them appropriate for the compact visual scope of bonsai and a miniature deciduous forest can even be created using such species as Japanese maple, Japanese zelkova or hornbeam.[133]Tree shaping is the practice of changing living trees and other woody plants into man made shapes for art and useful structures. There are a few different methods[134] of shaping a tree. There is a gradual method and there is an instant method. The gradual method slowly guides the growing tip along predetermined pathways over time whereas the instant method bends and weaves saplings 2 to 3 m (6.6 to 9.8 ft) long into a shape that becomes more rigid as they thicken up.[135] Most artists use grafting of living trunks, branches, and roots, for art or functional structures and there are plans to grow "living houses" with the branches of trees knitting together to give a solid, weatherproof exterior combined with an interior application of straw and clay to provide a stucco-like inner surface.[135]Tree shaping has been practised for at least several hundred years, the oldest known examples being the living root bridges built and maintained by the Khasi people of Meghalaya, India using the roots of the rubber tree (Ficus elastica).[136][137]Cork is produced from the thick bark of the cork oak (Quercus suber). It is harvested from the living trees about once every ten years in an environmentally sustainable industry.[138] More than half the world's cork comes from Portugal and is largely used to make stoppers for wine bottles.[139] Other uses include floor tiles, bulletin boards, balls, footwear, cigarette tips, packaging, insulation and joints in woodwind instruments.[139]The bark of other varieties of oak has traditionally been used in Europe for the tanning of hides though bark from other species of tree has been used elsewhere. The active ingredient, tannin, is extracted and after various preliminary treatments, the skins are immersed in a series of vats containing solutions in increasing concentrations. The tannin causes the hide to become supple, less affected by water and more resistant to bacterial attack.[140]At least 120 drugs come from plant sources, many of them from the bark of trees.[141] Quinine originates from the cinchona tree (Cinchona) and was for a long time the remedy of choice for the treatment of malaria.[142] Aspirin was synthesised to replace the sodium salicylate derived from the bark of willow trees (Salix) which had unpleasant side effects.[143] The anti-cancer drug Paclitaxel is derived from taxol, a substance found in the bark of the Pacific yew (Taxus brevifolia).[144] Other tree based drugs come from the paw-paw (Carica papaya), the cassia (Cassia spp.), the cocoa tree (Theobroma cacao), the tree of life (Camptotheca acuminata) and the downy birch (Betula pubescens).[141]The papery bark of the white birch tree (Betula papyrifera) was used extensively by Native Americans. Wigwams were covered by it and canoes were constructed from it. Other uses included food containers, hunting and fishing equipment, musical instruments, toys and sledges.[145] Nowadays, bark chips, a by-product of the timber industry, are used as a mulch and as a growing medium for epiphytic plants that need a soil-free compost.[146]Trees create a visual impact in the same way as do other landscape features and give a sense of maturity and permanence to park and garden. They are grown for the beauty of their forms, their foliage, flowers, fruit and bark and their siting is of major importance in creating a landscape. They can be grouped informally, often surrounded by plantings of bulbs, laid out in stately avenues or used as specimen trees. As living things, their appearance changes with the season and from year to year.[147]Trees are often planted in town environments where they are known as street trees or amenity trees. They can provide shade and cooling through evapotranspiration, absorb greenhouse gases and pollutants, intercept rainfall, and reduce the risk of flooding. It has been shown that they are beneficial to humans in creating a sense of well-being and reducing stress. Many towns have initiated tree-planting programmes.[148] In London for example, there is an initiative to plant 20,000 new street trees and to have an increase in tree cover of 5% by 2025, equivalent to one tree for every resident.[149]Latex is a sticky defensive secretion that protects plants against herbivores. Many trees produce it when injured but the main source of the latex used to make natural rubber is the Pará rubber tree (Hevea brasiliensis). Originally used to create bouncy balls and for the waterproofing of cloth, natural rubber is now mainly used in tyres for which synthetic materials have proved less durable.[150] The latex exuded by the balatá tree (Manilkara bidentata) is used to make golf balls and is similar to gutta-percha, made from the latex of the "getah perca" tree Palaquium. This is also used as an insulator, particularly of undersea cables, and in dentistry, walking sticks and gun butts. It has now largely been replaced by synthetic materials.[151]Resin is another plant exudate that may have a defensive purpose. It is a viscous liquid composed mainly of volatile terpenes and is produced mostly by coniferous trees. It is used in varnishes, for making small castings and in ten-pin bowling balls. When heated, the terpenes are driven off and the remaining product is called "rosin" and is used by stringed instrumentalists on their bows. Some resins contain essential oils and are used in incense and aromatherapy. Fossilised resin is known as amber and was mostly formed in the Cretaceous (145 to 66 million years ago) or more recently. The resin that oozed out of trees sometimes trapped insects or spiders and these are still visible in the interior of the amber.[152]The camphor tree (Cinnamomum camphora) produces an essential oil[116] and the eucalyptus tree (Eucalyptus globulus) is the main source of eucalyptus oil which is used in medicine, as a fragrance and in industry.[153]Dead trees pose a safety risk, especially during high winds and severe storms, and removing dead trees involves a financial burden, whereas the presence of healthy trees can clean the air, increase property values, and reduce the temperature of the built environment and thereby reduce building cooling costs. During times of drought, trees can fall into water stress, which may cause a tree to become more susceptible to disease and insect problems, and ultimately may lead to a tree's death. Irrigating trees during dry periods can reduce the risk of water stress and death.[154]Trees have been venerated since time immemorial. To the ancient Celts, certain trees, especially the oak, ash and thorn, held special significance[155] as providing fuel, building materials, ornamental objects and weaponry. Other cultures have similarly revered trees, often linking the lives and fortunes of individuals to them or using them as oracles. In Greek mythology, dryads were believed to be shy nymphs who inhabited trees.The Oubangui people of west Africa plant a tree when a child is born. As the tree flourishes, so does the child but if the tree fails to thrive, the health of the child is considered at risk. When it flowers it is time for marriage. Gifts are left at the tree periodically and when the individual dies, their spirit is believed to live on in the tree.[156]Trees have their roots in the ground and their trunk and branches extended towards the sky. This concept is found in many of the world's religions as a tree which links the underworld and the earth and holds up the heavens. In Norse mythology, Yggdrasil is a central cosmic tree whose roots and branches extend to various worlds. Various creatures live on it.[157] In India, Kalpavriksha is a wish-fulfilling tree, one of the nine jewels that emerged from the primitive ocean. Icons are placed beneath it to be worshipped, tree nymphs inhabit the branches and it grants favours to the devout who tie threads round the trunk.[158] Democracy started in North America when the Great Peacemaker formed the Iroquois Confederacy, inspiring the warriors of the original five American nations to bury their weapons under the Tree of Peace, an eastern white pine (Pinus strobus).[159] In the creation story in the Bible, the tree of life and the knowledge of good and evil was planted by God in the Garden of Eden.[160]Sacred groves exist in China, India, Africa and elsewhere. They are places where the deities live and where all the living things are either sacred or are companions of the gods. Folklore lays down the supernatural penalties that will result if desecration takes place for example by the felling of trees. Because of their protected status, sacred groves may be the only relicts of ancient forest and have a biodiversity much greater than the surrounding area.[161]Some Ancient Indian tree deities, such as Puliyidaivalaiyamman, the Tamil deity of the  tamarind tree, or Kadambariyamman, associated with the kadamba tree were seen as manifestations of a goddess who offers her blessings by giving fruits in abundance.[162]Trees have a theoretical maximum height of 130 m (430 ft),[163] but the tallest known specimen on earth is believed to be a coast redwood (Sequoia sempervirens) at Redwood National Park, California. It has been named Hyperion and is 115.85 m (380.1 ft) tall.[164] In 2006, it was reported to be 379.1 ft (115.5 m) tall.[165] The tallest known broad-leaved tree is a mountain ash (Eucalyptus regnans) growing in Tasmania with a height of 99.8 m (327 ft).[166]The largest tree by volume is believed to be a giant sequoia (Sequoiadendron giganteum) known as the General Sherman Tree in the Sequoia National Park in Tulare County, California. Only the trunk is used in the calculation and the volume is estimated to be 1,487 m3 (52,500 cu ft).[167]The oldest living tree with a verified age is also in California. It is a Great Basin bristlecone pine (Pinus longaeva) growing in the White Mountains. It has been dated by drilling a core sample and counting the annual rings. It is estimated to currently be 5,068 years old.[a][168]A little farther south, at Santa Maria del Tule, Oaxaca, Mexico, is the tree with the broadest trunk. It is a Montezuma cypress (Taxodium mucronatum) known as Árbol del Tule and its diameter at breast height is 11.62 m (38.1 ft) giving it a girth of 36.2 m (119 ft). The tree's trunk is far from round and the exact dimensions may be misleading as the circumference includes much empty space between the large buttress roots.[169]Wohlleben, Peter; Flannery, Tim F.; Simard, S.; Billinghurst, Jane (2016). The Hidden Life of Trees: What They Feel, How They Communicate: Discoveries from a Secret World. ISBN 9781771642484. OCLC 933722592.
Monocotyledon
An economically important monocotMonocotyledons (/ˌmɒnəˌkɒtəlˈiːdən/),[d][13][14] commonly referred to as monocots, (Lilianae sensu Chase & Reveal) are flowering plants (angiosperms) whose seeds typically contain only one embryonic leaf, or cotyledon. They constitute one of the major groups into which the flowering plants have traditionally been divided, the rest of the flowering plants having two cotyledons and therefore classified as dicotyledons, or dicots. However, molecular phylogenetic research has shown that while the monocots form a monophyletic group or clade (comprising all the descendants of a common ancestor), the dicots do not. Monocots have almost always been recognized as a group, but with various taxonomic ranks and under several different names. The APG III system of 2009 recognises a clade called "monocots" but does not assign it to a taxonomic rank.The monocots include about 60,000 species. The largest family in this group (and in the flowering plants as a whole) by number of species are the orchids (family Orchidaceae), with more than 20,000 species. About half as many species belong to the true grasses (Poaceae), which are economically the most important  family of monocots. In agriculture the majority of the biomass produced comes from monocots. These include not only major grains (rice, wheat, maize, etc.), but also forage grasses, sugar cane, and the bamboos. Other economically important monocot crops include various palms (Arecaceae), bananas and plantains (Musaceae), gingers and their relatives, turmeric and cardamom (Zingiberaceae), asparagus (Asparagaceae), pineapple (Bromeliaceae), water chestnut (Cyperaceae), and leeks, onion and garlic (Amaryllidaceae). Many houseplants are monocot epiphytes. Additionally most of the horticultural bulbs, plants cultivated for their blooms, such as lilies, daffodils, irises, amaryllis, cannas, bluebells and tulips, are monocots.The monocots or monocotyledons have, as the name implies, a single (mono-) cotyledon, or embryonic leaf, in their seeds. Historically, this feature was used to contrast the monocots with the dicotyledons or dicots which typically have two cotyledons; however modern research has shown that the dicots are not a natural group, and the term can only be used to indicate all angiosperms that are not monocots and is used in that respect here. From a diagnostic point of view the number of cotyledons is neither a particularly useful characteristic (as they are only present for a very short period in a plant's life), nor is it completely reliable. The single cotyledon is only one of a number of modifications of the body plan of the ancestral monocotyledons, whose adaptive advantages are poorly understood, but may have been related to adaption to aquatic habitats, prior to radiation to terrestrial habitats. Nevertheless, monocots are sufficiently distinctive that there has rarely been disagreement as to membership of this group, despite considerable diversity in terms of external morphology.[15] However, morphological features that reliably characterise major clades are rare.[16]Thus monocots are distinguishable from other angiosperms both in terms of their uniformity and diversity. On the one hand the organisation of the shoots, leaf structure and floral configuration are more uniform than in the remaining angiosperms, yet within these constraints a wealth of diversity exists, indicating a high degree of evolutionary success.[17] Monocot diversity includes perennial geophytes such as ornamental flowers including (orchids (Asparagales), tulips and lilies) (Liliales), rosette and succulent epiphytes (Asparagales), mycoheterotrophs (Liliales, Dioscoreales, Pandanales), all in the lilioid monocots, major cereal grains (maize, rice, barley, rye and wheat) in the grass family and forage grasses (Poales) as well as woody tree-like palm trees (Arecales), bamboo, reeds and bromeliads (Poales), bananas and ginger (Zingiberales) in the commelinid monocots, as well as both emergent (Poales, Acorales) and aroids, as well as floating or submerged aquatic plants such as seagrass (Alismatales).[18][19][20][21]The most important distinction is their growth pattern, lacking a lateral meristem (cambium) that allows for continual growth in diameter with height (secondary growth), and therefore this characteristic is a basic limitation in shoot construction. Although largely herbaceous, some arboraceous monocots reach great height, length and mass. The latter include agaves, palms, pandans, and bamboos.[22][23] This creates challenges in water transport that monocots deal with in various ways. Some, such as species of Yucca, develop anomalous secondary growth, while palm trees utilise an anomalous primary growth form described as establishment growth (see Vascular system). The axis undergoes primary thickening, that progresses from internode to internode, resulting in a typical inverted conical shape of the basal primary axis (see Tillich, Figure 1). The limited conductivity also contributes to limited branching of the stems. Despite these limitations a wide variety of adaptive growth forms has resulted (Tillich, Figure 2) from epiphytic orchids (Asparagales) and bromeliads (Poales) to submarine Alismatales (including the reduced Lemnoideae) and mycotrophic Burmanniaceae (Dioscreales) and Triuridaceae (Pandanales). Other forms of adaptation include the climbing vines of Araceae (Alismatales) which use negative phototropism (skototropism) to locate host trees (i.e. the darkest area),[24] while some palms such as Calamus manan (Arecales) produce the longest shoots in the plant kingdom, up to 185 m long.[25] Other monocots, particularly Poales, have adopted a therophyte life form.[26][27][28][29][30]The cotyledon, the primordial Angiosperm leaf consists of a proximal leaf base or hypophyll and a distal hyperphyll. In monocots the hypophyll tends to be the dominant part in contrast to other angiosperms. From these, considerable diversity arises. Mature monocot leaves are generally narrow and linear, forming a sheathing around the stem at its base, although there are many exceptions. Leaf venation is of the striate type, mainly arcuate-striate or longitudinally striate (parallel), less often palmate-striate or pinnate-striate with the leaf veins emerging at the leaf base and then running together at the apices. There is usually only one leaf per node because the leaf base encompasses more than half the circumference.[31] The evolution of this monocot characteristic has been attributed to developmental differences in early zonal differentiation rather than meristem activity (leaf base theory).[15][16][32]The lack of cambium in the primary root limits its ability to grow sufficiently to maintain the plant. This necessitates early development of roots derived from the shoot (adventitious roots). In addition to roots, monocots develop runners and rhizomes, which are creeping shoots. Runners serve vegetative propagation, have elongated internodes, run on or just below the surface of the soil and in most case bear scale leaves. Rhizomes frequently have an additional storage function and rhizome producing plants are considered geophytes (Tillich, Figure 11). Other geophytes develop bulbs, a short axial body bearing leaves whose bases store food. Additional outer non-storage leaves may form a protective function (Tillich, Figure 12). Other storage organs may be tubers or corms, swollen axes. Tubers may form at the end of underground runners and persist. Corms are short lived vertical shoots with terminal inflorescences and shrivel once flowering has occurred. However, intermediate forms may occur such as in Crocosmia (Asparagales). Some monocots may also produce shoots that grow directly down into the soil, these are geophilous shoots (Tillich, Figure 11) that help overcome the limited trunk stability of large woody monocots.[33][32][34][15]In nearly all cases the perigone consists of two alternating trimerous whorls of tepals, being homochlamydeous, without differentiation between calyx and corolla. In zoophilous (pollinated by animals) taxa, both whorls are corolline (petal-like). Anthesis (the period of flower opening) is usually fugacious (short lived). Some of the more persistent perigones demonstrate thermonastic opening and closing (responsive to changes in temperature). About two thirds of monocots are zoophilous, predominantly by insects. These plants need to advertise to pollinators and do so by way of phaneranthous (showy) flowers. Such optical signalling is usually a function of the tepal whorls but may also be provided by semaphylls (other structures such as filaments, staminodes or stylodia which have become modified to attract pollinators). However, some monocot plants may have aphananthous (inconspicuous) flowers and still be pollinated by animals. In these the plants rely either on chemical attraction or other structures such as coloured bracts fulfill the role of optical attraction. In some phaneranthous plants such structures may reinforce floral structures. The production of fragrances for olfactory signalling are common in monocots. The perigone also functions as a landing platform for pollinating insects.[17]The embryo consists of a single cotyledon, usually with two vascular bundles.[32]The traditionally listed differences between monocots and "dicots" are as follows. This is a broad sketch only, not invariably applicable, as there are a number of exceptions. The differences indicated are more true for monocots versus eudicots.[34][35][36]A number of these differences are not unique to the monocots, and while still useful no one single feature, will infallibly identify a plant as a monocot.[35] For example, trimerous flowers and monosulcate pollen are also found in magnoliids,[34] of which exclusively adventitious roots are found in some of the Piperaceae.[34] Similarly, at least one of these traits, parallel leaf veins, is far from universal among the monocots. Monocots with broad leaves and reticulate leaf veins, typical of dicots, are found in a wide variety of monocot families: for example, Trillium, Smilax (greenbriar), and Pogonia (an orchid), and the Dioscoreales (yams).[34] Potamogeton are one of several monocots with tetramerous flowers. Other plants exhibit a mixture of characteristics. Nymphaeaceae (water lilies) have reticulate veins, a single cotyledon, adventitious roots and a monocot like vascular bundle. These examples reflect their shared ancestry.[35] Nevertheless, this list of traits is a generally valid set of contrasts, especially when contrasting monocots with eudicots rather than non-monocot flowering plants in general.[34]Monocot apomorphies (characteristics that are derived during radiation rather than inherited from an ancestral form) include herbaceous habit, leaves with parallel venation and sheathed base, embryo with a single cotyledon, atactostele stele, numerous adventitious roots, sympodial growth, and trimerous (3 parts per whorl) flowers that are pentacyclic (5 whorled) with 3 sepals, 3 petals, 2 whorls of 3 stamens each and 3 carpels. In contrast monosculate pollen is considered an ancestral trait, probably plesiomorphic.[36]The distinctive features of the monocots have contributed to the relative taxonomic stability of the group. Douglas E. Soltis and others[37][38][39][40] identify thirteen synapomorphies (shared characteristics that unite monophyletic groups of taxa);Monocots have a distinctive arrangement of vascular tissue known as an atactostele in which the vascular tissue is scattered rather than arranged in concentric rings. Collenchyma is absent in monocot stems, roots and leaves. Many monocots are herbaceous and do not have the ability to increase the width of a stem (secondary growth) via the same kind of vascular cambium found in non-monocot woody plants.[34] However, some monocots do have secondary growth, and because it does not arise from a single vascular cambium producing xylem inwards and phloem outwards, it is termed "anomalous secondary growth".[42]  Examples of large monocots which either exhibit secondary growth, or can reach large sizes without it, are palms (Arecaceae), screwpines (Pandanaceae), bananas (Musaceae), Yucca, Aloe, Dracaena, and Cordyline.[34]The monocots form one of five major lineages of mesangiosperms (core angiosperms), which in themselves form 99.95% of all angiosperms. The monocots and the eudicots, are the largest and most diversified angiosperm radiations accounting for 22.8% and 74.2% of all angiosperm species respectively.[43]Of these, the grass family (Poaceae) is the most economically important, which together with the orchids Orchidaceae  account for half of the species diversity, accounting for 34% and 17% of all monocots respectively and are among the largest families of angiosperms. They are also among the dominant members of many plant communities.[43]The monocots are one of the major divisions of the flowering plants or angiosperms. They have been recognized as a natural group since the sixteenth century when Lobelius (1571), searching for a characteristic to group plants by, decided on leaf form and their venation. He observed that the majority had broad leaves with net-like venation, but a smaller group were grass-like plants with long straight parallel veins.[44] In doing so he distinguished between the dicotyledons, and the latter (grass-like) monocotyledon group, although he had no formal names for the two groups.[45][46][47]Formal description dates from John Ray's studies of seed structure in the 17th century. Ray, who is often considered the first botanical systematist,[48] observed the dichotomy of cotyledon structure in his examination of seeds. He reported his findings in a paper read to the Royal Society on 17 December 1674, entitled "A Discourse on the Seeds of Plants".[34]The greatest number of plants that come of seed spring at first out of the earth with two leaves which being for the most part of a different figure from the succeeding leaves are by our gardeners not improperly called the seed leaves...  In the first kind the seed leaves are nothing but the two lobes of the seed having their plain sides clapt together like the two halfs of a walnut and therefore are of the just figure of the seed slit in sunder flat wise...Of seeds that spring out of the earth with leaves like the succeeding and no seed leaves I have observed two sorts. 1. Such as are congenerous to the first kind precedent that is whose pulp is divided into two lobes and a radicle... 2. Such which neither spring out of the ground with seed leaves nor have their pulp divided into lobesJohn Ray (1674), pp. 164, 166[49] Since this paper appeared a year before the publication of Malpighi's Anatome Plantarum (1675–1679), Ray has the priority. At the time, Ray did not fully realise the importance of his discovery[50] but progressively developed this over successive publications. And since these were in Latin, "seed leaves" became folia seminalia[51] and then cotyledon, following Malpighi.[52][53] Malpighi and Ray were familiar with each other's work,[50] and Malpighi in describing the same structures had introduced the term cotyledon,[54] which Ray adopted in his subsequent writing.Mense quoque Maii, alias seminales plantulas Fabarum, & Phaseolorum, ablatis pariter binis seminalibus foliis, seu cotyledonibus, incubandas posuiIn the month of May, also, I incubated two seed plants, Faba and Phaseolus, after removing the two seed leaves, or cotyledonsMarcello Malpighi (1679), p. 18[54] In this experiment, Malpighi also showed that the cotyledons were critical to the development of the plant, proof that Ray required for his theory.[55] In his Methodus plantarum nova[56] Ray also developed and justified the "natural" or pre-evolutionary approach to classification, based on characteristics selected a posteriori in order to group together taxa that have the greatest number of shared characteristics. This approach, also referred to as polythetic would last till evolutionary theory enabled Eichler to develop the phyletic system that superseded it in the late nineteenth century, based on an understanding of the acquisition of characteristics.[57][58][59] He also made the crucial observation Ex hac seminum divisione sumum potest generalis plantarum distinctio, eaque meo judicio omnium prima et longe optima, in eas sci. quae plantula seminali sunt bifolia aut διλόβω, et quae plantula sem. adulta analoga. (From this division of the seeds derives a general distinction amongst plants, that in my judgement is first and by far the best, into those seed plants which are bifoliate, or bilobed, and those that are analogous to the adult), that is between monocots and dicots.[60][55] He illustrated this with by quoting from Malpighi and including reproductions of Malpighi's drawings of cotyledons (see figure).[61] Initially Ray did not develop a classification of flowering plants (florifera) based on a division by the number of cotyledons, but developed his ideas over successive publications,[62] coining the terms Monocotyledones and Dicotyledones in 1703,[63] in the revised version of his Methodus (Methodus plantarum emendata), as a primary method for dividing them, Herbae floriferae, dividi possunt, ut diximus, in Monocotyledones & Dicotyledones (Flowering plants, can be divided, as we have said, into Monocotyledons & Dicotyledons).[64]Although Linnaeus (1707–1778) did not utilise Ray's discovery, basing his own classification solely on floral reproductive morphology, the term was used shortly after his classification appeared (1753) by Scopoli and who is credited for its introduction.[g] Every taxonomist since then, starting with De Jussieu and De Candolle, has used Ray's distinction as a major classification characteristic.[h][33] In De Jussieu's system (1789), he followed Ray, arranging his Monocotyledones into three classes based on stamen position and placing them between Acotyledones and Dicotyledones.[68] De Candolle's system (1813) which was to predominate thinking through much of the 19th century used a similar general arrangement, with two subgroups of his Monocotylédonés (Monocotyledoneae).[3] Lindley (1830) followed De Candolle in using the terms Monocotyledon and Endogenae[i] interchangeably. They considered the monocotyledons to be a group of  vascular plants (Vasculares) whose vascular bundles were thought to arise from within (Endogènes or endogenous).[69]Monocotyledons remained in a similar position as a major division of the flowering plants throughout the nineteenth century, with minor variations. George Bentham and Hooker (1862–1883) used Monocotyledones, as would Wettstein,[70] while August Eichler used Mononocotyleae[10] and Engler, following de Candolle, Monocotyledoneae.[71] In the twentieth century, some authors used alternative names such as Bessey's (1915) Alternifoliae[2] and Cronquist's (1966) Liliatae.[1] Later (1981) Cronquist changed Liliatae to Liliopsida,[72] usages also adopted by Takhtajan simultaneously.[32]  Thorne (1992)[8] and Dahlgren (1985)[73] also used Liliidae as a synonym.Taxonomists had considerable latitude in naming this group, as the Monocotyledons were a group above the rank of family. Article 16 of the ICBN allows either a descriptive name or a name formed from the name of an included family.In summary they have been variously named, as follows:Over the 1980s, a more general review of the classification of angiosperms was undertaken. The 1990s saw considerable progress in plant phylogenetics and cladistic theory, initially based on rbcL gene sequencing and cladistic analysis, enabling a phylogenetic tree to be constructed for the flowering plants.[74] The establishment of major new clades necessitated a departure from the older but widely used classifications such as Cronquist and Thorne, based largely on morphology rather than genetic data. These developments complicated discussions on plant evolution and necessitated a major taxonomic restructuring.[75][76]This DNA based molecular phylogenetic research confirmed on the one hand that the monocots remained as a well defined monophyletic group or clade, in contrast to the other historical divisions of the flowering plants, which had to be substantially reorganized.[34] No longer could the angiosperms be simply divided into monocotyledons, and dicotyledons but it was apparent that the monocotyledons were but one of a relatively large number of defined groups within the angiosperms.[77] Correlation with morphological criteria showed that the defining feature was not cotyledon number but the separation of angiosperms into two major pollen types, uniaperturate (monosulcate and monosulcate-derived) and triaperturate (tricolpate and tricolpate-derived), with the monocots situated within the uniaperturate groups.[74] The formal taxonomic ranking of Monoctyledons thus became replaced with monocots as an informal clade.[78][34] This is the name that has been most commonly used since the publication of the Angiosperm Phylogeny Group (APG) system in 1998 and regularly updated since.[75][79][76][80][81][82]Within the angiosperms, there are two major grades, a small early branching basal grade, the basal angiosperms (ANA grade) with three lineages and a larger late branching grade, the core angiosperms (mesangiosperms) with five lineages, as shown in the cladogram.Amborellales  Nymphaeales  Austrobaileyales  magnoliids  Chloranthales  monocots  Ceratophyllales  eudicots  While the monocotyledons have remained extremely stable in their outer borders as a well-defined and coherent monophylectic group, the deeper internal relationships have undergone considerable flux, with many competing classification systems over time.[33]Historically, Bentham (1877), considered the monocots to consist of four alliances, Epigynae, Coronariae, Nudiflorae and Glumales, based on floral characteristics. He describes the attempts to subdivide the group since the days of Lindley as largely unsuccessful.[83] Like most subsequent classification systems it failed to distinguish between two major orders, Liliales and Asparagales, now recognised as quite separate.[84] A major advance in this respect was the work of Rolf Dahlgren (1980),[85] which would form the basis of the Angiosperm Phylogeny Group's (APG) subsequent modern classification of monocot families. Dahlgren who used the alternate name Lilliidae considered the monocots as a subclass of angiosperms characterised by a single cotyledon and the presence of triangular protein bodies in the sieve tube plastids. He divided the monocots into seven superorders, Alismatiflorae, Ariflorae, Triuridiflorae, Liliiflorae, Zingiberiflorae, Commeliniflorae and Areciflorae. With respect to the specific issue regarding Liliales and Asparagales, Dahlgren followed Huber (1969)[86] in adopting a splitter approach, in contrast to the longstanding tendency to view Liliaceae as a very broad sensu lato family. Following Dahlgren's untimely death in 1987, his work was continued by his widow, Gertrud Dahlgren, who published a revised version of the classification in 1989. In this scheme the suffix -florae was replaced with -anae (e.g. Alismatanae) and the number of superorders expanded to ten with the addition of Bromelianae, Cyclanthanae and Pandananae.[87]Molecular studies have both confirmed the monophyly of the monocots and helped elucidate relationships within this group. The APG system does not assign the monocots to a taxonomic rank, instead recognizing a monocots clade.[88][89][90][91] However, there has remained some uncertainty regarding the exact relationships between the major lineages, with a number of competing models (including APG).[21]The APG system establishes eleven orders of monocots.[92][82] These form three grades, the alismatid monocots, lilioid monocots and the commelinid monocots by order of branching, from early to late. In the following cladogram numbers indicate crown group (most recent common ancestor of the sampled species of the clade of interest) divergence times in mya (million years ago).[93]Acorales  Alismatales  Petrosaviales  Dioscoreales 115  Pandanales 91  Liliales 121  Asparagales 120  ArecalesPoalesZingiberalesCommelinalesOf some 70,000 species,[94] by far the largest number (65%) are found in two families, the orchids and grasses. The orchids (Orchidaceae, Asparagales) contain about 25,000 species and the grasses (Poaceae, Poales) about 11,000. Other well known groups within the Poales order include the Cyperaceae (sedges) and Juncaceae (rushes), and the monocots also include familiar families such as the palms (Arecaceae, Arecales) and lilies (Liliaceae, Liliales).[84][95]In prephyletic classification systems monocots were generally positioned between plants other than angiosperms and dicots, implying that monocots were more primitive. With the introduction of phyletic thinking in taxonomy (from the system of Eichler 1875–1878 onwards) the predominant theory of monocot origins was the ranalean (ranalian) theory, particularly in the work of Bessey (1915),[2] which traced the origin of all flowering plants to a Ranalean type, and reversed the sequence making dicots the more primitive group.[33]The monocots form a monophyletic group arising early in the history of the flowering plants, but the fossil record is meagre.[96] The earliest fossils presumed to be monocot remains date from the early Cretaceous period. For a very long time, fossils of palm trees were believed to be the oldest monocots,[97] first appearing 90 million years ago (mya), but this estimate may not be entirely true.[98] At least some putative monocot fossils have been found in strata as old as the eudicots.[99] The oldest fossils that are unequivocally monocots are pollen from the Late Barremian–Aptian – Early Cretaceous period, about 120-110 million years ago, and are assignable to clade-Pothoideae-Monstereae Araceae; being Araceae, sister to other Alismatales.[100][101][102] They have also found flower fossils of Triuridaceae (Pandanales) in Upper Cretaceous rocks in New Jersey,[100] becoming the oldest known sighting of saprophytic/mycotrophic habits in angiosperm plants and among the oldest known fossils of monocotyledons.Topology of the angiosperm phylogenetic tree could infer that the monocots would be among the oldest lineages of angiosperms, which would support the theory that they are just as old as the eudicots. The pollen of the eudicots dates back 125 million years, so the lineage of monocots should be that old too.[43]Kåre Bremer, using rbcL sequences and the mean path length method for estimating divergence times, estimated the age of the monocot crown group (i.e. the time at which the ancestor of today's Acorus diverged from the rest of the group) as 134 million years.[103][104] Similarly, Wikström et al.,[105] using Sanderson's non-parametric rate smoothing approach,[106] obtained ages of 127–141 million years for the crown group of monocots.[107] All these estimates have large error ranges (usually 15-20%), and Wikström et al. used only a single calibration point,[105] namely the split between Fagales and Cucurbitales, which was set to 84 Ma, in the late Santonian period. Early molecular clock studies using strict clock models had estimated the monocot crown age to 200 ± 20 million years ago[108] or 160 ± 16 million years,[109] while studies using relaxed clocks have obtained 135-131 million years[110] or 133.8 to 124 million years.[111] Bremer's estimate of 134 million years[103] has been used as a secondary calibration point in other analyses.[112] Some estimates place the emergence of the monocots as far back as 150 mya in the Jurassic period.[21]The age of the core group of so-called 'nuclear monocots' or 'core monocots', which correspond to all orders except Acorales and Alismatales,[113] is about 131 million years to present, and crown group age is about 126 million years to the present. The subsequent branching in this part of the tree (i.e. Petrosaviaceae, Dioscoreales + Pandanales and Liliales clades appeared), including the crown Petrosaviaceae group may be in the period around 125–120 million years BC (about 111 million years so far[103]), and stem groups of all other orders, including Commelinidae would have diverged about or shortly after 115 million years.[112] These and many clades within these orders may have originated in southern Gondwana, i.e. Antarctica, Australasia, and southern South America.[114]The aquatic monocots of Alismatales have commonly been regarded as "primitive".[115][116][117][72][118][119][120][121][122] They have also been considered to have the most primitive foliage, which were cross-linked as Dioscoreales[73] and Melanthiales.[8][123] Keep in mind that the "most primitive" monocot is not necessarily "the sister of everyone else".[43] This is because the ancestral or primitive characters are inferred by means of the reconstruction of character states, with the help of the phylogenetic tree. So primitive characters of monocots may be present in some derived groups. On the other hand, the basal taxa may exhibit many morphological autapomorphies. So although Acoraceae is the sister group to the remaining monocotyledons, the result does not imply that Acoraceae is "the most primitive monocot" in terms of its character states. In fact, Acoraceae is highly derived in many morphological characters, and that is precisely why Acoraceae and Alismatales occupied relatively derived positions in the trees produced by Chase et al.[88] and others.[39][124]Some authors support the idea of an aquatic phase as the origin of monocots.[125] The phylogenetic position of Alismatales (many water), which occupy a relationship with the rest except the Acoraceae, do not rule out the idea, because it could be 'the most primitive monocots' but not 'the most basal'. The Atactostele stem, the long and linear leaves, the absence of secondary growth (see the biomechanics of living in the water), roots in groups instead of a single root branching (related to the nature of the substrate), including sympodial use, are consistent with a water source. However, while monocots were sisters of the aquatic Ceratophyllales, or their origin is related to the adoption of some form of aquatic habit, it would not help much to the understanding of how it evolved to develop their distinctive anatomical features: the monocots seem so different from the rest of angiosperms and it's difficult to relate their morphology, anatomy and development and those of broad-leaved angiosperms.[126][127]In the past, taxa which had petiolate leaves with reticulate venation were considered "primitive" within the monocots, because of the superficial resemblance to the leaves of dicotyledons. Recent work suggests that these taxa are sparse in the phylogenetic tree of monocots, such as fleshy fruited taxa (excluding taxa with aril seeds dispersed by ants), the two features would be adapted to conditions that evolved together regardless.[67][128][129][130] Among the taxa involved were Smilax, Trillium (Liliales), Dioscorea (Dioscoreales), etc. A number of these plants are vines that tend to live in shaded habitats for at least part of their lives, and may also have a relationship with their shapeless stomata.[131] Reticulate venation seems to have appeared at least 26 times in monocots, in fleshy fruits 21 times (sometimes lost later), and the two characteristics, though different, showed strong signs of a tendency to be good or bad in tandem, a phenomenon described as "concerted convergence" ("coordinated convergence").[129][130]The name monocotyledons is derived from the traditional botanical name "Monocotyledones" or Monocotyledoneae in Latin, which refers to the fact that most members of this group have one cotyledon, or embryonic leaf, in their seeds.Some monocots, such as grasses, have hypogeal emergence, where the mesocotyl elongates and pushes the coleoptile (which encloses and protects the shoot tip) toward the soil surface.[132] Since elongation occurs above the cotyledon, it is left in place in the soil where it was planted. Many dicots have epigeal emergence, in which the hypocotyl elongates and becomes arched in the soil. As the hypocotyl continues to elongate, it pulls the cotyledons upward, above the soil surface.The IUCN Red List describes four species as extinct, four as extinct in the wild, 626 as possibly extinct, 423 as critically endangered, 632 endangered, 621 vulnerable, and 269 near threatened of 4,492 whose status is known.[133]Monocots are among the most important plants economically and culturally, and account for most of the staple foods of the world, such as cereal grains and starchy root crops, and palms, orchids and lilies, building materials, and many medicines.[43] Of the monocots, the grasses are of enormous economic importance as a source of animal and human food,[84] and form the largest component of agricultural species in terms of biomass produced.[95][134]
Compartmentalization of decay in trees
Compartmentalization Of Decay In Trees (CODIT), also known as Compartmentalization Of Disease In Trees by some, is a concept created by Dr. Alex Shigo after years of studying tree decay patterns.  Though disputed upon its introduction in the late 1970s, the concept is now widely accepted by modern arboriculture and is referenced widely in publications including Shigo's 'Modern Arboriculture' and 'A New Tree Biology'.In keeping with the theory of spontaneous generation, in which living things can develop from non-living things, scientists traditionally believed that tree decay led to fungal growth.[citation needed]  With the advent of germ theory, however, German forester Robert Hartig in the early 20th century theorized the opposite was the case, and developed a new model for tree decay: when trees are wounded, fungi infect the wounds, and the result is decayed wood.  Over time, the research of Shigo and others helped to expand this theory, leading to the modern concept of tree decay: when trees are wounded, many organisms, not just fungi, infect the wood at different times and in different ways; trees respond to these infections with both chemical and physical changes; discolored and decayed wood results, but is limited by compartmentalization.According to CODIT, when a tree is wounded cells undergo changes to form "walls" around the wound, slowing or preventing the spread of disease and decay to the rest of the tree.By increasing understanding of how trees respond to decay, CODIT has had many applications.  For example, arborists are frequently called upon to analyze the danger posed to people or property by a damaged or decaying tree.  By knowing how decay is likely to spread, such hazard tree analyses may be more accurate, thereby preventing unnecessary tree removal, property damage, or injury.  For another example, in the production of maple syrup holes are drilled into a tree's vascular tissues, which necessarily damages the tree.  CODIT has helped farmers to better understand the effects of different tapping techniques and accordingly to change their methods to minimize damage and maximize production.[1][citation needed]Work done by Gilman et al. at the University of Florida shows that a wound's proximity to leaf mass greatly influences compartmentalization as well as wound closure.
Yggdrasil
Yggdrasil (/ˈɪɡdrəsɪl, ˈɪɡdrəzɪl/; from Old Norse Yggdrasill, pronounced [ˈyɡːˌdrasilː]) is an immense mythical tree that connects the nine worlds in Norse cosmology.Yggdrasil is attested in the Poetic Edda, compiled in the 13th century from earlier traditional sources, and the Prose Edda, written in the 13th century by Snorri Sturluson. In both sources, Yggdrasil is an immense ash tree that is center to the cosmos and considered very holy. The gods go to Yggdrasil daily to assemble at their things, traditional governing assemblies. The branches of Yggdrasil extend far into the heavens, and the tree is supported by three roots that extend far away into other locations; one to the well Urðarbrunnr in the heavens, one to the spring Hvergelmir, and another to the well Mímisbrunnr. Creatures live within Yggdrasil, including the dragon Níðhöggr, an unnamed eagle, and the stags Dáinn, Dvalinn, Duneyrr and Duraþrór.Conflicting scholarly theories have been proposed about the etymology of the name Yggdrasill, the possibility that the tree is of another species than ash, its connection to the many sacred trees and groves in Germanic paganism and mythology, and the fate of Yggdrasil during the events of Ragnarök.The generally accepted meaning of Old Norse Yggdrasill is "Odin's horse", meaning "gallows". This interpretation comes about because drasill means "horse" and Ygg(r) is one of Odin's many names. The Poetic Edda poem Hávamál describes how Odin sacrificed himself by hanging from a tree, making this tree Odin's gallows. This tree may have been Yggdrasil. Gallows can be called "the horse of the hanged" and therefore Odin's gallows may have developed into the expression "Odin's horse", which then became the name of the tree.[1]Nevertheless, scholarly opinions regarding the precise meaning of the name Yggdrasill vary, particularly on the issue of whether Yggdrasill is the name of the tree itself or if only the full term askr Yggdrasil (where Old Norse askr means "ash tree") refers specifically to the tree. According to this interpretation, askr Yggdrasils would mean the world tree upon which "the horse [Odin's horse] of the highest god [Odin] is bound". Both of these etymologies rely on a presumed but unattested *Yggsdrasill.[1]A third interpretation, presented by F. Detter, is that the name Yggdrasill refers to the word yggr ("terror"), yet not in reference to the Odinic name, and so Yggdrasill would then mean "tree of terror, gallows". F. R. Schröder has proposed a fourth etymology according to which yggdrasill means "yew pillar", deriving yggia from *igwja (meaning "yew-tree"), and drasill from *dher- (meaning "support").[1]In the Poetic Edda, the tree is mentioned in the three poems Völuspá, Hávamál and  Grímnismál.In the second stanza of the Poetic Edda poem Völuspá, the völva (a shamanic seeress) reciting the poem to the god Odin says that she remembers far back to "early times", being raised by jötnar, recalls nine worlds and "nine wood-ogresses" (Old Norse: nío ídiðiur), and when Yggdrasil was a seed ("glorious tree of good measure, under the ground").[2] In stanza 19, the völva says:An ash I know there stands,Yggdrasill is its name,a tall tree, showeredwith shining loam.From there come the dewsthat drop in the valleys.It stands forever green overUrðr's well.[3]In stanza 20, the völva says that from the lake under the tree come three "maidens deep in knowledge" named Urðr, Verðandi, and Skuld. The maidens "incised the slip of wood," "laid down laws" and "chose lives" for the children of mankind and the destinies (ørlǫg) of men.[4] In stanza 27, the völva details that she is aware that "Heimdallr's hearing is couched beneath the bright-nurtured holy tree."[5] In stanza 45, Yggdrasil receives a final mention in the poem. The völva describes, as a part of the onset of Ragnarök, that Heimdallr blows Gjallarhorn, that Odin speaks with Mímir's head, and then:Yggdrasill shivers,the ash, as it stands.The old tree groans,and the giant slips free.[6]In stanza 138 of the poem Hávamál, Odin describes how he once sacrificed himself to himself by hanging on a tree. The stanza reads:I know that I hung on a windy treenine long nights,wounded with a spear, dedicated to Odin,myself to myself,on that tree of which no man knowsfrom where its roots run.[7]In the stanza that follows, Odin describes how he had no food nor drink there, that he peered downward, and that "I took up the runes, screaming I took them, then I fell back from there."[7] While Yggdrasil is not mentioned by name in the poem and other trees exist in Norse mythology, the tree is near universally accepted as Yggdrasil, and if the tree is Yggdrasil, then the name Yggdrasil directly relates to this story.[8]In the poem Grímnismál, Odin (disguised as Grímnir) provides the young Agnar with cosmological lore. Yggdrasil is first mentioned in the poem in stanza 29, where Odin says that, because the "bridge of the Æsir burns" and the "sacred waters boil," Thor must wade through the rivers Körmt and Örmt and two rivers named Kerlaugar to go "sit as judge at the ash of Yggdrasill." In the stanza that follows, a list of names of horses are given that the Æsir ride to "sit as judges" at Yggdrasil.[9]In stanza 31, Odin says that the ash Yggdrasil has three roots that grow in three directions. He details that beneath the first lives Hel, under the second live frost jötnar, and beneath the third lives mankind. Stanza 32 details that a squirrel named Ratatoskr must run across Yggdrasil and bring "the eagle's word" from above to Níðhöggr below. Stanza 33 describes that four harts named Dáinn, Dvalinn, Duneyrr and Duraþrór consume "the highest boughs" of Yggdrasil.[9]In stanza 34, Odin says that more serpents lie beneath Yggdrasil "than any fool can imagine" and lists them as Góinn and Móinn (possibly meaning Old Norse "land animal"[10]), which he describes as sons of Grafvitnir (Old Norse, possibly "ditch wolf"[11]), Grábakr (Old Norse "Greyback"[10]), Grafvölluðr (Old Norse, possibly "the one digging under the plain" or possibly amended as "the one ruling in the ditch"[11]), Ófnir (Old Norse "the winding one, the twisting one"[12]), and Sváfnir (Old Norse, possibly "the one who puts to sleep = death"[13]), who Odin adds that he thinks will forever gnaw on the tree's branches.[9]In stanza 35, Odin says that Yggdrasil "suffers agony more than men know", as a hart bites it from above, it decays on its sides, and Níðhöggr bites it from beneath.[14] In stanza 44, Odin provides a list of things that are what he refers to as the "noblest" of their kind. Within the list, Odin mentions Yggdrasil first, and states that it is the "noblest of trees".[15]Yggdrasil is mentioned in two books in the Prose Edda; Gylfaginning and Skáldskaparmál. In Gylfaginning, Yggdrasil is introduced in chapter 15. In chapter 15, Gangleri (described as king Gylfi in disguise) asks where is the chief or holiest place of the gods. High replies "It is the ash Yggdrasil. There the gods must hold their courts each day". Gangleri asks what there is to tell about Yggdrasil. Just-As-High says that Yggdrasil is the biggest and best of all trees, that its branches extend out over all of the world and reach out over the sky. Three of the roots of the tree support it, and these three roots also extend extremely far: one "is among the Æsir, the second among the frost jötnar, and the third over Niflheim. The root over Niflheim is gnawed at by the wyrm Níðhöggr, and beneath this root is the spring Hvergelmir. Beneath the root that reaches the frost jötnar is the well Mímisbrunnr, "which has wisdom and intelligence contained in it, and the master of the well is called Mimir". Just-As-High provides details regarding Mímisbrunnr and then describes that the third root of the well "extends to heaven" and that beneath the root is the "very holy" well Urðarbrunnr. At Urðarbrunnr the gods hold their court, and every day the Æsir ride to Urðarbrunnr up over the bridge Bifröst. Later in the chapter, a stanza from Grímnismál mentioning Yggdrasil is quoted in support.[16]In chapter 16, Gangleri asks "what other particularly notable things are there to tell about the ash?" High says there is quite a lot to tell about. High continues that an eagle sits on the branches of Yggdrasil and that it has much knowledge. Between the eyes of the eagle sits a hawk called Veðrfölnir. A squirrel called Ratatoskr scurries up and down the ash Yggdrasil carrying "malicious messages" between the eagle and Níðhöggr. Four stags named Dáinn, Dvalinn, Duneyrr, and Duraþrór run between the branches of Yggdrasil and consume its foliage. In the spring Hvergelmir are so many snakes along with Níðhöggr "that no tongue can enumerate them". Two stanzas from Grímnismál are then cited in support. High continues that the norns that live by the holy well Urðarbrunnr each day take water from the well and mud from around it and pour it over Yggdrasil so that the branches of the ash do not rot away or decay. High provides more information about Urðarbrunnr, cites a stanza from Völuspá in support, and adds that dew falls from Yggdrasil to the earth, explaining that "this is what people call honeydew, and from it bees feed".[17]In chapter 41, the stanza from Grímnismál is quoted that mentions that Yggdrasil is the foremost of trees.[18] In chapter 54, as part of the events of Ragnarök, High describes that Odin will ride to the well Mímisbrunnr and consult Mímir on behalf of himself and his people. After this, "the ash Yggdrasil will shake and nothing will be unafraid in heaven or on earth", and then the Æsir and Einherjar will don their war gear and advance to the field of Vígríðr. Further into the chapter, the stanza in Völuspá that details this sequence is cited.[19]In the Prose Edda book Skáldskaparmál, Yggdrasil receives a single mention, though not by name. In chapter 64, names for kings and dukes are given. "Illustrious one" is provided as an example, appearing in a Christianity-influenced work by the skald Hallvarðr Háreksblesi: "There is not under the pole of the earth [Yggdrasil] an illustrious one closer to the lord of monks [God] than you."[20]Hilda Ellis Davidson comments that the existence of nine worlds around Yggdrasil is mentioned more than once in Old Norse sources, but the identity of the worlds is never stated outright, though it can be deduced from various sources. Davidson comments that "no doubt the identity of the nine varied from time to time as the emphasis changed or new imagery arrived". Davidson says that it is unclear where the nine worlds are located in relation to the tree; they could either exist one above the other or perhaps be grouped around the tree, but there are references to worlds existing beneath the tree, while the gods are pictured as in the sky, a rainbow bridge (Bifröst) connecting the tree with other worlds. Davidson opines that "those who have tried to produce a convincing diagram of the Scandinavian cosmos from what we are told in the sources have only added to the confusion".[21]Davidson notes parallels between Yggdrasil and shamanic lore in northern Eurasia:The conception of the tree rising through a number of worlds is found in northern Eurasia and forms part of the shamanic lore shared by many peoples of this region. This seems to be a very ancient conception, perhaps based on the Pole Star, the centre of the heavens, and the image of the central tree in Scandinavia may have been influenced by it.... Among Siberian shamans, a central tree may be used as a ladder to ascend the heavens.[21]Davidson says that the notion of an eagle atop a tree and the world serpent coiled around the roots of the tree has parallels in other cosmologies from Asia. She goes on to say that Norse cosmology may have been influenced by these Asiatic cosmologies from a northern location. Davidson adds, on the other hand, that it is attested that the Germanic peoples worshiped their deities in open forest clearings and that a sky god was particularly connected with the oak tree, and therefore "a central tree was a natural symbol for them also".[21]Connections have been proposed between the wood Hoddmímis holt (Old Norse "Hoard-Mímir's"[22] holt) and the tree Mímameiðr ("Mímir's tree"), generally thought to refer to the world tree Yggdrasil, and the spring Mímisbrunnr.[22] John Lindow concurs that Mímameiðr may be another name for Yggdrasil and that if the Hoard-Mímir of the name Hoddmímis holt is the same figure as Mímir (associated with the spring named after him, Mímisbrunnr), then the Mímir's holt—Yggdrasil—and Mímir's spring may be within the same proximity.[23]Carolyne Larrington notes that it is nowhere expressly stated what will happen to Yggdrasil during the events of Ragnarök. Larrington points to a connection between the primordial figure of Mímir and Yggdrasil in the poem Völuspá, and theorizes that "it is possible that Hoddmimir is another name for Mimir, and that the two survivors hide in Yggdrasill."[24]Rudolf Simek theorizes that the survival of Líf and Lífþrasir through Ragnarök by hiding in Hoddmímis holt is "a case of reduplication of the anthropogeny, understandable from the cyclic nature of the Eddic eschatology." Simek says that Hoddmímis holt "should not be understood literally as a wood or even a forest in which the two keep themselves hidden, but rather as an alternative name for the world-tree Yggdrasill. Thus, the creation of mankind from tree trunks (Askr, Embla) is repeated after the Ragnarǫk as well." Simek says that in Germanic regions, the concept of mankind originating from trees is ancient. Simek additionally points out legendary parallels in a Bavarian legend of a shepherd who lives inside a tree, whose descendants repopulate the land after life there has been wiped out by plague (citing a retelling by F. R. Schröder). In addition, Simek points to an Old Norse parallel in the figure of Örvar-Oddr, "who is rejuvenated after living as a tree-man (Ǫrvar-Odds saga 24–27)".[25]Continuing as late as the 19th century, warden trees were venerated in areas of Germany and Scandinavia, considered to be guardians and bringers of luck, and offerings were sometimes made to them. A massive birch tree standing atop a burial mound and located beside a farm in western Norway is recorded as having had ale poured over its roots during festivals. The tree was felled in 1874.[26]Davidson comments that "the position of the tree in the centre as a source of luck and protection for gods and men is confirmed" by these rituals to Warden Trees. Davidson notes that the gods are described as meeting beneath Yggdrasil to hold their things, and that the pillars venerated by the Germanic peoples, such as the pillar Irminsul, were also symbolic of the center of the world. Davidson details that it would be difficult to ascertain whether a tree or pillar came first, and that this likely depends on if the holy location was in a thickly wooded area or not. Davidson notes that there is no mention of a sacred tree at Þingvellir in Iceland yet that Adam of Bremen describes a huge tree standing next to the Temple at Uppsala in Sweden, which Adam describes as remaining green throughout summer and winter, and that no one knew what type of tree it was. Davidson comments that while it is uncertain that Adam's informant actually witnessed that tree is unknown, but that the existence of sacred trees in pre-Christian Germanic Europe is further evidenced by records of their destruction by early Christian missionaries, such as Thor's Oak by Saint Boniface.[26]Ken Dowden comments that behind Irminsul, Thor's Oak in Geismar, and the sacred tree at Uppsala "looms a mythic prototype, an Yggdrasil, the world-ash of the Norsemen".[27]Modern works of art depicting Yggdrasil include Die Nornen (painting, 1888) by K. Ehrenberg; Yggdrasil (fresco, 1933) by Axel Revold, located in the University of Oslo library auditorium in Oslo, Norway; Hjortene beiter i løvet på Yggdrasil asken (wood relief carving, 1938) on the Oslo City Hall by Dagfin Werenskjold; and the bronze relief on the doors of the Swedish Museum of National Antiquities (around 1950) by B. Marklund in Stockholm, Sweden. Poems mentioning Yggdrasil include Vårdträdet by Viktor Rydberg and Yggdrasill by J. Linke.[28]
Radioactive decay
Radioactive decay (also known as nuclear decay, radioactivity or nuclear radiation) is the process by which an unstable atomic nucleus loses energy (in terms of mass in its rest frame) by emitting radiation, such as an alpha particle, beta particle with neutrino or only a neutrino in the case of electron capture, or a gamma ray or electron in the case of internal conversion. A material containing such unstable nuclei is considered radioactive. Certain highly excited short-lived nuclear states can decay through neutron emission, or more rarely, proton emission.Radioactive decay is a stochastic (i.e. random) process at the level of single atoms. According to quantum theory, it is impossible to predict when a particular atom will decay,[1][2][3] regardless of how long the atom has existed. However, for a collection of atoms, the collection's expected decay rate is characterized in terms of their measured decay constants or half-lives. This is the basis of radiometric dating. The half-lives of radioactive atoms have no known upper limit, spanning a time range of over 55 orders of magnitude, from nearly instantaneous to far longer than the age of the universe.A radioactive nucleus with zero spin can have no defined orientation, and hence emits the total momentum of its decay products isotropically (all directions and without bias). If there are multiple particles produced during a single decay, as in beta decay, their relative angular distribution, or spin directions may not be isotropic. Decay products from a nucleus with spin may be distributed non-isotropically with respect to that spin direction, either because of an external influence such as an electromagnetic field, or because the nucleus was produced in a dynamic process that constrained the direction of its spin. Such a parent process could be a previous decay, or a nuclear reaction.[4][5][6][note 1]The decaying nucleus is called the parent radionuclide (or parent radioisotope[note 2]), and the process produces at least one daughter nuclide. Except for gamma decay or internal conversion from a nuclear excited state, the decay is a nuclear transmutation resulting in a daughter containing a different number of protons or neutrons (or both). When the number of protons changes, an atom of a different chemical element is created.The first decay processes to be discovered were alpha decay, beta decay, and gamma decay. Alpha decay occurs when the nucleus ejects an alpha particle (helium nucleus). This is the most common process of emitting nucleons, but highly excited nuclei can eject single nucleons, or in the case of cluster decay, specific light nuclei of other elements. Beta decay occurs in two ways:(i) beta-minus decay, when the nucleus emits an electron and an antineutrino in a process that changes a neutron to a proton, or(ii) beta-plus decay, when the nucleus emits a positron and a neutrino in a process that changes a proton to a neutron. Highly excited neutron-rich nuclei, formed as the product of other types of decay, occasionally lose energy by way of neutron emission, resulting in a change from one isotope to another of the same element. The nucleus may capture an orbiting electron, causing a proton to convert into a neutron in a process called electron capture. All of these processes result in a well-defined nuclear transmutation.By contrast, there are radioactive decay processes that do not result in a nuclear transmutation. The energy of an excited nucleus may be emitted as a gamma ray in a process called gamma decay, or that energy may be lost when the nucleus interacts with an orbital electron causing its ejection from the atom, in a process called internal conversion.Another type of radioactive decay results in products that vary, appearing as two or more "fragments" of the original nucleus with a range of possible masses. This decay, called spontaneous fission, happens when a large unstable nucleus spontaneously splits into two (or occasionally three) smaller daughter nuclei, and generally leads to the emission of gamma rays, neutrons, or other particles from those products.For a summary table showing the number of stable and radioactive nuclides in each category, see radionuclide. There are 28 naturally occurring chemical elements on Earth that are radioactive, consisting of 33 radionuclides (5 elements have 2 different radionuclides) that date before the time of formation of the solar system. These 33 are known as primordial nuclides. Well-known examples are uranium and thorium, but also included are naturally occurring long-lived radioisotopes, such as potassium-40. Another 50 or so shorter-lived radionuclides, such as radium and radon, found on Earth, are the products of decay chains that began with the primordial nuclides, or are the product of ongoing cosmogenic processes, such as the production of carbon-14 from nitrogen-14 in the atmosphere by cosmic rays. Radionuclides may also be produced artificially in particle accelerators or nuclear reactors, resulting in 650 of these with half-lives of over an hour, and several thousand more with even shorter half-lives. [See here for a list of these sorted by half life.]Radioactivity was discovered in 1896 by the French scientist Henri Becquerel, while working with phosphorescent materials.[7] These materials glow in the dark after exposure to light, and he suspected that the glow produced in cathode ray tubes by X-rays might be associated with phosphorescence. He wrapped a photographic plate in black paper and placed various phosphorescent salts on it. All results were negative until he used uranium salts. The uranium salts caused a blackening of the plate in spite of the plate being wrapped in black paper. These radiations were given the name "Becquerel Rays".It soon became clear that the blackening of the plate had nothing to do with phosphorescence, as the blackening was also produced by non-phosphorescent salts of uranium and metallic uranium. It became clear from these experiments that there was a form of invisible radiation that could pass through paper and was causing the plate to react as if exposed to light.At first, it seemed as though the new radiation was similar to the then recently discovered X-rays. Further research by Becquerel, Ernest Rutherford, Paul Villard, Pierre Curie, Marie Curie, and others showed that this form of radioactivity was significantly more complicated. Rutherford was the first to realize that all such elements decay in accordance with the same mathematical exponential formula. Rutherford and his student Frederick Soddy were the first to realize that many decay processes resulted in the transmutation of one element to another. Subsequently, the radioactive displacement law of Fajans and Soddy was formulated to describe the products of alpha and beta decay.[8][9]The early researchers also discovered that many other chemical elements, besides uranium, have radioactive isotopes. A systematic search for the total radioactivity in uranium ores also guided Pierre and Marie Curie to isolate two new elements: polonium and radium. Except for the radioactivity of radium, the chemical similarity of radium to barium made these two elements difficult to distinguish.Marie and Pierre Curie’s study of radioactivity is an important factor in science and medicine. After their research on Becquerel's rays led them to the discovery of both radium and polonium, they coined the term "radioactivity".[10] Their research on the penetrating rays in uranium and the discovery of radium launched an era of using radium for the treatment of cancer. Their exploration of radium could be seen as the first peaceful use of nuclear energy and the start of modern nuclear medicine.[10]The dangers of ionizing radiation due to radioactivity and X-rays were not immediately recognized.The discovery of x‑rays by Wilhelm Röntgen in 1895 led to widespread experimentation by scientists, physicians, and inventors. Many people began recounting stories of burns, hair loss and worse in technical journals as early as 1896. In February of that year, Professor Daniel and Dr. Dudley of Vanderbilt University performed an experiment involving X-raying Dudley's head that resulted in his hair loss. A report by Dr. H.D. Hawks, of his suffering severe hand and chest burns in an X-ray demonstration, was the first of many other reports in Electrical Review.[11]Other experimenters, including Elihu Thomson and Nikola Tesla, also reported burns. Thomson deliberately exposed a finger to an X-ray tube over a period of time and suffered pain, swelling, and blistering.[12] Other effects, including ultraviolet rays and ozone, were sometimes blamed for the damage,[13] and many physicians still claimed that there were no effects from X-ray exposure at all.[12]Despite this, there were some early systematic hazard investigations, and as early as 1902 William Herbert Rollins wrote almost despairingly that his warnings about the dangers involved in the careless use of X-rays were not being heeded, either by industry or by his colleagues. By this time, Rollins had proved that X-rays could kill experimental animals, could cause a pregnant guinea pig to abort, and that they could kill a fetus.[14] He also stressed that "animals vary in susceptibility to the external action of X-light" and warned that these differences be considered when patients were treated by means of X-rays.However, the biological effects of radiation due to radioactive substances were less easy to gauge. This gave the opportunity for many physicians and corporations to market radioactive substances as patent medicines. Examples were radium enema treatments, and radium-containing waters to be drunk as tonics. Marie Curie protested against this sort of treatment, warning that the effects of radiation on the human body were not well understood. Curie later died from aplastic anaemia, likely caused by exposure to ionizing radiation. By the 1930s, after a number of cases of bone necrosis and death of radium treatment enthusiasts, radium-containing medicinal products had been largely removed from the market (radioactive quackery).Only a year after Röntgen's discovery of X rays, the American engineer Wolfram Fuchs (1896) gave what is probably the first protection advice, but it was not until 1925 that the first International Congress of Radiology (ICR) was held and considered establishing international protection standards. The effects of radiation on genes, including the effect of cancer risk, were recognized much later. In 1927, Hermann Joseph Muller published research showing genetic effects and, in 1946, was awarded the Nobel Prize in Physiology or Medicine for his findings.The second ICR was held in Stockholm in 1928 and proposed the adoption of the rontgen unit, and the 'International X-ray and Radium Protection Committee' (IXRPC) was formed. Rolf Sievert was named Chairman, but a driving force was George Kaye of the British National Physical Laboratory. The committee met in 1931, 1934 and 1937.After World War II, the increased range and quantity of radioactive substances being handled as a result of military and civil nuclear programmes led to large groups of occupational workers and the public being potentially exposed to harmful levels of ionising radiation. This was considered at the first post-war ICR convened in London in 1950, when the present International Commission on Radiological Protection (ICRP) was born.[15]Since then the ICRP has developed the present international system of radiation protection, covering all aspects of radiation hazard.The International System of Units (SI) unit of radioactive activity is the becquerel (Bq), named in honor of the scientist Henri Becquerel. One Bq is defined as one transformation (or decay or disintegration) per second.An older unit of radioactivity is the curie, Ci, which was originally defined as "the quantity or mass of radium emanation in equilibrium with one gram of radium (element)".[16] Today, the curie is defined as 7010370000000000000♠3.7×1010 disintegrations per second, so that 1 curie (Ci) = 7010370000000000000♠3.7×1010 Bq.For radiological protection purposes, although the United States Nuclear Regulatory Commission permits the use of the unit curie alongside SI units,[17] the European Union European units of measurement directives required that its use for "public health ... purposes" be phased out by 31 December 1985.[18]The effects of ionizing radiation are often measured in units of gray for mechanical or sievert for damage to tissue.Early researchers found that an electric or magnetic field could split radioactive emissions into three types of beams. The rays were given the names alpha, beta, and gamma, in increasing order of their ability to penetrate matter. Alpha decay is observed only in heavier elements of atomic number 52 (tellurium) and greater, with the exception of beryllium-8 which decays to two alpha particles. The other two types of decay are produced by all of the elements. Lead, atomic number 82, is the heaviest element to have any isotopes stable (to the limit of measurement) to radioactive decay. Radioactive decay is seen in all isotopes of all elements of atomic number 83 (bismuth) or greater. Bismuth-209, however, is only very slightly radioactive, with a half-life greater than the age of the universe; radioisotopes with extremely long half-lives are considered effectively stable for practical purposes.In analysing the nature of the decay products, it was obvious from the direction of the electromagnetic forces applied to the radiations by external magnetic and electric fields that alpha particles carried a positive charge, beta particles carried a negative charge, and gamma rays were neutral. From the magnitude of deflection, it was clear that alpha particles were much more massive than beta particles. Passing alpha particles through a very thin glass window and trapping them in a discharge tube allowed researchers to study the emission spectrum of the captured particles, and ultimately proved that alpha particles are helium nuclei. Other experiments showed beta radiation, resulting from decay and cathode rays, were high-speed electrons. Likewise, gamma radiation and X-rays were found to be high-energy electromagnetic radiation.The relationship between the types of decays also began to be examined: For example, gamma decay was almost always found to be associated with other types of decay, and occurred at about the same time, or afterwards. Gamma decay as a separate phenomenon, with its own half-life (now termed isomeric transition), was found in natural radioactivity to be a result of the gamma decay of excited metastable nuclear isomers, which were in turn created from other types of decay.Although alpha, beta, and gamma radiations were most commonly found, other types of emission were eventually discovered. Shortly after the discovery of the positron in cosmic ray products, it was realized that the same process that operates in classical beta decay can also produce positrons (positron emission), along with neutrinos (classical beta decay produces antineutrinos). In a more common analogous process, called electron capture, some proton-rich nuclides were found to capture their own atomic electrons instead of emitting positrons, and subsequently these nuclides emit only a neutrino and a gamma ray from the excited nucleus (and often also Auger electrons and characteristic X-rays, as a result of the re-ordering of electrons to fill the place of the missing captured electron). These types of decay involve the nuclear capture of electrons or emission of electrons or positrons, and thus acts to move a nucleus toward the ratio of neutrons to protons that has the least energy for a given total number of nucleons. This consequently produces a more stable (lower energy) nucleus.(A theoretical process of positron capture, analogous to electron capture, is possible in antimatter atoms, but has not been observed, as complex antimatter atoms beyond antihelium are not experimentally available.[19] Such a decay would require antimatter atoms at least as complex as beryllium-7, which is the lightest known isotope of normal matter to undergo decay by electron capture.)Shortly after the discovery of the neutron in 1932, Enrico Fermi realized that certain rare beta-decay reactions immediately yield neutrons as a decay particle (neutron emission). Isolated proton emission was eventually observed in some elements. It was also found that some heavy elements may undergo spontaneous fission into products that vary in composition. In a phenomenon called cluster decay, specific combinations of neutrons and protons other than alpha particles (helium nuclei) were found to be spontaneously emitted from atoms.Other types of radioactive decay were found to emit previously-seen particles, but via different mechanisms. An example is internal conversion, which results in an initial electron emission, and then often further characteristic X-rays and Auger electrons emissions, although the internal conversion process involves neither beta nor gamma decay. A neutrino is not emitted, and none of the electron(s) and photon(s) emitted originate in the nucleus, even though the energy to emit all of them does originate there. Internal conversion decay, like isomeric transition gamma decay and neutron emission, involves the release of energy by an excited nuclide, without the transmutation of one element into another.Rare events that involve a combination of two beta-decay type events happening simultaneously are known (see below). Any decay process that does not violate the conservation of energy or momentum laws (and perhaps other particle conservation laws) is permitted to happen, although not all have been detected. An interesting example discussed in a final section, is bound state beta decay of rhenium-187. In this process, beta electron-decay of the parent nuclide is not accompanied by beta electron emission, because the beta particle has been captured into the K-shell of the emitting atom. An antineutrino is emitted, as in all negative beta decays.Radionuclides can undergo a number of different reactions. These are summarized in the following table. A nucleus with mass number A and atomic number Z is represented as (A, Z). The column "Daughter nucleus" indicates the difference between the new nucleus and the original nucleus. Thus, (A − 1, Z) means that the mass number is one less than before, but the atomic number is the same as before.If energy circumstances are favorable, a given radionuclide may undergo many competing types of decay, with some atoms decaying by one route, and others decaying by another. An example is copper-64, which has 29 protons, and 35 neutrons, which decays with a half-life of about 12.7 hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay to the opposite particle. This particular nuclide (though not all nuclides in this situation) is almost equally likely to decay through positron emission (18%), or through electron capture (43%), as it does through electron emission (39%). The excited energy states resulting from these decays which fail to end in a ground energy state, also produce later internal conversion and gamma decay in almost 0.5% of the time.More common in heavy nuclides is competition between alpha and beta decay. The daughter nuclides will then normally decay through beta or alpha, respectively, to end up in the same place.Radioactive decay results in a reduction of summed rest mass, once the released energy (the disintegration energy) has escaped in some way. Although decay energy is sometimes defined as associated with the difference between the mass of the parent nuclide products and the mass of the decay products, this is true only of rest mass measurements, where some energy has been removed from the product system. This is true because the decay energy must always carry mass with it, wherever it appears (see mass in special relativity) according to the formula E = mc2. The decay energy is initially released as the energy of emitted photons plus the kinetic energy of massive emitted particles (that is, particles that have rest mass). If these particles come to thermal equilibrium with their surroundings and photons are absorbed, then the decay energy is transformed to thermal energy, which retains its mass.Decay energy therefore remains associated with a certain measure of mass of the decay system, called invariant mass, which does not change during the decay, even though the energy of decay is distributed among decay particles. The energy of photons, the kinetic energy of emitted particles, and, later, the thermal energy of the surrounding matter, all contribute to the invariant mass of the system. Thus, while the sum of the rest masses of the particles is not conserved in radioactive decay, the system mass and system invariant mass (and also the system total energy) is conserved throughout any decay process. This is a restatement of the equivalent laws of conservation of energy and conservation of mass.The decay rate, or activity, of a radioactive substance is characterized by:Constant quantities:Although these are constants, they are associated with the statistical behavior of populations of atoms. In consequence, predictions using these constants are less accurate for minuscule samples of atoms.In principle a half-life, a third-life, or even a (1/√2)-life, can be used in exactly the same way as half-life; but the mean life and half-life t1/2 have been adopted as standard times associated with exponential decay.Time-variable quantities:These are related as follows:where N0 is the initial amount of active substance — substance that has the same percentage of unstable particles as when the substance was formed.Radioactivity is one very frequently given example of exponential decay. The law describes the statistical behaviour of a large number of nuclides, rather than individual atoms. In the following formalism, the number of nuclides or the nuclide population N, is of course a discrete variable (a natural number)—but for any physical sample N is so large that it can be treated as a continuous variable. Differential calculus is used to model the behaviour of nuclear decay.The mathematics of radioactive decay depend on a key assumption that a nucleus of a radionuclide has no "memory" or way of translating its history into its present behavior. A nucleus does not "age" with the passage of time. Thus, the probability of its breaking down does not increase with time, but stays constant no matter how long the nucleus has existed. This constant probability may vary greatly between different types of nuclei, leading to the many different observed decay rates. However, whatever the probability is, it does not change. This is in marked contrast to complex objects which do show aging, such as automobiles and humans. These systems do have a chance of breakdown per unit of time, that increases from the moment they begin their existence.Consider the case of a nuclide A that decays into another B by some process A → B (emission of other particles, like electron neutrinos νe and electrons e− as in beta decay, are irrelevant in what follows). The decay of an unstable nucleus is entirely random in time so it is impossible to predict when a particular atom will decay. However, it is equally likely to decay at any instant in time. Therefore, given a sample of a particular radioisotope, the number of decay events −dN expected to occur in a small interval of time dt is proportional to the number of atoms present N, that is[20]Particular radionuclides decay at different rates, so each has its own decay constant λ. The expected decay −dN/N is proportional to an increment of time, dt:The negative sign indicates that N decreases as time increases, as the decay events follow one after another. The solution to this first-order differential equation is the function:where N0 is the value of N at time t = 0, with the decay constant expressed as λ[20]We have for all time t:where Ntotal is the constant number of particles throughout the decay process, which is equal to the initial number of A nuclides since this is the initial substance.If the number of non-decayed A nuclei is:then the number of nuclei of B, i.e. the number of decayed A nuclei, isThe number of decays observed over a given interval obeys Poisson statistics. If the average number of decays is <N>, the probability of a given number of decays N is[20]Chain of two decaysNow consider the case of a chain of two decays: one nuclide A decaying into another B by one process, then B decaying into another C by a second process, i.e. A → B → C. The previous equation cannot be applied to the decay chain, but can be generalized as follows. Since A decays into B, then B decays into C, the activity of A adds to the total number of B nuclides in the present sample, before those B nuclides decay and reduce the number of nuclides leading to the later sample. In other words, the number of second generation nuclei B increases as a result of the first generation nuclei decay of A, and decreases as a result of its own decay into the third generation nuclei C.[21] The sum of these two terms gives the law for a decay chain for two nuclides:The rate of change of NB, that is dNB/dt, is related to the changes in the amounts of A and B, NB can increase as B is produced from A and decrease as B produces C.Re-writing using the previous results:The subscripts simply refer to the respective nuclides, i.e. NA is the number of nuclides of type A, NA0 is the initial number of nuclides of type A, λA is the decay constant for A - and similarly for nuclide B. Solving this equation for NB gives:In the case where B is a stable nuclide (λB = 0), this equation reduces to the previous solution:as shown above for one decay. The solution can be found by the integration factor method, where the integrating factor is eλBt. This case is perhaps the most useful, since it can derive both the one-decay equation (above) and the equation for multi-decay chains (below) more directly.Chain of any number of decaysFor the general case of any number of consecutive decays in a decay chain, i.e. A1 → A2 ··· → Ai ··· → AD, where D is the number of decays and i is a dummy index (i = 1, 2, 3, ...D), each nuclide population can be found in terms of the previous population. In this case N2 = 0, N3 = 0,..., ND = 0. Using the above result in a recursive form:The general solution to the recursive problem is given by Bateman's equations:[22]In all of the above examples, the initial nuclide decays into just one product.[23] Consider the case of one initial nuclide that can decay into either of two products, that is A → B and A → C in parallel. For example, in a sample of potassium-40, 89.3% of the nuclei decay to calcium-40 and 10.7% to argon-40. We have for all time t:which is constant, since the total number of nuclides remains constant. Differentiating with respect to time:defining the total decay constant λ in terms of the sum of partial decay constants λB and λC:Notice that Solving this equation for NA:where NA0 is the initial number of nuclide A. When measuring the production of one nuclide, one can only observe the total decay constant λ. The decay constants λB and λC determine the probability for the decay to result in products B or C as follows:because the fraction λB/λ of nuclei decay into B while the fraction λC/λ of nuclei decay into C.The above equations can also be written using quantities related to the number of nuclide particles N in a sample;where L = 7023602200000000000♠6.022×1023 is Avogadro's constant, Ar is the relative atomic mass number, and the amount of the substance is in moles.For the one-decay solution A → B:the equation indicates that the decay constant λ has units of t−1, and can thus also be represented as 1/τ, where τ is a characteristic time of the process called the time constant.In a radioactive decay process, this time constant is also the mean lifetime for decaying atoms. Each atom "lives" for a finite amount of time before it decays, and it may be shown that this mean lifetime is the arithmetic mean of all the atoms' lifetimes, and that it is τ, which again is related to the decay constant as follows:This form is also true for two-decay processes simultaneously A → B + C, inserting the equivalent values of decay constants (as given above)into the decay solution leads to:A more commonly used parameter is the half-life. Given a sample of a particular radionuclide, the half-life is the time taken for half the radionuclide's atoms to decay. For the case of one-decay nuclear reactions:the half-life is related to the decay constant as follows: set N = N0/2 and t = T1/2 to obtainThis relationship between the half-life and the decay constant shows that highly radioactive substances are quickly spent, while those that radiate weakly endure longer. Half-lives of known radionuclides vary widely, from more than 1019 years, such as for the very nearly stable nuclide 209Bi, to 10−23 seconds for highly unstable ones.The factor of ln(2) in the above relations results from the fact that the concept of "half-life" is merely a way of selecting a different base other than the natural base e for the lifetime expression. The time constant τ is the e -1 -life, the time until only 1/e remains, about 36.8%, rather than the 50% in the half-life of a radionuclide. Thus, τ is longer than t1/2. The following equation can be shown to be valid:Since radioactive decay is exponential with a constant probability, each process could as easily be described with a different constant time period that (for example) gave its "(1/3)-life" (how long until only 1/3 is left) or "(1/10)-life" (a time period until only 10% is left), and so on. Thus, the choice of τ and t1/2 for marker-times, are only for convenience, and from convention. They reflect a fundamental principle only in so much as they show that the same proportion of a given radioactive substance will decay, during any time-period that one chooses.Mathematically, the nth life for the above situation would be found in the same way as above—by setting N = N0/n, t = T1/n and substituting into the decay solution to obtainA sample of 14C has a half-life of 5,730 years and a decay rate of 14 disintegration per minute (dpm) per gram of natural carbon.If an artifact is found to have radioactivity of 4 dpm per gram of its present C, we can find the approximate age of the object using the above equation:The radioactive decay modes of electron capture and internal conversion are known to be slightly sensitive to chemical and environmental effects that change the electronic structure of the atom, which in turn affects the presence of 1s and 2s electrons that participate in the decay process. A small number of mostly light nuclides are affected. For example, chemical bonds can affect the rate of electron capture to a small degree (in general, less than 1%) depending on the proximity of electrons to the nucleus. In 7Be, a difference of 0.9% has been observed between half-lives in metallic and insulating environments.[24] This relatively large effect is because beryllium is a small atom whose valence electrons are in 2s atomic orbitals, which are subject to electron capture in 7Be because (like all s atomic orbitals in all atoms) they naturally penetrate into the nucleus.In 1992, Jung et al. of the Darmstadt Heavy-Ion Research group observed an accelerated β− decay of 163Dy66+. Although neutral 163Dy is a stable isotope, the fully ionized 163Dy66+ undergoes β− decay into the K and L shells to 163Ho66+ with a half-life of 47 days.[25]Rhenium-187 is another spectacular example. 187Re normally beta decays to 187Os with a half-life of 41.6 × 109 years,[26] but studies using fully ionised 187Re atoms (bare nuclei) have found that this can decrease to only 33 years. This is attributed to "bound-state β− decay" of the fully ionised atom – the electron is emitted into the "K-shell" (1s atomic orbital), which cannot occur for neutral atoms in which all low-lying bound states are occupied.[27]A number of experiments have found that decay rates of other modes of artificial and naturally occurring radioisotopes are, to a high degree of precision, unaffected by external conditions such as temperature, pressure, the chemical environment, and electric, magnetic, or gravitational fields.[28] Comparison of laboratory experiments over the last century, studies of the Oklo natural nuclear reactor (which exemplified the effects of thermal neutrons on nuclear decay), and astrophysical observations of the luminosity decays of distant supernovae (which occurred far away so the light has taken a great deal of time to reach us), for example, strongly indicate that unperturbed decay rates have been constant (at least to within the limitations of small experimental errors) as a function of time as well.[citation needed]Recent results suggest the possibility that decay rates might have a weak dependence on environmental factors. It has been suggested that measurements of decay rates of silicon-32, manganese-54, and radium-226 exhibit small seasonal variations (of the order of 0.1%).[29][30][31]  However, such measurements are highly susceptible to systematic errors, and a subsequent paper[32] has found no evidence for such correlations in seven other isotopes (22Na, 44Ti, 108Ag, 121Sn, 133Ba, 241Am, 238Pu), and sets upper limits on the size of any such effects. The decay of radon-222 was once reported to exhibit large 4% peak-to-peak seasonal variations (see plot),[33]  which were proposed to be related to either solar flare activity or the distance from the Sun, but detailed analysis of the experiment's design flaws, along with comparisons to other, much more stringent and systematically controlled, experiments refute this claim.[34]An unexpected series of experimental results for the rate of decay of heavy highly charged radioactive ions circulating in a storage ring has provoked theoretical activity in an effort to find a convincing explanation. The rates of weak decay of two radioactive species with half lives of about 40 s and 200 s are found to have a significant oscillatory modulation, with a period of about 7 s.[35]The observed phenomenon is known as the GSI anomaly, as the storage ring is a facility at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany.  As the decay process produces an electron neutrino, some of the proposed explanations for the observed rate oscillation invoke neutrino properties. Initial ideas related to flavour oscillation met with skepticism.[36]  A more recent proposal involves mass differences between neutrino mass eigenstates.[37]The neutrons and protons that constitute nuclei, as well as other particles that approach close enough to them, are governed by several interactions. The strong nuclear force, not observed at the familiar macroscopic scale, is the most powerful force over subatomic distances. The electrostatic force is almost always significant, and, in the case of beta decay, the weak nuclear force is also involved.The combined effects of these forces produces a number of different phenomena in which energy may be released by rearrangement of particles in the nucleus, or else the change of one type of particle into others. These rearrangements and transformations may be hindered energetically, so that they do not occur immediately. In certain cases, random quantum vacuum fluctuations are theorized to promote relaxation to a lower energy state (the "decay") in a phenomenon known as quantum tunneling. Radioactive decay half-life of nuclides has been measured over timescales of 55 orders of magnitude, from 2.3 × 10−23 seconds (for hydrogen-7) to 6.9 × 1031 seconds (for tellurium-128).[38] The limits of these timescales are set by the sensitivity of instrumentation only, and there are no known natural limits to how brief or long a decay half-life for radioactive decay of a radionuclide may be.The decay process, like all hindered energy transformations, may be analogized by a snowfield on a mountain. While friction between the ice crystals may be supporting the snow's weight, the system is inherently unstable with regard to a state of lower potential energy. A disturbance would thus facilitate the path to a state of greater entropy; the system will move towards the ground state, producing heat, and the total energy will be distributable over a larger number of quantum states thus resulting in an avalanche. The total energy does not change in this process, but, because of the second law of thermodynamics, avalanches have only been observed in one direction and that is toward the "ground state" — the state with the largest number of ways in which the available energy could be distributed.Such a collapse (a gamma-ray decay event) requires a specific activation energy. For a snow avalanche, this energy comes as a disturbance from outside the system, although such disturbances can be arbitrarily small. In the case of an excited atomic nucleus decaying by gamma radiation in a spontaneous emission of electromagnetic radiation, the arbitrarily small disturbance comes from quantum vacuum fluctuations.[39]A radioactive nucleus (or any excited system in quantum mechanics) is unstable, and can, thus, spontaneously stabilize to a less-excited system. The resulting transformation alters the structure of the nucleus and results in the emission of either a photon or a high-velocity particle that has mass (such as an electron, alpha particle, or other type).[citation needed]According to the Big Bang theory, stable isotopes of the lightest five elements (H, He, and traces of Li, Be, and B) were produced very shortly after the emergence of the universe, in a process called Big Bang nucleosynthesis. These lightest stable nuclides (including deuterium) survive to today, but any radioactive isotopes of the light elements produced in the Big Bang (such as tritium) have long since decayed. Isotopes of elements heavier than boron were not produced at all in the Big Bang, and these first five elements do not have any long-lived radioisotopes. Thus, all radioactive nuclei are, therefore, relatively young with respect to the birth of the universe, having formed later in various other types of nucleosynthesis in stars (in particular, supernovae), and also during ongoing interactions between stable isotopes and energetic particles. For example, carbon-14, a radioactive nuclide with a half-life of only 5,730 years, is constantly produced in Earth's upper atmosphere due to interactions between cosmic rays and nitrogen.Nuclides that are produced by radioactive decay are called radiogenic nuclides, whether they themselves are stable or not. There exist stable radiogenic nuclides that were formed from short-lived extinct radionuclides in the early solar system.[40][41] The extra presence of these stable radiogenic nuclides (such as Xe-129 from primordial I-129) against the background of primordial stable nuclides can be inferred by various means.Radioactive decay has been put to use in the technique of radioisotopic labeling, which is used to track the passage of a chemical substance through a complex system (such as a living organism). A sample of the substance is synthesized with a high concentration of unstable atoms. The presence of the substance in one or another part of the system is determined by detecting the locations of decay events.On the premise that radioactive decay is truly random (rather than merely chaotic), it has been used in hardware random-number generators. Because the process is not thought to vary significantly in mechanism over time, it is also a valuable tool in estimating the absolute ages of certain materials. For geological materials, the radioisotopes and some of their decay products become trapped when a rock solidifies, and can then later be used (subject to many well-known qualifications) to estimate the date of the solidification. These include checking the results of several simultaneous processes and their products against each other, within the same sample. In a similar fashion, and also subject to qualification, the rate of formation of carbon-14 in various eras, the date of formation of organic matter within a certain period related to the isotope's half-life may be estimated, because the carbon-14 becomes trapped when the organic matter grows and incorporates the new carbon-14 from the air. Thereafter, the amount of carbon-14 in organic matter decreases according to decay processes that may also be independently cross-checked by other means (such as checking the carbon-14 in individual tree rings, for example).The Szilard–Chalmers effect is defined as the breaking of a chemical bond between an atom and the molecule that the atom is part of, as a result of a nuclear reaction of the atom. The effect can be used to separate isotopes by chemical means. The discovery of this effect is due to Leó Szilárd and Thomas A. Chalmers.[42]Radioactive primordial nuclides found in the Earth are residues from ancient supernova explosions that occurred before the formation of the solar system. They are the fraction of radionuclides that survived from that time, through the formation of the primordial solar nebula, through planet accretion, and up to the present time. The naturally occurring short-lived radiogenic radionuclides found in today's rocks, are the daughters of those radioactive primordial nuclides. Another minor source of naturally occurring radioactive nuclides are cosmogenic nuclides, that are formed by cosmic ray bombardment of material in the Earth's atmosphere or crust. The decay of the radionuclides in rocks of the Earth's mantle and crust contribute significantly to Earth's internal heat budget.The daughter nuclide of a decay event may also be unstable (radioactive). In this case, it too will decay, producing radiation. The resulting second daughter nuclide may also be radioactive. This can lead to a sequence of several decay events called a decay chain (see this article for specific details of important natural decay chains). Eventually, a stable nuclide is produced. Any decay daughters that are the result of an alpha decay will also result in helium atoms being created.An example is the natural decay chain of 238U:Some radionuclides may have several different paths of decay. For example, approximately 36% of bismuth-212 decays, through alpha-emission, to thallium-208 while approximately 64% of bismuth-212 decays, through beta-emission, to polonium-212. Both thallium-208 and polonium-212 are radioactive daughter products of bismuth-212, and both decay directly to stable lead-208.The trefoil symbol used to indicate ionising radiation.2007 ISO radioactivity danger symbol intended for IAEA Category 1, 2 and 3 sources defined as dangerous sources capable of death or serious injury.[43]The dangerous goods transport classification sign for radioactive materials
Chalaza
The chalaza (/kəˈleɪzə/; from Greek χάλαζα "hailstone"; plural chalazas or chalazae, /kəˈleɪzi/) is a structure inside bird and reptile eggs and plant ovules. It attaches or suspends the yolk or nucellus within the larger structure.In the eggs of most birds and reptiles, the chalazae are two spiral bands of tissue that suspend the yolk in the center of the white (the albumen).  The function of the chalazae is to hold the yolk in place. In baking, the chalazae are sometimes removed in order to ensure a uniform texture.In plant ovules, the chalaza is located opposite the micropyle opening of the integuments. It is the tissue where the integuments and nucellus are joined. Nutrients from the plant travel through vascular tissue in the funiculus and outer integument through the chalaza into the nucellus. During the development of the embryo sac inside a flowering plant ovule, the three cells at the chalazal end become the antipodal cells.In most flowering plants, the pollen tube enters the ovule through the micropyle opening in the integuments for fertilization (porogamy). In chalazogamous fertilization, the pollen tubes penetrate the ovule through the chalaza rather than the micropyle opening.[1]Chalazogamy was first discovered in monoecious plant species of the Casuarinaceae family by Melchior Treub, but has since then also been observed in others, for example in pistachio and walnut.
Lenticel
A lenticel is a porous tissue consisting of cells with large intercellular spaces in the periderm of the secondarily thickened organs and the bark of woody stems and roots of dicotyledonous flowering plants.[2] It functions as a pore, providing a pathway for the direct exchange of gases between the internal tissues and atmosphere through the bark, which is otherwise impermeable to gases. The name lenticel, pronounced with an [s], derives from its lenticular (lens-like) shape.[3] The shape of lenticels is one of the characteristics used for tree identification.[4]Before there was much evidence for the existence and functionality of lenticels, the fossil record has shown the first primary mechanism of aeration in early vascular plants to be the stomata.[5] However, if there are internal stresses present, stomatal tissue expansion or damage can result. Woody plants, with vascular and cork cambia activity, are prime candidates for the latter. This necessity of aeration structures that combated stomatal damage in the presence of the secondary tissues of these woody plants is where lenticels are believed to have evolved.The extinct arboreal plants of the genera Lepidodendron and Sigillaria were the first to have distinct aeration structures that rendered these modifications. "Parichnoi" (singular: parichnos) are canal-like structures that, in association with foliar traces of the stem, connected the stem's outer and middle cortex to the mesophyll of the leaf. Parichnoi were thought to eventually give rise to lenticels as they helped solve the issue of long-range oxygen transport in these woody plants during the Carboniferous period. They also evolved to acquire secondary connections as they evolved to become transversely elongated to efficiently aerate the maximum number of vertical rays as well as the central core tissue of the stem.[6] The evolutionary significance of these parichnoi was their functionality in the absence of cauline stomata, where they can also be affected and destroyed by pressure similar to what can damage to stomatal tissue. Evidently, in both conifers and Lepidodendroids, the parichnoi, as the primary lenticular structure, appear as paired structures on either side of leaf scars. The development and increase in the number of these primitive lenticels were key to providing a system that was open for aeration and gas exchange in these plants.[7]In plant bodies that produce secondary growth, lenticels promote gas exchange of oxygen, carbon dioxide, and water vapor.[8] Lenticel formation usually begins beneath stomatal complexes during primary growth preceding the development of the first periderm.The formation of lenticels seem to be directly related to the growth and strength of the shoot and on the hydrose of the tissue, which refers to the internal moisture.[9] As stems and roots mature lenticel development continues in the new periderm (for example, periderm that forms at the bottom of cracks in the bark).Lenticels are found as raised circular, oval, or elongated areas on stems and roots. In woody plants, lenticels commonly appear as rough, cork-like structures on young branches. Underneath them, porous tissue creates a number of large intercellular spaces between cells. This tissue fills the lenticel and arises from cell division in the phellogen or substomatal ground tissue. Discoloration of lenticels may also occur, such as in mangoes, that may be due to the amount of lignin in cell walls.[10][11]In oxygen deprived conditions, making respiration a daily challenge, different species may possess specialized structures where lenticels can be found. For example, in a common mangrove species, lenticels appear on pneumatophores (specialized roots), where the parenchyma cells that connect to the aerenchyma structure increase in size and go through cell division.[12] In contrast, lenticels in grapes are located on the pedicels and act as a function of temperature. If they are blocked, hypoxia and successive ethanol accumulation may result and lead to cell death.[13]Lenticels are also present on many fruits, quite noticeably on many apples and pears. On European pears, they can serve as an indicator of when to pick the fruit, as light lenticels on the immature fruit darken and become brown and shallow from the formation of cork cells[14][15] Certain bacterial and fungal infections can penetrate fruits through their lenticels, with susceptibility sometimes increasing with its age.[16]As mentioned previously, the term lenticel is usually associated with the breakage of periderm tissue that is associated with gas exchange; however, lenticels also refer to the lightly colored spots found on apples (a type of pome fruit). "Lenticel" seems to be the most appropriate term to describe both structures mentioned in light of their similar function in gas exchange. Pome lenticels can be derived from (1) no longer functioning stomata, (2) epidermal breaks from the removal of trichomes, and (3) other epidermal breaks that usually occur in the early development of young pome fruits. The closing of pome lenticels can arise when the cuticle over the stomata opening or the substomatal layer seals. Closing can also begin if the substomatal cells become suberized, like cork. The number of lenticels usually varies between the species of apples, where the range may be from 450 to 800 or from 1500 to 2500 in Winesap and Spitzenburg apples, respectively. This wide range may be due to the water availability during the early stages of development of each apple type.[17]“Lenticel breakdown” is a global skin disorder of apples in which lenticels develop dark 1–8 mm diameter pits shortly after processing and packing.[18][19] It is most common on the ‘Gala’ (Malus × domestica) variety, particularly the ‘Royal Gala’, and also occurs in ‘Fuji’, ‘Granny Smith’, ‘Golden Delicious’, and ‘Delicious’ varieties.[18][19] It is more common in arid regions, and is thought to be related to relative humidity and temperature.[18][19] The effect can be mitigated by spraying the fruit with lipophilic coatings prior to harvest.[18]Lenticels are also present on potato tubers.[20]Lenticels on Prunus serrulaLenticels on Wild Cherry or Gean.Alder bark (Alnus glutinosa) with characteristic lenticels and abnormal lenticels on callused areas.Lenticels on potatoes of the Monalisa variety.Lenticels on Williams pear varieties.
Curing (food preservation)
Curing is any of various food preservation and flavoring processes of foods such as meat, fish and vegetables, by the addition of combinations of  salt, nitrates, nitrites,[1] or sugar, with the aim of drawing moisture out of the food by the process of osmosis. Many curing processes also involve smoking, spicing, or cooking.  Dehydration was the earliest form of food curing.[1] Because curing increases the solute concentration in the food and hence decreases its water potential, the food becomes inhospitable for the microbe growth that causes food spoilage. Curing can be traced back to antiquity, and was the primary way of preserving meat and fish until the late-19th century.Nitrates and nitrites, in conjunction with salt, are the most common agents in curing meat, because they further inhibit the growth of Clostridium botulinum. The combination of table salt with nitrates or nitrites, called curing salt, is often dyed pink to distinguish it from table salt.[2] Neither table salt, nor any of the nitrites or nitrates commonly used in curing (e.g. sodium nitrate,[3] sodium nitrite,[3] and potassium nitrate[4])is naturally pink.Meat preservation in general (of meat from livestock, game, and poultry) comprises the set of all treatment processes for preserving the properties, taste, texture, and color of raw, partially cooked, or cooked meats while keeping them edible and safe to consume. Curing has been the dominant method of meat preservation for thousands of years, although modern developments like refrigeration and synthetic preservatives have begun to complement and supplant it.While meat-preservation processes like curing were mainly developed in order to prevent disease and to increase food security, the advent of modern preservation methods mean that in most developed countries today[update] curing is instead mainly practised for its cultural value and desirable impact on the texture and taste of food. For  lesser-developed countries, curing remains a key process in the  production, transport and availability of meat.Untreated meat decomposes rapidly if it is not preserved, at a speed that depends on several factors, including ambient humidity, temperature, and the presence of pathogens. Most meats cannot be kept at room temperature in excess of a few days without spoiling.If kept in excess of this time, meat begins to change color and exude a foul odor, indicating the decomposition of the food. Ingestion of such spoiled meat can cause serious food poisonings, like botulism.While the short shelf life of fresh meat does not pose a significant problem when access to it is easy and supply is abundant, in times of scarcity and famine, or when the meat must be carried over long voyages, it spoils very quickly. In such circumstances the usefulness of preserving foods containing nutritional value for transport and storage is obvious.Curing can significantly extend the life of meat before it spoils, by making it inhospitable to the growth of spoilage microbes.A survival technique since prehistory, the preservation of meat has become, over the centuries, a topic of political, economic, and social importance worldwide.Food curing dates back to ancient times, both in the form of smoked meat and salt-cured meat.[5]Several sources describe the salting of meat in the ancient Mediterranean world. Diodore of Sicily in his Bibliotheca historica wrote that the Cosséens[6] in the mountains of Persia salted the flesh of carnivorous animals.[7] Strabo indicates that people at Borsippa were catching bats and salting them to eat.[8] The ancient Greeks prepared tarichos (τάριχος), which was meat and fish conserved by salt or other means.[a] The Romans called this dish salsamentum – which term later included salted fat, the sauces and spices used for its preparation.[9]Also evidence of ancient sausage production exists. The Roman gourmet Apicius speaks of a sausage-making technique involving œnogaros (a mixture of the fermented fish sauce garum with oil or wine).[10] Preserved meats were furthermore a part of religious traditions: resulting meat for offerings to the gods was salted before being given to priests, after which it could be picked up again by the offerer, or even sold in the butcher's.[9]A trade in salt meat occurred across ancient Europe. In Polybius's time,[11] the Gauls exported salt pork each year to Rome in large quantities, where it was sold in different cuts: rear cuts, middle cuts, hams, and sausages. This meat, after having been salted with the greatest care, was sometime smoked. These goods had to have been considerably important, since they fed part of the Roman people and the armies. The Belgae were celebrated above all for the care which they gave to the fattening of their pigs. Their herds of sheep and pigs were so many, they could provide skins and salt meat not only for Rome, but also for most of Italy.[citation needed] The Ceretani of Spain drew a large export income from their hams, which were so succulent, they were in no way inferior to those of Cantabria. These tarichos of pig would become especially sought, to the point that the ancients considered this meat the most nourishing of all and the easiest to digest.[9]In Ethiopia, according to Pliny,[12] and in Libya according to Saint Jerome, the Acridophages (literally, the locust-eaters) salted and smoked the crickets which arrived at their settlements in the spring in great swarms and which constituted, it was said, their sole food.The smoking of meat was a traditional practice in North America, where Plains Indians hung their meat at the top of their tipis to increase the amount of smoke coming into contact with the food.[5]In Europe, medieval cuisine made great use of meat and vegetables, and the guild of butchers was amongst the most powerful. During the 12th century,[13] salt beef was consumed by all social classes. Smoked meat was called carbouclée in Romance tongues[14] and bacon if it was pork.[15]The Middle Ages made pâté a masterpiece: that which is, in the 21st century, merely spiced minced meat (or fish), baked in a terrine and eaten cold, was at that time composed of a dough envelope stuffed with varied meats and superbly decorated for ceremonial feasts. The first French recipe, written in verse by Gace de La Bigne, mentions in the same pâté three great partridges, six fat quail, and a dozen larks. Le Ménagier de Paris mentions pâtés of fish, game, young rabbit, fresh venison, beef, pigeons, mutton, veal, and pork, and even pâtés of lark, turtledove, cow, baby bird, goose, and hen. Bartolomeo Sacchi, called Platine, prefect of the Vatican Library, gives the recipe for a pâté of wild beasts: the flesh, after being boiled with salt and vinegar, was larded and placed inside an envelope of spiced fat, with a mélange of pepper, cinnamon and pounded lard; one studded the fat with cloves until it was entirely covered, then placed it inside a pâte.In the 16th century, the most fashionable pâtés were of woodcock, au bec doré, chapon, beef tongue, cow feet, sheep feet, chicken, veal, and venison.[16] In the same era, Pierre Belon notes that the inhabitants of Crete and Chios lightly salted then oven-dried entire hares, sheep, and roe deer cut into pieces, and that in Turkey, cattle and sheep, cut and minced rouelles, salted then dried, were eaten on voyages with onions and no other preparation.[17]During the Age of Discovery, salt meat was one of the main foods for sailors on long voyages, for instance in the merchant marine or the navy. In the 18th century, salted Irish beef, transported in barrels, was considered finest.[18]Scientific research on meat by chemists and pharmacists led to the creation of a new, extremely practical product: meat extract, which could appear in different forms. The need to properly feed soldiers during long campaigns outside the country, such as the Napoleonic Wars, and to nourish a constantly growing population often living in appalling conditions drove scientific research, but  a confectioner, Nicolas Appert, in 1795 developed through experimentation a method which would become universal and in one language bears his name: airtight storage, called appertisation in French.With the spread of appertisation, the 19th-century world entered the era of the "food industry", which developed new products such as canned salt meat (for example corned beef), but also led to lowered standards of food quality and hygiene[dubious  – discuss] – such as those Upton Sinclair described in The Jungle. These bad practices led to the creation of the Pure Food and Drug Act in 1906, followed by the national agencies for health security and the establishment of food traceability over the course of the 20th century.[citation needed] It also led to continuing technological innovation.In France, the summer of 1857 was so hot that most butchers refused to slaughter animals and charcutiers lost considerable amounts of meat, due to inadequate conservation methods. A member of the Academy of Medicine and his son issued a 34-page summary of works printed between 1663 and 1857, which proposed some solutions: not less than 91 texts exist, of which 64 edited for only the years between 1851 and 1857.[19]Salt (sodium chloride) is the primary ingredient used in meat curing.[5] Removal of water and addition of salt to meat creates a solute-rich environment where osmotic pressure draws water out of microorganisms, slowing down their growth.[5][20]  Doing this requires a concentration of salt of nearly 20%.[20] In addition, salt causes the soluble proteins to come to the surface of the meat that was used to make the sausages.  These proteins coagulate when the sausage is heated, helping to hold the sausage together.[21]The sugar added to meat for the purpose of curing it comes in many forms, including honey, corn syrup solids, and maple syrup.[22] However, with the exception of bacon, it does not contribute much to the flavor,[23] but it does alleviate the harsh flavor of the salt.[5] Sugar also contributes to the growth of beneficial bacteria such as Lactobacillus by feeding them.[24]Nitrates and nitrites not only help kill bacteria, but also produce a characteristic flavor and give meat a pink or red color.[25] Nitrite (NO−2) is generally supplied by sodium nitrite or (indirectly) by potassium nitrate. Nitrite salts are most often used in curing. Nitrate is specifically used only in a few curing conditions and products where nitrite (which may be generated from nitrate) must be generated in the product over long periods of time.Nitrite further breaks down in the meat into nitric oxide (NO), which then binds to the iron atom in the center of myoglobin's heme group, reducing oxidation and causing a reddish-brown color (nitrosomyoglobin) when raw and the characteristic cooked-ham pink color (nitrosohemochrome or nitrosyl-heme) when cooked. The addition of ascorbate to cured meat reduces formation of nitrosamines (see below), but increases the nitrosylation of iron.The use of nitrite and nitrate salts for meat in the US has been formally used since 1925.[citation needed] Because of the relatively high toxicity of nitrite (the lethal dose in humans is about 22 mg/kg of body weight), the maximum allowed nitrite concentration in meat products is 200 ppm. Plasma nitrite is reduced in persons with endothelial dysfunction.[26]The use of nitrites in food preservation is controversial due to the potential for the formation of nitrosamines when nitrites are present in high concentrations and the product is cooked at high temperatures.[25] The effect is seen for red or processed meat, but not for white meat or fish.[27][28] Nitrates and nitrites may cause cancer and the production of carcinogenic nitrosamines can be potently inhibited by the use of the antioxidants Vitamin C and the alpha-tocopherol form of Vitamin E during curing.[29] Under simulated gastric conditions, nitrosothiols rather than nitrosamines are the main nitroso species being formed.[27] The use of either compound is therefore regulated; for example, in the United States, the concentration of nitrates and nitrites is generally limited to 200 ppm or lower.[25] While the meat industry considers them irreplaceable because of their low cost and efficacy at maintaining color, botulism is an extremely rare disease (less than 1000 cases reported worldwide per year), and almost always associated with home preparations of food storing.[30] Furthermore, while the FDA has set a limit of 200 ppm of nitrates for cured meat, they are not allowed and not recognized as safe in most other foods, even foods that are not cooked at high temperatures, such as cheese.[31]Processed meats without "added nitrites" may be misleading as they may be using naturally occurring nitrites from celery instead.[32]Meat can also be preserved by "smoking". If the smoke is hot enough to slow-cook the meat, this will also keep it tender.[33]  One method of smoking calls for a smokehouse with damp wood chips or sawdust.[34]  In North America, hardwoods such as hickory, mesquite, and maple are commonly used for smoking, as are the wood from fruit trees such as apple, cherry, and plum, and even corncobs.Smoking helps seal the outer layer of the food being cured, making it more difficult for bacteria to enter. It can be done in combination with other curing methods such as salting. Common smoking styles include hot smoking, smoke roasting (pit barbecuing) and cold smoking.  Smoke roasting and hot smoking cook the meat while cold smoking does not. If the meat is cold smoked, it should be dried quickly to limit bacterial growth during the critical period where the meat is not yet dry. This can be achieved, as with jerky, by slicing the meat thinly.The smoking of food directly with wood smoke is known to contaminate the food with carcinogenic polycyclic aromatic hydrocarbons.[35]Since the 20th century, with respect to the relationship between diet and human disease (e.g. cardiovascular, etc.), scientists have conducted studies on the effects of lipolysis on vacuum-packed or frozen meat. In particular, by analyzing entrecôtes of frozen beef during 270 days at −20 °C (−4 °F), scientists found an important phospholipase that accompanies the loss of some unsaturated fat n-3 and n-6, which are already low in the flesh of ruminants.[36]In 2015, the International Agency for Research on Cancer of the World Health Organization classified processed meat, that is, meat that has undergone salting, curing, fermenting, or smoking, as "carcinogenic to humans".[37][38][39]The improvement of methods of meat preservation, and of the means of transport of preserved products, has notably permitted the separation of areas of production and areas of consumption, which can now be distant without it posing a problem, permitting the exportation of meats.For example, the appearance in the 1980s of preservation techniques under controlled atmosphere sparked a small revolution in the world's market for sheep meat: the lamb of New Zealand, one of the world's largest exporters of lamb, could henceforth be sold as fresh meat, since it could be preserved from 12 to 16 weeks, which would be a sufficient duration for it to reach Europe by boat. Before, meat from New Zealand was frozen, thus had a much lower value on European shelves. With the arrival of the new "chilled" meats, New Zealand could compete even more strongly with local producers of fresh meat.[40] The use of controlled atmosphere to avoid the depreciation which affects frozen meat is equally useful in other meat markets, such as that for pork, which now also enjoys an international trade.[41]
Order (biology)
In biological classification, the order (Latin: ordo) isWhat does and does not belong to each order is determined by a taxonomist, as is whether a particular order should be recognized at all. Often there is no exact agreement, with different taxonomists each taking a different position. There are no hard rules that a taxonomist needs to follow in describing or recognizing an order. Some taxa are accepted almost universally, while others are recognised only rarely.[1]For some groups of organisms, consistent suffixes are used to denote that the rank is an order. The Latin suffix -(i)formes meaning "having the form of" is used for the scientific name of orders of birds and fishes, but not for those of mammals and invertebrates. The suffix -ales is for the name of orders of plants, fungi, and algae.[2]For some clades covered by the International Code of Zoological Nomenclature, a number of additional classifications are sometimes used, although not all of these are officially recognised.In their 1997 classification of mammals, McKenna and Bell used two extra levels between superorder and order: "grandorder" and "mirorder".[3] Michael Novacek (1986) inserted them at the same position. Michael Benton (2005) inserted them between superorder and magnorder instead.[4] This position was adopted by Systema Naturae 2000 and others.In botany, the ranks of subclass and suborder are secondary ranks pre-defined as respectively above and below the rank of order.[5] Any number of further ranks can be used as long as they are clearly defined.[5]The superorder rank is commonly used, with the ending -anae that was initiated by Armen Takhtajan's publications from 1966 onwards.[6]The order as a distinct rank of biological classification having its own distinctive name (and not just called a higher genus (genus summum)) was first introduced by the German botanist Augustus Quirinus Rivinus in his classification of plants that appeared in a series of treatises in the 1690s. Carl Linnaeus was the first to apply it consistently to the division of all three kingdoms of nature (minerals, plants, and animals) in his Systema Naturae (1735, 1st. Ed.).For plants, Linnaeus' orders in the Systema Naturae and the Species Plantarum were strictly artificial, introduced to subdivide the artificial classes into more comprehensible smaller groups.  When the word ordo was first consistently used for natural units of plants, in 19th century works such as the Prodromus of de Candolle and the Genera Plantarum of Bentham & Hooker, it indicated taxa that are now given the rank of family (see ordo naturalis, natural order).In French botanical publications, from Michel Adanson's Familles naturelles des plantes (1763) and until the end of the 19th century, the word famille (plural: familles) was used as a French equivalent for this Latin ordo. This equivalence was explicitly stated in the Alphonse De Candolle's Lois de la nomenclature botanique (1868), the precursor of the currently used International Code of Nomenclature for algae, fungi, and plants.In the first international Rules of botanical nomenclature from the International Botanical Congress of 1905, the word family (familia) was assigned to the rank indicated by the French "famille", while order (ordo) was reserved for a higher rank, for what in the 19th century had often been named a cohors[8] (plural cohortes).Some of the plant families still retain the names of Linnaean "natural orders" or even the names of pre-Linnaean natural groups recognised by Linnaeus as orders in his natural classification (e.g. Palmae or Labiatae). Such names are known as descriptive family names.In zoology, the Linnaean orders were used more consistently. That is, the orders in the zoology part of the Systema Naturae refer to natural groups. Some of his ordinal names are still in use (e.g. Lepidoptera for the order of moths and butterflies, or Diptera for the order of flies, mosquitoes, midges, and gnats).[citation needed]In virology, the International Committee on Taxonomy of Viruses's virus classification includes fifteen taxa: realm, subrealm, kingdom, subkingdom, phylum, subphylum, class, subclass, order, suborder, family, subfamily, genus, subgenus, and species, to be applied for viruses, viroids and  satellite nucleic acids.[9] There are nine viral orders, each ending in the suffix -virales.[10]
Flowering plant
Basal angiospermsCore angiospermsThe flowering plants, also known as angiosperms, Angiospermae[5][6] or Magnoliophyta,[7] are the most diverse group of land plants, with 416 families, approximately 13,164 known genera and c. 295,383 known species.[8] Like gymnosperms, angiosperms are seed-producing plants. However, they are distinguished from gymnosperms by characteristics including flowers, endosperm within the seeds, and the production of fruits that contain the seeds. Etymologically, angiosperm means a plant that produces seeds within an enclosure; in other words, a fruiting plant. The term comes from the Greek words angeion ("case" or "casing") and sperma ("seed").The ancestors of flowering plants diverged from gymnosperms in the Triassic Period, 245 to 202 million years ago (mya), and the first flowering plants are known from 160 mya. They diversified extensively during the Early Cretaceous, became widespread by 120 mya, and replaced conifers as the dominant trees from 100 to 60 mya.Angiosperms differ from other seed plants in several ways, described in the table below. These distinguishing characteristics taken together have made the angiosperms the most diverse and numerous land plants and the most commercially important group to humans.[a]Angiosperm stems are made up of seven layers as shown above. The amount and complexity of tissue-formation in flowering plants exceeds that of gymnosperms. The vascular bundles of the stem are arranged such that the xylem and phloem form concentric rings.In the dicotyledons, the bundles in the very young stem are arranged in an open ring, separating a central pith from an outer cortex. In each bundle, separating the xylem and phloem, is a layer of meristem or active formative tissue known as cambium. By the formation of a layer of cambium between the bundles (interfascicular cambium), a complete ring is formed, and a regular periodical increase in thickness results from the development of xylem on the inside and phloem on the outside. The soft phloem becomes crushed, but the hard wood persists and forms the bulk of the stem and branches of the woody perennial. Owing to differences in the character of the elements produced at the beginning and end of the season, the wood is marked out in transverse section into concentric rings, one for each season of growth, called annual rings.Among the monocotyledons, the bundles are more numerous in the young stem and are scattered through the ground tissue. They contain no cambium and once formed the stem increases in diameter only in exceptional cases.The characteristic feature of angiosperms is the flower. Flowers show remarkable variation in form and elaboration, and provide the most trustworthy external characteristics for establishing relationships among angiosperm species. The function of the flower is to ensure fertilization of the ovule and development of fruit containing seeds. The floral apparatus may arise terminally on a shoot or from the axil of a leaf (where the petiole attaches to the stem). Occasionally, as in violets, a flower arises singly in the axil of an ordinary foliage-leaf. More typically, the flower-bearing portion of the plant is sharply distinguished from the foliage-bearing or vegetative portion, and forms a more or less elaborate branch-system called an inflorescence.There are two kinds of reproductive cells produced by flowers. Microspores, which will divide to become pollen grains, are the "male" cells and are borne in the stamens (or microsporophylls). The "female" cells called megaspores, which will divide to become the egg cell (megagametogenesis), are contained in the ovule and enclosed in the carpel (or megasporophyll).The flower may consist only of these parts, as in willow, where each flower comprises only a few stamens or two carpels. Usually, other structures are present and serve to protect the sporophylls and to form an envelope attractive to pollinators. The individual members of these surrounding structures are known as sepals and petals (or tepals in flowers such as Magnolia where sepals and petals are not distinguishable from each other). The outer series (calyx of sepals) is usually green and leaf-like, and functions to protect the rest of the flower, especially the bud. The inner series (corolla of petals) is, in general, white or brightly colored, and is more delicate in structure. It functions to attract insect or bird pollinators. Attraction is effected by color, scent, and nectar, which may be secreted in some part of the flower. The characteristics that attract pollinators account for the popularity of flowers and flowering plants among humans.While the majority of flowers are perfect or hermaphrodite (having both pollen and ovule producing parts in the same flower structure), flowering plants have developed numerous morphological and physiological mechanisms to reduce or prevent self-fertilization. Heteromorphic flowers have short carpels and long stamens, or vice versa, so animal pollinators cannot easily transfer pollen to the pistil (receptive part of the carpel). Homomorphic flowers may employ a biochemical (physiological) mechanism called self-incompatibility to discriminate between self and non-self pollen grains. In other species, the male and female parts are morphologically separated, developing on different flowers.The botanical term "Angiosperm", from the Ancient Greek αγγείον, angeíon (bottle, vessel) and σπέρμα, (seed), was coined in the form Angiospermae by Paul Hermann in 1690, as the name of one of his primary divisions of the plant kingdom. This included flowering plants possessing seeds enclosed in capsules, distinguished from his Gymnospermae, or flowering plants with achenial or schizo-carpic fruits, the whole fruit or each of its pieces being here regarded as a seed and naked. The term and its antonym were maintained by Carl Linnaeus with the same sense, but with restricted application, in the names of the orders of his class Didynamia. Its use with any approach to its modern scope became possible only after 1827, when Robert Brown established the existence of truly naked ovules in the Cycadeae and Coniferae,[11] and applied to them the name Gymnosperms.[citation needed] From that time onward, as long as these Gymnosperms were, as was usual, reckoned as dicotyledonous flowering plants, the term Angiosperm was used antithetically by botanical writers, with varying scope, as a group-name for other dicotyledonous plants.In 1851, Hofmeister discovered the changes occurring in the embryo-sac of flowering plants, and determined the correct relationships of these to the Cryptogamia. This fixed the position of Gymnosperms as a class distinct from Dicotyledons, and the term Angiosperm then gradually came to be accepted as the suitable designation for the whole of the flowering plants other than Gymnosperms, including the classes of Dicotyledons and Monocotyledons. This is the sense in which the term is used today.In most taxonomies, the flowering plants are treated as a coherent group. The most popular descriptive name has been Angiospermae (Angiosperms), with Anthophyta ("flowering plants") a second choice. These names are not linked to any rank. The Wettstein system and the Engler system use the name Angiospermae, at the assigned rank of subdivision. The Reveal system treated flowering plants as subdivision Magnoliophytina (Frohne & U. Jensen ex Reveal, Phytologia 79: 70 1996), but later split it to Magnoliopsida, Liliopsida, and Rosopsida. The Takhtajan system and Cronquist system treat this group at the rank of division, leading to the name Magnoliophyta (from the family name Magnoliaceae). The Dahlgren system and Thorne system (1992) treat this group at the rank of class, leading to the name Magnoliopsida. The APG system of 1998, and the later 2003[12] and 2009[13] revisions, treat the flowering plants as a clade called angiosperms without a formal botanical name. However, a formal classification was published alongside the 2009 revision in which the flowering plants form the Subclass Magnoliidae.[14]The internal classification of this group has undergone considerable revision. The Cronquist system, proposed by Arthur Cronquist in 1968 and published in its full form in 1981, is still widely used but is no longer believed to accurately reflect phylogeny. A consensus about how the flowering plants should be arranged has recently begun to emerge through the work of the Angiosperm Phylogeny Group (APG), which published an influential reclassification of the angiosperms in 1998. Updates incorporating more recent research were published as the APG II system in 2003,[12] the APG III system in 2009,[13][15] and the APG IV system in 2016.Traditionally, the flowering plants are divided into two groups,which in the Cronquist system are called Magnoliopsida (at the rank of class, formed from the family name Magnoliaceae) and Liliopsida (at the rank of class, formed from the family name Liliaceae). Other descriptive names allowed by Article 16 of the ICBN include Dicotyledones or Dicotyledoneae, and Monocotyledones or Monocotyledoneae, which have a long history of use. In English a member of either group may be called a dicotyledon (plural dicotyledons) and monocotyledon (plural monocotyledons), or abbreviated, as dicot (plural dicots) and monocot (plural monocots). These names derive from the observation that the dicots most often have two cotyledons, or embryonic leaves, within each seed. The monocots usually have only one, but the rule is not absolute either way. From a broad diagnostic point of view, the number of cotyledons is neither a particularly handy nor a reliable character.Recent studies, as by the APG, show that the monocots form a monophyletic group (clade) but that the dicots do not (they are paraphyletic). Nevertheless, the majority of dicot species do form a monophyletic group, called the eudicots or tricolpates. Of the remaining dicot species, most belong to a third major clade known as the magnoliids, containing about 9,000 species. The rest include a paraphyletic grouping of early branching taxa known collectively as the basal angiosperms, plus the families Ceratophyllaceae and Chloranthaceae.There are eight groups of living angiosperms:The exact relationship between these eight groups is not yet clear, although there is agreement that the first three groups to diverge from the ancestral angiosperm were Amborellales, Nymphaeales, and Austrobaileyales.[17] The term basal angiosperms refers to these three groups. Among the remaining five groups (core angiosperms), the relationship between the three broadest of these groups (magnoliids, monocots, and eudicots) remains unclear. Zeng and colleagues (Fig. 1) describe four competing schemes.[18] Of these, eudicots and monocots are the largest and most diversified, with ~ 75% and 20% of angiosperm species, respectively. Some analyses make the magnoliids the first to diverge, others the monocots.[19] Ceratophyllum seems to group with the eudicots rather than with the monocots. The 2016 Angiosperm Phylogeny Group revision (APG IV) retained the overall higher order relationship described in APG III.[13]AmborellaNymphaealesAustrobaileyalesmagnoliidsChloranthalesmonocotsCeratophyllumeudicots1. Phylogeny of the flowering plants, as of APG III (2009).[13]AmborellaNymphaealesAustrobaileyalesmonocotsChloranthalesmagnoliidsCeratophyllumeudicots2. Example of alternative phylogeny (2010)[19]Amborellales  Nymphaeales  Austrobaileyales  magnoliids  Chloranthales  monocots  Ceratophyllales   eudicots  3. APG IV (2016)[1]Amborellales Melikyan, Bobrov & Zaytzeva 1999Nymphaeales Salisbury ex von Berchtold & Presl 1820Austrobaileyales Takhtajan ex Reveal 1992Chloranthales Mart. 1835Canellales Cronquist 1957Piperales von Berchtold & Presl 1820Magnoliales de Jussieu ex von Berchtold & Presl 1820Laurales de Jussieu ex von Berchtold & Presl 1820Acorales Link 1835Alismatales Brown ex von Berchtold & Presl 1820Petrosaviales Takhtajan 1997Dioscoreales Brown 1835Pandanales Brown ex von Berchtold & Presl 1820Liliales Perleb 1826Asparagales Link 1829Arecales Bromhead 1840Poales Small 1903Zingiberales Grisebach 1854Commelinales de Mirbel ex von Berchtold & Presl 1820Ceratophyllales Link 1829Ranunculales de Jussieu ex von Berchtold & Presl 1820Proteales de Jussieu ex von Berchtold & Presl 1820Trochodendrales Takhtajan ex Cronquist 1981Buxales Takhtajan ex Reveal 1996Gunnerales Takhtajan ex Reveal 1992Dilleniales de Candolle ex von Berchtold & Presl 1820Saxifragales von Berchtold & Presl 1820Vitales de Jussieu ex von Berchtold & Presl 1820Zygophyllales Link 1829Celastrales Link 1829Oxalidales von Berchtold & Presl 1820Malpighiales de Jussieu ex von Berchtold & Presl 1820Fabales Bromhead 1838Rosales von Berchtold & Presl 1820Cucurbitales de Jussieu ex von Berchtold & Presl 1820Fagales Engler 1892Geraniales de Jussieu ex von Berchtold & Presl 1820Myrtales de Jussieu ex von Berchtold & Presl 1820Crossosomatales Takhtajan ex Reveal 1993Picramniales Doweld 2001Sapindales de Jussieu ex von Berchtold & Presl 1820Huerteales Doweld 2001Malvales de Jussieu ex von Berchtold & Presl 1820Brassicales Bromhead 1838Berberidopsidales Doweld 2001Santalales Brown ex von Berchtold & Presl 1820CaryophyllalesCornales Link 1829Ericales von Berchtold & Presl 1820Icacinales Van Tieghem 1900Metteniusales Takhtajan 1997Garryales Mart. 1835Gentianales de Jussieu ex von Berchtold & Presl 1820Solanales de Jussieu ex von Berchtold & Presl 1820Boraginales de Jussieu ex von Berchtold & Presl 1820Vahliales Doweld 2001Lamiales Bromhead 1838Aquifoliales Senft 1856Escalloniales Mart. 1835Asterales Link 1829Bruniales Dumortier 1829Apiales Nakai 1930Paracryphiales Takhtajan ex Reveal 1992Dipsacales de Jussieu ex von Berchtold & Presl 1820Fossilized spores suggest that higher plants (embryophytes) have lived on land for at least 475 million years.[20] Early land plants reproduced sexually with flagellated, swimming sperm, like the green algae from which they evolved. An adaptation to terrestrialization was the development of upright meiosporangia for dispersal by spores to new habitats. This feature is lacking in the descendants of their nearest algal relatives, the Charophycean green algae. A later terrestrial adaptation took place with retention of the delicate, avascular sexual stage, the gametophyte, within the tissues of the vascular sporophyte. This occurred by spore germination within sporangia rather than spore release, as in non-seed plants. A current example of how this might have happened can be seen in the precocious spore germination in Selaginella, the spike-moss. The result for the ancestors of angiosperms was enclosing them in a case, the seed. The first seed bearing plants, like the ginkgo, and conifers (such as pines and firs), did not produce flowers. The pollen grains (male gametophytes) of Ginkgo and cycads produce a pair of flagellated, mobile sperm cells that "swim" down the developing pollen tube to the female and her eggs.The apparently sudden appearance of nearly modern flowers in the fossil record initially posed such a problem for the theory of evolution that Charles Darwin called it an "abominable mystery".[21] However, the fossil record has considerably grown since the time of Darwin, and recently discovered angiosperm fossils such as Archaefructus, along with further discoveries of fossil gymnosperms, suggest how angiosperm characteristics may have been acquired in a series of steps. Several groups of extinct gymnosperms, in particular seed ferns, have been proposed as the ancestors of flowering plants, but there is no continuous fossil evidence showing exactly how flowers evolved. Some older fossils, such as the upper Triassic Sanmiguelia, have been suggested. Based on current evidence, some propose that the ancestors of the angiosperms diverged from an unknown group of gymnosperms in the Triassic period (245–202 million years ago). Fossil angiosperm-like pollen from the Middle Triassic (247.2–242.0 Ma) suggests an older date for their origin.[22]  A close relationship between angiosperms and gnetophytes, proposed on the basis of morphological evidence, has more recently been disputed on the basis of molecular evidence that suggest gnetophytes are instead more closely related to other gymnosperms.[citation needed]The evolution of seed plants and later angiosperms appears to be the result of two distinct rounds of whole genome duplication events.[23] These occurred at 319 million years ago and 192 million years ago. Another possible whole genome duplication event at 160 million years ago  perhaps created the ancestral line that led to all modern flowering plants.[24] That event was studied by sequencing the genome of an ancient flowering plant, Amborella trichopoda,[25] and directly addresses Darwin's "abominable mystery."The earliest known macrofossil confidently identified as an angiosperm, Archaefructus liaoningensis, is dated to about 125 million years BP (the Cretaceous period),[26] whereas pollen considered to be of angiosperm origin takes the fossil record back to about 130 million years BP. However, one study has suggested that the early-middle Jurassic plant Schmeissneria, traditionally considered a type of ginkgo, may be the earliest known angiosperm, or at least a close relative.[27] In addition, circumstantial chemical evidence has been found for the existence of angiosperms as early as 250 million years ago. Oleanane, a secondary metabolite produced by many flowering plants, has been found in Permian deposits of that age together with fossils of gigantopterids.[28][29] Gigantopterids are a group of extinct seed plants that share many morphological traits with flowering plants, although they are not known to have been flowering plants themselves.[citation needed]In 2013 flowers encased in amber were found and dated 100 million years before present. The amber had frozen the act of sexual reproduction in the process of taking place. Microscopic images showed tubes growing out of pollen and penetrating the flower's stigma. The pollen was sticky, suggesting it was carried by insects.[30]Recent DNA analysis based on molecular systematics[31][32] showed that Amborella trichopoda, found on the Pacific island of New Caledonia, belongs to a sister group of the other flowering plants, and morphological studies[33] suggest that it has features that may have been characteristic of the earliest flowering plants.The orders Amborellales, Nymphaeales, and Austrobaileyales diverged as separate lineages from the remaining angiosperm clade at a very early stage in flowering plant evolution.[34]The great angiosperm radiation, when a great diversity of angiosperms appears in the fossil record, occurred in the mid-Cretaceous (approximately 100 million years ago). However, a study in 2007 estimated that the division of the five most recent (the genus Ceratophyllum, the family Chloranthaceae, the eudicots, the magnoliids, and the monocots) of the eight main groups occurred around 140 million years ago.[35]By the late Cretaceous, angiosperms appear to have dominated environments formerly occupied by ferns and cycadophytes, but large canopy-forming trees replaced conifers as the dominant trees only close to the end of the Cretaceous 66 million years ago or even later, at the beginning of the Tertiary.[36] The radiation of herbaceous angiosperms occurred much later.[37] Yet, many fossil plants recognizable as belonging to modern families (including beech, oak, maple, and magnolia) had already appeared by the late Cretaceous.It has been proposed that the swift rise of angiosperms to dominance was facilitated by a reduction in their genome size. During the early Cretaceous period, only angiosperms underwent rapid genome downsizing, while genome sizes of ferns and gymnosperms remained unchanged.  Smaller genomes–and smaller nuclei–allow for faster rates of cell division and smaller cells.  Thus, species with smaller genomes can pack more, smaller cells–in particular veins and stomata–into a given leaf volume.  Genome downsizing therefore facilitated higher rates of leaf gas exchange (transpiration and photosynthesis) and faster rates of growth.  This would have countered some of the negative physiological effects of genome duplications, facilitated increased uptake of carbon dioxide despite concurrent declines in atmospheric CO2 concentrations, and allowed the flowering plants to outcompete other land plants.[38]It is generally assumed that the function of flowers, from the start, was to involve mobile animals in their reproduction processes. That is, pollen can be scattered even if the flower is not brightly colored or oddly shaped in a way that attracts animals; however, by expending the energy required to create such traits, angiosperms can enlist the aid of animals and, thus, reproduce more efficiently.Island genetics provides one proposed explanation for the sudden, fully developed appearance of flowering plants. Island genetics is believed to be a common source of speciation in general, especially when it comes to radical adaptations that seem to have required inferior transitional forms. Flowering plants may have evolved in an isolated setting like an island or island chain, where the plants bearing them were able to develop a highly specialized relationship with some specific animal (a wasp, for example). Such a relationship, with a hypothetical wasp carrying pollen from one plant to another much the way fig wasps do today, could result in the development of a high degree of specialization in both the plant(s) and their partners. Note that the wasp example is not incidental; bees, which, it is postulated, evolved specifically due to mutualistic plant relationships, are descended from wasps.[39]Animals are also involved in the distribution of seeds. Fruit, which is formed by the enlargement of flower parts, is frequently a seed-dispersal tool that attracts animals to eat or otherwise disturb it, incidentally scattering the seeds it contains (see frugivory). Although many such mutualistic relationships remain too fragile to survive competition and to spread widely, flowering proved to be an unusually effective means of reproduction, spreading (whatever its origin) to become the dominant form of land plant life.Flower ontogeny uses a combination of genes normally responsible for forming new shoots.[40] The most primitive flowers probably had a variable number of flower parts, often separate from (but in contact with) each other. The flowers tended to grow in a spiral pattern, to be bisexual (in plants, this means both male and female parts on the same flower), and to be dominated by the ovary (female part). As flowers evolved, some variations developed parts fused together, with a much more specific number and design, and with either specific sexes per flower or plant or at least "ovary-inferior".Flower evolution continues to the present day; modern flowers have been so profoundly influenced by humans that some of them cannot be pollinated in nature. Many modern domesticated flower species were formerly simple weeds, which sprouted only when the ground was disturbed. Some of them tended to grow with human crops, perhaps already having symbiotic companion plant relationships with them, and the prettiest did not get plucked because of their beauty, developing a dependence upon and special adaptation to human affection.[41]A few paleontologists have also proposed that flowering plants, or angiosperms, might have evolved due to interactions with dinosaurs. One of the idea's strongest proponents is Robert T. Bakker. He proposes that herbivorous dinosaurs, with their eating habits, provided a selective pressure on plants, for which adaptations either succeeded in deterring or coping with predation by herbivores.[42]In August 2017, scientists presented a detailed description and 3D model image of what the first flower possibly looked like, and presented the hypothesis that it may have lived about 140 million years ago.[43][44]A Bayesian analysis of 52 angiosperm taxa suggested that the crown group of angiosperms evolved between 178 million years ago and 198 million years ago.[45]The number of species of flowering plants is estimated to be in the range of 250,000 to 400,000.[46][47][48] This compares to around 12,000 species of moss[49] or 11,000 species of pteridophytes,[50] showing that the flowering plants are much more diverse. The number of families in APG (1998) was 462. In APG II[12] (2003) it is not settled; at maximum it is 457, but within this number there are 55 optional segregates, so that the minimum number of families in this system is 402. In APG III (2009) there are 415 families.[13][51]The diversity of flowering plants is not evenly distributed. Nearly all species belong to the eudicot (75%), monocot (23%), and magnoliid (2%) clades. The remaining 5 clades contain a little over 250 species in total; i.e. less than 0.1% of flowering plant diversity, divided among 9 families. The 43 most-diverse of 443 families of flowering plants by species,[52] in their APG circumscriptions, areOf these, the Orchidaceae, Poaceae, Cyperaceae, Araceae, Bromeliaceae, Arecaceae, and Iridaceae are monocot families; Piperaceae, Lauraceae, and Annonaceae are magnoliid dicots; the rest of the families are eudicots.Double fertilization refers to a process in which two sperm cells fertilize cells in the ovule. This process begins when a pollen grain adheres to the stigma of the pistil (female reproductive structure), germinates, and grows a long pollen tube. While this pollen tube is growing, a haploid generative cell travels down the tube behind the tube nucleus. The generative cell divides by mitosis to produce two haploid (n) sperm cells. As the pollen tube grows, it makes its way from the stigma, down the style and into the ovary. Here the pollen tube reaches the micropyle of the ovule and digests its way into one of the synergids, releasing its contents (which include the sperm cells). The synergid that the cells were released into degenerates and one sperm makes its way to fertilize the egg cell, producing a diploid (2n) zygote. The second sperm cell fuses with both central cell nuclei, producing a triploid (3n) cell. As the zygote develops into an embryo, the triploid cell develops into the endosperm, which serves as the embryo's food supply. The ovary will now develop into a fruit and the ovule will develop into a seed.As the development of embryo and endosperm proceeds within the embryo sac, the sac wall enlarges and combines with the nucellus (which is likewise enlarging) and the integument to form the seed coat. The ovary wall develops to form the fruit or pericarp, whose form is closely associated with type of seed dispersal system.[54]Frequently, the influence of fertilization is felt beyond the ovary, and other parts of the flower take part in the formation of the fruit, e.g., the floral receptacle in the apple, strawberry, and others.[citation needed]The character of the seed coat bears a definite relation to that of the fruit. They protect the embryo and aid in dissemination; they may also directly promote germination. Among plants with indehiscent fruits, in general, the fruit provides protection for the embryo and secures dissemination. In this case, the seed coat is only slightly developed. If the fruit is dehiscent and the seed is exposed, in general, the seed-coat is well developed, and must discharge the functions otherwise executed by the fruit.[citation needed]Flowering plants generate gametes using a specialized cell division called meiosis.  Meiosis takes place in the ovule (a structure within the ovary that is located within the pistil at the center of the flower) (see diagram labeled "Angiosperm lifecycle").  A diploid cell (megaspore mother cell) in the ovule undergoes meiosis (involving two successive cell divisions) to produce four cells (megaspores) with haploid nuclei.[55]  One of these four cells (megaspore) then undergoes three successive mitotic divisions to produce an immature embryo sac (megagametophyte) with eight haploid nuclei.  Next, these nuclei are segregated into separate cells by cytokinesis to producing 3 antipodal cells, 2 synergid cells and an egg cell.  Two polar nuclei are left in the central cell of the embryo sac.[citation needed]Pollen is also produced by meiosis in the male anther (microsporangium).  During meiosis, a diploid microspore mother cell undergoes two successive meiotic divisions to produce 4 haploid cells (microspores or male gametes). Each of these microspores, after further mitoses, becomes a pollen grain (microgametophyte) containing two haploid generative (sperm) cells and a tube nucleus.  When a pollen grain makes contact with the female stigma, the pollen grain forms a pollen tube that grows down the style into the ovary.  In the act of fertilization, a male sperm nucleus fuses with the female egg nucleus to form a diploid zygote that can then develop into an embryo within the newly forming seed.  Upon germination of the seed, a new plant can grow and mature.[citation needed]The adaptive function of meiosis is currently a matter of debate.  A key event during meiosis in a diploid cell is the pairing of homologous chromosomes and homologous recombination (the exchange of genetic information) between homologous chromosomes.  This process promotes the production of increased genetic diversity among progeny and the recombinational repair of damages in the DNA to be passed on to progeny.  To explain the adaptive function of meiosis in flowering plants, some authors emphasize diversity[56] and others emphasize DNA repair.[57]Apomixis (reproduction via asexually formed seeds) is found naturally in about 2.2% of angiosperm genera.[58]  One type of apomixis, gametophytic apomixis found in a dandelion species[59]  involves formation of an unreduced embryo sac due to incomplete meiosis (apomeiosis) and development of an embryo from the unreduced egg inside the embryo sac, without fertilization (parthenogenesis).[citation needed]Agriculture is almost entirely dependent on angiosperms, which provide virtually all plant-based food, and also provide a significant amount of livestock feed. Of all the families of plants, the Poaceae, or grass family (providing grains), is by far the most important, providing the bulk of all feedstocks (rice, maize, wheat, barley, rye, oats, pearl millet, sugar cane, sorghum). The Fabaceae, or legume family, comes in second place. Also of high importance are the Solanaceae, or nightshade family (potatoes, tomatoes, and peppers, among others); the Cucurbitaceae, or gourd family (including pumpkins and melons); the Brassicaceae, or mustard plant family (including rapeseed and the innumerable varieties of the cabbage species Brassica oleracea); and the Apiaceae, or parsley family. Many of our fruits come from the Rutaceae, or rue family (including oranges, lemons, grapefruits, etc.), and the Rosaceae, or rose family (including apples, pears, cherries, apricots, plums, etc.).[citation needed]In some parts of the world, certain single species assume paramount importance because of their variety of uses, for example the coconut (Cocos nucifera) on Pacific atolls, and the olive (Olea europaea) in the Mediterranean region.[60]Flowering plants also provide economic resources in the form of wood, paper, fiber (cotton, flax, and hemp, among others), medicines (digitalis, camphor), decorative and landscaping plants, and many other uses. The main area in which they are surpassed by other plants—namely, coniferous trees (Pinales), which are non-flowering (gymnosperms)—is timber and paper production.[61]
Forest ecology
Forest ecology is the scientific study of the interrelated patterns, processes, flora, fauna and ecosystems in forests.  The management of forests is known as forestry, silviculture, and forest management.  A forest ecosystem is a natural woodland unit consisting of all plants, animals and micro-organisms (Biotic components) in that area functioning together with all of the non-living physical (abiotic) factors of the environment.[1] The forest ecosystem is very important.[clarification needed]Forest ecology is one branch of a biotically-oriented classification of types of ecological study (as opposed to a classification based on organizational level or complexity, for example population or community ecology).  Thus, forests are studied at a number of organizational levels, from the individual organism to the ecosystem.  However, as the term forest connotes an area inhabited by more than one organism, forest ecology most often concentrates on the level of the population, community or ecosystem.  Logically, trees are an important component of forest research, but the wide variety of other life forms and abiotic components in most forests means that other elements, such as wildlife or soil nutrients, are often the focal point.  Thus, forest ecology is a highly diverse and important branch of ecological study.[citation needed]Forest ecology studies share characteristics and methodological approaches with other areas of terrestrial plant ecology.  However, the presence of trees makes forest ecosystems and their study unique in numerous ways.Since trees can grow larger than other plant life-forms, there is the potential for a wide variety of forest structures (or physiognomies).  The infinite number of possible spatial arrangements of trees of varying size and species makes for a highly intricate and diverse micro-environment in which environmental variables such as solar radiation, temperature, relative humidity, and wind speed can vary considerably over large and small distances.  In addition, an important proportion of a forest ecosystem's biomass is often underground, where soil structure, water quality and quantity, and levels of various soil nutrients can vary greatly.[2]  Thus, forests are often highly heterogeneous environments compared to other terrestrial plant communities.  This heterogeneity in turn can enable great biodiversity of species of both plants and animals. Some structures, such as tree ferns may be keystone species for a diverse range of other species.[3]A number of factors within the forest affect biodiversity; primary factors enhancing wildlife abundance and biodiversity are the presence of diverse tree species within the forest and the absence of even aged timber management.[4] For example, the wild turkey thrives when uneven heights and canopy variations exist and its numbers are diminished by even aged timber management.Forest management techniques that mimic natural disturbance events (variable retention forestry [5]) can allow community diversity to recover rapidly for a variety of groups including beetles.[6]In 2017, the biologist Dr. Roberto Cazzolla Gatti and his colleagues tested [7] a global correlation between vascular plant species richness (S) and average forest canopy height (H). They found a significant correlation between H and S both at global and macro-climate scales, with the strongest confidence in the tropics. The authors of this study suggested that the higher the forest canopy, the bigger the number of species a forest can host.Forests accumulate large amounts of standing biomass, and many are capable of accumulating it at high rates, i.e. they are highly productive.  Such high levels of biomass and tall vertical structures represent large stores of potential energy that can be converted to kinetic energy under the right circumstances.  Two such conversions of great importance are fires and treefalls, both of which radically alter the biota and the physical environment where they occur.  Also, in forests of high productivity, the rapid growth of the trees themselves induces biotic and environmental changes, although at a slower rate and lower intensity than relatively instantaneous disturbances such as fires.Woody material, often referred to as coarse woody debris, decays relatively slowly in many forests in comparison to most other organic materials, due to a combination of environmental factors and wood chemistry (see lignin).  Trees growing in arid and/or cold environments do so especially slowly.  Thus, tree trunks and branches can remain on the forest floor for long periods, affecting such things as wildlife habitat, fire behavior, and tree regeneration processes.Lastly, forest trees store large amounts of water because of their large size and anatomical/physiological characteristics.  They are therefore important regulators of hydrological processes, especially those involving groundwater hydrology and local evaporation and rainfall/snowfall patterns.[8]  Thus, forest ecological studies are sometimes closely aligned with meteorological and hydrological studies in regional ecosystem or resource planning studies. Perhaps more importantly the duff or leaf litter can form a major repository of water storage. When this litter is removed or compacted ( through grazing or human overuse), erosion and flooding are exacerbated as well as deprivation of dry season water for forest organisms.The ecological potential of a particular species is a measure of its capacity to effectively compete in a given geographical area, ahead of other species, as they all try to occupy a natural space. For some areas it has been quantified, as for instance by Hans-Jürgen Otto, for central Europe.[9] He takes three groups of parameters:Every parameter is scored between 0 and 5 for each considered species, and then a global mean value calculated. A value above 3.5 is considered high, below 3.0 low, and intermediate for those in between. In this study Fagus sylvatica has a score of 3.82, Fraxinus excelsior 3.08 and Juglans regia 2.92; and are examples of the three categories.
Taxodium mucronatum
Taxodium distichum var. mucronatum (Ten.) A.HenryTaxodium mexicanum CarrièreTaxodium distichum var. mexicanum (Carrière) GordonCuprespinnata mexicana (Carrière) J.NelsonTaxodium mucronatum, also known as Montezuma bald cypress,[3] Montezuma cypress, sabino, or ahuehuete is a species of Taxodium that is native to Mexico, and Guatemala.[4]  Ahuehuete is derived from the Nahuatl name for the tree, āhuēhuētl, which means "upright drum in water"[5] or "old man of the water."[2]It is a large evergreen or semi-evergreen tree growing to 40 m (130 ft) tall and with a trunk of 1–3 m (3.3–9.8 ft) diameter (occasionally much more; see below). The leaves are spirally arranged but twisted at the base to lie in two horizontal ranks, 1–2 cm (0.39–0.79 in) long and 1–2 mm (0.039–0.079 in) broad. The cones are ovoid, 1.5–2.5 cm (0.59–0.98 in) long and 1–2 cm (0.39–0.79 in) broad.Unlike bald cypress and pond cypress, Montezuma cypress rarely produces cypress knees from the roots.[2] Trees from the Mexican highlands achieve a notable stoutness.One specimen, the Árbol del Tule in Santa María del Tule, Oaxaca, Mexico, is the stoutest tree in the world with a diameter of 11.42 m (37.5 ft). Several other specimens from 3–6 m (9.8–19.7 ft) diameter are known. The second stoutest tree in the world is the Big Baobab, an African Baobab.Montezuma cypress is primarily a riparian tree, growing along upland riversides, but can also be found next to springs and marshes. It occurs from 300 to 2,500 m (980 to 8,200 ft), in Mexico mainly in highlands at 1,600–2,300 m (5,200–7,500 ft) in altitude. T. mucronatum is very drought-tolerant and fast-growing[6] and favors climates that are rainy throughout the year or at least with high summer rainfall.Taxodium mucronatum is native to much of Mexico as far south as the highlands of southern Mexico.[2] Two disjunct populations exist in the United States.  One is in the Rio Grande Valley of southernmost Texas, while the other is in southern New Mexico, near Las Cruces.[7][8] Within Guatemala, the tree is restricted to Huehuetenango Department.[4]The sabino became the national tree of Mexico in 1910.[9] The tree is sacred to the native peoples of Mexico, and is featured in the Zapotec creation myth.[10] To the Aztecs, the combined shade of an āhuēhuētl and a pōchōtl (Ceiba pentandra) metaphorically represented a ruler's authority.[11]  According to legend, Hernán Cortés wept under an ahuehuete in Popotla[12] after suffering defeat during the Battle of La Noche Triste.[13]Montezuma cypresses have been used as ornamental trees since Pre-Columbian times. The Aztecs planted āhuēhuētl along processional paths in the gardens of Chapultepec because of its association with government.[14] Artificial islands called chinampas were formed in the shallow lakes of the Valley of Mexico by adding soil to rectangular areas enclosed by trees such as āhuēhuētl;[2] they also lined the region's canals prior to Spanish conquest.[9]Ahuehuetes are frequently cultivated in Mexican parks and gardens.  The wood is used to make house beams and furniture.[13] The Aztecs used its resin to treat gout, ulcers, skin diseases, wounds, and toothaches.  A decoction made from the bark was used as a diuretic and an emmenagogue. Pitch derived from the wood was used as a cure for bronchitis The leaves acted as a relaxant and could help reduce itching.[15]John Naka, a world-renowned bonsai master, donated his very first bonsai, a Montezuma Cypress, to the National Bonsai and Penjing Museum of the United States.A linear grove is located in the main courtyard of the Getty Center Art Museum, thriving since 1995.[16]New World Species:New World Species:
Throughfall
In Hydrology, throughfall is the process which describes how wet leaves shed excess water onto the ground surface. These drops have greater erosive power because they are heavier than rain drops. Furthermore, where there is a high canopy, falling drops may reach terminal velocity, about 8 metres (26 ft), thus maximizing the drop's erosive potential.[1]Rates of throughfall are higher in areas of forest where the leaves are broad-leaved. This is because the flat leaves allow water to collect. Drip-tips also facilitate throughfall. Rates of throughfall are lower in coniferous forests as conifers can only hold individual droplets of water on their needles.
Sequoiadendron giganteum
Sequoiadendron giganteum (giant sequoia; also known as giant redwood, Sierra redwood, Sierran redwood, Wellingtonia or simply Big Tree—a nickname used by John Muir[2]) is the sole living species in the genus Sequoiadendron, and one of three species of coniferous trees known as redwoods, classified in the family Cupressaceae in the subfamily Sequoioideae, together with Sequoia sempervirens (coast redwood) and Metasequoia glyptostroboides (dawn redwood). Giant sequoia specimens are the most massive trees on Earth.[3] The common use of the name sequoia generally refers to Sequoiadendron giganteum, which occurs naturally only in groves on the western slopes of the Sierra Nevada Mountains of California.The etymology of the genus name has been presumed—initially in The Yosemite Book by Josiah Whitney in 1868[4]—to be in honor of Sequoyah (1767–1843), who was the inventor of the Cherokee syllabary.[5] An etymological study published in 2012, however, concluded that the name was more likely to have originated from the Latin sequi (meaning to follow) since the number of seeds per cone in the newly-classified genus fell in mathematical sequence with the other four genera in the suborder.[6]Giant sequoia specimens are the most massive individual trees in the world.[3] They grow to an average height of 50–85 m (164–279 ft) and 6–8 m (20–26 ft) in trunk diameter. Record trees have been measured at 94.8 m (311 ft) tall. Trunk diameters of 17 m (56 ft) have been claimed via research figures taken out of context.[7] The specimen known to have the greatest diameter at breast height is the General Grant tree, at 8.8 m (28.9 ft).[8] Between 2014 and 2016, specimens of coast redwood were found to have greater trunk diameters than all known giant sequoias.[9] The trunks of coast redwoods taper at lower heights than those of giant sequoias which have more columnar trunks that maintain larger diameters to greater heights.The oldest known giant sequoia is 3,500 years old based on dendrochronology. Giant sequoias are among the oldest living organisms on Earth. Giant sequoia bark is fibrous, furrowed, and may be 90 cm (3 ft) thick at the base of the columnar trunk. The bark provides significant protection from fire damage. The leaves are evergreen, awl-shaped, 3–6 mm (1⁄8–1⁄4 in) long, and arranged spirally on the shoots. The seed cones are 4–7 cm (1 1⁄2–3 in) long and mature in 18–20 months, though they typically remain green and closed for a maximum of 20 years; each cone has 30–50 spirally arranged scales, with several seeds on each scale, giving an average of 230 seeds per cone. Seed is dark brown, 4–5 mm (0.16–0.20 in) long, and 1 mm (0.04 in) broad, with a 1-millimeter (0.04 in) wide, yellow-brown wing along each side. Some seeds shed when the cone scales shrink during hot weather in late summer, but most are liberated by insect damage or when the cone dries from the heat of fire.The giant sequoia regenerates by seed. Young trees start to bear cones at the age of 12 years. Trees up to about 20 years old may produce sprouts from their stumps subsequent to injury, but unlike coast redwoods, shoots do not form on the stumps of mature trees. Giant sequoias of all ages may sprout from their boles when branches are lost to fire or breakage.At any given time, a large tree may be expected to have about 11,000 cones. Cone production is greatest in the upper portion of the canopy. A mature giant sequoia has been estimated to disperse 300–400 thousand seeds annually. The winged seeds may be carried up to 180 m (590 ft) from the parent tree.Lower branches die readily from being shaded, but trees younger than 100 years retain most of their dead branches. Trunks of mature trees in groves are generally free of branches to a height of 20–50 m (70–160 ft), but solitary trees retain lower branches.Because of its size, the tree has been studied for its water pull. Water from the roots can be pushed up only a few meters by osmotic pressure but can reach extreme heights by using a system of branching capillarity (capillary action) in the tree's xylem (the water tubules) and sub-pressure from evaporating water at the leaves.[10] Sequoias supplement water from the soil with fog, taken up through air roots, at heights to where the root water cannot be pulled.[11]The natural distribution of giant sequoias is restricted to a limited area of the western Sierra Nevada, California. They occur in scattered groves, with a total of 68 groves (see list of sequoia groves for a full inventory), comprising a total area of only 144.16 km2 (35,620 acres). Nowhere does it grow in pure stands, although in a few small areas, stands do approach a pure condition. The northern two-thirds of its range, from the American River in Placer County southward to the Kings River, has only eight disjunct groves. The remaining southern groves are concentrated between the Kings River and the Deer Creek Grove in southern Tulare County. Groves range in size from 12.4 km2 (3,100 acres) with 20,000 mature trees, to small groves with only six living trees. Many are protected in Sequoia and Kings Canyon National Parks and Giant Sequoia National Monument.The giant sequoia is usually found in a humid climate characterized by dry summers and snowy winters. Most giant sequoia groves are on granitic-based residual and alluvial soils. The elevation of the giant sequoia groves generally ranges from 1,400–2,000 m (4,600–6,600 ft) in the north, to 1,700–2,150 metres (5,580–7,050 ft) to the south. Giant sequoias generally occur on the south-facing sides of northern mountains, and on the northern faces of more southerly slopes.High levels of reproduction are not necessary to maintain the present population levels. Few groves, however, have sufficient young trees to maintain the present density of mature giant sequoias for the future. The majority of giant sequoias are currently undergoing a gradual decline in density since European settlement.While the present day distribution of this species is limited to a small area of California, it was once much more widely distributed in prehistoric times, and was a reasonably common species in North American and Eurasian coniferous forests until its range was greatly reduced by the last ice age. Older fossil specimens reliably identified as giant sequoia have been found in Cretaceous era sediments from a number of sites in North America and Europe, and even as far afield as New Zealand[12] and Australia.[13]A group of sequoias was planted and is naturally propagating on Mount San Jacinto in Riverside County, southern California. The trees were planted by the United States Forest Service after a 1974 wildfire.[14]Giant sequoias are in many ways adapted to forest fires. Their bark is unusually fire resistant, and their cones will normally open immediately after a fire.[15] The giant sequoias are having difficulty reproducing in their original habitat (and very rarely reproduce in cultivation) due to the seeds only being able to grow successfully in full sun and in mineral-rich soils, free from competing vegetation. Although the seeds can germinate in moist needle humus in the spring, these seedlings will die as the duff dries in the summer. They therefore require periodic wildfire to clear competing vegetation and soil humus before successful regeneration can occur. Without fire, shade-loving species will crowd out young sequoia seedlings, and sequoia seeds will not germinate. When fully grown, these trees typically require large amounts of water and are therefore often concentrated near streams.Fires also bring hot air high into the canopy via convection, which in turn dries and opens the cones. The subsequent release of large quantities of seeds coincides with the optimal postfire seedbed conditions. Loose ground ash may also act as a cover to protect the fallen seeds from ultraviolet radiation damage.Due to fire suppression efforts and livestock grazing during the early and mid 20th century, low-intensity fires no longer occurred naturally in many groves, and still do not occur in some groves today. The suppression of fires leads to ground fuel build-up and the dense growth of fire-sensitive white fir, which increases the risk of more intense fires that can use the firs as ladders to threaten mature giant sequoia crowns. Natural fires may also be important in keeping carpenter ants in check.[16]In 1970, the National Park Service began controlled burns of its groves to correct these problems. Current policies also allow natural fires to burn. One of these untamed burns severely damaged the second-largest tree in the world, the Washington tree, in September 2003, 45 days after the fire started. This damage made it unable to withstand the snowstorm of January 2005, leading to the collapse of over half the trunk.In addition to fire, two animal agents also assist giant sequoia seed release. The more significant of the two is a longhorn beetle (Phymatodes nitidus) that lays eggs on the cones, into which the larvae then bore holes. Reduction of the vascular water supply to the cone scales allows the cones to dry and open for the seeds to fall. Cones damaged by the beetles during the summer will slowly open over the next several months. Some research indicates many cones, particularly higher in the crowns, may need to be partially dried by beetle damage before fire can fully open them. The other agent is the Douglas squirrel (Tamiasciurus douglasi) that gnaws on the fleshy green scales of younger cones. The squirrels are active year round, and some seeds are dislodged and dropped as the cone is eaten.[17]The giant sequoia was well known to Native American tribes living in its area. Native American names for the species include wawona, toos-pung-ish and hea-mi-withic, the latter two in the language of the Tule River Tribe.The first reference to the giant sequoia by Europeans is in 1833, in the diary of the explorer J. K. Leonard; the reference does not mention any locality, but his route would have taken him through the Calaveras Grove.[18] This discovery was not publicized. The next European to see the species was John M. Wooster, who carved his initials in the bark of the 'Hercules' tree in the Calaveras Grove in 1850; again, this received no publicity. Much more publicity was given to the "discovery" by Augustus T. Dowd of the Calaveras Grove in 1852, and this is commonly cited as the species' discovery.[18] The tree found by Dowd, christened the 'Discovery Tree', was felled in 1853.The first scientific naming of the species was by John Lindley in December 1853, who named it Wellingtonia gigantea, without realizing this was an invalid name under the botanical code as the name Wellingtonia had already been used earlier for another unrelated plant (Wellingtonia arnottiana in the family Sabiaceae). The name "Wellingtonia" has persisted in England as a common name.[19] The following year, Joseph Decaisne transferred it to the same genus as the coast redwood, naming it Sequoia gigantea, but again this name was invalid, having been applied earlier (in 1847, by Endlicher) to the coast redwood. The name Washingtonia californica was also applied to it by Winslow in 1854, though this too is invalid, belonging to the palm genus Washingtonia.In 1907, it was placed by Carl Ernst Otto Kuntze in the otherwise fossil genus Steinhauera, but doubt as to whether the giant sequoia is related to the fossil originally so named makes this name invalid.The nomenclatural oversights were finally corrected in 1939 by J. Buchholz, who also pointed out the giant sequoia is distinct from the coast redwood at the genus level and coined the name Sequoiadendron giganteum for it.The etymology of the genus name has been presumed—initially in The Yosemite Book by Josiah Whitney in 1868[4]—to be in honor of Sequoyah (1767–1843), who was the inventor of the Cherokee syllabary.[5] An etymological study published in 2012, however, concluded that the name was more likely to have originated from the Latin sequi (meaning to follow) since the number of seeds per cone in the newly-classified genus fell in mathematical sequence with the other four genera in the suborder.[6]John Muir wrote of the species in about 1870:"Do behold the King in his glory, King Sequoia! Behold! Behold! seems all I can say. Some time ago I left all for Sequoia and have been and am at his feet, fasting and praying for light, for is he not the greatest light in the woods, in the world? Where are such columns of sunshine, tangible, accessible, terrestrialized?' [20]Wood from mature giant sequoias is highly resistant to decay, but due to being fibrous and brittle, it is generally unsuitable for construction. From the 1880s through the 1920s, logging took place in many groves in spite of marginal commercial returns. The Hume-Bennett Lumber Company was the last to harvest giant sequoia, going out of business in 1924.[21] Due to their weight and brittleness, trees would often shatter when they hit the ground, wasting much of the wood. Loggers attempted to cushion the impact by digging trenches and filling them with branches. Still, as little as 50% of the timber is estimated to have made it from groves to the mill. The wood was used mainly for shingles and fence posts, or even for matchsticks.Pictures of the once majestic trees broken and abandoned in formerly pristine groves, and the thought of the giants put to such modest use, spurred the public outcry that caused most of the groves to be preserved as protected land. The public can visit an example of 1880s clear-cutting at Big Stump Grove near General Grant Grove. As late as the 1980s, some immature trees were logged in Sequoia National Forest, publicity of which helped lead to the creation of Giant Sequoia National Monument.[citation needed]The wood from immature trees is less brittle, with recent tests on young plantation-grown trees showing it similar to coast redwood wood in quality. This is resulting in some interest in cultivating giant sequoia as a very high-yielding timber crop tree, both in California and also in parts of western Europe, where it may grow more efficiently than coast redwoods. In the northwest United States, some entrepreneurs have also begun growing giant sequoias for Christmas trees. Besides these attempts at tree farming, the principal economic uses for giant sequoia today are tourism and horticulture.Giant sequoia is a very popular ornamental tree in many areas. It is successfully grown in most of western and southern Europe, the Pacific Northwest of North America north to southwest British Columbia, the southern United States, southeast Australia, New Zealand and central-southern Chile. It is also grown, though less successfully, in parts of eastern North America.Trees can withstand temperatures of −31 °C (−25 °F) or colder for short periods of time, provided the ground around the roots is insulated with either heavy snow or mulch. Outside its natural range, the foliage can suffer from damaging windburn.A wide range of horticultural varieties have been selected, especially in Europe, including blue, compact blue, powder blue, hazel smith, pendulum—or weeping—varieties, and grafted cultivars.[22]The tallest giant sequoia ever measured outside of the United States[23] is a specimen planted near Ribeauvillé in France in 1856 and measured in 2014 at a height between 57.7 m (189 ft)[24] and 58.1 m (191 ft)[25] at age 158 years.The giant sequoia was first brought into cultivation in Britain in 1853 by the horticulturist Patrick Matthew of Perthshire from seeds sent by his botanist son John in California.[26] A much larger shipment of seed collected from the Calaveras Grove by William Lobb, acting for the Veitch Nursery near Exeter, arrived in England in December 1853;[27] seed from this batch was widely distributed throughout Europe.Growth in Britain is very fast, with the tallest tree, at Benmore in southwest Scotland, reaching 56.4 m (185 ft) in 2014 at age 150 years,[28] and several others from 50–53 m (164–174 ft) tall; the stoutest is around 12 m (39 ft) in girth and 4 m (13 ft) in diameter, in Perthshire. The Royal Botanic Gardens at Kew in London also contains a large specimen. Biddulph Grange Garden in Staffordshire holds a fine collection of both Sequoiadendron giganteum and also Sequoia sempervirens (Coast Redwood). The General Sherman of California has a volume of 1,489 m3 (52,600 cu ft); by way of comparison, the largest giant sequoias in Great Britain have volumes no greater than 90–100 m3 (3,200–3,500 cu ft), one example being the 90 m3 (3,200 cu ft) specimen in the New Forest.Sequoiadendron giganteum has gained the Royal Horticultural Society’s Award of Garden Merit.[30] [31] Numerous giant sequoia were planted in Italy from 1860 through 1905. Several regions contain specimens that range from 40 to 48 metres (131 to 157 ft) in height. The largest tree is in Roccavione, in the Piedmont, with a basal circumference of 16 metres (52 ft). One notable tree survived a 200-metre (660 ft) tall flood wave in 1963 that was caused by a landslide at Vajont Dam. There are numerous giant sequoia in parks and reserves.[32]Growth rates in some areas of Europe are remarkable. One young tree in Italy reached 22 m (72 ft) tall and 88 cm (2.89 ft) trunk diameter in 17 years (Mitchell, 1972).Growth further northeast in Europe is limited by winter cold. In Denmark, where extreme winters can reach −32 °C (−26 °F), the largest tree was 35 m (115 ft) tall and 1.7 m (5.6 ft) diameter in 1976 and is bigger today. One in Poland has purportedly survived temperatures down to −37 °C (−35 °F) with heavy snow cover.Two members of the German Dendrology Society, E. J. Martin and Illa Martin, introduced the giant sequoia into German forestry at the Sequoiafarm Kaldenkirchen in 1952.[33]Twenty-nine giant sequoias, measuring around 30 m (98 ft) in height, grow in Belgrade's municipality of Lazarevac in Serbia.[34]The oldest sequoiadendron in the Czech Republic, at 44 m (144 ft), grows in Ratměřice u Votic castle garden.Giant sequoias are grown successfully in the Pacific Northwest and southern US, and less successfully in eastern North America. Giant sequoia cultivation is very successful in the Pacific Northwest from western Oregon north to southwest British Columbia, with fast growth rates. In Washington and Oregon, it is common to find giant sequoias that have been successfully planted in both urban and rural areas. In the Seattle area, large specimens exceeding 90 ft (27 m) are fairly common and exist in several city parks and many private yards (especially east Seattle including Capitol Hill, Washington Park, & Leschi/Madrona, as well as Tacoma's Jefferson Park).In the northeastern US there has been some limited success in growing the species, but growth is much slower there, and it is prone to Cercospora and Kabatina fungal diseases due to the hot, humid summer climate there. A tree at Blithewold Gardens, in Bristol, Rhode Island is reported to be 27 metres (89 ft) tall, reportedly the tallest in the New England states.[35][36] The tree at the Tyler Arboretum in Delaware County, Pennsylvania at 29.1 metres (95 ft) may be the tallest in the northeast.[37] Specimens also grow in the Arnold Arboretum in Boston, Massachusetts (planted 1972, 18 m tall in 1998), at Longwood Gardens near Wilmington, Delaware, in the New Jersey State Botanical Garden at Skylands in Ringwood State Park, Ringwood, New Jersey, and in the Finger Lakes region of New York. Private plantings of giant sequoias around the Middle Atlantic States are not uncommon, and other publicly accessible specimens can be visited at the U.S. National Arboretum in Washington, D.C.   A few trees have been established in Colorado as well.[38] Additionally, numerous sequoias have been planted with success in the state of Michigan.[39]A cold-tolerant cultivar 'Hazel Smith' selected in about 1960 is proving more successful in the northeastern US. This clone was the sole survivor of several hundred seedlings grown at a nursery in New Jersey.  The U.S. National Arboretum has a specimen grown from a cutting in 1970 that can be seen in the Gotelli Conifer Collection.The Ballarat Botanical Gardens contain a significant collection, many of them about 150 years old. Jubilee Park and the Hepburn Mineral Springs Reserve in Daylesford, Cook Park in Orange, New South Wales and Carisbrook's Deep Creek park in Victoria both have specimens. Jamieson Township in the Victorian high country has 2 specimens which were planted in the early 1860s.[40]  In Tasmania specimens are to be seen in private and public gardens, as they were popular in the mid Victorian era. The Westbury Village Green has mature specimens with more in Deloraine. The Tasmanian Arboretum contains young wild collected material. The National Arboretum Canberra has begun a grove. They also grow in the abandoned arboretum at Mount Banda Banda in New South Wales.Several impressive specimens of Sequoiadendron giganteum can be found in the South Island of New Zealand. Notable examples include a set of trees in a public park of Picton, as well as robust specimens in the public and botanical parks of Christchurch and Queenstown.[citation needed] There are also several in private gardens in Wanaka.[citation needed] There is also a tree at Rangiora High School, which was planted for Queen Victoria's Golden Jubilee and is thus over 125 years old.[41]Some sequoias, such as the Mother of the Forest, were undoubtedly far larger than any living tree today.[citation needed] However, as of 2009, the top ten largest giant sequoias sorted by volume of their trunks are:[7][note 1]
Allspice
Allspice, also called pimenta,[a] Jamaica pimenta, or myrtle pepper, is the dried unripe fruit (berries, used as a spice) of Pimenta dioica, a midcanopy tree native to the Greater Antilles, southern Mexico, and Central America, now cultivated in many warm parts of the world.[2] The name "allspice" was coined as early as 1621 by the English, who thought it combined the flavour of cinnamon, nutmeg and cloves.[3]Several unrelated fragrant shrubs are called "Carolina allspice" (Calycanthus floridus), "Japanese allspice" (Chimonanthus praecox), or "wild allspice" (Lindera benzoin). "Allspice" is also sometimes used to refer to the herb costmary (Tanacetum balsamita).[citation needed]Allspice is the dried fruit of the Pimenta dioica plant. The fruits are picked when green and unripe and are traditionally dried in the sun. When dry they are brown and resemble large, smooth peppercorns. Fresh leaves are similar in texture to bay leaves and similarly used in cooking. Leaves and wood are often used for smoking meats where allspice is a local crop. Care must be taken during drying to ensure that volatile oil, such as eugenol, remains in the end products.[4]Allspice can also be found in essential oil form.Allspice is one of the most important ingredients of Caribbean cuisine. It is used in Jamaican jerk seasoning (the wood is used to smoke jerk in Jamaica, although the spice is a good substitute), in moles, and in pickling; it is also an ingredient in commercial sausage preparations and curry powders. Allspice is also indispensable in Middle Eastern cuisine, particularly in the Levant, where it is used to flavour a variety of stews and meat dishes. In Arab cuisine, for example, many main dishes call for allspice as the sole spice added for flavouring. In the West Indies, an allspice liqueur is produced under the name "pimento dram" due to conflation of pimenta and pimento.[a]In the United States, it is used mostly in desserts, but it is also responsible for giving Cincinnati-style chili its distinctive aroma and flavor. Allspice is commonly used in Great Britain, and appears in many dishes, including cakes and also in beauty products. In Poland, allspice is used in a variety of dishes, including savory foods like deli meats, soups, marinades and pickles, and to a lesser extent in desserts and fruit drinks. Even in many countries where allspice is not very popular in the household, as in Germany, it is used in large amounts by commercial sausage makers.The allspice tree, classified as an evergreen shrub, can reach 10–18 m (33–59 ft) in height. Allspice can be a small, scrubby tree, quite similar to the bay laurel in size and form. It can also be a tall, canopy tree, sometimes grown to provide shade for coffee trees planted underneath it. It can be grown outdoors in the tropics and subtropics with normal garden soil and watering. Smaller plants can be killed by frost, although larger plants are more tolerant. It adapts well to container culture and can be kept as a houseplant or in a greenhouse.To protect the pimenta trade, the plant was guarded against export from Jamaica. Many attempts at growing the pimenta from seeds were reported, but all failed. At one time, the plant was thought to grow nowhere except in Jamaica, where the plant was readily spread by birds. Experiments were then performed using the constituents of bird droppings; however, these were also totally unsuccessful. Eventually, passage through the avian gut, whether due to the acidity or the elevated temperature, was found to be essential for germinating the seeds. Today, pimenta is spread by birds in Tonga and Hawaii, where it has become naturalized on Kauaʻi and Maui.[5]Allspice (P. dioica) was encountered by Christopher Columbus on the island of Jamaica during his second voyage to the New World, and named by Diego Álvarez Chanca. It was introduced into European and Mediterranean cuisines in the 16th century. It continued to be grown primarily in Jamaica, though a few other Central American countries produced allspice in comparatively small quantities.[6]
Ecosystem
An ecosystem is a community made up of living organisms and nonliving components such as air, water, and mineral soil.[3] Ecosystems can be studied in two different ways. They can be thought of as interdependent collections of plants and animals, or as structured systems and communities governed by general rules.[4] The living (biotic) and non-living (abiotic) components interact through nutrient cycles and energy flows.[5] Ecosystems include interactions among organisms, and between organisms and their environment.[6] Ecosystems can be of any size but each ecosystem has a specific, limited space.[7]  Some scientists view the entire planet as one ecosystem.[8]Energy, water, nitrogen and soil minerals are essential abiotic components of an ecosystem. The energy used by ecosystems comes primarily from the sun, via photosynthesis. Photosynthesis uses energy from the sun and also captures carbon dioxide from the atmosphere. Animals also play an important role in the movement of matter and energy through ecosystems. They influence the amount of plant and microbial biomass that lives in the system. As organic matter dies, carbon is released back into the atmosphere. This process also facilitates nutrient cycling by converting nutrients stored in dead biomass back to a form that can be used again by plants and other microbes.[9] Ecosystems are controlled by both external and internal factors. External factors such as climate, the parent material that forms the soil, topography and time each affect ecosystems. However, these external factors are not themselves influenced by the ecosystem.[10]  Ecosystems are dynamic: they are subject to periodic disturbances and are often in the process of recovering from past disturbances and seeking balance.[11] Internal factors are different: They not only control ecosystem processes but are also controlled by them. Another way of saying this is that internal factors are subject to feedback loops.[10]Humans operate within ecosystems and can influence both internal and external factors.[10] Global warming is an alleged example of a cumulative effect of human activities. Ecosystems provide benefits, called "ecosystem services", which people depend on for their livelihood. Ecosystem management is more efficient than trying to manage individual species.There is no single definition of what constitutes an ecosystem.[4] German ecologist Ernst-Detlef Schulze and coauthors defined an ecosystem as an area which is "uniform regarding the biological turnover, and contains all the fluxes above and below the ground area under consideration." They explicitly reject Gene Likens' use of entire river catchments as "too wide a demarcation" to be a single ecosystem, given the level of heterogeneity within such an area.[12] Other authors have suggested that an ecosystem can encompass a much larger area, even the whole planet.[8] Schulze and coauthors also rejected the idea that a single rotting log could be studied as an ecosystem because the size of the flows between the log and its surroundings are too large, relative to the proportion cycles within the log.[12] Philosopher of science Mark Sagoff considers the failure to define "the kind of object it studies" to be an obstacle to the development of theory in ecosystem ecology.[4]Ecosystems can be studied in a variety of ways. Those include theoretical studies or more practical studies that monitor specific ecosystems over long periods of time or look at differences between ecosystems to better understand how they work. Some studies involve experimenting with direct manipulation of the ecosystem.[13] Studies can be carried out at a variety of scales, ranging from whole-ecosystem studies to studying microcosms or mesocosms (simplified representations of ecosystems).[14] American ecologist Stephen R. Carpenter has argued that microcosm experiments can be "irrelevant and diversionary" if they are not carried out in conjunction with field studies done at the ecosystem scale. Microcosm experiments often fail to accurately predict ecosystem-level dynamics.[15]The Hubbard Brook Ecosystem Study started in 1963 to study the White Mountains in New Hampshire. It was the first successful attempt to study an entire watershed as an ecosystem. The study used stream chemistry as a means of monitoring ecosystem properties, and developed a detailed biogeochemical model of the ecosystem.[16] Long-term research at the site led to the discovery of acid rain in North America in 1972. Researchers documented the depletion of soil cations (especially calcium) over the next several decades.[17]Terrestrial ecosystems (found on land) and aquatic ecosystems (found in water) are concepts related to ecosystems. Aquatic ecosystems are split into marine ecosystems and freshwater ecosystems.Ecosystems are controlled both by external and internal factors. External factors, also called state factors, control the overall structure of an ecosystem and the way things work within it, but are not themselves influenced by the ecosystem. The most important of these is climate.[10] Climate determines the biome in which the ecosystem is embedded. Rainfall patterns and seasonal temperatures influence photosynthesis and thereby determine the amount of water and energy available to the ecosystem.[10] Parent material determines the nature of the soil in an ecosystem, and influences the supply of mineral nutrients. Topography also controls ecosystem processes by affecting things like microclimate, soil development and the movement of water through a system. For example, ecosystems can be quite different if situated in a small depression on the landscape, versus one present on an adjacent steep hillside.[10]Other external factors that play an important role in ecosystem functioning include time and potential biota. Similarly, the set of organisms that can potentially be present in an area can also significantly affect ecosystems. Ecosystems in similar environments that are located in different parts of the world can end up doing things very differently simply because they have different pools of species present.[10] The introduction of non-native species can cause substantial shifts in ecosystem function.Unlike external factors, internal factors in ecosystems not only control ecosystem processes but are also controlled by them. Consequently, they are often subject to feedback loops.[10] While the resource inputs are generally controlled by external processes like climate and parent material, the availability of these resources within the ecosystem is controlled by internal factors like decomposition, root competition or shading.[10] Other factors like disturbance, succession or the types of species present are also internal factors. Primary production is the production of organic matter from inorganic carbon sources. This mainly occurs through photosynthesis. The energy incorporated through this process supports life on earth, while the carbon makes up much of the organic matter in living and dead biomass, soil carbon and fossil fuels. It also drives the carbon cycle, which influences global climate via the greenhouse effect.Through the process of photosynthesis, plants capture energy from light and use it to combine carbon dioxide and water to produce carbohydrates and oxygen. The photosynthesis carried out by all the plants in an ecosystem is called the gross primary production (GPP).[18] About 48–60% of the GPP is consumed in plant respiration. The remainder, that portion of GPP that is not used up by respiration, is known as the net primary production (NPP).[19] Energy and carbon enter ecosystems through photosynthesis, are incorporated into living tissue, transferred to other organisms that feed on the living and dead plant matter, and eventually released through respiration.[19]The carbon and energy incorporated into plant tissues (net primary production) is either consumed by animals while the plant is alive, or it remains uneaten when the plant tissue dies and becomes detritus. In terrestrial ecosystems, roughly 90% of the net primary production ends up being broken down by decomposers. The remainder is either consumed by animals while still alive and enters the plant-based trophic system, or it is consumed after it has died, and enters the detritus-based trophic system. In aquatic systems, the proportion of plant biomass that gets consumed by herbivores is much higher.[20]In trophic systems photosynthetic organisms are the primary producers. The organisms that consume their tissues are called primary consumers or secondary producers—herbivores. Organisms which feed on microbes (bacteria and fungi) are termed microbivores. Animals that feed on primary consumers—carnivores—are secondary consumers. Each of these constitutes a trophic level.[20] The sequence of consumption—from plant to herbivore, to carnivore—forms a food chain. Real systems are much more complex than this—organisms will generally feed on more than one form of food, and may feed at more than one trophic level. Carnivores may capture some prey which are part of a plant-based trophic system and others that are part of a detritus-based trophic system (a bird that feeds both on herbivorous grasshoppers and earthworms, which consume detritus). Real systems, with all these complexities, form food webs rather than food chains.[20]Ecosystem ecology studies "the flow of energy and materials through organisms and the physical environment". It seeks to understand the processes which govern the stocks of material and energy in ecosystems, and the flow of matter and energy through them. The study of ecosystems can cover 10 orders of magnitude, from the surface layers of rocks to the surface of the planet.[21]The carbon and nutrients in dead organic matter are broken down by a group of processes known as decomposition. This releases nutrients that can then be re-used for plant and microbial production and returns carbon dioxide to the atmosphere (or water) where it can be used for photosynthesis. In the absence of decomposition, the dead organic matter would accumulate in an ecosystem, and nutrients and atmospheric carbon dioxide would be depleted.[22] Approximately 90% of terrestrial net primary production goes directly from plant to decomposer.[20]Decomposition processes can be separated into three categories—leaching, fragmentation and chemical alteration of dead material. As water moves through dead organic matter, it dissolves and carries with it the water-soluble components. These are then taken up by organisms in the soil, react with mineral soil, or are transported beyond the confines of the ecosystem (and are considered lost to it).[22] Newly shed leaves and newly dead animals have high concentrations of water-soluble components and include sugars, amino acids and mineral nutrients. Leaching is more important in wet environments and much less important in dry ones.[22]Fragmentation processes break organic material into smaller pieces, exposing new surfaces for colonization by microbes. Freshly shed leaf litter may be inaccessible due to an outer layer of cuticle or bark, and cell contents are protected by a cell wall. Newly dead animals may be covered by an exoskeleton. Fragmentation processes, which break through these protective layers, accelerate the rate of microbial decomposition.[22] Animals fragment detritus as they hunt for food, as does passage through the gut. Freeze-thaw cycles and cycles of wetting and drying also fragment dead material.[22]The chemical alteration of the dead organic matter is primarily achieved through bacterial and fungal action. Fungal hyphae produce enzymes which can break through the tough outer structures surrounding dead plant material. They also produce enzymes which break down lignin, which allows them access to both cell contents and to the nitrogen in the lignin. Fungi can transfer carbon and nitrogen through their hyphal networks and thus, unlike bacteria, are not dependent solely on locally available resources.[22]Decomposition rates vary among ecosystems.[23] The rate of decomposition is governed by three sets of factors—the physical environment (temperature, moisture, and soil properties), the quantity and quality of the dead material available to decomposers, and the nature of the microbial community itself.[24] Temperature controls the rate of microbial respiration; the higher the temperature, the faster microbial decomposition occurs. It also affects soil moisture, which slows microbial growth and reduces leaching. Freeze-thaw cycles also affect decomposition—freezing temperatures kill soil microorganisms, which allows leaching to play a more important role in moving nutrients around. This can be especially important as the soil thaws in the spring, creating a pulse of nutrients which become available.[24]Decomposition rates are low under very wet or very dry conditions. Decomposition rates are highest in wet, moist conditions with adequate levels of oxygen. Wet soils tend to become deficient in oxygen (this is especially true in wetlands), which slows microbial growth. In dry soils, decomposition slows as well, but bacteria continue to grow (albeit at a slower rate) even after soils become too dry to support plant growth.Ecosystems continually exchange energy and carbon with the wider environment. Mineral nutrients, on the other hand, are mostly cycled back and forth between plants, animals, microbes and the soil. Most nitrogen enters ecosystems through biological nitrogen fixation, is deposited through precipitation, dust, gases or is applied as fertilizer.[25] Since most terrestrial ecosystems are nitrogen-limited, nitrogen cycling is an important control on ecosystem production.[25]Until modern times, nitrogen fixation was the major source of nitrogen for ecosystems. Nitrogen-fixing bacteria either live symbiotically with plants or live freely in the soil. The energetic cost is high for plants which support nitrogen-fixing symbionts—as much as 25% of gross primary production when measured in controlled conditions. Many members of the legume plant family support nitrogen-fixing symbionts. Some cyanobacteria are also capable of nitrogen fixation. These are phototrophs, which carry out photosynthesis. Like other nitrogen-fixing bacteria, they can either be free-living or have symbiotic relationships with plants.[25] Other sources of nitrogen include acid deposition produced through the combustion of fossil fuels, ammonia gas which evaporates from agricultural fields which have had fertilizers applied to them, and dust.[25] Anthropogenic nitrogen inputs account for about 80% of all nitrogen fluxes in ecosystems.[25]When plant tissues are shed or are eaten, the nitrogen in those tissues becomes available to animals and microbes. Microbial decomposition releases nitrogen compounds from dead organic matter in the soil, where plants, fungi, and bacteria compete for it. Some soil bacteria use organic nitrogen-containing compounds as a source of carbon, and release ammonium ions into the soil. This process is known as nitrogen mineralization. Others convert ammonium to nitrite and nitrate ions, a process known as nitrification. Nitric oxide and nitrous oxide are also produced during nitrification.[25] Under nitrogen-rich and oxygen-poor conditions, nitrates and nitrites are converted to nitrogen gas, a process known as denitrification.[25]Other important nutrients include phosphorus, sulfur, calcium, potassium, magnesium and manganese.[26][23] Phosphorus enters ecosystems through weathering. As ecosystems age this supply diminishes, making phosphorus-limitation more common in older landscapes (especially in the tropics).[26] Calcium and sulfur are also produced by weathering, but acid deposition is an important source of sulfur in many ecosystems. Although magnesium and manganese are produced by weathering, exchanges between soil organic matter and living cells account for a significant portion of ecosystem fluxes. Potassium is primarily cycled between living cells and soil organic matter.[26]Biodiversity plays an important role in ecosystem functioning.[28] The reason for this is that ecosystem processes are driven by the number of species in an ecosystem, the exact nature of each individual species, and the relative abundance organisms within these species.[29] Ecosystem processes are broad generalizations that actually take place through the actions of individual organisms. The nature of the organisms—the species, functional groups and trophic levels to which they belong—dictates the sorts of actions these individuals are capable of carrying out and the relative efficiency with which they do so. Ecological theory suggests that in order to coexist, species must have some level of limiting similarity—they must be different from one another in some fundamental way, otherwise one species would competitively exclude the other.[30] Despite this, the cumulative effect of additional species in an ecosystem is not linear—additional species may enhance nitrogen retention, for example, but beyond some level of species richness, additional species may have little additive effect.[29] The addition (or loss) of species which are ecologically similar to those already present in an ecosystem tends to only have a small effect on ecosystem function. Ecologically distinct species, on the other hand, have a much larger effect. Similarly, dominant species have a large effect on ecosystem function, while rare species tend to have a small effect. Keystone species tend to have an effect on ecosystem function that is disproportionate to their abundance in an ecosystem.[29] Similarly, an ecosystem engineer is any organism that creates, significantly modifies, maintains or destroys a habitat.Ecosystems are dynamic entities. They are subject to periodic disturbances and are in the process of recovering from some past disturbance.[11] When a perturbation occurs, an ecoystem responds by moving away from its initial state. The tendency of an ecosystem to remain close to its equilibrium state, despite that disturbance, is termed its resistance. On the other hand, the speed with which it returns to its initial state after disturbance is called its resilience.[11] Time plays a role in the development of soil from bare rock and the recovery of a community from disturbance.[10]From one year to another, ecosystems experience variation in their biotic and abiotic environments. A drought, an especially cold winter and a pest outbreak all constitute short-term variability in environmental conditions. Animal populations vary from year to year, building up during resource-rich periods and crashing as they overshoot their food supply. These changes play out in changes in net primary production decomposition rates, and other ecosystem processes.[11] Longer-term changes also shape ecosystem processes—the forests of eastern North America still show legacies of cultivation which ceased 200 years ago, while methane production in eastern Siberian lakes is controlled by organic matter which accumulated during the Pleistocene.[11]Disturbance also plays an important role in ecological processes. F. Stuart Chapin and coauthors define disturbance as "a relatively discrete event in time and space that alters the structure of populations, communities, and ecosystems and causes changes in resources availability or the physical environment".[31] This can range from tree falls and insect outbreaks to hurricanes and wildfires to volcanic eruptions. Such disturbances can cause large changes in plant, animal and microbe populations, as well soil organic matter content.[11] Disturbance is followed by succession, a "directional change in ecosystem structure and functioning resulting from biotically driven changes in resources supply."[31]The frequency and severity of disturbance determine the way it affects ecosystem function. A major disturbance like a volcanic eruption or glacial advance and retreat leave behind soils that lack plants, animals or organic matter. Ecosystems that experience such disturbances undergo primary succession. A less severe disturbance like forest fires, hurricanes or cultivation result in secondary succession and a faster recovery.[11] More severe disturbance and more frequent disturbance result in longer recovery times. Classifying ecosystems into ecologically homogeneous units is an important step towards effective ecosystem management.[32] There is no single, agreed-upon way to do this. A variety of systems exist, based on vegetation cover, remote sensing, and bioclimatic classification systems.[32] Ecological land classification is a cartographical delineation or regionalisation of distinct ecological areas, identified by their geology, topography, soils, vegetation, climate conditions, living species, habitats, water resources, and sometimes also anthropic factors.[33]Human activities are important in almost all ecosystems. Although humans exist and operate within ecosystems, their cumulative effects are large enough to influence external factors like climate.[10]Ecosystems provide a variety of goods and services upon which people depend.[34] Ecosystem goods include the "tangible, material products" of ecosystem processes such as food, construction material, medicinal plants.[35] They also include less tangible items like tourism and recreation, and genes from wild plants and animals that can be used to improve domestic species.[34] Ecosystem services, on the other hand, are generally "improvements in the condition or location of things of value".[35] These include things like the maintenance of hydrological cycles, cleaning air and water, the maintenance of oxygen in the atmosphere, crop pollination and even things like beauty, inspiration and opportunities for research.[34] While ecosystem goods have traditionally been recognized as being the basis for things of economic value, ecosystem services tend to be taken for granted.[35]When natural resource management is applied to whole ecosystems, rather than single species, it is termed ecosystem management.[36] Although definitions of ecosystem management abound, there is a common set of principles which underlie these definitions.[37] A fundamental principle is the long-term sustainability of the production of goods and services by the ecosystem;[37] "intergenerational sustainability [is] a precondition for management, not an afterthought".[34] While ecosystem management can be used as part of a plan for wilderness conservation, it can also be used in intensively managed ecosystems[34] (see, for example, agroecosystem and close to nature forestry).As human population and per capita consumption grow, so do the resource demands imposed on ecosystems and the effects of the human ecological footprint. Natural resources are vulnerable and limited. The environmental impacts of anthropogenic actions are becoming more apparent. Problems for all ecosystems include: environmental pollution, climate change and biodiversity loss. For terrestrial ecosystems further threats include air pollution, soil degradation,  and deforestation. For aquatic ecosystems threats include also unsustainable exploitation of marine resources (for example overfishing of certain species), marine pollution, microplastics pollution, water pollution, and building on coastal areas.[38] Society is increasingly becoming aware that ecosystem services are not only limited but also that they are threatened by human activities. The need to better consider long-term ecosystem health and its role in enabling human habitation and economic activity is urgent. To help inform decision-makers, many ecosystem services are being assigned economic values, often based on the cost of replacement with anthropogenic alternatives. The ongoing challenge of prescribing economic value to nature, for example through biodiversity banking, is prompting transdisciplinary shifts in how we recognize and manage the environment, social responsibility, business opportunities, and our future as a species.[citation needed]The term "ecosystem" was first used in 1935 in a publication by British ecologist Arthur Tansley.[fn 1][39] Tansley devised the concept to draw attention to the importance of transfers of materials between organisms and their environment.[40] He later refined the term, describing it as "The whole system, ... including not only the organism-complex, but also the whole complex of physical factors forming what we call the environment".[41] Tansley regarded ecosystems not simply as natural units, but as "mental isolates".[41] Tansley later defined the spatial extent of ecosystems using the term ecotope.[42]G. Evelyn Hutchinson, a limnologist who was a contemporary of Tansley's, combined Charles Elton's ideas about trophic ecology with those of Russian geochemist Vladimir Vernadsky. As a result, he suggested that mineral nutrient availability in a lake limited algal production. This would, in turn, limit the abundance of animals that feed on algae. Raymond Lindeman took these ideas further to suggest that the flow of energy through a lake was the primary driver of the ecosystem. Hutchinson's students, brothers Howard T. Odum and Eugene P. Odum, further developed a "systems approach" to the study of ecosystems. This allowed them to study the flow of energy and material through ecological systems.[40]
Musa (genus)
Around 70, see text.Musa is one of two or three genera in the family Musaceae; it includes bananas and plantains. Around 70 species of Musa are known, with a broad variety of uses.Though they grow as high as trees, banana and plantain plants are not woody and their apparent "stem" is made up of the bases of the huge leaf stalks. Thus, they are technically gigantic herbs.Musa species are used as food plants by the larvae of some Lepidoptera species, including the giant leopard moth and other Hypercompe species, including H. albescens (only recorded on Musa), H. eridanus, and H. icasia.The genus Musa was first named by Carl Linnaeus in 1753.[2] The name is a Latinization of the Arabic name for the fruit,  mauz (موز). Mauz meaning Musa is discussed in the 11th-century Arabic encyclopedia The Canon of Medicine, which was translated to Latin in medieval times and well known in Europe.[Note 1] Muz is also the Turkish, Persian, and Somali name for the fruit. Some sources assert that Musa is named for Antonius Musa, physician to the Emperor Augustus.[3] The word "banana" came to English from Spanish and Portuguese, which in turn apparently obtained it from a West African language (possibly Wolof).[4]From the time of Linnaeus until the 1940s, different types of edible bananas and plantains were given Linnaean binomial names, such as Musa cavendishii, as if they were species. In fact, edible bananas have an extremely complicated origin involving hybridization, mutation, and finally selection by humans. Most edible bananas are seedless (parthenocarpic), hence sterile, so they are propagated vegetatively. The giving of species names to what are actually very complex, largely asexual, hybrids (mostly of two species of wild bananas, Musa acuminata and Musa balbisiana) led to endless confusion in banana botany. In the 1940s and 1950s, it became clear to botanists that the cultivated bananas and plantains could not usefully be assigned Linnean binomials, but were better given cultivar names.[citation needed]Ernest Entwistle Cheesman carried out a major revision of the Musaceae in the 1940s.  Following his approach, the genus Musa was divided into five sections: Ingentimusa, Australimusa, Callimusa, Musa, and Rhodochlamys. These were reduced to three in 2002. Previously, the 2n = 20-chromosome species were separated into the sections Australimusa and Callimusa and the 2n = 22-chromosome species were separated into the sections Musa and Rhodochlamys. Studies have shown that genetic differences between each section in the same chromosome group are smaller than those within each section. This means the traditional separation of the sections can no longer be substantiated. Wong's studies do, however, maintain the separation between the 20- and 22-chromosome species. At present, the 2n = 14 chromosome Ingentimusa section also remains distinct.[5]The World Checklist of Selected Plant Families accepts 68 species and two primary hybrids, as of  January 2013[update], which are listed below.[6] The assignment to sections is based on GRIN (where this gives the species),[7] regrouped according to Wong et al.[5][A] and [C] indicate known placement in the former sections Australimusa and Callimusa, respectively.[8]A number of distinct groups of plants bearing edible fruit have been developed from species of Musa. In English, fruits which are sweet and used for dessert are usually called "bananas", whereas starchier varieties used for cooking are called "plantains", but these terms do not have any botanical significance. By far the largest and now the most widely distributed group of cultivated bananas is derived from section Musa, particularly M. acuminata and M. balbisiana, either alone or in various hybrid combinations. The next but much smaller group is derived from members of section Callimusa (previously classified as Australimusa) and is restricted in importance to Polynesia. Of even more restricted importance are small groups of hybrids from Papua New Guinea; a group from section Musa to which Musa schizocarpa has also contributed, and a group of hybrids between section Musa and section Callimusa.[citation needed]When the Linnaean binomial system was abandoned for cultivated bananas, an alternate genome-based system for the nomenclature of edible bananas in section Musa was devised. Thus, the plant previously known by the "species" name Musa cavendishii became Musa (AAA Group) 'Dwarf Cavendish'. The "new" name shows clearly that 'Dwarf Cavendish' is a triploid, with three sets of chromosomes, all derived from Musa acuminata, which is designated by the letter "A". When Musa balbisiana is involved, the letter "B" is used to denote its genome. Thus, the cultivar 'Rajapuri' may be called Musa (AAB Group) 'Rajapuri'. 'Rajapuri' is also a triploid, expected to have two sets of chromosomes from Musa acuminata and one from Musa balbisiana. In the genome of edible bananas from section Musa, combinations such as AA, BB, ABB, BBB and even AAAB can be found.For a more detailed explanation of this system and a list of some edible banana and plantain cultivars using it, see the List of banana cultivars.No such nomenclature system has been developed for the group of edible bananas derived from section Callimusa. This group is known generally as the "Fe'i" or "Fehi" bananas, and numerous cultivars are found in the South Pacific region. They are very distinctive plants with upright fruit bunches, featuring in three of Paul Gauguin's paintings. The flesh can be cooked before eating and is bright orange, with a high level of beta carotene. Fe'i bananas are no longer very important for food, as imported foods have grown in popularity, although some have ritual significance. Investigations are under way to use the Fe'i karat bananas (the name derives from "carrot" due to the intense orange-yellow color of the fruit) in prevention of childhood blindness in Pohnpei.[10] Fe'i bananas probably derive mainly from Musa maclayi, although their origins are not as well understood as the section Musa bananas. Cultivars can be formally named, as e.g. Musa (Fe'i Group) 'Utafun'.
Stobrum
Stobrum is a tree native to Carmania, with scented wood, which was an object of exchange in ancient days in the Roman Empire. [1]The tree was mentioned by Pliny the Elder in his Natural History:The Arabians import from Carmania also the wood of a tree called stobrum, which they employ in fumigations, by steeping it in palm wine, and then setting fire to it. The odour first ascends to the ceiling, and then descends in volumes to the floor; it is very agreeable, but is apt to cause an oppression of the head, though unattended with pain; it is used for promoting sleep in persons when ill.[2] Pliny's editors John Bostock and Henry Thomas Riley note regarding stobrum:Although the savin shrub, the Juniperus sabina of Carl Linnaeus, bears this name in Greek, it is evident, as Fée says, that Pliny does not allude to it, but to a coniferous tree, as it is that family which produces a resinous wood with a balsamic odour when ignited. Bauhin and others would make the tree meant to be the Thuja occidentalis of Carl Linnaeus; but, as Fée observes, that tree is in reality a native originally of Canada, while the Thuja orientalis is a native of Japan. He suggests, however, that the "Thuja articulata" of Mount Atlas (Tetraclinis articulata) may have possibly been the citrus of Pliny.[3]Bostock and Riley place the tree in the Thuja genus.  Of note in contradiction of Bostock and Riley, the land of the Sabians is echoed in the name of Juniperus sabina. Stobrum is also noted in early Indian economic history: R.N. Saletore notes it, again on Pliny's reference:The Arabs imported the fragrant wood of the bratus tree from the country of the Elymaei, the stobrum from Carmania, cinnamon from Ethiopia and also cassia from the same country.[4]
Sacred grove
A sacred grove or sacred woods are any grove of trees that are of special religious importance to a particular culture. Sacred groves feature in various cultures throughout the world. They were important features of the mythological landscape and cult practice of Celtic, Baltic, Germanic, ancient Greek, Near Eastern, Roman, and Slavic polytheism, and were also used in India, Japan, and West Africa. Examples of sacred groves include the Greco-Roman temenos, the Norse hörgr, and the Celtic nemeton, which was largely but not exclusively associated with Druidic practice. During the Northern Crusades, there was a common practice of building churches on the sites of sacred groves. The Lakota and various other North American tribes consider particular forests or other natural landmarks to be sacred.Ancient holy trees remain in the English and Estonian countryside and are mentioned often in folklore and fairytales.There are two mentions on this tradition in the Bible:Abraham planted a grove in Beersheba, and called there the name of God.andwhere the women wove hangings for the grove.Excavations at Labraunda have revealed a large shrine assumed to be that of Zeus Stratios mentioned by Herodotus[1] as a large sacred grove of plane trees sacred to Carians. In Syria, there was a grove sacred to Adonis at Afqa.The most famous sacred groves in mainland Greece was the oak grove at Dodona. Outside the walls of Athens, the site of the Platonic Academy was a sacred grove of olive trees, still recalled in the phrase "the groves of Academe".In central Italy, the town of Nemi recalls the Latin nemus Aricinum, or "grove of Ariccia", a small town a quarter of the way around the lake. In Antiquity the area had no town, but the grove was the site of one of the most famous of Roman cults and temples: that of Diana Nemorensis, a study of which served as the seed for Sir James Frazer's seminal work on the anthropology of religion, The Golden Bough.[2]A sacred grove behind the House of the Vestal Virgins on the edge of the Roman Forum lingered until its last vestiges were burnt in the Great Fire of Rome in 64 CE.In the town of Spoleto, Umbria, two stones from the late third century BCE, inscribed in archaic Latin, established punishments for the profanation of the woods dedicated to Jupiter (Lex Luci Spoletina) have survived; they are preserved in the National Archeological Museum of Spoleto.[3]The Bosco Sacro (literally sacred grove) in the garden of Bomarzo, Italy, lends its associations to the uncanny atmosphere.Lucus Pisaurensis,[4] the Sacred Grove of Pesaro, Italy was discovered by Patrician Annibale degli Abati Olivieri in 1737 on property he owned along the 'Forbidden Road' (Collina di Calibano),[5] just outside Pesaro.   This Sacred Grove is the site of the Votive Stones of Pesaro and was dedicated to Salus, the ancient Roman demi-goddess of well-being.[6]The city of Massilia, a Greek colony, had a sacred grove so close by it that Julius Caesar had it cut down to facilitate his siege.[7]  In Pharsalia, the poet Lucan dramatized it as a place where sunlight could not reach through the branches, where no animal or bird lived, where the wind did not blow, but branches moved on their own, where human sacrifice was practiced, in a clear attempt to dramatize the situation and distract from the sacrilege entailed in its destruction.[8]Sacred groves have survived in the Baltic states longer than in other parts of Europe. The main Baltic Prussian sanctuary, which is also considered a sacred grove was Romowe. An important wave of destruction of sacred groves was carried out in the lands of present-day Lithuania after its Christianization in 1387, and in Samogitia in 1413. However, some groves, such as in Šventybrastis, still survive in Lithuania. A sacred grove is known as alka(s) in Lithuanian and svētbirz(i)s in Latvian. Conversely, in Estonia numerous sacred groves (hiis) have survived to the present day and have recently been protected by the government of the country.Sacred groves feature prominently in Scandinavia. The most famous sacred grove of Northern Europe was at the Temple at Uppsala in Old Uppsala, where every tree was considered sacred – described by Adam of Bremen. The practice of blót – the sacrificial ritual in Norse paganism was usually held in lunds or sacred groves. According to Adam of Bremen, in Scandinavia, pagan kings sacrificed nine males of each species at the sacred groves every ninth year.[9]The Celts used sacred groves, called nemeton in Gaulish, for performing rituals, based on Celtic mythology. The deity involved was usually Nemetona – a Celtic goddess. Druids oversaw such rituals. Existence of such groves have been found in Germany, Switzerland, Czech Republic and Hungary in Central Europe,  in many sites of ancient Gaul in France, as well as England and Northern Ireland. Sacred groves had been plentiful up until the 1st century BC, when the Romans attacked and conquered Gaul. One of the best known nemeton sites is that in the Nevet forest near Locronan in Brittany, France. Gournay-sur-Aronde (Gournay-on-Aronde), a village in the Oise department of France, also houses the remains of a nemeton.[10][11]Nemetons were often fenced off by enclosures, as indicated by the German term Viereckschanze – meaning a quadrangular space surrounded by a ditch enclosed by wooden palisades.Many of these groves, like the sacred grove at Didyma, Turkey are thought to be nemetons, sacred groves protected by druids based on Celtic mythology. In fact, according to Strabo, the central shrine at Galatia was called Drunemeton.[12] Some of these were also sacred groves in Greek times (as in the case of Didyma), but were based on a different or slightly changed mythology.Based on historical data, it is estimated that there are around 2500 sacred natural sites in Estonia, the largest of them covering up to 100 hectares. Although rather exceptional among most of the technologically developed countries, in Estonia both the sacred natural sites and indigenous customs connected to them are still in use. Therefore, the heritage that is connected to sacred natural sites has great importance to the national identity and environment of Estonians.In collaboration between followers of Estonian native religion (Maausk) and governmental ministries, a national plan was prepared in 2008: "Sacred Natural Sites in Estonia: Study and Conservation 2008–2012" which includes about 550 hiis (sacred groves). The National Plan on Sacred Natural Sites consists of a historical overview about sacred natural sites in Estonia, a current situation analysis, and several concrete conservation measures and instructions on how to apply them. The coordinating steering committee of the Conservation Plan consist of Environment, Agriculture, Internal Affairs and Education and Research ministries, National Heritage Board and MK. The University of Tartu is the implementing agency. Measures of the Conservation Plan are designed to handle natural sanctuaries and values connected to them in all aspects.The Conservation Plan foresees creating a database which supports researching and managing natural sanctuaries. The database would consist of folkloric, archaeological, natural, historical and other data on sacred natural sites and provide information on the exact location, condition and form of ownership of each site.[13]Sacred groves are also present in Ghana. One of Ghana's most famous sacred groves – the Buoyem Sacred Grove – and numerous other sacred groves are present in the Techiman Municipal District and nearby districts of the Brong Ahafo Region. They provide a refuge for wildlife which has been exterminated in nearby areas, and one grove most notably houses 20,000 fruit bats in underground caves.[14] The capital of the historical Ghana Empire, contained a sacred grove called al-gâba (Ar. "the forest") for performing religious rites of the Soninke people. Other sacred groves in Ghana include sacred groves along the coastal savannahs of Ghana.[15] Many sacred groves in Ghana are now under federal protection – like the Anweam Sacred Grove in the Esukawkaw Forest Reserve[16] Other well-known sacred groves in present-day Ghana include the Malshegu Sacred Grove in Northern Ghana – one of the last remaining closed canopy forests in the savannah regions,[17] and the Jachie sacred grove.In India, sacred groves are scattered all over the country, and do enjoy protection. Prior to 2002 these forest regions were not recognized under any of the existing laws. But in 2002 an amendment was brought in Wildlife Protection Act, 1972 to include Sacred Groves under the act. Some NGOs work with local villagers to protect such groves. Each grove is associated with a presiding deity, and the groves are referred to by different names in different parts of India. They were maintained by local communities with hunting and logging strictly prohibited within these patches. While most of these sacred deities are associated with local Hindu gods, sacred groves of Islamic and Buddhist origins are also known. Sacred groves occur in a variety of places – from scrub forests in the Thar Desert of Rajasthan maintained by the Bishnois, to rain forests in the Kerala Western Ghats. Himachal Pradesh in the North and Kerala in the South are specifically known for their large numbers of sacred groves. The Kodavas of Karnataka maintained over 1000 sacred groves in Kodagu alone.[18]Around 14,000 sacred groves have been reported from all over India, which act as reservoirs of rare fauna, and more often rare flora, amid rural and even urban settings. Experts believe that the total number of sacred groves could be as high as 100,000. Threats to the groves include urbanization, and over-exploitation of resources. While many of the groves are looked upon as abode of Hindu gods, in the recent past a number of them have been partially cleared for construction of shrines and temples.[19][20]Ritualistic dances and dramatizations based on the local deities that protect the groves are called Theyyam in Kerala and Nagmandalam, among other names, in Karnataka. There are sacred groves in Ernakulam region in a place named Mangatoor in Kerala. Sacred groves are being destroyed as a part of urbanization. The family "Nalukettil Puthenpurayil" still protects sacred groves.Sacred groves in Japan are typically associated with Shinto shrines, and are located all over Japan. They have existed since ancient times and shrines are often built in the midst of preexisting groves. The Cryptomeria tree is venerated in Shinto practice, and considered sacred.Among the sacred groves associated with such jinjas or Shinto shrines is the 20-hectare wooded area associated with Atsuta Shrine (熱田神宮, Atsuta-jingū) at Atsuta-ku, Nagoya.  The 1500-hectare forest associated with Kashima Shrine was declared a "protected area" in 1953.[21]  Today it is part of the Kashima Wildlife Preservation Area.  The woods include over 800 kinds of trees and varied animal and plant life.[22]Tadasu no Mori (糺の森) is a general term for a wooded area associated with the Kamo Shrine, which is a Shinto sanctuary near the banks of the Kamo River in northeast Kyoto.[23]  The ambit of today's forest encompasses approximately 12.4 hectares, which are preserved as a national historical site (国の史跡).[24]  The Kamigamo Shrine and the Shimogamo Shrine, along with other Historic Monuments of Ancient Kyoto (Kyoto, Uji and Otsu Cities), have been designated World Heritage Sites since 1994.The Utaki sacred sites (often with associated burial grounds) on Okinawa are based on Ryukyuan religion, and usually are associated with toun or kami-asagi – regions dedicated to the gods where people are forbidden to go. Sacred groves are often present in such places, as also in Gusukus – fortified areas which contain sacred sites within them.[25] The Seifa-utaki was designated as a UNESCO World Heritage Site designated in 2003.[26] It consists of a triangular cavern formed by gigantic rocks, and contains a sacred grove with rare, indigenous trees like the Kubanoki (a kind of palm) and the yabunikkei or Cinnamomum japonicum (a form of wild cinnamon). Direct access to the grove is forbidden.Much of the ways of the ancient inhabitants of Malaysia have largely been forgotten, mostly due to the taboos among the local populace on putting certain esoteric knowledge down in ink, thus only passed down through examples and word of mouth from mother to daughter and father to son. However, much can be observed by the ways and habits of the natives of Malaysia which include 18 tribes of Orang Asli (Malay for Natural People) and the Malays, who are often regarded as the 19th tribe.There is a practice of tree planting around houses to the extent that the walls and wooden structures are allowed to give way to the roots of creeping plants, purposely sown at the bases of these structures. With increased migration towards the larger cities, these houses are abandoned and allowed to return to nature. As most traditional Orang Asli and Malay houses are made of only wood, bamboo, rattan and woven palm leaves (being built without using a single nail), the remains of those houses crumble easily into its surrounding.Besides that, a practice of creating arches of vine and creeping flowering plants so that each time one were to enter the gates of the house, one has to bow, as if implying or imitating respect upon entry to a sacred grove which were practiced by their ancestors. Such practices are even performed by those who have migrated into the cities who prefer to live in houses on the ground, rather than in high rise apartments. A garden of fruit trees surrounded by larger trees are planted around the houses to provide shade and an illusion of being at 'home' as well as to provide sustenance (in the form of fruits and seeds) to squirrels, foxes, insects and birds. Commonly, a cat, or in most instances, many cats are kept to patrol the gardens and guard from harmful spirits as well as against rats which were believed to carry unclean spirits and diseases.However, one of the most striking example of the tree reverence among them can be seen in the graveyards which are considered as holy ground, on which no stone structure can be built upon. The whole area are covered by large and tall trees, so much foliage that the scorching tropical sun is reduced to a dim shadow as temperatures drop to a comfortable cool. Malay folklore relates that the trees whisper prayers to the creator in absolution of the past transgression of the ground's once human inhabitants. The trees are also allowed to take root into the graves where the grave keepers (penjaga kubur in Malay) slowly remove gravestones (which used to be made from wood) as they are ejected from the grounds onto the surface. There is also a ritual of planting small tree sapling on fresh graves by family members who will then water it and tend to it periodically. Petals from fresh red and pink roses are also brought upon visitation to be scattered on the graves and a ritual of pouring rose water upon the soils are also performed.The Malays regard visiting the graves from between sunset to sunrise as a taboo as it is believed that as sunrise is the beginning of day to mankind, sunset is perceived as the beginning of day to those who dwell in the grave area. Burials are almost always postponed until the next day except in certain cases where it is allowed, provided that additional rules are observed, such as, women and children are not allowed at the night time burial ceremony.An ancient ritual of renaming the deceased as she or he is laid into the earth is also practiced. The Orang Asli and Malay (see Malaysian names) naming system has a living name and a spirit name, which is given during the ritual of burial. This name is known as nama arwah (spirit name). The living name is usually the given name plus the word 'anak' which means 'son/daughter of' or 'bin' and 'binti' which mean 'son of' or 'daughter of' respectively; followed by the name of the father. When a person dies, the father's name is replaced with his or her mother's name and this is made known during the reading of burial sentences.Granted World Heritage status by UNESCO in 1997,[27][28] Lumbini Grove is a Buddhist pilgrimage site in the Rupandehi District of Nepal. It is the place where, according to Buddhist tradition, Queen Mayadevi gave birth to Siddhartha Gautama in 623 BCE.[27][28] Gautama, who achieved nirvana some time around 543 BCE,[29][30] became the Lord Gautama Buddha and founded Buddhism after achieving Enlightenment.[31][32][33] The Mayadevi Temple is located at Lumbini.The concept of sacred groves is present in Nigerian mythology as well. The Osun-Osogbo Sacred Grove, containing dense forests, is located just outside the city of Osogbo, and is regarded as one of the last virgin high forests in Nigeria. It is dedicated to the fertility goddess in Yoruba mythology, and is dotted with shrines and sculptures. Oloye Susanne Wenger, an Austrian artist, helped revive the grove. The grove was declared a UNESCO World Heritage Site in 2005.[34]Sacred groves, mostly connected to Thai folk belief, are known to have existed in Thailand since medieval times. Recently, new areas are being marked off as sacred as an environmental movement.The Lakota and various other North American tribes consider particular forests or other natural landmarks to be sacred. This is one of the reasons that there has been recent dispute over the nullification of acknowledgment of Native American reservation land by the US government and an attempt to compensate Native Americans for the reacquisition of this sacred space.[35][better source needed]The Bohemian Grove, located at 20601 Bohemian Avenue, in Monte Rio, California, is a sacred grove belonging to a private San Francisco-based men's art club known as the Bohemian Club. In mid-July each year, Bohemian Grove hosts a two-week, three-weekend encampment of some of the most powerful men in the world, where they perform symbolic rituals, such as Cremation of Care.
Gynoecium
Gynoecium (/ɡaɪˈniːsɪəm/, from Ancient Greek γυνή, gyne, meaning woman, and οἶκος, oikos, meaning house) is most commonly used as a collective term for the parts of a flower that produce ovules and ultimately develop into the fruit and seeds. The gynoecium is the innermost whorl of a flower, it consists of (one or more) pistils and is typically surrounded by the pollen-producing reproductive organs, the stamens, collectively called the androecium. The gynoecium is often referred to as the "female" portion of the flower, although rather than directly producing female gametes (i.e. egg cells), the gynoecium produces megaspores, each of which develops into a female gametophyte which then produces egg cells.The term gynoecium is also used by botanists to refer to a cluster of archegonia and any associated modified leaves or stems present on a gametophyte shoot in mosses, liverworts and hornworts. The corresponding terms for the male parts of those plants are clusters of antheridia within the androecium.Flowers that bear a gynoecium but no stamens are called pistillate or carpellate. Flowers lacking a gynoecium are called staminate.The gynoecium is often referred to as female because it gives rise to female (egg-producing) gametophytes, however, strictly speaking sporophytes do not have a sex, only gametophytes do.[1]Gynoecium development and arrangement is important in systematic research and identification of angiosperms, but can be the most challenging of the floral parts to interpret.[2]The gynoecium may consist of one or more separate pistils. A pistil typically consists of an expanded basal portion called the ovary, an elongated section called a style and an apical structure that receives pollen called a stigma.The word "pistil" comes from Latin pistillum meaning pestle. A sterile pistil in a male flower is referred to as a pistillode.The pistils of a flower are considered to be composed of carpels.[note 1] A carpel is the female reproductive part of the flower, interpreted as modified leaves bearing structures called ovules, inside which the egg cells ultimately form. A pistil may consist of one carpel, with its ovary, style and stigma, or several carpels may be joined together with a single ovary, the whole unit called a pistil. The gynoecium may consist of one or more uni-carpellate (with one carpel) pistils, or of one multi-carpellate pistil. The number of carpels is described by terms such as tricarpellate (three carpels).Carpels are thought to be phylogenetically derived from ovule-bearing leaves or leaf homologues (megasporophylls), which evolved to form a closed structure containing the ovules. This structure is typically rolled and fused along the margin.Although many flowers satisfy the above definition of a carpel, there are also flowers that do not have carpels according to this definition because in these flowers the ovule(s), although enclosed, are borne directly on the shoot apex, and only later become enclosed by the carpel.[5][10] Different remedies have been suggested for this problem. An easy remedy that applies to most cases is to redefine the carpel as an appendage that encloses ovule(s) and may or may not bear them.[6][7][11]If a gynoecium has a single carpel, it is called monocarpous. If a gynoecium has multiple, distinct (free, unfused) carpels, it is apocarpous. If a gynoecium has multiple carpels "fused" into a single structure, it is syncarpous. A syncarpous gynoecium can sometimes appear very much like a monocarpous gynoecium.("fused") carpelsThe degree of connation ("fusion") in a syncarpous gynoecium can vary. The carpels may be "fused" only at their bases, but retain separate styles and stigmas. The carpels may be "fused" entirely, except for retaining separate stigmas. Sometimes (e.g., Apocynaceae) carpels are fused by their styles or stigmas but possess distinct ovaries. In a syncarpous gynoecium, the "fused" ovaries of the constituent carpels may be referred to collectively as a single compound ovary. It can be a challenge to determine how many carpels fused to form a syncarpous gynoecium. If the styles and stigmas are distinct, they can usually be counted to determine the number of carpels. Within the compound ovary, the carpels may have distinct locules divided by walls called septa. If a syncarpous gynoecium has a single style and stigma and a single locule in the ovary, it may be necessary to examine how the ovules are attached. Each carpel will usually have a distinct line of placentation where the ovules are attached.Pistils begin as small primordia on a floral apical meristem, forming later than, and closer to the (floral) apex than sepal, petal and stamen primordia. Morphological and molecular studies of pistil ontogeny reveal that carpels are most likely homologous to leaves.[citation needed]A carpel has a similar function to a megasporophyll, but typically includes a stigma, and is fused, with ovules enclosed in the enlarged lower portion, the ovary.[12]In some basal angiosperm lineages, Degeneriaceae and Winteraceae, a carpel begins as a shallow cup where the ovules develop with laminar placentation, on the upper surface of the carpel. The carpel eventually forms a folded, leaf-like structure, not fully sealed at its margins. No style exists, but a broad stigmatic crest along the margin allows pollen tubes access along the surface and between hairs at the margins.[12]Two kinds of fusion have been distinguished: postgenital fusion that can be observed during the development of flowers, and congenital fusion that cannot be observed i.e., fusions that occurred during phylogeny. But it is very difficult to distinguish fusion and non-fusion processes in the evolution of flowering plants. Some processes that have been considered congenital (phylogenetic) fusions appear to be non-fusion processes such as, for example, the de novo formation of intercalary growth in a ring zone at or below the base of primordia.[13][14][11] Therefore, "it is now increasingly acknowledged that the term 'fusion,' as applied to phylogeny (as in 'congenital fusion') is ill-advised."[15]Basal angiosperm groups tend to have carpels arranged spirally around a conical or dome-shaped receptacle. In later lineages, carpels tend to be in whorls.The relationship of the other flower parts to the gynoecium can be an important systematic and taxonomic character. In some flowers, the stamens, petals, and sepals are often said to be "fused" into a "floral tube" or hypanthium. However, as Leins & Erbar (2010) pointed out, "the classical view that the wall of the inferior ovary results from the "congenital" fusion of dorsal carpel flanks and the floral axis does not correspond to the ontogenetic processes that can actually be observed. All that can be seen is an intercalary growth in a broad circular zone that changes the shape of the floral axis (receptacle)."[11] And what happened during evolution is not a phylogenetic fusion but the formation of a unitary intercalary meristem. Evolutionary developmental biology investigates such developmental processes that arise or change during evolution.If the hypanthium is absent, the flower is hypogynous, and the stamens, petals, and sepals are all attached to the receptacle below the gynoecium. Hypogynous flowers are often referred to as having a superior ovary. This is the typical arrangement in most flowers.If the hypanthium is present up to the base of the style(s), the flower is epigynous. In an epigynous flower, the stamens, petals, and sepals are attached to the hypanthium at the top of the ovary or, occasionally, the hypanthium may extend beyond the top of the ovary. Epigynous flowers are often referred to as having an inferior ovary. Plant families with epigynous flowers include orchids, asters, and evening primroses.Between these two extremes are perigynous flowers, in which a hypanthium is present, but is either free from the gynoecium (in which case it may appear to be a cup or tube surrounding the gynoecium) or connected partly to the gynoecium (with the stamens, petals, and sepals attached to the hypanthium part of the way up the ovary). Perigynous flowers are often referred to as having a half-inferior ovary (or, sometimes, partially inferior or half-superior). This arrangement is particularly frequent in the rose family and saxifrages.Occasionally, the gynoecium is born on a stalk, called the gynophore, as in Isomeris arborea.Within the ovary, each ovule is born by a placenta or arises as a continuation of the floral apex. The placentas often occur in distinct lines called lines of placentation. In monocarpous or apocarpous gynoecia, there is typically a single line of placentation in each ovary. In syncarpous gynoecia, the lines of placentation can be regularly spaced along the wall of the ovary (parietal placentation), or near the center of the ovary. In the latter case, separate terms are used depending on whether or not the ovary is divided into separate locules. If the ovary is divided, with the ovules born on a line of placentation at the inner angle of each locule, this is axile placentation. An ovary with free central placentation, on the other hand, consists of a single compartment without septae and the ovules are attached to a central column that arises directly from the floral apex (axis). In some cases a single ovule is attached to the bottom or top of the locule (basal or apical placentation, respectively).In flowering plants, the ovule (from Latin ovulum meaning small egg) is a complex structure born inside ovaries. The ovule initially consists of a stalked, integumented megasporangium (also called the nucellus). Typically, one cell in the megasporangium undergoes meiosis resulting in one to four megaspores. These develop into a megagametophyte (often called the embryo sac) within the ovule. The megagametophyte typically develops a small number of cells, including two special cells, an egg cell and a binucleate central cell, which are the gametes involved in double fertilization. The central cell, once fertilized by a sperm cell from the pollen becomes the first cell of the endosperm, and the egg cell once fertilized become the zygote that develops into the embryo. The gap in the integuments through which the pollen tube enters to deliver sperm to the egg is called the micropyle. The stalk attaching the ovule to the placenta is called the funiculus.Stigmas can vary from long and slender to globe-shaped to feathery. The stigma is the receptive tip of the carpel(s), which receives pollen at pollination and on which the pollen grain germinates. The stigma is adapted to catch and trap pollen, either by combining pollen of visiting insects or by various hairs, flaps, or sculpturings.[16]The style and stigma of the flower are involved in most types of self incompatibility reactions. Self-incompatibility, if present, prevents fertilization by pollen from the same plant or from genetically similar plants, and ensures outcrossing.
Silviculture
Silviculture is the practice of controlling the establishment, growth, composition, health, and quality of forests to meet diverse needs and values.The name comes from the Latin silvi- (forest) + culture (as in growing). The study of forests and woods is termed silvology. Silviculture also focuses on making sure that the treatment(s) of forest stands are used to preserve and to better their productivity.[1]Generally, silviculture is the science and art of growing and tending forest crops, based on a knowledge of silvics, i.e., the study of the life history and general characteristics of forest trees and stands, with particular reference to locality factors.[2] More particularly, silviculture is the theory and practice of controlling the establishment, composition, constitution, and growth of forests. No matter how forestry as a science is constituted, the kernel of the business of forestry has historically been silviculture, as it includes direct action in the forest, and in it all economic objectives and technical considerations ultimately converge.[3] The focus of silviculture is regeneration, but more recently, recreational use of forestland has challenged silviculture as the primary income generation from forests, due to increasing recognizance of forestland's use for leisure and recreation.[4]Some of the distinction between forestry and silviculture is that silviculture is applied at the stand level and forestry is broader. For example, John D. Matthews says "complete regimes for regenerating, tending, and harvesting forests" are called "silvicultural systems".[5]Adaptive management is common in silviculture, where forestry can include natural, conserved land without a stand level management and treatment being applied. A common taxonomy divides silviculture into regenerating, tending, and harvesting techniques.The origin of forestry in German-speaking Europe has defined silvicultural systems broadly as high forest (Hochwald), coppice with standards (Mittelwald) and compound coppice, short rotation coppice, and coppice (Niederwald). There are other systems as well. These varied silvicultural systems include several harvesting methods, which are often wrongly said to be a silvicultural systems, but may also be called rejuvenating or regenerating method depending on the purpose.The high forest system is further subdivided in German:[6] These names give the impression is that these are neatly defined systems, but in practice there are variations within these harvesting methods in accordance with to local ecology and site conditions. While location of an archetypal form of harvesting technique can be identified (they all originated somewhere with a particular forester, and have been described in the scientific literature), and broad generalizations can be made, these are merely rules of thumb rather than strict blueprints on how techniques might be applied. This misunderstanding has meant that many older English textbooks did not capture the true complexity of silviculture as practiced where it originated in Mitteleuropa.This silviculture was culturally predicated on wood production in temperate and boreal climates and did not deal with tropical forestry. The misapplication of this philosophy to those tropical forests has been problematic. There is also an alternative silvicultural tradition which developed in Japan and thus created a different biocultural landscape called satoyama.After harvesting comes regeneration, which may be split into natural and artificial (see below), and tending, which includes release treatments, pruning, thinning and intermediate treatments.[7] It is conceivable that any of these 3 phases (harvesting, regeneration, and tending) may happen at the same time within a stand, depending on the goal for that particular stand.Regeneration is basic to the continuation of forested, as well as to the afforestation of treeless land. Regeneration can take place through self-sown seed ("natural regeneration"), by artificially sown seed, or by planted seedlings. In whichever case, the performance of regeneration depends on its growth potential and the degree to which its environment allows the potential to be expressed.[8] Seed, of course, is needed for all regeneration modes, both for natural or artificial sowing and for raising planting stock in a nursery.Natural regeneration is a "human-assisted natural regeneration" means of establishing a forest age class from natural seeding or sprouting in an area after harvesting in that area through selection cutting, shelter (or seed-tree) harvest, soil preparation, or restricting the size of a clear-cut stand to secure natural regeneration from the surrounding trees.The process of natural regeneration involves the renewal of forests by means of self-sown seeds, root suckers, or coppicing. In natural forests, conifers rely almost entirely on regeneration through seed. Most of the broadleaves, however, are able to regenerate by the means of emergence of shoots from stumps (coppice) and broken stems.[9][full citation needed]Any seed, self-sown or artificially applied, requires a seedbed suitable for securing germination.In order to germinate, a seed requires suitable conditions of temperature, moisture, and aeration. For seeds of many species, light is also necessary, and facilitates the germination of seeds in other species,[10] but spruces are not exacting in their light requirements, and will germinate without light. White spruce seed germinated at 35 °F (1.7 °C) and 40 °F (4.4 °C) after continuous stratification for one year or longer and developed radicles less than 6 cm (2.4 in) long in the cold room.[11] When exposed to light, those germinants developed chlorophyll and were normally phototropic with continued elongation.For survival in the short and medium terms, a germinant needs: a continuing supply of moisture; freedom from lethal temperature; enough light to generate sufficient photosynthate to support respiration and growth, but not enough to generate lethal stress in the seedling; freedom from browsers, tramplers, and pathogens; and a stable root system. Shade is very important to the survival of young seedlings.[12][13] In the longer term, there must be an adequate supply of essential nutrients and an absence of smothering.In undisturbed forest, decayed windfallen stemwood provides the most favorable seedbed for germination and survival, moisture supply being dependable, and the elevation of seedlings somewhat above the general level of the forest floor reduces the danger of smothering by leaves and snow-pressed minor vegetation; nor is such a microsite likely to be subject to flooding. Advantages conferred by those microsites include: more light, higher temperatures in the rooting zone, and better mycorrhizal development.[14][15][16] In a survey in the Porcupine Hills, Manitoba, 90% of all spruce seedlings were rooted in rotten wood.[16][17]Mineral soil seedbeds are more receptive than the undisturbed forest floor,[18] and are generally moister and more readily rewetted than the organic forest floor. However, exposed mineral soil, much more so than organic-surfaced soil, is subject to frost heaving and shrinkage during drought. The forces generated in soil by frost or drought are quite enough to break roots.[19]The range of microsites occurring on the forest floor can be broadened, and their frequency and distribution influenced by site preparation. Each microsite has its own microclimate. Microclimates near the ground are better characterized by vapour pressure deficit and net incident radiation, rather than the standard measurements of air temperature, precipitation, and wind pattern.[13]Aspect is an important component of microclimate, especially in relation to temperature and moisture regimes. Germination and seedling establishment of Engelmann spruce were much better on north than on south aspect seedbeds in the Fraser Experimental Forest, Colorado; the ratios of seeds to 5-year-old seedlings were determined as 32:1, 76:1, and 72:1 on north aspect bladed-shaded, bladed-unshaded, and undisturbed-shaded seedbeds, respectively.[20] Clearcut openings of 1.2 to 2.0 hectares (3.0 to 4.9 acres) adjacent to an adequate seed source, and not more than 6 tree-heights wide, could be expected to secure acceptable regeneration (4,900, 5-year-old trees per hectare), whereas on undisturbed-unshaded north aspects, and on all seedbed treatments tested on south aspects, seed to seedling ratios were so high that the restocking of any clearcut opening would be questionable.At least seven variable factors may influence seed germination: seed characteristics, light, oxygen, soil reaction (pH), temperature, moisture, and seed enemies.[21] Moisture and temperature are the most influential, and both are affected by exposure. The difficulty of securing natural regeneration of Norway spruce and Scots pine in northern Europe led to the adoption of various forms of reproduction cuttings that provided partial shade or protection to seedlings from hot sun and wind.[22] The main objective of echeloned strips or border-cuttings with northeast exposure was to protect regeneration from overheating, and was originated in Germany and deployed successfully by A. Alarik in 1925 and others in Sweden.[23] On south and west exposures, direct insolation and heat reflected from tree trunks often result in temperatures lethal to young seedlings,[24] as well as desiccation of the surface soil, which inhibits germination. The sun is less injurious on eastern exposures because of the lower temperature in the early morning, related to higher humidity and presence of dew.In 1993, Henry Baldwin, after noting that summer temperatures in North America are often higher than those in places where border-cuttings have been found useful, reported the results of a survey of regeneration in a stand of red spruce plus scattered white spruce that had been isolated by clearcutting on all sides, so furnishing an opportunity for observing regeneration on different exposures in this old-field stand at Dummer, New Hampshire.[22] The regeneration included a surprisingly large number of balsam fir seedlings from the 5% stand component of that species. The maximum density of spruce regeneration, determined 4 rods (20 m) inside from the edge of the stand on a north 20°E exposure, was 600,000/ha, with almost 100,000 balsam fir seedlings.A prepared seedbed remains receptive for a relatively short period, seldom as long as 5 years, sometimes as short as 3 years. Seedbed receptivity on moist, fertile sites decreases with particular rapidity, and especially on such sites, seedbed preparation should be scheduled to take advantage of good seed years. In poor seed years, site preparation can be carried out on mesic and drier sites with more chance of success, because of the generally longer receptivity of seedbeds there than those on moister sites.[25] Although an indifferent seed year can suffice if seed distribution is good and environmental conditions favourable to seedling germination and survival,[26] small amounts of seed are particularly vulnerable to depredation by small mammals.[27] Considerable flexibility is possible in timing site preparation to coincide with cone crops. Treatment can be applied either before any logging takes place, between partial cuts, or after logging.[28] In cut and leave strips, seedbed preparation can be carried out as a single operation, pre-scarifying the leave strips, post-scarifying the cut strips.[28]Broadcast burning is not recommended as a method of preparing sites for natural regeneration, as it rarely exposes enough mineral soil to be sufficiently receptive, and the charred organic surfaces are a poor seedbed for spruce.[29][30][31][32] A charred surface may get too hot for good germination and may delay germination until fall, with subsequent overwinter mortality of unhardened seedlings.[33] Piling and burning of logging slash, however, can leave suitable exposures of mineral soil.[28]With a view to reducing the time needed to produce planting stock, experiments were carried out with white spruce and three other coniferous species from Wisconsin seed in the longer, frost-free growing season in Florida, 125 vs. 265 days in central Wisconsin and northern Florida, respectively.[34] As the species studied are adapted to long photoperiods, extended daylengths of 20 hours were applied in Florida. Other seedlings were grown under extended daylength in Wisconsin and with natural daylength in both areas. After two growing seasons, white spruce under long days in Florida were about the same as those in Wisconsin, but twice as tall as plants under natural Wisconsin photoperiods. Under natural days in Florida, with the short local photoperiod, white spruce was severely dwarfed and had a low rate of survival. Black spruce responded similarly. After two growing seasons, long day plants of all 4 species in Florida were well balanced, with good development of both roots and shoots, equaling or exceeding the minimum standards for 2+1 and 2+2 outplanting stock of Lake States species. Their survival when lifted in February and outplanted in Wisconsin equalled that of 2+2 Wisconsin-grown transplants. Artificial extension of the photoperiod in the northern Lake States greatly increased height increment of white and black spruces in the second growing season.Optimum conditions for seedling growth have been determined for the production of containerized planting stock.[35] Alternating day/night temperatures have been found more suitable than a constant temperature; at 400 lumens/m² light regime, a 28 °C/20 °C day/night temperatures have been recommended for white spruce.[35][36] However, temperature optima are not necessarily the same at different ages and sizes.[35] In 1984, R. Tinus investigated the effects of combinations of day and night temperature on height, caliper, and dry weight of 4 seed sources of Engelmann spruce. The 4 seed sources appeared to have very similar temperature requirements, with night optima about the same of slightly lower than daylight optima.[37]Tree provenance is important in artificial regeneration. Good provenance takes into account suitable tree genetics and a good environmental fit for planted / seeded trees in a forest stand. The wrong genotype can lead to failed regeneration, or poor trees that are prone to pathogens and undesired outcomes.Artificial regeneration has been a more common method involving planting because it is more dependable than natural regeneration. Planting can involve using seedlings (from a nursery), (un)rooted cuttings, or seeds.[38]Whichever method is chosen it can be assisted by tending techniques also known as intermediate stand treatments.The fundamental genetic consideration in artificial regeneration is that seed and planting stock must be adapted to the planting environment. Most commonly, the method of managing seed and stock deployment is through a system of defined seed zones, within which seed and stock can be moved without risk of climatic maladaptation.[39] Ontario adopted a seed zone system in the 1970s based on G.A. Hills' 1952[40] site regions and provincial resource district boundaries, but Ontario's seed zones are now based on homogeneous climatic regions developed with the Ontario Climate Model.[41][39] The regulations stipulate that source-identified seedlots may be either a general collection, when only the seed zone of origin is known, or a stand collection from a specific latitude and longitude. The movement of general-collection seed and stock across seed zone boundaries is prohibited, but the use of stand-collection seed and stock in another seed zone is acceptable when the Ontario Climate Model shows that the planting site and place of seed origin are climatically similar. The 12 seed zones for white spruce in Quebec are based mainly on ecological regions, with a few modifications for administrative convenience.[42]Seed quality varies with source. Seed orchards produce seed of the highest quality, then, in order of decreasing seed quality produced, seed production areas and seed collection areas follow, with controlled general collections and uncontrolled general collections producing the least characterized seed.When seed is first separated from cones it is mixed with foreign matter, often 2 to 5 times the volume of the seed. The more or less firmly attached membranous wings on the seed must be detached before it is cleaned of foreign matter.[43] The testa must not incur damage during the dewinging process. Two methods have been used, dry and wet. Dry seed may be rubbed gently through a sieve that has a mesh through which only seed without wings can pass. Large quantities of seed can be processed in dewinging machines, which use cylinders of heavy wire mesh and rapidly revolving stiff brushes within to remove the wings. In the wet process, seed with wings attached are spread out 10 cm to 15 cm deep on a tight floor and slightly moistened throughout; light leather flails are used to free seed from the wings. B. Wang described a unique wet dewinging procedure in 1973 using a cement mixer,[44] used at the Petawawa tree seed processing facility. Wings of white and Norway spruce seed can be removed by dampening the seed slightly before it is run through a fanning mill for the last time.[43] Any moistened seed must be dried before fermentation or moulding sets in.A fluorescein diacetate (FDA) biochemical viability test for several species of conifer seed, including white spruce, estimates the proportion of live seed (viability) in a seedlot, and hence the percentage germination of a seedlot. The accuracy of predicting percentage germination was within +/- 5 for most seedlots.[45] White spruce seed can be tested for viability by an indirect method, such as the fluorescein diacetate (FDA) test[45] or ‘Ultra-sound';[28] or by the direct growth method of ‘germination'. Samples of white spruce seed inspected in 1928 varied in viability from 50% to 100%, but averaged 93%.[46] A 1915 inspection reported 97% viability for white spruce seed.[43]The results of a germination test are commonly expressed as germinative capacity or a germination percentage, which is the percentage of seeds that germinate during a period of time, ending when germination is practically complete. During extraction and processing, white spruce seeds gradually lost moisture, and total germination increased. Mittal et al. (1987)[47] reported that white spruce seed from Algonquin Park, Ontario, obtained the maximum rate (94% in 6 days) and 99% total germination in 21 days after 14-week pre-chilling. The pre-treatment of 1% sodium hypochlorite increased germinability.Encouraged by Russian success in using ultrasonic waves to improve the germinative energy and percentage germination of seeds of agricultural crops, Timonin (1966)[48] demonstrated benefits to white spruce germination after exposure of seeds to 1, 2, or 4 minutes of ultrasound generated by an M.S.E. ultrasonic disintegrator with a power consumption of 280 VA and power impact of 1.35 amperes.[48]:Tables 3.18 and 3.19 However, no seeds germinated after 6 minutes of exposure to ultrasound.Seed dormancy is a complex phenomenon and is not always consistent within species.[49] Cold stratification of white spruce seed to break dormancy has been specified as a requirement,[50][51][52][53] but Heit (1961)[54] and Hellum (1968)[55] regarded stratification as unnecessary. Cone handling and storage conditions affect dormancy in that cold, humid storage (5 °C, 75% to 95% relative humidity) of the cones prior to extraction seemingly eliminated dormancy by overcoming the need to stratify.[49] Periods of cold, damp weather during the period of cone storage might provide natural cold (stratification) treatment. Once dormancy was removed in cone storage, subsequent kiln-drying and seed storage did not reactivate dormancy.Haddon and Winston (1982)[49] found a reduction in viability of stratified seeds after 2 years of storage and suggested that stress might have been caused by stratification, e.g., by changes in seed biochemistry, reduced embryo vigor, seed aging or actual damage to the embryo. They further questioned the quality of the 2-year-old seed even though high germination occurred in the samples that were not stratified.Cold stratification is the term applied to the storing of seeds in (and, strictly, in layers with) a moist medium, often peat or sand, with a view to maintaining viability and overcoming dormancy. Cold stratification is the term applied to storage at near-freezing temperatures, even if no medium is used. A common method of cold stratification, is to soak seed in tap water for up to 24 h, superficially dry it, then store moist for some weeks or even months at temperatures just above freezing.[56][57][58] Although Hellum (1968)[55] found that cold stratification of an Alberta seed source led to irregular germination, with decreasing germination with increasing length of the stratification period, Hocking's (1972)[59] paired test with stratified and nonstratified Alberta seed from several sources revealed no trends in response to stratification. Hocking suggested that seed maturity, handling, and storage needed to be controlled before the need for stratification could be determined. Later, Winston and Haddon (1981)[60] found that the storage of white spruce cones for 4 weeks at 5 °C prior to extraction obviated the need for stratification.Seed maturity cannot be predicted accurately from cone flotation, cone moisture content, cone specific gravity; but the province of B.C. found embryo occupying 90% + of the corrosion cavity and megagametophyte being firm and whitish in colour are the best predictors for white spruce in B.C.,[61] and Quebec can forecast seed maturity some weeks in advance by monitoring seed development in relation to heat-sums and the phenological progression of the inflorescence of fireweed (Epilobium angustifolium L.), an associated plant species.[62] Cone collection earlier than one week before seed maturity would reduce seed germination and viability during storage.[62] Four stages of maturation were determined by monitoring carbohydrates, polyols, organic acids, respiration, and metabolic activity. White spruce seeds require a 6-week post-harvest ripening period in the cones to obtain maximum germinability,[63] however, based on cumulative degree-days, seed from the same trees and stand showed 2-week cone storage was sufficient.[64]See Plant nurseryPlantations may be considered successful when outplant performance satisfies certain criteria. The term "free growing" is applied in some jurisdictions. Ontario's "Free-to-Grow" (FTG) equivalent relates to a forest stand that meets a minimum stocking standard and height requirement, and is essentially free of competition from surrounding vegetation that might impede growth.[65] The FTG concept was introduced with the advent of the Forest Management Agreement program in Ontario in 1980 and became applicable to all management units in 1986. Policy, procedures, and methodologies readily applicable by forest unit managers to assess the effectiveness of regeneration programs were still under development during the Class Environmental Assessment hearings.In British Columbia, the Forest Practices Code (1995)[66] governs performance criteria. To minimize the subjectivity of assessing deciduous competition as to whether or not a plantation is established, minimum specifications of number, health, height, and competition have been specified in British Columbia. However, minimum specifications are still subjectively set and may need to be fine-tuned in order to avoid unwarranted delay in according established status to a plantation. For example, a vigorous white spruce with a strong, multi-budded leading shoot and its crown fully exposed to light on 3 sides would not qualify as free-growing in the current British Columbia Code but would hardly warrant description as unestablished.Competition arises when individual organisms are sufficiently close together to incur growth constraint through mutual modification of the local environment.[67] Plants may compete for light, moisture and nutrients, but seldom for space per se. Vegetation management directs more of the site's resources into usable forest products, rather than just eliminating all competing plants.[68] Ideally, site preparation ameliorates competition to levels that relieve the outplant of constraints severe enough to cause prolonged check.The diversity of boreal and sub-boreal broadleaf-conifer mixed tree species stands, commonly referred to as the "mixedwoods", largely preclude the utility of generalizations and call for the development of management practices incorporating the greater inherent complexity of broadleaf-conifer mixtures, relative to single-species or mixed-species conifer forest.[69] After harvesting or other disturbance, mixedwood stands commonly enter a prolonged period in which hardwoods overtop the coniferous component, subjecting them to intense competition in an understorey. It is well established that the regeneration and growth potential of understorey conifers in mixedwood stands is correlated to the density of competing hardwoods.[70] To help apply "free-to-grow" regulations in British Columbia and Alberta, management guidelines based on distance-dependent relations within a limited radius of crop trees were developed, but Lieffers et al. (2002)[71] found that free-growing stocking standards did not adequately characterize light competition between broadleaf and conifer components in boreal mixedwood stands, and further noted that adequate sampling using current approaches would be operationally prohibitive.Many promising plantations have failed through lack of tending. Young crop trees are often ill-equipped to fight it out with competition resurgent following initial site preparation and planting.Perhaps the most direct evaluation of the effect of competition on plantation establishment is provided by an effective herbicide treatment. The fact that herbicide treatment does not always produce positive results should not obscure the demonstrated potential of herbicides for significantly promoting plantation establishment. Factors that can vitiate the effectiveness of a herbicide treatment include: weather, especially temperature, prior to and during application; weather, especially wind, during application; weather, especially precipitation, in the 12 to 24 hours after application; vegetation characteristics, including species, size, shape, phenological stage, vigour, and distribution of weeds; crop characteristics, including species, phenology, and condition; the effects of other treatments, such as preliminary shearblading, burning or other prescribed or accidental site preparation; and the herbicide used, including dosage, formulation, carrier, spreader, and mode of application. There is a lot that can go wrong, but a herbicide treatment can be as good or better than any other method of site preparation.The study of competition dynamics requires both a measure of the competition level and a measure of crop response. Various competition indices have been developed, e.g., by Bella (1971)[72] and Hegyi (1974)[73] based on stem diameter, by Arney (1972),[74] Ek and Monserud (1974),[75] and Howard and Newton (1984)[76] based on canopy development, and Daniels (1976),[77] Wagner (1982),[78] and Weiner (1984)[79] with proximity-based models. Studies generally considered tree response to competition in terms of absolute height or basal area, but Zedaker (1982)[80] and Brand (1986)[81] sought to quantify crop tree size and environmental influences by using relative growth measures.Tending is the term applied to pre-harvest silvicultural treatment of forest crop trees at any stage after initial planting or seeding. The treatment can be of the crop itself (e.g., spacing, pruning, thinning, and improvement cutting) or of competing vegetation (e.g., weeding, cleaning).[2]How many trees per unit area (spacing) that should be planted is not an easily answered question. Establishment density targets or regeneration standards have commonly been based on traditional practice, with the implicit aim of getting the stand quickly to the free-to-grow stage.[82] Money is wasted if more trees are planted than are needed to achieve desired stocking rates, and the chance to establish other plantations is proportionately diminished. Ingress (natural regeneration) on a site is difficult to predict and often becomes surprisingly evident only some years after planting has been carried out. Early stand development after harvesting or other disturbance undoubtedly varies greatly among sites, each of which has its own peculiar characteristics.For all practical purposes, the total volume produced by a stand on a given site is constant and optimum for a wide range of density or stocking. It can be decreased, but not increased, by altering the amount of growing stock to levels outside this range.[83] Initial density affects stand development in that close spacing leads to full site utilization more quickly than wider spacing.[84] Economic operability can be advanced by wide spacing even if total production is less than in closely spaced stands.Beyond the establishment stage, the relationship of average tree size and stand density is very important.[82] Various density-management diagrams conceptualizing the density-driven stand dynamics have been developed.[85][86] Smith and Brand's (1988)[87] diagram has mean tree volume on the vertical axis and the number of trees/ha on the horizontal axis: a stand can either have many little trees or a few big ones. The self-thinning line shows the largest number of trees of a given size/ha that can be carried at any given time. However, Willcocks and Bell (1995)[82] caution against using such diagrams unless specific knowledge of the stand trajectory is known.In the Lake States, plantations have been made with the spacing between trees varying from 3 by 3 to 10 by 10 feet (0.9 m by 0.9 m to 3.0 m by 3.0 m).[88] Kittredge recommended that no fewer than 600 established trees per acre (1483/ha) be present during the early life of a plantation. To insure this, at least 800 trees per acre (1077/ha) should be planted where 85% survival may be expected, and at least 1200/ac (2970/ha) if only half of them can be expected to live.[43] This translates into recommended spacings of 5 by 5 to 8 by 8 feet (1.5 m by 1.5 m to 2.4 m by 2.4 m) for plantings of conifers, including white spruce in the Lake States.A strategy for enhancing natural forests' economic value is to increase their concentration of economically important, indigenous tree species by planting seeds or seedlings for future harvest, which can be accomplished with enrichment planting (EP).[89] This means increasing the planting density (i.e., the numbers of plants per hectare) in an already growing forest stand."[90]Over-crowded regeneration tends to stagnate. The problem is aggravated in species that have little self-pruning ability, such as white spruce. Spacing is a thinning (of natural regeneration), in which all trees other than those selected for retention at fixed intervals are cut. The term juvenile spacing is used when most or all of the cut trees are unmerchantable.[92] Spacing can be used to obtain any of a wide range of forest management objectives, but it is especially undertaken to reduce density and control stocking in young stands and prevent stagnation, and to shorten the rotation, i.e., to speed the production of trees of a given size. Volume growth of individual trees and the merchantable growth of stands are increased.[93] The primary rationale for spacing is that thinning is the projected decline in maximum allowable cut.[94] And since wood will be concentrated on fewer, larger, and more uniform stems, operating and milling costs will be minimized.Methods for spacing may be: manual, using various tools, including power saws, brush saws, and clippers; mechanical, using choppersand mulchers; chemical; or combinations of several methods. One treatment has had notable success in spacing massively overstocked (<100 000 stems/ha) natural regeneration of spruce and fir in Maine. Fitted to helicopter, the Thru-Valve boom emits herbicide spray droplets 1000 µm to 2000 µm in diameter[95] at very low pressure. Swaths 1.2 m wide and leave strips 2.4 m wide were obtained with "knife-edge" precision when the herbicide was applied by helicopter flying at a height of 21 m at a speed of 40–48 km/h. It seems likely that no other method could be as cost-effective.Twenty years after spacing to 2.5 × 2.5 m, 30-year-old mixed stands of balsam fir and white spruce in the Green River watershed, New Brunswick, averaged 156.9 m3/ha.[96]A spacing study of 3 conifers (white spruce, red pine and jack pine) was established at Moodie, Manitoba, on flat, sandy, nutritionally poor soils with a fresh moisture regime.[97] Twenty years after planting, red pine had the largest average dbh, 15% greater than jack pine, while white spruce dbh was less than half that of the pines. Crown width showed a gradual increase with spacing for all 3 conifers. Results to date were suggesting optimum spacings between 1.8 m and 2.4 m for both pines; white spruce was not recommended for planting on such sites.Comparable data are generated by espacement trials, in which trees are planted at a range of densities. Spacings of 1.25 m, 1.50 m, 1.75 m, 2.00 m, 2.50 m, and 3.00 m on 4 site classes were used in the 1922 trial at Petawawa, Ontario. In the first of 34 old field white spruce plantations used to investigate stand development in relation to spacing at Petawawa, Ontario, regular rows were planted at average spacings of from 4 × 4 to 7 × 7 feet (1.22 m × 1.22 m to 2.13 m × 2.13 m).[98] Spacings up to 10 × 10 feet (3.05 m × 3.03 m) were subsequently included in the study. Yield tables based on 50 years of data showed:A smaller espacement trial, begun in 1951 near Thunder Bay, Ontario, included white spruce at spacings of 1.8 m, 2.7 m, and 3.6 m.[99] At the closest spacing, mortality had begun at 37 years, but not at the wider spacings.The oldest interior spruce espacement trial in British Columbia was established in 1959 near Houston in the Prince Rupert Forest Region.[100] Spacings of 1.2 m, 2.7 m, 3.7 m, and 4.9 m were used, and trees were measured 6, 12, 16, 26, and 30 years after planting. At wide espacements, trees developed larger diameters, crowns, and branches, but (at 30 years) basal area and total volume/ha were greatest in the closest espacement (Table 6.38). In more recent trials in the Prince George Region of British Columbia (Table 6.39) and in Manitoba,[101] planting density of white spruce had no effect on growth after up to 16 growing seasons, even at spacings as low as 1.2 m. The slowness of juvenile growth and of crown closure delay the response to intra-competition. Initially, close spacing might even provide a positive nurse effect to offset any negative response to competition.See ThinningThinning is an operation that artificially reduces the number of trees growing in a stand with the aim of hastening the development of the remainder.[102] The goal of thinning is to control the amount and distribution of available growing space. By altering stand density, foresters can influence the growth, quality, and health of residual trees. It also provides an opportunity to capture mortality and cull the commercially less desirable, usually smaller and malformed, trees. Unlike regeneration treatments, thinnings are not intended to establish a new tree crop or create permanent canopy openings.Thinning greatly influences the ecology and micro-meteorology of the stand, lowering the inter-tree competition for water. The removal of any tree from a stand has repercussions on the remaining trees both above-ground and below. Silvicultural thinning is a powerful tool that can be used to influence stand development, stand stability, and the characteristics of the harvestable products.When considering intensive conifer plantations designed for maximum production, it is essential to remember that tending and thinning regimes and wind and snow damage are intimately related.[103]Previous studies have demonstrated that repeated thinnings over the course of a forest rotation increase carbon stores relative to stands that are clear-cut on short rotations and that the carbon benefits differ according to thinning method (e.g., thinning from above versus below).[104]In the early development of forest stand, density of trees remain high and there is competition among trees for nutrients.When natural regeneration or artificial seeding has resulted in dense, overstocked young stands, natural thinning will in most cases eventually reduce stocking to more silviculturally desirable levels. But by the time some trees reach merchantable size, others will be overmature and defective, and others will still be unmerchantable. To reduce this unbalance and to obtain more economic returns, in the early stage, one kind of cleaning is done which is known as precommercial thinning. Generally, one or two times precommercial thinning is done to facilitate the growth of the treeThe yield of merchantable wood can be greatly increased and the rotation shortened by precommercial thinning.[105] Mechanical and chemical methods have been applied, but their costliness has militated against their ready adoption.Pruning, as a silvicultural practice, refers to the removal of the lower branches of the young trees (also giving the shape to the tree) so clear knot-free wood can subsequently grow over the branch stubs. Clear knot-free lumber has a higher value. Pruning has been extensively carried out in the Radiata pine plantations of New Zealand and Chile, however the development of Finger joint technology in the production of lumber and mouldings has led to many forestry companies reconsidering their pruning practices. "Brashing" is an alternative name for the same process.[106]Pruning can be done to all trees, or more cost effectively to a limited number of trees. There are two types of pruning: natural or self-pruning and artificial pruning. Most cases of self-pruning happen when branches do not receive enough sunlight and die. Wind can also take part in natural pruning which can break branches.[107] Artificial pruning is where people are paid to come and cut the branches. Or it can be natural, where trees are planted close enough that the effect is to cause self-pruning of low branches as energy is put into growing up for light reasons and not branchiness.The term stand conversion refers to a change from one silvicultural system to another and includes species conversion, i.e., a change from one species (or set of species) to another.[2] Such change can be effected intentionally by various silvicultural means, or incidentally by default e.g., when high-grading has removed the coniferous content from a mixedwood stand, which then becomes exclusively self-perpetuating aspen. In general, such sites as these are the most likely to be considered for conversion.In discussing yields that might be expected from the Canadian spruce forests, Haddock (1961)[108] noted that Wright's (1959)[109] quotation of spruce yields in the British Isles of 220 cubic feet per acre (15.4 m3/ha) per year and in Germany of 175 cubic feet per acre (12.25 m3/ha) per year was misleading, at least if it was meant to imply that such yields might be approached in the Boreal Forest Region of Canada. Haddock thought that Wright's suggestion of 20 to 40 (average 30) cubic feet per acre (1.4 m3/ha to 2.8 m3/ha (average 2.1 m3/ha) per year was more reasonable, but still somewhat optimistic.The principal way forest resource managers influence growth and yield is to manipulate the mixture of species and number (density) and distribution (stocking) of individuals that form the canopy of the stand.[110][111] Species composition of much of the boreal forest in North America already differs greatly from its pre-exploitation state. There is less spruce and more hardwoods in the second-growth forest than in the original forest; Hearnden et al. (1996)[112] calculated that the spruce cover type had declined from 18% to only 4% of the total forested area in Ontario. Mixedwood occupies a greater proportion of Ontario's second-growth forest (41%) than in the original (36%), but its component of white spruce is certainly much diminished.Growth performance is certainly influenced by site conditions and thus by the kind and degree of site preparation in relation to the nature of the site. It is important to avoid the assumption that site preparation of a particular designation will have a particular silvicultural outcome. Scarification, for instance, not only covers a wide range of operations that scarify, but also any given way of scarifying can have significantly different results depending on site conditions at the time of treatment. In point of fact, the term is commonly misapplied. Scarification is defined[2] as "Loosening the top soil of open areas, or breaking up the forest floor, in preparation for regenerating by direct seeding or natural seedfall", but the term is often misapplied to practices that include scalping, screefing, and blading, which pare off low and surface vegetation, together with most off its roots to expose a weed-free surface, generally in preparation for sowing or planting thereon.Thus, it is not surprising that literature can be used to support the view that the growth of seedlings on scarified sites is much superior to that of growth on similar sites that have not been scarified,[113][114][115] while other evidence supports the contrary view that scarification can reduce growth.[116][117][118] Detrimental results can be expected from scarification that impoverishes the rooting zone or exacerbates edaphic or climatic constraints.Burning site preparation has enhanced spruce seedling growth,[114] but it must be supposed that burning could be detrimental if the nutrient capital is significantly depleted.An obvious factor greatly influencing regeneration is competition from other vegetation. In a pure stand of Norway spruce, for instance, Roussel (1948)[119] found the following relationships:A factor of some importance in solar radiation–reproduction relationships is excess heating of the soil surface by radiation.[120] This is especially important for seedlings, such as spruce, whose first leaves do not shade the base of the stem at the soil surface. Surface temperatures in sandy soils on occasion reach lethal temperatures of 50 °C to 60 °C.Silvicultural regeneration methods combine both the harvest of the timber on the stand and re-establishment of the forest. The proper practice of sustainable forestry[121] should mitigate the potential negative impacts, but all harvest methods will have some impacts on the land and residual stand.[122] The practice of sustainable forestry limits the impacts such that the values of the forest are maintained in perpetuity. Silvicultural prescriptions are specific solutions to a specific set of circumstances and management objectives.[123]  Following are some common methods:Conventional clearcut harvesting is relatively simple: all trees on a cutblock are felled and bunched with bunches aligned to the skidding direction, and a skidder then drags the bunches to the closest log deck.[124] Feller-buncher operators concentrate on the width of the felled swath, the number of trees in a bunch, and the alignment of the bunch. Providing a perimeter boundary is felled during daylight, night-shift operations can continue without the danger of trespassing beyond the block. Productivity of equipment is maximized because units can work independently of one another.An even-aged regeneration method that can employ either natural or artificial regeneration. It involves the complete removal of the forest stand at one time.[125]  Clearcutting can be biologically appropriate with species that typically regenerate from stand replacing fires or other major disturbances, such as Lodgepole Pine (Pinus contorta). Alternatively, clearcutting can change the dominating species on a stand with the introduction of non-native and invasive species as was shown at the Blodgett Experimental Forest near Georgetown California. Additionally, clearcutting can prolong slash decomposition, expose soil to erosion, impact visual appeal of a landscape and remove essential wildlife habitat. It is particularly useful in regeneration of tree species such as Douglas-fir (Pseudotsuga menziesii) which is shade intolerant.[verification needed].  In addition, the general public's distaste for even-aged silviculture, particularly clearcutting, is likely to result in a greater role for uneven-aged management on public lands as well.[126] Across Europe, and in parts of North America, even-aged, production-orientated and intensively managed plantations are beginning to be regarded in the same way as old industrial complexes: something to abolish or convert to something else.[127]Clearcutting will impact many site factors important in their effect on regeneration, including air and soil temperatures. Kubin and Kemppainen (1991),[128] for instance, measured temperatures in northern Finland from 1974 through 1985 in 3 clear-felled areas and in 3 neighouring forest stands dominated by Norway spruce. Clear felling had no significant influence on air temperature at 2 m above the ground surface, but the daily air temperature maxima at 10 cm were greater in the clear-felled area than in the uncut forest, while the daily minima at 10 cm were lower. Night frosts were more common in the clear-felled area. Daily soil temperatures at 5 cm depth were 2 °C to 3 °C greater in the clear-felled area than in the uncut forest, and temperatures at depths of 50 cm and 100 cm were 3 °C to 5 °C greater. The differences between the clear-felled and uncut areas did not diminish during the 12 years following cutting.A regeneration method which depends on the sprouting of cut trees.  Most hardwoods, the coast redwood, and certain pines naturally sprout from stumps and can be managed through coppicing. Coppicing is generally used to produce fuelwood, pulpwood, and other products dependent on small trees. A close relative of coppicing is pollarding.[129] Three systems of coppice woodland management are generally recognized: simple coppice, coppice with standards, and the coppice selection system.[130]Prochnau (1963),[131] 4 years after sowing, found that 14% of viable white spruce seed sown on mineral soil had produced surviving seedlings, at a seed:seedling ratio of 7.1:1.  With Engelmann spruce, Smith and Clark (1960)[132] obtained average 7th year seed:seedling ratios of 21:1 on scarified seedbeds on dry sites, 38:1 on moist sites, and 111:1 on litter seedbeds.The group selection method is an uneven-aged regeneration method that can be used when mid-tolerant species regeneration is desired.  The group selection method can still result in residual stand damage in dense stands, however directional falling can minimize the damage.  Additionally, foresters can select across the range of diameter classes in the stand and maintain a mosaic of age and diameter classes.Classical European silviculture achieved impressive results with systems such as Henri Biolley's méthode du contrôle in Switzerland, in which the number and size of trees harvested were determined by reference to data collected from every tree in every stand measured every 7 years.[133]While not designed to be applied to boreal mixedwoods, the méthode du contrôle is described briefly here to illustrate the degree of sophistication applied by some European foresters to the management of their forests. Development of management techniques that allowed for stand development to be monitored and guided into sustainable paths were in part a response to past experience, particularly in Central European countries, of the negative effects of pure, uniform stands with species often unsuited to the site, which greatly increased the risk of soil degradation and biotic diseases. Increased mortality and decreased increment generated widespread concern, especially after reinforcement by other environmental stresses.More or less uneven-aged, mixed forests of preponderantly native species, on the other hand, treated along natural lines, have proved to be healthier and more resistant to all kinds of external dangers; and in the long run such stands are more productive and easier to protect.However, irregular stands of this type are definitely more difficult to manage—new methods and techniques had to be sought particularly for the establishment of inventories, as well as increment control and yield regulation. In Germany, for instance, since the beginning of the nineteenth century under the influence of G.L. Hartig (1764–1837), yield regulation has been effected almost exclusively by allotment or formula methods based on the conception of the uniform normal forest with a regular succession of cutting areas.In France, on the other hand, efforts were made to apply another kind of forest management, one that aimed to bring all parts of the forest to a state of highest productive capacity in perpetuity. In 1878, the French forester A. Gurnaud (1825–1898) published a description of a méthode du contrôle for determining increment and yield. The method was based on the fact that through careful, selective harvesting, the productivity of the residual stand can be improved, because timber is removed as a cultural operation. In this method, the increment of stands is accurately determined periodically with the object of gradually converting the forest, through selective management and continuous experimentation, to a condition of equilibrium at maximum productive capacity.Henri Biolley (1858–1939) was the first to apply Gurnaud's inspired ideas to practical forestry. From 1890 on, he managed the forests of his Swiss district according to these principles, devoting himself for almost 50 years to the study of increment and a treatment of stands directed towards the highest production, and proving the practicability of the check method. In 1920, he published this study giving a theoretical basis of management of forests under the check method, describing the procedures to be applied in practice (which he partly developed and simplified), and evaluating the results.Biolley's pioneering work formed the basis upon which most Swiss forest management practices were later developed, and his ideas have been generally accepted. Today, with the trend of intensifying forest management and productivity in most countries, the ideas and application of careful, continuous treatment of stands with the aid of the volume check method are meeting with ever-growing interest. In Britain and Ireland, for example, there is increased application of Continuous Cover Forestry principles to create permanently irregular structures in many woodlands.[134]Spot and row seeders use less seed that does broadcast ground or aerial seeding but may induce clumping. Row and spot seeding confer greater ability to control seed placement than does broadcast seeding. Also, only a small percentage of the total area needs to be treated.In the aspen type of the Great Lakes region, direct sowing of the seed of conifers has usually failed.[135] However, Gardner (1980)[136] after trials in Yukon, which included broadcast seeding of white spruce seed at 2.24 kg/ha that secured 66.5% stocking in the Scarified Spring Broadcast treatment 3 years after seeding, concluded that the technique held "considerable promise".An even-aged regeneration method that retains widely spaced residual trees in order to provide uniform seed dispersal across a harvested area. In the seed-tree method, 2-12 seed trees per acre (5-30/ha) are left standing in order to regenerate the forest. They will be retained until regeneration has become established at which point they may be removed. It may not always be economically viable or biologically desirable to re-enter the stand to remove the remaining seed trees. Seed-tree cuts can also be viewed as a clearcut with natural regeneration and can also have all of the problems associated with clearcutting. This method is most suited for light-seeded species and those not prone to windthrow.Selection systems are appropriate where uneven stand structure is desired, particularly where the need to retain continuous cover forest for aesthetic or environmental reasons outweighs other management considerations. Selection logging has been suggested as being of greater utility than shelterwood systems in regenerating old-growth Engelmann Spruce Sub-alpine Fir (ESSF) stands in southern British Columbia.[137] In most areas, selection logging favours regeneration of fir more than the more light-demanding spruce.[138][28][139] In some areas, selection logging can be expected to favour spruce over less tolerant hardwood species (Zasada 1972)[140] or lodgepole pine.[28]The use of shelters to improve germination and survival in spot seedings seeks to capture the benefits of greenhouse culture, albeit miniature. The Hakmet seed shelter, for instance, is a semi-transparent plastic cone 8 cm high, with openings of 7 cm diameter in the 7.5 cm diameter base and 17 mm diameter in the 24 mm diameter top.[141] This miniature greenhouse increases air humidity, reduces soil desiccation, and raises air and soil temperatures to levels more favourable to germination and seedling growth than those offered by unprotected conditions. The shelter is designed to break down after a few years of exposure to ultraviolet radiation.Seed shelters and spring sowing significantly improved stocking compared with bare spot seeding, but sheltering did not significantly improve growth. Stocking of bare seedspots was extremely low, possibly due to smothering of seedlings by abundant broadleaf and herbaceous litter, particularly that from aspen and red raspberry, and exacerbated by strong competition from graminoids and raspberry.Cone shelters (Cerkon™) usually produced greater survival than unsheltered seeding on scarified seedspots in trials of direct seeding techniques in interior Alaska, and funnel shelters (Cerbel™) usually produced greater survival than unsheltered seeding on non-scarified seedspots.[142] Both shelter types are manufactured by AB Cerbo in Trollhättan, Sweden. Both are made of light-degradable, white, opaque plastic, and are 8 cm high when installed.White spruce seed was sown in Alaska on a burned site in summer 1984, and protected by white plastic cones on small spots scarified by hand, or by white funnels placed directly into the residual ash and organic material.[143] A group of 6 ravens (Corvus corax) was observed in the area about 1 week after sowing was completed in mid-June. Damage averaged 68% with cones and 50% with funnels on an upland area, and 26% with funnels on a floodplain area. Damage by ravens was only 0.13% on unburned but otherwise similar areas.In seeding trials in Manitoba between 1960 and 1966 aimed at converting aspen stands to spruce–aspen mixedwoods, 1961 scarification in the Duck Mountain Provincial Forest remained receptive to natural seeding for many years.[144]In general terms, the shelterwood system is a series of partial cuts that removes the trees of an existing stand over several years and eventually culminates in a final cut that creates a new even-aged stand.[145] It is an even-aged regeneration method that removes trees in a series of three harvests: 1) Preparatory cut; 2) Establishment cut; and 3) Removal cut. The success of practising a shelterwood system is closely related to: 1. the length of the regeneration period, i.e. the time from the shelterwood cutting to the date when a new generation of trees has been established; 2.the quality of the new tree stand with respect to stand density and growth; and 3.the value increment of the shelter trees. Information on the establishment, survival and growth of seedlings influenced by the cover of shelter trees, as well as on the growth of these trees, is needed as a basis for modelling the economic return of practising a shelterwood system.[146] The method's objective is to establish new forest reproduction under the shelter of the retained trees. Unlike the seed-tree method, residual trees alter understory environmental conditions (i.e. sunlight, temperature, and moisture) that influence tree seedling growth. This method can also find a middle ground with the light ambiance by having less light accessible to competitors while still being able to provide enough light for tree regeneration.[147] Hence, shelterwood methods are most often chosen for site types characterized by extreme conditions, in order to create a new tree generation within a reasonable time period. These conditions are valid foremost on level ground sites which are either dry and poor or moist and fertile.[148]Shelterwood systems involve 2, 3, or exceptionally more partial cuttings. A final cut is made once adequate natural regeneration has been obtained. The shelterwood system is most commonly applied as a 2-cut uniform shelterwood, first an initial regeneration (seed) cut, the second a final harvest cut. In stands less than 100 years old, a light preparatory cut can be useful.[140] A series of intermediate cuts at intervals of 10–20 years has been recommended for intensively managed stands.[138]From operational or economic standpoints, however, there are disadvantages to the shelterwood system: harvesting costs are higher; trees left for deferred cutting may be damaged during the regeneration cut or related extraction operations; the increased risk of blowdown threatens the seed source; damage from bark beetles is likely to increase; regeneration may be damaged during the final cut and related extraction operations; the difficulty of any site preparation would be increased; and incidental damage to regeneration might be caused by any site preparation operations.[20][116][140][149][138]The single-tree selection method is an uneven-aged regeneration method most suitable when shade tolerant species regeneration is desired. It is typical for older and diseased trees to be removed, thus thinning the stand and allowing for younger, healthy trees to grow. Single-tree selection can be very difficult to implement in dense or sensitive stands and residual stand damage can occur. This method is also disturbs the canopy layer the least out of all other methods.[150]Spot seeding was found to be the most economical and reliable of the direct seeding methods for converting aspen and paper birch to spruce and pine.[151] In the Chippewa National Forest (Lake States), seed-spot sowing of 10 seeds each of white spruce and white pine under 40-year aspen after different degrees of cutting on gave second-season results clearly indicating the need to remove or disturb the forest floor to obtain germination of seeded white spruce and white pine.[135]Spot seeding of coniferous seed, including white spruce, has had occasional success, but several constraining factors commonly limit germination success: the drying out of the forest floor before the roots of germinants reach underlying moisture reserves; and, particularly under hardwoods, the smothering of small seedlings by snow-pressed leaf litter and lesser vegetation. Kittredge and Gervorkiantz (1929)[135] determined that removal of the aspen forest floor increased germination percentage after the second season in seed spots of both white pine and white spruce, in 4 plots, from 2.5% to 5%, from 8% to 22%, from 1% to 9.5%, and from 0% to 15%.Spot seeding requires less seed than broadcast seeding and tends to achieve more uniform spacing, albeit sometimes with clumping. The devices used in Ontario for manual spot seeding are the "oil can" seeder, seeding sticks, and shakers.[152] The oil can is a container fitted with a long spout through which a predetermined number of seeds are released with each flick of the seeder.Harvesting cutblocks where only a portion of the trees are to be removed is very different from clearcutting.[124] First, trails must be located to provide access for the felling and skidding/forwarding equipment. These trails must be carefully located to ensure that the trees remaining meet the desired quality criteria and stocking density. Second, the equipment must not damage the residual stand. The further desiderata are outlined by Sauder (1995).[124]The dearth of seed and a deficiency of receptive seedbeds were recognized as major reasons for the lack of success of clearcut harvesting. One remedy attempted in British Columbia and Alberta has been alternate strip cutting.[153] The greater seed source from uncut trees between the cut strips, and the disturbance to the forest floor within the cut strips could be expected to increase the amount of natural regeneration. Trees were cut to a diameter limit in the cut strips, but large trees in the leave strips often proved too much of a temptation and were cut too,[28] thus removing those trees that would otherwise have been the major source of seed.An unfortunate consequence of strip thinning was the build-up of spruce beetle populations. Shaded slash from the initial cut, together with an increase in the number of windthrown trees in the leave strips, provided conditions ideally suited to the beetle.[154]DeLong et al. (1991)[155] suggested underplanting 30- to 40-year-old aspen stands, on the basis of the success of natural spruce in regenerating under stands of such stands: "By planting, spacing can be controlled enabling easier protection of the spruce during stand entry for harvesting of the aspen overstorey".A harvesting and regeneration method which is a relatively new silvicultural system that retains forest structural elements (stumps, logs, snags, trees, understory species and undisturbed layers of forest floor) for at least one rotation in order to preserve environmental values associated with structurally complex forests.[156]"Uneven-aged and even-aged methods differ in the scale and intensity of disturbance. Uneven-aged methods maintain a mix of tree sizes or ages within a habitat patch by periodically harvesting individual or small groups of trees, Even-aged methods harvest most or all of the overstory and create a fairly uniform habitat patch dominated by trees of the same age".[157] Even-aged management systems have been the prime methods to use when studying the effects on birds.[158]A survey in 1955–56 to determine survival, development, and the reasons for success or failure of conifer pulpwood plantations (mainly of white spruce) in Ontario and Quebec up to 32 years old found that the bulk of the mortality occurred within the first 4 years of planting, unfavourable site and climate being the main causes of failure.[159]Naturally regenerated trees in an understorey prior to harvesting constitute a classic case of good news and bad news. Understorey white spruce is of particular importance in mixedwoods dominated by aspen, as in the B15, B18a, and B19a Sections of Manitoba,[160] and elsewhere. Until the latter part of the last century, white spruce understorey was mostly viewed as money in the bank on a long-term, low interest deposit, with final yield to be realized after slow natural succession,[161] but the resource became increasingly threatened with the intensification of harvesting of aspen. White spruce plantations on mixedwood sites proved expensive, risky, and generally unsuccessful.[161] This prompted efforts to see what might be done about growing aspen and white spruce on the same landbase by protecting existing white spruce advance growth, leaving a range of viable crop trees during the first cut, then harvesting both hardwoods and spruce in the final cut. Information about the understorey component is critical to spruce management planning. The ability of then current harvesting technology and crews employed to provide adequate protection for white spruce understories was questioned by Brace and Bella. Specialized equipment and training, perhaps with financial incentives, may be needed to develop procedures that would confer the degree of protection needed for the system to be feasible. Effective understorey management planning requires more than improved mixedwood inventory.Avoidance of damage to the understorey will always be a desideratum. Sauder's (1990)[162] paper on mixedwood harvesting describes studies designed to evaluate methods of reducing non-trivial damage to understorey residuals that would compromise their chance of becoming a future crop tree. Sauder concluded that: (1) operational measures that protected residual stems may not unduly increase costs, (2) all felling, conifers and hardwoods, needs to be done in one operation to minimize the entry of the feller-buncher into the residual stand, (3) several operational procedures can reduce understorey damage, some of them without incurring extra costs, and (4) successful harvesting of treatment blocks depends primarily on the intelligent location of skid trails and landings. In summary, the key to protecting the white spruce understorey without sacrificing logging efficiency is a combination of good planning, good supervision, the use of appropriate equipment, and having conscientious, well-trained operators.Even the best plan will not reduce understorey damage unless its implementation is supervised.[163]New stands need to be established to provide for future supply of commercial white spruce from 150 000 ha of boreal mixedwoods in 4 of Rowe's (1972)[160] regional Forest Sections straddling Alberta, Saskatchewan, and Manitoba, roughly from Peace River AB to Brandon MB.[164] In the 1980s, with harvesting using conventional equipment and procedures, a dramatic increase in the demand for aspen posed a serious problem for the associated spruce understorey. Formerly, white spruce in the understories had developed to commercial size through natural succession under the protection of the hardwoods. Brace articulated a widespread concern: "The need for protection of spruce as a component of boreal mixedwoods goes beyond concern for the future commercial softwood timber supply. Concerns also include fisheries and wildlife habitat, aesthetics and recreation, a general dissatisfaction with cleacutting in mixedwoods and a strong interest in mixedwood perpetuation, as expressed recently in 41 public meetings on forestry development in northern Alberta...".[164]On the basis of tests of 3 logging systems in Alberta, Brace (1990)[165] affirmed that significant amounts of understorey can be retained using any of those systems provided that sufficient effort is directed towards protection. Potential benefits would include increased short-term softwood timber supply, improved wildlife habitat and cutblock aesthetics, as well as reduced public criticism of previous logging practices. Stewart et al. (2001)[166] developed statistical models to predict the natural establishment and height growth of understorey white spruce in the boreal mixedwood forest in Alberta using data from 148 permanent sample plots and supplementary information about height growth of white spruce regeneration and the amount and type of available substrate. A discriminant model correctly classified 73% of the sites as to presence or absence of a white spruce understorey, based on the amount of spruce basal area, rotten wood, ecological nutrient regime, soil clay fraction, and elevation, although it explained only 30% of the variation in the data. On sites with a white spruce understorey, a regression model related the abundance of regeneration to rotten wood cover, spruce basal area, pine basal area, soil clay fraction, and grass cover (R² = 0.36). About half the seedlings surveyed grew on rotten wood, and only 3% on mineral soil, and seedlings were 10 times more likely to have established on these substrates than on litter. Exposed mineral soil covered only 0.3% of the observed transect area.Advance growth management, i.e., the use of suppressed understorey trees, can reduce reforestation costs, shorten rotations, avoid denuding the site of trees, and also reduce adverse impacts on aesthetic, wildlife, and watershed values.[167][168] To be of value, advance growth must have acceptable species composition and distribution, have potential for growth following release, and not be vulnerable to excessive damage from logging.The age of advance growth is difficult to estimate from its size,[169] as white that appears to be 2- to 3-year-old may well be more than 20 years old.[170] However, age does not seem to determine the ability of advance growth of spruce to respond to release,[167][168][171] and trees older than 100 years have shown rapid rates of growth after release. Nor is there a clear relationship between the size of advance growth and its growth rate when released.Where advance growth consists of both spruce and fir, the latter is apt to respond to release more quickly than the former, whereas spruce does respond.[172][173] If the ratio of fir to spruce is large, however, the greater responsiveness to release of fir may subject the spruce to competition severe enough to negate much of the effect of release treatment. Even temporary relief from shrub competition has increased height growth rates of white spruce in northwestern New Brunswick, enabling the spruce to overtop the shrubs.[174]Site preparation is any of various treatments applied to a site in order to ready it for seeding or planting. The purpose is to facilitate the regeneration of that site by the chosen method. Site preparation may be designed to achieve, singly or in any combination: improved access, by reducing or rearranging slash, and amelioration of adverse forest floor, soil, vegetation, or other biotic factors. Site preparation is undertaken to ameliorate one or more constraints that would otherwise be likely to thwart the objectives of management. A valuable bibliography on the effects of soil temperature and site preparation on subalpine and boreal tree species has been prepared by McKinnon et al. (2002).[175]Site preparation is the work that is done before a forest area is regenerated. Some types of site preparation are burning.Broadcast burning is commonly used to prepare clearcut sites for planting, e.g., in central British Columbia,[176] and in the temperate region of North America generally.[177]Prescribed burning is carried out primarily for slash hazard reduction and to improve site conditions for regeneration; all or some of the following benefits may accrue:Prescribed burning for preparing sites for direct seeding was tried on a few occasions in Ontario, but none of the burns was hot enough to produce a seedbed that was adequate without supplementary mechanical site preparation.[152]Changes in soil chemical properties associated with burning include significantly increased pH, which Macadam (1987)[176] in the Sub-boreal Spruce Zone of central British Columbia found persisting more than a year after the burn. Average fuel consumption was 20 to 24 t/ha and the forest floor depth was reduced by 28% to 36%. The increases correlated well with the amounts of slash (both total and ≥7 cm diameter) consumed. The change in pH depends on the severity of the burn and the amount consumed; the increase can be as much as 2 units, a 100-fold change.[178] Deficiencies of copper and iron in the foliage of white spruce on burned clearcuts in central British Columbia might be attributable to elevated pH levels.[179]Even a broadcast slash fire in a clearcut does not give a uniform burn over the whole area. Tarrant (1954),[180] for instance, found only 4% of a 140-ha slash burn had burned severely, 47% had burned lightly, and 49% was unburned. Burning after windrowing obviously accentuates the subsequent heterogeneity.Marked increases in exchangeable calcium also correlated with the amount of slash at least 7 cm in diameter consumed.[176] Phosphorus availability also increased, both in the forest floor and in the 0 cm to 15 cm mineral soil layer, and the increase was still evident, albeit somewhat diminished, 21 months after burning. However, in another study[181] in the same Sub-boreal Spruce Zone found that although it increased immediately after the burn, phosphorus availability had dropped to below pre-burn levels within 9 months.Nitrogen will be lost from the site by burning,[176][181][182] though concentrations in remaining forest floor were found by Macadam (1987)[176] to have increased in 2 of 6 plots, the others showing decreases. Nutrient losses may be outweighed, at least in the short term, by improved soil microclimate through the reduced thickness of forest floor where low soil temperatures are a limiting factor.The Picea/Abies forests of the Alberta foothills are often characterized by deep accumulations of organic matter on the soil surface and cold soil temperatures, both of which make reforestation difficult and result in a general deterioration in site productivity; Endean and Johnstone (1974)[183] describe experiments to test prescribed burning as a means of seedbed preparation and site amelioration on representative clear-felled Picea/Abies areas. Results showed that, in general, prescribed burning did not reduce organic layers satisfactorily, nor did it increase soil temperature, on the sites tested. Increases in seedling establishment, survival, and growth on the burned sites were probably the result of slight reductions in the depth of the organic layer, minor increases in soil temperature, and marked improvements in the efficiency of the planting crews. Results also suggested that the process of site deterioration has not been reversed by the burning treatments applied.Slash weight (the oven-dry weight of the entire crown and that portion of the stem < 4 inches in diameter) and size distribution are major factors influencing the forest fire hazard on harvested sites.[184] Forest managers interested in the application of prescribed burning for hazard reduction and silviculture, were shown a method for quantifying the slash load by Kiil (1968).[185] In west-central Alberta, he felled, measured, and weighed 60 white spruce, graphed (a) slash weight per merchantable unit volume against diameter at breast height (dbh), and (b) weight of fine slash (<1.27 cm) also against dbh, and produced a table of slash weight and size distribution on one acre of a hypothetical stand of white spruce. When the diameter distribution of a stand is unknown, an estimate of slash weight and size distribution can be obtained from average stand diameter, number of trees per unit area, and merchantable cubic foot volume. The sample trees in Kiil's study had full symmetrical crowns. Densely growing trees with short and often irregular crowns would probably be overestimated; open-grown trees with long crowns would probably be underestimated.The need to provide shade for young outplants of Engelmann spruce in the high Rocky Mountains is emphasized by the U.S. Forest Service. Acceptable planting spots are defined as microsites on the north and east sides of down logs, stumps, or slash, and lying in the shadow cast by such material.[186] Where the objectives of management specify more uniform spacing, or higher densities, than obtainable from an existing distribution of shade-providing material, redistribution or importing of such material has been undertaken.Site preparation on some sites might be done simply to facilitate access by planters, or to improve access and increase the number or distribution of microsites suitable for planting or seeding.Wang et al. (2000)[187] determined field performance of white and black spruces 8 and 9 years after outplanting on boreal mixedwood sites following site preparation (Donaren disc trenching versus no trenching) in 2 plantation types (open versus sheltered) in southeastern Manitoba. Donaren trenching slightly reduced the mortality of black spruce but significantly increased the mortality of white spruce. Significant difference in height was found between open and sheltered plantations for black spruce but not for white spruce, and root collar diameter in sheltered plantations was significantly larger than in open plantations for black spruce but not for white spruce. Black spruce open plantation had significantly smaller volume (97 cm³) compared with black spruce sheltered (210 cm³), as well as white spruce open (175 cm³) and sheltered (229 cm³) plantations. White spruce open plantations also had smaller volume than white spruce sheltered plantations. For transplant stock, strip plantations had a significantly higher volume (329 cm³) than open plantations (204 cm³). Wang et al. (2000)[187] recommended that sheltered plantation site preparation should be used.Up to 1970, no "sophisticated" site preparation equipment had become operational in Ontario,[188] but the need for more efficacious and versatile equipment was increasingly recognized. By this time, improvements were being made to equipment originally developed by field staff, and field testing of equipment from other sources was increasing.According to J. Hall (1970),[188] in Ontario at least, the most widely used site preparation technique was post-harvest mechanical scarification by equipment front-mounted on a bulldozer (blade, rake, V-plow, or teeth), or dragged behind a tractor (Imsett or S.F.I. scarifier, or rolling chopper). Drag type units designed and constructed by Ontario's Department of Lands and Forests used anchor chain or tractor pads separately or in combination, or were finned steel drums or barrels of various sizes and used in sets alone or combined with tractor pad or anchor chain units.J. Hall's (1970)[188] report on the state of site preparation in Ontario noted that blades and rakes were found to be well suited to post-cut scarification in tolerant hardwood stands for natural regeneration of yellow birch. Plows were most effective for treating dense brush prior to planting, often in conjunction with a planting machine. Scarifying teeth, e.g., Young's teeth, were sometimes used to prepare sites for planting, but their most effective use was found to be preparing sites for seeding, particularly in backlog areas carrying light brush and dense herbaceous growth. Rolling choppers found application in treating heavy brush but could be used only on stone-free soils. Finned drums were commonly used on jack pine–spruce cutovers on fresh brushy sites with a deep duff layer and heavy slash, and they needed to be teamed with a tractor pad unit to secure good distribution of the slash. The S.F.I. scarifier, after strengthening, had been "quite successful" for 2 years, promising trials were under way with the cone scarifier and barrel ring scarifier, and development had begun on a new flail scarifier for use on sites with shallow, rocky soils. Recognition of the need to become more effective and efficient in site preparation led the Ontario Department of Lands and Forests to adopt the policy of seeking and obtaining for field testing new equipment from Scandinavia and elsewhere that seemed to hold promise for Ontario conditions, primarily in the north. Thus, testing was begun of the Brackekultivator from Sweden and the Vako-Visko rotary furrower from Finland.Site preparation treatments that create raised planting spots have commonly improved outplant performance on sites subject to low soil temperature and excess soil moisture. Mounding can certainly have a big influence on soil temperature. Draper et al. (1985),[189] for instance, documented this as well as the effect it had on root growth of outplants (Table 30).The mounds warmed up quickest, and at soil depths of 0.5 cm and 10 cm averaged 10 and 7 °C higher, respectively, than in the control. On sunny days, daytime surface temperature maxima on the mound and organic mat reached 25 °C to 60 °C, depending on soil wetness and shading. Mounds reached mean soil temperatures of 10 °C at 10 cm depth 5 days after planting, but the control did not reach that temperature until 58 days after planting. During the first growing season, mounds had 3 times as many days with a mean soil temperature greater than 10 °C than did the control microsites.Draper et al.'s (1985)[189] mounds received 5 times the amount of photosynthetically active radiation (PAR) summed over all sampled microsites throughout the first growing season; the control treatment consistently received about 14% of daily background PAR, while mounds received over 70%. By November, fall frosts had reduced shading, eliminating the differential. Quite apart from its effect on temperature, incident radiation is also important photosynthetically. The average control microsite was exposed to levels of light above the compensation point for only 3 hours, i.e., one-quarter of the daily light period, whereas mounds received light above the compensation point for 11 hours, i.e., 86% of the same daily period. Assuming that incident light in the 100-600 µEm‾²s‾1 intensity range is the most important for photosynthesis, the mounds received over 4 times the total daily light energy that reached the control microsites.With linear site preparation, orientation is sometimes dictated by topography or other considerations, but where the orientation can be chosen, it can make a significant difference. A disk-trenching experiment in the Sub-boreal Spruce Zone in interior British Columbia investigated the effect on growth of young outplants (lodgepole pine) in 13 microsite planting positions: berm, hinge, and trench; in north, south, east, and west aspects, as well as in untreated locations between the furrows.[190] Tenth-year stem volumes of trees on south, east, and west-facing microsites were significantly greater than those of trees on north-facing and untreated microsites. However, planting spot selection was seen to be more important overall than trench orientation.In a Minnesota study, the N–S strips accumulated more snow, but the snow melted faster than on E–W strips in the first year after felling.[191] Snow-melt was faster on strips near the centre of the strip-felled area than on border strips adjoining the intact stand. The strips, 50 feet (15.24 m) wide, alternating with uncut strips 16 feet (4.88 m) wide, were felled in a Pinus resinosa stand, aged 90 to 100 years.
Norse mythology
Norse mythology is the body of myths of the North Germanic peoples , stemming from Norse paganism and continuing after the Christianization of Scandinavia and into the Scandinavian folklore of the modern period. The northernmost extension of Germanic mythology, Norse mythology consists of tales of various deities, beings, and heroes derived from numerous sources from both before and after the pagan period, including medieval manuscripts, archaeological representations, and folk tradition.Numerous gods are mentioned in the source texts such as the hammer-wielding, humanity-protecting thunder-god Thor, who relentlessly fights his foes; the one-eyed, raven-flanked god Odin, who craftily pursues knowledge throughout the worlds and bestowed among humanity the runic alphabet; the beautiful, seiðr-working, feathered cloak-clad goddess Freya who rides to battle to choose among the slain; the vengeful, skiing goddess Skaði, who prefers the wolf howls of the winter mountains to the seashore; the powerful god Njörð, who may calm both sea and fire and grant wealth and land; the god Frey, whose weather and farming associations bring peace and pleasure to humanity; the goddess Iðunn, who keeps apples that grant eternal youthfulness; the mysterious god Heimdall, who is born of nine mothers, can hear grass grow, has gold teeth, and possesses a resounding horn; the jötunn Loki, who brings tragedy to the gods by engineering the death of the goddess Frigg's beautiful son Baldr; and numerous other deities.Most of the surviving mythology centres on the plights of the gods and their interaction with various other beings, such as humanity and the jötnar, beings who may be friends, lovers, foes or family members of the gods. The cosmos in Norse mythology consists of Nine Worlds that flank a central cosmological tree, Yggdrasil. Units of time and elements of the cosmology are personified as deities or beings. Various forms of a creation myth are recounted, where the world is created from the flesh of the primordial being Ymir, and the first two humans are Ask and Embla. These worlds are foretold to be reborn after the events of Ragnarök when an immense battle occurs between the gods and their enemies, and the world is enveloped in flames, only to be reborn anew. There the surviving gods will meet, and the land will be fertile and green, and two humans will repopulate the world.Norse mythology has been the subject of scholarly discourse since the 17th century when key texts were brought to the attention of the intellectual circles of Europe. By way of comparative mythology and historical linguistics, scholars have identified elements of Germanic mythology reaching as far back as Proto-Indo-European mythology. In the modern period, the Romanticist Viking revival re-awoke an interest in the subject matter, and references to Norse mythology may now be found throughout modern popular culture. The myths have further been revived in a religious context among adherents of Germanic Neopaganism.The historical religion of the Norse people is commonly referred to as Norse mythology. In certain literature the terms Scandinavian mythology[1][2][3] or Nordic mythology have been used.[4]Norse mythology is primarily attested in dialects of Old Norse, a North Germanic language spoken by the Scandinavian people during the European Middle Ages, and the ancestor of modern Scandinavian languages. The majority of these Old Norse texts were created in Iceland, where the oral tradition stemming from the pre-Christian inhabitants of the island was collected and recorded in manuscripts. This occurred primarily in the 13th century. These texts include the Prose Edda, composed in the 13th century by Snorri Sturluson, and the Poetic Edda, a collection of poems from earlier traditional material anonymously compiled in the 13th century.[5]The Prose Edda was composed as a prose manual for producing skaldic poetry—traditional Old Norse poetry composed by skalds. Originally composed and transmitted orally, skaldic poetry utilizes alliterative verse, kennings, and various metrical forms. The Prose Edda presents numerous examples of works by various skalds from before and after the Christianization process and also frequently refers back to the poems found in the Poetic Edda. The Poetic Edda consists almost entirely of poems, with some prose narrative added, and this poetry—Eddic poetry—utilizes fewer kennings. In comparison to skaldic poetry, Eddic poetry is relatively unadorned.[5]The Prose Edda features layers of euhemerization, a process in which deities and supernatural beings are presented as having been either actual, magic-wielding human beings who have been deified in time or beings demonized by way of Christian mythology.[6] Texts such as Heimskringla, composed in the 13th century by Snorri and Gesta Danorum, composed in Latin by Saxo Grammaticus in Denmark in the 12th century, are the results of heavy amounts of euhemerization.[7]Numerous further texts, such as the sagas, provide further information. The saga corpus consists of thousands of tales recorded in Old Norse ranging from Icelandic family histories (Sagas of Icelanders) to Migration period tales mentioning historic figures such as Attila the Hun (legendary sagas). Objects and monuments such as the Rök Runestone and the Kvinneby amulet feature runic inscriptions—texts written in the runic alphabet, the indigenous alphabet of the Germanic peoples—that mention figures and events from Norse mythology.[8]Objects from the archaeological record may also be interpreted as depictions of subjects from Norse mythology, such as amulets of the god Thor's hammer Mjölnir found among pagan burials and small silver female figures interpreted as valkyries or dísir, beings associated with war, fate or ancestor cults.[9] By way of historical linguistics and comparative mythology, comparisons to other attested branches of Germanic mythology (such as the Old High German Merseburg Incantations) may also lend insight.[10] Wider comparisons to the mythology of other Indo-European peoples by scholars has resulted in the potential reconstruction of far earlier myths.[11]Of the mythical tales and poems that are presumed to have existed during the Middle Ages, Viking Age, Migration Period, and prior, only a tiny amount of poems and tales survive.[12] Later sources reaching into the modern period, such as a medieval charm recorded as used by the Norwegian woman Ragnhild Tregagås—convicted of witchcraft in Norway in the 14th century—and spells found in the 17th century Icelandic Galdrabók grimoire also sometimes make references to Norse mythology.[13] Other traces, such as place names bearing the names of gods may provide further information about deities, such as a potential association between deities based on the placement of locations bearing their names, their local popularity, and associations with geological features.[14]Central to accounts of Norse mythology are the plights of the gods and their interaction with various other beings, such as with the jötnar, who may be friends, lovers, foes or family members of the gods. Numerous gods are mentioned in the source texts. As evidenced by records of personal names and place names, the most popular god among the Scandinavians during the Viking Age was Thor, who is portrayed as unrelentingly pursuing his foes, his mountain-crushing, thunderous hammer Mjölnir in hand. In the mythology, Thor lays waste to numerous jötnar who are foes to the gods or humanity, and is wed to the beautiful, golden-haired goddess Sif.[15]The god Odin is also frequently mentioned in surviving texts. One-eyed, wolf and raven-flanked, and spear in hand, Odin pursues knowledge throughout the worlds. In an act of self-sacrifice, Odin is described as having hanged himself on the cosmological tree Yggdrasil to gain knowledge of the runic alphabet, which he passed on to humanity, and is associated closely with death, wisdom, and poetry. Odin has a strong association with death; Odin is portrayed as the ruler of Valhalla, where valkyries carry half of those slain in battle. Odin's wife is the powerful goddess Frigg who can see the future but tells no one, and together they have a beloved son, Baldr. After a series of dreams had by Baldr of his impending death, his death is engineered by Loki, and Baldr thereafter resides in Hel, a realm ruled over by a goddess of the same name.[16]Odin must share half of his share of the dead with a powerful goddess; Freyja. She is beautiful, sensual, wears a feathered cloak, and practices seiðr. She rides to battle to choose among the slain and brings her chosen to her afterlife field Fólkvangr. Freyja weeps for her missing husband Óðr, and seeks after him in faraway lands.[17] Freyja's brother, the god Freyr, is also frequently mentioned in surviving texts, and in his association with the weather, royalty, human sexuality, and agriculture brings peace and pleasure to humanity. Deeply lovesick after catching sight of the beautiful jötunn Gerðr, Freyr seeks and wins her love, yet at the price of his future doom.[18] Their father is the powerful god Njörðr. Njörðr is strongly associated with ships and seafaring, and so also wealth and prosperity. Freyja and Freyr's mother is Njörðr's sister (her name is unprovided in the source material). However, there is more information about his pairing with the skiing and hunting goddess Skaði. Their relationship is ill-fated, as Skaði cannot stand to be away from her beloved mountains, nor Njörðr from the seashore.[19] Together, Freyja, Freyr, and Njörðr form a portion of gods known as the Vanir. While the Aesir and the Vanir retain distinct identification, they came together as the result of the Aesir–Vanir War.[20]While they receive less mention, numerous other gods and goddesses appear in the source material. (For a list of these deities, see List of Germanic deities.) Some of the gods heard less of include the apple-bearing goddess Iðunn and her husband, the skaldic god Bragi; the gold-toothed god Heimdallr, born of nine mothers; the ancient god Týr, who lost a hand while binding the great wolf Fenrir; and the goddess Gefjon, who formed modern day Zealand, Denmark.[21]Various beings outside of the gods are mentioned. Elves and dwarfs are commonly mentioned and appear to be connected, but their attributes are vague and the relation between the two is ambiguous. Elves are described as radiant and beautiful, whereas dwarfs often act as earthen smiths.[22] A group of beings variously described as jötnar, thursar, and trolls (in English these are all often glossed as "giants") frequently appear. These beings may either aid, deter, or take their place among the gods.[23] The norns, dísir, and aforementioned valkyries also receive frequent mention. While their functions and roles may overlap and differ, all are collective female beings associated with fate.[24]The cosmology of the worlds in which all beings inhabit—nine in total—centers around a cosmological tree, Yggdrasil. The gods inhabit the heavenly realm of Asgard whereas humanity inhabits Midgard, a region in the center of the cosmos. Outside of the gods, humanity, and the jötnar, these Nine Worlds are inhabited by beings, such as elves and dwarfs. Travel between the worlds is frequently recounted in the myths, where the gods and other beings may interact directly with humanity. Numerous creatures live on Yggdrasil, such as the insulting messenger squirrel Ratatoskr and the perching hawk Veðrfölnir. The tree itself has three major roots, and at the base of one of these roots live a trio of Norns.[25] Elements of the cosmos are personified, such as the Sun (Sól, a goddess), the Moon (Máni, a god), and Earth (Jörð, a goddess), as well as units of time, such as day (Dagr, a god) and night (Nótt, a jötunn).[26]The afterlife is a complex matter in Norse mythology. The dead may go to the murky realm of Hel—a realm ruled over by a female being of the same name, may be ferried away by valkyries to Odin's martial hall Valhalla, or may be chosen by the goddess Freyja to dwell in her field Fólkvangr.[27] The goddess Rán may claim those that die at sea, and the goddess Gefjon is said to be attended by virgins upon their death.[28] Texts also make reference to reincarnation.[29] Time itself is presented between cyclic and linear, and some scholars have argued that cyclic time was the original format for the mythology.[30] Various forms of a cosmological creation story are provided in Icelandic sources, and references to a future destruction and rebirth of the world—Ragnarok—are frequently mentioned in some texts.[31]According to the Poetic Edda poem Völuspá and the Prose Edda, the first human couple consisted of Ask and Embla; driftwood found by a trio of gods and imbued with life in the form of three gifts. After the cataclysm of Ragnarok, this process is mirrored in the survival of two humans from a wood; Líf and Lífþrasir. From this two humankind are foretold to repopulate the new, green earth.[32]Numerous heroes appear in Norse mythology and are celebrated in a variety of poems, songs, and narratives.  Within the Prose and Poetic Edda, notable humans include Gylfi, the first King of Sweden, in the Gylfaginning, King Geirröth in the [Grímnismál], and two peasant children Þjálfi and Röskva, who are tricked into bond service to Thor by Loki and appear in Skáldskaparmál and the Gylfaginning.  The Prose Edda also describes the afterlife for humans, with honorable warriors feasting and battling endlessly in Valhalla, while those who died dishonorably or out of battle were sent to Niffelheim.With the widespread publication of Norse myths and legends at this time, references to the Norse gods and heroes spread into European literary culture, especially in Scandinavia, Germany, and Britain. In the later 20th century, references to Norse mythology became common in science fiction and fantasy literature, role-playing games, and eventually other cultural products such as comic books and Japanese animation. Traces of the religion can also be found in music and has its own genre, viking metal. Bands such as Amon Amarth, Bathory and Månegarm generally sing about Norse mythology.
Triassic
The Triassic ( /traɪˈæsɪk/) is a geologic period and system which spans 50.6 million years from the end of the Permian Period 251.9 million years ago (Mya), to the beginning of the Jurassic Period 201.3 Mya.[8] The Triassic is the first period of the Mesozoic Era. Both the start and end of the period are marked by major extinction events.[9]The Triassic began in the wake of the Permian–Triassic extinction event, which left the Earth's biosphere impoverished; it would take well into the middle of this period for life to recover its former diversity. Therapsids and archosaurs were the chief terrestrial vertebrates during this time. A specialized subgroup of archosaurs, called dinosaurs, first appeared in the Late Triassic but did not become dominant until the succeeding Jurassic Period.[10]The first true mammals, themselves a specialized subgroup of therapsids, also evolved during this period, as well as the first flying vertebrates, the pterosaurs, who, like the dinosaurs, were a specialized subgroup of archosaurs. The vast supercontinent of Pangaea existed until the mid-Triassic, after which it began to gradually rift into two separate landmasses, Laurasia to the north and Gondwana to the south.The global climate during the Triassic was mostly hot and dry,[11] with deserts spanning much of Pangaea's interior. However, the climate shifted and became more humid as Pangaea began to drift apart. The end of the period was marked by yet another major mass extinction, the Triassic–Jurassic extinction event, that wiped out many groups and allowed dinosaurs to assume dominance in the Jurassic.The Triassic was named in 1834 by Friedrich von Alberti, after the three distinct rock layers (tri meaning "three") that are found throughout Germany and northwestern Europe—red beds, capped by marine limestone, followed by a series of terrestrial mud- and sandstones—called the "Trias".[12]The Triassic is usually separated into Early, Middle, and Late Triassic Epochs, and the corresponding rocks are referred to as Lower, Middle, or Upper Triassic. The faunal stages from the youngest to oldest are:During the Triassic, almost all the Earth's land mass was concentrated into a single supercontinent centered more or less on the equator and spanning from pole to pole, called Pangaea ("all the land"). From the east, along the equator, the Tethys sea penetrated Pangaea, causing the Paleo-Tethys Ocean to be closed.Later in the mid-Triassic a similar sea penetrated along the equator from the west. The remaining shores were surrounded by the world-ocean known as Panthalassa ("all the sea"). All the deep-ocean sediments laid down during the Triassic have disappeared through subduction of oceanic plates; thus, very little is known of the Triassic open ocean.The supercontinent Pangaea was rifting during the Triassic—especially late in that period—but had not yet separated. The first nonmarine sediments in the rift that marks the initial break-up of Pangaea, which separated New Jersey from Morocco, are of Late Triassic age; in the U.S., these thick sediments comprise the Newark Group.[14]Because a super-continental mass has less shoreline compared to one broken up, Triassic marine deposits are globally relatively rare, despite their prominence in Western Europe, where the Triassic was first studied. In North America, for example, marine deposits are limited to a few exposures in the west. Thus Triassic stratigraphy is mostly based on organisms that lived in lagoons and hypersaline environments, such as Estheria crustaceans.At the beginning of the Mesozoic Era, Africa was joined with Earth's other continents in Pangaea.[15] Africa shared the supercontinent's relatively uniform fauna which was dominated by theropods, prosauropods and primitive ornithischians by the close of the Triassic period.[15] Late Triassic fossils are found throughout Africa, but are more common in the south than north.[15] The time boundary separating the Permian and Triassic marks the advent of an extinction event with global impact, although African strata from this time period have not been thoroughly studied.[15]During the Triassic peneplains are thought to have formed in what is now Norway and southern Sweden.[16][17][18] Remnants of this peneplain can be traced as a tilted summit accordance in the Swedish West Coast.[16] In northern Norway Triassic peneplains may have been buried in sediments to be then re-exposed as coastal plains called strandflats.[17] Dating of illite clay from a strandflat of Bømlo, southern Norway, have shown that landscape there became weathered in Late Triassic times (c. 210 million years ago) with the landscape likely also being shaped during that time.[19]At Paleorrota geopark, located in Rio Grande do Sul, Brazil, the Santa Maria Formation and Caturrita Formations are exposed. In these formations, one of the earliest dinosaurs, Staurikosaurus, as well as the mammal ancestors Brasilitherium and Brasilodon have been discovered.The Triassic continental interior climate was generally hot and dry, so that typical deposits are red bed sandstones and evaporites. There is no evidence of glaciation at or near either pole; in fact, the polar regions were apparently moist and temperate, providing a climate suitable for forests and vertebrates, including reptiles. Pangaea's large size limited the moderating effect of the global ocean; its continental climate was highly seasonal, with very hot summers and cold winters.[20] The strong contrast between the Pangea supercontinent and the global ocean triggered intense cross-equatorial monsoons.[20]The Triassic may have mostly been a dry period, but evidence exists that it was punctuated by several episodes of increased rainfall in tropical and subtropical latitudes of the Tethys Sea and its surrounding land.[21] Sediments and fossils suggestive of a more humid climate are known from the Anisian to Ladinian of the Tethysian domain, and from the Carnian and Rhaetian of a larger area that includes also the Boreal domain (e.g., Svalbard Islands), the North American continent, the South China block and Argentina.The best studied of such episodes of humid climate, and probably the most intense and widespread, was the Carnian Pluvial Event.Three categories of organisms can be distinguished in the Triassic record: survivors from the Permian–Triassic extinction event, new groups which flourished briefly, and other new groups which went on to dominate the Mesozoic Era.On land, the surviving vascular plants included the lycophytes, the dominant cycadophytes, ginkgophyta (represented in modern times by Ginkgo biloba), ferns, horsetails and glossopterids. The spermatophytes, or seed plants, came to dominate the terrestrial flora: in the northern hemisphere, conifers, ferns and bennettitales flourished. Glossopteris (a seed fern) was the dominant southern hemisphere tree during the Early Triassic period.In marine environments, new modern types of corals appeared in the Early Triassic, forming small patches of reefs of modest extent compared to the great reef systems of Devonian or modern times. Serpulids appeared in the Middle Triassic.[22] Microconchids were abundant. The shelled cephalopods called ammonites recovered, diversifying from a single line that survived the Permian extinction.The fish fauna was remarkably uniform, suggesting that very few families survived the Permian extinction. There were also many types of marine reptiles. These included the Sauropterygia, which featured pachypleurosaurus and nothosaurs (both common during the Middle Triassic, especially in the Tethys region), placodonts, and the first plesiosaurs. The first of the lizardlike Thalattosauria (askeptosaurs) and the highly successful ichthyosaurs, which appeared in Early Triassic seas soon diversified, and some eventually developed to huge size during the Late Triassic. Subequatorial saurichthyids have also been described in Early Triassic strata.[23]Groups of terrestrial fauna, which appeared in the Triassic period or achieved a new level of evolutionary success during it include:[24][25] The Permian–Triassic extinction devastated terrestrial life. Biodiversity rebounded as the surviving species repopulated empty terrain, but these were short-lived. Diverse communities with complex food-web structures took 30 million years to reestablish.[9]Temnospondyl amphibians were among those groups that survived the Permian–Triassic extinction; some lineages (e.g. trematosaurs) flourished briefly in the Early Triassic, while others (e.g. capitosaurs) remained successful throughout the whole period, or only came to prominence in the Late Triassic (e.g. plagiosaurs, metoposaurs). As for other amphibians, the first Lissamphibia, progenitors of first frogs, are known from the Early Triassic, but the group as a whole did not become common until the Jurassic, when the temnospondyls had become very rare. Other survivors the Chroniosuchia and Embolomeri were more closely related to amniotes than temnospondyls. Those became extinct after some million years.Most of the Reptiliomorpha, stem-amniotes that gave rise to the amniotes, disappeared in Triassic, but two water-dwelling groups survived; Embolomeri that only survived into the early part of the period, and the Chroniosuchia, which survived until the end of Triassic.Archosauromorph reptiles, especially archosaurs, progressively replaced the synapsids that had dominated the previous Permian period. The Cynognathus was the characteristic top predator in earlier Triassic (Olenekian and Anisian) on Gondwana. Both kannemeyeriid dicynodonts and gomphodont cynodonts remained important herbivores during much of the period, and ecteniniids played a role as large-sized, cursorial predators in the Late Triassic. During the Carnian (early part of the Late Triassic), some advanced cynodonts gave rise to the first mammals. At the same time the Ornithodira, which until then had been small and insignificant, evolved into pterosaurs and a variety of dinosaurs. The Crurotarsi were the other important archosaur clade, and during the Late Triassic these also reached the height of their diversity, with various groups including the phytosaurs, aetosaurs, several distinct lineages of Rauisuchia, and the first crocodylians (the Sphenosuchia). Meanwhile, the stocky herbivorous rhynchosaurs and the small to medium-sized insectivorous or piscivorous Prolacertiformes were important basal archosauromorph groups throughout most of the Triassic.Among other reptiles, the earliest turtles, like Proganochelys and Proterochersis, appeared during the Norian Age (Stage) of the Late Triassic Period. The Lepidosauromorpha, specifically the Sphenodontia, are first found in the fossil record of the earlier Carnian Age. The Procolophonidae were an important group of small lizard-like herbivores.During the Triassic, archosaurs displaced therapsids as the dominant amniotes. This "Triassic Takeover" may have contributed to the evolution of mammals by forcing the surviving therapsids and their mammaliaform successors to live as small, mainly nocturnal insectivores. Nocturnal life may have forced the mammaliaforms to develop fur and a higher metabolic rate.[29]Postosuchus An apex predator of its time which preyed on anything smaller than itselfStaurikosaurus feeding on a dicynodont, in geopark PaleorrotaLystrosaurus was the most common land vertebrate during the Early Triassic, when animal life had been greatly diminishedReconstruction of Proterosuchus, a genus of crocodile-like carnivorous reptile that existed in the Early TriassicCynognathus was a mammal-like cynodont from the Early Triassic. The first true mammals evolved during this periodPlateosaurus was an early sauropodomorph, or "prosauropod", of the Late TriassicCoelophysis, one of the first dinosaurs, appeared in the Late TriassicLife reconstruction of Tanystropheus longobardicusThe Monte San Giorgio lagerstätte, now in the Lake Lugano region of northern Italy and Switzerland, was in Triassic times a lagoon behind reefs with an anoxic bottom layer, so there were no scavengers and little turbulence to disturb fossilization, a situation that can be compared to the better-known Jurassic Solnhofen Limestone lagerstätte.The remains of fish and various marine reptiles (including the common pachypleurosaur Neusticosaurus, and the bizarre long-necked archosauromorph Tanystropheus), along with some terrestrial forms like Ticinosuchus and Macrocnemus, have been recovered from this locality. All these fossils date from the Anisian/Ladinian transition (about 237 million years ago).The Triassic period ended with a mass extinction, which was particularly severe in the oceans; the conodonts disappeared, as did all the marine reptiles except ichthyosaurs and plesiosaurs. Invertebrates like brachiopods, gastropods, and molluscs were severely affected. In the oceans, 22% of marine families and possibly about half of marine genera went missing.Though the end-Triassic extinction event was not equally devastating in all terrestrial ecosystems, several important clades of crurotarsans (large archosaurian reptiles previously grouped together as the thecodonts) disappeared, as did most of the large labyrinthodont amphibians, groups of small reptiles, and some synapsids (except for the proto-mammals). Some of the early, primitive dinosaurs also became extinct, but more adaptive ones survived to evolve into the Jurassic. Surviving plants that went on to dominate the Mesozoic world included modern conifers and cycadeoids.The cause of the Late Triassic extinction is uncertain. It was accompanied by huge volcanic eruptions that occurred as the supercontinent Pangaea began to break apart about 202 to 191 million years ago (40Ar/39Ar dates),[41] forming the Central Atlantic Magmatic Province (CAMP),[42] one of the largest known inland volcanic events since the planet had first cooled and stabilized. Other possible but less likely causes for the extinction events include global cooling or even a bolide impact, for which an impact crater containing Manicouagan Reservoir in Quebec, Canada, has been singled out. However, the Manicouagan impact melt has been dated to 214±1 Mya. The date of the Triassic-Jurassic boundary has also been more accurately fixed recently, at 201.3 Mya. Both dates are gaining accuracy by using more accurate forms of radiometric dating, in particular the decay of uranium to lead in zircons formed at time of the impact. So, the evidence suggests the Manicouagan impact preceded the end of the Triassic by approximately 10±2 Ma. It could, therefore, not be the immediate cause of the observed mass extinction.[43]The number of Late Triassic extinctions is disputed. Some studies suggest that there are at least two periods of extinction towards the end of the Triassic, separated by 12 to 17 million years. But arguing against this is a recent study of North American faunas. In the Petrified Forest of northeast Arizona there is a unique sequence of late Carnian-early Norian terrestrial sediments. An analysis in 2002 found no significant change in the paleoenvironment.[44] Phytosaurs, the most common fossils there, experienced a change-over only at the genus level, and the number of species remained the same. Some aetosaurs, the next most common tetrapods, and early dinosaurs, passed through unchanged. However, both phytosaurs and aetosaurs were among the groups of archosaur reptiles completely wiped out by the end-Triassic extinction event.It seems likely then that there was some sort of end-Carnian extinction, when several herbivorous archosauromorph groups died out, while the large herbivorous therapsids—the kannemeyeriid dicynodonts and the traversodont cynodonts—were much reduced in the northern half of Pangaea (Laurasia).These extinctions within the Triassic and at its end allowed the dinosaurs to expand into many niches that had become unoccupied. Dinosaurs became increasingly dominant, abundant and diverse, and remained that way for the next 150 million years. The true "Age of Dinosaurs" is during the following Jurassic and Cretaceous periods, rather than the Triassic.
Iroquois
The Iroquois (/ˈɪrəkwɔɪ/ or /ˈɪrəkwɑː/) or Haudenosaunee (/ˈhoʊdənoʊˈʃoʊni/)[1] (People of the Longhouse) are a historically powerful northeast Native American confederacy. They were known during the colonial years to the French as the Iroquois League, and later as the Iroquois Confederacy, and to the English as the Five Nations, comprising the Mohawk, Onondaga, Oneida, Cayuga, and Seneca. After 1722, they accepted the Tuscarora people from the Southeast into their confederacy and became known as the Six Nations.The Iroquois have absorbed many other peoples into their tribes as a result of warfare, adoption of captives, and by offering shelter to displaced peoples. Culturally all are considered members of the clans and tribes into which they are adopted by families.The historic St. Lawrence Iroquoians, Wyandot (Huron), Erie, and Susquehannock, all independent peoples, also spoke Iroquoian languages. In the larger sense of linguistic families, they are often considered Iroquoian peoples because of their similar languages and cultures, all culturally and linguistically descended from the Proto-Iroquoian people and language; however, they were traditionally enemies of the nations in the Iroquois League.[2] In addition, Cherokee is an Iroquoian language. The Cherokee people are believed to have migrated south from the Great Lakes area in ancient times, settling in the backcountry of the Southeast United States, including what is now Tennessee.In 2010, more than 45,000 enrolled Six Nations people lived in Canada, and about 80,000 in the United States.[citation needed]The most common name for the confederacy, Iroquois, is of somewhat obscure origin. The first time it appears in writing is in the account of Samuel de Champlain of his journey to Tadoussac in 1603, where it occurs as "Irocois".[3] Other spellings appearing in the earliest sources include "Erocoise", "Hiroquois", "Hyroquoise", "Irecoies", "Iriquois", "Iroquaes", "Irroquois", and "Yroquois", as the French transliterated the term into their own phonetic system.[4] In the French spoken at the time, this would have been pronounced as [irokwe] or [irokwɛ].[5] Over the years, several competing theories have been proposed for this name's ultimate origin—the earliest such proposal is by the Jesuit priest Pierre François Xavier de Charlevoix, who wrote in 1744:The name Iroquois is purely French, and is formed from the [Iroquoian-language] term Hiro or Hero, which means I have said—with which these Indians close all their addresses, as the Latins did of old with their dixi—and of Koué, which is a cry sometimes of sadness, when it is prolonged, and sometimes of joy, when it is pronounced shorter.[6]In 1883, Horatio Hale wrote that the Charlevoix etymology was dubious, and that "no other nation or tribe of which we have any knowledge has ever borne a name composed in this whimsical fashion".[6] Hale suggested instead that the term came from Huron, and was cognate with Mohawk ierokwa "they who smoke" or Cayuga iakwai "a bear". J.N.B. Hewitt responded to Hale's etymology in 1888 by expressing doubt that either of those words exist in the respective languages. His preferred etymology at the time was from Montagnais irin "true, real" and ako "snake", plus the French -ois suffix, though he later revised his theory to state that the source was Algonquin Iriⁿakhoiw.[7][8] However, none of these etymologies gained widespread acceptance. By 1978 Ives Goddard could write: "No such form is attested in any Indian language as a name for any Iroquoian group, and the ultimate origin and meaning of the name are unknown."[9]A more modern etymology is that advocated by Gordon M. Day in 1968, who elaborates upon an earlier etymology given by Charles Arnaud in 1880. Arnaud had claimed that the word came from Montagnais irnokué, meaning "terrible man", via the reduced form irokue. Day proposes a hypothetical Montagnais phrase irno kwédač, meaning "a man, an Iroquois", as the origin of this term. For the first element irno, Day cites cognates from other attested Montagnais dialects: irinou, iriniȣ, and ilnu; and for the second element kwédač he suggests a relation to kouetakiou, kȣetat-chiȣin, and goéṭètjg – names used by neighboring Algonquian tribes to refer to the Iroquois, Hurons, and Laurentians.[10]More recently, Peter Bakker has proposed a Basque origin for "Iroquois". Basque fishermen and whalers are known to have frequented the waters of the Northeast in the 1500s, so much so that a Basque-based pidgin developed for communication with the Algonquian tribes of the region. Bakker claims that it is unlikely that "-quois" derives from a root specifically used to refer to the Iroquois, citing as evidence that several other Indian tribes of the region were known to the French by names terminating in the same element, e.g. "Armouchiquois", "Charioquois", "Excomminquois", and "Souriquois". He proposes instead that the word derives from hilokoa (via the intermediate form irokoa), from the Basque roots hil "to kill", ko (the locative genitive suffix), and a (the definite article suffix). In favor of an original form beginning with /h/, Bakker cites alternate spellings such as "hyroquois" sometimes found in documents from the period, and the fact that in the Southern dialect of Basque, the word hil is pronounced il. He also argues that the /l/ was rendered as /r/ since the former is not attested in the phonemic inventory of any language in the region (including Maliseet, which developed an /l/ later). Thus the word according to Bakker is translatable as "the killer people". It is similar to other terms used by Eastern Algonquian tribes to refer to their enemy the Iroquois, which translate as "murderers".[11][12]The Five Nations historically referred to themselves by a different autonym, Haudenosaunee, meaning "People of the Longhouse".[13] It is also occasionally preferred by scholars of Native American history, who consider the name "Iroquois" to be derogatory in origin.[14] This name derives from two phonetically similar but etymologically distinct words in the Seneca language: Hodínöhšö:ni:h, meaning "those of the extended house," and Hodínöhsö:ni:h, meaning "house builders".[15][16][9] The name "Haudenosaunee" first appears in English in Lewis Henry Morgan (1851), where it is written as Ho-dé-no-sau-nee. The spelling "Hotinnonsionni" is also attested from later in the nineteenth century.[17][18] An alternate designation, Ganonsyoni, is occasionally encountered as well.[19] This term derives from the Mohawk kanǫhsyǫ́·ni ("the extended house"), or from a cognate expression in a related Iroquoian language, and is frequently encountered in earlier sources variously spelled "Kanosoni", "akwanoschioni", "Aquanuschioni", "Cannassoone", "Canossoone", "Ke-nunctioni", or "Konossioni".[18] More transparently, the Iroquois confederacy is also often referred to simply as the Six Nations (or, for the period before the entry of the Tuscarora in 1722, the Five Nations).[20] The word is Rotinonsionni in the Mohawk language.[21]The Iroquois Confederacy is believed to have been founded by the Peacemaker in 1142, bringing together five distinct nations in the southern Great Lakes area into "The Great League of Peace".[22] Each nation within this Iroquoian confederacy had a distinct language, territory, and function in the League. Iroquois influence at the peak of its power extended into present-day Canada, westward along the Great Lakes and down both sides of the Allegheny mountains into present-day Virginia and Kentucky and into the Ohio Valley.The League is governed by a Grand Council, an assembly of fifty chiefs or sachems, each representing one of the clans of one of the nations.[23]The original Iroquois League (as the French knew them) or Five Nations (as the British knew them), occupied large areas of present-day New York State up to the St. Lawrence River, west of the Hudson River, and south into northwestern Pennsylvania. From east to west, the League was composed of the Mohawk, Oneida, Onondaga, Cayuga, and Seneca nations. In or close to 1722, the Tuscarora tribe joined the League,[24] having migrated from the Carolinas after being displaced by Anglo-European settlement. Also an Iroquoian-speaking people, the Tuscarora were accepted into what became the Six Nations.Other independent Iroquoian-speaking peoples, such as the Erie, Susquehannock, Huron (Wendat) and Wyandot, lived at various times along the St. Lawrence River, and around the Great Lakes. In the American Southeast, the Cherokee were an Iroquoian-language people who had migrated to that area centuries before European contact. None of these was part of the Haudenosaunee. Those on the borders of Haudenosaunee territory in the Great Lakes region competed and warred with the member nations.When Europeans first arrived in North America, the Haudenosaunee were based in what is now the northeastern United States, primarily in what is referred to today as Central New York and Western New York, west of the Hudson River and through the Finger Lakes region, and upstate New York along the St. Lawrence River area downstream to today's Montreal.[25]French, Dutch and British colonists in both New France (Canada) and the what became the Thirteen Colonies recognized a need to gain favor with the Iroquois people, who occupied a significant portion of lands west of colonial settlements. Their first relations with them were for fur trading, which was favorable and became lucrative to both sides. The colonists also sought to establish positive relations to secure their settlement borders.For nearly 200 years the Iroquois were a powerful factor in North American colonial policy-making decisions. Alignment with Iroquois offered political and strategic advantages to the European colonies, but the Iroquois preserved considerable independence. Some of their people settled in mission villages along the St. Lawrence River, becoming more closely tied to the French. While they participated in French raids on Dutch and later English settlements, where some Mohawk and other Iroquois settled, in general the Iroquois resisted attacking their own peoples.The Iroquois remained a politically unique, undivided, large Native American polity up until the American Revolution. The League kept its treaty promises to the British Crown. But when the British were defeated, they ceded the Iroquois territory without consultation; many Iroquois had to abandon their lands in the Mohawk Valley and elsewhere and relocate in the northern lands retained by the British. The Crown gave them land in compensation for the 5 million acres they had lost in the south, but it was not equivalent to earlier territory.The Iroquois League has also been known as the "Iroquois Confederacy". Modern scholars distinguish between the League and the Confederacy.[26][27][28] According to this interpretation, the Iroquois League refers to the ceremonial and cultural institution embodied in the Grand Council, while the Iroquois Confederacy is the decentralized political and diplomatic entity that emerged in response to European colonization. According to that theory, "The League" still exists. The Confederacy dissolved after the defeat of the British and allied Iroquois nations in the American Revolutionary War.[26] Today's Iroquois/Six Nations people do not make any distinction between "The League" and "the Confederacy" and use the terms interchangeably, preferring the name Haudenosaunee Confederacy.After the defeat of the British, they ceded most of the Iroquois territory to the United States, without bringing their allies to the negotiating table. Many of the Iroquois migrated to Canada, forced out of New York because of hostility to the British allies in the aftermath of a fierce war. Those remaining in New York were required to live mostly on reservations. In 1784, a total of 6,000 Iroquois had to confront 240,000 New Yorkers, with land-hungry New Englanders poised to migrate west. "Oneidas alone, who were only 600 strong, owned six million acres, or about 2.4 million hectares. Iroquoia was a land rush waiting to happen."[29]In addition to the major cessions of Iroquois land, the Oneida and others who gained reservations in New York faced increasing pressures for their lands. By the War of 1812, they had lost control of considerable property.The League has since been compared to a modern-day example of anarcho-communism[30] or libertarian socialism.[31]Knowledge of Iroquois history stems from Haudenosaunee oral tradition, archaeological evidence, accounts from Jesuit missionaries, and subsequent European historians. Historian Scott Stevens credits the early modern European value for the written word over oral tradition and cultures as contributing to a prejudiced, racialized element within writings about the Iroquois that continued into the 19th century.[32] The historiography of the Iroquois peoples is a topic of much debate, especially regarding the American colonial period.[33][34]Jesuit accounts of the Iroquois portrayed them as savages because of comparisons to French culture; the Jesuits perceived them to lack government, law, letters, and religion.[35]:p.153 But the Jesuits made considerable effort to study their languages and cultures, and some came to respect them. A major problem with contemporary European sources from the 17th and 18th centuries, both French and British, was that Europeans, coming from a patriarchal society, did not understand the matrilineal kinship system of Iroquois society and the related power of women.[36] The Canadian historian D. Peter MacLeod, writing about the relationship between the Canadian Iroquois and the French in the time of the Seven Years' War, said: Most critically, the importance of clan mothers, who possessed considerable economic and political power within Canadian Iroquois communities, was blithely overlooked by patriarchal European scribes. Those references that do exist, show clan mothers meeting in council with their male counterparts to take decisions regarding war and peace and joining in delegations to confront the Onontio [the Iroquois term for the French governor-general] and the French leadership in Montreal, but only hint at the real influence wielded by these women".[36]Eighteenth-century English historiography focuses on the diplomatic relations with the Iroquois, supplemented by such images as John Verelst's Four Mohawk Kings, and publications such as the Anglo-Iroquoian treaty proceedings printed by Benjamin Franklin.[35]:p.161 One historical narrative persistent in the 19th and 20th centuries casts the Iroquois as "an expansive military and political power ... [who] subjugated their enemies by violent force and for almost two centuries acted as the fulcrum in the balance of power in colonial North America".[35]:p.148 Historian Scott Stevens noted that the Iroquois themselves began to influence the writing of their history in the 19th century, including Joseph Brant (Mohawk), and David Cusick (Tuscarora). John Arthur Gibson (Seneca, 1850–1912) was an important figure of his generation in recounting versions of Iroquois history in epics on the Peacemaker.[37] Notable women historians among the Iroquois emerged in the following decades, including Laura "Minnie" Kellog (Oneida, 1880–1949) and Alice Lee Jemison (Seneca, 1901–1964).[35]:p.162The Iroquois League was established prior to European contact, with the banding together of five of the many Iroquoian peoples who had emerged south of the Great Lakes.[38][a] Reliable sources link the origins of the Iroquois confederacy to 1142 and an agricultural shift when corn was adopted as a staple crop.[39] Many archaeologists and anthropologists believe that the League was formed about 1450.[40][41] Arguments have been made for an earlier date.[note1 1] One theory argues that the League formed shortly after a solar eclipse on August 31, 1142, an event thought to be expressed in oral tradition about the League's origins.[42][43][44] They subsequently created a highly egalitarian society. One British colonial administrator declared in 1749 that the Iroquois had "such absolute Notions of Liberty that they allow no Kind of Superiority of one over another, and banish all Servitude from their Territories."[45]Anthropologist Dean Snow argues that the archaeological evidence does not support a date earlier than 1450. He has said that recent claims for a much earlier date "may be for contemporary political purposes".[46]:p.231 In contrast, other scholars note that at the time when anthropological studies were made, researchers consulted only male informants, although the Iroquois people had distinct oral traditions held by males and females. Thus half of the historical story, that told by women, was lost.[47] For this reason, origin tales tend to emphasize Deganawidah and Hiawatha, while the role of Jigonsaseh largely remains unknown because this part of the oral history was held by women.[47]According to oral traditions, the League was formed through the efforts of two men and one woman. They were Dekanawida, sometimes known as the Great Peacemaker, Hiawatha, and Jigonhsasee, known as the Mother of Nations, whose home acted as a sort of United Nations. They brought the Peacemaker's message, known as the Great Law of Peace, to the squabbling Iroquoian nations, who were fighting, raiding and feuding with one another and other tribes, both Algonkian and Iroquoian. Five nations originally joined as the League, giving rise to the many historic references to Five Nations of the Iroquois[b] or as often, just The Five Nations.[38] With the addition of the southern Tuscarora in the 18th century, these original five tribes are the ones that still compose the Haudenosaunee in the early 21st century: the Mohawk, Onondaga, Oneida, Cayuga, and Seneca.Other Iroquoian-language peoples,[48] including the populous Wyandot (Huron), with related social organization and cultures, became extinct as tribes as a result of disease and war.[c] They did not join the League when invited[d] and were much reduced after the Beaver Wars and high mortality from Eurasian infectious diseases. While the First Nations and Native Americans sometimes tried to remain neutral in the various colonial frontier wars, some also allied with one nation or another, through the French and Indian War. The Six Nations were split in their alliances between the French and British in that war, the North American front of the Seven Years' War. In warfare the tribes were decentralized, and often bands acted independently.According to legend, an evil Onondaga chieftain named Tadodaho was the last converted to the ways of peace by The Great Peacemaker and Hiawatha. He was offered the position as the titular chair of the League's Council, representing the unity of all nations of the League.[49] This is said to have occurred at Onondaga Lake near present-day Syracuse, New York. The title Tadodaho is still used for the League's chair, the fiftieth chief who sits with the Onondaga in council.With the formation of the League, internal conflicts were minimized. The council of fifty thereafter ruled on disputes, seeking consensus in their decisions.[38] Raids within the member tribes ended, and they directed warfare against competitors. This allowed the Iroquois to increase in numbers while their rivals declined.[38] The political cohesion of the Iroquois rapidly became one of the strongest forces in 17th- and 18th-century northeastern North America. The confederacy did not speak for all five tribes, which continued to act independently. But about 1678,[38] the council exerted more power in negotiations with the colonial governments of Pennsylvania and New York.[38] Thereafter, the editors of American Heritage write the Iroquois became very adroit at playing the French off against the British,[38] as individual tribes had played the Swedes, Dutch, and English.[38] The editors of American Heritage magazine suggest the Iroquois spokesmen were as politically sophisticated as many a modern politician.[38]As has been noted above, other Iroquoian-language peoples were encountered by early European colonists. While the tribes raided each other, they also traded with the members of the Iroquois who were nearby.[38] The explorer Robert La Salle in the 17th century identified the Mosopelea as among the Ohio Valley peoples defeated by the Iroquois in the early 1670s.[50]  The Erie and peoples of the upper Allegheny valley declined earlier during the Beaver Wars. By 1676 the Susquehannock[e] were known to be broken as a power from the effects of three years of epidemic disease, war with the Iroquois, and frontier battles, as settlers took advantage of the weakened tribe.[38]According to one theory of early Iroquois history, after becoming united in the League, the Iroquois invaded the Ohio River Valley in the territories that would become the eastern Ohio Country down as far as present-day Kentucky to seek additional hunting grounds. They displaced about 1200 Siouan-speaking tribepeople of the Ohio River valley, such as the Quapaw (Akansea), Ofo (Mosopelea), and Tutelo and other closely related tribes out of the region. These tribes migrated to regions around the Mississippi River and the piedmont regions of the east coast.[51]In Reflections in Bullough's Pond, historian Diana Muir argues that the pre-contact Iroquois were an imperialist, expansionist culture whose cultivation of the corn/beans/squash agricultural complex enabled them to support a large population. They made war primarily against neighboring Algonquian peoples. Muir uses archaeological data to argue that the Iroquois expansion onto Algonquian lands was checked by the Algonquian adoption of agriculture. This enabled them to support their own populations large enough to have sufficient warriors to defend against the threat of Iroquois conquest.[52] The People of the Confederacy dispute whether any of this historical interpretation relates to the League of the Great Peace which they contend is the foundation of their heritage.[citation needed]The Iroquois may be the Kwedech described in the oral legends of the Mi'kmaq nation of Eastern Canada. These legends relate that the Mi'kmaq in the late pre-contact period had gradually driven their enemies – the Kwedech – westward across New Brunswick, and finally out of the Lower St. Lawrence River region. The Mi'kmaq named the last-conquered land Gespedeg or "last land," from which the French derived Gaspé. The "Kwedech" are generally considered to have been Iroquois, specifically the Mohawk; their expulsion from Gaspé by the Mi'kmaq has been estimated as occurring c. 1535–1600.[53][page needed]Around 1535, Jacques Cartier reported Iroquoian-speaking groups on the Gaspé peninsula and along the St. Lawrence River. Archeologists and anthropologists have defined the St. Lawrence Iroquoians as a distinct and separate group (and possibly several discrete groups), living in the villages of Hochelaga and others nearby (near present-day Montreal), which had been visited by Cartier. By 1608, when Samuel de Champlain visited the area, that part of the St. Lawrence River valley had no settlements, but was controlled by the Mohawk as a hunting ground. The fate of the Iroquoian people that Cartier encountered remains a mystery, and all that can be stated for certain is when Champlain arrived, they were gone.[54] On the Gaspé peninsula, Champlain encountered Algonquian-speaking groups. The precise identity of any of these groups is still debated. On 29 July 1609, Champlain assisted his allies in defeating a Mohawk war party by the shores of what is now called Lake Champlain, and again in June 1610, Champlain fought against the Mohawks.[55]The Iroquois became well known in the southern colonies in the 17th century by this time. After the first English settlement in Jamestown, Virginia (1607), numerous 17th-century accounts describe a powerful people known to the Powhatan Confederacy as the Massawomeck, and to the French as the Antouhonoron. They were said to come from the north, beyond the Susquehannock territory. Historians have often identified the Massawomeck / Antouhonoron as the Haudenosaunee.In 1649, an Iroquois war party, consisting mostly of Senecas and Mohawks, destroyed the Huron village of Wendake. In turn, this ultimately resulted in the breakup of the Huron nation. With no northern enemy remaining, the Iroquois turned their forces on the Neutral Nations on the north shore of Lakes Erie and Ontario, the Susquehannocks, their southern neighbor. Then they destroyed other Iroquoian-language tribes, including the Erie, to the west, in 1654, over competition for the fur trade.[56][page needed] Then they destroyed the Mohicans. After their victories, they reigned supreme in an area from the Mississippi River to the Atlantic Ocean; from the St. Lawrence River to the Chesapeake Bay.[57]At that time the Iroquois numbered about 10,000, insufficient to offset the European immigration of up to 100,000 people a year. They had become victims of their own success.[57]The Five Nations of the League established a trading relationship with the Dutch at Fort Orange (modern Albany, New York), trading furs for European goods, an economic relationship that profoundly changed their way of life and led to much over-hunting of beavers.[58]Between 1665 and 1670, the Iroquois established seven villages on the northern shores of Lake Ontario in present-day Ontario, collectively known as the "Iroquois du Nord" villages. The villages were all abandoned by 1701.[59]Over the years 1670–1710, the Five Nations achieved political dominance of much of Virginia west of the Fall Line and extending to the Ohio River valley in present-day West Virginia and Kentucky. As a result of the Beaver Wars, they pushed Siouan-speaking tribes out and reserved the territory as a hunting ground by right of conquest. They finally sold the British colonists their remaining claim to the lands south of the Ohio in 1768 at the Treaty of Fort Stanwix.Beginning in 1609, the League engaged in a decades-long series of wars, the so-called Beaver Wars, against the French, their Huron allies, and other neighboring tribes, including the Petun, Erie, and Susquehannock.[58] Trying to control access to game for the lucrative fur trade, they put great pressure on the Algonquian peoples of the Atlantic coast (the Lenape or Delaware), the Anishinaabe peoples of the boreal Canadian Shield region, and not infrequently fought the English colonies as well. During the Beaver Wars, they were said to have defeated and assimilated the Huron (1649), Petun (1650), the Neutral Nation (1651),[60][61] Erie Tribe (1657), and Susquehannock (1680).[62] The traditional view is that these wars were a way to control the lucrative fur trade in order to access European goods on which they had become dependent.[63][page needed][64][page needed]Recent scholarship has elaborated on this view, arguing that the Beaver Wars were an escalation of the "Mourning Wars", which were an integral part of early Iroquoian culture.[65] This view suggests that the Iroquois launched large-scale attacks against neighboring tribes in order to avenge or replace the massive number of deaths resulting from battles or smallpox epidemics.In 1628, the Mohawk defeated the Mahican to gain a monopoly in the fur trade with the Dutch at Fort Orange (present-day Albany), New Netherland. The Mohawk would not allow northern native peoples to trade with the Dutch.[58] By 1640, there were almost no beavers left on their lands, forcing the Iroquois to play the role of the middlemen in the fur trade, as Indian peoples to the west and north possessed the beavers with the thick pelts that the Europeans would pay the best price for.[58] In 1645, a tentative peace was forged between the Iroquois and the Huron, Algonquin, and French.In 1646, Jesuit missionaries at Sainte-Marie among the Hurons went as envoys to the Mohawk lands to protect the fragile peace of the time. Mohawk attitudes toward the peace soured while the Jesuits were traveling, and their warriors attacked the party en route. The missionaries were taken to the village of Ossernenon (near present-day Auriesville, New York), where the moderate Turtle and Wolf clans recommended setting the priests free. Angered, members of the Bear clan killed Jean de Lalande, and Isaac Jogues on October 18, 1646.[66] The Catholic Church has commemorated the two French priests and Jesuit lay Brother René Goupil (killed 29 September 1642) [67] as among the eight North American Martyrs.In 1649 during the Beaver Wars, the Iroquois used recently purchased Dutch guns to attack the Huron, who were allied with the French. These attacks, primarily against the Huron towns of Taenhatentaron (St. Ignace[68]) and St. Louis[69] in what is now Simcoe County, Ontario were the final battles that effectively destroyed the Huron Confederacy.[70] The Jesuit missions in Huronia on the shores of Georgian Bay were abandoned in the face of the Iroquois attacks with the Jesuits leading the surviving Hurons east towards the French settlements on the St. Lawrence.[66] The Jesuit Relations expressed some amazement that the Five Nations had been able to dominate the area "for five hundred leagues around, although their numbers are very small".[66] From 1651 to 1652, the Iroquois attacked the Susquehannock, located to their south in present-day Pennsylvania, without sustained success.In the early 17th century, the Iroquois Confederacy was at the height of its power, with a total population of about 12,000 people.[71] In 1653 the Onondaga Nation extended a peace invitation to New France. An expedition of Jesuits, led by Simon Le Moyne, established Sainte Marie de Ganentaa in 1656 in their territory. The Jesuits were forced to abandon the mission by 1658 as hostilities resumed, possibly because of the sudden death of 500 native people from an epidemic of smallpox, a European infectious disease to which they had no immunity.From 1658 to 1663, the Iroquois were at war with the Susquehannock and their Lenape and Province of Maryland allies. In 1663, a large Iroquois invasion force was defeated at the Susquehannock main fort. In 1663, the Iroquois were at war with the Sokoki tribe of the upper Connecticut River. Smallpox struck again, and through the effects of disease, famine, and war, the Iroquois were under threat of extinction. In 1664, an Oneida party struck at allies of the Susquehannock on Chesapeake Bay.In 1665, three of the Five Nations made peace with the French. The following year, the Governor-General of New France, the Marquis de Tracy, sent the Carignan regiment under to confront the Mohawk and the Oneida.[72] The Mohawk avoided battle, but the French burned their villages, referred to as "castles" by the French, and crops.[72] In 1667, the remaining two Iroquois Nations signed a peace treaty with the French and agreed to allow their missionaries to visit their villages. The French Jesuit missionaries were known as the "black-robes" to the Iroquois, who began to urge that Catholic converts should relocate to the village of Caughnawga outside of Montreal.[72] This treaty lasted for 17 years.Around 1670, the Iroquois drove the Siouan-speaking Mannahoac tribe out of the northern Virginia Piedmont region. They began to claim ownership of the territory by right of conquest. In 1672, the Iroquois were defeated by a war party of Susquehannock. The Iroquois appealed to the French for support and asked Governor Frontenac to assist them against the Susquehannock.It would be a shame for him to allow his children to be crushed, as they saw themselves to be ... they not having the means of going to attack their fort, which was very strong, nor even of defending themselves if the others came to attack them in their villages.[73]Some[which?] old histories state that the Iroquois defeated the Susquehannock during this time period. As no record of a defeat has been found, historians have concluded that no defeat occurred.[73] In 1677, the Iroquois adopted the majority of the Iroquoian-speaking Susquehannock into their nation.[74] In January 1676, the Governor of New York colony, Edmund Andros, sent a letter to the chiefs of the Iroquois asking for their help in King Philip's War as the English colonists in New England were having much difficulty fighting the Wampanoag under the leadership of Metacom.[75] In exchange for guns from the English, which the Iroquois greatly valued, an Iroquois war party launched a devastating raid on the Wampanoag in February 1676, destroying villages and with them, supplies of food while taking many prisoners.[75]By 1677, the Iroquois formed an alliance with the English through an agreement known as the Covenant Chain. By 1680, the Iroquois Confederacy was in a strong position, having eliminated the Susquehannock and the Wampanoag, taken vast number of captives to increase the size of their population, and had secured an alliance with the English that guaranteed supplies of guns and ammunition.[76] Together the allies battled to a standstill the French, who were allied with the Huron. These Iroquoian people had been a traditional and historic foe of the Confederacy. The Iroquois colonized the northern shore of Lake Ontario and sent raiding parties westward all the way to Illinois Country. The tribes of Illinois were eventually defeated, not by the Iroquois, but by the Potawatomi.In 1679, the Susquehannock, with Iroquois help, attacked Maryland's Piscataway and Mattawoman allies. Peace was not reached until 1685. During the same period, French Jesuit missionaries were active in Iroquoia, which led to a voluntary mass relocation of many Haudenosaunee to the St. Lawrence valley at Kahnawake and Kanesatake near Montreal.[77] It was the intention of the French to use the Catholic Haudenosaunee in the St. Lawrence valley as a buffer to keep the Haudenosaunee allied with the English living in what is now upstate New York away from Montreal, the center of the French fur trade.[77] The attempts of both the English and the French to use their Haudenosaunee allies for their own purposes were foiled as the two groups of Haudenosaunee showed a "profound reluctance to kill one another".[77] Following the move of the Catholic Iroquois to the St. Lawrence valley, historians commonly describe the Iroquois living outside of Montreal as the Canadian Iroquois while the Iroquois who remained in the historical heartland of Iroquoia in modern upstate New York are described as the League Iroquois.[78]In 1684, the governor of New France, Joseph-Antoine Le Febvre de La Barre, decided to launch a punitive expedition against the Seneca, who were attacking French and Algonquian fur traders in the Mississippi river valley, and asked for the Catholic Haudenosaunee to contribute men for his expedition.[79] La Barre's expedition ended in fiasco in September 1684 when influenza broke out among the troupes de la Marine while the Canadian Iroquois warriors refused to fight, instead only engaging in verbal battles as they exchanged insults with the Seneca warriors.[80] King Louis XIV of France was not amused when he heard of La Barre's failure, which led him to sack La Barre as governor of New France, and sent as his replacement Jacques-René de Brisay de Denonville, Marquis de Denonville, Governor of New France from 1685 to 1689, who arrived in August with orders from the king to crush the Haudenosaunee confederacy.[81] The Sun King had instructed Denonville to ensure that the "grandeur" of France be respected even in the remote woods of North America. In 1684, the Iroquois invaded Virginia and Illinois territory again and unsuccessfully attacked French outposts in the latter. Trying to reduce warfare in the Shenandoah Valley of Virginia, later that year, the Virginia Colony agreed in a conference at Albany to recognize the Iroquois' right to use the North-South path, known as the Great Warpath, running east of the Blue Ridge, provided they did not intrude on the English settlements east of the Fall Line.In 1687, the Marquis de Denonville set out for Fort Frontenac (modern Kingston, Ontario) with a well-organized force. In July 1687 Denonville took with him on his expedition a mixed force of troupes de la Marine, French-Canadian militiamen, and 353 Indian warriors from the Jesuit mission settlements, of which 220 were Haudenosaunee.[80] They met with 50 hereditary sachems from the Onondaga council fire, who came under a flag of truce in what on the north shore of Lake Ontario in what is now southern Ontario.[80] Denonville recaptured the fort for New France and seized, chained, and shipped the 50 Iroquois chiefs to Marseilles, France, to be used as galley slaves.[80] Several of the Catholic Haudenosaunee were outraged at the way in which French enslaved a diplomatic party that had come under the flag of truce and had enslaved all of the Cayuga people living in several villages on the north shore of Lake Ontario, which led to least 100 of them to desert to the Seneca.[82] Denonville justified enslaving the people he encountered, saying that as an "civilized European" he did not respect the customs of "savages" and would do as he liked with them. On 13 August 1687, an advance party of French soldiers walked into a Seneca ambush and were nearly killed to a man; however the Seneca had mistaken the advance party for the main French force and fled when the main French force came up.[81] The remaining Catholic Haudenosaunee warriors refused orders from Denonville to pursue the retreating Seneca.[81]Denonville ravaged the land of the Seneca, landing a French armada at Irondequoit Bay, striking straight into the seat of Seneca power, and destroying many of its villages. Fleeing before the attack, the Seneca moved farther west, east and south down the Susquehanna River. Although great damage was done to the Seneca homeland, the Senecas' military might was not appreciably weakened. The Confederacy and the Seneca developed an alliance with the English who were settling in the east. The destruction of the Seneca land infuriated the members of the Iroquois Confederacy. On August 4, 1689, they retaliated by burning to the ground Lachine, a small town adjacent to Montreal. Fifteen hundred Iroquois warriors had been harassing Montreal defenses for many months prior to that.They finally exhausted and defeated Denonville and his forces. His tenure was followed by the return of Frontenac, who succeeded Denonville as Governor for the next nine years (1689–1698). Frontenac had been arranging a new plan of attack to lessen the effects of the Iroquois in North America. Realizing the danger of continuing to hold the sachems, he located the 13 surviving leaders of the 50 originally taken and returned with them to New France in October 1689. In 1690, Frontenac destroyed the village of Schenectady and in 1693 Frontenac burned down three Mohawk villages and took 300 prisoners.[83]In 1696, Frontenac decided to take the field against the Iroquois, although at this time he was seventy-six years of age. Frontenac decided to target the Oneida and Onondaga this time, instead of the Mohawk whom were the favorite enemies of the French.[83] On July 6, he left Lachine at the head of a considerable force and traveled to the village of the Onondaga, where he arrived a month later. With support from the French, the Algonquian nations drove the Iroquois out of the territories north of Lake Erie and west of present-day Cleveland, Ohio, regions which they had conquered during the Beaver Wars.[84] In the meantime, the Iroquois had abandoned their villages. As pursuit was impracticable, the French army commenced its return march on August 10. Under Frontenac's leadership, the Canadian militia became increasingly adept at guerrilla warfare, taking the war into Iroquois territory and attacking a number of English settlements. The Iroquois never threatened the French colony again.[85]During King William's War (North American part of the War of the Grand Alliance), the Iroquois were allied with the English. In July 1701, they concluded the "Nanfan Treaty", deeding the English a large tract north of the Ohio River. The Iroquois claimed to have conquered this territory 80 years earlier. France did not recognize the validity of the treaty, as it had settlements in the territory at that time and the English had virtually none. Meanwhile, the Iroquois were negotiating peace with the French; together they signed the Great Peace of Montreal that same year.After the 1701 peace treaty with the French, the Iroquois remained mostly neutral. During the course of the 17th century, the Iroquois had acquired a fearsome reputation among the Europeans, and it was the policy of the Six Nations to use this reputation to play off the French against the British in order to extract the maximum amount of material rewards.[86] In 1689, the English Crown provided the Six Nations goods worth £100 in exchange for help against the French, in the year 1693 the Iroquois had received goods worth £600, and in the year 1701 the Six Nations had received goods worth £800.[87]During Queen Anne's War (North American part of the War of the Spanish Succession), they were involved in planned attacks against the French. Peter Schuyler, mayor of Albany, arranged for three Mohawk chiefs and a Mahican chief (known incorrectly as the Four Mohawk Kings) to travel to London in 1710 to meet with Queen Anne in an effort to seal an alliance with the British. Queen Anne was so impressed by her visitors that she commissioned their portraits by court painter John Verelst. The portraits are believed to be the earliest surviving oil portraits of Aboriginal peoples taken from life.[88]In the first quarter of the 18th century, the Iroquoian-speaking Tuscarora fled north from the pressure of British colonization of North Carolina and intertribal warfare; they had been subject to having captives sold into Indian slavery. They petitioned to become the sixth nation of the Iroquois Confederacy. This was a non-voting position, but they gained the protection of the Haudenosaunee.The Iroquois program toward the defeated tribes favored assimilation within the 'Covenant Chain' and Great Law of Peace, over wholesale slaughter. Both the Lenni Lenape, and the Shawnee were briefly tributary to the Six Nations, while subjected Iroquoian populations emerged in the next period as the Mingo, speaking a dialect like that of the Seneca, in the Ohio region. During the War of Spanish Succession, known to Americans as "Queen Anne's War", the Iroquois remained neutral, through leaning towards the British.[83] Anglican missionaries were active with the Iroquois and devised a system of writing for them.[83]In 1721 and 1722, Lt. Governor Alexander Spotswood of Virginia concluded a new Treaty at Albany with the Iroquois, renewing the Covenant Chain and agreeing to recognize the Blue Ridge as the demarcation between the Virginia Colony and the Iroquois. But, as European settlers began to move beyond the Blue Ridge and into the Shenandoah Valley in the 1730s, the Iroquois objected. Virginia officials told them that the demarcation was to prevent the Iroquois from trespassing east of the Blue Ridge, but it did not prevent English from expanding west. Tensions increased over the next decades, and the Iroquois were on the verge of going to war with the Virginia Colony. In 1743, Governor Gooch paid them the sum of 100 pounds sterling for any settled land in the Valley that was claimed by the Iroquois. The following year at the Treaty of Lancaster, the Iroquois sold Virginia all their remaining claims in the Shenandoah Valley for 200 pounds in gold.[89]During the French and Indian War (the North American theater of the Seven Years' War), the League Iroquois sided with the British against the French and their Algonquian allies, who were traditional enemies. The Iroquois hoped that aiding the British would also bring favors after the war. Few Iroquois warriors joined the campaign. By contrast, the Canadian Iroquois supported the French.In 1711, refugees from is now southern-western Germany known as the Palatines appealed to the Iroquois clan mothers for permission to settle on their land.[90] By spring of 1713, about 150 Palatine families had leased land from the Iroquois.[91] The Iroquois taught the Palatines how to grow "the Three Sisters" as they called their staple crops of beans, corn and squash and where to find edible nuts, roots and berries.[91] In return, the Palatines taught the Iroquois how to grow wheat and oats, and how to use iron ploughs and hoes to farm.[91] As a result of the money earned from land rented to the Palatines, the Iroquois elite gave up living in longhouses and started living in European style houses, having an income equal to a middle-class English family.[91] By the middle of the 18th century, a multi-cultural world had emerged with the Iroquois living alongside German and Scots-Irish settlers.[92] The settlements of the Palatines were intermixed with the Iroquois villages.[93] In 1738, an Irishman, William Johnson, who was successful as a fur trader, settled with the Iroquois.[94] Johnson who become very rich from the fur trade and land speculation, learned the languages of the Iroquois while bedding as many of their women as possible, and become the main intermediary between the British and the League.[94] In 1745, Johnson was appointed the Northern superintendent of Indian Affairs, formalizing his position.[95]On 9 July 1755, a force of British Army regulars and the Virginia militia under General Edward Braddock advancing into the Ohio river valley was almost completely destroyed by the French and their Indian allies at the Battle of the Monongahela.[95] Johnson, who had the task of enlisting the League Iroquois on the British side, led a mixed Anglo-Iroquois force to victory at Lac du St Sacrement, known to the British as Lake George.[95] In the Battle of Lake George, a group of Catholic Mohawk (from Kahnawake) and French forces ambushed a Mohawk-led British column; the Mohawk were deeply disturbed as they had created their confederacy for peace among the peoples and had not had warfare against each other. Johnson attempted to ambush a force of 1,000 French troops and 700 Canadian Iroquios under the command of Baron Dieskau, who beat off the attack and killed the old Mohawk war chief, Peter Hendricks.[95] On 8 September 1755, Diskau attacked Johnson's camp, but was repulsed with heavy losses.[95] Though the Battle of Lake George was a British victory, the heavy losses taken by the Mohawk and Oneida at the battle caused the League to declare neutrality in the war.[95] Despite Johnson's best efforts, the League Iroquois remained neutral for next several years, and a series of French victories at Oswego, Louisbourg, Fort William Henry and Fort Carillon ensured the League Iroquois would not fight on what appeared to be the losing side.[96]In February 1756, the French learned from a spy, Oratory, an Oneida chief, that a British were stockpiling supplies at the Oneida Carrying Place, a crucial portage between Albany and Oswego to support an offensive in the spring into what is now Ontario.[97] As the frozen waters melted south of Lake Ontario on average two weeks before the waters did north of Lake Ontario, the British would be able to move against the French bases at Fort Frontenac and Fort Niagara before the French forces in Montreal could come to their relief, which from the French perspective necessitated a preemptive strike at the Oneida Carrying Place in the winter.[97] To carry out this strike, the Marquis de Vaudreuil, the Governor-General of New France, assigned the task to Gaspard-Joseph Chaussegros de Léry, an officer of the troupes de le Marine, who required and received the assistance of the Canadian Iroquois to guide him to the Oneida Carrying Place.[98] The Canadian Iroquois joined the expedition, which left Montreal on 29 February 1756 on the understanding that they would only fight against the British, not the League Iroquois, and they would not be assaulting a fort.[99]On 13 March 1756, an Oswegatchie Indian traveler informed the expedition that the British had built two forts at the Oneida Carrying Place, which caused the majority of the Canadian Iroquois to want to turn back, as they argued the risks of assaulting a fort would mean too many casualties, and many did in fact abandon the expedition.[100] On 26 March 1756, Léry's force of troupes de le Marine and French-Canadian militiamen, who had not eaten for two days, received much needed food when the Canadian Iroquois ambushed a British wagon train bringing supplies to Fort William and Fort Bull.[101] As far as the Canadian Iroquois were concerned, the raid was a success as they captured 9 wagons full of supplies and took 10 prisoners without losing a man, and for them, engaging in a frontal attack against the two wooden forts as Léry wanted to do was irrational.[102] The Canadian Iroquois informed Léry "if I absolutely wanted to die, I was the master of the French, but they were not going to follow me".[103] In the end, about 30 Canadian Iroquois reluctantly joined Léry's attack on Fort Bull on the morning of 27 March 1756, when the French and their Indian allies stormed the fort, finally smashing their way in through the main gate with a battering ram at noon.[104] Of the 63 people in Fort Bull, half of whom were civilians, only 3 soldiers, one carpenter and one woman survived the Battle of Fort Bull as Léry reported "I could not restrain the ardor of the soldiers and the Canadians. They killed everyone they encountered".[105] Afterwards, the French destroyed all of the British supplies and Fort Bull itself, which secured the western flank of New France.[105] On the same day, the main force of the Canadian Iroquois ambushed a relief force from Fort William coming to the aid of Fort Bull, and did not slaughter their prisoners as the French did at Fort Bull; for the Iroquois, prisoners were very valuable as they increased the size of the tribe.[106]The crucial difference between the European and First Nations way of war was that Europe had millions of people, which meant that British and French generals were willing to see thousands of their own men die in battle in order to secure victory as their losses could always be made good; by contrast, the Iroquois had a considerably smaller population, and could not afford heavy losses, which could cripple a community. The Iroquois custom of "Mourning wars" to take captives who would become Iroquois reflected the continual need for more people in the Iroquois communities. Iroquois warriors were brave, but would only fight to the death if necessary, usually to protect their women and children; otherwise, the crucial concern for Iroquois chiefs was always to save manpower.[107] The Canadian historian D. Peter MacLeod wrote that the Iroquois way of war was based on their hunting philosophy, where a successful hunter would bring down an animal efficiently without taking any losses to his hunting party, and in the same way, a successful war leader would inflict losses on the enemy without taking any losses in return.[108]The Iroquois only entered the war on the British side again in late 1758 after the British took Louisbourg and Fort Frontenac.[96] At the Treaty of Fort Easton in October 1758, the Iroquois forced the Lenape and Shawnee who had been fighting for the French to declare neutrality.[96] In July 1759, the Iroquois helped Johnson take Fort Niagara.[96] In the ensuing campaign, the League Iroquois assisted General Jeffrey Amherst as he took various French forts by the Great Lakes and the St. Lawrence valley as he advanced towards Montreal, which he took in September 1760.[96] The British historian Michael Johnson wrote the Iroquois had "played a major supporting role" in the final British victory in the Seven Years' War.[96] In 1763, Johnson left his old home of Fort Johnson for the lavish estate, which he called Johnson Hall, which become a center of social life in the region.[96] Johnson was close to two white families, the Butlers and the Croghans, and three Mohawk families, the Brants, the Hills, and the Peters.[96]After the war, to protect their alliance, the British government issued the Royal Proclamation of 1763, forbidding Anglo-European (white) settlements beyond the Appalachian Mountains. Colonists largely ignored the order, and the British had insufficient soldiers to enforce it.Faced with confrontations, the Iroquois agreed to adjust the line again in the Treaty of Fort Stanwix (1768). Sir William Johnson, 1st Baronet, British Superintendent of Indian Affairs for the Northern District, had called the Iroquois nations together in a grand conference in western New York, which a total of 3,102 Indians attended.[29] They had long had good relations with Johnson, who had traded with them and learned their languages and customs. As Alan Taylor noted in his history, The Divided Ground: Indians, Settlers, and the Northern Borderland of the American Revolution (2006), the Iroquois were creative and strategic thinkers. They chose to sell to the British Crown all their remaining claim to the lands between the Ohio and Tennessee rivers, which they did not occupy, hoping by doing so to draw off English pressure on their territories in the Province of New York.[29]During the American Revolution, the Iroquois first tried to stay neutral. The Reverend Samuel Kirkland, a Congregational minister working as a missionary, pressured the Oneida and the Tuscarora for a pro-American neutrality while Guy Johnson and his cousin John Johnson pressured the Mohawk, the Cayuga and the Seneca to fight for the British.[109]  Pressed to join one side or the other, the Tuscarora and the Oneida sided with the colonists, while the Mohawk, Seneca, Onondaga, and Cayuga remained loyal to Great Britain, with whom they had stronger relationships. Joseph Louis Cook offered his services to the United States and received a Congressional commission as a lieutenant colonel—the highest rank held by any Native American during the war.[110] The Mohawk war chief Joseph Brant together with John Butler and John Johnson raised racially mixed forces of irregulars to fight for the Crown.[111] Molly Brant had been the common-law wife of Sir William Johnson, and it was through her patronage that her brother Joseph came to be a war chief.[112]The Mohawk war chief Joseph Brant, other war chiefs, and British allies conducted numerous operations against frontier settlements in the Mohawk Valley, including the Cherry Valley massacre, destroying many villages and crops, and killing and capturing inhabitants. The destructive raids by Brant and other Loyalists led to appeals to Congress for help.[112] The Continentals retaliated and in 1779, George Washington ordered the Sullivan Campaign, led by Col. Daniel Brodhead and General John Sullivan, against the Iroquois nations to "not merely overrun, but destroy", the British-Indian alliance. They burned many Iroquois villages and stores throughout western New York; refugees moved north to Canada. By the end of the war, few houses and barns in the valley had survived the warfare. In the aftermath of the Sullivan expedition, Brant visited Quebec City to ask General Sir Frederick Haildmand for assurances that the Mohawk and the other Loyalist Iroquois would receive a new homeland in Canada as compensation for their loyalty to the Crown if the British should lose.[112]The American Revolution was a war that caused a great divide amongst the colonists between Patriots and Loyalists; it caused a divide between the colonies and Great Britain, and it also caused a rift that would break the Iroquois Confederacy. At the onset of the Revolution, the Iroquois Confederacy's Six Nations attempted to take a stance of neutrality. However, almost inevitably, the Iroquois nations eventually had to take sides in the conflict. It is easy to see how the American Revolution would have caused conflict and confusion among the Six Nations. For years they had been used to thinking about the English and their colonists as one and the same people. In the American Revolution, the Iroquois Confederacy now had to deal with relationships between two governments.[113]The Iroquois Confederation's population had changed significantly since the arrival of Europeans. Disease had reduced their population to a fraction of what it had been in the past.[114] Therefore, it was in their best interest to be on the good side of whoever would prove to be the winning side in the war, for the winning side would dictate how future relationships would be with the Iroquois in North America. Dealing with two governments made it hard to maintain a neutral stance, because the governments could get jealous easily if the Confederacy was interacting or trading more with one side over the other, or even if there was simply a perception of favoritism. Because of this challenging situation, the Six Nations had to choose sides. The Oneida and Tuscarora decided to support the American colonists, while the rest of the Iroquois League (the Cayuga, Mohawk, Onondaga, and Seneca) sided with the British and their Loyalists among the colonists.There were many reasons that the Six Nations could not remain neutral and uninvolved in the Revolutionary War. One of these is simple proximity; the Iroquois Confederacy was too close to the action of the war to not be involved. The Six Nations were very discontented with the encroachment of the English and their colonists upon their land. They were particularly concerned with the border established in the Proclamation of 1763 and the Treaty of Fort Stanwix in 1768.[115]During the American Revolution, the authority of the British government over the frontier was highly contested. The colonists tried to take advantage of this as much as possible by seeking their own profit and claiming new land. In 1775, the Six Nations were still neutral when "a Mohawk person was killed by a Continental soldier".[114]:370 Such a case shows how the Six Nations' proximity to the war drew them into it. They were concerned about being killed, and about their lands being taken from them. They could not show weakness and simply let the colonists and British do whatever they wanted. Many of the English and colonists did not respect the treaties made in the past.  "A number of His Majesty's subjects in the American colonies viewed the proclamation as a temporary prohibition which would soon give way to the opening of the area for settlement ... and that it was simply an agreement to quiet the minds of the Indians".[115] The Six Nations had to take a stand to show that they would not accept such treatment, and they looked to build a relationship with a government that would respect their territory.In addition to being in close proximity to the war, the new lifestyle and economics of the Iroquois Confederacy since the arrival of the Europeans in North America made it nearly impossible for the Iroquois to isolate themselves from the conflict. By this time, the Iroquois had become dependent upon the trade of goods from the English and colonists, and had adopted many European customs, tools, and weapons. For example, they were increasingly dependent on firearms for hunting.[116] After becoming so reliant, it would have been hard to even consider cutting off trade that brought goods that were a central part of everyday life.As Barbara Graymont stated, "Their task was an impossible one to maintain neutrality. Their economies and lives had become so dependent on each other for trading goods and benefits it was impossible to ignore the conflict. Meanwhile they had to try and balance their interactions with both groups. They did not want to seem as they were favoring one group over the other, because of sparking jealousy and suspicion from either side". Furthermore, the English had made many agreements with the Six Nations over the years, yet most of the Iroquois' day-to-day interaction had been with the colonists. This made it a confusing situation for the Iroquois because they could not tell who the true heirs of the agreement were, and couldn't know if agreements with England would continue to be honored by the colonists if they were to win independence.Supporting either side in the Revolutionary War was a complicated decision. Each nation individually weighed their options to come up with a final stance that ultimately broke neutrality and ended the collective agreement of the Confederation. The British were clearly the most organized, and seemingly most powerful. In many cases, the British presented the situation to the Iroquois as the colonists just being "naughty children". On the other, the Iroquois considered that "the British government was three thousand miles away. This placed them at a disadvantage in attempting to enforce both the Proclamation of 1763 and the Treaty at Fort Stanwix 1768 against land hungry frontiersmen."[116]:49 In other words, even though the British were the strongest and best organized faction, the Six Nations had concerns about whether they would truly be able to enforce their agreements from so far away.The Iroquois also had concerns about the colonists. The British asked for Iroquois support in the war. "In 1775, the Continental Congress sent a delegation to the Iroquois in Albany to ask for their neutrality in the war coming against the British".[114]:370 It had been clear in prior years that the colonists had not been respectful of the land agreements made in 1763 and 1768. The Iroquois Confederacy was particularly concerned over the possibility of the colonists winning the war, for if a revolutionary victory were to occur, the Iroquois very much saw it as the precursor to their lands being taken away by the victorious colonists, who would no longer have the British Crown to restrain them.[117] Continental army officers such as George Washington had attempted to destroy the Iroquois.[114]On a contrasting note, it was the colonists who had formed the most direct relationships with the Iroquois due to their proximity and trade ties. For the most part, the colonists and Iroquois had lived in relative peace since the English arrival on the continent a century and a half before. The Iroquois had to determine whether their relationships with the colonists were reliable, or whether the English would prove to better serve their interests. They also had to determine whether there were really any differences between how the English and the colonists would treat them.The war ensued, and the Iroquois broke their confederation. Hundreds of years of precedent and collective government was trumped by the immensity of the American Revolutionary War. The Oneida and Tuscarora decided to support the colonists, while the rest of the Iroquois League (the Cayuga, Mohawk, Onondaga, and Seneca) sided with the British and Loyalists. At the conclusion of the war the fear that the colonists would not respect the Iroquois' pleas came true, especially after the majority of the Six Nations decided to side with the British and were no longer considered trustworthy by the newly independent Americans. In 1783 the Treaty of Paris was signed. While the treaty included peace agreements between all of the European nations involved in the war as well as the newborn United States, it made no provisions for the Iroquois, who were left to be treated with by the new United States government as it saw fit.[113]After the Revolutionary War, the ancient central fireplace of the League was re-established at Buffalo Creek. The United States and the Iroquois signed the treaty of Fort Stanwix in 1784 under which the Iroquois ceded much of their historical homeland to the Americans, which was followed by another treaty in 1794 at Canandaigua which they ceded even more land to the Americans.[118] The governor of New York state, George Clinton, was constantly pressuring the Iroquois to sell their land to white settlers, and as alcoholism became a major problem in the Iroquois communities, many did sell their land in order to buy more alcohol, usually to unscrupulous agents of land companies.[119] At the same time, American settlers continued to push into the lands beyond the Ohio river, leading to a war between the Western Confederacy and the United States.[118] One of the Iroquois chiefs, Cornplanter, persuaded the remaining Iroquois in New York state to remain neutral and not to join the Western Confederacy.[118] At the same time, American policies to make the Iroquois more settled started to have some effect. Traditionally, for the Iroquois farming was woman's work and hunting was men's work; by the early 19th century, American policies to have the men farm the land and cease hunting were having effect.[120] During this time, the Iroquois living in New York state become demoralized  as more of their land was sold to land speculators while alcoholism, violence, and broken families became major problems on their reservations.[120] The Oneida and the Cayuga sold almost all of their land and moved out of their traditional homelands.[120]By 1811, Methodist and Episcopalian missionaries established missions to assist the Oneida and Onondaga in western New York. However, white settlers continued to move into the area. By 1821, a group of Oneida led by Eleazar Williams, son of a Mohawk woman, went to Wisconsin to buy land from the Menominee and Ho-Chunk and thus move their people further westward.[121] In 1838, the Holland Land Company used forged documents to cheat the Seneca of almost all of their land in western New York, but a Quaker missionary, Asher Wright, launched lawsuits that led to one of the Seneca reservations being returned in 1842 and another in 1857.[120] However, as late as the 1950s both the United States and New York governments confiscated land belonging to the Six Nations for roads, dams and reservoirs with the land being given to Cornplanter for keeping the Iroquois from joining the Western Confederacy in the 1790s being confiscated and flooded by the Kinzua Dam.[120] Captain Joseph Brant and a group of Iroquois left New York to settle in the Province of Quebec (present-day Ontario). To partially replace the lands they had lost in the Mohawk Valley and elsewhere because of their fateful alliance with the British Crown, they were given a large land grant on the Grand River, at Six Nations of the Grand River First Nation. Brant's crossing of the river gave the original name to the area: Brant's Ford. By 1847, European settlers began to settle nearby and named the village Brantford. The original Mohawk settlement was on the south edge of the present-day Canadian city at a location still favorable for launching and landing canoes. In the 1830s many additional Onondaga, Oneida, Seneca, Cayuga, and Tuscarora relocated into the Indian Territory, the Province of Upper Canada, and Wisconsin.Many Iroquois (mostly Mohawk) and Iroquois-descended Métis people living in Lower Canada (primarily at Kahnawake) took employment with the Montreal-based North West Company during its existence from 1779 to 1821 and became voyageurs or free traders working in the North American fur trade as far west as the Rocky Mountains. They are known to have settled in the area around Jasper's House[122] and possibly as far west as the Finlay River[123] and north as far as the Pouce Coupe and Dunvegan areas,[124] where they founded new Aboriginal communities which have persisted to the present day claiming either First Nations or Métis identity and indigenous rights. The Michel Band, Mountain Métis,[125] and Aseniwuche Winewak Nation of Canada[126] in Alberta and the Kelly Lake community in British Columbia all claim Iroquois ancestry.During the 18th century, the Catholic Canadian Iroquois living outside of Montreal reestablished ties with the League Iroquois.[127] During the American Revolution, the Canadian Iroquois declared their neutrality and refused to fight for the Crown despite the offers of Sir Guy Carlton, the governor of Quebec.[127] Many Canadian Iroquois worked for the both Hudson's Bay Company and the Northwest Company as a voyageurs in the fur trade in the late 18th and early 19th centuries.[127] In the War of 1812, the Canadian Iroquois again declared their neutrality.[127] The Canadian Iroquois communities at Oka and Kahnaweke were prosperous settlements in the 19th century, supporting themselves via farming and the sale of sleds, snowshoes, boats, and baskets.[127] In 1884, about 100 Canadian Iroquois were hired by the British government to serve as river pilots and boatmen for the relief expedition for the besieged General Charles Gordon in Khartoum in the Sudan, taking the force commanded by Field Marshal Wolsely up the Nile from Cairo to Khartoum.[127] On their way back to Canada, the Canadian Iroquois river pilots and boatmen stopped in London, where they were personally thanked by Queen Victoria for their services to Queen and Country.[127] In 1886, when a bridge was being built at the St. Lawrence, a number of Iroquois men from Kahnawke were hired to help built and the Iroquois workers proved so skilled as steelwork erectors that since that time, a number of bridges and skycrapers in Canada and the United States have been built by the Iroquois steelmen.[127]During World War I, it was Canadian policy to encourage men from the First Nations to enlist in the Canadian Expeditionary Force (CEF), where their skills at hunting made them excellent as snipers and scouts.[128] As the Iroquois Six Nations were considered to be the most warlike of all Canada's First Nations, and in turn, the Mohawk were considered to the most warlike of all the Six Nations, the government especially encouraged the Iroquois and above all the Mohawks to join the CEF.[129] About the half of the 4, 000 or so First Nations men who served in the CEF were Iroquois.[130] Men from the Six Nations reservation at Brantford were encouraged to join the 114th Haldimand Battalion (also known as "Brock's Rangers) of the CEF, where two entire companies including the officers were all Iroquois.[128] The 114th Battalion was formed in December 1915 and broken up in November 1916 to provide reinforcements for other battalions.[128] Iroquois captured by the Germans were often subjected to cruel treatment. A Mohawk from Brantford, William Forster Lickers, who enlisted in the CEF in September 1914 was captured at the Second Battle of Ypres in April 1915, where he was savagely beaten by his captors as one German officer wanted to see if "Indians could feel pain".[131] Lickers was beaten so badly that he was left paralyzed for the rest of his life, through the officer was well pleased to establish that Indians did indeed feel pain.[131]The Six Nations council at Brantford tended to see themselves as a sovereign nation that was allied to the Crown through the Covenant Chain going back to the 17th century and thus allied to King George V personally instead of being under the authority of Canada.[132] One Iroquois clan mother in a letter sent in August 1916 to a recruiting sergeant who refused to allow her teenage son to join the CEF under the grounds that he was underage, declared the Six Nations were not subject to the laws of Canada and he had no right to refuse her son because Canadian laws did not apply to them.[132] As she explained, the Iroquois regarded the Covenant Chain as still being in effect, meaning the Iroquois were only fighting in the war because they were allied to the Crown and were responding to an appeal for help from their ally, King George V, who had asked them to enlist in the CEF.[132]The complex political environment which emerged in Canada with the Haudenosaunee grew out of the Anglo-American era of European colonization. At the end of the War of 1812, Britain shifted Indian affairs from the military to civilian control. With the creation of the Dominion of Canada in 1867, civil authority, and thus Indian affairs, passed to Canadian officials with Britain retaining control of military and security matters. At the turn of the century, the Canadian government began passing a series of Acts which were strenuously objected to by the Iroquois Confederacy. During World War I, an act attempted to conscript Six Nations men for military service. Under the Soldiers Resettlement Act, legislation was introduced to redistribute native land. Finally in 1920, an Act was proposed to force citizenship on "Indians" with or without their consent, which would then automatically remove their share of any tribal lands from tribal trust and make the land and the person subject to the laws of Canada.[133]The Haudenosaunee hired a lawyer to defend their rights in the Supreme Court of Canada. The Supreme Court refused to take the case, declaring that the members of the Six Nations were British citizens. In effect, as Canada was at the time a division of the British government, it was not an international state, as defined by international law. In contrast, the Iroquois Confederacy had been making treaties and functioning as a state since 1643 and all of their treaties had been negotiated with Britain, not Canada.[133] As a result, a decision was made in 1921 to send a delegation to petition the King of England,[134] whereupon Canada's External Affairs division blocked issuing passports. In response, the Iroquois began issuing their own passports and sent Levi General,[133] the Cayuga Chief "Deskaheh,"[134] to England with their attorney. Winston Churchill dismissed their complaint claiming that it was within the realm of Canadian jurisdiction and referred them back to Canadian officials.On 4 December 1922, Charles Stewart, Superintendent of Indian Affairs, and Duncan Campbell Scott, Deputy Superintendent of the Canadian Department of Indian Affairs traveled to Brantford to negotiate a settlement on the issues with the Six Nations. After the meeting, the Native delegation brought the offer to the tribal council, as was customary under Haudenosaunee law. The council agreed to accept the offer, but before they could respond, the Royal Canadian Mounted Police conducted a liquor raid on the Iroquois' Grand River territory. The siege lasted three days[133] and prompted the Haudenosaunee to send Deskaheh to Washington, DC, to meet with the chargé d'affaires of the Netherlands asking the Dutch Queen to sponsor them for membership in the League of Nations.[134] Under pressure from the British, the Netherlands reluctantly refused sponsorship.[135]Deskaheh and the tribal attorney proceeded to Geneva and attempted to gather support. "On 27 September 1923, delegates representing Estonia, Ireland, Panama and Persia signed a letter asking for communication of the Six Nations' petition to the League's assembly," but the effort was blocked.[133] Six Nations delegates traveled to the Hague and back to Geneva attempting to gain supporters and recognition,[134] while back in Canada, the government was drafting a mandate to replace the traditional Haudenosaunee Confederacy Council with one that would be elected under the auspices of the Canadian Indian Act. In an unpublicized signing on 17 September 1924, Prime Minister Mackenzie King and Governor-General Lord Byng of Vimy signed the Order in Council, which set elections on the Six Nations reserve for 21 October. Only 26 ballots were cast.The long-term effect of the Order was that the Canadian government had wrested control over the Haudenosaunee trust funds from the Iroquois Confederation and decades of litigation would follow.[133] In 1979, over 300 Indian chiefs visited London to oppose Patriation of the Canadian Constitution, fearing that their rights to be recognized in the Royal Proclamation of 1763 would be jeopardized. In 1981, hoping again to clarify that judicial responsibilities of treaties signed with Britain were not transferred to Canada, several Alberta Indian chiefs filed a petition with the British High Court of Justice. They lost the case but gained an invitation from the Canadian government to participate in the constitutional discussions which dealt with protection of treaty rights.[134]In 1990, a long-running dispute over ownership of land at Oka, Quebec caused a violent stand-off. The Mohawk reservation at Oka had become dominated by a group called the Mohawk Warrior Society that emerged in smuggling across the U.S-Canada border and were well armed with assault rifles. On 11 July 1990, the Mohawk Warrior Society tried to stop the building of a golf course on land claimed by the Mohawk people, which led to a shoot-out between the Warrior Society and the Sûreté du Québec left a policeman dead.[136] In the resulting Oka Crisis, the Warrior Society occupied both the land that they claimed belonged to the Mohawk people and the Mercier bridge linking Montreal to the mainland.[136] On 17 August 1990, Quebec Premier Robert Bourassa asked for the Canadian Army to intervene to maintain "public safety", leading to the deployment of the Royal 22e Régiment to Oka and Montreal.[136] The stand-off ended on 26 September 1990 with a melee between the soldiers and the warriors.[136] The dispute over ownership of the land at Oka continues.See: Indian Termination PolicyIn the period between World War II and The Sixties the US government followed a policy of Indian Termination for its Native citizens. In a series of laws, attempting to mainstream tribal people into the greater society, the government strove to end the U.S. government's recognition of tribal sovereignty, eliminate trusteeship over Indian reservations, and implement state law applicability to native persons. In general the laws were expected to create taxpaying citizens, subject to state and federal taxes as well as laws, from which Native people had previously been exempt.[137]On 13 August 1946 the Indian Claims Commission Act of 1946, Pub. L. No. 79-726, ch. 959, was passed. Its purpose was to settle for all time any outstanding grievances or claims the tribes might have against the U.S. for treaty breaches, unauthorized taking of land, dishonorable or unfair dealings, or inadequate compensation. Claims had to be filed within a five-year period, and most of the 370 complaints that were submitted[138] were filed at the approach of the 5-year deadline in August, 1951.[139]On 2 July 1948 Congress enacted [Public Law 881] 62 Stat. 1224, which transferred criminal jurisdiction over offenses committed by and against "Indians" to the State of New York. It covered all reservations lands within the state and prohibited the deprivation of hunting and fishing rights which may have been guaranteed to "any Indian tribe, band, or community, or members thereof." It further prohibited the state from requiring tribal members to obtain fish and game licenses.[140] Within 2 years, Congress passed [Public Law 785] 64 Stat. 845, on 13 September 1950[141] which extended New York's authority to civil disputes between Indians or Indians and others within the State. It allowed the tribes to preserve customs, prohibited taxation on reservations,[142] and reaffirmed hunting and fishing rights. It also prohibited the state from enforcing judgments regarding any land disputes or applying any State Laws to tribal lands or claims prior to the effective date of the law 13 September 1952.[141] During congressional hearings on the law, tribes strongly opposed the passage, fearful that states would deprive them of their reservations. The State of New York disavowed any intention to break up or deprive tribes of their reservations and asserted that they did not have the ability to do so.[143]On 1 August 1953, United States Congress issued a formal statement, House Concurrent Resolution 108, which was the formal policy presentation announcing the official federal policy of Indian termination. The resolution called for the "immediate termination of the Flathead, Klamath, Menominee, Potawatomi, and Turtle Mountain Chippewa, as well as all tribes in the states of California, New York, Florida, and Texas."  All federal aid, services, and protection offered to Native people were to cease, and the federal trust relationship and management of reservations would end.[144] Individual members of terminated tribes were to become full United States citizens with all the rights, benefits and responsibilities of any other United States citizen. The resolution also called for the Interior Department to quickly identify other tribes who would be ready for termination in the near future.[145]Beginning in 1953, a Federal task force began meeting with the tribes of the Six Nations. Despite tribal objections, legislation was introduced into Congress for termination.[146] The proposed legislation involved more than 11,000 Indians of the Iroquois Confederation and was divided into two separate bills. One bill dealt with the Mohawk, Oneida, Onondaga, Cayuga and Tuscarora tribes and the other dealt with the Seneca.[147] The arguments the Six Nations made in their hearings with committees were that their treaties showed that the United States recognized that their lands belonged to the Six Nations, not the United States and that "termination contradicted any reasonable interpretation that their lands would not be claimed or their nations disturbed" by the federal government.[148] The bill for the Iroquois Confederation died in committee without further serious consideration.[146]On 31 August 1964,[149] H. R. 1794 An Act to authorize payment for certain interests in lands within the Allegheny Indian Reservation in New York was passed by Congress and sent to the president for signature. The bill authorized payment for resettling and rehabilitation of the Seneca Indians who were being dislocated by the construction of the Kinzua Dam on the Allegheny River. Though only 127 Seneca families (about 500 people) were being dislocated, the legislation benefited the entire Seneca Nation, because the taking of the Indian land for the dam abridged a 1794 treaty agreement. In addition, the bill provided that within three years, a plan from the Interior Secretary should be submitted to Congress withdrawing all federal supervision over the Seneca Nation, though technically civil and criminal jurisdiction had lain with the State of New York since 1950.[150]Accordingly, on 5 September 1967 a memo from the Department of the Interior announced proposed legislation was being submitted to end federal ties with the Seneca.[151][152] In 1968 a new liaison was appointed from the BIA for the tribe to assist the tribe in preparing for termination and rehabilitation.[153] The Seneca were able to hold off termination until President Nixon issued[154] his Special Message to the Congress on Indian Affairs in July 1970.[155] Thus, no New York tribes then living in New York were terminated during this period.In a twist of fate, one former New York Tribe did lose its federal recognition. The Emigrant Indians of New York included the Oneidas, Stockbridge-Munsee, and Brothertown Indians of Wisconsin.[156] In an effort to fight termination and force the government into recognizing their outstanding land claims from New York, the three tribes filed litigation with the Claims Commission in the 1950s.[157] They won their claim on 11 August 1964.[156] Public Law 90-93 81 Stat. 229 Emigrant New York Indians of Wisconsin Judgment Act established federal trusteeship to pay the Oneidas and Stockbridge-Munsee, effectively ending Congressional termination efforts for them. Though the law did not specifically state the Brothertown Indians were terminated, it authorized all payments to be made directly to each enrollee with special provisions for minors to be handled by the Secretary. The payments were not subject to state or federal taxes.[158] Beginning in 1978, the Brothertown Indians submitted a petition to regain federal recognition.[157] In 2012 the Department of the Interior, in the final determination on the Brothertown petition found that Congress had terminated their tribal status when it granted them citizenship in 1838 and therefore only Congress could restore their tribal status.[159] They are still seeking Congressional approval.[160]For the Haudenosaunee, grief for a loved one who died was a powerful emotion that if not attended would cause all sorts of problems for the grieving who if left without consolation would go mad.[161] Rituals to honor the dead were very important and the most important of all was the Condolence ceremony to provide consolation for those who lost a family member or friend.[162] Since it was believed that the death of a family member also weakened the spiritual strength of the surviving family members, it was considered crucially important to replace the lost family member by providing a substitute who could be adopted or alternatively could be tortured to provide an outlet for the grief.[163] Hence the "mourning wars".One of the central features of traditional Iroquois life was the "mourning wars" when Haudenosaunee warriors would raid neighboring peoples in search of captives to replace those Haudenosaunee who had died.[164] War for the Haudenosaunee was primarily for captives, and the usual factors that were considered benefits of war for the Europeans like expansion of territory or glory in battle did not count for the Haudeenosaunee, who only cared about taking captives.[165] A successful war party was one that had taken many prisoners without suffering losses in return; killing enemies was considered acceptable if necessary, but disapproved of as it reduced the number of potential captives.[165] Captives were seen as far more important than scalps. Additionally, war served as a way for young men to demonstrate their valor and courage, which was not only a prerequisite for becoming a chief, but also essential if one wanted to get married and hence have sex.[166] A man considered a coward was viewed as unattractive by Haudenosaunee women who saw bravery in war as a very attractive feature in a man.[166] In the precontact era, war was relativity bloodless as First Nations peoples fought one another in suits of wooden armor.[167] In 1609, the French explorer Samuel de Champlain observed several battles between the Algonquins and the Iroquois which featured hardly any killing, which seemed to be the norm for First Nations wars.[167] At a battle between the Algonquins and the Iroquois by the shores of Lake Champlain, the only people killed were two Iroquois warriors when Champlain demonstrated the power of his musket to his Algonquin allies.The clan mothers would demand a "mourning war" to provide consolation and renewed spiritual strength for a family that lost a member to death by accusing the warriors of cowardice; either the warriors would go on a "mourning war" or would be marked as cowards forever, which make them unmarriageable.[164] At this point, the warriors would then usually leave to raid a neighboring people in search of captives.[168] The captives were either adopted into Haudenosaunee families to become Haudenosaunee, or were to be killed after bouts of ritualized torture as a way of expressing rage at the death of a family member.[169] The male captives were usually received with blows as they were marched into the community, and were then all captives regardless of their sex or age were stripped naked and tied to poles in the middle of the community.[169] After having sensitive parts of their bodies burned and some of their fingernails pulled out, the prisoners were allowed to rest and given food and water.[169] In the following days, the captives had to dance naked before the community, and then it was decided if they were to be adopted or killed.[169] If those who were adopted into the Haudenosaunee families made a sincere effort to become Haudenosaunee, then they would be embraced by the community, and if they did not, then they were swiftly executed.[169]Those slated for execution had to wear red and black facial paint and were "adopted" by a family who addressed the prisoner as "uncle", "aunt", "nephew" or "niece" depending on their age and sex, and would bring them food and water.[170] The captive would be executed after a day-long torture session of burning and removing body parts, which the prisoner was expected to behave with stoicism and nobility (an expectation not usually met) before being scalped alive, had hot sand applied to the exposed skull and finally killed by cutting out their hearts.[170] Afterwards, the victim's body was cut and eaten by the community.[170] The practice of ritual torture and execution together with cannibalism ended some time in the early 18th century, and by late-18th-century European writers like Philip Mazzei and James Adair were denying that the Haudenosaunee engaged in ritual torture and cannibalism, saying they had seen no evidence of such practices during their visits to Haudenosaunee villages.[170]For Iroquois, the purpose of war was to take prisoners first and foremost, with the Onondaga chief Teganissorens telling the governor of New York, Sir Robert Hunter, in 1711: "We are not like you Christians, for when you have prisoners of one another you send them home, by such means you can never rout one another".[165] The converse of this strategy was that the Iroquois would not accept losses in battle as it defeated the whole purpose of the "mourning wars", which was to add to their numbers, not decrease them.[165] The French during their wars with the Haudenosaunee were often astonished when a war party that was on the verge of victory over them could be made to retreat by merely killing one or two of their number.[165] The European notion of a glorious death in battle had no counterpart with the Haudenosaunee.[165] Death in battle was accepted only when absolutely necessary, and the Iroquois believed the souls of those who died in battle were destined to spend eternity as angry ghosts haunting the world in search of vengeance.[171] For this reason, those who died in battle were never buried in community cemeteries, as it would bring the presence of unhappy ghosts into the community.[172]For these reasons, the Haudenosaunee engaged in tactics that the French, the British and later on the Americans all considered to be cowardly.[172] The Haudenosaunee preferred ambushes and surprise attacks, would almost never attack a fortified place or attack frontally, or would retreat if outnumbered.[172] If Kanienkeh was invaded, the Haudenosaunee would attempt to ambush the enemy, or alternatively they would retreat behind the wooden walls of their villages to endure a siege.[172] If the enemy appeared too powerful as when the French invaded Kanienkeh in 1693, the Haudenosaunee burned their villages and their crops and the entire population retreated into the woods to wait for the French to depart.[172] The main weapons for the Iroquois were bows and arrows with flint tips and quivers made from corn husks.[173] Shields and war clubs were made from wood.[174] After contact was established with Europeans, metal knives and hatchets were extensively used together with tomahawks with iron or steel blades.[174] Before taking to the field, war chiefs would lead ritual purification ceremonies where the warriors would dance around a pole painted red.[174]When European diseases that the Indians had no immunity to like smallpox devastated the Five Nations in the 17th century, causing thousands of deaths, the League began a period of "mourning wars" without precedent, which led to the virtual destruction of the Huron, Petun and Neutral peoples.[175] By the 1640s, it is estimated that smallpox had reduced the population of the Haudenosaunee by least 50%, which required massive "mourning wars" to make up these losses.[176] The American historian Daniel Richter wrote it was at this point that war changed from being sporadic, small-scale raids launched in response to individual deaths and became "the constant and increasing undifferentiated symptom of societies in demographic crisis".[176] Furthermore, the introduction of guns, which could pierce the wooden armor, made First Nations warfare bloodier and more deadly than it had been in the pre-contact era, ending the age when armed conflicts were more brawls than battles as Europeans would had understood the term.[167] At the same time, guns could be only be obtained by trading furs with the Europeans, and once the Haudenosaunee exhausted their supplies of beaver by about 1640, they were forced to buy beaver pelts from Indians living further north, which led them to attempt to eliminate other middlemen in order to monopolize the fur trade in a series of "beaver wars".[177] Richter wrote "the mourning war tradition, deaths from disease, dependence on firearms, and the trade in furs combined to produce a dangerous spiral: epidemics led to deadlier mourning wars fought with firearms; the need for guns increased the need for pelts to trade for them; the quest for furs provoked wars with other nations; and deaths in those wars began the mourning war cycle anew".[177] From 1640 to 1701, the Five Nations was almost continuously at war, battling at various times the French, the Huron, the Erie, the Neutral, the Lenape, the Susquenhannock, the Petun, the Abenaki, the Ojibwa, and the Algonquins, fighting campaigns from Virginia to the Mississippi and all the way to what is now northern Ontario.[178]Despite taking thousands of captives, the Five Nations populations continued to fall, as diseases continued to take their toll while Jesuits, whom the Haudenosaunee were forced to accept after making peace with the French in 1667, encouraged Catholic converts to move to the St. Lawrence river valley.[179] In the 1640s, the Mohawks could field about 800 warriors, and the 1670s, could field only 300 warriors, which suggested a population decline.[180]The Iroquois League traditions allowed for the dead to be symbolically replaced through captives taken in "mourning wars", the blood feuds and vendettas that were an essential aspect of Iroquois culture.[181] As a way of expediting the mourning process, raids were conducted to take vengeance and seize captives. Captives were generally adopted directly by the grieving family to replace the member(s) who had been lost.This process not only allowed the Iroquois to maintain their own numbers, but also to disperse and assimilate their enemies. The adoption of conquered peoples, especially during the period of the Beaver Wars (1609-1701), meant that the Iroquois League was composed largely of naturalized members of other tribes. Cadwallader Colden wrote, "It has been a constant maxim with the Five Nations, to save children and young men of the people they conquer, to adopt them into their own Nation, and to educate them as their own children, without distinction; These young people soon forget their own country and nation and by this policy the Five Nations make up the losses which their nation suffers by the people they lose in war."  Those who attempted to return to their families were harshly punished; for instance, the French fur trader Pierre-Esprit Radisson was captured by an Iroquois raiding party as a teenager, was adopted by a Mohawk family, ran away to return to his family in Trois-Rivières, and upon being recaptured was punished by having his fingernails pulled out and having one of his fingers cut to the bone.[182] However, Radisson was not executed as his adopted parents provided gifts to the families of the men Radisson had killed when he fled as compensation for their loss; several of the Huron who fled with Radisson were not so lucky and were executed.[182]By 1668, two-thirds of the Oneida village were assimilated Algonquians and Hurons. At Onondaga there were Native Americans of seven different nations and among the Seneca eleven.[183] They also adopted European captives, as did the Catholic Mohawk in settlements outside Montreal. This tradition of adoption and assimilation was common to native people of the northeast but was quite different from European settlers' notions of combat.At the time of first European contact the Iroquois lived in a small number of large villages scattered throughout their territory. Each nation had between one and four villages at any one time, and villages were moved approximately every five to twenty years as soil and firewood were depleted.[184]  These settlements were surrounded by a palisade and usually located in a defensible area such as a hill, with access to water.[185] Because of their appearance with the palisade, Europeans termed them castles. Villages were usually built on level or raised ground, surrounded by log palisades and sometimes ditches.[186]Within the villages the inhabitants lived in longhouses. Longhouses varied in size from 15 to 150 feet long and 15 to 25 feet in breadth.[186] Longhouses were usually built of layers of elm bark on a frame of rafters and standing logs raised upright.[186] In 1653, Dutch official and landowner Adriaen van der Donck described a Mohawk longhouse in his Description of New Netherland.Their houses are mostly of one and the same shape, without any special embellishment or remarkable design. When building a house, large or small,—for sometimes they build them as long as some hundred feet, though never more than twenty feet wide—they stick long, thin, peeled hickory poles in the ground, as wide apart and as long as the house is to be. The poles are then bent over and fastened one to another, so that it looks like a wagon or arbor as are put in gardens. Next, strips like split laths are laid across these poles from one end to the other. ... This is then well covered all over with very tough bark. ... From one end of the house to the other along the center they kindle fires, and the area left open, which is also in the middle, serves as a chimney to release the smoke. Often there are sixteen or eighteen families in a house ... This means that often a hundred or a hundred and fifty or more lodge in one house.Usually, between 2 and 20 families lived in a single longhouse with sleeping platforms being 2 feet above the ground and food left to dry on the rafters.[186] A castle might contain twenty or thirty longhouses. In addition to the castles the Iroquois also had smaller settlements which might be occupied seasonally by smaller groups, for example for fishing or hunting.[185] Living in the smoke-filled longhouses often caused conjunctivitis.[173]Total population for the five nations has been estimated at 20,000 before 1634. After 1635 the population dropped to around 6,800, chiefly due to the epidemic of smallpox introduced by contact with European settlers.[184] The Iroquois lived in extended families divided clans headed by clan mothers that grouped into moieities ("halves"). The typical clan consisted of about 50 to 200 people.[187] The division of the Iroquois went as follows:Cayuga Moiety (A) clans: Bear, Beaver, Heron, Turtle, WolfMoiety (B) clans: Turtle, Bear, DeerTuscaroraMoiety (A) clans: Bear, WolfMoeity (B) clans: Eel, Snipe, Beaver, Turtle, DeerSenecaMoeity (A) clans: Heron, Beaver, Bear, Wolf, TurtleMoeity (B) clans: Deer, Hawk, Eel, SnipeOnondagaMoeity (A) clans: Tortoise, Wolf, Snipe, Eagle, BeaverMoeity (B) clan: Bear, Hawk, Eel, DeerOneidaMoeity (A) clan: wolfMoeity (B) clans: Bear, TurtleMohawkMoeity (A) clans: Wolf, BearMoeity (B) clan: Turtle.[187] Government was by the 50 sachems representing the various clans who were chosen by the clan mothers.[187] Assisting the sachems were the "Pinetree Chiefs" who served as diplomats and the "War Chiefs" who led the war parties; neither the "Pinetree Chiefs" or the "War Chiefs" were allowed to vote at council meetings.[188]By the late 1700s The Iroquois were building smaller log cabins resembling those of the colonists, but retaining some native features, such as bark roofs with smoke holes and a central fireplace.[189] The main woods used by the Iroquois to make their utensils were oak, birch, hickory and elm.[186] Bones and antlers were used to make hunting and fishing equipment.[190]The Iroquois are a mix of horticulturalists, farmers, fishers, gatherers and hunters, though their main diet traditionally has come from farming. The main crops they cultivated are corn, beans and squash, which were called the three sisters (De-oh-há-ko) and are considered special gifts from the Creator.[173] These crops are grown strategically. The cornstalks grow, the bean plants climb the stalks, and the squash grow beneath, inhibiting weeds and keeping the soil moist under the shade of their broad leaves. In this combination, the soil remained fertile for several decades. The food was stored during the winter, and it lasted for two to three years. When the soil in one area eventually lost its fertility, the Haudenosaunee moved their village. For the Iroquois, farming was traditionally women's work and the entire process of planting, maintaining, harvesting and cooking the "Three Sisters" were done by women.[173] At harvest time, Iroquois women would use corn husks to make hats, dolls, rope and moccasins.[173] Besides for the "Three Sisters", the Iroquois also eat artichokes, leeks, cucumbers, turnips, pumpkins, a number of different berries such blackberries, blueberries, gooseberries, etc. and wild nuts.[173] The "Three Sisters" were ground up into hominy and soups in clay pots, which were disregarded for metal pots after the contact was made with Europeans.[173]Gathering is the traditional job of the women and children. Wild roots, greens, berries and nuts were gathered in the summer. During spring, sap is tapped from the maple trees and boiled into maple syrup, and herbs are gathered for medicine. After the coming of Europeans, the Iroquois started to grow apples, pears, cherries, and peaches.[173]The Iroquois hunted mostly deer but also other game such as wild turkey and migratory birds. Muskrat and beaver were hunted during the winter. Archaeologists have the bones of bison, elk, deer, bear, raccoon, and porcupines at Iroquois villages.[173]  Fishing was also a significant source of food because the Iroquois had villages mostly in the St.Lawrence and Great Lakes areas. The Iroquois used nets made from vegetable fiber with weights of pebbles for fishing.[173] They fished salmon, trout, bass, perch and whitefish until the St. Lawrence became too polluted by industry. In the spring the Iroquois netted, and in the winter fishing holes were made in the ice.[191] Allium tricoccum is also a part of traditional Iroquois cuisine.[192] Starting about 1620, the Iroquois started to raise pigs, geese and chickens, which they had acquired from the Dutch.[173]In 1644 Johannes Megapolensis described Mohawk traditional wear.In summer they go naked, having only their private parts covered with a patch. The children and young folks to ten, twelve and fourteen years of age go stark naked. In winter, they hang about them simply an undressed deer or bear or panther skin; or they take some beaver and otter skins, wild cat, racoon, martin, otter, mink, squirrel or such like skins ... and sew some of them to others, until it is a square piece, and that is then a garment for them; or they buy of us Dutchmen two and a half ells [about 170 centimetres (5.6 ft)] of duffel, and that they hang simply about them, just as it was torn off, without sewing it.[185]On their feet the Iroquois wore moccasins, "true to nature in its adjustment to the foot, beautiful in its materials and finish, and durable as an article of apparel."[117]The moccason is made of one piece of deer-skin. It is seamed up at the heel, and also in front, above the foot, leaving the bottom of the moccasin without a seam. In front the deer-skin is gathered, in place of being crimped; over this part porcupine quills or beads are worked, in various patterns. The plain moccasin rises several inches above the ankle ... and is fastened with deer strings; but usually this part is turned down, so as to expose a part of the instep, and is ornamented with bead-work.[117]Moccasins of a sort were also made of corn husks.In 1653 Dutch official Adriaen van der Donck wrote:Around their waist they all [i.e.both men and women] wear a belt made of leather, whalefin, whalebone, or wampum. The men pull a length of duffel cloth—if they have it—under this belt, front and rear, and pass it between the legs. It is over half an ell [35 centimetres (14 in)] wide and nine quarter-ells [155 centimetres (61 in)] long, which leaves a square flap hanging down in front and back ... Before duffel cloth was common in that country, and sometimes even now when it cannot be had, they took for that purpose some dressed leather or fur—The women also wear a length of woolen cloth of full width [165 centimetres (65 in)] and an ell and a quarter [90 centimetres (35 in)] long, which comes halfway down the leg. It is like a petticoat, but under it, next to the body, they wear a deerskin which also goes around the waist and ends in cleverly cut pointed edging and fringes. The wealthier women and those who have a liking for it wear such skirts wholly embroidered with wampum ... As for covering the upper part of the body both men and women use a sheet of duffel cloth of full width, i.e. nine and a half quarter-ells, and about three ells 210 centimetres (83 in) long. It is usually worn over the right shoulder and tied in a knot around the waist and from there hangs down to the feet.[185]During the 17th century, Iroquois clothing changed rapidly as a result of the introduction of scissors and needles obtained from the Europeans, and the British scholar Michael Johnson has cautioned that European accounts of Iroquois clothing from the latter 17th century may not have entirely reflected traditional pre-contact Iroquois clothing.[174] In the 17th century women normally went topless in the warm months while wearing a buckskin skirt overlapping on the left while in the winter women covered their upper bodies with a cape-like upper garment with an opening for the head.[193] By the 18th century, cloth colored red and blue obtained from Europeans became the standard material for clothing with the men and women wearing blouses and shirts that usually decorated with beadwork and ribbons and were often worn alongside sliver broaches.[194]By the latter 18th century, women were wearing muslin or calico long, loose-fitting overdresses.[194] The tendency of Iroquois women to abandon their traditional topless style of dressing in the warm months reflected European influence.[194] Married women wore their hair in a single braid held in place by a comb made of bone, antler or silver while unmarried wore their hair in several braids.[194] Warriors wore moccasins, leggings and short kilts and on occasion wore robes that were highly decorated with painted designs.[194] Initially, men's clothing was made of buckskin and were decorated with porcupine quill-work and later on was made of broadcloth obtained from Europeans.[194] The bodies and faces of Iroquois men were heavily tattooed with geometric designs and their noses and ears were pieced with rings made up of wampun or silver.[194] On the warpath, the faces and bodies of the warriors were painted half red, half black.[194] The men usually shaved most of their hair with leaving only a tuft of hair in the center, giving the name Mohawk to their hair style.[194] A cap made of either buckskin or cloth tied to wood splints called the Gus-to-weh that was decorated with feathers was often worn by men.[194] Buckskin ammunition pouches with straps over the shoulder together with belts or slashes that carried powder horn and tomahawks were usually worn by warriors.[194] Quilled knife cases were worn around the neck.[195]  Chiefs wore headdresses made of deer antler.[194] By the 18th century, Iroquois men normally wore shirts and leggings made of broadcloth and buckskin coats.[194] In the 17th and 18th centuries silver armbands and gorgets were popular accessories.[194]By the 1900s most Iroquois were wearing the same clothing as their non-Iroquois neighbors. Today most nations only wear their traditional clothing to ceremonies or special events.[196]Men wore a cap with a single long feather rotating in a socket called a gustoweh. Later, feathers in the gustoweh denote the wearer's tribe by their number and positioning. The Mohawk wear three upright feathers, the Oneida two upright and one down. The Onondaga wear one feather pointing upward and another pointing down. The Cayuga have a single feather at a forty-five degree angle. The Seneca wear a single feather pointing up, and the Tuscarora have no distinguishing feathers.[citation needed]Writing in 1851 Morgan wrote that women's outfits consisted of a skirt (gä-kä'-ah) "usually of blue broadcloth, and elaborately embroidered with bead-work. It requires two yards of cloth, which is worn with the selvedge at the top and bottom; the skirt being secured about the waist and descending nearly to the top of the moccasin." Under the skirt, between the knees and the moccasins,  women wore leggings (gise'-hǎ), called pantalettes by Morgan, "of red broadcloth, and ornamented with a border of beadwork around the lower edge ... In ancient times the gise'-hǎ was made of deer-skin and embroidered with porcupine-quill work." An over-dress (ah-de-a'-da-we-sa) of muslin or calico was worn over the skirt, it is "gathered slightly at the waist, and falls part way down the skirt ... In front it is generally buttoned with silver broaches." The blanket (e'yose) is two or three yards of blue or green broadcloth "it falls from the head or neck in natural folds the width of the cloth, as the selvedges are at the top and bottom, and it is gathered round the person like a shawl."[117]The women wore their hair very long and tied together at the back, or "tied at the back of the head and folded into a tress of about a hand's length, like a beaver tail ... they wear around the forehead a strap of wampum shaped like the headband that some was worn in olden times." "The men have a long lock hanging down, some on one side of the head, and some on both sides. On the top of their heads they have a streak of hair from the forehead to the neck, about the breadth of three fingers, and this they shorten until it is about two or three fingers long, and it stands right on end like a cock's comb or hog's bristles; on both sides of this cock's comb they cut all the hair short, except for the aforesaid locks, and they also leave on the bare places here and there small locks, such as aree in sweeping brushes and then they are in fine array."[185] This is the forerunner to what is today called a "Mohawk hairstyle."The women did not paint their faces. The men "paint their faces red, blue, etc."[185]Plants traditionally used by the Iroquois include Agrimonia gryposepala, which was to treat diarrhea,[197] and interrupted fern, used for blood and venereal diseases and conditions.[198] Cone flower (Echinacea), an immune system booster and treatment for respiratory disease was also known and used.[citation needed] They also give an infusion of Chelidonium majus, another plant & milk to pigs that drool and have sudden movements.[199]:p.45 They use Ranunculus acris, in that apply a poultice of the smashed plant to the chest for pains and for colds, take an infusion of the roots for diarrhea,[200]:p.320 and apply a poultice of plant fragments with another plant to the skin for excess water in the blood.[199]:p.42 Symphyotrichum novae-angliae is used in a decoction for weak skin, use a decoction of the roots and leaves for fevers, use the plant as a "love medicine",[200]:p.463 and use an infusion of whole plant and rhizomes from another plant to treat mothers with intestinal fevers,.[199]:p.65 A decoction of the roots of chicory is used as a wash and applied as a poultice to chancres and fever sores.[200]:p.476 A decoction of the root of Allium tricoccum is used to treat worms in children, and they also use the decoction as a spring tonic to "clean you out".[200]:p.281 Epigaea repens is also utilized, as they use a compound for labor pains in parturition, use a compound decoction for rheumatism, take a decoction of the leaves for indigestion, and they also take a decoction of the whole plant or roots, stalks and leaves taken for the kidneys.[200]:p.410 A pounded infusion of the roots of Potentilla canadensis is given as an antidiarrheal.[201] They also use Senna hebecarpa as a worm remedy and take a compound decoction of it as a laxative.[202] The whole plant of Solidago rugosa is used for biliousness and as liver medicine, and they take decoction of its flowers and leaves dizziness, weakness or sunstroke.[203] The Iroquois take a compound decoction of the Carex oligosperma as an emetic before running or playing lacrosse.[204] They also use Waldsteinia fragarioides taking a compound decoction of  the plants  as a blood remedy, and applying a poultice of the smashed plants to snakebites.[205]The Iroquois also used quinine, chamomile, ipecac, and a form of penicillin.[206]The Iroquois have historically followed a matriarchal system. No person is entitled to 'own' land, but it is believed that the Creator appointed women as stewards of the land. Traditionally, the Clan Mothers appoint leaders, as they have raised children and are therefore held to a higher regard. By the same token, if a leader does not prove sound, becomes corrupt or does not listen to the people, the Clan Mothers have the power to strip him of his leadership.[207]The Iroquois have traditionally followed a matrilineal system, with women holding property and hereditary leadership passing through their lines. Historically women have held the dwellings, horses and farmed land, and a woman's property before marriage has stayed in her possession without being mixed with that of her husband. Men and women have traditionally had separate roles but both hold real power in the Nations. The work of a woman's hands is hers to do with as she sees fit. Historically, at marriage, a young couple lived in the longhouse of the wife's family. A woman choosing to divorce a shiftless or otherwise unsatisfactory husband is able to ask him to leave the dwelling and take his possessions with him.[208]The children of a traditional marriage belong to their mother's clan and gain their social status through hers. Her brothers are important teachers and mentors to the children, especially introducing boys to men's roles and societies. The clans are matrilineal, that is, clan ties are traced through the mother's line. If a couple separates, the woman traditionally keeps the children.[209] The chief of a clan can be removed at any time by a council of the women elders of that clan. The chief's sister has historically been responsible for nominating his successor.[209] The clan mothers, the elder women of each clan, are highly respected. It is regarded as incest by the Iroquois to marry within one's matrilineal clan, but considered acceptable to marry someone from the same patrilineal clan.[210]Like many cultures, the Iroquois' spiritual beliefs changed over time and varied across tribes. Generally, the Iroquois believed in numerous deities, including the Great Spirit, the Thunderer, and the Three Sisters (the spirits of beans, maize, and squash). The Great Spirit was thought to have created plants, animals, and humans to control "the forces of good in nature", and to guide ordinary people.[211] Orenda was the Iroquoian name for the magical potence found in people and their environment.[212] The Iroquois believed in the orenda, the spiritual force that flowed all things, and believed if people were respectful of nature, then the orenda would harnessed to bring about positive results.[213] There were three types of spirits for the Iroquois: 1) Those living on the earth 2) Those living above the earth and 3) the highest level of spirits controlling the universe from high above with the most highest being known variously as the Great Spirit, the Great Creator or the Master of Life.[213]Sources provide different stories about Iroquois creation beliefs. Brascoupé and Etmanskie focus on the first person to walk the earth, called the Skywoman or Aientsik. Aientsik's daughter Tekawerahkwa gave birth to twins, Tawiskaron, who created vicious animals and river rapids, while Okwiraseh created "all that is pure and beautiful".[214]  After a battle where Okwiraseh defeated Tawiskaron, Tawiskaron was confined to "the dark areas of the world", where he governed the night and destructive creatures.[214] Other scholars present the "twins" as the Creator and his brother, Flint.[215] The Creator was responsible for game animals, while Flint created predators and disease. Saraydar (1990) suggests the Iroquois do not see the twins as polar opposites but understood their relationship to be more complex, noting "Perfection is not to be found in gods or humans or the worlds they inhabit."[216]Descriptions of Iroquois spiritual history consistently refer to dark times of terror and misery prior to the Iroquois Confederacy, ended by the arrival of the Great Peacemaker. Tradition asserts that the Peacemaker demonstrated his authority as the Creator's messenger by climbing a tall tree above a waterfall, having the people cut down the tree, and reappearing the next morning unharmed.[216] The Peacemaker restored mental health to a few of the most "violent and dangerous men", Ayonhwatha and Thadodaho, who then helped him bear the message of peace to others. [217]After the arrival of the Europeans, some Iroquois became Christians, among them the first Native American Saint, Kateri Tekakwitha, a young woman of Mohawk-Algonquin parents. The Seneca sachem Handsome Lake, also known as Ganeodiyo,[218] introduced a new religious system to the Iroquois in the late 18th century,[219] which incorporated Quaker beliefs along with traditional Iroquoian culture.[211] Handsome Lake's teachings include a focus on parenting, appreciation of life, and peace.[218]  A key aspect of Handsome Lake's teachings is the principle of equilibrium, wherein each person's talents combined into a functional community. By the 1960s, at least 50% of Iroquois followed this religion.[211]Dreams play a significant role in Iroquois spirituality, providing information about a person's desires and prompting individuals to fulfill dreams. To communicate upward, humans can send prayers to spirits by burning tobacco.[211]Iroquois ceremonies are primarily concerned with farming, healing, and thanksgiving. Key festivals correspond to the agricultural calendar, and include Maple, Planting, Strawberry, Green Maize, Harvest, and Mid-Winter (or New Year's), which is held in early February.[211] The ceremonies were given by the Creator to the Iroquois to balance good with evil.[216] In the 17th century, Europeans described the Iroquois as having 17 festivals, but only 8 are observed today.[213] The most important of the ceremonies were the New Year Festival, the Maple Festival held in late March to celebrate spring, the Sun Shooting Festival which also celebrates spring, the Seed Dance in May to celebrate the planting of the crops, the Strawberry Festival in June to celebrate the ripening of the strawberries, the Thunder Ceremony to bring rain in July, the Green Bean Festival in early August, the Green Corn Festival in late August and the Harvest Festival in October.[213] Of all the festivals, the most important were the Green Corn Festival to celebrate the maturing of the corn and the New Year Festival.[213] During all of the festivals, men and women from the False Face Society, the Medicine Society and the Husk Face Society would dance wearing their masks in attempt to humor the spirits that controlled nature.[213] The most important of the occasions for the masked dancers to appear were the New Year Festival, which was felt to be an auspicious occasion to chase the malevolent spirits that were believed to cause disease.[213]During healing ceremonies, a carved "False Face Mask" is worn to represent spirits in a tobacco-burning and prayer ritual. False Face Masks are carved in living trees, then cut free to be painted and decorated.[220] False Faces represent grandfathers of the Iroquois, and are thought to reconnect humans and nature and to frighten illness-causing spirits.[218] The False Face Society continues today among modern Iroquois. The Iroquois have three different medical societies. The False Face Company conducts rituals to cure sick people by driving away spirits; the Husk Face Society is made up of those had dreams seen as messages from the spirits and the Secret Medicine Society likewise conducts rituals to cure the sick.[221] There are 12 different types of masks worn by the societies.[190] The types of masks areA) The Secret Society of Medicine Men and the Company of Mystic Animals1) Divided mask that painted half black and half red.2) Masks with exaggerated long noses.3) Horn masks4) Blind masks without eye sockets.B) Husk Face Society5) Masks made of braided cornC) False Face Society6) Whistling masks7) Masks with smiling faces.8) Masks with protruding tongues.9) Masks with exaggerated hanging mouths.10) Masks with exaggerated straight lops.11) Masks with spoon-lips.12) Masks with a disfigured twisted mouth.The "crooked face" masks with the twisted mouths, the masks with the spoon lips and the whistling masks are the Doctor masks.[190] The other masks are "Common Face" or "Beggar" masks that are worn by those who help the Doctors.[173] The Husk Face Society performs rituals to communicate with the spirits in nature to ensure a good crop, the False Face Society performs rituals to chase away evil spirits and the Secret Medicine Society performs rituals to cure diseases.[222] The grotesque masks represent the faces of the spirits that the dancers are attempting to please.[190] Those wearing Doctor masks blow hot ashes into the faces of the sick to chase away the evil spirits that are believed to be causing the illness.[190] The masked dancers often carried turtle shell rattles and long staffs.[173]Condolence ceremonies are conducted by the Iroquois for both ordinary and important people, but most notably when sachems died. Such ceremonies were still held on Iroquois reservations as late as the 1970s.[211] After death, the soul is thought to embark on a journey, undergo a series of ordeals, and arrive in the sky world. This journey is thought to take one year, during which the Iroquois mourn for the dead. After the mourning period, a feast is held to celebrate the soul's arrival in the skyworld."Keepers of the faith" are part-time specialists who conduct religious ceremonies. Both men and women can be appointed as keepers of the faith by tribe elders.[211]The Iroquois traditionally celebrate six major festivals throughout the year.[117] These usually combine a spiritual component and ceremony, a feast, a chance to celebrate together, sports, entertainment and dancing. These celebrations have historically been oriented to the seasons and celebrated based on the cycle of nature rather than fixed calendar dates.For instance, the Mid-winter festival, Gi'-ye-wä-no-us-quä-go-wä ("The supreme belief") ushers in the new year. This festival is traditionally held for one week around the end of January to early February, depending on when the new moon occurs that year.[117]:pp.200–201Iroquois art from the 16th and 17th centuries as found on bowls, pottery and clay pipes show a mixture of animal, geometrical and human imagery.[195] Moose hair was sometimes attached to tumplines or burden straps for decorative effect.[195] Porcupine quillwork was sewn onto bags, clothing and moccasins, usually in geometrical designs.[195] Other designs included the "great turtle" upon North America was said to rest; the circular "skydome" and wavy designs.[195] Beads and clothes often featured semi-circles and waves which meant to represent the "skydome" which consisted of the entire universe together with the supernatural world above it, parallel lines for the earth and curved lines for the "celestial tree".[195] Floral designs were first introduced in the 17th century, reflecting French influence, but did not become truly popular until the 19th century.[195] Starting about 1850 the Iroquois art began to frequently feature floral designs on moccasins, caps, pouches and pincushions, which were purchased by Euro-Americans.[223] The British historian Michael Johnson described the Iroquois artwork meant to be sold to whites in the 19th century as having a strong feel of "Victoriana" to them.[223] Silver was much valued by the Iroquois from the 17th century onward, and starting in the 18th century, the Iroquois became "excellent silversmiths", making silver earrings, gorgets and rings.[223]The favorite sport of the Iroquois was lacrosse (O-tä-dä-jish′-quä-äge in Seneca).[117] This version was played between two teams of six or eight players, made up of members of two sets of clans (Wolf, Bear, Beaver, and Turtle on one side vs. Deer, Snipe, Heron, and Hawk on the other among the Senecas). The goals were two sets of poles roughly 450 yards (410 m) apart.[note 1] The poles were about 10 feet (3.0 m) high and placed about 15 feet (4.6 m) apart.[note 2] A goal was scored by carrying or throwing a deer-skin ball between the goal posts using netted sticks—touching the ball with hands was prohibited. The game was played to a score of five or seven. The modern version of lacrosse remains popular as of 2015.[224]A popular winter game was the snow-snake game.[117] The "snake" was a hickory pole about 5–7 feet (1.5–2.1 m) long and about .25 inches (0.64 cm) in diameter, turned up slightly at the front and weighted with lead. The game was played between two sides of up to six players each, often boys, but occasionally between the men of two clans. The snake, or Gawa′sa, was held by placing the index finger against the back end and balancing it on the thumb and other fingers. It was not thrown but slid across the surface of the snow. The side whose snake went the farthest scored one point. Other snakes from the same side which went farther than any other snake of the opposing side also scored a point; the other side scored nothing. This was repeated until one side scored the number of points which had been agreed to for the game, usually seven or ten.The Peach-stone game (Guskä′eh) was a gambling game in which the clans bet against each other.[117]  Traditionally it was played on the final day of the Green Corn, Harvest, and Mid-winter festivals. The game was played using a wooden bowl about one foot in diameter and six peach-stones (pits) ground to oval shape and burned black on one side. A "bank" of beans, usually 100, was used to keep score and the winner was the side who won them all. Two players sat on a blanket-covered platform raised a few feet off the floor. To play the peach stones were put into the bowl and shaken. Winning combinations were five of either color or six of either color showing.Players started with five beans each from the bank. The starting player shook the bowl; if he shook a five the other player paid him one bean, if a six five beans. If he shook either he got to shake again. If he shook anything else the turn passed to his opponent. All his winnings were handed over to a "manager" or "managers" for his side. If a player lost all of his beans another player from his side took his place and took five beans from the bank. Once all beans had been taken from the bank the game continued, but with the draw of beans now coming from the winnings of the player's side, which were kept out of sight so that no one but the managers knew how the game was going. The game was finished when one side had won all the beans.The game sometimes took quite a while to play, depending on the starting number of beans, and games lasting more than a day were common.The First Nations Lacrosse Association is recognized by the Federation of International Lacrosse as a sovereign state for international lacrosse competitions. It is the only sport in which the Iroquois field national teams and the only indigenous people's organization sanctioned for international competition by any world sporting governing body.Each clan has a group of personal names which may be used to name members. The clan mother is responsible for keeping track of those names not in use, which may then be reused to name infants. When a child becomes an adult he takes a new "adult" name in place of his "baby" name. Some names are reserved for chiefs or faith keepers, and when a person assumes that office he takes the name in a ceremony in which he is considered to "resuscitate" the previous holder. If a chief resigns or is removed he gives up the name and resumes his previous one.[225]Although the Iroquois are sometimes mentioned as examples of groups who practiced cannibalism, the evidence is mixed as to whether such a practice could be said to be widespread among the Six Nations, and to whether it was a notable cultural feature. Some anthropologists have found evidence of ritual torture and cannibalism at Iroquois sites, for example, among the Onondaga in the sixteenth century.[226][227] However, other scholars, most notably anthropologist William Arens in his controversial book, The Man-Eating Myth, have challenged the evidence, suggesting the human bones found at sites point to funerary practices, asserting that if cannibalism was practiced among the Iroquois, it was not widespread.[228] Modern anthropologists seem to accept the probability that cannibalism did exist among the Iroquois,[229] with Thomas Abler describing the evidence from the Jesuit Relations and archaeology as making a "case for cannibalism in early historic times ... so strong that it cannot be doubted."[230] Scholars are also urged to remember the context for a practice that now shocks the modern Western society. Sanday reminds us that the ferocity of the Iroquois' rituals "cannot be separated from the severity of conditions ... where death from hunger, disease, and warfare became a way of life".[231]The missionaries Johannes Megapolensis, François-Joseph Bressani, and the fur trader Pierre-Esprit Radisson present first-hand accounts of cannibalism among the Mohawk.  A common theme is ritualistic roasting and eating the heart of a captive who has been tortured and killed.[185] "To eat your enemy is to perform an extreme form of physical dominance."[232]As mentioned above in section 5.1 War, Haudenosaunee peoples participated in “mourning wars” to obtain captives.[233] Leland Donald suggests in “Slavery in Indigenous North America” that captives and slaves were interchangeable roles.[234] There have been archaeological studies to support that Haudenosaunee peoples did in fact have a hierarchal system that included slaves.[235] Note that the term slave in Haudenosaunee culture is identified by spiritual and revengeful purposes, not to be mistaken for the term slave in the African Slave Trade.[236]CaptureTo obtain slaves, Haudenosaunee peoples battled in “mourning wars”.[237][238][239] After the wars were over, Haudenosaunee warriors journeyed back to their villages with the new slaves they had captured. During these journeys, slaves were routinely tortured or even killed by their captors.[238][240] Leland Donald writes that captives “were killed if they could not keep up, tried to escape, or members of the attacking party could not restrain their emotions”.[238] Daniel Richter suggests that keeping the pace may not have been an easy task, writing that “warriors might slowly lead prisoners by a rope between the lines of men, women and children [captives]”.[241] If a prisoner survived all the obstacles on the march back to a Haudenosaunee village, the torture did not end. Slaves were mutilated and beaten for several days upon arrival by Haudenosaunee warriors.[242] After the initiation process, slaves were either killed, or welcomed into the nation where they would be replacing a deceased member of that community.[243]Adoption PolicySlaves brought onto Haudenosaunee territory were mainly adopted into families or kin groups that had lost a person.[238] Although if that person had been vital for the community they “were usually replaced by other kin-group members” and “captives were…adopted to fill lesser places”.[244] During adoption rituals, slaves were to reject their former life and be renamed as part of their “genuine assimilation”.[245] The key goal of Haudenosaunee slavery practices was to have slaves assimilate to Haudenosaunee culture to rebuild population after one or many deaths.[244] Children[246]and Indigenous peoples of neighbouring villages[247] to the Haudenosaunee are said to have been good slaves because of their better ability to assimilate. That being said, the role of a slave was not a limited position and whenever slaves were available for capture they were taken, no matter their age, race, gender etc.[248]Once adopted, slaves in Haudenosaunee communities had potential to move up in society.[249] Since slaves were replacing dead nation members, they took on the role of that former member if they could prove that they could live up to it.[249] Their rights within the aforementioned framework were still limited though, meaning slaves performed chores or labor for their adoptive families.[246] Also, there are a few cases where slaves were never adopted into families and their only role was to perform tasks in the village.[238] These types of slaves may have been used solely for exchange.[250] Slave trade was common in Haudenosaunee culture and it aimed to increase Haudenosaunee population.[251]TortureSlaves were often tortured once captured by the Haudenosaunee. Torture methods consisted of, most notably, finger mutilation, among other things.[252][253] Slaves endured torture not only on their journey back to Haudenosaunee nations, but also during initiation rituals and sometimes throughout their enslavement.[243] Finger mutilation was common as a sort of marking of a slave.[254] In "Northern Iroquoian Slavery", Starna and Watkins suggest that sometimes torture was so brutal that captives died before being adopted.[255] Initial torture upon entry into the Haudenosaunee culture also involved binding, bodily mutilation with weapons, and starvation, and for female slaves: sexual assault.[253][256][243][257] Starvation may have lasted longer depending on the circumstance. Louis Hennepin was captured by Haudenosaunee peoples in the 17th Century and recalled being starved during his adoption as one of "Aquipaguetin"’s replacement sons.[258] Indigenous slaves were also starved by their captors, such as Louis Hennepin was.[259] If torture lead to the slave’s death, often times Haudenosaunee peoples ate the victim.[260][261] The brutality of Haudenosaunee slavery wasn’t without its purposes though; torture was used to demonstrate a power dynamic between the slave and the “master” to constantly remind the slave that they were inferior.[262][263]LanguageLanguage played another role in Haudenosaunee slavery practices. Slaves were often referred to as "domestic animals" or "dogs" which were equivalent to the word to "slave".[264] This use of language suggests that slaves were dehumanized, that slaves were "domesticated" and another that slaves were to be eaten as Haudenosaunee peoples ate dogs.[265][266] Jaques Bruyas wrote a dictionary of the Mohawk language where the word “Gatsennen” is defined as "Animal domestique, serviteur, esclave" the English translation being "domestic animal, butler, slave".[267] There are also more language accounts of slaves being compared to animals (mostly dogs) in Oneida and Onondaga language.[264] This language not only serves as a proof that slavery did exist, but also that slaves were at the bottom of the hierarchy.[268]Changes After ContactInevitably, Haudenosaunee slavery practices changed after European contact. With the arrival of European diseases came the increase in Haudenosaunee peoples obtaining captives as their population kept decreasing.[269][270] During the 17th Century, Haudenosaunee peoples banded together to stand against settlers.[271] By the end of the century, Haudenosaunee populations were mostly made up of captives from other nations.[250] Among the Indigenous groups targeted by the Haudenosaunee were the Wyandot who were captured in such large numbers that they lost their independence for a large period of time.[250][272] “Mourning wars” became essential to rebuild numbers, but also Haudenosaunee warriors began targeting French and later English colonizers.[250][273] Similarly to Indigenous slaves, European slaves were tortured by the Haudenosaunee using finger mutilation and sometimes cannibalism.[260] European captives did not make for good slaves though because they resisted even more so than Indigenous captives and they did not understand rituals such as renaming and forgetting their past.[274] For this reason most European captives were either used as ransom or murdered upon arrival to Haudenosaunee territory.[250] A lot of Europeans were not captured though, and instead they became trading partners with the Haudenosaunee.[269] Indigenous slaves were now being traded amongst European settlers and some slaves even ended up in Quebec households.[269] In the end, European contact lead to adoptees outnumbering the Haudenosaunee in their own communities, these slaves were too hard to control in large numbers and so came the finality of Haudenosaunee slavery practices.[250]The first five nations listed below formed the original Five Nations (listed from east to west, as they were oriented to the sunrise); the Tuscarora became the sixth nation in 1722.1 Not one of the original Five Nations; joined 1722.2 Settled between the Oneida and Onondaga.Within each of the six nations, people belonged to a number of matrilineal clans. The number of clans varies by nation, currently from three to eight, with a total of nine different clan names.According to the Worldmark Encyclopedia of Cultures and Daily Life, The Iroquois Confederacy had 10,000 people at its peak, but by the 18th century, their population had decreased to 4,000, recovering only to 7,000 by 1910.[276]According to data compiled in 1995 by Doug George-Kanentiio, a total of 51,255 Six Nations people lived in Canada. These included 15,631 Mohawk in Quebec; 14,051 Mohawk in Ontario; 3,970 Oneida in Ontario; and a total of 17,603 of the Six Nations at the Grand River Reserve in Ontario.[277] More recently according to the Six Nations Elected Council, some 12,436 on the Six Nations of the Grand River reserve, the largest First Nations reserve in Canada,[278] as of December 2014 and 26,034 total in Canada.[279]In 1995, tribal registrations among the Six Nations in the United States numbered about 30,000 in total, with the majority of 17,566 in New York. The remainder were more than 10,000 Oneida in Wisconsin, and about 2200 Seneca-Cayuga in Oklahoma.[277] As the nations individually determine their rules for membership or citizenship, they report the official numbers. (Some traditional members of the nations refuse to be counted.)[277] There is no federally recognized Iroquois nation or tribe, nor are any Native Americans enrolled as Iroquois.In the 2000 United States census, 80,822 people identified as having Iroquois ethnicity (which is similar to identifying as European), with 45,217 claiming only Iroquois ancestry. There are the several reservations in New York: Cayuga Nation of New York(~450[citation needed],) St. Regis Mohawk Reservation (3248 in 2014),[280] Onondaga Reservation (473 in 2014),[280] Oneida Indian Nation (~ 1000[citation needed]), Seneca Nation of New York (~8000[citation needed]) and the Tuscarora Reservation (1100 in 2010[citation needed]). Some lived at the Oneida Nation of Wisconsin on the reservation there counting some 21,000 according to the 2000 census. Seneca-Cayuga Nation in Oklahoma has more than 5,000 people in 2011.[281] In the 2010 Census, 81,002 persons identified as Iroquois, and 40,570 as Iroquois only across the United States.[282] Including the Iroquois in Canada, the total population numbered over 125,000 as of 2009.[276]The Grand Council of the Six Nations is an assembly of 56 Hoyenah (chiefs) or sachems. Today, the seats on the Council are distributed among the Six Nations as follows:When anthropologist Lewis Henry Morgan studied the Grand Council in the 19th century, he interpreted it as a central government. This interpretation became influential, but Richter argues that while the Grand Council served an important ceremonial role, it was not a government in the sense that Morgan thought.[26][27][28] According to this view, Iroquois political and diplomatic decisions are made on the local level, and are based on assessments of community consensus. A central government that develops policy and implements it for the people at large is not the Iroquois model of government.Unanimity in public acts was essential to the Council. In 1855, Minnie Myrtle observed that no Iroquois treaty was binding unless it was ratified by 75% of the male voters and 75% of the mothers of the nation.[283] In revising Council laws and customs, a consent of two-thirds of the mothers was required.[283] The need for a double supermajority to make major changes made the Confederacy a de facto consensus government.[284]The women traditionally held real power, particularly the power to veto treaties or declarations of war.[283] The members of the Grand Council of Sachems were chosen by the mothers of each clan. If any leader failed to comply with the wishes of the women of his tribe and the Great Law of Peace, the mother of his clan could demote him, a process called "knocking off the horns". The deer antlers, an emblem of leadership, were removed from his headgear, thus returning him to private life.[283][285]Councils of the mothers of each tribe were held separately from the men's councils. The women used men as runners to send word of their decisions to concerned parties, or a woman could appear at the men's council as an orator, presenting the view of the women. Women often took the initiative in suggesting legislation.[283]The term "wampum" refers to beads made from purple and white mollusk shells on threads of elm bark.[174] Species used to make wampum include the highly prized quahog clam (Mercenaria mercenaria) which produces the famous purple colored beads. For white colored beads the shells from the channeled whelk (Busycotypus canaliculatus), knobbed whelk (Busycon carica), lightning whelk (Sinistrofulgur perversum), and snow whelk (Sinistrofulgur laeostomum) are used.[286]Wampum was primarily used to make wampum belts by the Iroquois, which Iroquois tradition claims was invented by Hiawatha to console chiefs and clan mothers who lost family members to war.[174] Wampum belts played a major role in the Condolence Ceremony and in the raising of new chiefs.[174] Wampum belts are used to signify the importance of a specific message being presented. Treaty making often involved wampum belts to signify the importance of the treaty.[174] A famous example is "The Two Row Wampum" or "Guesuenta", meaning "it brightens our minds", which was originally presented to the Dutch settlers, and then French, representing a canoe and a sailboat moving side-by-side along the river of life, not interfering with the other's course. All non-Native settlers are, by associations, members of this treaty. Both chiefs and clan mothers wear wampum belts as symbol of their offices.[174]"The Covenant Belt" was presented to the Iroquois at the signing of the Canandaigua Treaty. The belt has a design of thirteen human figures representing symbolically the Thirteen Colonies of the United States. The house and the two figures directly next to the house represent the Iroquois people and the symbolic longhouse. The figure on the left of the house represent the Seneca Nation who are the symbolic guardians of the western door (western edge of Iroquois territory) and the figure to the right of the house represents the Mohawk who are the keepers of the eastern door (eastern edge of Iroquois territory).[286]The Hiawatha belt is the national belt of the Iroquois and is represented in the Iroquois Confederacy flag. The belt has four squares and a tree in the middle which represents the original five nations of the Iroquois. Going from left to right the squares represent the Seneca, Cayuga, Oneida and Mohawk. The Onondaga are represented by an eastern white pine which represents the Tree of Peace. Traditionally the Onondaga are the peace keepers of the confederacy. The placement of the nations on the belt represents the actually geographical distribution of the six nations over their shared territory, with the Seneca in the far west and the Mohawk in the far east of Iroquois territory.[286]The Haudenosaunee flag created in the 1980s is  based on the Hiawatha Belt ... created from purple and white wampum beads centuries ago to symbolize the union forged when the former enemies buried their weapons under the Great Tree of Peace."[287] It represents the original five nations that were united by the Peacemaker and Hiawatha. The tree symbol in the center represents an Eastern White Pine, the needles of which are clustered in groups of five.[288]Historians in the 20th century have suggested the Iroquois system of government influenced the development of the United States's government.[citation needed] Contact between the leaders of the English colonists and the Iroquois started with efforts to form an alliance via the use of treaty councils. Prominent individuals such as Benjamin Franklin and Thomas Jefferson were often in attendance.[citation needed] Bruce Johansen proposes that the Iroquois had a representative form of government.[289] The Six Nations' governing committee was elected by the men and women of the tribe, one member from each of the six nations. Giving each member the same amount of authority in the council ensured no man received too much power, providing some of the same effect as the United States's future system of checks and balances.[citation needed]Consensus has not been reached on how influential the Iroquois model was to the development of United States' documents such as the Articles of Confederation and the U.S. Constitution.[290] The influence thesis has been discussed by historians such as Donald Grinde[291] and Bruce Johansen.[292] In 1988, the United States Congress passed a resolution to recognize the influence of the Iroquois League upon the Constitution and Bill of Rights.[293] In 1987, Cornell University held a conference on the link between the Iroquois' government and the U.S. Constitution.[294]Scholars such as Jack N. Rakove challenge this thesis. Stanford University historian Rakove writes, "The voluminous records we have for the constitutional debates of the late 1780s contain no significant references to the Iroquois" and notes that there are ample European precedents to the democratic institutions of the United States.[295] Historian Francis Jennings noted that supporters of the thesis frequently cite the following statement by Benjamin Franklin, made in a letter from Benjamin Franklin to James Parker in 1751:[296] "It would be a very strange thing, if six Nations of ignorant savages should be capable of forming a Scheme for such a Union ... and yet that a like union should be impracticable for ten or a Dozen English Colonies," but he disagrees that it establishes influence. Rather, he thinks Franklin was promoting union against the "ignorant savages" and called the idea "absurd".[297]The anthropologist Dean Snow has stated that although Franklin's Albany Plan may have drawn inspiration from the Iroquois League, there is little evidence that either the Plan or the Constitution drew substantially from that source. He argues that "... such claims muddle and denigrate the subtle and remarkable features of Iroquois government. The two forms of government are distinctive and individually remarkable in conception."[298]Similarly, the anthropologist Elizabeth Tooker has concluded that "there is virtually no evidence that the framers borrowed from the Iroquois." She argues that the idea is a myth resulting from a claim made by linguist and ethnographer J.N.B. Hewitt that was exaggerated and misunderstood after his death in 1937.[299] According to Tooker, the original Iroquois constitution did not involve representative democracy and elections; deceased chiefs' successors were selected by the most senior woman within the hereditary lineage in consultation with other women in the tribe.[299]The Grand Council of the Iroquois Confederacy declared war on Germany in 1917 during World War I and again in 1942 in World War II.[300]The Haudenosaunee government has issued passports since 1923, when Haudenosaunee authorities issued a passport to Cayuga statesman Deskaheh (Levi General) to travel to the League of Nations headquarters.[301]More recently, passports have been issued since 1997.[302] Before 2001 these were accepted by various nations for international travel, but with increased security concerns across the world since the September 11 attacks, this is no longer the case.[303] In 2010, the Iroquois Nationals lacrosse team was allowed by the U.S. to travel on their own passports to the 2010 World Lacrosse Championship in England only after the personal intervention of Secretary of State Hillary Clinton. However, the British government refused to recognize the Iroquois passports and denied the team members entry into the United Kingdom.[304][305]The Onondaga Nation spent $1.5 million on a subsequent upgrade to the passports designed to meet 21st-century international security requirements.[306]
Oaxaca
Oaxaca is located in Southeastern Mexico.[10] It is bordered by the states of Guerrero to the west, Puebla to the northwest, Veracruz to the north, Chiapas to the east. To the south, Oaxaca has a significant coastline on the Pacific Ocean.The state is best known for its indigenous peoples and cultures. The most numerous and best known are the Zapotecs and the Mixtecs, but there are sixteen that are officially recognized. These cultures have survived better than most others in Mexico due to the state's rugged and isolating terrain. Most live in the Central Valleys region, which is also an economically important area for tourism, with people attracted for its archeological sites such as Monte Albán, and Mitla,[11] and its various native cultures and crafts. Another important tourist area is the coast, which has the major resort of Huatulco and sandy beaches of Puerto Escondido, Puerto Ángel, Zipolite, Bahia de Tembo, and Mazunte.[12] Oaxaca is also one of the most biologically diverse states in Mexico, ranking in the top three, along with Chiapas and Veracruz, for numbers of reptiles, amphibians, mammals and plants.[13]The name of the state comes from the name of its capital city, Oaxaca. This name comes from the Nahuatl word "Huaxyacac",[14] which refers to a tree called a "guaje" (Leucaena leucocephala) found around the capital city. The name was originally applied to the Valley of Oaxaca by Nahuatl-speaking Aztecs and passed on to the Spanish during the conquest of the Oaxaca region. The modern state was created in 1824, and the state seal was designed by Alfredo Canseco Feraud and approved by the government of Eduardo Vasconcelos.[15] Nahuatl word "Huaxyacac" [waːʃ.ˈja.kak] was transliterated as "Oaxaca" using Medieval Spanish orthography, in which the x represented the voiceless postalveolar fricative ([ʃ], the equivalent of English sh in "shop"), making "Oaxaca" pronounced as [waˈʃaka]. However, during the sixteenth century the voiceless fricative sound evolved into a voiceless velar fricative ([x], like the ch in Scottish "loch"), and Oaxaca began to be pronounced [waˈxaka]. In present-day Spanish, Oaxaca is pronounced [waˈxaka] or [waˈhaka], the latter pronunciation used mostly in dialects of southern Mexico, the Caribbean, much of Central America, some places in South America, and the Canary Islands and western Andalusia in Spain where [x] has become a voiceless glottal fricative ([h]).[16]Most of what is known about prehistoric Oaxaca comes from work in the Central Valleys region. Evidence of human habitation dating back to about 11,000 years BC has been found in the Guilá Naquitz cave near the town of Mitla. This area was recognized as a UNESCO World Heritage site in 2010 in recognition for the "earliest known evidence of domesticated plants in the continent, while corn cob fragments from the same cave are said to be the earliest documented evidence for the domestication of maize." More finds of nomadic peoples date back to about 5000 BC, with some evidence of the beginning of agriculture. By 2000 BC, agriculture had been established in the Central Valleys region of the state, with sedentary villages.[17] The diet developed around this time would remain until the Spanish Conquest, consisting primarily of harvested corn, beans, chocolate, tomatoes, chili peppers, squash and gourds. Meat was generally hunted and included tepescuintle, turkey, deer, peccary, armadillo and iguana.[18]The oldest known major settlements, such as Yanhuitlán and Laguna Zope are located in this area as well. The latter settlement is known for its small figures called "pretty women" or "baby face." Between 1200 and 900 BC, pottery was being produced in the area as well. This pottery has been linked with similar work done in La Victoria, Guatemala. Other important settlements from the same time period include Tierras Largas, San José Mogote and Guadalupe, whose ceramics show Olmec influence.[17] The major native language family, Oto-Manguean, is thought to have been spoken in northern Oaxaca around 4400 BC and to have evolved into nine distinct branches by 1500 BC.[18]Historic events in Oaxaca as far back as the 12th century are described in pictographic codices painted by Zapotecs and Mixtecs in the beginning of the colonial period, but outside of the information that can be obtained through their study, little historical information from pre-colonial Oaxaca exist, and our knowledge of this period relies largely on archaeological remains.[19] By 500 BC, the central valleys of Oaxaca were mostly inhabited by the Zapotecs, with the Mixtecs on the western side. These two groups were often in conflict throughout the pre-Hispanic period.[20] Archeological evidence indicates that between 750 and 1521, there may have been population peaks of as high as 2.5 million.[19]The Zapotecs were the earliest to gain dominance over the Central Valleys region.[18] The first major dominion was centered in Monte Albán, which flourished from 500 BC until AD 750 .[19] At its height, Monte Albán was home to some 25,000 people and was the capital city of the Zapotec nation.[18] It remained a secondary center of power for the Zapotecs until the Mixtecs overran it in 1325.[20] The site contains a number of notable features including the Danzantes, a set of stone reliefs and the finding of fine quality ceramics.[17]Starting from AD 750 previous large urban centers such as Monte Alban fell across the Oaxaca area and smaller dominions grew and evolved until the Spanish Conquest in 1521.[19] Between 700 and 1300, the Mixtec were scattered among various dominions, including those of Achiutla, Tequixtepec-Chazumba, Apoala and Coixtlahuaca. The Zapotecs occupied a large region from Central Valleys region to the Isthmus of Tehuantepec.[19] However, no major city state like Monte Albán arose again, with villages and city-states remaining small, between 1,000 and 3,000 people with a palace, temple, market and residences. In a number of cases, there were Mesoamerican ball courts as well. These and larger centers also functioned as military fortresses in time of invasion. Important Zapotec and Mixtec sites include Yagul, Zaachila, Inguiteria, Yanhuitlan, Tamazulapan, Tejupan, and Teposcolula. During nearly all of this time, these various entities were at war with one another, and faced the threat of Aztec expansion.[19]While the Zapotec remained dominant in many parts of the Central Valleys and into the Isthmus of Tehuantepec, the Mixtec were pushing into Zapotec territory, taking Monte Alban. In areas they conquered, they became prolific builders, leaving behind numerous and still unexplored sites. However, the conquest of the Central Valleys was never completed with pressure coming from the Aztecs in Tenochtitlan in the 14th and 15th centuries. The Zapotecs and Mixtecs both allied themselves and fought among themselves as they tried to maintain their lands and valuable trade routes between the high central plains of Mexico and Central America.[18][20]The first Aztecs arrived to the Oaxaca area in 1250, but true expansion into the region began in the 15th century. In 1457, Moctezuma I invaded the Tlaxiaco and Coixtlahuaca areas, gaining control, demanding tribute and establishing military outposts.[19] These were Mixtec lands at first, pushing these people even further into Zapotec territory.[17] Under Axayacatl and Tizoc, the Aztec began to take control of trade routes in the area and part of the Pacific Coast. By this time, the Zapotec were led by Cosijoeza with the government in Zaachila in the latter 15th century. Under Ahuitzotl, the Aztecs temporarily pushed the Zapotecs into Tehuantepec and established a permanent military base at Huaxyacac (Oaxaca city). The Aztecs were stopped only by the Spanish Conquest[17] These conquests would change most of the place names in parts of Oaxaca to those from the Nahuatl language.[19] In 1486 the Aztecs established a fort on the hill of Huaxyácac (now called El Fortín), overlooking the present city of Oaxaca. This was the major Aztec military base charged with the enforcement of tribute collection and control of trade routes.[18]However, Aztec rule in Oaxaca would last only a little more than thirty years.[18]Very soon after the fall of Tenochtitlan (Mexico City), Spaniards arrived in Oaxaca. Moctezuma II had informed Hernando Cortes that the area had gold. In addition, when Zapotec leaders heard about the Spanish conquest of the Aztec Empire, they sent an offer of an alliance.[18] Several captains and representatives were sent to the area to explore the area, looking for gold, and routes to the Pacific to establish trade routes to Asian spice markets. The most prominent of Cortés' captains to arrive here were Gonzalo de Sandoval, Francisco de Orozco and Pedro de Alvarado. They overcame the main Aztec military stronghold only four months after the fall of Tenochtitlan.[17] Their reports about the area prompted Cortés to seek the title of the Marquis of the Valley of Oaxaca from the Spanish Crown.The valley Zapotecs, the Mixtecs of the Upper Mixteca, the Mazatecas and the Cuicatecas, for the most part, chose not to fight the newcomers, instead negotiating to keep most of the old hierarchy but with ultimate authority to the Spanish.[17][18] Resistance to the new order was sporadic and confined to the Pacific coastal plain, the Zapotec Sierra, the Mixea region and the Isthmus of Tehuantepec. The Mixe put up the most resistance to intrusions on their lands. They not only resisted during the first decade or so of Spanish occupation, like other groups, but through the rest of the 16th century. The last major Mixe rebellion came in 1570, when they burned and looted Zapotec communities and threatened to destroy the Spanish presidio of Villa Alta. However, this rebellion was put down by the Spanish, in alliance with about 2,000 Mixtecs and Aztecs. From this point, the Mixe retreated far into the mountains to isolate themselves, where they are found today.[18]The first priest in the territory was Juan Diaz, who accompanied Francisco de Orozco and built the first church in what is now the city of Oaxaca. He was followed by Bartolome de Olmade and others who began the superficial conversion of a number of indigenous people, including the baptism of Zapotec leader Cosijoeza. In 1528, the Dominicans settled in the city of Oaxaca, forming the Bishopric of Oaxaca in 1535, and began to spread out from there, eventually reaching Tehuantepec and the coast. Other orders followed such as the Jesuits in 1596, the Mercedarians in 1601, and others in the 17th and 18th centuries.[17][18]Spanish conquest and subsequent colonization had a devastating effect on the native population, due to European diseases and forced labor. In some areas the native population nearly or completely disappeared.[19] It has been estimated that the native population of the region declined from 1.5 million in 1520 to 150,000 in 1650.[18] Eventually, this would prompt the Spanish to import African slaves to some regions of the state, mostly in the Costa Chica. This poor treatment of indigenous and African populations would continue through the colonial period.[21] Initially, the Spanish did not change native power structures and allowed nobles to keep their privileges as long as they were loyal to the Spanish crown. However, all indigenous people were eventually lumped into one category as the Spanish halted warfare among the city-states and created the official category of "indio" (Indian).[19]Settlers arriving from Spain brought with them domestic animals that had never been seen in Oaxaca: horses, cows, goats, sheep, chickens, mules and oxen.[18] New crops such as sugar cane, vanilla and tobacco were introduced.[19] However, landholding still remained mostly in indigenous hands, in spite of the fact that only 9% of Oaxaca's terrain is arable. Spanish officials and merchants tried to take indigenous privileges due to their social status, but this was resisted. While some of this was violent, the dominant response was to resort to the administrative-judicial system or yield. Violence was reserved for the worst of situations.[18] One native product to reach economic importance during the colonial period was the cochineal insect, used for the making of dyes for textiles. This product was exported to Europe, especially in the 17th and 18th centuries. The use of this insect faded in the 19th century with the discovery of cheaper dyes.[19]For much of the colonial period, the state (then an intendencia or province) was relatively isolated with few roads and other forms of communication. Most politics and social issues were strictly on the local level. Despite Spanish domination, the indigenous peoples of Oaxaca have maintained much of their culture and identity, more so than most other places in Mexico. Part of this is due to the geography of the land, making many communities isolated.[19]By 1810, the city of Oaxaca had 18,000 inhabitants, most of whom where mestizos or mixed indigenous/European. During the Mexican War of Independence the government of this area remained loyal to the Spanish Crown. When representatives of Miguel Hidalgo y Costilla came to meet with them, they were hanged and their heads left out in view. Some early rebel groups emerged in the state, such as those led by Felipe Tinoco and Catarino Palacios, but they were also eventually executed. After 1812, insurgents began to have some success in the state, especially in the areas around Huajuapan de León, where Valerio Trujano defended the city against royalist forces until José María Morelos y Pavón was able to come in with support to keep the area in rebel hands. After that point, insurgents had greater success in various parts of the state, but the capital remained in royalist hands until the end of the war.[17]The state was initially a department after the war ended in 1821, but after the fall of emperor Agustín de Iturbide, it became a state in 1824 with Jose Maria Murguia named as its first governor.[17]During the 19th century, Oaxaca and the rest of Mexico was split between liberal (federalist) and conservative (centralist) factions. The political and military struggles between the factions resulted in wars and intrigues. Vicente Guerrero, a liberal, was executed by firing squad in Cuilapam in 1831. Liberal Manuel Gomez Pedraza became governor in 1832 but was opposed by General Estaban Moctezuma. He and commandant Luis Quintanar persecuted liberals in the state, including Benito Juárez. The constant warfare had a negative effect on the state's economy and those in the Tehuantepec area supported a separatist movement which was partially successful in the 1850s.[17]Two Oaxacans, Benito Juarez and Porfirio Díaz were prominent players in the Reform War. It is difficult to overstate Juárez's meaning to the state. He was born on March 21, 1806 in the village of San Pablo Guelatao and was full blooded Zapotec. He began his career studying to be a priest then a lawyer.[18][22] In 1847, Juarez became governor of Oaxaca, but still faced stern opposition from conservatives such as Lope San Germán. With the success of the Plan de Ayutla, Juarez became governor again, and worked to remove privileges and properties from the Church and landed classes. The Constitution of 1857, was ratified in Oaxaca city, and Juarez left the governor's position to become President of Mexico.[17] He was president during one of Mexico's most turbulent times, fighting invading French forces and conservatives. As a liberal, he imposed many of the reforms which remain today including those in education and separation of church and state. He is also considered to be a legend and a symbol for the indigenous population of the state.[18]Porfirio Díaz was Juárez's ally through the French Intervention. French imperial forces took Oaxaca city, which was defended by Porfirio Díaz, landing the latter in prison. The capital was later recaptured by the liberals under Carlos Oronoz. However, soon after Juarez took back the presidency, Porfirio Díaz declared rebellion against him from Oaxaca in 1872 under the Plan de Tuxtepec. Juárez died in office. Diaz would succeed in obtaining the presidency and did not relinquish it until the Mexican Revolution.[17]During Diaz's rule, called the Porfiriato, a number of modernization efforts were undertaken in the state such as public lighting, first with gas then with electricity, railroad lines, new agriculture techniques and the revitalization of commerce. However, most of the benefits of these advances went to national and international corporations and workers and indigenous farmers organized against the regime.[17]After the Mexican Revolution broke out, Diaz was soon ousted and the rest of the war was among the various factions that had power in different parts of the country. Various leaders such as Francisco I. Madero, Victoriano Huerta and Venustiano Carranza came to the state during this time. However; the most important force in the area was the Liberation Army of the South under Emiliano Zapata. This army would ally and fight against the previous leaders, especially Venustiano Carranza,[17] and hold various portions of the state until 1920.[18] At the end of the Revolution, a new state constitution was written and accepted in 1922.[17]A series of major disasters occurred in the state from the 1920s to the 1940s. In 1928, a series of earthquakes destroyed many of the buildings in the capital. A much larger earthquake in 1931, was the largest in the state's history, devastating a number of cities along the coast. The 1930s brought the Great Depression, which along with the disasters, prompted wide scale migration to Mexico City. In 1944, torrential rains caused massive flooding in the Tuxtepec region, causing hundreds of deaths.[23]In the 1940s and 1950s, new infrastructure projects were begun. These included the Izúcar-Tehuantepec section of the Panamerican Highway and the construction of the Miguel Alemán Dam.[23] From the 1980s to the present, there has been much development of the tourism industry in the state. This tourism, as well as the population growth of the capital, prompted the construction of the Oaxaca-Mexico City highway in 1994.[24] Development of tourism has been strongest in the Central Valleys area surrounding the capital, with secondary developments in Huatulco and other locations along the coast. This development was threatened by the violence associated with the 2006 uprising, which severely curtailed the number of incoming tourists for several years.[25]On February 12, 2008, a 6.4 magnitude earthquake was recorded in Oaxaca.[26]From the Mexican Revolution until the 2000s, the ruling PRI party held control of almost all of Oaxacan politics from the local to the state level.[27] Challenges to the rule were sporadic and included the student movements of the 1970s, which did bring down the state government.[28] Teachers' strikes had been frequent since then, culminating in the 2006 uprising in Oaxaca city, which brought in groups protesting the heavy marginalization of the poor.[25] The PRI lost its 80-year hold on the state government in 2010 with the election of the PAN gubernatorial candidate Gabino Cué Monteagudo. This has led to speculation of major changes for the state.[27]In 2017, a series of earthquakes brought death and destruction to parts of Mexico, including Oaxaca. According to the US Geological Survey, early on September 23, 2017, a magnitude 6.1 earthquake shook Matías Romero, about 275 miles southeast of Mexico City. The epicenter was about 12 miles from Matías Romero and centered approximately between the two even more violent earthquakes felt by Mexico earlier in the month, and is considered an aftershock. On September 8, an 8.1 magnitude quake had struck off of the southern Pacific coast, near Chiapas state. Mexico City, on September 19, then endured a 7.1 magnitude quake, which also marked the 32nd anniversary of the devastating 1985 earthquake, in which more than 10,000 people had been killed.[29]The state is located in the south of Mexico, bordered by the states of Puebla, Veracruz, Chiapas and Guerrero with the Pacific Ocean to the south. It has a territory of 93,967 km2 (36,281 sq mi), accounting for less than 5% of Mexico's territory.[30][31] Here several mountain chains come together,[18] with the elevation varying from sea level to 3,759 m (12,333 ft) asl,[31] averaging at 1,500 m (4,921 ft) asl.[18] Oaxaca has one of the most rugged terrains in Mexico, with mountain ranges that abruptly fall into the sea. Between these mountains are mostly narrow valleys, canyons and ravines. Major elevations in the state include Zempoaltepetl (3,396 m or 11,142 ft asl), El Espinazo del Diablo, Nindú Naxinda Yucunino and Cerro Encantado.[31] Oaxaca's has 533 km (331 mi) of coastline with nine major bays.[14]The mountains are mostly formed by the convergence of the Sierra Madre del Sur, the Sierra Madre de Oaxaca and the Sierra Atravesada into what is called the Oaxaca Complex (Complejo Oaxaqueño). The Sierra Madre del Sur runs along the coast with an average width of 150 km (93 mi) and a minimum height of 2,000 meters (6,562 ft) asl with peaks over 2,500 m (8,202 ft) asl. In various regions the chain is locally known by other names, such as the Sierra de Miahuatlán and the Sierra de la Garza. The Sierra Madre de Oaxaca enters the state from the Puebla and Veracruz borders in the Tuxtepec region, running northwest to southeast towards the Central Valleys region, then onto the Tehuantepec area. Local names for parts of this range include Sierra de Tamazulapan, Sierra de Nochixtlan, Sierra de Huautla, Sierra de Juárez, Sierra de Ixtlan and others. Average altitude is 2,500 m (8,202 ft) asl with peaks over 3,000 m (9,843 ft) asl and width averages at about 75 km (47 mi). The Sierra Atravesada is a prolongation of the Sierra Madre de Chiapas. This range is not as high as the other two with an average elevation of just over 600 meters (1,969 ft). Most of it is located in the Juchitán district running east-west.[31]The only valleys of any real size are the Central Valleys between Etla and Miahuatlán, which contains the city of Oaxaca. Smaller populated valleys include Nochixtlan, Nejapa, Cuicatlan and Tuxtepec. Small mesas contain population centers such as Putla, Juxtlahuaca, Tamazulapan, Zacatepec, Tlaxiaco and Huajuapan. The largest canyons in the state are those in the Cuicatlán area and include the Cortés, Galicia and María in the municipality of Tlaxiaco. There are a very large number of small canyons as well as ravines and arroyos of all sizes.[31]The mountainous terrain allows for no navigable rivers; instead, there are a large number of smaller ones, which often change name from area to area. The continental divide passes through the state, meaning that there is drainage towards both the Gulf of Mexico and the Pacific Ocean. Most of the drainage towards the Gulf is represented by the Papaloapan and Coatzacoalcos Rivers and their tributaries such as the Grande and Salado Rivers. Three rivers account for most of the water headed for the Pacific: the Mixteco, Atoyac and Tehuantepec Rivers with their tributaries.[31] Other important rivers and streams include the Tequisistlán, Santo Domingo, Putla, Minas, Puxmetacán-Trinidad, La Arena, Cajonos, Tenango, Tonto, Huamelula, San Antonio, Ayutla, Joquila, Copalita, Calapa, Colotepec, Aguacatenango-Jaltepec, Los Perros, El Corte, Espíritu Santo, Sarabia, Ostuta, Petapa and Petlapa.[32]Major cities include Huajuapan de León, Juchitán de Zaragoza, Oaxaca(Oaxaca de Juárez), Puerto Escondido, Salina Cruz, San Pedro Pochutla, San Juan Bautista Cuicatlán, San Juan Bautista Tuxtepec, Santa Cruz Xoxocotlán, Santa Lucía del Camino, Santa María Asunción Tlaxiaco, Santiago Pinotepa Nacional and Tehuantepec (Santo Domingo Tehuantepec).Regions and districts of Oaxaca are:[33]While the state is within the tropical latitudes, its climate varies with altitude.[31] There are three principal climate regions in the state. The first is the hot and Subtropical lands. This accounts for about 30% of the state. The next is the semi hot and semi humid regions which account for about 18%, and temperate and semi humid at about 16%. All of these climates experience a rainy season in the summer and early fall.[32] As most of the state is over 2,000 m (6,562 ft) above sea level, average temperature is about 18 °C (64.4 °F), except near the coast. The coastline along with the regions of Yautepec, Putla, parts of Huahuapan and Silacayoapan are hot and relatively dry. Hot and humid climates predominate in Villa Alta, and the Central Valleys area and all others over 2,000 m (6,562 ft) above sea level have a temperate climate. A few of the highest peaks, such as those in Tehuantepec and Putla have a cold climate. Precipitation varies from between 430 to 2,700 mm (16.9 to 106.3 in) per year. The Sierra Mazteca, Textepec and other areas near the Veracruz border have rains year round. The rest of the state receives the majority of its rain during the summer and early fall. The higher elevations can experience freezing temperatures in December and January.[31] The Chivela mountain pass in Isthmus of Tehuantepec provides a gap for the wind to pass between mountain ranges,[34] creating the best conditions for wind power in Mexico.[35]The state has a total population of about 3.5 million, with women outnumbering men by 150,000 and about 60% of the population under the age of 30. It is ranked tenth in population in the country. Fifty three percent of the population lives in rural areas.[36] Most of the state's population growth took place between 1980 and 1990. Life expectancy is 71.7 for men and 77.4 for women, just under the national average. Births far outpace deaths. In 2007, there were 122,579 birth and 19,439 deaths.[37] Approximately 85% profess the Catholic faith.[38]Demographically, Oaxaca stands out due to the high percentage of indigenous peoples.[39][40] It is estimated that at least a third are speakers of indigenous languages (with 50% not able to speak Spanish), accounting for 53% of Mexico's total indigenous language speaking population.[38][39] The state straddles two Mesoamerican cultural areas. The first extends into the state from the Mayan lands of Chiapas, Yucatán and Guatemala. The northeast of the state is part of the cultures of the Valley of Mexico, with historical influence seen from ancient cities such as Teotihuacan, Tula and Tenochtitlan.[19]The main reason that indigenous languages and cultures have been able to survive here is the rugged terrain, which isolate communities.[18][41] This also has the effect of dividing the state into small secluded communities, which have developed independently over time. There are 16 ethno linguistic groups recognized by the Instituto Nacional Indigenista[42] who maintain their individual languages, customs and traditions well into the colonial period and to some extent to the present day.[18] However, some studies put the number of cultures in the state as high as 4,000.[19] This makes Oaxaca the most ethnically complex of Mexico's 31 states.[18]The most populous indigenous groups in Oaxaca are the Zapotec or Mixtec. Several other languages of the Oto-Manguean languages are spoken in Oaxaca: The Triques, Amuzgos and Cuicatecs are linguistically most closely related to the Mixtecs, The languages of the Chocho, Popoloca and Ixcatec peoples are most closely related to that of the Mazatecs. The Chatino language is grouped with the Zapotecan branch of Oto-Manguean. The languages of the Zoque and Mixe peoples belong to the Mixe–Zoquean languages. Other ethnic groups include the Chontalees, Chinantecs, the Huaves and Nahuas.[43] As of 2005, a total of 1,091,502 people were counted as speaking an indigenous language.[42]The largest indigenous group in the state are the Zapotecs at about 350,000 people or about 31% of the total indigenous population.[18][38][42] The Zapotec have an extremely long history in the Central Valleys region and unlike other indigenous groups, do not have a migration story. For them, they have always been here. Zapotecs have always called themselves Be'ena'a, which means The People. Zapotec territory extends in and around the Central Valleys region of the state, around the capital city of Oaxaca. The Zapotec language has historically been and is still the most widely spoken in the state, with four dialects that correspond to the four subdivisions of these people: Central Valleys and Isthmus, the Sierra de Ixtlan, Villa Alta and Coapan.[40] Zapotec communities can be found in 67 municipalities. The various Zapotec dialects account for 64 of the total 173 still surviving forms of Oto-Manguean.[18]The second largest group are the Mixtecs at just over 240,000 people or 27% of the indigenous population.[38][42] These people established themselves in the northwest of Oaxaca and far southern Puebla over 3,000 years ago, making them one of the oldest communities in the region. These same people put pressure on the Zapotec kingdoms until the Spanish conquered both peoples in the 16th century.[40] Mixtec territory is divided into three sub regions. The Upper Mixteca covers 38 municipalities and is the most populated region. The Lower Mixteca includes 31 municipalities. The Coastal Mixtecs are a small group. Today, the Mixtecs call themselves Ñuu Savi, the people of the rain. The Mixtecan language family, as one of the largest and most diverse families in the Oto-Manguean group, includes three groups of languages: Mixtec, Cuicatec, and Trique.[18]The Mazatecos number at about 165,000 or 15% of Oaxaca's indigenous population.[38] (perfil soc) These people occupy the northernmost area of the state, in the upper Sierra Madre Oriental mountains and the Papaloapan Basin. The Mazatecos call themselves Ha shuta enima, which means People of Custom. Some historians believe that the Mazatecos descend from the Nonoalca-Chichimecas, who migrated south from Tula early in the 12th century. While most live in Oaxaca, a significant number of Mazatecos also occupy Veracruz and Puebla.[18]The Chinantecos account for about 10% of Oaxaca's indigenous people, numbering at about 104,000.[42][44] They inhabit the Chinantla region of north central Oaxaca near the border of Veracruz. The Chinanteco language has as many as 14 different dialects and is part of the Oto-Manguean linguistic group. Historians believe that those living in this region struggled to maintain their independence against sudden and numerous attacks by the Zapotecs, Mixtecs, Mixes and Aztecs. The latter, led by Moctezuma I, finally conquered the Chinantla region during the 15th century.[18][44]The Mixe people account for another 10% of the indigenous population at just over 103,000 people.[38][42] The Mixe are an isolated group in the northeastern part of the state, close to the border of Veracruz. Their region includes 19 municipalities and 108 communities. The Mixes call themselves Ayuuk, which means The People. It is unknown where the Mixe migrated from, with some speculating from as far as Peru, but they arrived in waves from 1300 to 1533. They came into conflict with the Mixtecs and Zapotecs, but allied themselves with the Zapotecs against the Aztecs, then resisted the Spanish. The Mixe language has seven dialects and this group has the highest rate of monolingualism (36% of speakers in the year 2000) of any Indigenous group in Mexico.[18]Minorities include the Chatino (42,477),[18] the Trique (18,292),[44] the Huave people (15,324),[44] the Cuicatecos (12,128),[44] the Zoque, also called the Aiyuuk (roughly 10,000), the Amuzgos (4,819),[44] the Chontales of Oaxaca (4,610), the Tacuates (1,725),[44] the Chocho or Chocholtec (524), the Ixcatecos (207),[44] the Popolocas (61)[44] and a small population of Nahuatl speaking peoples in the border area with Puebla.[40]Ritualistitic and shamanic religious practices were prevalent in Oaxaca valley, until the Spanish invaded the valley in 1521. Proselytism was also started in 1521, Christianity was ushered into the valley and eventually took firm roots.[45][46]The ancient religious practices have been dated by archaeological findings (over a 15 years period of excavations by two Archaeologists of Michigan University) to be more than 7000 years old. Initially, 7000 years ago, the people were "hunters and gatherers with no fixed abode".[attribution needed][45][46] With development of agricultural practices, with maize as the main crop and settled villages getting established over several centuries, a warrior type of societal culture evolved by 500 BC, with the Zapotec state getting into shape. Concurrently, ceremonious religious practices with ritualistic and shamanistic dancing around stone marked floors came to be observed (a pre-Zapotec dance floor dated to 6650 BC testifies this). Even cannibalistic practices were noted. The ritualistic practices were formalized, as permanent settlements were established, and temples were built to perform the rituals as per a set of calendar annual events. There were two interconnected calendars prevalent at the time- one of 260 days and another of 365 days, which synchronized every 52 years. In subsequent years, as upper strata of society (an "elite class") came into existence, the religious practices and the temple got more formalized with priests controlling the community's religion. Religion started to evolve around the ritualistic practices but with more defined role of religion under the monarchic rule which came into effect along with "the religious systems that were the previous source of social authority". Monte Alban was founded around 500 BCE. It is inferred that from 1500 BC, Zapotec society evolved as an organized "autonomous ascribed-status peasant societies". The ritual buildings in the valley dated to this period testify this observation.[45][46] Dr. Richard Sosis, an anthropologist at the University of Connecticut has summarised the archaeological findings with the observation:[45]the Michigan archaeologists' study delineated the process of religion adapting to different environments as Oaxacan society changed. Among foragers, ritual serves to cement solidarity, he said, and the "powerful moralistic gods that we associate with contemporary religions" are a later development, introduced at the stage when priests have acquired control of a religion and "are effectively controlling the masses through ritual activities that instill the fear of supernatural punishment.When Christianity made inroads into the Valley in 1521, the valley was part of the Aztec tribute empire with Tenochtitlan as the capital (present day Mexico City) and Spanish settlements came into existence to exploit the rich land and mineral resources of the valley. The first record of Baptism in the valley was that of the King of Teozapotlan, the most important Valley ruler, in 1521. He was baptized as Don Juan Cortes. Nobles, who converted to Christianity, were permitted to keep their traditional rights under a 1557 order by Phillip II of Spain. Spaniards pursued proselytisation activity with dedicated single-minded devotion throughout the 18th century with the "goal of saving the souls of their subjects". It took many years of dogged persuasion to discourage the Zapotec people to give up their pantheon of idolatry, shamanistic and cannibalistic practices of the Mesoamerican religion, which was denigrated by the Church.[45]Now, in Mexico, Roman Catholics are 89% of the total population.[47] Only 47% of Oaxacan Catholics attend church services weekly, one of the lowest rates of the developing world.[48] In absolute terms, Mexico has the world's second largest number of Catholics after Brazil. While most indigenous Mexicans are at least nominally Catholic, some combine or syncretize Catholic practices with native traditions.[49]The National Presbyterian Church in Mexico has a relatively high percentage of followers in Oaxaca, one of its stronger states.[50]Although it is the fifth-largest state in Mexico, it has the most biodiversity. There are more than 8,400 registered plant species, 738 bird species and 1,431 terrestrial vertebrate species, accounting for 50% of all species in Mexico. It is also among the five highest-ranking areas in the world for endangered species.[25][39] The state has important ecological zones such as the Selva Zoque in the northeast.[51] Vegetation varies from those adapted to hot and arid conditions such as cacti, to evergreen tropical forest on the coasts.[31] Forests in the higher elevations consist of conifers, broadleafed trees and a mixture of the two. In the lower elevations by the coast there are evergreen and deciduous rainforest, with those dropping leaves doing so in the dry season. In the driest areas mesquite, some cactus and grasslands can be found.[52] There are also 58 species of aquatic plants.[25]Wildlife includes a wide variety of birds, small to medium-sized mammals and some larger ones such as deer and wildcats, reptiles and amphibians. Off the coast there are fish and shellfish, as well as dolphins and whales which pass by during their migrations.[31] The state is a prolific place for reptiles such as turtles, lizards, snakes and crocodiles. Of the 808 registered reptile species nationwide, 245 are found in the state.[39] The state has the most amphibian species at 133,[25] with one-third of all Mexican species of frogs and salamanders.[39] It is home to 120 species of freshwater fish, 738 species of birds (70% of Mexico's total) and 190 species of mammals.[25] Some insect forms such as grasshoppers, larvae and cochineal have economic importance for the state and there are several species of 'giant' stick insects indigenous to the region (such as Bacteria horni which has a body length of up to 22 cm).[25] The most important ocean creatures commercially are shrimp, tuna, bonito, huachinango and mojarra. Sea turtles used to be exploited for both their meat and eggs but this was stopped by the federal government in the 1990s.[31] The coast of Oaxaca is an important breeding area for sea turtles such as the leatherback (Dermochelys coriacea), which is classified as endangered throughout its global range. Despite conservation efforts starting in the 1970s, the number of nesting sites and nesting turtles has dramatically decreased.[53]Conservation efforts in the state are hampered by high marginalization, lack of economic alternatives, agricultural conflicts, change of land use (agricultural activities, fires), over-exploitation and pollution of natural water sources, inadequate forest management and illegal tree felling, unsustainable coastal tourist developments, climate change, limited local capacity, and limited local knowledge and valuation of natural resources.[39] However, there are seven officially protected natural areas in the state: Benito Juárez National Park at 3,272 ha (8,090 acres), Huatulco National Park at 11,845 ha (29,270 acres), Lagunas de Chacahua National Park at 14,920 ha (36,900 acres), Playa de Escobilla Sanctuary at 30 ha (74 acres), Playa de la Bahía de Chacahua Sanctuary at 31 ha (77 acres), Tehuacán-Cuicatlán Biosphere Reserve at 490,678 ha (1,212,490 acres) and Yagul Natural Monument at 1,076 ha (2,660 acres).[52]Lagunas de Chacahua National Park, created in 1937,[54] lies about 54 km (34 mi) west of Puerto Escondido, near a village called Zapotalito. It can be reached via Federal Highway 200 or by boat from Puerto Escondido. The park encompasses 132.73 square kilometres (51.25 square miles), about 30 km2 (12 sq mi) of which is taken by various lagoons such as the ‘Laguna de Chacahua, ‘Laguna de La Pastoria, and Laguna Las Salinas.[55] There are various smaller lagoons that are connected by narrow channels.[54] The rest of the park consists of dry land.[55]The park has 10 different types of vegetation: "selva espinosa", swampland, deciduous, sub-tropical broadleaf, mangrove, savannah, "bosque de galleria", "tular", palm trees, and coastal dunes. 246 species of flowers and 189 species of animals have been documented so far in the park. Birds such as storks, herons, wild ducks, blue-winged teals, pelicans, and spoonbills can be found here. Three species of turtles also visit the park to lay their eggs.[55]Benito Juárez National Park is located 5 km (3.1 mi) to the north of Oaxaca within the municipal limits of San Felipe del Agua and Donaji, Oaxaca, and San Andres Huayapan of the central district. It was designated as a national park under a presidential decree, in 1937. The topography of the park has an elevation range varying from 1,650 to 3,050 metres (5,413 to 10,007 feet) above sea level. The climate is Coastal sub-humid and Temperate sub-humid. The main rivers that flow through the park are the Huayapan and San Felipe rivers. Most of their flows used to be utilized to meet drinking water needs of Oaxaca through an aqueduct in the early part of the 18th century, during the colonial period. However, it is now tapped for water supply through piped system to the city.[56][57] The park covers 2,737 hectares (6,760 acres), including the 3,111-meter (10,207 ft) high "Cerro de San Felipe" (San Felipe Mountain), part of the Sierra Madre de Oaxaca which has metamorphic rock formations. It has a rich biodiversity of flora and fauna. There are pine and oak forests in the upper reaches of the mountain, while the lower reaches have scrub oaks, and tropical deciduous forest in the canyons. Most of the forest is secondary growth, having been previously forested.[56][57]Huatulco National Park, also known as Bahias de Huatulco National Park – Huatulco, was initially declared a protected area and later decreed as a National Park on July 24, 1998.[58] Located in the Santa Maria Huatulco town, to the west of Cruz Huatulco, it extends to an area of 11,890 hectares (29,400 acres). In the low lands of the park, there 9,000 species of plant (about 50% of the species are reported throughout the country) in the forest and mangroves in the coastal belt. Fauna species have been identified as 264, which includes armadillos and white-tailed deer. Bird species are counted at 701, which include hummingbirds, pelicans and hawks. The amphibian and reptile species are counted to be 470, which include Black Iguana, salamanders and snakes. Dolphins, whales and turtles are sighted species off the coast line, out of the identified 100 marine species. Vegetation is dominated by the low forest growth of caducifolia in 80% area with the unusual feature of 50 ft (15.24 m) high trees.[59]Tehuacán-Cuicatlán Biosphere Reserve, which encompasses the states of Puebla and Oaxaca in Mexico, was established as reserve in 1998 covering an area of 490,187 ha (1,211,280 acres), with an altitudinal range of 600 to 2,950 m (1,969 to 9,678 ft). It is in the valley of the Tehuacán-Cuicatlán-Quiotepec. The six rivers which flow through the reserve are the Tomellín, Chiquito, Las Vueltas, Salado, Zapotitán and Río Grande of the Papaloapan watershed, which finally flow into the Gulf of Mexico. On account of wide variation in topography and annual rainfall, the micro-climatic conditions in the reserve has created a biosphere reserve, which is very rich in flora and fauna. The rich biodiversity of the preserve consists of 910 plant genus, 2,700 vascular species, 102 species of mammals, 356 species of birds which includes the endangered Green Macaw (Ara militaris), and 53 species of reptiles. However, the reserve is faced with threats from poaching, deforestation, overgrazing, and trash scattered on the highways and secondary roads that pass close and through the reserve. Inadequate patrolling staff is an issue which needs to be addressed to remove the threats to the biosphere reserve.[60]The state was created by a federal decree in 1824, and is the fifth largest state in Mexico.[19] The state government consists of an executive branch, headed by the governor, a unicameral legislature and a judiciary branch headed by a state supreme court presided over by seven judges.[61]The area of Oaxaca has been divided into small entities since far back into the pre-Hispanic period. Much of the reason for this is the highly mountainous geography, although the occupation of the area by numerous ethnicities is a factor as well. The area resisted large scale Spanish domination through the colonial era, and maintained local traditions and customs better than other areas of Mexico. Even today, the state has far more municipalities and semi autonomous local authorities than any other state in the nation.[19] Oaxaca is divided into 570 municipalities, about one-quarter of the total of the country.[18] Many of the municipalities of the state had been ill-defined from colonial times until the 1990 INEGI survey which delineated them with exact coordinates.[19] The most populated municipality is the capital, followed by San Juan Bautista Tuxtepec and Juchitán de Zaragoza.[62] There is also a system of thirty districts to group municipalities.[19][63]The state has traditionally been divided into seven regions, which took into account variables such as ethnic makeup, economics and geography. Today, the state is divided into eight regions called Valles Centrales, La Cañada, La Mixteca, Sierra Madre del Sur, Sierra Norte, El Istmo, La Costa and El Golfo. These still take into account the traditional variables, but geography plays a larger role.[63] La Cañada Region comprises the fourth and fifth districts with a total of 45 municipalities. The Coast Region consists of the 21st, 22nd and 30th districts with a total of 50 municipalities; the Isthmus Region consists of the 28th and 29th districts with a total of 41 municipalities; the Mixteca Region consists of the 1st, 2nd, 3rd, 8th, 9th, 10th and 16th districts with a total of 155 municipalities; the Papaloapam Region consists of the 6th and 7th districts with a total of 20 municipalities; the Sierra Sur Region consists of the 15th, 23rd, 26th and 27th districts with 70 municipalities; the Sierra North Region consists of the 12th, 13th and 14th districts with 69 municipalities; the Central Valleys Regions consists of the 11th, 17th, 18th, 19th, 20th, 24th and 25th districts with 121 municipalities.[64]According to the Mexican government agency Conapo (National Population Council), Oaxaca is the third most economically marginalized state in Mexico.[25][65] The state has 3.3% of the population but produces only 1.5% of the GNP.[66] The main reason for this is the lack of infrastructure and education, especially in the interior of the state outside of the capital. Eighty percent of the state's municipalities do not meet federal minimums for housing and education. Most development projects are planned for the capital and the surrounding area. Little has been planned for the very rural areas and the state lacks the resources to implement them.[65] The largest sector of Oaxaca's economy is agriculture, mostly done communally in ejidos or similar arrangements. About 31% of the population is employed in agriculture, about 50% in commerce and services and 22% in industry.[32] The commerce sector dominates the gross domestic product at 65.4%, followed by industry/mining at 18.9% and agriculture at 15.7%.[67]In 45.5% of Oaxaca's municipalities, the population has declined due to migration. Poverty and migration are caused mostly by the lack of economic development in the state, which leaves most of the population working in the least productive sector. This has led to wide scale migration, mostly from the rural areas, to find employment. Within Oaxaca, many people leave rural villages to work in the city of Oaxaca, the Papaloapan area and the coast. Within Mexico, many leave for Mexico City, Mexico State, Sinaloa, Baja California and Baja California Sur. Most of those leaving the state are agricultural workers. As of 2005, over 80,000 people from Oaxaca state live in some other part of Mexico.[65][68] Most of those leaving Oaxaca and Mexico go to the United States. Much of the current wave of emigration began in the late 1970s, and by the 1980s Oaxaca ranked 8th in the number of people leaving for the US from Mexico. Today, that percentage has fallen to 20th. Most of those migrate to the United States, concentrated in California and Illinois.[65] In 2007, estimates of the number of Oaxacans residing in Los Angeles, California ranged from 50,000 to 250,000.[69]The economy of Oaxaca is based on agriculture, especially in the interior of the state.[66] Only 9% of the territory is suitable for agriculture due to the mountainous terrain, so there are limits to this sector.[18][65] The production of food staples, such as corn and beans, is mostly for internal consumption but this production cannot meet demand.[65] The total agricultural production of the state was estimated at 13.4 million tons with a value of 10,528 million pesos in 2007. As of 2000, 1,207,738 hectares are used for the raising of crops, most of which occurs during the annual rainy season, with only 487,963 having crops growing year round. Only 81,197 hectares have irrigation.[67] The variation of climate allows for a wider range of agricultural crops than would otherwise grow in a geographical region of this size.[31] Oaxaca is the nation's second highest producer of grains and agave. It is third in the production of peanuts, mango and sugar cane. It is the second largest producer of goat meat, providing about 10% of the national total.[32] In the more temperate areas crops such as corn, beans, sorghum, peanuts, alfalfa and wheat are grown. In more tropical areas, crops also include coffee, sesame seed, rice, sugar cane and pineapple.[67]Livestock is raised on 3,050,106 hectares or 32% of the state's land. Cattle dominate in the Tuxtepec, Isthmus and Coast regions, with pigs dominating in higher elevations such as the Central Valleys Region. Other animals include sheep, goats, domestic fowl and bees. The value of this production was estimated at 2,726.4 million pesos with cattle comprising over half of this.[67] Coffee is grown in mountain areas near the Pacific Ocean in municipalities such as Santa María Huatulco, Pluma Hidalgo, Candelaria Loxicha, San Miguel del Puerto and San Mateo Piñas. The growing of coffee here dates back to the 17th and 18th centuries when English pirates introduced the plant. Coastal fishing is also a major source of income and in 2007 the total fishing catch was estimated at 9,300 tons with a value of over 174 million pesos.[67]Mining has traditionally been important to the economy and history. Hernán Cortés sought and received the title of the Marquis of the Valley of Oaxaca in order to claim mineral and other rights.[18] Currently coal, salt, chalk, petroleum, marble, lime, graphite, titanium, silver, gold and lead are still extracted.[31][67] Most mines today are located in Etla, Ixtlán, San Pedro Taviche, Pápalo and Salinas Cruz. There is an oil and natural gas refinery in Salinas Cruz, which provides products to the state and other areas on Mexico's Pacific coast.[67]Tourism is important to the state as it is the only sector that is growing and brings substantial income from outside the state, although most tourism is concentrated in the capital and along the coast.[25][66] In 2007, there were 1,927 small grocery stores, 70 tianguis and 167 municipal markets. Tourism accounts for about 30% of the commerce sector of Oaxaca's economy.[67] The state attracts visitors from Mexico and abroad.[25] The state government has been pushing this sector heavily as a means of growing the economy,[25] with major infrastructure projects such as the Oaxaca-Puerto Escondido-Huatulco highway (scheduled to finish in 2018) and the Iberdrola hydroelectric dam.[66]In 2000, there were 612 hotels with 15,368 rooms. Thirteen of these were classed as five stars. The state received 1,564,936 visitors that year, over 80% of whom were from Mexico. The Central Valleys region receives the most visitors (60%), followed by the La Mixteca and Papaloapan regions (29%) and the coast (11%), in spite of the fact that only 7% of the state's attractions are in the Oaxaca city area.[67] One reason for this is that the city of Oaxaca is only four and a half hours away from Mexico City via the federal highway.[14]The state has a total of 18,933.4 km (11,764.7 mi) of roadways. Most of these roadways are in the Papaloapam, Mixteca, Isthmus and Coast Regions.[70] The primary highways in the state include Oaxaca (city)-Cuacnopalan toll road and the Pan-American highway, which crosses the state completely from Puebla to Chiapas. Federal highway 200 hugs the coast connecting communities such as Puerto Escondido, Salinas Cruz and Huatulco with Acapulco and Chiapas. Federal highway 185, also called "Transístmica", crosses the state from the Veracruz border to the coast at Salina Cruz. Federal highway 125 runs from the Puebla state line along the western part of the state. Federal highway 135 leads from Puebla to Oaxaca City then down to Pochutla. Federal highway 175 runs from the Veracruz border to the city of Oaxaca. Other highways include Federal highway 147 and Federal highway 182.[71]There is a railroad line connecting the city of Oaxaca with Mexico City for cargo. The state's major port is Salina Cruz which primarily services ships belonging to PEMEX, bringing crude oil and refined petroleum products along the Mexican coast as well as the United States and Japan.[71] There is also a railroad from Salina Cruz to Veracruz and to Tapachula.Oaxaca-Xoxocotlan Airport (IATA code OAX) is approximately 7 km (4.3 mi) south of Oaxaca city centre. This airport has a runway that measures 2,450 metres (8,038 feet) and a total extension of 435 hectares (1,070 acres) with two hangars.[71] According to figures published by Grupo Aeroportuario del Sureste (ASUR), the airport received 523,104 passengers in 2009. Airlines that fly to the state include Aeroméxico, Volaris, Interjet, and VivaAerobus arriving from Mexico City, Cancun, Guadalajara, Monterrey, and Tijuana. In addition the airport also has nonstop flights to the US thru United Airlines and American Airlines to Houston and Dallas.Local public transportation is offered various local business using pickup trucks, buses and small cargo trucks.(eumed) Oaxaca city has separate first class and second class bus stations, offering services to most places within the state of Oaxaca, including the coastal resorts of Huatulco, Puerto Escondido, Puerto Ángel and Pinotepa Nacional, and also long-distance services to Puebla and Mexico City and other Mexican locations such as Veracruz. Intercity bus services is provided by companies such as ADO, Cristòbal Colòn, SUR, Fletes y Pasajes and AU. Smaller providers provide service in vans, especially between the city of Oaxaca and the coast. These operators have existed only semi-legally in the past but legal issues have since been resolved.[71]From the latter half of the 20th century, the state has produced a number of notable painters such as Rufino Tamayo, Rodolfo Nieto, Rodolfo Morales, and Francisco Toledo. These four painters have been influential in the establishment of new movements of art from the state. These movements have spurred exhibitions, galleries, museums and schools such as the Museo de Arte Contemporaneo (MACO) and Instituto de Artes Gráficas de Oaxaca (IAGO).[72] Many of today's artists from Oaxaca have been inspired by past indigenous paintings as well as the colonial era works of Miguel Cabrera.[73]The state has not produced as many writers as painters but some important names include Adalberto Carriedo, Jacobo Dalevuelta, Andrés Henestrosa and Natalia Toledo.[74]Music and dance are almost inextricably linked to the state's folkloric heritage. Even more modern composers such as Macedonio Alcalá, Samuel, Mondragón Noriega and José López Alavés are strongly influenced by traditional melodies. Traditional music and dance has its roots in the indigenous traditions that existed long before the Spanish arrived. To these traditions were added elements from European culture and Catholicism. The three main traditions to be found in the state are those of the Zapotecs and the Mixtecs, with a small but distinct community of Afro-Mexicans. Some of the best known dances include Los Diablos, La Tortuga, Las Mascaritas and Los Tejorones. In the Afro-Mexican Costa Chica region, a dance called Las Chilenas stands out. La Sandunga is a song that typifies the musical style of the Tehuantepec region and a musical style called "son bioxho" is an endemic form of the son style played with drums, an empty tortoise shell and a reed flute.[75]Oaxacan cuisine varies widely due to the relative geographic isolation of its peoples, and the climates in which foods are produced.[76] Oaxaca's gastronomy is known for its "seven moles," chapulines (grasshoppers), Oaxaca tamales in banana leaves, tasajo (meat cuts?) and mescal.[77] Regional variations include the wide variety of vegetables in the Central Valleys region, fish and shellfish in the Coast and Isthmus regions and the year-round availability of tropical fruit in the Papaloapan area on the Veracruz border. Like most of the rest of Mexico, corn is the staple food, with corn tortillas, called "blandas" accompanying most meals. Black beans are preferred.[76] Oaxaca produces seven varieties of mole called manchamanteles, chichilo, Amarillo, rojo, verde, coloradito and negro.[78] These moles and other dishes are flavored with a variety of chili peppers such as pasillas Oaxaqueños, amarillos, chilhuacles, chilcostles, chile anchos and costeños. Epazote, pitiona and hoja santa are favored herbs in Oaxacan cooking. The last is indispensable for the preparation of verde version of mole.[76]Chocolate, which is grown in the state, plays an important part in the making of certain moles, but is best known for its role as a beverage. The cacao beans are ground then combined with sugar, almonds, cinnamon and other ingredients to form bars. Pieces of these bars are mixed with hot milk or water and drunk.[76][78] Oaxaca cheese is a soft white string cheese which is similar to mozzarella. It is sold in "ropes" which are wound onto themselves into balls. It is eaten cold or lightly melted on quesadillas and other dishes. One unique aspect to Oaxacan cuisine is the consumption of "chapulines," which are a type of grasshopper that has been fried and seasoned with salt, lime and chili pepper.[78]There is a saying in Oaxaca, "Para todo mal, mezcal, para todo bien, también" (For everything bad, mezcal; for everything good, too.) Alcoholic and non alcoholic drinks (as well as food items) based on the maguey plant have been consumed in many parts of Mexico since early in the pre-Hispanic period. The tradition of the making of the distilled liquor called mezcal has been a strong tradition in the Oaxacan highlands since the colonial period. One reason for this is the quality and varieties of maguey grown here. Some varieties, such as espadín and arroquense are cultivated but one variety called tobalá is still made with wild maguey plants. It is made with the heart of the plant which is roasted in pits (giving the final product a smokey flavor) and is sometimes flavored with a chicken or turkey breast added to the mash. It is mezcal, not tequila, and may contain a "worm," which is really a larva that infests maguey plants. The final distilled product can be served as is or can be flavored (called cremas) with almonds, coffee, cocoa fruits and other flavors.[79]The town of Santiago Matatlán calls itself the world capital of mezcal. The best known producer here is Rancho Zapata, which also has a restaurant. It is owned by a man that goes only by the name of Tío (uncle) Pablo, who won first prize for his mecal in Chicago in 2003. In many parts of the Central Valleys area, one can find small stands and stores selling locally made mezcal on roadsides.[25]Most tourist attractions are located in the city of Oaxaca and the Central Valleys region that surrounds it. This area is the cultural, geographical and political center of the state, filled with pre-Hispanic ruins, Baroque churches and monasteries, indigenous markets and villages devoted to various crafts. The capital city, along with nearby Monte Albán together are listed as a World Heritage Site.[25][41] Many of the attractions in the city proper are located between the main square or Zocalo and along Andador Macedonio Alcalà Street, known as the Andador Turístico or Tourist Walkway. These include the Cathedral, the Basilica of Nuestra Señora de la Soledad, Museum of Contemporary Art (MACO), Rufino Tamayo Museum and the Mercado 20 de Noviembre Market, known for its food stands.[25] The most important annual festival is the Guelaguetza, also called the Fiesta del Lunes del Cerro (Festival of Mondays at the Mountain) which occurs each July.[77][80]The largest and most important archeological site is Monte Albán, which was capital of the Zapotec empire.[14] Also important as an archaeological site is the ancient Zapotec center of Mitla at the eastern end of the Central Valleys which is noted for its unique ancient stone fretwork and abstract mosaics.[25][14] Between Mitla and Monte Albán there are a number of other important archeological sites such as Yagul, Dainzú and Lambityeco. The most important of these three is Lambityeco, in the middle of the Tlacolula Valley. It was occupied from 600 BCE to 800 CE and coincides with Monte Alban. It was important at that time for its production of salt.[25] Yagul is a ceremonial center on the side of a mountain. Features include a Mesoamerican ball court, the La Rana courtyard, a temple, palace and other buildings.[77]Other attractions in the area include colonial constructions such as the monasteries in Cuilapan, Tlaxiaco, Coixthlahuaca, Yanhuitlán and Santo Domingo. Churches include the Cathedral in Oaxaca and the main church of Teposcolula.[77] Hierve el Agua is an area with "petrified" waterfalls, where water with extremely high mineral content falls over the side of cliffs, forming stone waterfall-like structures. The name means "boiling water" but the water is not hot; rather it pushes up from the ground in places which looks like water boiling.[77] Santa María del Tule is home to an enormous Montezuma cypress (Taxodium mucronatum) tree which is over 2,000 years old. The town of Zaachila is known for its archeological site and weekly market.[25]The second most important zone for tourism is the coast especially from Puerto Escondido to Huatulco, with sandy beaches on the Pacific Ocean, dolphins, sea turtles, and lagoons with water birds. Many beaches are nearly virgin with few visitors but several areas have been developed such as Puerto Escondido, Huatulco, Puerto Ángel, Zipolite, San Agustinillo and Mazunte.[25][41] Puerto Escondido is an important destination for tourism from within Mexico with beaches such as Playa Carrizalillo and also attracts international surfers to Zicatela Beach, where an annual surfing competition is held.[25] There are also areas of Oaxaca that are promoted for ecotourism such as Lagunas de Chacahua National Park set in 14,267 hectares of lagoons, rivers, beaches, mangroves, rainforest and grasslands with some 136 species of birds, 23 of reptiles, 4 amphibians and twenty types of mammals.[25]Yagul Natural Monument, located in the Tlacolula Valley, 35 km to the east of Oaxaca city, was a settlement in the early part of the Monte Alban 1 Period (500 CE). It flourished as an urban centre, following the abandonment of Monte Alban around 800 BCE. However, even Yagul was abandoned for a brief period, before it became a city-state in Oaxaca. This status continued until the Spanish Conquistadores invaded the valley, which was then a settlement of Zapotecs.[81] The fortified complex is laid out in three zones; the central part approached through a series of steps is a built-up platform that leads to the temples and palaces. It has the largest ball court in the valley and stated to be the largest in the Mesoamerican region.[citation needed] The palace of the rulers is an enormous monolith with six porticos and several entrances, built in stone and clay and covered with stucco. The main tomb has a stone façade, which is beautified with carved human heads and features hieroglyphic motifs on the door slab on both sides. To the south of the Palace of the Six Porticos, there is a narrow street that is paved with stone mosaics extracted from the nearby mountain. The street terminates into a long, narrow room called the 'Sala de Consejo' (Council Chamber).[81][82]Because of its indigenous tradition and abundance of raw materials, Oaxaca is a leading producer of handcrafts in Mexico. Handcrafted items here are noted for their variety and quality. Oaxacan handcrafts are traditionally made with wood, wool, clay and leather and are sold in many venues from local tianguis markets to upscale international stores. The best-known wood craft is the making of "alebrije" figures, which are usually miniature, brightly colored real or imaginary animals. These were originally created from paper and cardboard in Mexico City, but this craft was adapted to native Oaxacan woodcarving to the form it has today. Carver Manuel Jiménez of Arrazola is credited with the creating of the Oaxacan version of this craft. Other wood crafts include the making of masks, toys and utensils. Major woodcarving areas include San MartínTilcajete and Arrazola.[77][83]Pottery has a long tradition that extends into the pre-Hispanic period. Oaxaca shares many pottery types with other parts of Mexico along with two of its own: barro negro and the green glazed pottery of Atzompa. The first is centered in the town of San Bartolo Coyotepec near the capital city. This pottery gets its color from the local clay used to make it and its shine from a technique developed by Doña Rosa Nieto in the mid-20th century. The Atompa green-glazed ware is made much the same way it was in colonial times, although there have been some recent innovations with color and decorative techniques. This pottery is found in Santa María Atzompa, near Oaxaca city.[77]Another major craft category is textiles. Textiles from cotton and other fibers date to early in the pre-Hispanic period on backstrap looms. This form of weaving has been dominated by women since that time. The Spanish introduced the wide European frame loom, which is mostly used by men. Traditional clothing items such as huipils are still made on backstrap looms, while the European looms are used to produce larger and heavier items such as rugs, ponchos and blankets. Most items are produced with cotton or wool fibers, although some maguey fibers can be found and palm fronds are used to produce mats and hats. Embroidery is an important part of indigenous clothing, especially for women. One municipality noted for its indigenous and embroidered clothing is Santo Tomás Jalietza, just south of the city of Oaxaca. The Xochimilco neighborhood of the capital is known for its embroidered tablecloths, napkins and other tableware.[84]Both precious and non-precious metals are worked in the state. Many gold and silver jewelry items are made with filigree (fine metal thread) which is weaved and wrapped into shapes. This technique is Arab in origin and was introduced by the Spanish. The municipalities of Santo Domingo Tehuantepec, Juchitán de Zaragoza and Huajuapan de León are known for this work. Other metals, especially iron, are forged into utilitarian and decorative items in places such as Santiago Jamiltepec and Tlacolula de Matamoros. Items produced include mirrors, frames, figures, knives, machetes and more.[77][85]The state of Oaxaca has no official flag, but the state government uses a flag with a white background and a shield in the center.[clarification needed]The shield consists of a red canvas, wrapped around its upper end; inside within a white oval is the inscription "EL RESPETO AL DERECHO AJENO ES LA PAZ" (Respect for the rights of others is peace), and the slogan words are separated from each other by symbolic representations of nopales. The inner oval is divided into three parts: on the bottom are two arms breaking chains; in the upper left is a stylised image of the state of Oaxaca, with the flower and fruit, in a stylised form, of the huaje tree; and at the top right is the profile of one of the palaces from the archaeological site of Mitla, with a Dominican Cross to its right. Around the oval are distributed seven golden stars, three on the bottom, two on the right above the oval and two to the left above the oval. On the bottom of the canvas is the phrase "ESTADO LIBRE Y SOBERANO DE OAXACA" (The Free and Sovereign State of Oaxaca). Above the canvas is the Shield of Mexico.While the educational system of the state provides services to 1.1 million students in 12,244 schools, with 54,274 teachers,[70] the Mexican government agency Conapo ranks Oaxaca as the third most marginalized state in Mexico, based on factors such as education and housing. 80% of the municipalities of the state do not meet minimum requirements for these services. The Sierra Sur and La Mixteca regions has the most number of municipalities in this category.[65] The average child in Oaxaca attends school for 6.39 years, below the national average of 8 years.[65]In rural areas of the state, there is extremely limited education offerings beyond elementary school. Indigenous people comprise 33% of the state population, of which only 5% ever attain an education beyond the primary grade levels. In addition, 90% of all indigenous teachers do not have satisfactory academic backgrounds.[86]Concerning the general population, most of those aged 15 years or older have finished primary school, but completion of secondary school is well below the national average.[87] Just over 21% of the population is illiterate, above the national average of 12.4%. 45% of those over 15 years of age have not finished primary school. Only a small minority of the population has professional aspirations with 6.7% attaining studies at the bacchelaurate level or above.[65]Higher level education in Oaxaca has traditionally been limited to a few schools, although this is changing.[citation needed] The largest university in the state is the Benito Juárez Autonomous University of Oaxaca (UABJO), located in the capital city of Oaxaca de Juarez.[88] Founded in 1827 as the Oaxacan Institute for Arts and Sciences, today UABJO offers the widest range of curricula in the state. In addition standard undergraduate studies, specialized schools such as the UABJO School of Medicine and UABJO School of Law offer advanced academic degrees (i.e. Juris Doctorate, M.D., PhD) in their respective fields.[89][90] Other universities the Instituto Tecnológico de Oaxaca, which offers several undergraduate and graduate level programs, and the Universidad de la Sierra Juárez, which was opened in 2005 to help provide higher education to underserved rural areas in the Sierra Juarez mountains.[91] The UABJO has expanded its educational offerings, in coordination with the UNAM offers the type of open and distance education.[92]In addition there is the SUNEO university system. Two of the largest institutions of this system are the Universidad Tecnológica de la Mixteca (UTM) and the Universidad del Mar (UMAR). The first offers bachelor's, master's and postgraduate courses in the areas of computing, electronics, design and business studies, while the second offers undergraduate and master's degrees in the areas of social sciences and marine sciences.[citation needed]Ninety five percent of Oaxaca's population receives health care from one or more government programs.[70] Government health services used include IMSS; Seguridad Social, ISSSTE and that related to PEMEX.(infraes) The state sponsors the Servicios de Salud de Oaxaca (SSO) which primarily works to provide antibiotics and other medicines to public dispensaries. It is meant to supplement other federal and state services such as IMSS.[93] There are 1,020 primary care medical facilities and 28 hospitals in the state, 3,240,024 people are registered in one or more government programs and are attended by 3,337 doctors, 5,400 paramedics and 6,887 other health providers.[70] Hospital Regional de Alta Especialidad de Oaxaca was constructed by the federal government as the first "level three" or high level specialty hospital in the state. It was opened in 2006 and is located in San Bartolo Coyotepec.[94]One particular health problem the state has is outbreaks of dengue fever during the rainy season, which occurs from June to October. Some of these cases are hemorrhagic. The problem is more severe in the tropical lowlands of the state, near the ocean.[95]Despite the health services that exist, there are serious problems and deficiencies. As of 1997, life expectancy in the state was 71.5 years, 9 years higher than in 1990. The death rate has decreased from 5.79 deaths per thousand to 5.14.[70] While much of Mexico's health care system struggles to meet needs, the system in Oaxaca, one of the country's poorest states, has it particularly bad. The relatively prosperous state of Nuevo León has 3,207 hospital beds, while Oaxaca has only 1,760, despite the fact that the two states have about the same population. There is about the same ratio of doctors between the two states.[96] Forty four percent of pregnant women receive pre-natal care from people who are not medically qualified. 70 women each year die from complications from pregnancy and childbirth, and most of these are avoidable, due to bleeding and eclampsia. For every 100,000 live births in Oaxaca, there are 95.1 maternal deaths, over the national average of 63.3, putting the state in the top five.[97]The state lacks sufficient numbers of health care workers and lacks specialized hospital and other facilities. Other problems include obsolete medical equipment, lack of medicines. Many of these problems have persisted for decades.[98] Health care providers offer an average of 20,000 consults each day, covering a population of 800,000 people.[99][100] In 2000, there was only one doctor for every 180 people.[98]In 2006, health care workers held a work stoppage and march, demanding improvements in the health care system along with the ouster of Governor Ulises Ruiz Ortiz. Most of the participating strikers were from the hospital and emergency room sectors, from 15 hospitals and 650 health centers in the state.[99]Football, baseball and basketball are popular in Oaxaca. Football is most popular in Oaxaca city and in Huajuapan de Leon, having a notable international player by the name of Ricardo Osorio. The baseball team, Guerreros de Oaxaca, play at the Eduardo Vasconcelos Stadium in Oaxaca de Juarez and play in the Mexican League.[101] The Oaxacan Academy of Baseball is located in the municipality of San Bartolo Coyotepec. It was created in 2009 by Alfredo Harp Helú, owner of the Diablos Rojos and Guerreros de Oaxaca teams. The goal of the academy is to reach youth people through sports and education, especially those who show talent for the sport of baseball.[102] Vinicio Castilla is the most notable player hailing from Oaxaca, having played third base in Major League Baseball for the Atlanta Braves, Colorado Rockies, Tampa Bay Devil Rays, Houston Astros and San Diego Padres. He became the owner of the Oaxaca Guerreros in 1995 and three years later they won the championship. Basketball is practiced in all of Oaxaca, mostly played during local festivals, especially in the Sierra Norte. The area also has a tournament with the Copa Juárez as the prize.[citation needed]The best known beach in Puerto Escondido is Playa Zicatela, due to its fame as a surfing attraction. The "tubes" produced by the waves that come ashore here attract advanced and professional surfers from all over Mexico and internationally.[103] The Torneo Internacional de Surf (International Surfing Tournament) is held here each year in November and is a world class event. It has attracted names such as Nathaniel Curran from the U.S., Cris Davison from Australia and Marco Polo from Brazil, with its 50,000 USD first prize.[103]Because of its geography and landscape, mountain biking is also common in Oaxaca and is practiced primarily in the Sierra Norte in Ixtlan de Juarez, San Antonio Cuajimoloyas, Santa Catarian Ixtepeji, Benito Juarez Lachatao and San Isidro Llano Grande. Surfing is common in places such as Huatulco Bay and Puerto Escondido, with the annual Zicatela beach tournament held in November.[104] Snorkeling and scuba diving take place in Puerto Escondido, principally in Playa Carrizalillo and Playa Manzanillo, Playa Marinero and Puerto Angelito and at Huatulco. Sport fishing is common in Puerto Escondido and in Huatulco with tournaments held in November and May respectively. Anglers, catch sailfish, dorado, marlin and others. In Huajuapan de Leon there is a fishing tournament at the Yosocuta Dam in July; it is noted for its black bass (lobina).[105] Kayaking also takes places along the Copalita River in Huatulco.[106]Coordinates: 16°54′N 96°25′W﻿ / ﻿16.900°N 96.417°W﻿ / 16.900; -96.417
Botany
Botany, also called plant science(s), plant biology or phytology, is the science of plant life and a branch of biology. A botanist, plant scientist or phytologist is a scientist who specialises in this field. The term "botany" comes from the Ancient Greek word βοτάνη (botanē) meaning "pasture", "grass", or "fodder"; βοτάνη is in turn derived from βόσκειν (boskein), "to feed" or "to graze".[1][2][3]  Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress. Nowadays, botanists (in the strict sense) study approximately 410,000 species of land plants of which some 391,000 species are vascular plants (including ca 369,000 species of flowering plants),[4] and ca 20,000 are bryophytes.[5]Botany originated in prehistory as herbalism with the efforts of early humans to identify – and later cultivate – edible, medicinal and poisonous plants, making it one of the oldest branches of science. Medieval physic gardens, often attached to monasteries, contained plants of medical importance. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of Carl Linnaeus that remains in use to this day.In the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.Modern botany is a broad, multidisciplinary subject with inputs from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st century plant science are molecular genetics and epigenetics, which are the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods, materials such as timber, oil, rubber, fibre and drugs, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.Botany originated as herbalism, the study and use of plants for their medicinal properties.[6] Many records of the Holocene period date early botanical knowledge as far back as 10,000 years ago.[7] This early unrecorded knowledge of plants was discovered in ancient sites of human occupation within Tennessee, which make up much of the Cherokee land today.[7] The early recorded history of botany includes many ancient writings and plant classifications. Examples of early botanical works have been found in ancient texts from India dating back to before 1100 BC,[8][9] in archaic Avestan writings, and in works from China before it was unified in 221 BC.[8][10]Modern botany traces its roots back to Ancient Greece specifically to Theophrastus (c. 371–287 BC), a student of Aristotle who invented and described many of its principles and is widely regarded in the scientific community as the "Father of Botany".[11] His major works, Enquiry into Plants and On the Causes of Plants, constitute the most important contributions to botanical science until the Middle Ages, almost seventeen centuries later.[11][12]Another work from Ancient Greece that made an early impact on botany is De Materia Medica, a five-volume encyclopedia about herbal medicine written in the middle of the first century by Greek physician and pharmacologist Pedanius Dioscorides. De Materia Medica was widely read for more than 1,500 years.[13] Important contributions from the medieval Muslim world include Ibn Wahshiyya's Nabatean Agriculture, Abū Ḥanīfa Dīnawarī's (828–896) the Book of Plants, and Ibn Bassal's The Classification of Soils. In the early 13th century, Abu al-Abbas al-Nabati, and Ibn al-Baitar (d. 1248) wrote on botany in a systematic and scientific manner.[14][15][16]In the mid-16th century, "botanical gardens" were founded in a number of Italian universities – the Padua botanical garden in 1545 is usually considered to be the first which is still in its original location. These gardens continued the practical value of earlier "physic gardens", often associated with monasteries, in which plants were cultivated for medical use. They supported the growth of botany as an academic subject. Lectures were given about the plants grown in the gardens and their medical uses demonstrated. Botanical gardens came much later to northern Europe; the first in England was the University of Oxford Botanic Garden in 1621. Throughout this period, botany remained firmly subordinate to medicine.[17]German physician Leonhart Fuchs (1501–1566) was one of "the three German fathers of botany", along with theologian Otto Brunfels (1489–1534) and physician Hieronymus Bock (1498–1554) (also called Hieronymus Tragus).[18][19] Fuchs and Brunfels broke away from the tradition of copying earlier works to make original observations of their own. Bock created his own system of plant classification.Physician Valerius Cordus (1515–1544) authored a botanically and pharmacologically important herbal Historia Plantarum in 1544 and a pharmacopoeia of lasting importance, the Dispensatorium in 1546.[20] Naturalist Conrad von Gesner (1516–1565) and herbalist John Gerard (1545–c. 1611) published herbals covering the medicinal uses of plants. Naturalist Ulisse Aldrovandi (1522–1605) was considered the father of natural history, which included the study of plants. In 1665, using an early microscope, Polymath Robert Hooke discovered cells, a term he coined, in cork, and a short time later in living plant tissue.[21]During the 18th century, systems of plant identification were developed comparable to dichotomous keys, where unidentified plants are placed into taxonomic groups (e.g. family, genus and species) by making a series of choices between pairs of characters. The choice and sequence of the characters may be artificial in keys designed purely for identification (diagnostic keys) or more closely related to the natural or phyletic order of the taxa in synoptic keys.[22] By the 18th century, new plants for study were arriving in Europe in increasing numbers from newly discovered countries and the European colonies worldwide. In 1753, Carl von Linné (Carl Linnaeus) published his Species Plantarum, a hierarchical classification of plant species that remains the reference point for modern botanical nomenclature. This established a standardised binomial or two-part naming scheme where the first name represented the genus and the second identified the species within the genus.[23] For the purposes of identification, Linnaeus's Systema Sexuale classified plants into 24 groups according to the number of their male sexual organs. The 24th group, Cryptogamia, included all plants with concealed reproductive parts, mosses, liverworts, ferns, algae and fungi.[24]Increasing knowledge of plant anatomy, morphology and life cycles led to the realisation that there were more natural affinities between plants than the artificial sexual system of Linnaeus. Adanson (1763), de Jussieu (1789), and Candolle (1819) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed. The Candollean system reflected his ideas of the progression of morphological complexity and the later classification by Bentham and Hooker, which was influential until the mid-19th century, was influenced by Candolle's approach. Darwin's publication of the Origin of Species in 1859 and his concept of common descent required modifications to the Candollean system to reflect evolutionary relationships as distinct from mere morphological similarity.[25]Botany was greatly stimulated by the appearance of the first "modern" textbook, Matthias Schleiden's Grundzüge der Wissenschaftlichen Botanik, published in English in 1849 as Principles of Scientific Botany.[26]  Schleiden was a microscopist and an early plant anatomist who co-founded the cell theory with Theodor Schwann and Rudolf Virchow and was among the first to grasp the significance of the cell nucleus that had been described by Robert Brown in 1831.[27]In 1855, Adolf Fick formulated Fick's laws that enabled the calculation of the rates of molecular diffusion in biological systems.[28]Building upon the gene-chromosome theory of heredity that originated with Gregor Mendel (1822–1884), August Weismann (1834–1914) proved that inheritance only takes place through gametes. No other cells can pass on inherited characters.[29] The work of Katherine Esau (1898–1997) on plant anatomy is still a major foundation of modern botany. Her books Plant Anatomy and Anatomy of Seed Plants have been key plant structural biology texts for more than half a century.[30][31]The discipline of plant ecology was pioneered in the late 19th century by botanists such as Eugenius Warming, who produced the hypothesis that plants form communities, and his mentor and successor Christen C. Raunkiær whose system for describing plant life forms is still in use today. The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles, Arthur Tansley and Frederic Clements. Clements is credited with the idea of climax vegetation as the most complex vegetation that an environment can support and Tansley introduced the concept of ecosystems to biology.[32][33][34] Building on the extensive earlier work of Alphonse de Candolle, Nikolai Vavilov (1887–1943) produced accounts of the biogeography, centres of origin, and evolutionary history of economic plants.[35]Particularly since the mid-1960s there have been advances in understanding of the physics of plant physiological processes such as transpiration (the transport of water within plant tissues),  the temperature dependence of rates of water evaporation from the leaf surface and the molecular diffusion of water vapour and carbon dioxide through stomatal apertures. These developments, coupled with new methods for measuring the size of stomatal apertures, and the rate of photosynthesis have enabled precise description of the rates of gas exchange between plants and the atmosphere.[36][37] Innovations in statistical analysis by Ronald Fisher,[38] Frank Yates and others at Rothamsted Experimental Station facilitated rational experimental design and data analysis in botanical research.[39] The discovery and identification of the auxin plant hormones by Kenneth V. Thimann in 1948 enabled regulation of plant growth by externally applied chemicals. Frederick Campion Steward pioneered techniques of micropropagation and plant tissue culture controlled by plant hormones.[40] The synthetic auxin 2,4-Dichlorophenoxyacetic acid or 2,4-D was one of the first commercial synthetic herbicides.[41]20th century developments in plant biochemistry have been driven by modern techniques of organic chemical analysis, such as spectroscopy, chromatography and electrophoresis. With the rise of the related molecular-scale biological approaches of molecular biology, genomics, proteomics and metabolomics, the relationship between the plant genome and most aspects of the biochemistry, physiology, morphology and behaviour of plants can be subjected to detailed experimental analysis.[42] The concept originally stated by Gottlieb Haberlandt in 1902[43] that all plant cells are totipotent and can be grown in vitro ultimately enabled the use of genetic engineering experimentally to knock out a gene or genes responsible for a specific trait, or to add genes such as GFP that report when a gene of interest is being expressed. These technologies enable the biotechnological use of whole plants or plant cell cultures grown in bioreactors to synthesise pesticides, antibiotics or other pharmaceuticals, as well as the practical application of genetically modified crops designed for traits such as improved yield.[44]Modern morphology recognises a continuum between the major morphological categories of root, stem (caulome), leaf (phyllome) and trichome.[45] Furthermore, it emphasises structural dynamics.[46] Modern systematics aims to reflect and discover phylogenetic relationships between plants.[47][48][49][50] Modern Molecular phylogenetics largely ignores morphological characters, relying on DNA sequences as data. Molecular analysis of DNA sequences from most families of flowering plants enabled the Angiosperm Phylogeny Group to publish in 1998 a phylogeny of flowering plants, answering many of the questions about relationships among angiosperm families and species.[51] The theoretical possibility of a practical method for identification of plant species and commercial varieties by DNA barcoding is the subject of active current research.[52][53]The study of plants is vital because they underpin almost all animal life on Earth by generating a large proportion of the oxygen and food that provide humans and other organisms with aerobic respiration with the chemical energy they need to exist. Plants, algae and cyanobacteria are the major groups of organisms that carry out photosynthesis, a process that uses the energy of sunlight to convert water and carbon dioxide[54] into sugars that can be used both as a source of chemical energy and of organic molecules that are used in the structural components of cells.[55] As a by-product of photosynthesis, plants release oxygen into the atmosphere, a gas that is required by nearly all living things to carry out cellular respiration. In addition, they are influential in the global carbon and water cycles and plant roots bind and stabilise soils, preventing soil erosion.[56] Plants are crucial to the future of human society as they provide food, oxygen, medicine, and products for people, as well as creating and preserving soil.[57]Historically, all living things were classified as either animals or plants[58] and botany covered the study of all organisms not considered animals.[59] Botanists examine both the internal functions and processes within plant organelles, cells, tissues, whole plants, plant populations and plant communities. At each of these levels, a botanist may be concerned with the classification (taxonomy), phylogeny and evolution, structure (anatomy and morphology), or function (physiology) of plant life.[60]The strictest definition of "plant" includes only the "land plants" or embryophytes, which include seed plants (gymnosperms, including the pines, and flowering plants) and the free-sporing cryptogams including ferns, clubmosses, liverworts, hornworts and mosses. Embryophytes are multicellular eukaryotes descended from an ancestor that obtained its energy from sunlight by photosynthesis. They have life cycles with alternating haploid and diploid phases. The sexual haploid phase of embryophytes, known as the gametophyte, nurtures the developing diploid embryo sporophyte within its tissues for at least part of its life,[61] even in the seed plants, where the gametophyte itself is nurtured by its parent sporophyte.[62] Other groups of organisms that were previously studied by botanists include bacteria (now studied in bacteriology), fungi (mycology) – including lichen-forming fungi (lichenology), non-chlorophyte algae (phycology), and viruses (virology). However, attention is still given to these groups by botanists, and fungi (including lichens) and photosynthetic protists are usually covered in introductory botany courses.[63][64]Palaeobotanists study ancient plants in the fossil record to provide information about the evolutionary history of plants. Cyanobacteria, the first oxygen-releasing photosynthetic organisms on Earth, are thought to have given rise to the ancestor of plants by entering into an endosymbiotic relationship with an early eukaryote, ultimately becoming the chloroplasts in plant cells. The new photosynthetic plants (along with their algal relatives) accelerated the rise in atmospheric oxygen started by the cyanobacteria, changing the ancient oxygen-free, reducing, atmosphere to one in which free oxygen has been abundant for more than 2 billion years.[65][66]Among the important botanical questions of the 21st century are the role of plants as primary producers in the global cycling of life's basic ingredients: energy, carbon, oxygen, nitrogen and water, and ways that our plant stewardship can help address the global environmental issues of resource management, conservation, human food security, biologically invasive organisms, carbon sequestration, climate change, and sustainability.[67]Virtually all staple foods come either directly from primary production by plants, or indirectly from animals that eat them.[68]  Plants and other photosynthetic organisms are at the base of most food chains because they use the energy from the sun and nutrients from the soil and atmosphere, converting them into a form that can be used by animals. This is what ecologists call the first trophic level.[69]  The modern forms of the major staple foods, such as hemp, teff, maize, rice, wheat and other cereal grasses, pulses, bananas and plantains,[70] as well as hemp, flax and cotton grown for their fibres, are the outcome of prehistoric selection over thousands of years from among wild ancestral plants with the most desirable characteristics.[71]Botanists study how plants produce food and how to increase yields, for example through plant breeding, making their work important to humanity's ability to feed the world and provide food security for future generations.[72] Botanists also study weeds, which are a considerable problem in agriculture, and the biology and control of plant pathogens in agriculture and natural ecosystems.[73] Ethnobotany is the study of the relationships between plants and people. When applied to the investigation of historical plant–people relationships ethnobotany may be referred to as archaeobotany or palaeoethnobotany.[74] Some of the earliest plant-people relationships arose between the indigenous people of Canada in identifying edible plants from inedible plants.[75] This relationship the indigenous people had with plants was recorded by ethnobotanists.[75]Plant biochemistry is the study of the chemical processes used by plants. Some of these processes are used in their primary metabolism like the photosynthetic Calvin cycle and crassulacean acid metabolism.[76] Others make specialised materials like the cellulose and lignin used to build their bodies, and secondary products like resins and aroma compounds.Plants and various other groups of photosynthetic eukaryotes collectively known as "algae" have unique organelles known as chloroplasts. Chloroplasts are thought to be descended from cyanobacteria that formed endosymbiotic relationships with ancient plant and algal ancestors. Chloroplasts and cyanobacteria contain the blue-green pigment chlorophyll a.[77] Chlorophyll a (as well as its plant and green algal-specific cousin chlorophyll b)[a] absorbs light in the blue-violet and orange/red parts of the spectrum while reflecting and transmitting the green light that we see as the characteristic colour of these organisms. The energy in the red and blue light that these pigments absorb is used by chloroplasts to make energy-rich carbon compounds from carbon dioxide and water by oxygenic photosynthesis, a process that generates molecular oxygen (O2) as a by-product.The light energy captured by chlorophyll a is initially in the form of electrons (and later a proton gradient) that's used to make molecules of ATP and NADPH which temporarily store and transport energy. Their energy is used in the light-independent reactions of the Calvin cycle by the enzyme rubisco to produce molecules of the 3-carbon sugar glyceraldehyde 3-phosphate (G3P). Glyceraldehyde 3-phosphate is the first product of photosynthesis and the raw material from which glucose and almost all other organic molecules of biological origin are synthesised. Some of the glucose is converted to starch which is stored in the chloroplast.[81] Starch is the characteristic energy store of most land plants and algae, while inulin, a polymer of fructose is used for the same purpose in the sunflower family Asteraceae. Some of the glucose is converted to sucrose (common table sugar) for export to the rest of the plant.Unlike in animals (which lack chloroplasts), plants and their eukaryote relatives have delegated many biochemical roles to their chloroplasts, including synthesising all their fatty acids,[82][83] and most amino acids.[84] The fatty acids that chloroplasts make are used for many things, such as providing material to build cell membranes out of and making the polymer cutin which is found in the plant cuticle that protects land plants from drying out. [85]Plants synthesise a number of unique polymers like the polysaccharide molecules cellulose, pectin and xyloglucan[86] from which the land plant cell wall is constructed.[87]Vascular land plants make lignin, a polymer used to strengthen the secondary cell walls of xylem tracheids and vessels to keep them from collapsing when a plant sucks water through them under water stress. Lignin is also used in other cell types like sclerenchyma fibres that provide structural support for a plant and is a major constituent of wood. Sporopollenin is a chemically resistant polymer found in the outer cell walls of spores and pollen of land plants responsible for the survival of early land plant spores and the pollen of seed plants in the fossil record. It is widely regarded as a marker for the start of land plant evolution during the Ordovician period.[88]The concentration of carbon dioxide in the atmosphere today is much lower than it was when plants emerged onto land during the Ordovician and Silurian periods. Many monocots like maize and the pineapple and some dicots like the Asteraceae have since independently evolved[89] pathways like Crassulacean acid metabolism and the C4 carbon fixation pathway for photosynthesis which avoid the losses resulting from photorespiration in the more common C3 carbon fixation pathway. These biochemical strategies are unique to land plants.Phytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism.[90] Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees,[91] and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy.[92] Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine).[93] Native Americans have used various plants as ways of treating illness or disease for thousands of years.[94] This knowledge Native Americans have on plants has been recorded by enthnobotanists and then in turn has been used by pharmaceutical companies as a way of drug discovery.[95]Plants can synthesise useful coloured dyes and pigments such as the anthocyanins responsible for the red colour of red wine, yellow weld and blue woad used together to produce Lincoln green, indoxyl, source of the blue dye indigo traditionally used to dye denim and the artist's pigments gamboge and rose madder.Sugar, starch, cotton, linen, hemp, some types of rope, wood and particle boards, papyrus and paper, vegetable oils, wax, and natural rubber are examples of commercially important materials made from plant tissues or their secondary products. Charcoal, a pure form of carbon made by pyrolysis of wood, has a long history as a metal-smelting fuel, as a filter material and adsorbent and as an artist's material and is one of the three ingredients of gunpowder. Cellulose, the world's most abundant organic polymer,[96] can be converted into energy, fuels, materials and chemical feedstock. Products made from cellulose include rayon and cellophane, wallpaper paste, biobutanol and gun cotton. Sugarcane, rapeseed and soy are some of the plants with a highly fermentable sugar or oil content that are used as sources of biofuels, important alternatives to fossil fuels, such as biodiesel.[97]  Sweetgrass was used by Native Americans to ward off bugs like mosquitoes.[98] These bug repelling properties of sweetgrass were later found by the American Chemical Society in the molecules phytol and coumarin.[98]Plant ecology is the science of the functional relationships between plants and their habitats—the environments where they complete their life cycles. Plant ecologists study the composition of local and regional floras, their biodiversity, genetic diversity and fitness, the adaptation of plants to their environment, and their competitive or mutualistic interactions with other species.[99] Some ecologists even rely on empirical data from indigenous people that is gathered by ethnobotanists.[100] This information can relay a great deal of information on how the land once was thousands of years ago and how it has changed over that time.[100] The goals of plant ecology are to understand the causes of their distribution patterns, productivity, environmental impact, evolution, and responses to environmental change.[101]Plants depend on certain edaphic (soil) and climatic factors in their environment but can modify these factors too. For example, they can change their environment's albedo, increase runoff interception, stabilise mineral soils and develop their organic content, and affect local temperature. Plants compete with other organisms in their ecosystem for resources.[102][103] They interact with their neighbours at a variety of spatial scales in groups, populations and communities that collectively constitute vegetation. Regions with characteristic vegetation types and dominant plants as well as similar abiotic and biotic factors, climate, and geography make up biomes like tundra or tropical rainforest.[104]Herbivores eat plants, but plants can defend themselves and some species are parasitic or even carnivorous. Other organisms form mutually beneficial relationships with plants. For example, mycorrhizal fungi and rhizobia provide plants with nutrients in exchange for food, ants are recruited by ant plants to provide protection,[106] honey bees, bats and other animals pollinate flowers[107][108] and humans and other animals[109] act as dispersal vectors to spread spores and seeds.Plant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates.[110] Estimates of atmospheric CO2 concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants.[111] Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates.[112] Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.[113]Inheritance in plants follows the same fundamental principles of genetics as in other multicellular organisms. Gregor Mendel discovered the genetic laws of inheritance by studying inherited traits such as shape in Pisum sativum (peas). What Mendel learned from studying plants has had far reaching benefits outside of botany. Similarly, "jumping genes" were discovered by Barbara McClintock while she was studying maize.[114] Nevertheless, there are some distinctive genetic differences between plants and other organisms.Species boundaries in plants may be weaker than in animals, and cross species hybrids are often possible. A familiar example is peppermint, Mentha × piperita, a sterile hybrid between Mentha aquatica and spearmint, Mentha spicata.[115] The many cultivated varieties of wheat are the result of multiple inter- and intra-specific crosses between wild species and their hybrids.[116] Angiosperms with monoecious flowers often have self-incompatibility mechanisms that operate between the pollen and stigma so that the pollen either fails to reach the stigma or fails to germinate and produce male gametes.[117] This is one of several methods used by plants to promote outcrossing.[118] In many land plants the male and female gametes are produced by separate individuals. These species are said to be dioecious when referring to vascular plant sporophytes and dioicous when referring to bryophyte gametophytes.[119]Unlike in higher animals, where parthenogenesis is rare, asexual reproduction may occur in plants by several different mechanisms. The formation of stem tubers in potato is one example. Particularly in arctic or alpine habitats, where opportunities for fertilisation of flowers by animals are rare, plantlets or bulbs, may develop instead of flowers, replacing sexual reproduction with asexual reproduction and giving rise to clonal populations genetically identical to the parent. This is one of several types of apomixis that occur in plants. Apomixis can also happen in a seed, producing a seed that contains an embryo genetically identical to the parent.[120]Most sexually reproducing organisms are diploid, with paired chromosomes, but doubling of their chromosome number may occur due to errors in cytokinesis. This can occur early in development to produce an autopolyploid or partly autopolyploid organism, or during normal processes of cellular differentiation to produce some cell types that are polyploid (endopolyploidy), or during gamete formation. An allopolyploid plant may result from a hybridisation event between two different species. Both autopolyploid and allopolyploid plants can often reproduce normally, but may be unable to cross-breed successfully with the parent population because there is a mismatch in chromosome numbers. These plants that are reproductively isolated from the parent species but live within the same geographical area, may be sufficiently successful to form a new species.[121] Some otherwise sterile plant polyploids can still reproduce vegetatively or by seed apomixis, forming clonal populations of identical individuals.[121] Durum wheat is a fertile tetraploid allopolyploid, while bread wheat is a fertile hexaploid. The commercial banana is an example of a sterile, seedless triploid hybrid. Common dandelion is a triploid that produces viable seeds by apomictic seed.As in other eukaryotes, the inheritance of endosymbiotic organelles like mitochondria and chloroplasts in plants is non-Mendelian. Chloroplasts are inherited through the male parent in gymnosperms but often through the female parent in flowering plants.[122]A considerable amount of new knowledge about plant function comes from studies of the molecular genetics of model plants such as the Thale cress, Arabidopsis thaliana, a weedy species in the mustard family (Brassicaceae).[90] The genome or hereditary information contained in the genes of this species is encoded by about 135 million base pairs of DNA, forming one of the smallest genomes among flowering plants. Arabidopsis was the first plant to have its genome sequenced, in 2000.[123] The sequencing of some other relatively small genomes, of rice (Oryza sativa)[124] and Brachypodium distachyon,[125] has made them important model species for understanding the genetics, cellular and molecular biology of cereals, grasses and monocots generally.Model plants such as Arabidopsis thaliana are used for studying the molecular biology of plant cells and the chloroplast. Ideally, these organisms have small genomes that are well known or completely sequenced, small stature and short generation times. Corn has been used to study mechanisms of photosynthesis and phloem loading of sugar in C4 plants.[126] The single celled green alga Chlamydomonas reinhardtii, while not an embryophyte itself, contains a green-pigmented chloroplast related to that of land plants, making it useful for study.[127] A red alga Cyanidioschyzon merolae has also been used to study some basic chloroplast functions.[128] Spinach,[129] peas,[130] soybeans and a moss Physcomitrella patens are commonly used to study plant cell biology.[131]Agrobacterium tumefaciens, a soil rhizosphere bacterium, can attach to plant cells and infect them with a callus-inducing Ti plasmid by horizontal gene transfer, causing a callus infection called crown gall disease. Schell and Van Montagu (1977) hypothesised that the Ti plasmid could be a natural vector for introducing the Nif gene responsible for nitrogen fixation in the root nodules of legumes and other plant species.[132] Today, genetic modification of the Ti plasmid is one of the main techniques for introduction of transgenes to plants and the creation of genetically modified crops.Epigenetics is the study of heritable changes in gene function that cannot be explained by changes in the underlying DNA sequence[133] but cause the organism's genes to behave (or "express themselves") differently.[134] One example of epigenetic change is the marking of the genes by DNA methylation which determines whether they will be expressed or not. Gene expression can also be controlled by repressor proteins that attach to silencer regions of the DNA and prevent that region of the DNA code from being expressed. Epigenetic marks may be added or removed from the DNA during programmed stages of development of the plant, and are responsible, for example, for the differences between anthers, petals and normal leaves, despite the fact that they all have the same underlying genetic code. Epigenetic changes may be temporary or may remain through successive cell divisions for the remainder of the cell's life. Some epigenetic changes have been shown to be heritable,[135] while others are reset in the germ cells.Epigenetic changes in eukaryotic biology serve to regulate the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. A single fertilised egg cell, the zygote, gives rise to the many different plant cell types including parenchyma, xylem vessel elements, phloem sieve tubes, guard cells of the epidermis, etc. as it continues to divide. The process results from the epigenetic activation of some genes and inhibition of others.[136]Unlike animals, many plant cells, particularly those of the parenchyma, do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. Exceptions include highly lignified cells, the sclerenchyma and xylem which are dead at maturity, and the phloem sieve tubes which lack nuclei. While plants use many of the same epigenetic mechanisms as animals, such as chromatin remodelling, an alternative hypothesis is that plants set their gene expression patterns using positional information from the environment and surrounding cells to determine their developmental fate.[137]The chloroplasts of plants have a number of biochemical, structural and genetic similarities to cyanobacteria, (commonly but incorrectly known as "blue-green algae") and are thought to be derived from an ancient endosymbiotic relationship between an ancestral eukaryotic cell and a cyanobacterial resident.[138][139][140][141]The algae are a polyphyletic group and are placed in various divisions, some more closely related to plants than others. There are many differences between them in features such as cell wall composition, biochemistry, pigmentation, chloroplast structure and nutrient reserves. The algal division Charophyta, sister to the green algal division Chlorophyta, is considered to contain the ancestor of true plants.[142] The Charophyte class Charophyceae and the land plant sub-kingdom Embryophyta together form the monophyletic group or clade Streptophytina.[143]Nonvascular land plants are embryophytes that lack the vascular tissues xylem and phloem. They include mosses, liverworts and hornworts. Pteridophytic vascular plants with true xylem and phloem that reproduced by spores germinating into free-living gametophytes evolved during the Silurian period and diversified into several lineages during the late Silurian and early Devonian. Representatives of the lycopods have survived to the present day. By the end of the Devonian period, several groups, including the lycopods, sphenophylls and progymnosperms, had independently evolved "megaspory" – their spores were of two distinct sizes, larger megaspores and smaller microspores. Their reduced gametophytes developed from megaspores retained within the spore-producing organs (megasporangia) of the sporophyte, a condition known as endospory. Seeds consist of an endosporic megasporangium surrounded by one or two sheathing layers (integuments). The young sporophyte develops within the seed, which on germination splits to release it. The earliest known seed plants date from the latest Devonian Famennian stage.[144][145] Following the evolution of the seed habit, seed plants diversified, giving rise to a number of now-extinct groups, including seed ferns, as well as the modern gymnosperms and angiosperms.[146] Gymnosperms produce "naked seeds" not fully enclosed in an ovary; modern representatives include conifers, cycads, Ginkgo, and Gnetales. Angiosperms produce seeds enclosed in a structure such as a carpel or an ovary.[147][148] Ongoing research on the molecular phylogenetics of living plants appears to show that the angiosperms are a sister clade to the gymnosperms.[149]Plant physiology encompasses all the internal chemical and physical activities of plants associated with life.[150] Chemicals obtained from the air, soil and water form the basis of all plant metabolism. The energy of sunlight, captured by oxygenic photosynthesis and released by cellular respiration, is the basis of almost all life. Photoautotrophs, including all green plants, algae and cyanobacteria gather energy directly from sunlight by photosynthesis. Heterotrophs including all animals, all fungi, all completely parasitic plants, and non-photosynthetic bacteria take in organic molecules produced by photoautotrophs and respire them or use them in the construction of cells and tissues.[151] Respiration is the oxidation of carbon compounds by breaking them down into simpler structures to release the energy they contain, essentially the opposite of photosynthesis.[152]Molecules are moved within plants by transport processes that operate at a variety of spatial scales. Subcellular transport of ions, electrons and molecules such as water and enzymes occurs across cell membranes. Minerals and water are transported from roots to other parts of the plant in the transpiration stream. Diffusion, osmosis, and active transport and mass flow are all different ways transport can occur.[153] Examples of elements that plants need to transport are nitrogen, phosphorus, potassium, calcium, magnesium, and sulfur. In vascular plants, these elements are extracted from the soil as soluble ions by the roots and transported throughout the plant in the xylem. Most of the elements required for plant nutrition come from the chemical breakdown of soil minerals.[154]  Sucrose produced by photosynthesis is transported from the leaves to other parts of the plant in the phloem and plant hormones are transported by a variety of processes.Plants are not passive, but respond to external signals such as light, touch, and injury by moving or growing towards or away from the stimulus, as appropriate. Tangible evidence of touch sensitivity is the almost instantaneous collapse of leaflets of Mimosa pudica, the insect traps of Venus flytrap and bladderworts, and the pollinia of orchids.[156]The hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century. Darwin experimented on the movements of plant shoots and roots towards light[157] and gravity, and concluded "It is hardly an exaggeration to say that the tip of the radicle . . acts like the brain of one of the lower animals . . directing the several movements".[158] About the same time, the role of auxins (from the Greek auxein, to grow) in control of plant growth was first outlined by the Dutch scientist Frits Went.[159] The first known auxin, indole-3-acetic acid (IAA), which promotes cell growth, was only isolated from plants about 50 years later.[160] This compound mediates the tropic responses of shoots and roots towards light and gravity.[161] The finding in 1939 that plant callus could be maintained in culture containing IAA, followed by the observation in 1947 that it could be induced to form roots and shoots by controlling the concentration of growth hormones were key steps in the development of plant biotechnology and genetic modification.[162]Cytokinins are a class of plant hormones named for their control of cell division or cytokinesis. The natural cytokinin zeatin was discovered in corn, Zea mays, and is a derivative of the purine adenine. Zeatin is produced in roots and transported to shoots in the xylem where it promotes cell division, bud development, and the greening of chloroplasts.[163][164] The gibberelins, such as Gibberelic acid are diterpenes synthesised from acetyl CoA via the mevalonate pathway. They are involved in the promotion of germination and dormancy-breaking in seeds, in regulation of plant height by controlling stem elongation and the control of flowering.[165] Abscisic acid (ABA) occurs in all land plants except liverworts, and is synthesised from carotenoids in the chloroplasts and other plastids. It inhibits cell division, promotes seed maturation, and dormancy, and promotes stomatal closure. It was so named because it was originally thought to control abscission.[166] Ethylene is a gaseous hormone that is produced in all higher plant tissues from methionine. It is now known to be the hormone that stimulates or regulates fruit ripening and abscission,[167][168] and it, or the synthetic growth regulator ethephon which is rapidly metabolised to produce ethylene, are used on industrial scale to promote ripening of cotton, pineapples and other climacteric crops.Another class of phytohormones is the jasmonates, first isolated from the oil of Jasminum grandiflorum[169] which regulates wound responses in plants by unblocking the expression of genes required in the systemic acquired resistance response to pathogen attack.[170]In addition to being the primary energy source for plants, light functions as a signalling device, providing information to the plant, such as how much sunlight the plant receives each day. This can result in adaptive changes in a process known as photomorphogenesis. Phytochromes are the photoreceptors in a plant that are sensitive to light.[171]Plant anatomy is the study of the structure of plant cells and tissues, whereas plant morphology is the study of their external form.[172]All plants are multicellular eukaryotes, their DNA stored in nuclei.[173][174] The characteristic features of plant cells that distinguish them from those of animals and fungi include a primary cell wall composed of the polysaccharides cellulose, hemicellulose and pectin, [175] larger vacuoles than in animal cells and the presence of plastids with unique photosynthetic and biosynthetic functions as in the chloroplasts. Other plastids contain storage products such as starch (amyloplasts) or lipids (elaioplasts). Uniquely, streptophyte cells and those of the green algal order Trentepohliales[176] divide by construction of a phragmoplast as a template for building a cell plate late in cell division.[81]The bodies of vascular plants including clubmosses, ferns and  seed plants (gymnosperms and angiosperms) generally have aerial and subterranean subsystems. The shoots consist of stems bearing green photosynthesising leaves and reproductive structures. The underground vascularised roots bear root hairs at their tips and generally lack chlorophyll.[178] Non-vascular plants, the liverworts, hornworts and mosses do not produce ground-penetrating vascular roots and most of the plant participates in photosynthesis.[179] The sporophyte generation is nonphotosynthetic in liverworts but may be able to contribute part of its energy needs by photosynthesis in mosses and hornworts.[180]The root system and the shoot system are interdependent – the usually nonphotosynthetic root system depends on the shoot system for food, and the usually photosynthetic shoot system depends on water and minerals from the root system.[178] Cells in each system are capable of creating cells of the other and producing adventitious shoots or roots.[181] Stolons and tubers are examples of shoots that can grow roots.[182] Roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants.[183] In the event that one of the systems is lost, the other can often regrow it. In fact it is possible to grow an entire plant from a single leaf, as is the case with Saintpaulia,[184] or even a single cell – which can dedifferentiate into a callus (a mass of unspecialised cells) that can grow into a new plant.[181]In vascular plants, the xylem and phloem are the conductive tissues that transport resources between shoots and roots. Roots are often adapted to store food such as sugars or starch,[178] as in sugar beets and carrots.[183]Stems mainly provide support to the leaves and reproductive structures, but can store water in succulent plants such as cacti, food as in potato tubers, or reproduce vegetatively as in the stolons of strawberry plants or in the process of layering.[185] Leaves gather sunlight and carry out photosynthesis.[186] Large, flat, flexible, green leaves are called foliage leaves.[187] Gymnosperms, such as conifers, cycads, Ginkgo, and gnetophytes are seed-producing plants with open seeds.[188] Angiosperms are seed-producing plants that produce flowers and have enclosed seeds.[147] Woody plants, such as azaleas and oaks, undergo a secondary growth phase resulting in two additional types of tissues: wood (secondary xylem) and bark (secondary phloem and cork). All gymnosperms and many angiosperms are woody plants.[189] Some plants reproduce sexually, some asexually, and some via both means.[190]Although reference to major morphological categories such as root, stem, leaf, and trichome are useful, one has to keep in mind that these categories are linked through intermediate forms so that a continuum between the categories results.[191] Furthermore, structures can be seen as processes, that is, process combinations.[46]Systematic botany is part of systematic biology, which is concerned with the range and diversity of organisms and their relationships, particularly as determined by their evolutionary history.[192] It involves, or is related to, biological classification, scientific taxonomy and phylogenetics. Biological classification is the method by which botanists group organisms into categories such as genera or species. Biological classification is a form of scientific taxonomy. Modern taxonomy is rooted in the work of Carl Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to align better with the Darwinian principle of common descent – grouping organisms by ancestry rather than superficial characteristics. While scientists do not always agree on how to classify organisms, molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions along evolutionary lines and is likely to continue to do so. The dominant classification system is called Linnaean taxonomy. It includes ranks and binomial nomenclature. The nomenclature of botanical organisms is codified in the International Code of Nomenclature for algae, fungi, and plants (ICN) and administered by the International Botanical Congress.[193][194]Kingdom Plantae belongs to Domain Eukarya and is broken down recursively until each species is separately classified. The order is: Kingdom; Phylum (or Division); Class; Order; Family; Genus (plural genera); Species. The scientific name of a plant represents its genus and its species within the genus, resulting in a single worldwide name for each organism.[194] For example, the tiger lily is Lilium columbianum. Lilium is the genus, and columbianum the specific epithet. The combination is the name of the species. When writing the scientific name of an organism, it is proper to capitalise the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term is ordinarily italicised (or underlined when italics are not available).[195][196][197]The evolutionary relationships and heredity of a group of organisms is called its phylogeny. Phylogenetic studies attempt to discover phylogenies. The basic approach is to use similarities based on shared inheritance to determine relationships.[198] As an example, species of Pereskia are trees or bushes with prominent leaves. They do not obviously resemble a typical leafless cactus such as an Echinocactus. However, both Pereskia and Echinocactus have spines produced from areoles (highly specialised pad-like structures) suggesting that the two genera are indeed related.[199][200]Judging relationships based on shared characters requires care, since plants may resemble one another through convergent evolution in which characters have arisen independently. Some euphorbias have leafless, rounded bodies adapted to water conservation similar to those of globular cacti, but characters such as the structure of their flowers make it clear that the two groups are not closely related. The cladistic method takes a systematic approach to characters, distinguishing between those that carry no information about shared evolutionary history – such as those evolved separately in different groups (homoplasies) or those left over from ancestors (plesiomorphies) – and derived characters, which have been passed down from innovations in a shared ancestor (apomorphies). Only derived characters, such as the spine-producing areoles of cacti, provide evidence for descent from a common ancestor. The results of cladistic analyses are expressed as cladograms: tree-like diagrams showing the pattern of evolutionary branching and descent.[201]From the 1990s onwards, the predominant approach to constructing phylogenies for living plants has been molecular phylogenetics, which uses molecular characters, particularly DNA sequences, rather than morphological characters like the presence or absence of spines and areoles. The difference is that the genetic code itself is used to decide evolutionary relationships, instead of being used indirectly via the characters it gives rise to. Clive Stace describes this as having "direct access to the genetic basis of evolution."[202] As a simple example, prior to the use of genetic evidence, fungi were thought either to be plants or to be more closely related to plants than animals. Genetic evidence suggests that the true evolutionary relationship of multicelled organisms is as shown in the cladogram below – fungi are more closely related to animals than to plants.[203]plantsfungianimalsIn 1998, the Angiosperm Phylogeny Group published a phylogeny for flowering plants based on an analysis of DNA sequences from most families of flowering plants. As a result of this work, many questions, such as which families represent the earliest branches of angiosperms, have now been answered.[51] Investigating how plant species are related to each other allows botanists to better understand the process of evolution in plants.[204] Despite the study of model plants and increasing use of DNA evidence, there is ongoing work and discussion among taxonomists about how best to classify plants into various taxa.[205] Technological developments such as computers and electron microscopes have greatly increased the level of detail studied and speed at which data can be analysed.[206]
Magnolia
Magnolia is a large genus of about 210[notes 1] flowering plant species in the subfamily Magnolioideae of the family Magnoliaceae. It is named after French botanist Pierre Magnol.Magnolia is an ancient genus. Appearing before bees did, the flowers are theorized to have evolved to encourage pollination by beetles. To avoid damage from pollinating beetles, the carpels of Magnolia flowers are extremely tough.[1]  Fossilised specimens of M. acuminata have been found dating to 20 million years ago, and of plants identifiably belonging to the Magnoliaceae date to 95 million years ago.[2] Another aspect of Magnolia considered to represent an ancestral state is that the flower bud is enclosed in a bract rather than in sepals; the perianth parts are undifferentiated and called tepals rather than distinct sepals and petals. Magnolia shares the tepal characteristic with several other flowering plants near the base of the flowering plant lineage such as Amborella and Nymphaea (as well as with many more recently derived plants such as Lilium).The natural range of Magnolia species is a disjunct distribution, with a main centre in east and southeast Asia and a secondary centre in eastern North America, Central America, the West Indies, and some species in South America.As with all Magnoliaceae, the perianth is undifferentiated, with  9–15 tepals in 3 or more whorls. The flowers are bisexual with numerous adnate carpels and stamens are arranged in a spiral fashion on the elongated receptacle. The fruit dehisces along the dorsal sutures of the carpels. The pollen is monocolpate, and the embryo development is of the Polygonum type.(Kapil 1964)(Xu and Rudall 2006)The name Magnolia first appeared in 1703 in the Genera[3] of Charles Plumier (1646–1704), for a flowering tree from the island of Martinique (talauma). English botanist William Sherard, who studied botany in Paris under Joseph Pitton de Tournefort, a pupil of Magnol, was most probably the first after Plumier to adopt the genus name Magnolia. He was at least responsible for the taxonomic part of Johann Jacob Dillenius's Hortus Elthamensis[4] and of Mark Catesby's Natural History of Carolina, Florida and the Bahama Islands.[5] These were the first works after Plumier's Genera that used the name Magnolia, this time for some species of flowering trees from temperate North America. The species that Plumier originally named Magnolia was later described as Annona dodecapetala by Lamarck,[6] and has since been named Magnolia plumieri and Talauma plumieri (and still a number of other names) but is now known as Magnolia dodecapetala.[notes 2]Carl Linnaeus, who was familiar with Plumier's Genera, adopted the genus name Magnolia in 1735 in his first edition of Systema Naturae, without a description, but with a reference to Plumier's work. In 1753, he took up Plumier's Magnolia in the first edition of Species Plantarum. There he described a monotypic genus, with the sole species being Magnolia virginiana. Since Linnaeus never saw a herbarium specimen (if there ever was one) of Plumier's Magnolia and had only his description and a rather poor picture at hand, he must have taken it for the same plant which was described by Catesby in his 1730 Natural History of Carolina.  He placed it in the synonymy of Magnolia virginiana var. fœtida, the taxon now known as Magnolia grandiflora. Under Magnolia virginiana Linnaeus described five varieties (glauca, fœtida, grisea, tripetala, and acuminata). In the tenth edition of Systema Naturae (1759), he merged grisea with glauca, and raised the four remaining varieties to species status.[notes 3]By the end of the 18th century, botanists and plant hunters exploring Asia began to name and describe the Magnolia species from China and Japan. The first Asiatic species to be described by western botanists were Magnolia denudata and Magnolia liliiflora,[notes 4] and Magnolia coco and Magnolia figo.[notes 5] Soon after that, in 1794, Carl Peter Thunberg collected and described Magnolia obovata from Japan and at roughly the same time Magnolia kobus was also first collected.[7]With the number of species increasing, the genus was divided into the two subgenera Magnolia and Yulania. Magnolia contains the American evergreen species M. grandiflora, which is of horticultural importance, especially in the southeastern United States, and M. virginiana, the type species. Yulania contains several deciduous Asiatic species, such as M. denudata and M. kobus, which have become horticulturally important in their own right and as parents in hybrids. Classified in Yulania, is also the American deciduous M. acuminata (cucumber tree), which has recently attained greater status as the parent responsible for the yellow flower colour in many new hybrids.Relations in the family Magnoliaceae have been puzzling taxonomists for a long time. Because the family is quite old and has survived many geological events (such as ice ages, mountain formation, and continental drift), its distribution has become scattered. Some species or groups of species have been isolated for a long time, while others could stay in close contact. To create divisions in the family (or even within the genus Magnolia), solely based upon morphological characters, has proven to be a nearly impossible task.[notes 6]By the end of the 20th century, DNA sequencing had become available as a method of large-scale research on phylogenetic relationships. Several studies, including studies on many species in the family Magnoliaceae, were carried out to investigate relationships.[8][9][10] What these studies all revealed was that genus Michelia and Magnolia subgenus Yulania were far more closely allied to each other than either one of them was to Magnolia subgenus Magnolia. These phylogenetic studies were supported by morphological data.[11]As nomenclature is supposed to reflect relationships, the situation with the species names in Michelia and Magnolia subgenus Yulania was undesirable. Taxonomically, three choices are available: 1 to join Michelia and Yulania species in a common genus, not being Magnolia (for which the name Michelia has priority), 2 to raise subgenus Yulania to generic rank, leaving Michelia names and subgenus Magnolia names untouched, or 3 to join Michelia with genus Magnolia into genus Magnolia s.l. (a big genus). Magnolia subgenus Magnolia cannot be renamed because it contains M. virginiana, the type species of the genus and of the family.Not many Michelia species have so far become horticulturally or economically important, apart for their wood. Both subgenus Magnolia and subgenus Yulania include species of major horticultural importance, and a change of name would be very undesirable for many people, especially in the horticultural branch. In Europe, Magnolia even is more or less a synonym for Yulania, since most of the cultivated species on this continent have Magnolia (Yulania) denudata as one of their parents. Most taxonomists who acknowledge close relations between Yulania and Michelia therefore support the third option and join Michelia with Magnolia.The same goes, mutatis mutandis, for the (former) genera Talauma and Dugandiodendron, which are then placed in subgenus Magnolia, and genus Manglietia, which could be joined with subgenus Magnolia or may even earn the status of an extra subgenus. Elmerrillia seems to be closely related to Michelia and Yulania, in which case it will most likely be treated in the same way as Michelia is now. The precise nomenclatural status of small or monospecific genera like Kmeria, Parakmeria, Pachylarnax, Manglietiastrum, Aromadendron, Woonyoungia, Alcimandra, Paramichelia and Tsoongiodendron remains uncertain. Taxonomists who merge Michelia into Magnolia tend to merge these small genera into Magnolia s.l. as well. Botanists do not yet agree on whether to recognize a big Magnolia or the different small genera. For example, Flora of China offers two choices: a large genus Magnolia which includes about 300 species, everything in the Magnoliaceae except Liriodendron (tulip tree), or 16 different genera, some of them recently split out or re-recognized, each of which contains up to 50 species.[12] The western co-author favors the big genus Magnolia, whereas the Chinese recognize the different small genera.Species of Magnolia are most commonly listed under three subgenera, 12 sections, and 13 subsections, such as that used here, following the classification of the Magnolia Society.[13] It does not represent the last word on the subclassification of the genus Magnolia (see above), as a clear consensus has not yet been reached. Each species entry follows this pattern: Botanical name Naming auth. -  (REGION FOUND)The subdivision structure is as follows:Anthers open by splitting at the front facing the centre of the flower, deciduous or evergreen, flowers produced after the leaves.Anthers open by splitting at the sides, deciduous, flowers mostly produced before leaves (except M. acuminata)Charles Plumier (1646–1704) described a flowering tree from the island of Martinique in his Genera,[3] giving it the name Magnolia, after the French botanist Pierre Magnol.In general,  the genus Magnolia has attracted horticultural interest. Some, such as the shrub M. stellata (star magnolia) and the tree M. × soulangeana (saucer magnolia) flower quite early in the spring, before the leaves open. Others flower in late spring or early summer, including M. virginiana (sweetbay magnolia) and M. grandiflora (southern magnolia).Hybridisation has been immensely successful in combining the best aspects of different species to give plants which flower at an earlier age than the parent species, as well as having more impressive flowers. One of the most popular garden magnolias, M. × soulangeana, is a hybrid of M. liliiflora and M. denudata.In the eastern United States, five native species are frequently in cultivation: M. acuminata (as a shade tree), M. grandiflora, M. virginiana, M. tripetala, and M. macrophylla. The last two species must be planted where high winds are not a frequent problem because of the size of their leaves.The flowers of many species are considered edible. In parts of England, the petals of M. grandiflora are pickled and used as a spicy condiment. In some Asian cuisines, the buds are pickled and used to flavor rice and scent tea. In Japan, the young leaves and flower buds of Magnolia hypoleuca are broiled and eaten as a vegetable. Older leaves are made into a powder and used as seasoning; dried, whole leaves are placed on a charcoal brazier and filled with miso, leeks, daikon, and shiitake, and broiled. There is a type of miso which is seasoned with Magnolia, hoba miso.[22][23]In parts of Japan, the leaves of M. obovata are used for wrapping food and as cooking dishes.The bark and flower buds of M. officinalis have long been used in traditional Chinese medicine, where they are known as hou po (厚朴). In Japan, kōboku, M. obovata, has been used in a similar manner.The cucumbertree, M. acuminata, grows to large size and is harvested as a timber tree in northeastern US forests. Its wood is sold as "yellow poplar" along with that of the tuliptree, Liriodendron tulipifera. The Fraser magnolia, M. fraseri, also attains enough size sometimes to be harvested, as well.Magnolias are used as food plants by the larvae of some Lepidoptera species, including the giant leopard moth.The aromatic bark contains magnolol, honokiol, 4-O-methylhonokiol, and obovatol.[24][25][26][27][28][29] Magnolol[30] and honokiol[31] activate the nuclear receptor peroxisome proliferator-activated receptor gamma.The Canadian artist, Sarah Maloney,[34] has created a series of sculptures of Magnolia flowers in bronze and steel, entitled First Flowers,[35] in which she draws our attention to the dual symbols of beginnings in the flower, as both an evolutionary archetype and also one of the first trees to flower in spring (see illustration).List of AGM magnolias
Tree
In botany, a tree is a perennial plant with an elongated stem, or trunk, supporting branches and leaves in most species. In some usages, the definition of a tree may be narrower, including only woody plants with secondary growth, plants that are usable as lumber or plants above a specified height. Trees are not a taxonomic group but include a variety of plant species that have independently evolved a woody trunk and branches as a way to tower above other plants to compete for sunlight. Trees tend to be long-lived, some reaching several thousand years old. In wider definitions, the taller palms, tree ferns, bananas, and bamboos are also trees. Trees have been in existence for 370 million years. It is estimated that there are just over 3 trillion mature trees in the world.[1]A tree typically has many secondary branches supported clear of the ground by the trunk. This trunk typically contains woody tissue for strength, and vascular tissue to carry materials from one part of the tree to another. For most trees it is surrounded by a layer of bark which serves as a protective barrier. Below the ground, the roots branch and spread out widely; they serve to anchor the tree and extract moisture and nutrients from the soil. Above ground, the branches divide into smaller branches and shoots. The shoots typically bear leaves, which capture light energy and convert it into sugars by photosynthesis, providing the food for the tree's growth and development.Trees usually reproduce using seeds. Flowers and fruit may be present, but some trees, such as conifers, instead have pollen cones and seed cones. Palms, bananas, and bamboos also produce seeds, but tree ferns produce spores instead.Trees play a significant role in reducing erosion and moderating the climate. They remove carbon dioxide from the atmosphere and store large quantities of carbon in their tissues. Trees and forests provide a habitat for many species of animals and plants. Tropical rainforests are among the most biodiverse habitats in the world. Trees provide shade and shelter, timber for construction, fuel for cooking and heating, and fruit for food as well as having many other uses. In parts of the world, forests are shrinking as trees are cleared to increase the amount of land available for agriculture. Because of their longevity and usefulness, trees have always been revered, with sacred groves in various cultures, and they play a role in many of the world's mythologies.Although "tree" is a term of common parlance, there is no universally recognised precise definition of what a tree is, either botanically or in common language.[2] In its broadest sense, a tree is any plant with the general form of an elongated stem, or trunk, which supports the photosynthetic leaves or branches at some distance above the ground.[3] Trees are also typically defined by height,[4] with smaller plants from 0.5 to 10 m (1.6 to 32.8 ft) being called shrubs,[5] so the minimum height of a tree is only loosely defined.[4] Large herbaceous plants such as papaya and bananas are trees in this broad sense.[2][6]A commonly applied narrower definition is that a tree has a woody trunk formed by secondary growth, meaning that the trunk thickens each year by growing outwards, in addition to the primary upwards growth from the growing tip.[4][7] Under such a definition, herbaceous plants such as palms, bananas and papayas are not considered trees regardless of their height, growth form or stem girth. Certain monocots may be considered trees under a slightly looser definition;[8] while the Joshua tree, bamboos and palms do not have secondary growth and never produce true wood with growth rings,[9][10] they may produce "pseudo-wood" by lignifying cells formed by primary growth.[11]Aside from structural definitions, trees are commonly defined by use; for instance, as those plants which yield lumber.[12]The tree growth habit is an evolutionary adaptation found in different groups of plants: by growing taller, trees are able to compete better for sunlight.[13] Trees tend to be tall and long-lived,[14] some reaching several thousand years old.[15] Several trees are among the oldest organisms now living.[16] Trees have modified structures such as thicker stems composed of specialised cells that add structural strength and durability, allowing them to grow taller than many other plants and to spread out their foliage. They differ from shrubs, which have a similar growth form, by usually growing larger and having a single main stem;[5] but there is no consistent distinction between a tree and a shrub,[17] made more confusing by the fact that trees may be reduced in size under harsher environmental conditions such as on mountains and subarctic areas. The tree form has evolved separately in unrelated classes of plants in response to similar environmental challenges, making it a classic example of parallel evolution. With an estimated 60,000-100,000 species, the number of trees worldwide might total twenty-five per cent of all living plant species.[18][19] The greatest number of these grow in tropical regions and many of these areas have not yet been fully surveyed by botanists, making tree diversity and ranges poorly known.[20]The majority of tree species are angiosperms. There are about 1000 species of gymnosperm trees,[21] including conifers, cycads, ginkgophytes and gnetales; they produce seeds which are not enclosed in fruits, but in open structures such as pine cones, and many have tough waxy leaves, such as pine needles.[22] Most angiosperm trees are eudicots, the "true dicotyledons", so named because the seeds contain two cotyledons or seed leaves. There are also some trees among the old lineages of flowering plants called  basal angiosperms or paleodicots; these include Amborella, Magnolia, nutmeg and avocado,[23] while trees such as bamboo, palms and bananas are monocots.Wood gives structural strength to the trunk of most types of tree; this supports the plant as it grows larger. The vascular system of trees allows water, nutrients and other chemicals to be distributed around the plant, and without it trees would not be able to grow as large as they do. Trees, as relatively tall plants, need to draw water up the stem through the xylem from the roots by the suction produced as water evaporates from the leaves. If insufficient water is available the leaves will die.[24] The three main parts of trees include the root, stem, and leaves; they are integral parts of the vascular system which interconnects all the living cells. In trees and other plants that develop wood, the vascular cambium allows the expansion of vascular tissue that produces woody growth. Because this growth ruptures the epidermis of the stem, woody plants also have a cork cambium that develops among the phloem. The cork cambium gives rise to thickened cork cells to protect the surface of the plant and reduce water loss. Both the production of wood and the production of cork are forms of secondary growth.[25]Trees are either evergreen, having foliage that persists and remains green throughout the year,[26] or deciduous, shedding their leaves at the end of the growing season and then having a dormant period without foliage.[27] Most conifers are evergreens, but larches (Larix and Pseudolarix) are deciduous, dropping their needles each autumn, and some species of cypress (Glyptostrobus, Metasequoia and Taxodium) shed small leafy shoots annually in a process known as cladoptosis.[5] The crown is a name for the spreading top of a tree including the branches and leaves,[28] while the uppermost layer in a forest, formed by the crowns of the trees, is known as the canopy.[29] A sapling is a young tree.[30]Many tall palms are herbaceous[31] monocots; these do not undergo secondary growth and never produce wood.[9][10] In many tall palms, the terminal bud on the main stem is the only one to develop, so they have unbranched trunks with large spirally arranged leaves. Some of the tree ferns, order Cyatheales, have tall straight trunks, growing up to 20 metres (66 ft), but these are composed not of wood but of rhizomes which grow vertically and are covered by numerous adventitious roots.[32]The number of trees in the world, according to a 2015 estimate, is 3.04 trillion, of which 1.39 trillion (46%) are in the tropics or sub-tropics, 0.61 trillion (20%) in the temperate zones, and 0.74 trillion (24%) in the coniferous boreal forests. The estimate is about eight times higher than previous estimates, and is based on tree densities measured on over 400,000 plots. It remains subject to a wide margin of error, not least because the samples are mainly from Europe and North America. The estimate suggests that about 15 billion trees are cut down annually and about 5 billion are planted. In the 12,000 years since the start of human agriculture, the number of trees worldwide has decreased by 46%.[1][33][34][35]In suitable environments, such as the Daintree Rainforest in Queensland, or the mixed podocarp and broadleaf forest of Ulva Island, New Zealand, forest is the more-or-less stable climatic climax community at the end of a plant succession, where open areas such as grassland are colonised by taller plants, which in turn give way to trees that eventually form a forest canopy.[36][37]In cool temperate regions, conifers often predominate; a widely distributed climax community in the far north of the northern hemisphere is moist taiga or northern coniferous forest (also called boreal forest).[38][39] Taiga is the world's largest land biome, forming 29% of the world's forest cover.[40] The long cold winter of the far north is unsuitable for plant growth and trees must grow rapidly in the short summer season when the temperature rises and the days are long. Light is very limited under their dense cover and there may be little plant life on the forest floor, although fungi may abound.[41] Similar woodland is found on mountains where the altitude causes the average temperature to be lower thus reducing the length of the growing season.[42]Where rainfall is relatively evenly spread across the seasons in temperate regions, temperate broadleaf and mixed forest typified by species like oak, beech, birch and maple is found.[43] Temperate forest is also found in the southern hemisphere, as for example in the Eastern Australia temperate forest, characterised by Eucalyptus forest and open acacia woodland.[44]In tropical regions with a monsoon or monsoon-like climate, where a drier part of the year alternates with a wet period as in the Amazon rainforest, different species of broad-leaved trees dominate the forest, some of them being deciduous.[45] In tropical regions with a drier savanna climate and insufficient rainfall to support dense forests, the canopy is not closed, and plenty of sunshine reaches the ground which is covered with grass and scrub. Acacia and baobab are well adapted to living in such areas.[46]The roots of a tree serve to anchor it to the ground and gather water and nutrients to transfer to all parts of the tree. They are also used for reproduction, defence, survival, energy storage and many other purposes. The radicle or embryonic root is the first part of a seedling to emerge from the seed during the process of germination. This develops into a taproot which goes straight downwards. Within a few weeks lateral roots branch out of the side of this and grow horizontally through the upper layers of the soil. In most trees, the taproot eventually withers away and the wide-spreading laterals remain. Near the tip of the finer roots are single cell root hairs. These are in immediate contact with the soil particles and can absorb water and nutrients such as potassium in solution. The roots require oxygen to respire and only a few species such as the mangrove and the pond cypress (Taxodium ascendens) can live in permanently waterlogged soil.[47]In the soil, the roots encounter the hyphae of fungi. Many of these are known as mycorrhiza and form a mutualistic relationship with the tree roots. Some are specific to a single tree species, which will not flourish in the absence of its mycorrhizal associate. Others are generalists and associate with many species. The tree acquires minerals such as phosphorus from the fungus while it obtains the carbohydrate products of photosynthesis from the tree.[48] The hyphae of the fungus can link different trees and a network is formed, transferring nutrients from one place to another. The fungus promotes growth of the roots and helps protect the trees against predators and pathogens. It can also limit damage done to a tree by pollution as the fungus accumulate heavy metals within its tissues.[49] Fossil evidence shows that roots have been associated with mycorrhizal fungi since the early Paleozoic, four hundred million years ago, when the first vascular plants colonised dry land.[50]Some trees such as the alders (Alnus species) have a symbiotic relationship with Frankia species, a filamentous bacterium that can fix nitrogen from the air, converting it into ammonia. They have actinorhizal root nodules on their roots in which the bacteria live. This process enables the tree to live in low nitrogen habitats where they would otherwise be unable to thrive.[51] The plant hormones called cytokinins initiate root nodule formation, in a process closely related to mycorrhizal association.[52]It has been demonstrated that some trees are interconnected through their root system, forming a colony. The interconnections are made by the inosculation process, a kind of natural grafting or welding of vegetal tissues. The tests to demonstrate this networking are performed by injecting chemicals, sometimes radioactive, into a tree, and then checking for its presence in neighbouring trees.[53]The roots are, generally, an underground part of the tree, but some tree species have evolved roots that are aerial. The common purposes for aerial roots may be of two kinds, to contribute to the mechanical stability of the tree, and to obtain oxygen from air. An instance of mechanical stability enhancement is the red mangrove that develops prop roots that loop out of the trunk and branches and descend vertically into the mud.[54] A similar structure is developed by the Indian banyan.[55] Many large trees have buttress roots which flare out from the lower part of the trunk. These brace the tree rather like angle brackets and provide stability, reducing sway in high winds. They are particularly prevalent in tropical rainforests where the soil is poor and the roots are close to the surface.[56]Some tree species have developed root extensions that pop out of soil, in order to get oxygen, when it is not available in the soil because of excess water. These root extensions are called pneumatophores, and are present, among others, in black mangrove and pond cypress.[54]The main purpose of the trunk is to raise the leaves above the ground, enabling the tree to overtop other plants and outcompete them for light.[57] It also transports water and nutrients from the roots to the aerial parts of the tree, and distributes the food produced by the leaves to all other parts, including the roots.[58]In the case of angiosperms and gymnosperms, the outermost layer of the trunk is the bark, mostly composed of dead cells of phellem (cork).[59] It provides a thick, waterproof covering to the living inner tissue. It protects the trunk against the elements, disease, animal attack and fire. It is perforated by a large number of fine breathing pores called lenticels, through which oxygen diffuses. Bark is continually replaced by a living layer of cells called the cork cambium or phellogen.[59] The London plane (Platanus × acerifolia) periodically sheds its bark in large flakes. Similarly, the bark of the silver birch (Betula pendula) peels off in strips. As the tree's girth expands, newer layers of bark are larger in circumference, and the older layers develop fissures in many species. In some trees such as the pine (Pinus species) the bark exudes sticky resin which deters attackers whereas in rubber trees (Hevea brasiliensis) it is a milky latex that oozes out. The quinine bark tree (Cinchona officinalis) contains bitter substances to make the bark unpalatable.[58] Large tree-like plants with lignified trunks in the Pteridophyta, Arecales, Cycadophyta and Poales such as the tree ferns, palms, cycads and bamboos have different structures and outer coverings.[60]Although the bark functions as a protective barrier, it is itself attacked by boring insects such as beetles. These lay their eggs in crevices and the larvae chew their way through the cellulose tissues leaving a gallery of tunnels. This may allow fungal spores to gain admittance and attack the tree. Dutch elm disease is caused by a fungus (Ophiostoma species) carried from one elm tree to another by various beetles. The tree reacts to the growth of the fungus by blocking off the xylem tissue carrying sap upwards and the branch above, and eventually the whole tree, is deprived of nourishment and dies. In Britain in the 1990s, 25 million elm trees were killed by this disease.[61]The innermost layer of bark is known as the phloem and this is involved in the transport of the sap containing the sugars made by photosynthesis to other parts of the tree. It is a soft spongy layer of living cells, some of which are arranged end to end to form tubes. These are supported by parenchyma cells which provide padding and include fibres for strengthening the tissue.[62] Inside the phloem is a layer of undifferentiated cells one cell thick called the vascular cambium layer. The cells are continually dividing, creating phloem cells on the outside and wood cells known as xylem on the inside.[63]The newly created xylem is the sapwood. It is composed of water-conducting cells and associated cells which are often living, and is usually pale in colour. It transports water and minerals from the roots to the upper parts of the tree. The oldest, inner part of the sapwood is progressively converted into heartwood as new sapwood is formed at the cambium. The conductive cells of the heartwood are blocked in some species, and the surrounding cells are more often dead. Heartwood is usually darker in colour than the sapwood. It is the dense central core of the trunk giving it rigidity. Three quarters of the dry mass of the xylem is cellulose, a polysaccharide, and most of the remainder is lignin, a complex polymer. A transverse section through a tree trunk or a horizontal core will show concentric circles or lighter or darker wood - tree rings. These rings are the annual growth rings[64] There may also be rays running at right angles to growth rings. These are vascular rays which are thin sheets of living tissue permeating the wood.[65] Many older trees may become hollow but may still stand upright for many years.[66]Trees do not usually grow continuously throughout the year but mostly have spurts of active expansion followed by periods of rest. This pattern of growth is related to climatic conditions; growth normally ceases when conditions are either too cold or too dry. In readiness for the inactive period, trees form buds to protect the meristem, the zone of active growth. Before the period of dormancy, the last few leaves produced at the tip of a twig form scales. These are thick, small and closely wrapped and enclose the growing point in a waterproof sheath. Inside this bud there is a rudimentary stalk and neatly folded miniature leaves, ready to expand when the next growing season arrives. Buds also form in the axils of the leaves ready to produce new side shoots. A few trees, such as the eucalyptus, have "naked buds" with no protective scales and some conifers, such as the Lawson's cypress, have no buds but instead have little pockets of meristem concealed among the scale-like leaves.[67]When growing conditions improve, such as the arrival of warmer weather and the longer days associated with spring in temperate regions, growth starts again. The expanding shoot pushes its way out, shedding the scales in the process. These leave behind scars on the surface of the twig. The whole year's growth may take place in just a few weeks. The new stem is unlignified at first and may be green and downy. The Arecaceae (palms) have their leaves spirally arranged on an unbranched trunk.[67] In some tree species in temperate climates, a second spurt of growth, a Lammas growth may occur which is believed to be a strategy to compensate for loss of early foliage to insect predators.[68]Primary growth is the elongation of the stems and roots. Secondary growth consists of a progressive thickening and strengthening of the tissues as the outer layer of the epidermis is converted into bark and the cambium layer creates new phloem and xylem cells. The bark is inelastic.[69] Eventually the growth of a tree slows down and stops and it gets no taller. If damage occurs the tree may in time become hollow.[70]Leaves are structures specialised for photosynthesis and are arranged on the tree in such a way as to maximise their exposure to light without shading each other.[71] They are an important investment by the tree and may be thorny or contain phytoliths, lignins, tannins or poisons to discourage herbivory. Trees have evolved leaves in a wide range of shapes and sizes, in response to environmental pressures including climate and predation. They can be broad or needle-like, simple or compound, lobed or entire, smooth or hairy, delicate or tough, deciduous or evergreen. The needles of coniferous trees are compact but are structurally similar to those of broad-leaved trees. They are adapted for life in environments where resources are low or water is scarce. Frozen ground may limit water availability and conifers are often found in colder places at higher altitudes and higher latitudes than broad leaved trees. In conifers such as fir trees, the branches hang down at an angle to the trunk, enabling them to shed snow. In contrast, broad leaved trees in temperate regions deal with winter weather by shedding their leaves. When the days get shorter and the temperature begins to decrease, the leaves no longer make new chlorophyll and the red and yellow pigments already present in the blades become apparent.[71] Synthesis in the leaf of a plant hormone called auxin also ceases. This causes the cells at the junction of the petiole and the twig to weaken until the joint breaks and the leaf floats to the ground. In tropical and subtropical regions, many trees keep their leaves all year round. Individual leaves may fall intermittently and be replaced by new growth but most leaves remain intact for some time. Other tropical species and those in arid regions may shed all their leaves annually, such as at the start of the dry season.[72] Many deciduous trees flower before the new leaves emerge.[73] A few trees do not have true leaves but instead have structures with similar external appearance such as Phylloclades – modified stem structures[74] – as seen in the genus Phyllocladus.[75]Trees can be pollinated either by wind or by animals, mostly insects. Many angiosperm trees are insect pollinated. Wind pollination may take advantage of increased wind speeds high above the ground.[76] Trees use a variety of methods of seed dispersal. Some rely on wind, with winged or plumed seeds. Others rely on animals, for example with edible fruits. Others again eject their seeds (ballistic dispersal), or use gravity so that seeds fall and sometimes roll.[77]Seeds are the primary way that trees reproduce and their seeds vary greatly in size and shape. Some of the largest seeds come from trees, but the largest tree, Sequoiadendron giganteum, produces one of the smallest tree seeds.[78] The great diversity in tree fruits and seeds reflects the many different ways that tree species have evolved to disperse their offspring.For a tree seedling to grow into an adult tree it needs light. If seeds only fell straight to the ground, competition among the concentrated saplings and the shade of the parent would likely prevent it from flourishing. Many seeds such as birch are small and have papery wings to aid dispersal by the wind. Ash trees and maples have larger seeds with blade shaped wings which spiral down to the ground when released. The kapok tree has cottony threads to catch the breeze.[79]The seeds of conifers, the largest group of gymnosperms, are enclosed in a cone and most species have seeds that are light and papery that can be blown considerable distances once free from the cone.[80] Sometimes the seed remains in the cone for years waiting for a trigger event to liberate it. Fire stimulates release and germination of seeds of the jack pine, and also enriches the forest floor with wood ash and removes competing vegetation.[81] Similarly, a number of angiosperms including Acacia cyclops and Acacia mangium have seeds that germinate better after exposure to high temperatures.[82]The flame tree Delonix regia does not rely on fire but shoots its seeds through the air when the two sides of its long pods crack apart explosively on drying.[79] The miniature cone-like catkins of alder trees produce seeds that contain small droplets of oil that help disperse the seeds on the surface of water. Mangroves often grow in water and some species have propagules, which are buoyant fruits with seeds that start germinating before becoming detached from the parent tree.[83][84] These float on the water and may become lodged on emerging mudbanks and successfully take root.[79]Other seeds, such as apple pips and plum stones, have fleshy receptacles and smaller fruits like hawthorns have seeds enclosed in edible tissue; animals including mammals and birds eat the fruits and either discard the seeds, or swallow them so they pass through the gut to be deposited in the animal's droppings well away from the parent tree. The germination of some seeds is improved when they are processed in this way.[85] Nuts may be gathered by animals such as squirrels that cache any not immediately consumed.[86] Many of these caches are never revisited, the nut-casing softens with rain and frost, and the seed germinates in the spring.[87] Pine cones may similarly be hoarded by red squirrels, and grizzly bears may help to disperse the seed by raiding squirrel caches.[88]The single extant species of Ginkgophyta (Ginkgo biloba) has fleshy seeds produced at the ends of short branches on female trees,[89] and Gnetum, a tropical and subtropical group of gymnosperms produce seeds at the tip of a shoot axis.[90]The earliest trees were tree ferns, horsetails and lycophytes, which grew in forests in the Carboniferous period. The first tree may have been Wattieza, fossils of which have been found in New York State in 2007 dating back to the Middle Devonian (about 385 million years ago). Prior to this discovery, Archaeopteris was the earliest known tree.[91] Both of these reproduced by spores rather than seeds and are considered to be links between ferns and the gymnosperms which evolved in the Triassic period. The gymnosperms include conifers, cycads, gnetales and ginkgos and these may have appeared as a result of a whole genome duplication event which took place about 319 million years ago.[92] Ginkgophyta was once a widespread diverse group[93] of which the only survivor is the maidenhair tree Ginkgo biloba. This is considered to be a living fossil because it is virtually unchanged from the fossilised specimens found in Triassic deposits.[94]During the Mesozoic (245 to 66 million years ago) the conifers flourished and became adapted to live in all the major terrestrial habitats. Subsequently, the tree forms of flowering plants evolved during the Cretaceous period. These began to displace the conifers during the Tertiary era (66 to 2 million years ago) when forests covered the globe.[95] When the climate cooled 1.5 million years ago and the first of four ice ages occurred, the forests retreated as the ice advanced. In the interglacials, trees recolonised the land that had been covered by ice, only to be driven back again in the next ice age.[95]Trees are an important part of the terrestrial ecosystem,[96] providing essential habitats including many kinds of forest for communities of organisms. Epiphytic plants such as ferns, some mosses, liverworts, orchids and some species of parasitic plants (e.g., mistletoe) hang from branches;[97] these along with arboreal lichens, algae, and fungi provide micro-habitats for themselves and for other organisms, including animals. Leaves, flowers and fruits are seasonally available. On the ground underneath trees there is shade, and often there is undergrowth, leaf litter, and decaying wood that provide other habitat.[98][99] Trees stabilise the soil, prevent rapid run-off of rain water, help prevent desertification, have a role in climate control and help in the maintenance of biodiversity and ecosystem balance.[100]Many species of tree support their own specialised invertebrates. In their natural habitats, 284 different species of insect have been found on the English oak (Quercus robur)[101] and 306 species of invertebrate on the Tasmanian oak (Eucalyptus obliqua).[102] Non-native tree species provide a less biodiverse community, for example in the United Kingdom the sycamore (Acer pseudoplatanus), which originates from southern Europe, has few associated invertebrate species, though its bark supports a wide range of lichens, bryophytes and other epiphytes.[103]In ecosystems such as mangrove swamps, trees play a role in developing the habitat, since the roots of the mangrove trees reduce the speed of flow of tidal currents and trap water-borne sediment, reducing the water depth and creating suitable conditions for further mangrove colonisation. Thus mangrove swamps tend to extend seawards in suitable locations.[104] Mangrove swamps also provide an effective buffer against the more damaging effects of cyclones and tsunamis.[105]Silviculture is the practice of controlling the establishment, growth, composition, health, and quality of forests, which are areas that have a high density of trees. Cultivated trees are planted and tended by humans, usually because they provide food (fruits or nuts), ornamental beauty, or some type of wood product that benefits people. An area of land planted with fruit or nut trees is an orchard.[106] A small wooded area, usually with no undergrowth, is called a grove[107] and a small wood or thicket of trees and bushes is called a coppice or copse.[108] A large area of land covered with trees and undergrowth is called woodland or forest.[109] An area of woodland composed primarily of trees established by planting or artificial seeding is known as a plantation.[110]Trees are the source of many of the world's best known fleshy fruits. Apples, pears, plums, cherries and citrus are all grown commercially in temperate climates and a wide range of edible fruits are found in the tropics. Other commercially important fruit include dates, figs and olives. Palm oil is obtained from the fruits of the oil palm (Elaeis guineensis). The fruits of the cocoa tree (Theobroma cacao) are used to make cocoa and chocolate and the berries of coffee trees, Coffea arabica and Coffea canephora, are processed to extract the coffee beans. In many rural areas of the world, fruit is gathered from forest trees for consumption.[111] Many trees bear edible nuts which can loosely be described as being large, oily kernels found inside a hard shell. These include coconuts (Cocos nucifera), Brazil nuts (Bertholletia excelsa), pecans (Carya illinoinensis), hazel nuts (Corylus), almonds (Prunus dulcis), walnuts (Juglans regia), pistachios (Pistacia vera) and many others. They are high in nutritive value and contain high-quality protein, vitamins and minerals as well as dietary fibre. Walnuts are particularly beneficial to health and contain a higher level of antioxidants than do other nuts.[112] A variety of nut oils are extracted by pressing for culinary use; some such as walnut, pistachio and hazelnut oils are prized for their distinctive flavours, but they tend to spoil quickly.[113]In temperate climates there is a sudden movement of sap at the end of the winter as trees prepare to burst into growth. In North America, the sap of the sugar maple (Acer saccharum) is most often used in the production of a sweet liquid, maple syrup. About 90% of the sap is water, the remaining 10% being a mixture of various sugars and certain minerals.[114] The sap is harvested by drilling holes in the trunks of the trees and collecting the liquid that flows out of the inserted spigots. It is piped to a sugarhouse where it is heated to concentrate it and improve its flavour. One litre of maple syrup is obtained from every forty litres of sap and has a sugar content of exactly 66%.[114] Similarly in northern Europe the spring rise in the sap of the silver birch (Betula pendula) is tapped and collected, either to be drunk fresh or fermented into an alcoholic drink. In Alaska, the sap of the sweet birch (Betula lenta) is made into a syrup with a sugar content of 67%. Sweet birch sap is more dilute than maple sap; a hundred litres are required to make one litre of birch syrup.[115]Various parts of trees are used as spices. These include cinnamon, made from the bark of the cinnamon tree (Cinnamomum zeylanicum) and allspice, the dried small fruits of the pimento tree (Pimenta dioica). Nutmeg is a seed found in the fleshy fruit of the nutmeg tree (Myristica fragrans) and cloves are the unopened flower buds of the clove tree (Syzygium aromaticum).[116]Many trees have flowers rich in nectar which are attractive to bees. The production of forest honey is an important industry in rural areas of the developing world where it is undertaken by small-scale beekeepers using traditional methods.[117] The flowers of the elder (Sambucus) are used to make elderflower cordial and petals of the plum (Prunus spp.) can be candied.[118] Sassafras oil is a flavouring obtained from distilling bark from the roots of the white sassafras tree (Sassafras albidum).The leaves of trees are widely gathered as fodder for livestock and some can be eaten by humans but they tend to be high in tannins which makes them bitter. Leaves of the curry tree (Murraya koenigii) are eaten, those of kaffir lime (Citrus × hystrix) (in Thai food)[119] and Ailanthus (in Korean dishes such as bugak) and those of the European bay tree (Laurus nobilis) and the California bay tree (Umbellularia californica) are used for flavouring food.[116] Camellia sinensis, the source of tea, is a small tree but seldom reaches its full height, being heavily pruned to make picking the leaves easier.[120]Wood has traditionally been used for fuel, especially in rural areas. In less developed nations it may be the only fuel available and collecting firewood is often a time consuming task as it becomes necessary to travel further and further afield in the search for fuel.[121] It is often burned inefficiently on an open fire. In more developed countries other fuels are available and burning wood is a choice rather than a necessity. Modern wood-burning stoves are very fuel efficient and new products such as wood pellets are available to burn.[122]Charcoal can be made by slow pyrolysis of wood by heating it in the absence of air in a kiln. The carefully stacked branches, often oak, are burned with a very limited amount of air. The process of converting them into charcoal takes about fifteen hours. Charcoal is used as a fuel in barbecues and by blacksmiths and has many industrial and other uses.[123]Wood smoke can be used to preserve food. In the hot smoking process the food is exposed to smoke and heat in a controlled environment. The food is ready to eat when the process is complete, having been tenderised and flavoured by the smoke it has absorbed. In the cold process, the temperature is not allowed to rise above 100 °F (38 °C). The flavour of the food is enhanced but raw food requires further cooking. If it is to be preserved, meat should be cured before cold smoking.[124]Timber, "trees that are grown in order to produce wood"[125] is cut into lumber (sawn wood) for use in construction. Wood has been an important, easily available material for construction since humans started building shelters. Engineered wood products are available which bind the particles, fibres or veneers of wood together with adhesives to form composite materials. Plastics have taken over from wood for some traditional uses.[126]Wood is used in the construction of buildings, bridges, trackways, piles, poles for power lines, masts for boats, pit props, railway sleepers, fencing, hurdles, shuttering for concrete, pipes, scaffolding and pallets. In housebuilding it is used in joinery, for making joists, roof trusses, roofing shingles, thatching, staircases, doors, window frames, floor boards, parquet flooring, panelling and cladding.[127]Wood is used to construct carts, farm implements, boats, dugout canoes and in shipbuilding. It is used for making furniture, tool handles, boxes, ladders, musical instruments, bows, weapons, matches, clothes pegs, brooms, shoes, baskets, turnery, carving, toys, pencils, rollers, cogs, wooden screws, barrels, coffins, skittles, veneers, artificial limbs, oars, skis, wooden spoons, sports equipment and wooden balls.[127]Wood is pulped for paper and used in the manufacture of cardboard and made into engineered wood products for use in construction such as fibreboard, hardboard, chipboard and plywood.[127] The wood of conifers is known as softwood while that of broad-leaved trees is hardwood.[128]Besides inspiring artists down the centuries, trees have been used to create art. Living trees have been used in bonsai and in tree shaping, and both living and dead specimens have been sculpted into sometimes fantastic shapes.[129]Bonsai (盆栽,  lit. The art of growing a miniature tree or trees in a low-sided pot or tray) is the practice of hòn non bộ originated in China and spread to Japan more than a thousand years ago, there are similar practices in other cultures like the living miniature landscapes of Vietnam hòn non bộ. The word bonsai is often used in English as an umbrella term for all miniature trees in containers or pots.[130]The purposes of bonsai are primarily contemplation (for the viewer) and the pleasant exercise of effort and ingenuity (for the grower).[131] Bonsai practice focuses on long-term cultivation and shaping of one or more small trees growing in a container, beginning with a cutting, seedling, or small tree of a species suitable for bonsai development. Bonsai can be created from nearly any perennial woody-stemmed tree or shrub species[132] that produces true branches and can be cultivated to remain small through pot confinement with crown and root pruning. Some species are popular as bonsai material because they have characteristics, such as small leaves or needles, that make them appropriate for the compact visual scope of bonsai and a miniature deciduous forest can even be created using such species as Japanese maple, Japanese zelkova or hornbeam.[133]Tree shaping is the practice of changing living trees and other woody plants into man made shapes for art and useful structures. There are a few different methods[134] of shaping a tree. There is a gradual method and there is an instant method. The gradual method slowly guides the growing tip along predetermined pathways over time whereas the instant method bends and weaves saplings 2 to 3 m (6.6 to 9.8 ft) long into a shape that becomes more rigid as they thicken up.[135] Most artists use grafting of living trunks, branches, and roots, for art or functional structures and there are plans to grow "living houses" with the branches of trees knitting together to give a solid, weatherproof exterior combined with an interior application of straw and clay to provide a stucco-like inner surface.[135]Tree shaping has been practised for at least several hundred years, the oldest known examples being the living root bridges built and maintained by the Khasi people of Meghalaya, India using the roots of the rubber tree (Ficus elastica).[136][137]Cork is produced from the thick bark of the cork oak (Quercus suber). It is harvested from the living trees about once every ten years in an environmentally sustainable industry.[138] More than half the world's cork comes from Portugal and is largely used to make stoppers for wine bottles.[139] Other uses include floor tiles, bulletin boards, balls, footwear, cigarette tips, packaging, insulation and joints in woodwind instruments.[139]The bark of other varieties of oak has traditionally been used in Europe for the tanning of hides though bark from other species of tree has been used elsewhere. The active ingredient, tannin, is extracted and after various preliminary treatments, the skins are immersed in a series of vats containing solutions in increasing concentrations. The tannin causes the hide to become supple, less affected by water and more resistant to bacterial attack.[140]At least 120 drugs come from plant sources, many of them from the bark of trees.[141] Quinine originates from the cinchona tree (Cinchona) and was for a long time the remedy of choice for the treatment of malaria.[142] Aspirin was synthesised to replace the sodium salicylate derived from the bark of willow trees (Salix) which had unpleasant side effects.[143] The anti-cancer drug Paclitaxel is derived from taxol, a substance found in the bark of the Pacific yew (Taxus brevifolia).[144] Other tree based drugs come from the paw-paw (Carica papaya), the cassia (Cassia spp.), the cocoa tree (Theobroma cacao), the tree of life (Camptotheca acuminata) and the downy birch (Betula pubescens).[141]The papery bark of the white birch tree (Betula papyrifera) was used extensively by Native Americans. Wigwams were covered by it and canoes were constructed from it. Other uses included food containers, hunting and fishing equipment, musical instruments, toys and sledges.[145] Nowadays, bark chips, a by-product of the timber industry, are used as a mulch and as a growing medium for epiphytic plants that need a soil-free compost.[146]Trees create a visual impact in the same way as do other landscape features and give a sense of maturity and permanence to park and garden. They are grown for the beauty of their forms, their foliage, flowers, fruit and bark and their siting is of major importance in creating a landscape. They can be grouped informally, often surrounded by plantings of bulbs, laid out in stately avenues or used as specimen trees. As living things, their appearance changes with the season and from year to year.[147]Trees are often planted in town environments where they are known as street trees or amenity trees. They can provide shade and cooling through evapotranspiration, absorb greenhouse gases and pollutants, intercept rainfall, and reduce the risk of flooding. It has been shown that they are beneficial to humans in creating a sense of well-being and reducing stress. Many towns have initiated tree-planting programmes.[148] In London for example, there is an initiative to plant 20,000 new street trees and to have an increase in tree cover of 5% by 2025, equivalent to one tree for every resident.[149]Latex is a sticky defensive secretion that protects plants against herbivores. Many trees produce it when injured but the main source of the latex used to make natural rubber is the Pará rubber tree (Hevea brasiliensis). Originally used to create bouncy balls and for the waterproofing of cloth, natural rubber is now mainly used in tyres for which synthetic materials have proved less durable.[150] The latex exuded by the balatá tree (Manilkara bidentata) is used to make golf balls and is similar to gutta-percha, made from the latex of the "getah perca" tree Palaquium. This is also used as an insulator, particularly of undersea cables, and in dentistry, walking sticks and gun butts. It has now largely been replaced by synthetic materials.[151]Resin is another plant exudate that may have a defensive purpose. It is a viscous liquid composed mainly of volatile terpenes and is produced mostly by coniferous trees. It is used in varnishes, for making small castings and in ten-pin bowling balls. When heated, the terpenes are driven off and the remaining product is called "rosin" and is used by stringed instrumentalists on their bows. Some resins contain essential oils and are used in incense and aromatherapy. Fossilised resin is known as amber and was mostly formed in the Cretaceous (145 to 66 million years ago) or more recently. The resin that oozed out of trees sometimes trapped insects or spiders and these are still visible in the interior of the amber.[152]The camphor tree (Cinnamomum camphora) produces an essential oil[116] and the eucalyptus tree (Eucalyptus globulus) is the main source of eucalyptus oil which is used in medicine, as a fragrance and in industry.[153]Dead trees pose a safety risk, especially during high winds and severe storms, and removing dead trees involves a financial burden, whereas the presence of healthy trees can clean the air, increase property values, and reduce the temperature of the built environment and thereby reduce building cooling costs. During times of drought, trees can fall into water stress, which may cause a tree to become more susceptible to disease and insect problems, and ultimately may lead to a tree's death. Irrigating trees during dry periods can reduce the risk of water stress and death.[154]Trees have been venerated since time immemorial. To the ancient Celts, certain trees, especially the oak, ash and thorn, held special significance[155] as providing fuel, building materials, ornamental objects and weaponry. Other cultures have similarly revered trees, often linking the lives and fortunes of individuals to them or using them as oracles. In Greek mythology, dryads were believed to be shy nymphs who inhabited trees.The Oubangui people of west Africa plant a tree when a child is born. As the tree flourishes, so does the child but if the tree fails to thrive, the health of the child is considered at risk. When it flowers it is time for marriage. Gifts are left at the tree periodically and when the individual dies, their spirit is believed to live on in the tree.[156]Trees have their roots in the ground and their trunk and branches extended towards the sky. This concept is found in many of the world's religions as a tree which links the underworld and the earth and holds up the heavens. In Norse mythology, Yggdrasil is a central cosmic tree whose roots and branches extend to various worlds. Various creatures live on it.[157] In India, Kalpavriksha is a wish-fulfilling tree, one of the nine jewels that emerged from the primitive ocean. Icons are placed beneath it to be worshipped, tree nymphs inhabit the branches and it grants favours to the devout who tie threads round the trunk.[158] Democracy started in North America when the Great Peacemaker formed the Iroquois Confederacy, inspiring the warriors of the original five American nations to bury their weapons under the Tree of Peace, an eastern white pine (Pinus strobus).[159] In the creation story in the Bible, the tree of life and the knowledge of good and evil was planted by God in the Garden of Eden.[160]Sacred groves exist in China, India, Africa and elsewhere. They are places where the deities live and where all the living things are either sacred or are companions of the gods. Folklore lays down the supernatural penalties that will result if desecration takes place for example by the felling of trees. Because of their protected status, sacred groves may be the only relicts of ancient forest and have a biodiversity much greater than the surrounding area.[161]Some Ancient Indian tree deities, such as Puliyidaivalaiyamman, the Tamil deity of the  tamarind tree, or Kadambariyamman, associated with the kadamba tree were seen as manifestations of a goddess who offers her blessings by giving fruits in abundance.[162]Trees have a theoretical maximum height of 130 m (430 ft),[163] but the tallest known specimen on earth is believed to be a coast redwood (Sequoia sempervirens) at Redwood National Park, California. It has been named Hyperion and is 115.85 m (380.1 ft) tall.[164] In 2006, it was reported to be 379.1 ft (115.5 m) tall.[165] The tallest known broad-leaved tree is a mountain ash (Eucalyptus regnans) growing in Tasmania with a height of 99.8 m (327 ft).[166]The largest tree by volume is believed to be a giant sequoia (Sequoiadendron giganteum) known as the General Sherman Tree in the Sequoia National Park in Tulare County, California. Only the trunk is used in the calculation and the volume is estimated to be 1,487 m3 (52,500 cu ft).[167]The oldest living tree with a verified age is also in California. It is a Great Basin bristlecone pine (Pinus longaeva) growing in the White Mountains. It has been dated by drilling a core sample and counting the annual rings. It is estimated to currently be 5,068 years old.[a][168]A little farther south, at Santa Maria del Tule, Oaxaca, Mexico, is the tree with the broadest trunk. It is a Montezuma cypress (Taxodium mucronatum) known as Árbol del Tule and its diameter at breast height is 11.62 m (38.1 ft) giving it a girth of 36.2 m (119 ft). The tree's trunk is far from round and the exact dimensions may be misleading as the circumference includes much empty space between the large buttress roots.[169]Wohlleben, Peter; Flannery, Tim F.; Simard, S.; Billinghurst, Jane (2016). The Hidden Life of Trees: What They Feel, How They Communicate: Discoveries from a Secret World. ISBN 9781771642484. OCLC 933722592.
Cork (material)
Cork is an impermeable buoyant material, the phellem layer of bark tissue that is harvested for commercial use primarily from Quercus suber (the cork oak), which is endemic to southwest Europe and northwest Africa. Cork is composed of suberin, a hydrophobic substance. Because of its impermeable, buoyant, elastic, and fire retardant properties, it is used in a variety of products, the most common of which is wine stoppers. The montado landscape of Portugal produces approximately half of cork harvested annually worldwide, with Corticeira Amorim being the leading company in the industry.[1] Cork was examined microscopically by Robert Hooke, which led to his discovery and naming of the cell.[2]There are about 2,200,000 hectares of cork forest worldwide; 34% in Portugal and 27% in Spain.Annual production is about 200,000 tons; 49.6% from Portugal, 30.5% from Spain, 5.8% from Morocco, 4.9% from Algeria, 3.5% from Tunisia, 3.1% Italy, and 2.6% from France.[3]Once the trees are about 25 years old the cork is traditionally stripped from the trunks every nine years, with the first two harvests generally producing lower quality cork. The trees live for about 300 years.The cork industry is generally regarded as environmentally friendly.[4] Cork production is generally considered sustainable because the cork tree is not cut down to obtain cork; only the bark is stripped to harvest the cork.[5] The tree continues to live and grow. The sustainability of production and the easy recycling of cork products and by-products are two of its most distinctive aspects. Cork oak forests also prevent desertification and are a particular habitat in the Iberian Peninsula and the refuge of various endangered species.[6]Carbon footprint studies conducted by Corticeira Amorim, Oeneo Bouchage of France and the Cork Supply Group of Portugal concluded that cork is the most environmentally friendly wine stopper in comparison to other alternatives. The Corticeira Amorim’s study, in particular ("Analysis of the life cycle of Cork, Aluminum and Plastic Wine Closures"), was developed by PricewaterhouseCoopers, according to ISO 14040.[7] Results concluded that, concerning the emission of greenhouse gases, each plastic stopper released 10 times more CO2, whilst an aluminium screw cap releases 26 times more CO2 than does a cork stopper.The cork oak is unrelated to the "cork trees" (Phellodendron), which have corky bark but are not used for cork production.Cork is extracted only from early May to late August, when the cork can be separated from the tree without causing permanent damage. When the tree reaches 25–30 years of age and about 24 in (60 cm) in circumference, the cork can be removed for the first time. However, this first harvest almost always produces poor quality or "virgin" cork (Portuguese cortiça virgem; Spanish corcho bornizo or corcho virgen[8]). Bark from initial harvests can be used to make flooring, shoes, insulation and other industrial products. Subsequent extractions usually occur at intervals of 9 years, though it can take up to 13 for the cork to reach an acceptable size. If the product is of high quality it is known as "gentle" cork (Portuguese cortiça amadia,[9] but also cortiça secundeira only if it is the second time; Spanish corcho segundero, also restricted to the "second time"[8]), and, ideally, is used to make stoppers for wine and champagne bottles.[10]The workers who specialize in removing the cork are known as extractors. An extractor uses a very sharp axe to make two types of cuts on the tree: one horizontal cut around the plant, called a crown or necklace, at a height of about 2–3 times the circumference of the tree, and several vertical cuts called rulers or openings. This is the most delicate phase of the work because, even though cutting the cork requires significant force, the extractor must not damage the underlying phellogen or the tree will be harmed.To free the cork from the tree, the extractor pushes the handle of the axe into the rulers. A good extractor needs to use a firm but precise touch in order to free a large amount of cork without damaging the product or tree.These freed portions of the cork are called planks. The planks are usually carried off by hand since cork forests are rarely accessible to vehicles. The cork is stacked in piles in the forest or in yards at a factory and traditionally left to dry, after which it can be loaded onto a truck and shipped to a processor.Cork's elasticity combined with its near-impermeability makes it suitable as a material for bottle stoppers, especially for wine bottles. Cork stoppers represent about 60% of all cork based production. Cork has an almost zero Poisson's ratio, which means the radius of a cork does not change significantly when squeezed or pulled.[11]Cork is an excellent gasket material. Some carburetor float bowl gaskets are made of cork, for example.Cork is also an essential element in the production of badminton shuttlecocks.Cork's bubble-form structure and natural fire retardant make it suitable for acoustic and thermal insulation in house walls, floors, ceilings and facades. The by-product of more lucrative stopper production, corkboard is gaining popularity as a non-allergenic, easy-to-handle and safe alternative to petrochemical-based insulation products.Sheets of cork, also often the by-product of stopper production, are used to make bulletin boards as well as floor and wall tiles.Cork's low density makes it a suitable material for fishing floats and buoys, as well as handles for fishing rods (as an alternative to neoprene).Granules of cork can also be mixed into concrete. The composites made by mixing cork granules and cement have lower thermal conductivity, lower density and good energy absorption. Some of the property ranges of the composites are density (400–1500 kg/m³), compressive strength (1–26 MPa) and flexural strength (0.5–4.0 MPa).[12]As late as the mid-17th century, French vintners did not use cork stoppers, using instead oil-soaked rags stuffed into the necks of bottles.[13]Wine corks can be made of either a single piece of cork, or composed of particles, as in champagne corks; corks made of granular particles are called "agglomerated corks".[14]Natural cork closures are used for about 80% of the 20 billion bottles of wine produced each year. After a decline in use as wine-stoppers due to the increase in the use of synthetic alternatives, cork wine-stoppers are making a comeback and currently represent approximately 60% of wine-stoppers today[when?].[citation needed]Because of the cellular structure of cork, it is easily compressed upon insertion into a bottle and will expand to form a tight seal. The interior diameter of the neck of glass bottles tends to be inconsistent, making this ability to seal through variable contraction and expansion an important attribute.  However, unavoidable natural flaws, channels, and cracks in the bark make the cork itself highly inconsistent. In a 2005 closure study, 45% of corks showed gas leakage during pressure testing both from the sides of the cork as well as through the cork body itself.[15]Since the mid-1990s, a number of wine brands have switched to alternative wine closures such as plastic stoppers, screw caps, or other closures. During 1972 more than half of the Australian bottled wine went bad due to corking. A great deal of anger and suspicion was directed at Portuguese and Spanish cork suppliers who were suspected of deliberately supplying bad cork to non-EEC wine makers to help prevent cheap imports. Cheaper wine makers developed the aluminium "Spelvin" cap with a polypropylene stopper wad. More expensive wines and  carbonated varieties continued to use cork, although much closer attention was paid to the quality. Even so, some high premium makers prefer the Spelvin as it is a guarantee that the wine will be good even after many decades of ageing. Some consumers may have conceptions about screw caps being representative of lower quality wines, due to their cheaper price; however, in Australia, for example, much of the non-sparkling wine production now uses these Spelvin caps as a cork alternative, although some have recently switched back to cork citing issues using screw caps.[16] These alternatives to cork have both advantages and disadvantages. For example, screwtops are generally considered to offer a trichloroanisole (TCA) free seal, but they also reduce the oxygen transfer rate between the bottle and the atmosphere to almost zero, which can lead to a reduction in the quality of the wine.[citation needed] TCA is the main documented cause of cork taint in wine. However, some in the wine industry say natural cork stoppers are important because they allow oxygen to interact with wine for proper aging, and are best suited for wines purchased with the intent to age.[17] Stoppers which resemble natural cork very closely can be made by isolating the suberin component of the cork from the undesirable lignin, mixing it with the same substance used for contact lenses and an adhesive, and molding it into a standardized product, free of TCA or other undesirable substances.[18]  Composite corks with real cork veneers are used in cheaper wines.[19]The study "Analysis of the life cycle of Cork, Aluminum and Plastic Wine Closures," conducted by PricewaterhouseCoopers and commissioned by a major cork manufacturer, Amorim, concluded that cork is the most environmentally responsible stopper, in a one-year life cycle analysis comparison with plastic stoppers and aluminum screw caps.[20][21]Cork is used in musical instruments, particularly woodwind instruments, where it is used to fasten together segments of the instrument, making the seams airtight. Low quality conducting baton handles are also often made out of cork.It is also used in shoes, especially those using welt construction to improve climate control and comfort.Because it is impermeable and moisture-resistant, cork is often used as an alternative to leather in handbags, wallets and other fashion items.Cork can be used to make bricks for the outer walls of houses, as in Portugal's pavilion at Expo 2000.On November 28, 2007, the Portuguese national postal service CTT issued the world's first postage stamp made of cork.[22][23]Cork is used as the core of both baseballs and cricket balls. A corked bat is made by replacing the interior of a baseball bat with cork – a practice known as "corking". It was historically a method of cheating at baseball; the efficacy of the practice is now discredited.Cork is often used, in various forms, in spacecraft heat shields[24] and fairings.Cork can be used in the paper pick-up mechanisms in inkjet and laser printers.Cork is used to make later-model pith helmets.[25]Corks are also hung from hats to keep insects away. (See cork hat)Cork has been used as a core material in sandwich composite construction.Cork can be used as the friction lining material of an automatic transmission clutch, as designed in certain mopeds.Cork can be used instead of wood or aluminium in automotive interiors.[26]Cork can also be used to make watch bands and faces as seen with Sprout Watches.Cork slabs are sometimes used by orchid growers as a natural mounting material.Cork paddles are used by glass blowers to manipulate and shape hot molten glass.
Natural rubber
Natural rubber, also called India rubber or caoutchouc, as initially produced, consists of polymers of the organic compound isoprene, with minor impurities of other organic compounds, plus water. Malaysia and Indonesia are two of the leading rubber producers. Forms of polyisoprene that are used as natural rubbers are classified as elastomers.Currently, rubber is harvested mainly in the form of the latex from the rubber tree or others. The latex is a sticky, milky colloid drawn off by making incisions in the bark and collecting the fluid in vessels in a process called "tapping". The latex then is refined into rubber ready for commercial processing.  In major areas, latex is allowed to coagulate in the collection cup. The coagulated lumps are collected and processed into dry forms for marketing.Natural rubber is used extensively in many applications and products, either alone or in combination with other materials. In most of its useful forms, it has a large stretch ratio and high resilience, and is extremely waterproof.[1]The major commercial source of natural rubber latex is the Pará rubber tree (Hevea brasiliensis), a member of the spurge family, Euphorbiaceae. This species is preferred because it grows well under cultivation. A properly managed tree responds to wounding by producing more latex for several years.Congo rubber, formerly a major source of rubber, came from vines in the genus Landolphia (L. kirkii, L. heudelotis, and L. owariensis).[2]Dandelion milk contains latex. The latex exhibits the same quality as the natural rubber from rubber trees. In the wild types of dandelion, latex content is low and varies greatly. In Nazi Germany, research projects tried to use dandelions as a base for rubber production, but failed.[3] In 2013, by inhibiting one key enzyme and using modern cultivation methods and optimization techniques, scientists in the Fraunhofer Institute for Molecular Biology and Applied Ecology (IME) in Germany developed a cultivar that is suitable for commercial production of natural rubber.[4] In collaboration with Continental Tires, IME began a pilot facility.Many other plants produce forms of latex rich in isoprene polymers, though not all produce usable forms of polymer as easily as the Pará. Some of them require more elaborate processing to produce anything like usable rubber, and most are more difficult to tap. Some produce other desirable materials, for example gutta-percha (Palaquium gutta)[5] and chicle from Manilkara species. Others that have been commercially exploited, or at least showed promise as rubber sources, include the rubber fig (Ficus elastica), Panama rubber tree (Castilla elastica), various spurges (Euphorbia spp.), lettuce (Lactuca species), the related Scorzonera tau-saghyz, various Taraxacum species, including common dandelion (Taraxacum officinale) and Russian dandelion (Taraxacum kok-saghyz), and perhaps most importantly for its hypoallergenic properties, guayule (Parthenium argentatum). The term gum rubber is sometimes applied to the tree-obtained version of natural rubber in order to distinguish it from the synthetic version.[1]The first use of rubber was by the indigenous cultures of Mesoamerica. The earliest archeological evidence of the use of natural latex from the Hevea tree comes from the Olmec culture, in which rubber was first used for making balls for the Mesoamerican ballgame. Rubber was later used by the  Maya and Aztec cultures – in addition to making balls Aztecs used rubber for other purposes such as making containers and to make textiles waterproof by impregnating them with the latex sap.[6][7]The Pará rubber tree is indigenous to South America. Charles Marie de La Condamine is credited with introducing samples of rubber to the Académie Royale des Sciences of France in 1736.[8] In 1751, he presented a paper by François Fresneau to the Académie (published in 1755) that described many of rubber's properties. This has been referred to as the first scientific paper on rubber.[8] In England, Joseph Priestley, in 1770, observed that a piece of the material was extremely good for rubbing off pencil marks on paper, hence the name "rubber". It slowly made its way around England. In 1764 François Fresnau discovered that turpentine was a rubber solvent. Giovanni Fabbroni is credited with the discovery of naphtha as a rubber solvent in 1779.South America remained the main source of the limited amounts of latex rubber used during much of the 19th century. The trade was heavily protected and exporting seeds from Brazil was a capital offense, although no law prohibited it. Nevertheless, in 1876, Henry Wickham smuggled 70,000 Pará rubber tree seeds from Brazil and delivered them to Kew Gardens, England. Only 2,400 of these germinated. Seedlings were then sent to India, British Ceylon (Sri Lanka), Dutch East Indies (Indonesia), Singapore, and British Malaya. Malaya (now Peninsular Malaysia) was later to become the biggest producer of rubber.In the early 1900s, the Congo Free State in Africa was also a significant source of natural rubber latex, mostly gathered by forced labor. King Leopold II's colonial state brutally enforced production quotas. Tactics to enforce the rubber quotas included removing the hands of victims to prove they had been killed. Soldiers often came back from raids with baskets full of chopped-off hands. Villages that resisted were razed to encourage better compliance locally.[9] See Atrocities in the Congo Free State for more information on the rubber trade in the Congo Free State in the late 1800s and early 1900s. Liberia and Nigeria started production.In India, commercial cultivation was introduced by British planters, although the experimental efforts to grow rubber on a commercial scale were initiated as early as 1873 at the Calcutta Botanical Gardens. The first commercial Hevea plantations were established at Thattekadu in Kerala in 1902. In later years the plantation expanded to Karnataka, Tamil Nadu and the Andaman and Nicobar Islands of India. India today is the world's 3rd largest producer and 4th largest consumer.[10]In Singapore and Malaya, commercial production was heavily promoted by Sir Henry Nicholas Ridley, who served as the first Scientific Director of the Singapore Botanic Gardens from 1888 to 1911. He distributed rubber seeds to many planters and developed the first technique for tapping trees for latex without causing serious harm to the tree.[11] Because of his fervent promotion of this crop, he is popularly remembered by the nickname "Mad Ridley".[12]Charles Goodyear developed vulcanization in 1839, although Mesoamericans used stabilized rubber for balls and other objects as early as 1600 BC.[13][14]Before World War II significant uses included door and window profiles, hoses, belts, gaskets, matting, flooring and dampeners (antivibration mounts) for the automotive industry. The use of rubber in car tires (initially solid rather than pneumatic) in particular consumed a significant amount of rubber.  Gloves (medical, household and industrial) and toy balloons were large consumers of rubber, although the type of rubber used is concentrated latex. Significant tonnage of rubber was used as adhesives in many manufacturing industries and products, although the two most noticeable were the paper and the carpet industries. Rubber was commonly used to make rubber bands and pencil erasers.Rubber produced as a fiber, sometimes called 'elastic', had significant value to the textile industry because of its excellent elongation and recovery properties. For these purposes, manufactured rubber fiber was made as either an extruded round fiber or rectangular fibers cut into strips from extruded film. Because of its low dye acceptance, feel and appearance, the rubber fiber was either covered by yarn of another fiber or directly woven with other yarns into the fabric. Rubber yarns were used in foundation garments. While rubber is still used in textile manufacturing, its low tenacity limits its use in lightweight garments because latex lacks resistance to oxidizing agents and is damaged by aging, sunlight, oil and perspiration. The textile industry turned to neoprene (polymer of chloroprene), a type of synthetic rubber, as well as another more commonly used elastomer fiber, spandex (also known as elastane), because of their superiority to rubber in both strength and durability.Rubber exhibits unique physical and chemical properties. Rubber's stress–strain behavior exhibits the Mullins effect and the Payne effect and is often modeled as hyperelastic. Rubber strain crystallizes.Due to the presence of weakened allylic C-H bonds in each repeat unit, natural rubber is susceptible to vulcanisation as well as being sensitive to ozone cracking.The two main solvents for rubber are turpentine and naphtha (petroleum). Because rubber does not dissolve easily, the material is finely divided by shredding prior to its immersion.An ammonia solution can be used to prevent the coagulation of raw latex.Rubber begins to melt at approximately 180 °C (356 °F).On a microscopic scale, relaxed rubber is a disorganized cluster of erratically changing wrinkled chains. In stretched rubber, the chains are almost linear. The restoring force is due to the preponderance of wrinkled conformations over more linear ones. For the quantitative treatment see ideal chain, for more examples see entropic force.Cooling below the glass transition temperature permits local conformational changes but a reordering is practically impossible because of the larger energy barrier for the concerted movement of longer chains. "Frozen" rubber's elasticity is low and strain results from small changes of bond lengths and angles: this caused the Challenger disaster, when the American Space Shuttle's flattened o-rings failed to relax to fill a widening gap.[15] The glass transition is fast and reversible: the force resumes on heating.The parallel chains of stretched rubber are susceptible to crystallization. This takes some time because turns of twisted chains have to move out of the way of the growing crystallites. Crystallization has occurred, for example, when, after days, an inflated toy balloon is found withered at a relatively large remaining volume. Where it is touched, it shrinks because the temperature of the hand is enough to melt the crystals.Vulcanization of rubber creates di- and polysulfide bonds between chains, which limits the degrees of freedom and results in chains that tighten more quickly for a given strain, thereby increasing the elastic force constant and making the rubber harder and less extensible.Raw rubber storage depots and rubber processing can produce malodour that is serious enough to become a source of complaints and protest to those living in the vicinity.[16]Microbial impurities originate during the processing of block rubber. These impurities break down during storage or thermal degradation and produce volatile organic compounds. Examination of these compounds using gas chromatography/mass spectrometry (GC/MS) and gas chromatography (GC) indicates that they contain sulphur, ammonia, alkenes, ketones, esters, hydrogen sulphite, nitrogen, and low molecular weight fatty acids (C2-C5).[17][18]When latex concentrate is produced from rubber, sulphuric acid is used for coagulation. This produces malodourous hydrogen sulphide.[18]The industry can mitigate these bad odours with scrubber systems.[18]Latex is the polymer cis-1,4-polyisoprene – with a molecular weight of 100,000 to 1,000,000 daltons. Typically, a small percentage (up to 5% of dry mass) of other materials, such as proteins, fatty acids, resins, and inorganic materials (salts) are found in natural rubber. Polyisoprene can also be created synthetically, producing what is sometimes referred to as "synthetic natural rubber", but the synthetic and natural routes are different.[1] Some natural rubber sources, such as gutta-percha, are composed of trans-1,4-polyisoprene, a structural isomer that has similar properties.Natural rubber is an elastomer and a thermoplastic. Once the rubber is vulcanized, it is a thermoset. Most rubber in everyday use is vulcanized to a point where it shares properties of both; i.e., if it is heated and cooled, it is degraded but not destroyed.The final properties of a rubber item depend not just on the polymer, but also on modifiers and fillers, such as carbon black, factice, whiting and others.Rubber particles are formed in the cytoplasm of specialized latex-producing cells called laticifers within rubber plants.[19] Rubber particles are surrounded by a single phospholipid membrane with hydrophobic tails pointed inward. The membrane allows biosynthetic proteins to be sequestered at the surface of the growing rubber particle, which allows new monomeric units to be added from outside the biomembrane, but within the lacticifer. The rubber particle is an enzymatically active entity that contains three layers of material, the rubber particle, a biomembrane and free monomeric units. The biomembrane is held tightly to the rubber core due to the high negative charge along the double bonds of the rubber polymer backbone.[20] Free monomeric units and conjugated proteins make up the outer layer. The rubber precursor is isopentenyl pyrophosphate (an allylic compound), which elongates by Mg2+-dependent condensation by the action of rubber transferase. The monomer adds to the pyrophosphate end of the growing polymer.[21] The process displaces the terminal high-energy pyrophosphate. The reaction produces a cis polymer. The initiation step is catalyzed by prenyltransferase, which converts three monomers of isopentenyl pyrophosphate into farnesyl pyrophosphate.[22] The farnesyl pyrophosphate can bind to rubber transferase to elongate a new rubber polymer.The required isopentenyl pyrophosphate is obtained from the mevalonate pathway, which derives from acetyl-CoA in the cytosol. In plants, isoprene pyrophosphate can also be obtained from the 1-deox-D-xyulose-5-phosphate/2-C-methyl-D-erythritol-4-phosphate pathway within plasmids.[23] The relative ratio of the farnesyl pyrophosphate initiator unit and isoprenyl pyrophosphate elongation monomer determines the rate of new particle synthesis versus elongation of existing particles. Though rubber is known to be produced by only one enzyme, extracts of latex host numerous small molecular weight proteins with unknown function. The proteins possibly serve as cofactors, as the synthetic rate decreases with complete removal.[24]Close to 28 million tons of rubber were produced in 2013, of which approximately 44% was natural. Since the bulk is synthetic, which is derived from petroleum, the price of natural rubber is determined, to a large extent, by the prevailing global price of crude oil.[25][26][27] Asia was the main source of natural rubber, accounting for about 94% of output in 2005. The three largest producers, Thailand, Indonesia (2.4 million tons)[28] and Malaysia, together account for around 72% of all natural rubber production. Natural rubber is not cultivated widely in its native continent of South America due to the existence of South American leaf blight, and other natural predators.Rubber latex is extracted from rubber trees. The economic life period of rubber trees in plantations is around 32 years — up to 7 years of immature phase and about 25 years of productive phase.The soil requirement is well-drained, weathered soil consisting of laterite, lateritic types, sedimentary types, nonlateritic red or alluvial soils.The climatic conditions for optimum growth of rubber trees are:Many high-yielding clones have been developed for commercial planting. These clones yield more than 2,000 kg of dry rubber per hectare per year, under ideal conditions.In places such as Kerala and Sri Lanka where coconuts are in abundance, the half shell of coconut was used as the latex collection container. Glazed pottery or aluminium or plastic cups became  more common in Kerala and other countries. The cups are supported by a wire that encircles the tree. This wire incorporates a spring so it can stretch as the tree grows. The latex is led into the cup by a galvanised "spout" knocked into the bark. Tapping normally takes place early in the morning, when the internal pressure of the tree is highest. A good tapper can tap a tree every 20 seconds on a standard half-spiral system, and a common daily "task" size is between 450 and 650 trees. Trees are usually tapped on alternate or third days, although many variations in timing, length and number of cuts are used. "Tappers would make a slash in the bark with a small hatchet. These slanting cuts allowed latex to flow from ducts located on the exterior or the inner layer of bark (cambium) of the tree. Since the cambium controls the growth of the tree, growth stops if it is cut. Thus, rubber tapping demanded accuracy, so that the incisions would not be too many given the size of the tree, or too deep, which could stunt its growth or kill it."[29]It is usual to tap a pannel at least twice, sometimes three times, during the tree's life. The economic life of the tree depends on how well the tapping is carried out, as the critical factor is bark consumption. A standard in Malaysia for alternate daily tapping is 25 cm (vertical) bark consumption per year. The latex-containing tubes in the bark ascend in a spiral to the right. For this reason, tapping cuts usually ascend to the left to cut more tubes.The trees drip latex for about four hours, stopping as latex coagulates naturally on the tapping cut, thus blocking the latex tubes in the bark. Tappers usually rest and have a meal after finishing their tapping work, then start collecting the liquid "field latex" at about midday.The four types of field coagula are "cuplump", "treelace", "smallholders' lump" and "earth scrap". Each has significantly different properties.[30] Some trees continue to drip after the collection leading to a small amount of "cup lump" that is collected at the next tapping. The latex that coagulates on the cut is also collected as "tree lace". Tree lace and cup lump together account for 10–20% of the dry rubber produced. Latex that drips onto the ground, "earth scrap", is also collected periodically for processing of low-grade product.Cup lump is the coagulated material found in the collection cup when the tapper next visits the tree to tap it again. It arises from latex clinging to the walls of the cup after the latex was last poured into the bucket, and from late-dripping latex exuded before the latex-carrying vessels of the tree become blocked. It is of higher purity and of greater value than the other three types.Tree lace is the coagulum strip that the tapper peels off the previous cut before making a new cut. It usually has higher copper and manganese contents than cup lump. Both copper and manganese are pro-oxidants and can damage the physical properties of the dry rubber.Smallholders' lump is produced by smallholders who collect rubber from trees far from the nearest factory. Many Indonesian smallholders, who farm paddies in remote areas, tap dispersed trees on their way to work in the paddy fields and collect the latex (or the coagulated latex) on their way home. As it is often impossible to preserve the latex sufficiently to get it to a factory that processes latex in time for it to be used to make high quality products, and as the latex would anyway have coagulated by the time it reached the factory, the smallholder will coagulate it by any means available, in any container available. Some smallholders use small containers, buckets etc., but often the latex is coagulated in holes in the ground, which are usually lined with plastic sheeting. Acidic materials and fermented fruit juices are used to coagulate the latex — a form of assisted biological coagulation. Little care is taken to exclude twigs, leaves, and even bark from the lumps that are formed, which may also include tree lace.Earth scrap is material that gathers around the base of the tree. It arises from latex overflowing from the cut and running down the bark, from rain flooding a collection cup containing latex, and from spillage from tappers' buckets during collection. It contains soil and other contaminants, and has variable rubber content, depending on the amount of contaminants. Earth scrap is collected by field workers two or three times a year and may be cleaned in a scrap-washer to recover the rubber, or sold to a contractor who cleans it and recovers the rubber. It is of low quality.Latex coagulates in the cups if kept for long and must be collected before this happens. The collected latex, "field latex", is transferred into coagulation tanks for the preparation of dry rubber or transferred into air-tight containers with sieving for ammoniation. Ammoniation preserves the latex in a colloidal state for longer periods of time.Latex is generally processed into either latex concentrate for manufacture of dipped goods or coagulated under controlled, clean conditions using formic acid. The coagulated latex can then be processed into the higher-grade, technically specified block rubbers such as SVR 3L or SVR CV or used to produce Ribbed Smoke Sheet grades.Naturally coagulated rubber (cup lump) is used in the manufacture of TSR10 and TSR20 grade rubbers. Processing for these grades is a size reduction and cleaning process to remove contamination and prepare the material for the final stage of drying.[31]The dried material is then baled and palletized for storage and shipment.Natural rubber is often vulcanized, a process by which the rubber is heated and sulfur, peroxide or bisphenol are added to improve resistance and elasticity and to prevent it from perishing. Before World War II, carbon black was often used as an additive to rubber to improve its strength, especially in vehicle tires.Natural rubber latex is shipped from factories in south-west Asia, South America, and West and Center Africa to destinations around the world. As the cost of natural rubber has risen significantly and rubber products are dense, the shipping methods offering the lowest cost per unit weight are preferred. Depending on destination, warehouse availability, and transportation conditions, some methods are preferred by certain buyers. In international trade, latex rubber is mostly shipped in 20-foot ocean containers. Inside the container, smaller containers are used to store the latex.[32]Uncured rubber is used for cements;[33] for adhesive, insulating, and friction tapes; and for crepe rubber used in insulating blankets and footwear. Vulcanized rubber has many more applications. Resistance to abrasion makes softer kinds of rubber valuable for the treads of vehicle tires and conveyor belts, and makes hard rubber valuable for pump housings and piping used in the handling of abrasive sludge.The flexibility of rubber is appealing in hoses, tires and rollers for devices ranging from domestic clothes wringers to printing presses; its elasticity makes it suitable for various kinds of shock absorbers and for specialized machinery mountings designed to reduce vibration. Its relative gas impermeability makes it useful in the manufacture of articles such as air hoses, balloons, balls and cushions. The resistance of rubber to water and to the action of most fluid chemicals has led to its use in rainwear, diving gear, and chemical and medicinal tubing, and as a lining for storage tanks, processing equipment and railroad tank cars. Because of their electrical resistance, soft rubber goods are used as insulation and for protective gloves, shoes and blankets; hard rubber is used for articles such as telephone housings, parts for radio sets, meters and other electrical instruments. The coefficient of friction of rubber, which is high on dry surfaces and low on wet surfaces, leads to its use for power-transmission belting and for water-lubricated bearings in deep-well pumps. Indian rubber balls or lacrosse balls are made of rubber.Around 25 million tonnes of rubber are produced each year, of which 30 percent is natural.[34] The remainder is synthetic rubber derived from petrochemical sources. The top end of latex production results in latex products such as surgeons' gloves, condoms, balloons and other relatively high-value products. The mid-range which comes from the technically specified natural rubber materials ends up largely in tires but also in conveyor belts, marine products, windshield wipers and miscellaneous goods. Natural rubber offers good elasticity, while synthetic materials tend to offer better resistance to environmental factors such as oils, temperature, chemicals and ultraviolet light. "Cured rubber" is rubber that has been compounded and subjected to the vulcanisation process to create cross-links within the rubber matrix.Some people have a serious latex allergy, and exposure to natural latex rubber products such as latex gloves can cause anaphylactic shock. The antigenic proteins found in Hevea latex may be deliberately reduced (though not eliminated)[35] through processing.Latex from non-Hevea sources, such as Guayule, can be used without allergic reaction by persons with an allergy to Hevea latex.[36]Some allergic reactions are not to the latex itself, but from residues of chemicals used to accelerate the cross-linking process. Although this may be confused with an allergy to latex, it is distinct from it, typically taking the form of Type IV hypersensitivity in the presence of traces of specific processing chemicals.[35][37]Natural rubber is susceptible to degradation by a wide range of bacteria[which?].[38][39][40][41][42][43][44][45]The bacteria Streptomyces coelicolor, Pseudomonas citronellolis, and Nocardia spp. are capable of degrading vulcanized natural rubber.[46]
Taxon
In biology, a taxon (plural taxa; back-formation from taxonomy) is a group of one or more populations of an organism or organisms seen by taxonomists to form a unit. Although neither is required, a taxon is usually known by a particular name and given a particular ranking, especially if and when it is accepted or becomes established. It is not uncommon, however, for taxonomists to remain at odds over what belongs to a taxon and the criteria used for inclusion. If a taxon is given a formal scientific name, its use is then governed by one of the nomenclature codes specifying which scientific name is correct for a particular grouping.Although preceded by Linnaeus's system in Systema Naturae (10th edition, 1758)[1] and unpublished work by Bernard and Antoine Laurent de Jussieu, the notion of a unit-based "natural system" of biological classification was first made widely available in 1805 through the publication, as the introduction to the third edition of Jean-Baptiste Lamarck's Flore françoise, of Augustin Pyramus de Candolle's Principes élémentaires de botanique, an exposition of a system for the "natural classification" of plants. Since then, systematists have striven to construct an accurate classification encompassing the diversity of life; today, a "good" or "useful" taxon is commonly taken to be one that reflects evolutionary relationships.[n 1] Many modern systematists, such as advocates of phylogenetic nomenclature, use cladistic methods that require taxa to be monophyletic (all descendants of some ancestor). Their basic unit, therefore, is the clade rather than the taxon. Similarly, among those contemporary taxonomists working with the traditional Linnean (binomial) nomenclature, few propose taxa they know to be paraphyletic.[2] An example of a well-established taxon that is not also a clade is the class Reptilia, the reptiles; birds are descendants of reptiles but are not included in the Reptilia.The term taxon was first used in 1926 by Adolf Meyer-Abich for animal groups, as a backformation from the word Taxonomy; the word Taxonomy had been coined a century before from the Greek components τάξις (taxis, meaning arrangement) and -νομία (-nomia meaning method).[3][4] For plants, it was proposed by Herman Johannes Lam in 1948, and it was adopted at the VII International Botanical Congress, held in 1950.[5]The Glossary of the International Code of Zoological Nomenclature (1999) defines[6] a  A taxon can be assigned a taxonomic rank, usually (but not necessarily) when it is given a formal name."Phylum" applies formally to any biological domain, but traditionally it was always used for animals, whereas "Division" was traditionally often used for plants, fungi, etc.A prefix is used to indicate a ranking of lesser importance. The prefix super- indicates a rank above, the prefix sub- indicates a rank below. In zoology the prefix infra- indicates a rank below sub-.  For instance, among the additional ranks of class are superclass, subclass and infraclass.Rank is relative, and restricted to a particular systematic schema. For example, liverworts have been grouped, in various systems of classification, as a family, order, class, or division (phylum). The use of a narrow set of ranks is challenged by users of cladistics; for example, the mere 10 ranks traditionally used between animal families (governed by the ICZN) and animal phyla (usually the highest relevant rank in taxonomic work) often cannot adequately represent the evolutionary history as more about a lineage's phylogeny becomes known. In addition, the class rank is quite often not an evolutionary but a phenetic or paraphyletic group and as opposed to those ranks governed by the ICZN (family-level, genus-level and species-level taxa), can usually not be made monophyletic by exchanging the taxa contained therein. This has given rise to phylogenetic taxonomy and the ongoing development of the PhyloCode, which has been proposed as a new alternative to replace Linnean classification and govern the application of names to clades.  Many cladists do not see any need to depart from traditional nomenclature as governed by the ICZN, ICN, etc.
Phosphorus
Phosphorus is a chemical element with symbol P and atomic number 15. Elemental phosphorus exists in two major forms, white phosphorus and red phosphorus, but because it is highly reactive, phosphorus is never found as a free element on Earth. It has a concentration in the Earth's crust of about one gram per kilogram (compare copper at about 0.06 grams). With few exceptions, minerals containing phosphorus are in the maximally oxidized state as inorganic phosphate rocks.Elemental phosphorus was first isolated (as white phosphorus) in 1669 and emitted a faint glow when exposed to oxygen – hence the name, taken from Greek mythology, Φωσφόρος meaning "light-bearer" (Latin Lucifer), referring to the "Morning Star", the planet Venus. The term "phosphorescence", meaning glow after illumination, derives from this property of phosphorus, although the word has since been used for a different physical process that produces a glow. The glow of phosphorus is caused by oxidation of the white (but not red) phosphorus — a process now called chemiluminescence.  Together with nitrogen, arsenic, antimony, and bismuth, phosphorus is classified as a pnictogen.Phosphorus is essential for life. Phosphates (compounds containing the phosphate ion, PO43−) are a component of DNA, RNA, ATP, and phospholipids. Elemental phosphorus was first isolated from human urine, and bone ash was an important early phosphate source. Phosphate mines contain fossils because phosphate is present in the fossilized deposits of animal remains and excreta. Low phosphate levels are an important limit to growth in some aquatic systems. The vast majority of phosphorus compounds mined are consumed as fertilisers. Phosphate is needed to replace the phosphorus that plants remove from the soil, and its annual demand is rising nearly twice as fast as the growth of the human population. Other applications include organophosphorus compounds in detergents, pesticides, and nerve agents.Phosphorus has several allotropes that exhibit strikingly different properties.[8] The two most common allotropes are white phosphorus and red phosphorus.[9]From the perspective of applications and chemical literature, the most important form of elemental phosphorus is white phosphorus, often abbreviated as WP. It is a soft, waxy solid which consists of tetrahedral P4 molecules, in which each atom is bound to the other three atoms by a single bond. This P4 tetrahedron is also present in liquid and gaseous phosphorus up to the temperature of 800 °C (1,470 °F) when it starts decomposing to P2 molecules.[10] White phosphorus exists in two crystalline forms: α (alpha) and β (beta). At room temperature, the α-form is stable, which is more common and it has cubic crystal structure and at 195.2 K (−78.0 °C), it transforms into β-form, which has hexagonal crystal structure. These forms differ in terms of the relative orientations of the constituent P4 tetrahedra.[11][12]White phosphorus is the least stable, the most reactive, the most volatile, the least dense, and the most toxic of the allotropes. White phosphorus gradually changes to red phosphorus. This transformation is accelerated by light and heat, and samples of white phosphorus almost always contain some red phosphorus and accordingly appear yellow. For this reason, white phosphorus that is aged or otherwise impure (e.g., weapons-grade, not lab-grade WP) is also called yellow phosphorus. When exposed to oxygen, white phosphorus glows in the dark with a very faint tinge of green and blue.  It is highly flammable and pyrophoric (self-igniting) upon contact with air.  Owing to its pyrophoricity, white phosphorus is used as an additive in napalm. The odour of combustion of this form has a characteristic garlic smell, and samples are commonly coated with white "phosphorus pentoxide", which consists of P4O10 tetrahedra with oxygen inserted between the phosphorus atoms and at their vertices. White phosphorus is insoluble in water but soluble in carbon disulfide.[13]Thermolysis of P4 at 1100 kelvin gives diphosphorus, P2. This species is not stable as a solid or liquid. The dimeric unit contains a triple bond and is analogous to N2. It can also be generated as a transient intermediate in solution by thermolysis of organophosphorus precursor reagents.[14] At still higher temperatures, P2 dissociates into atomic P.[13]Red phosphorus is polymeric in structure. It can be viewed as a derivative of P4 wherein one P-P bond is broken, and one additional bond is formed with the neighbouring tetrahedron resulting in a chain-like structure. Red phosphorus may be formed by heating white phosphorus to 250 °C (482 °F) or by exposing white phosphorus to sunlight.[15] Phosphorus after this treatment is amorphous. Upon further heating, this material crystallises. In this sense, red phosphorus is not an allotrope, but rather an intermediate phase between the white and violet phosphorus, and most of its properties have a range of values. For example, freshly prepared, bright red phosphorus is highly reactive and ignites at about 300 °C (572 °F),[16] though it is more stable than white phosphorus, which ignites at about 30 °C (86 °F).[17] After prolonged heating or storage, the color darkens (see infobox images); the resulting product is more stable and does not spontaneously ignite in air.[18]Violet phosphorus is a form of phosphorus that can be produced by day-long annealing of red phosphorus above 550 °C. In 1865, Hittorf discovered that when phosphorus was recrystallised from molten lead, a red/purple form is obtained. Therefore, this form is sometimes known as "Hittorf's phosphorus" (or violet or α-metallic phosphorus).[19]Black phosphorus is the least reactive allotrope and the thermodynamically stable form below 550 °C (1,022 °F). It is also known as β-metallic phosphorus and has a structure somewhat resembling that of graphite.[20][21] It is obtained by heating white phosphorus under high pressures (about 12,000 standard atmospheres or 1.2 gigapascals). It can also be produced at ambient conditions using metal salts, e.g. mercury, as catalysts.[22] In appearance, properties, and structure, it resembles graphite, being black and flaky, a conductor of electricity, and has puckered sheets of linked atoms.[23]Another form, scarlet phosphorus, is obtained by allowing a solution of white phosphorus in carbon disulfide to evaporate in sunlight.[19]When first isolated, it was observed that the green glow emanating from white phosphorus would persist for a time in a stoppered jar, but then cease. Robert Boyle in the 1680s ascribed it to "debilitation" of the air. Actually, it is oxygen being consumed. By the 18th century, it was known that in pure oxygen, phosphorus does not glow at all;[24] there is only a range of partial pressures at which it does. Heat can be applied to drive the reaction at higher pressures.[25]In 1974, the glow was explained by R. J. van Zee and A. U. Khan.[26][27] A reaction with oxygen takes place at the surface of the solid (or liquid) phosphorus, forming the short-lived molecules HPO and P2O2 that both emit visible light. The reaction is slow and only very little of the intermediates are required to produce the luminescence, hence the extended time the glow continues in a stoppered jar.Since its discovery, phosphors and phosphorescence were used loosely to describe substances that shine in the dark without burning. Although the term phosphorescence is derived from phosphorus, the reaction that gives phosphorus its glow is properly called chemiluminescence (glowing due to a cold chemical reaction), not phosphorescence (re-emitting light that previously fell onto a substance and excited it).[28]Twenty-three isotopes of phosphorus are known,[29] including all possibilities from 24P up to 46P. Only 31P is stable and is therefore present at 100% abundance. The half-integer nuclear spin and high abundance of 31P make phosphorus-31 NMR spectroscopy a very useful analytical tool in studies of phosphorus-containing samples.Two radioactive isotopes of phosphorus have half-lives suitable for biological scientific experiments. These are:The high energy beta particles from 32P penetrate skin and corneas and any 32P ingested, inhaled, or absorbed is readily incorporated into bone and nucleic acids.  For these reasons, Occupational Safety and Health Administration in the United States, and similar institutions in other developed countries require personnel working with 32P to wear lab coats, disposable gloves, and safety glasses or goggles to protect the eyes, and avoid working directly over open containers.  Monitoring personal, clothing, and surface contamination is also required.  Shielding requires special consideration.  The high energy of the beta particles gives rise to secondary emission of X-rays via Bremsstrahlung (braking radiation) in dense shielding materials such as lead.  Therefore, the radiation must be shielded with low density materials such as acrylic or other plastic, water, or (when transparency is not required), even wood.[30]In 2013, astronomers detected phosphorus in Cassiopeia A, which confirmed that this element is produced in supernovae as a byproduct of supernova nucleosynthesis. The phosphorus-to-iron ratio in material from the supernova remnant could be up to 100 times higher than in the Milky Way in general.[31]Phosphorus has a concentration in the Earth's crust of about one gram per kilogram (compare copper at about 0.06 grams).  It is not found free in nature, but is widely distributed in many minerals, usually as phosphates.[9] Inorganic phosphate rock, which is partially made of apatite (a group of minerals being, generally, pentacalcium triorthophosphate fluoride (hydroxide)), is today the chief commercial source of this element. According to the US Geological Survey (USGS), about 50 percent of the global phosphorus reserves are in the Arab nations.[32] Large deposits of apatite are located in China, Russia, Morocco,[33] Florida, Idaho,  Tennessee, Utah, and elsewhere.[34] Albright and Wilson in the UK and their Niagara Falls plant, for instance, were using phosphate rock in the 1890s and 1900s from Tennessee, Florida, and the Îles du Connétable (guano island sources of phosphate); by 1950, they were using phosphate rock mainly from Tennessee and North Africa.[35]Organic sources, namely urine, bone ash and (in the latter 19th century) guano, were historically of importance but had only limited commercial success.[36] As urine contains phosphorus, it has fertilising qualities which are still harnessed today in some countries, including Sweden, using methods for reuse of excreta. To this end, urine can be used as a fertiliser in its pure form or part of being mixed with water in the form of sewage or sewage sludge.The most prevalent compounds of phosphorus are derivatives of phosphate (PO43−), a tetrahedral anion.[37] Phosphate is the conjugate base of phosphoric acid, which is produced on a massive scale for use in fertilisers. Being triprotic, phosphoric acid converts stepwise to three conjugate bases:Phosphate exhibits a tendency to form chains and rings containing P-O-P bonds. Many polyphosphates are known, including ATP. Polyphosphates arise by dehydration of hydrogen phosphates such as HPO42− and H2PO4−. For example, the industrially important pentasodium triphosphate (also known as sodium tripolyphosphate, STPP) is produced industrially on by the megatonne by this condensation reaction:Phosphorus pentoxide (P4O10) is the acid anhydride of phosphoric acid, but several intermediates between the two are known. This waxy white solid reacts vigorously with water.With metal cations, phosphate forms a variety of salts. These solids are polymeric, featuring P-O-M linkages. When the metal cation has a charge of 2+ or 3+, the salts are generally insoluble, hence they exist as common minerals. Many phosphate salts are derived from hydrogen phosphate (HPO42−).PCl5 and PF5 are common compounds. PF5 is a colourless gas and the molecules have trigonal bipyramidal geometry. PCl5 is a colourless solid which has an ionic formulation of PCl4+ PCl6−, but adopts the trigonal bipyramidal geometry when molten or in the vapour phase.[13] PBr5 is an unstable solid formulated as PBr4+Br−and PI5 is not known.[13] The pentachloride and pentafluoride are Lewis acids. With fluoride, PF5 forms PF6−, an anion that is isoelectronic with SF6. The most important oxyhalide is phosphorus oxychloride, (POCl3), which is approximately tetrahedral.Before extensive computer calculations were feasible, it was thought that bonding in phosphorus(V) compounds involved d orbitals. Computer modeling of molecular orbital theory indicates that this bonding involves only s- and p-orbitals.[38]All four symmetrical trihalides are well known: gaseous PF3, the yellowish liquids PCl3 and PBr3, and the solid PI3. These materials are moisture sensitive, hydrolysing to give phosphorous acid. The trichloride, a common reagent, is produced by chlorination of white phosphorus:The trifluoride is produced from the trichloride by halide exchange. PF3 is toxic because it binds to haemoglobin.Phosphorus(III) oxide, P4O6 (also called tetraphosphorus hexoxide) is the anhydride of P(OH)3, the minor tautomer of phosphorous acid. The structure of P4O6 is like that of P4O10 without the terminal oxide groups.These compounds generally feature P-P bonds.[13] Examples include catenated derivatives of phosphine and organophosphines.  Compounds containing P=P double bonds have also been observed, although they are rare.Phosphides arise by reaction of metals with red phosphorus. The alkali metals (group 1) and alkaline earth metals can form ionic compounds containing the phosphide ion, P3−. These compounds react with water to form phosphine. Other phosphides, for example Na3P7, are known for these reactive metals. With the transition metals as well as the monophosphides there are metal-rich phosphides, which are generally hard refractory compounds with a metallic lustre, and phosphorus-rich phosphides which are less stable and include semiconductors.[39] Schreibersite is a naturally occurring metal-rich phosphide found in meteorites. The structures of the metal-rich and phosphorus-rich phosphides  can be complex.Phosphine (PH3) and its organic derivatives (PR3) are structural analogues of ammonia (NH3), but the bond angles at phosphorus are closer to 90° for phosphine and its organic derivatives. It is an ill-smelling, toxic compound. Phosphorus has an oxidation number of -3 in phosphine. Phosphine is produced by hydrolysis of calcium phosphide, Ca3P2. Unlike ammonia, phosphine is oxidised by air. Phosphine is also far less basic than ammonia. Other phosphines are known which contain chains of up to nine phosphorus atoms and have the formula PnHn+2.[13] The highly flammable gas diphosphine (P2H4) is an analogue of hydrazine.Phosphorous oxoacids are extensive, often commercially important, and sometimes structurally complicated. They all have acidic protons bound to oxygen atoms, some have nonacidic protons that are bonded directly to phosphorus and some contain phosphorus - phosphorus bonds.[13] Although many oxoacids of phosphorus are formed, only nine are commercially important, and three of them, hypophosphorous acid, phosphorous acid, and phosphoric acid, are particularly important.The PN molecule is considered unstable, but is a product of crystalline phosphorus nitride decomposition at 1100 K. Similarly, H2PN is considered unstable, and phosphorus nitride halogens like F2PN, Cl2PN, Br2PN, and I2PN oligomerise into cyclic Polyphosphazenes. For example, compounds of the formula (PNCl2)n exist mainly as rings such as the trimer hexachlorophosphazene. The phosphazenes arise by treatment of phosphorus pentachloride with ammonium chloride:PCl5 + NH4Cl → 1/n (NPCl2)n + 4 HClWhen the chloride groups are replaced by alkoxide (RO−), a family of polymers is produced with potentially useful properties.[40]Phosphorus forms a wide range of sulfides, where the phosphorus can be in P(V), P(III) or other oxidation states. The most famous is the three-fold symmetric P4S3 which is used in strike-anywhere matches. P4S10 and P4O10 have analogous structures.[41] Mixed oxyhalides and oxyhydrides of phosphorus(III) are almost unknown.Compounds with P-C and P-O-C bonds are often classified as organophosphorus compounds. They are widely used commercially. The PCl3 serves as a source of P3+ in routes to organophosphorus(III) compounds. For example, it is the precursor to triphenylphosphine:Treatment of phosphorus trihalides with alcohols and phenols gives phosphites, e.g. triphenylphosphite:Similar reactions occur for phosphorus oxychloride, affording triphenylphosphate:The name Phosphorus in Ancient Greece was the name for the planet Venus and is derived from the Greek words (φῶς = light, φέρω = carry), which roughly translates as light-bringer or light carrier.[15] (In Greek mythology and tradition, Augerinus (Αυγερινός = morning star, still in use today), Hesperus or Hesperinus (΄Εσπερος or Εσπερινός or Αποσπερίτης = evening star, still in use today) and Eosphorus (Εωσφόρος = dawnbearer, not in use for the planet after Christianity) are close homologues, and also associated with Phosphorus-the-planet).According to the Oxford English Dictionary, the correct spelling of the element is phosphorus. The word phosphorous is the adjectival form of the P3+ valence: so, just as sulfur forms sulfurous and sulfuric compounds, phosphorus forms phosphorous compounds (e.g., phosphorous acid) and P5+ valence phosphoric compounds (e.g., phosphoric acids and phosphates).The discovery of phosphorus, the first element to be discovered that was not known since ancient times,[42] is credited to the German alchemist Hennig Brand in 1669, although other chemists might have discovered phosphorus around the same time.[43] Brand experimented with urine, which contains considerable quantities of dissolved phosphates from normal metabolism.[15] Working in Hamburg, Brand attempted to create the fabled philosopher's stone through the distillation of some salts by evaporating urine, and in the process produced a white material that glowed in the dark and burned brilliantly. It was named phosphorus mirabilis ("miraculous bearer of light").[44]Brand's process originally involved letting urine stand for days until it gave off a terrible smell. Then he boiled it down to a paste, heated this paste to a high temperature, and led the vapours through water, where he hoped they would condense to gold. Instead, he obtained a white, waxy substance that glowed in the dark. Brand had discovered phosphorus. We now know that Brand produced ammonium sodium hydrogen phosphate, (NH4)NaHPO4. While the quantities were essentially correct (it took about 1,100 litres [290 US gal] of urine to make about 60 g of phosphorus), it was unnecessary to allow the urine to rot first. Later scientists discovered that fresh urine yielded the same amount of phosphorus.[28]Brand at first tried to keep the method secret,[45] but later sold the recipe for 200 thalers to D. Krafft from Dresden,[15] who could now make it as well, and toured much of Europe with it, including England, where he met with Robert Boyle. The secret that it was made from urine leaked out and first Johann Kunckel (1630–1703) in Sweden (1678) and later Boyle in London (1680) also managed to make phosphorus, possibly with the aid of his assistant, Ambrose Godfrey-Hanckwitz, who later made a business of the manufacture of phosphorus.Boyle states that Krafft gave him no information as to the preparation of phosphorus other than that it was derived from "somewhat that belonged to the body of man". This gave Boyle a valuable clue, so that he, too, managed to make phosphorus, and published the method of its manufacture.[15] Later he improved Brand's process by using sand in the reaction (still using urine as base material),Robert Boyle was the first to use phosphorus to ignite sulfur-tipped wooden splints, forerunners of our modern matches, in 1680.[46]Phosphorus was the 13th element to be discovered. For this reason, and due to its use in explosives, poisons and nerve agents, it is sometimes referred to as "the Devil's element".[47]In 1769, Johan Gottlieb Gahn and Carl Wilhelm Scheele showed that calcium phosphate (Ca3(PO4)2) is found in bones, and they obtained elemental phosphorus from bone ash. Antoine Lavoisier recognised phosphorus as an element in 1777.[48] Bone ash was the major source of phosphorus until the 1840s. The method started by roasting bones, then employed the use of clay retorts encased in a very hot brick furnace to distill out the highly toxic elemental phosphorus product.[49] Alternately, precipitated phosphates could be made from ground-up bones that had been de-greased and treated with strong acids. White phosphorus could then be made by heating the precipitated phosphates, mixed with ground coal or charcoal in an iron pot, and distilling off phosphorus vapour in a retort.[50] Carbon monoxide and other flammable gases produced during the reduction process were burnt off in a flare stack.In the 1840s, world phosphate production turned to the mining of tropical island deposits formed from bird and bat guano (see also Guano Islands Act). These became an important source of phosphates for fertiliser in the latter half of the 19th century.[51]Phosphate rock, which usually contains calcium phosphate, was first used in 1850 to make phosphorus, and following the introduction of the electric arc furnace by James Burgess Readman in 1888[52] (patented 1889),[53] elemental phosphorus production switched from the bone-ash heating, to electric arc production from phosphate rock. After the depletion of world guano sources about the same time, mineral phosphates became the major source of phosphate fertiliser production. Phosphate rock production greatly increased after World War II, and remains the primary global source of phosphorus and phosphorus chemicals today. See the article on peak phosphorus for more information on the history and present state of phosphate mining. Phosphate rock remains a feedstock in the fertiliser industry, where it is treated with sulfuric acid to produce various "superphosphate" fertiliser products.White phosphorus was first made commercially in the 19th century for the match industry. This used bone ash for a phosphate source, as described above. The bone-ash process became obsolete when the submerged-arc furnace for phosphorus production was introduced to reduce phosphate rock.[54][55] The electric furnace method allowed production to increase to the point where phosphorus could be used in weapons of war.[26][56] In World War I, it was used in incendiaries, smoke screens and tracer bullets.[56] A special incendiary bullet was developed to shoot at hydrogen-filled Zeppelins over Britain (hydrogen being highly flammable).[56] During World War II, Molotov cocktails made of phosphorus dissolved in petrol were distributed in Britain to specially selected civilians within the British resistance operation, for defence; and phosphorus incendiary bombs were used in war on a large scale. Burning phosphorus is difficult to extinguish and if it splashes onto human skin it has horrific effects.[13]Early matches used white phosphorus in their composition, which was dangerous due to its toxicity. Murders, suicides and accidental poisonings resulted from its use. (An apocryphal tale tells of a woman attempting to murder her husband with white phosphorus in his food, which was detected by the stew's giving off luminous steam).[26] In addition, exposure to the vapours gave match workers a severe necrosis of the bones of the jaw, the infamous "phossy jaw". When a safe process for manufacturing red phosphorus was discovered, with its far lower flammability and toxicity, laws were enacted, under the Berne Convention (1906), requiring its adoption as a safer alternative for match manufacture.[57] The toxicity of white phosphorus led to discontinuation of its use in matches.[58] The Allies used phosphorus incendiary bombs in World War II to destroy Hamburg, the place where the "miraculous bearer of light" was first discovered.[44]Most production of phosphorus-bearing material is for agriculture fertilisers. For this purpose, phosphate minerals are converted to phosphoric acid. It follows two distinct chemical routes, the main one being treatment of phosphate minerals with sulfuric acid. The other process utilises white phosphorus, which may be produced by reaction and distillation from very low grade phosphate sources. The white phosphorus is then oxidised to phosphoric acid and subsequently neutralised with base to give phosphate salts. Phosphoric acid produced from white phosphorus is relatively pure and is the main route for the production of phosphates for all purposes, including detergent production.In the early 1990s, Albright and Wilson's purified wet phosphoric acid business was being adversely affected by phosphate rock sales by China and the entry of their long-standing Moroccan phosphate suppliers into the purified wet phosphoric acid business.[59]In 2017, the USGS estimated 68 billion tons of world reserves, where reserve figures refer to the amount assumed recoverable at current market prices; 0.261 billion tons were mined in 2016.[60] Critical to contemporary agriculture, its annual demand is rising nearly twice as fast as the growth of the human population.[33]The production of phosphorus may have peaked already (as per 2011), leading to the possibility of global shortages by 2040.[61] In 2007, at the rate of consumption, the supply of phosphorus was estimated to run out in 345 years.[62] However, some scientists now believe that a "peak phosphorus" will occur in 30 years and that "At current rates, reserves will be depleted in the next 50 to 100 years."[63] Cofounder of Boston-based investment firm and environmental foundation Jeremy Grantham wrote in Nature in November 2012 that consumption of the element "must be drastically reduced in the next 20-40 years or we will begin to starve."[33][64] According to N.N. Greenwood and A. Earnshaw, authors of the textbook, Chemistry of the Elements, however, phosphorus comprises about 0.1% by mass of the average rock, and consequently the Earth's supply is vast, although dilute.[13]Presently, about 1,000,000 short tons (910,000 t) of elemental phosphorus is produced annually. Calcium phosphate (phosphate rock), mostly mined in Florida and North Africa, can be heated to 1,200–1,500 °C with sand, which is mostly SiO2, and coke (refined coal) to produce vaporised P4. The product is subsequently condensed into a white powder under water to prevent oxidation by air. Even under water, white phosphorus is slowly converted to the more stable red phosphorus allotrope. The chemical equation for this process when starting with fluoroapatite, a common phosphate mineral, is:Side products from this process include ferrophosphorus, a crude form of Fe2P, resulting from iron impurities in the mineral precursors. The silicate slag is a useful construction material. The fluoride is sometimes recovered for use in water fluoridation. More problematic is a "mud" containing significant amounts of white phosphorus. Production of white phosphorus is conducted in large facilities in part because it is energy intensive. The white phosphorus is transported in molten form. Some major accidents have occurred during transportation; train derailments at Brownston, Nebraska and Miamisburg, Ohio led to large fires. The worst incident in recent times was an environmental contamination in 1968 when the sea was polluted from spillage and/or inadequately treated sewage from a white phosphorus plant at Placentia Bay, Newfoundland.[65]Another process by which elemental phosphorus is extracted includes applying at high temperatures (1500 °C):[66]Historically, before the development of mineral-based extractions, white phosphorus was isolated on an industrial scale from bone ash.[67] In this process, the tricalcium phosphate in bone ash is converted to monocalcium phosphate with sulfuric acid:Monocalcium phosphate is then dehydrated to the corresponding metaphosphate:When ignited to a white heat with charcoal, calcium metaphosphate yields two-thirds of its weight of white phosphorus while one-third of the phosphorus remains in the residue as calcium orthophosphate:Phosphorus is an essential plant nutrient (often the limiting nutrient), and the bulk of all phosphorus production is in concentrated phosphoric acids for agriculture fertilisers, containing as much as 70% to 75% P2O5. Its annual demand is rising nearly twice as fast as the growth of the human population. That led to large increase in phosphate (PO43−) production in the second half of the 20th century.[33] Artificial phosphate fertilisation is necessary because phosphorus is essential to all living organisms; natural phosphorus-bearing compounds are mostly insoluble and inaccessible to plants, and the natural cycle of phosphorus is very slow.  Fertiliser is often in the form of superphosphate of lime, a mixture of calcium dihydrogen phosphate (Ca(H2PO4)2), and calcium sulfate dihydrate (CaSO4·2H2O) produced reacting sulfuric acid and water with calcium phosphate.Processing phosphate minerals with sulfuric acid for obtaining fertiliser is so important to the global economy that this is the primary industrial market for sulfuric acid and the greatest industrial use of elemental sulfur.[68]White phosphorus is widely used to make organophosphorus compounds through intermediate phosphorus chlorides and two phosphorus sulfides, phosphorus pentasulfide and phosphorus sesquisulfide.[69] Organophosphorus compounds have many applications, including in plasticisers, flame retardants, pesticides, extraction agents, nerve agents and water treatment.[13][70]Phosphorus is also an important component in steel production, in the making of phosphor bronze, and in many other related products.[71][72] Phosphorus is added to metallic copper during its smelting process to react with oxygen present as an impurity in copper and to produce phosphorus-containing copper (CuOFP) alloys with a higher hydrogen embrittlement resistance than normal copper.[73]The first striking match with a phosphorus head was invented by Charles Sauria in 1830. These matches (and subsequent modifications) were made with heads of white phosphorus, an oxygen-releasing compound (potassium chlorate, lead dioxide, or sometimes nitrate), and a binder. They were poisonous to the workers in manufacture,[74] sensitive to storage conditions, toxic if ingested, and hazardous when accidentally ignited on a rough surface.[75][76]  Production in several countries was banned between 1872 and 1925.[77] The international Berne Convention, ratified in 1906, prohibited the use of white phosphorus in matches.In consequence, the 'strike-anywhere' matches were gradually replaced by 'safety matches', wherein the white phosphorus was replaced by phosphorus sesquisulfide (P4S3), sulfur, or antimony sulfide. Such matches are difficult to ignite on any surface other than a special strip. The strip contains red phosphorus that heats up upon striking, reacts with the oxygen-releasing compound in the head, and ignites the flammable material of the head.[16][69]Sodium tripolyphosphate made from phosphoric acid is used in laundry detergents in some countries, but banned for this use in others.[18] This compound softens the water to enhance the performance of the detergents and to prevent pipe/boiler tube corrosion.[78]Inorganic phosphorus in the form of the phosphate PO3−4 is required for all known forms of life.[82] Phosphorus plays a major role in the structural framework of DNA and RNA. Living cells use phosphate to transport cellular energy with adenosine triphosphate (ATP), necessary for every cellular process that uses energy.  ATP is also important for phosphorylation, a key regulatory event in cells.  Phospholipids are the main structural components of all cellular membranes. Calcium phosphate salts assist in stiffening bones.[13]  Biochemists commonly use the abbreviation "Pi" to refer to inorganic phosphate.[83]Every living cell is encased in a membrane that separates it from its surroundings. Cellular membranes are composed of a phospholipid matrix and proteins, typically in the form of a bilayer. Phospholipids are derived from glycerol with two of the glycerol hydroxyl (OH) protons replaced by fatty acids as an ester, and the third hydroxyl proton has been replaced with phosphate bonded to another alcohol.[84]An average adult human contains about 0.7 kg of phosphorus, about 85–90% in bones and teeth in the form of apatite, and the remainder in soft tissues and extracellular fluids (~1%). The phosphorus content increases from about 0.5 weight% in infancy to 0.65–1.1 weight% in adults. Average phosphorus concentration in the blood is about 0.4 g/L, about 70% of that is organic and 30% inorganic phosphates.[85] An adult with healthy diet consumes and excretes about 1–3 grams of phosphorus per day, with consumption in the form of inorganic phosphate and phosphorus-containing biomolecules such as nucleic acids and phospholipids; and excretion almost exclusively in the form of phosphate ions such as H2PO−4 and HPO2−4. Only about 0.1% of body phosphate circulates in the blood, paralleling the amount of phosphate available to soft tissue cells.The main component of bone is hydroxyapatite as well as amorphous forms of calcium phosphate, possibly including carbonate. Hydroxyapatite is the main component of tooth enamel. Water fluoridation enhances the resistance of teeth to decay by the partial conversion of this mineral to the still harder material called fluoroapatite:[13]In medicine, phosphate deficiency syndrome may be caused by malnutrition, by failure to absorb phosphate, and by metabolic syndromes that draw phosphate from the blood (such as in refeeding syndrome after malnutrition[86]) or pass too much of it into the urine. All are characterised by hypophosphatemia, which is a condition of low levels of soluble phosphate levels in the blood serum and inside the cells. Symptoms of hypophosphatemia include neurological dysfunction and disruption of muscle and blood cells due to lack of ATP. Too much phosphate can lead to diarrhoea and calcification (hardening) of organs and soft tissue, and can interfere with the body's ability to use iron, calcium, magnesium, and zinc.[87]Phosphorus is an essential macromineral for plants, which is studied extensively in edaphology to understand plant uptake from soil systems.  Phosphorus is a limiting factor in many ecosystems; that is, the scarcity of phosphorus limits the rate of organism growth.  An excess of phosphorus can also be problematic, especially in aquatic systems where eutrophication sometimes leads to algal blooms.[33]The U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for phosphorus in 1997. If there is not sufficient information to establish EARs and RDAs, an estimate designated Adequate Intake (AI) is used instead. The current EAR for phosphorus for people ages 19 and up is 580 mg/day. The RDA is 700 mg/day. RDAs are higher than EARs so as to identify amounts that will cover people with higher than average requirements. RDA for pregnancy and lactation are also 700 mg/day. For children ages 1–18 years the RDA increases with age from 460 to 1250 mg/day. As for safety, the IOM sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of phosphorus the UL is 4000 mg/day. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes (DRIs).[88]The European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For people ages 15 and older, including pregnancy and lactation, the AI is set at 550 mg/day. For children ages 4–10 years the AI is 440 mg/day, for ages 11–17 640 mg/day. These AIs are lower than the U.S RDAs. In both systems, teenagers need more than adults.[89] The European Food Safety Authority reviewed the same safety question and decided that there was not sufficient information to set a UL.[90]For U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For phosphorus labeling purposes 100% of the Daily Value was 1000 mg, but as of May 27, 2016 it was revised to 1250 mg to bring it into agreement with the RDA.[91] A table of the old and new adult Daily Values is provided at Reference Daily Intake. The original deadline to be in compliance was July 28, 2018, but on September 29, 2017 the FDA released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies.[92]The main food sources for phosphorus are the same as those containing protein, although proteins do not contain phosphorus.  For example, milk, meat, and soya typically also have phosphorus. As a rule, if a diet has sufficient protein and calcium, the amount of phosphorus is probably sufficient.[93]Organic compounds of phosphorus form a wide class of materials; many are required for life, but some are extremely toxic. Fluorophosphate esters are among the most potent neurotoxins known. A wide range of organophosphorus compounds are used for their toxicity as pesticides (herbicides, insecticides, fungicides, etc.) and weaponised as nerve agents against enemy humans. Most inorganic phosphates are relatively nontoxic and essential nutrients.[13]The white phosphorus allotrope presents a significant hazard because it ignites in air and produces phosphoric acid residue. Chronic white phosphorus poisoning leads to necrosis of the jaw called "phossy jaw". White phosphorus is toxic, causing severe liver damage on ingestion and may cause a condition known as "Smoking Stool Syndrome".[94]In the past, external exposure to elemental phosphorus was treated by washing the affected area with 2% copper sulfate solution to form harmless compounds that are then washed away. According to the recent US Navy's Treatment of Chemical Agent Casualties and Conventional Military Chemical Injuries: FM8-285: Part 2 Conventional Military Chemical Injuries, "Cupric (copper(II)) sulfate has been used by U.S. personnel in the past and is still being used by some nations. However, copper sulfate is toxic and its use will be discontinued. Copper sulfate may produce kidney and cerebral toxicity as well as intravascular hemolysis."[95]The manual suggests instead "a bicarbonate solution to neutralise phosphoric acid, which will then allow removal of visible white phosphorus. Particles often can be located by their emission of smoke when air strikes them, or by their phosphorescence in the dark. In dark surroundings, fragments are seen as luminescent spots. Promptly debride the burn if the patient's condition will permit removal of bits of WP (white phosphorus) that might be absorbed later and possibly produce systemic poisoning. DO NOT apply oily-based ointments until it is certain that all WP has been removed. Following complete removal of the particles, treat the lesions as thermal burns."[note 1][citation needed] As white phosphorus readily mixes with oils, any oily substances or ointments are not recommended until the area is thoroughly cleaned and all white phosphorus removed.People can be exposed to phosphorus in the workplace by inhalation, ingestion, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) has set the phosphorus exposure limit (Permissible exposure limit) in the workplace at 0.1 mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 0.1 mg/m3 over an 8-hour workday. At levels of 5 mg/m3, phosphorus is immediately dangerous to life and health.[96]Phosphorus can reduce elemental iodine to hydroiodic acid, which is a reagent effective for reducing ephedrine or pseudoephedrine to methamphetamine.[97] For this reason, red and white phosphorus were designated by the United States Drug Enforcement Administration as List I precursor chemicals under 21 CFR 1310.02 effective on November 17, 2001.[98] In the United States, handlers of red or white phosphorus are subject to stringent regulatory controls.[98][99][100]
Bark isolate
Bark isolates are chemicals which have been extracted from bark, which include the medicines salicylic acid (active metabolite of aspirin) and paclitaxel (Taxol). The pharmacology of bark isolates is an ongoing topic of medical research.
Tulare County, California
Tulare County (/tʊˈlɛəri/ tuu-LAIR-ee) is a county in the U.S. state of California. As of the 2010 census, the population was 442,179.[4] Its county seat is Visalia.[6] The county is named for Tulare Lake, once the largest freshwater lake west of the Great Lakes. Drained for agricultural development, the site is now in Kings County, which was created in 1893 from the western portion of the formerly larger Tulare County.Tulare County comprises the Visalia-Porterville, CA Metropolitan Statistical Area. The county is located south of Fresno, spanning from the San Joaquin Valley east to the Sierra Nevada.Sequoia National Park is located in the county, as are part of Kings Canyon National Park, in its northeast corner (shared with Fresno County), and part of Mount Whitney, on its eastern border (shared with Inyo County).  As of the 2010 census, the population was 442,179, up from 368,021 at the 2000 census.The land was occupied for thousands of years by varying cultures of indigenous peoples.  Beginning in the eighteenth century, Spain established missions to colonize California and convert the American Indians to Christianity. Comandante Pedro Fages, while hunting for deserters in the Central Valley in 1772, discovered a great lake surrounded by marshes and filled with rushes; he named it Los Tules (the tules). It is from this lake that the county derives its name. The root of the name Tulare is found in the Nahuatl word tullin, designating cattail or similar reeds.After Mexico achieved independence, it continued to rule California. After the Mexican Cession and the Treaty of Guadalupe Hidalgo in 1848, the area became part of the United States. Tulare County was soon formed from parts of Mariposa County only 4 years later in 1852.  There were two early attempts to split off a new Buena Vista County in 1855, and Coso County in 1864, but both failed.  Parts of the county's territory were given to Fresno County in 1856, to Kern County and to Inyo County in 1866 and to Kings County in 1893.The infectious disease Tularemia caused by the bacterium Francisella tularensis is named after Tulare County.In 1908 Colonel Allen Allensworth and associates founded Allensworth as a black farming community. They intended to develop a place where African Americans could thrive free of white discrimination. It was the only community in California founded, financed and governed by African Americans.  While its first years were highly successful, the community encountered environmental problems from dropping water tables which eventually caused it to fail. Today the historic area is preserved as the Colonel Allensworth State Historic Park, which is listed on the National Register of Historic Places.According to the U.S. Census Bureau, the county has a total area of 4,839 square miles (12,530 km2), of which 4,824 square miles (12,490 km2) is land and 14 square miles (36 km2) (0.3%) is water.[7]Sequoia National Park is a national park in the southern Sierra Nevada, east of Visalia. It was established in 1890 as the second U.S. national park, after Yellowstone. The park spans 404,051 acres (1,635.14 km2). Encompassing a vertical relief of nearly 13,000 feet (3,962 m), the park contains among its natural resources the highest point in the contiguous 48 United States, Mount Whitney, at 14,505 feet (4,421 m) above sea level. The park is south of and contiguous with Kings Canyon National Park; the two are administered by the National Park Service as one unit, called Sequoia and Kings Canyon National Parks.Tulare County is a general law county under the California Constitution. That is, it does not have a county charter. The county is governed by a five-member Board of Supervisors. Supervisors are elected by districts for four-year terms. There are no term limits in effect. The Chairman and Vice-Chairman are elected annually by the Board of Supervisors from among its members.The Tulare County Sheriff provides court protection, county jail operation, patrol and detective functions in the unincorporated areas of the county. Incorporated towns have municipal police departments or contract with the Sheriff for their police operations.Tulare County Transit provides a countywide bus service linking the population centers. A connection to Delano in Kern County is also operated.The cities of Tulare, Porterville, and Visalia have their own local bus services.Greyhound and Orange Belt Stages provide long-distance, intercity bus service.The Porterville Municipal Airport located  3 nautical miles from Downtown Porterville has very limited commercial passenger service with WestAir. The airport offers general aviation to the public, it is also home to Porterville Air Attack Base on the south part of the airport. The Visalia Municipal Airport is a city-owned airport for the city of Visalia, California. Mefford Field is a city-owned general aviation airport located in Tulare.The nearest full operation commercial airports are Bakersfield's Meadows Field Airport to the South, and Fresno's Fresno Yosemite International Airport to the North.The following table includes the number of incidents reported and the rate per 1,000 persons for each type of offense.The 2010 United States Census reported that Tulare County had a population of 442,179. The racial makeup of Tulare County was 265,618 (60.1%) White, 7,196 (1.6%) African American, 6,993 (1.6%) Native American, 15,176 (3.4%) Asian, 509 (0.1%) Pacific Islander, 128,263 (29.0%) from other races, and 18,424 (4.2%) from two or more races.  Hispanic or Latino of any race were 268,065 persons (60.6%).[18]As of the census[23] of 2000, there were 368,021 people, 110,385 households, and 87,093 families residing in the county.  The population density was 76 people per square mile (29/km²).  There were 119,639 housing units at an average density of 25 per square mile (10/km²).  The racial makeup of the county was 58.1% White, 1.6% Black or African American, 1.6% Native American, 3.3% Asian, 0.1% Pacific Islander, 30.8% from other races, and 4.6% from two or more races.  50.8% of the population were Hispanic or Latino of any race. 6.2% were of American, 5.7% German and 5.0% English ancestry according to Census 2000. 56.3% spoke English, 38.9% Spanish and 1.1% Portuguese as their first language.There were 110,385 households out of which 44.9% had children under the age of 18 living with them, 58.1% were married couples living together, 14.5% had a female householder with no husband present, and 21.1% were non-families. 17.1% of all households were made up of individuals and 7.7% had someone living alone who was 65 years of age or older.  The average household size was 3.28 and the average family size was 3.67.In the county, the population was spread out with 33.8% under the age of 18, 10.6% from 18 to 24, 27.6% from 25 to 44, 18.2% from 45 to 64, and 9.8% who were 65 years of age or older.  The median age was 29 years. For every 100 females there were 100.0 males.  For every 100 females age 18 and over, there were 97.7 males.The median income for a household in the county was $33,983, and the median income for a family was $36,297. Males had a median income of $30,892 versus $24,589 for females. The per capita income for the county was $14,006.  About 18.8% of families and 23.9% of the population were below the poverty line, including 32.6% of those under age 18 and 10.5% of those age 65 or over.(reported by the sheriff's office or county police)[24]The United States Office of Management and Budget has designated Tulare County as the Visalia-Porterville, CA Metropolitan Statistical Area.[25]  The United States Census Bureau ranked the Visalia-Porterville, CA Metropolitan Statistical Area as the 111th most populous metropolitan statistical area of the United States as of July 1, 2012.[26]The Office of Management and Budget has further designated the Visalia-Porterville, CA Metropolitan Statistical Area as a component of the more extensive Visalia-Porterville-Hanford, CA Combined Statistical Area,[25] the 80th most populous combined statistical area and the 92nd most populous primary statistical area of the United States as of July 1, 2012.[26][27]Tulare is a strongly Republican county in Presidential and congressional elections. The last Democratic candidate for President to win a majority in the county was Lyndon Johnson in 1964.In the United States House of Representatives, Tulare County is split between 3 congressional districts:[30]In the California State Senate, it is split between 3 legislative districts:[32]In the California State Assembly, the county is split between the  23rd Assembly District, represented by Republican Jim Patterson, and the  26th Assembly District, represented by Republican Devon Mathis.[33]The dairy industry, with sales of milk products, brings in the most revenue for the county, typically more than US$ 1 billion a year annually.  Oranges, grapes, and cattle-related commodities also earn hundreds of millions of dollars annually.In 2001, Tulare became the most productive county in the U.S. in terms of agricultural revenues, at US$3.5 billion annually.  It surpassed Fresno County's US$3.2 billion, which had held the top spot for over two decades.  Due to the importance of agriculture in the county as well as its location in the state, since 1968 the city of Tulare has been the site of the annual World Ag Expo,[34] the world's largest agricultural exposition.Minor league sports teams, such as the baseball Visalia Rawhide of the class-A level California League (an affiliate to the Arizona Diamondbacks), two teams of the Minor League Football Association in Tulare and Visalia, and four teams of the Central California Basketball League based in Porterville, attract many residents and add to the amenities in the county.[citation needed]According to the County's 2010 Comprehensive Annual Financial Report,[35] the top employers in the county are:The population ranking of the following table is based on the 2010 census of Tulare County.[36]† county seat
Symbiosis
Symbiosis (from Greek συμβίωσις "living together", from σύν "together" and βίωσις "living")[2] is any type of a close and long-term biological interaction between two different biological organisms, be it mutualistic, commensalistic, or parasitic. The organisms, each termed a symbiont, may be of the same or of different species.  In 1879, Heinrich Anton de Bary defined it as "the living together of unlike organisms". The term was subject to a century-long debate about whether it should specifically denote mutualism, as in lichens; biologists have now abandoned that restriction.Symbiosis can be obligatory, which means that one or both of the symbionts entirely depend on each other for survival, or facultative (optional) when they can generally live independently.Symbiosis is also classified by physical attachment; symbiosis in which the organisms have bodily union is called conjunctive symbiosis, and symbiosis in which they are not in union is called disjunctive symbiosis.[3]When one organism lives on the surface of another, such as head lice on humans, it is called ectosymbiosis; when one partner lives inside the tissues of another, such as Symbiodinium within coral, it is termed endosymbiosis.[4][5]The definition of symbiosis was a matter of debate for 130 years.[6] In 1877, Albert Bernhard Frank used the term symbiosis to describe the mutualistic relationship in lichens.[7] In 1879, the German mycologist Heinrich Anton de Bary defined it as "the living together of unlike organisms".[8][9] The definition has varied among scientists with some advocating that it should only refer to persistent mutualisms, while others thought it should apply to all persistent biological interactions, in other words mutualisms, commensalism, or parasitism, but excluding brief interactions such as predation.[10] Current biology and ecology textbooks use the latter "de Bary" definition, or an even broader one where symbiosis means all interspecific interactions; the restrictive definition where symbiosis means only mutualism is no longer used.[11]In 1949, Edward Haskell (1949) proposed an integrative approach, proposing a classification of "co-actions",[12] later adopted by biologists as "interactions".[13][14][15]Biological interactions can involve individuals of the same species (intraspecific interactions) or individuals of different species (interspecific interactions). These can be further classified by either the mechanism of the interaction or the strength, duration and direction of their effects.[16]Relationships can be obligate, meaning that one or both of the symbionts entirely depend on each other for survival. For example, in lichens, which consist of fungal and photosynthetic symbionts, the fungal partners cannot live on their own.[8][17][18][19] The algal or cyanobacterial symbionts in lichens, such as Trentepohlia, can generally live independently, and their symbiosis is, therefore, facultative (optional).[20]Endosymbiosis is any symbiotic relationship in which one symbiont lives within the tissues of the other, either within the cells or extracellularly.[5][21] Examples include diverse microbiomes, rhizobia, nitrogen-fixing bacteria that live in root nodules on legume roots; actinomycete nitrogen-fixing bacteria called Frankia, which live in alder root nodules; single-celled algae inside reef-building corals; and bacterial endosymbionts that provide essential nutrients to about 10%–15% of insects.[citation needed]Ectosymbiosis is any symbiotic relationship in which the symbiont lives on the body surface of the host, including the inner surface of the digestive tract or the ducts of exocrine glands.[5][22]  Examples of this include ectoparasites such as lice, commensal ectosymbionts such as the barnacles which attach themselves to the jaw of baleen whales, and mutualist ectosymbionts such as cleaner fish.Competition can be defined as an interaction between organisms or species, in which the fitness of one is lowered by the presence of another. Limited supply of at least one resource (such as food, water, and territory) used by both usually facilitates this type of interaction, although the competition may also exist over other 'amenities', such as females for reproduction (in case of male organisms of the same species).[23]Mutualism or interspecies reciprocal altruism is a long-term relationship between individuals of different species where both individuals benefit.[24] Mutualistic relationships may be either obligate for both species, obligate for one but facultative for the other, or facultative for both.A large percentage of herbivores have mutualistic gut flora to help them digest plant matter, which is more difficult to digest than animal prey.[4]  This gut flora is made up of cellulose-digesting protozoans or bacteria living in the herbivores' intestines.[25]  Coral reefs are the result of mutualisms between coral organisms and various types of algae which live inside them.[26]  Most land plants and land ecosystems rely on mutualisms between the plants, which fix carbon from the air, and mycorrhyzal fungi, which help in extracting water and minerals from the ground.[27]An example of mutualism is the relationship between the ocellaris clownfish that dwell among the tentacles of Ritteri sea anemones. The territorial fish protects the anemone from anemone-eating fish, and in turn the stinging tentacles of the anemone protect the clownfish from its predators. A special mucus on the clownfish protects it from the stinging tentacles.[28]A further example is the goby, a fish which sometimes lives together with a shrimp. The shrimp digs and cleans up a burrow in the sand in which both the shrimp and the goby fish live. The shrimp is almost blind, leaving it vulnerable to predators when outside its burrow. In case of danger the goby touches the shrimp with its tail to warn it.  When that happens both the shrimp and goby quickly retreat into the burrow.[29] Different species of gobies (Elacatinus spp.) also clean up ectoparasites in other fish, possibly another kind of mutualism.[30]A non-obligate symbiosis is seen in encrusting bryozoans and hermit crabs. The bryozoan colony (Acanthodesia commensale) develops a cirumrotatory growth and offers the crab (Pseudopagurus granulimanus) a helicospiral-tubular extension of its living chamber that initially was situated within a gastropod shell.[31]Many types of tropical and sub-tropical ants that have evolved very complex relationships with certain tree species.[32]In endosymbiosis, the host cell lacks some of the nutrients which the endosymbiont provides. As a result, the host favors endosymbiont's growth processes within itself by producing some specialized cells. These cells affect the genetic composition of the host in order to regulate the increasing population of the endosymbionts and ensure that these genetic changes are passed onto the offspring via vertical transmission (heredity).[33]A spectacular example of obligate mutualism is the relationship between the siboglinid tube worms and symbiotic bacteria that live at hydrothermal vents and cold seeps. The worm has no digestive tract and is wholly reliant on its internal symbionts for nutrition. The bacteria oxidize either hydrogen sulfide or methane, which the host supplies to them. These worms were discovered in the late 1980s at the hydrothermal vents near the Galapagos Islands and have since been found at deep-sea hydrothermal vents and cold seeps in all of the world's oceans.[34]As the endosymbiont adapts to the host's lifestyle the endosymbiont changes dramatically. There is a drastic reduction in its genome size, as many genes are lost during the process of metabolism, and DNA repair and recombination, while important genes participating in the DNA to RNA transcription, protein translation and DNA/RNA replication are retained. The decrease in genome size is due to loss of protein coding genes and not due to lessening of inter-genic regions or open reading frame (ORF) size. Species that are naturally evolving and contain reduced sizes of genes can be accounted for an increased number of noticeable differences between them, thereby leading to changes in their evolutionary rates. When endosymbiotic bacteria related with insects are passed on to the offspring strictly via vertical genetic transmission, intracellular bacteria go across many hurdles during the process, resulting in the decrease in effective population sizes, as compared to the free living bacteria. The incapability of the endosymbiotic bacteria to reinstate their wild type phenotype via a recombination process is called Muller's ratchet phenomenon. Muller's ratchet phenomenon together with less effective population sizes leads to an accretion of deleterious mutations in the non-essential genes of the intracellular bacteria.[35] This can be due to lack of selection mechanisms prevailing in the relatively "rich" host environment.[36][37]Commensalism describes a relationship between two living organisms where one benefits and the other is not significantly harmed or helped.  It is derived from the English word commensal, used of human social interaction. It derives from a medieval Latin word meaning sharing food, formed from com- (with) and mensa (table).[24][38]Commensal relationships may involve one organism using another for transportation (phoresy) or for housing (inquilinism), or it may also involve one organism using something another created, after its death (metabiosis). Examples of metabiosis are hermit crabs using gastropod shells to protect their bodies, and spiders building their webs on plants.In a parasitic relationship, the parasite benefits while the host is harmed.[39] Parasitism takes many forms, from endoparasites that live within the host's body to ectoparasites and parasitic castrators that live on its surface and micropredators like mosquitoes that visit intermittently. Parasitism is an extremely successful mode of life; as many as half of all animals have at least one parasitic phase in their life cycles, and it is also frequent in plants and fungi. Moreover, almost all free-living animal species are hosts to parasites, often of more than one species.[citation needed]Mimicry is a form of symbiosis in which a species adopts distinct characteristics of another species to alter its relationship dynamic with the species being mimicked, to its own advantage. Among the many types of mimicry are Batesian and Müllerian, the first involving one-sided exploitation, the second providing mutual benefit. Batesian mimicry is an exploitative three-party interaction where one species, the mimic, has evolved to mimic another, the model, to deceive a third, the dupe. In terms of signalling theory, the mimic and model have evolved to send a signal; the dupe has evolved to receive it from the model. This is to the advantage of the mimic but to the detriment of both the model, whose protective signals are effectively weakened, and of the dupe, which is deprived of an edible prey. For example, a wasp is a strongly-defended model, which signals with its conspicuous black and yellow coloration that it is an unprofitable prey to predators such as birds which hunt by sight; many hoverflies are Batesian mimics of wasps, and any bird that avoids these hoverflies is a dupe.[40][41] In contrast, Müllerian mimicry is mutually beneficial as all participants are both models and mimics.[42][43] For example, different species of bumblebee mimic each other, with similar warning coloration in combinations of black, white, red, and yellow, and all of them benefit from the relationship.[44]Amensalism is an asymmetric interaction where one species is harmed or killed by the other, and one is unaffected by the other.[45][46] There are two types of amensalism, competition and antagonism (or antibiosis). Competition is where a larger or stronger organism deprives a smaller or weaker one from a resource. Antagonism occurs when one organism is damaged or killed by another through a chemical secretion. An example of competition is a sapling growing under the shadow of a mature tree. The mature tree can rob the sapling of necessary sunlight and, if the mature tree is very large, it can take up rainwater and deplete soil nutrients. Throughout the process, the mature tree is unaffected by the sapling. Indeed, if the sapling dies, the mature tree gains nutrients from the decaying sapling. An example of antagonism is Juglans nigra (black walnut), secreting juglone, a substance which destroys many herbaceous plants within its root zone.[47]A clear case of amensalism is where sheep or cattle trample grass. Whilst the presence of the grass causes negligible detrimental effects to the animal's hoof, the grass suffers from being crushed.[citation needed] Amensalism is often used to describe strongly asymmetrical competitive interactions, such as has been observed between the Spanish ibex and weevils of the genus Timarcha which feed upon the same type of shrub. Whilst the presence of the weevil has almost no influence on food availability, the presence of ibex has an enormous detrimental effect on weevil numbers, as they consume significant quantities of plant matter and incidentally ingest the weevils upon it.[48]Cleaning symbiosis is an association between individuals of two species, where one (the cleaner) removes and eats parasites and other materials from the surface of the other (the client).[49] It is putatively mutually beneficial, but biologists have long debated whether it is mutual selfishness, or simply exploitative.  Cleaning symbiosis is well-known among marine fish, where some small species of cleaner fish, notably wrasses but also species in other genera, are specialised to feed almost exclusively by cleaning larger fish and other marine animals.[50]Symbiosis is increasingly recognized as an important selective force behind evolution;[4][51] many species have a long history of interdependent co-evolution.[52] Eukaryotes (plants, animals, fungi, and protists) developed by symbiogenesis from a symbiosis between bacteria and archaea.[4][53][54] Evidence for this includes the fact that mitochondria and chloroplasts divide independently of the cell, and the observation that some organelles seem to have their own genome.[55]The biologist Lynn Margulis, famous for her work on endosymbiosis, contended that symbiosis is a major driving force behind evolution. She considered Darwin's notion of evolution, driven by competition, to be incomplete and claimed that evolution is strongly based on co-operation, interaction, and mutual dependence among organisms. According to Margulis and her son Dorion Sagan, "Life did not take over the globe by combat, but by networking."[56]About 80% of vascular plants worldwide form symbiotic relationships with fungi, in particular in arbuscular mycorrhizas.[57]Flowering plants and the animals that pollinate them have co-evolved. Many plants that are pollinated by insects (in entomophily), bats, or birds (in ornithophily) have highly specialized flowers modified to promote pollination by a specific pollinator that is correspondingly adapted.  The first flowering plants in the fossil record had relatively simple flowers. Adaptive speciation quickly gave rise to many diverse groups of plants, and, at the same time, corresponding speciation occurred in certain insect groups. Some groups of plants developed nectar and large sticky pollen, while insects evolved more specialized morphologies to access and collect these rich food sources. In some taxa of plants and insects the relationship has become dependent,[58] where the plant species can only be pollinated by one species of insect.[59]The acacia ant (Pseudomyrmex ferruginea) is an obligate plant ant that protects at least five species of "Acacia" (Vachellia)[a] from preying insects and from other plants competing for sunlight, and the tree provides nourishment and shelter for the ant and its larvae.[60][61]
Hòn Non Bộ
Hòn Non Bộ (Chữ Nôm: 𡉕𡽫部) is the Vietnamese art of making miniature landscapes, imitating the scenery of the islands, mountains and surrounding environment as found in nature. It is a particular local development of the Chinese art of penjing, as was bonsai in Japan.The phrase Hòn Non Bộ comes from the Vietnamese language:Hòn 𡉕 means Island, Non 𡽫 means Mountain, and Bộ 部 means a combination of water, mountain range and forest, or it can also mean "imitating the way the scenery looks in miniature".Hòn Non Bộ may be quite large and elaborate or small and simple. It was used to grace the courtyard entrance of the traditional Vietnamese home. Throughout Vietnam history, Hòn Non Bộ have been built for emperors, generals, and other important people as monuments, decorations, personal vistas, and as cultural icons.An example of Hòn Non Bộ scenery is on display at the Balboa Park, San Diego, California, US.[1]Miniature landscape art was first recorded after Vietnamese independence in the year 939.  A version of this was the Hòn Non Bộ (lit., "island-mountain-panorama"), which is designed to be seen from all sides. People, even the poorest, placed rocks and plants surrounded by water in containers or basins originally carved from stone. (Later these were formed from stucco, and then from concrete.) Individual Hòn Non Bộ could be a foot or two in height. Sometimes these were also known as tiểu cảnh, the art of mini-scenes, where the tree is the main subject and it is larger than the mountains portrayed.  Members of royalty built larger versions up to 20 or 25 feet high (with mountains always larger than the backdrop trees).  Almost always one or more of these landscapes were included in the grounds of their palaces and temples to form a part of the sacred enclosure.  At some point, these were often accompanied by parallel verses in Chinese, stereotyped quotations that everyone knew thanks to popular collections of expressions for use on various occasions. Incense sticks and some miniature figurines might also be a part of their construction. This was done even after Ngo Quyen's death ushered in period of civil strife.[2][3][4][5]Temples were built with Hòn Non Bộ in order to commemorate the deeds of the kings who ruled between the years equivalent to 968 and 1005.[4][6]From 1225 to 1400, the Trần Dynasty ruled Vietnam and repelled the invading Mongol forces of the Yuan Dynasty in 1258, 1285, and 1288. Most of the magnificent palaces were destroyed in the process.  These were subsequently rebuilt, complete with Hòn Non Bộ, using the labor of enemy collaborators.[7]In 1406, the Chinese Ming dynasty king ordered his army to invade Vietnam and confiscate all things related to that culture, such as books and art objects, and bring them back to China. The following year, the interim Vietnamese ruler was caught by invaders, carnage followed, and all works of art and architecture were destroyed—including Hòn Non Bô. Later, the Lê Dynasty (1428-1788) rebuilt many of the devastated palaces and Hòn Non Bộ were very popular features in the renovations. Mini-scenes and miniature landscapes made during this period used Cycas revoluta (sago palms) on the birthdays of kings, lords, and elderly high-class people.[8] The scholar Nguyễn Bỉnh Khiêm (aka Trang Trình, 1491–1585) was said to have used a Hòn Non Bộ to provide guidance while predicting the fate or destiny of others.[9]Hòn Non Bộ, as well as miniature plants and rocks, are mentioned in Đoạn Truòng Tân Thanh, a thousand-page book by Nguyễn Du (1766–1820).[10] During the Nguyễn Dynasty (1802–1945), the art of miniature plants without much additional landscaping, cây kiểng, flourished. (It was called cây cảnh in the north.) Kings enjoyed planting pines and junipers; mandarins loved growing Thuja orientalis and Casuarina; intellectuals or other notable figures liked Ficus; and lay people devoted themselves to planting mallow (Malva), Tamarindus indica, and Melaleuca leucadendra. Except for those planted by kings, all trees planted for pleasure by mandarins or lay people had to have their tops bent downward because it was considered impertinent to superiors to have treetops growing upward.[11]
List of vegetable oils
Vegetable oils are triglycerides extracted from plants. These oils have been part of human culture for millennia.[1] Edible vegetable oils are used in food, both in cooking and as supplements. Many oils, edible and otherwise, are burned as fuel, such as in oil lamps and as a substitute for petroleum-based fuels. Some of the many other uses include wood finishing, oil painting, and skin care.There are several types of plant oils, distinguished by the method used to extract the oil from the plant. The relevant part of the plant may be placed under pressure to extract the oil, giving an expressed (or pressed) oil. The oils included in this list are of this type. Oils may also be extracted from plants by dissolving parts of plants in water or another solvent. The solution may be separated from the plant material and concentrated, giving an extracted or leached oil. The mixture may also be separated by distilling the oil away from the plant material. Oils extracted by this latter method are called essential oils. Essential oils often have different properties and uses than pressed or leached vegetable oils. Finally, macerated oils are made by infusing parts of plants in a base oil, a process called liquid–liquid extraction.The term "vegetable oil" can be narrowly defined as referring only to substances that are liquid at room temperature,[2] or broadly defined without regard to a substance's state of matter at a given temperature.[3] While a large majority of the entries in this list fit the narrower of these definitions, some do not qualify as vegetable oils according to all understandings of the term.Although most plants contain some oil, only the oil from certain major oil crops[4] complemented by a few dozen minor oil crops[5] is widely used and traded.Vegetable oils can be classified in several ways, for example:The vegetable oils are grouped below in common classes of use.These oils make up a significant fraction of worldwide edible oil production. All are also used as fuel oils.Nut oils are generally used in cooking, for their flavor. Most are quite costly, because of the difficulty of extracting the oil.A number of citrus plants yield pressed oils. Some, such as lemon and orange oil, are used as essential oils, which is uncommon for pressed oils.[note 1][33] The seeds of many if not most members of the citrus family yield usable oils.[33][34][35][36]Members of the Cucurbitaceae include gourds, melons, pumpkins, and squashes. Seeds from these plants are noted for their oil content, but little information is available on methods of extracting the oil. In most cases, the plants are grown as food, with dietary use of the oils as a byproduct of using the seeds as food.[43]A number of oils are used as food supplements (or "nutraceuticals"), for their nutrient content or purported medicinal effect. Borage seed oil, blackcurrant seed oil, and evening primrose oil all have a significant amount of gamma-Linolenic acid (GLA) (about 23%, 15–20% and 7–10%, respectively), and it is this that has drawn the interest of researchers.A number of oils are used for biofuel (biodiesel and Straight Vegetable Oil) in addition to having other uses. Other oils are used only as biofuel.[note 4][146]Although diesel engines were invented, in part, with vegetable oil in mind,[147] diesel fuel is almost exclusively petroleum-based. Vegetable oils are evaluated for use as a biofuel based on:The oils listed immediately below are all (primarily) used for other purposes –  all but tung oil are edible –  but have been considered for use as biofuel.These oils are extracted from plants that are cultivated solely for producing oil-based biofuel.[note 5] These, plus the major oils described above, have received much more attention as fuel oils than other plant oils.Drying oils are vegetable oils that dry to a hard finish at normal room temperature. Such oils are used as the basis of oil paints, and in other paint and wood finishing applications. In addition to the oils listed here, walnut, sunflower and safflower oil are also considered to be drying oils.[175]A number of pressed vegetable oils are either not edible, or not used as an edible oil.
Cork (material)
Cork is an impermeable buoyant material, the phellem layer of bark tissue that is harvested for commercial use primarily from Quercus suber (the cork oak), which is endemic to southwest Europe and northwest Africa. Cork is composed of suberin, a hydrophobic substance. Because of its impermeable, buoyant, elastic, and fire retardant properties, it is used in a variety of products, the most common of which is wine stoppers. The montado landscape of Portugal produces approximately half of cork harvested annually worldwide, with Corticeira Amorim being the leading company in the industry.[1] Cork was examined microscopically by Robert Hooke, which led to his discovery and naming of the cell.[2]There are about 2,200,000 hectares of cork forest worldwide; 34% in Portugal and 27% in Spain.Annual production is about 200,000 tons; 49.6% from Portugal, 30.5% from Spain, 5.8% from Morocco, 4.9% from Algeria, 3.5% from Tunisia, 3.1% Italy, and 2.6% from France.[3]Once the trees are about 25 years old the cork is traditionally stripped from the trunks every nine years, with the first two harvests generally producing lower quality cork. The trees live for about 300 years.The cork industry is generally regarded as environmentally friendly.[4] Cork production is generally considered sustainable because the cork tree is not cut down to obtain cork; only the bark is stripped to harvest the cork.[5] The tree continues to live and grow. The sustainability of production and the easy recycling of cork products and by-products are two of its most distinctive aspects. Cork oak forests also prevent desertification and are a particular habitat in the Iberian Peninsula and the refuge of various endangered species.[6]Carbon footprint studies conducted by Corticeira Amorim, Oeneo Bouchage of France and the Cork Supply Group of Portugal concluded that cork is the most environmentally friendly wine stopper in comparison to other alternatives. The Corticeira Amorim’s study, in particular ("Analysis of the life cycle of Cork, Aluminum and Plastic Wine Closures"), was developed by PricewaterhouseCoopers, according to ISO 14040.[7] Results concluded that, concerning the emission of greenhouse gases, each plastic stopper released 10 times more CO2, whilst an aluminium screw cap releases 26 times more CO2 than does a cork stopper.The cork oak is unrelated to the "cork trees" (Phellodendron), which have corky bark but are not used for cork production.Cork is extracted only from early May to late August, when the cork can be separated from the tree without causing permanent damage. When the tree reaches 25–30 years of age and about 24 in (60 cm) in circumference, the cork can be removed for the first time. However, this first harvest almost always produces poor quality or "virgin" cork (Portuguese cortiça virgem; Spanish corcho bornizo or corcho virgen[8]). Bark from initial harvests can be used to make flooring, shoes, insulation and other industrial products. Subsequent extractions usually occur at intervals of 9 years, though it can take up to 13 for the cork to reach an acceptable size. If the product is of high quality it is known as "gentle" cork (Portuguese cortiça amadia,[9] but also cortiça secundeira only if it is the second time; Spanish corcho segundero, also restricted to the "second time"[8]), and, ideally, is used to make stoppers for wine and champagne bottles.[10]The workers who specialize in removing the cork are known as extractors. An extractor uses a very sharp axe to make two types of cuts on the tree: one horizontal cut around the plant, called a crown or necklace, at a height of about 2–3 times the circumference of the tree, and several vertical cuts called rulers or openings. This is the most delicate phase of the work because, even though cutting the cork requires significant force, the extractor must not damage the underlying phellogen or the tree will be harmed.To free the cork from the tree, the extractor pushes the handle of the axe into the rulers. A good extractor needs to use a firm but precise touch in order to free a large amount of cork without damaging the product or tree.These freed portions of the cork are called planks. The planks are usually carried off by hand since cork forests are rarely accessible to vehicles. The cork is stacked in piles in the forest or in yards at a factory and traditionally left to dry, after which it can be loaded onto a truck and shipped to a processor.Cork's elasticity combined with its near-impermeability makes it suitable as a material for bottle stoppers, especially for wine bottles. Cork stoppers represent about 60% of all cork based production. Cork has an almost zero Poisson's ratio, which means the radius of a cork does not change significantly when squeezed or pulled.[11]Cork is an excellent gasket material. Some carburetor float bowl gaskets are made of cork, for example.Cork is also an essential element in the production of badminton shuttlecocks.Cork's bubble-form structure and natural fire retardant make it suitable for acoustic and thermal insulation in house walls, floors, ceilings and facades. The by-product of more lucrative stopper production, corkboard is gaining popularity as a non-allergenic, easy-to-handle and safe alternative to petrochemical-based insulation products.Sheets of cork, also often the by-product of stopper production, are used to make bulletin boards as well as floor and wall tiles.Cork's low density makes it a suitable material for fishing floats and buoys, as well as handles for fishing rods (as an alternative to neoprene).Granules of cork can also be mixed into concrete. The composites made by mixing cork granules and cement have lower thermal conductivity, lower density and good energy absorption. Some of the property ranges of the composites are density (400–1500 kg/m³), compressive strength (1–26 MPa) and flexural strength (0.5–4.0 MPa).[12]As late as the mid-17th century, French vintners did not use cork stoppers, using instead oil-soaked rags stuffed into the necks of bottles.[13]Wine corks can be made of either a single piece of cork, or composed of particles, as in champagne corks; corks made of granular particles are called "agglomerated corks".[14]Natural cork closures are used for about 80% of the 20 billion bottles of wine produced each year. After a decline in use as wine-stoppers due to the increase in the use of synthetic alternatives, cork wine-stoppers are making a comeback and currently represent approximately 60% of wine-stoppers today[when?].[citation needed]Because of the cellular structure of cork, it is easily compressed upon insertion into a bottle and will expand to form a tight seal. The interior diameter of the neck of glass bottles tends to be inconsistent, making this ability to seal through variable contraction and expansion an important attribute.  However, unavoidable natural flaws, channels, and cracks in the bark make the cork itself highly inconsistent. In a 2005 closure study, 45% of corks showed gas leakage during pressure testing both from the sides of the cork as well as through the cork body itself.[15]Since the mid-1990s, a number of wine brands have switched to alternative wine closures such as plastic stoppers, screw caps, or other closures. During 1972 more than half of the Australian bottled wine went bad due to corking. A great deal of anger and suspicion was directed at Portuguese and Spanish cork suppliers who were suspected of deliberately supplying bad cork to non-EEC wine makers to help prevent cheap imports. Cheaper wine makers developed the aluminium "Spelvin" cap with a polypropylene stopper wad. More expensive wines and  carbonated varieties continued to use cork, although much closer attention was paid to the quality. Even so, some high premium makers prefer the Spelvin as it is a guarantee that the wine will be good even after many decades of ageing. Some consumers may have conceptions about screw caps being representative of lower quality wines, due to their cheaper price; however, in Australia, for example, much of the non-sparkling wine production now uses these Spelvin caps as a cork alternative, although some have recently switched back to cork citing issues using screw caps.[16] These alternatives to cork have both advantages and disadvantages. For example, screwtops are generally considered to offer a trichloroanisole (TCA) free seal, but they also reduce the oxygen transfer rate between the bottle and the atmosphere to almost zero, which can lead to a reduction in the quality of the wine.[citation needed] TCA is the main documented cause of cork taint in wine. However, some in the wine industry say natural cork stoppers are important because they allow oxygen to interact with wine for proper aging, and are best suited for wines purchased with the intent to age.[17] Stoppers which resemble natural cork very closely can be made by isolating the suberin component of the cork from the undesirable lignin, mixing it with the same substance used for contact lenses and an adhesive, and molding it into a standardized product, free of TCA or other undesirable substances.[18]  Composite corks with real cork veneers are used in cheaper wines.[19]The study "Analysis of the life cycle of Cork, Aluminum and Plastic Wine Closures," conducted by PricewaterhouseCoopers and commissioned by a major cork manufacturer, Amorim, concluded that cork is the most environmentally responsible stopper, in a one-year life cycle analysis comparison with plastic stoppers and aluminum screw caps.[20][21]Cork is used in musical instruments, particularly woodwind instruments, where it is used to fasten together segments of the instrument, making the seams airtight. Low quality conducting baton handles are also often made out of cork.It is also used in shoes, especially those using welt construction to improve climate control and comfort.Because it is impermeable and moisture-resistant, cork is often used as an alternative to leather in handbags, wallets and other fashion items.Cork can be used to make bricks for the outer walls of houses, as in Portugal's pavilion at Expo 2000.On November 28, 2007, the Portuguese national postal service CTT issued the world's first postage stamp made of cork.[22][23]Cork is used as the core of both baseballs and cricket balls. A corked bat is made by replacing the interior of a baseball bat with cork – a practice known as "corking". It was historically a method of cheating at baseball; the efficacy of the practice is now discredited.Cork is often used, in various forms, in spacecraft heat shields[24] and fairings.Cork can be used in the paper pick-up mechanisms in inkjet and laser printers.Cork is used to make later-model pith helmets.[25]Corks are also hung from hats to keep insects away. (See cork hat)Cork has been used as a core material in sandwich composite construction.Cork can be used as the friction lining material of an automatic transmission clutch, as designed in certain mopeds.Cork can be used instead of wood or aluminium in automotive interiors.[26]Cork can also be used to make watch bands and faces as seen with Sprout Watches.Cork slabs are sometimes used by orchid growers as a natural mounting material.Cork paddles are used by glass blowers to manipulate and shape hot molten glass.
Cotyledon
A cotyledon (/ˌkɒtɪˈliːdən/; "seed leaf" from Latin cotyledon,[1] from Greek: κοτυληδών kotylēdōn, gen.: κοτυληδόνος kotylēdonos, from κοτύλη kotýlē "cup, bowl") is a significant part of the embryo within the seed of a plant, and is defined by the Oxford English Dictionary as "The primary leaf in the embryo of the higher plants (Phanerogams); the seed-leaf."[2] Upon germination, the cotyledon may become the embryonic first leaves of a seedling. The number of cotyledons present is one characteristic used by botanists to classify the flowering plants (angiosperms). Species with one cotyledon are called monocotyledonous ("monocots"). Plants with two embryonic leaves are termed dicotyledonous ("dicots").In the case of dicot seedlings whose cotyledons are photosynthetic, the cotyledons are functionally similar to leaves. However, true leaves and cotyledons are developmentally distinct. Cotyledons are formed during embryogenesis, along with the root and shoot meristems, and are therefore present in the seed prior to germination. True leaves, however, are formed post-embryonically (i.e. after germination) from the shoot apical meristem, which is responsible for generating subsequent aerial portions of the plant.The cotyledon of grasses and many other monocotyledons is a highly modified leaf composed of a scutellum and a coleoptile. The scutellum is a tissue within the seed that is specialized to absorb stored food from the adjacent endosperm. The coleoptile is a protective cap that covers the plumule (precursor to the stem and leaves of the plant).Gymnosperm seedlings also have cotyledons, and these are often variable in number (multicotyledonous), with from 2 to 24 cotyledons forming a whorl at the top of the hypocotyl (the embryonic stem) surrounding the plumule. Within each species, there is often still some variation in cotyledon numbers, e.g. Monterey pine (Pinus radiata) seedlings have 5–9, and Jeffrey pine (Pinus jeffreyi) 7–13 (Mirov 1967), but other species are more fixed, with e.g. Mediterranean cypress always having just two cotyledons. The highest number reported is for big-cone pinyon (Pinus maximartinezii), with 24 (Farjon & Styles 1997).The cotyledons may be ephemeral, lasting only days after emergence, or persistent, enduring at least a year on the plant. The cotyledons contain (or in the case of gymnosperms and monocotyledons, have access to) the stored food reserves of the seed. As these reserves are used up, the cotyledons may turn green and begin photosynthesis, or may wither as the first true leaves take over food production for the seedling.[3]Cotyledons may be either epigeal, expanding on the germination of the seed, throwing off the seed shell, rising above the ground, and perhaps becoming photosynthetic; or hypogeal, not expanding, remaining below ground and not becoming photosynthetic. The latter is typically the case where the cotyledons act as a storage organ, as in many nuts and acorns.Hypogeal plants have (on average) significantly larger seeds than epigeal ones. They are also capable of surviving if the seedling is clipped off, as meristem buds remain underground (with epigeal plants, the meristem is clipped off if the seedling is grazed). The tradeoff is whether the plant should produce a large number of small seeds, or a smaller number of seeds which are more likely to survive.[4][5]Related plants show a mixture of hypogeal and epigeal development, even within the same plant family.  Groups which contain both hypogeal and epigeal species include, for example, the Araucariaceae family of Southern Hemisphere conifers,[6] the Fabaceae (pea family),[4] and the genus Lilium (see Lily seed germination types).  The frequently garden grown common bean - Phaseolus vulgaris - is epigeal while the closely related runner bean - Phaseolus coccineus - is hypogeal.The term cotyledon was coined by Marcello Malpighi (1628–1694).[a] John Ray was the first botanist to recognize that some plants have two and others only one, and eventually the first to recognize the immense importance of this fact to systematics, in Methodus plantarum (1682).[3][9]Theophrastus (3rd or 4th century BC) and Albertus Magnus (13th century) may also have recognized the distinction between the dicotyledons and monocotyledons.[10][11]
Eucalyptus
Aromadendron Andrews ex Steud.Eucalypton St.-Lag.Eudesmia R.Br.Symphyomyrtus SchauerEucalyptus /ˌjuːkəˈlɪptəs/[2] L'Héritier 1789[3] (plural eucalypti, eucalyptuses or eucalypts) is a diverse genus of flowering trees and shrubs (including a distinct group with a multiple-stem mallee growth habit) in the myrtle family, Myrtaceae. Members of the genus dominate the tree flora of Australia, and include Eucalyptus regnans, the tallest known flowering plant on Earth.[4] Australia is covered by 92,000,000 hectares (227,336,951 acres) of eucalypt forest, comprising three quarters of the area covered by native forest.[5]There are more than 700 species of eucalyptus and most are native to Australia; a very small number are found in adjacent areas of New Guinea and Indonesia. One species, Eucalyptus deglupta, ranges as far north as the Philippines. Of the 15 species found outside Australia, just nine are exclusively non-Australian. Species of eucalyptus are cultivated widely in the tropical and temperate world,  including the Americas, Europe, Africa, the Mediterranean Basin, the Middle East, China, and the Indian subcontinent. However, the range over which many eucalypts can be planted in the temperate zone is constrained by their limited cold tolerance.[6]Eucalyptus is one of three similar genera that are commonly referred to as "eucalypts", the others being Corymbia and Angophora. Many species, though by no means all, are known as gum trees because they exude copious kino from any break in the bark (e.g., scribbly gum). The generic name is derived from the Greek words ευ (eu) "well" and καλύπτω (kalýpto) "to cover", referring to the operculum on the calyx that initially conceals the flower.[7]Some eucalyptus species have attracted attention from horticulturists, global development researchers, and environmentalists because of desirable traits such as being fast-growing sources of wood, producing oil that can be used for cleaning and as a natural insecticide, or an ability to be used to drain swamps and thereby reduce the risk of malaria. Eucalyptus oil finds many uses like in fuels, fragrances, insect repellance and antimicrobial activity. Eucalyptus trees show allelopathic effects; they release compounds which inhibit other plant species from growing nearby. Outside their natural ranges, eucalypts are both lauded for their beneficial economic impact on poor populations[8][9]:22 and criticised for being "water-guzzling" aliens,[10] leading to controversy over their total impact.[11]On warm days, eucalyptus forests are sometimes shrouded in a smog-like mist of vaporised volatile organic compounds (terpenoids); the Australian Blue Mountains take their name from the haze.[12]A mature eucalyptus may take the form of a low shrub or a very large tree. The species can be divided into three main habits and four size categories.As a generalisation "forest trees" are single-stemmed and have a crown forming a minor proportion of the whole tree height. "Woodland trees" are single-stemmed, although they may branch at a short distance above ground level."Mallees" are multistemmed from ground level, usually less than 10 m (33 ft) in height, often with the crown predominantly at the ends of the branchlets and individual plants may combine to form either an open or closed formation. Many mallee trees may be so low-growing as to be considered a shrub.Two other tree forms are notable in Western Australia and described using the native names "mallet" and "marlock". The "mallet" is a small to medium-sized tree that does not produce lignotubers and has a relatively long trunk, a steeply branching habit and often a conspicuously dense terminal crown. This is the normal habit of mature healthy specimens of Eucalyptus occidentalis, E. astringens, E. spathulata, E. gardneri, E. dielsii, E. forrestiana, E. salubris, E. clivicola, and E. ornata. The smooth bark of mallets often has a satiny sheen and may be white, cream, grey, green, or copper.The term marlock has been variously used; in Forest Trees of Australia, it is defined as a small tree without lignotubers, but with a shorter, lower-branching trunk than a mallet. They usually grow in more or less pure stands. Clearly recognisable examples are stands of E. platypus, E. vesiculosa, and the unrelated E. stoatei.The term "morrell" is somewhat obscure in origin and appears to apply to trees of the western Australian wheatbelt and  goldfields which have a long, straight trunk, completely rough-barked. It is now used mainly for E. longicornis (red morrell) and E. melanoxylon (black morrell).Tree sizes follow the convention of:Eucalyptus regnans, a forest tree, showing crown dimension, TasmaniaEucalyptus camaldulensis, immature woodland trees, showing collective crown habit, Murray River, Tocumwal, New South WalesEucalyptus cretata, juvenile, showing low branching 'mallee' form, Melbourne, VictoriaEucalyptus angustissima, showing shrub form, MelbourneEucalyptus platypus, showing ‘marlock’ form, MelbourneNearly all eucalyptus are evergreen, but some tropical species lose their leaves at the end of the dry season. As in other members of the myrtle family, eucalyptus leaves are covered with oil glands. The copious oils produced are an important feature of the genus.  Although mature eucalyptus trees may be towering and fully leafed, their shade is characteristically patchy because the leaves usually hang downwards.The leaves on a mature eucalyptus plant are commonly lanceolate, petiolate, apparently alternate and waxy or glossy green. In contrast, the leaves of seedlings are often opposite, sessile and glaucous, but many exceptions to this pattern exist. Many species such as E. melanophloia and E. setosa retain the juvenile leaf form even when the plant is reproductively mature. Some species, such as E. macrocarpa, E. rhodantha, and E. crucis, are sought-after ornamentals due to this lifelong juvenile leaf form. A few species, such as E. petraea, E. dundasii, and E. lansdowneana, have shiny green leaves throughout their life cycle. E. caesia exhibits the opposite pattern of leaf development to most eucalyptus, with shiny green leaves in the seedling stage and dull, glaucous leaves in mature crowns. The contrast between juvenile and adult leaf phases is valuable in field identification.Four leaf phases are recognised in the development of a eucalyptus plant: the ‘seedling’, ‘juvenile’, ‘intermediate’, and ‘adult’ phases. However, no definite transitional point occurs between the phases. The intermediate phase, when the largest leaves are often formed, links the juvenile and adult phases.[13]In all except a few species, the leaves form in pairs on opposite sides of a square stem, consecutive pairs being at right angles to each other (decussate). In some narrow-leaved species, for example E. oleosa, the seedling leaves after the second leaf pair are often clustered in a detectable spiral arrangement about a five-sided stem. After the spiral phase, which may last from several to many nodes, the arrangement reverts to decussate by the absorption of some of the leaf-bearing faces of the stem. In those species with opposite adult foliage the leaf pairs, which have been formed opposite at the stem apex, become separated at their bases by unequal elongation of the stem to produce the apparently alternate adult leaves.The most readily recognisable characteristics of eucalyptus species are the distinctive flowers and fruit (capsules or "gumnuts"). Flowers have numerous fluffy stamens which may be white, cream, yellow, pink, or red; in bud, the stamens are enclosed in a cap known as an operculum which is composed of the fused sepals or petals, or both. Thus, flowers have no petals, but instead decorate themselves with the many showy stamens. As the stamens expand, the operculum is forced off, splitting away from the cup-like base of the flower; this is one of the features that unites the genus. The name Eucalyptus, from the Greek words eu-, which means well, and kaluptos, cover, meaning "well-covered", describes the operculum. The woody fruits or capsules are roughly cone-shaped and have valves at the end which open to release the seeds, which are waxy, rod-shaped, about 1 mm in length, and yellow-brown in colour. Most species do not flower until adult foliage starts to appear; E. cinerea and E. perriniana are notable exceptions.The appearance of eucalyptus bark varies with the age of the plant, the manner of bark shed, the length of the bark fibres, the degree of furrowing, the thickness, the hardness, and the colour. All mature eucalypts put on an annual layer of bark, which contributes to the increasing diameter of the stems. In some species, the outermost layer dies and is annually deciduous, either in long strips (as in E. sheathiana) or in variably sized flakes (E. diversicolor, E. cosmophylla, or E. cladocalyx). These are the gums or smooth-barked species. The gum bark may be dull, shiny, or satiny (as in E. ornata) or matte (E. cosmophylla). In many species, the dead bark is retained. Its outermost layer gradually fragments with weathering and sheds without altering the essentially rough-barked nature of the trunks or stems — for example E. marginata, E. jacksonii, E. obliqua, and E. porosa.E. globulus bark cells are able to photosynthesize in the absence of foliage, conferring an "increased capacity to re-fix internal CO2 following partial defoliation".[14] This allows the tree to grow in less-than-ideal climates, in addition to providing a better chance of recovery from damage sustained to its leaves in an event such as a fire.[15]Many species are ‘half-barks’ or ‘blackbutts’ in which the dead bark is retained in the lower half of the trunks or stems — for example, E. brachycalyx, E. ochrophloia, and E. occidentalis — or only in a thick, black accumulation at the base, as in E. clelandii. In some species in this category, for example E. youngiana and E. viminalis, the rough basal bark is very ribbony at the top, where it gives way to the smooth upper stems. The smooth upper bark of the half-barks and that of the completely smooth-barked trees and mallees can produce remarkable colour and interest, for example E. deglupta.[13]Different commonly recognised types of bark include:Bark detail of E. angophoroides, the apple-topped boxThe extraordinary coloured bark of E. deglupta native to Southeast AsiaThe 'box' bark of E. quadrangulata, the white-topped boxThe dark, fissured 'ironbark' of E. sideroxylonThe oldest definitive Eucalyptus fossils are surprisingly from South America, where eucalypts are no longer endemic, though have been introduced from Australia. The fossils are from the early Eocene (51.9 Mya), and were found in the Laguna del Hunco deposit in Chubut province in Argentina.[16] This shows that the genus had a Gondwanan distribution. Fossil leaves also occur in the Miocene of New Zealand, where the genus is not native today, but again have been introduced from Australia.[17]Despite the prominence of Eucalyptus in modern Australia, estimated to contribute some 75% of the modern vegetation, the fossil record is very scarce throughout much of the Cenozoic, and suggests that this rise to dominance is a geologically more recent phenomenon. The oldest reliably dated macrofossil of Eucalyptus is  a 21-million-year-old tree-stump encased in basalt in the upper Lachlan Valley in New South Wales. Other fossils have been found, but many are either unreliably dated or else unreliably identified.[18]It is useful to consider where Eucalyptus fossils have not been found. Extensive research has gone into the fossil floras of the Paleocene to Oligocene of South-Eastern Australia, and has failed to uncover a single Eucalyptus specimen. Although the evidence is sparse, the best hypothesis is that in the mid-Tertiary, the contintental margins of Australia only supported more mesic noneucalypt vegetation, and that eucalypts probably contributed to the drier vegetation of the arid continental interior. With the progressive drying out of the continent since the Miocene, eucalypts were displaced to the continental margins, and much of the mesic and rainforest vegetation that was once there was eliminated entirely.[18]The current superdominance of Eucalyptus in Australia may be an artefact of human influence on its ecology. In more recent sediments, numerous findings of a dramatic increase in the abundance of Eucalyptus pollen are associated with increased charcoal levels. Though this occurs at different rates throughout Australia, it is compelling evidence for a relationship between the artificial increase of fire frequency with the arrival of Aboriginals and increased prevalence of this exceptionally fire-tolerant genus.[18]Over 700 species of Eucalyptus are known; refer to the List of Eucalyptus species for a comprehensive list of species. Some have diverged from the mainstream of the genus to the extent that they are quite isolated genetically and are able to be recognised by only a few relatively invariant characteristics. Most, however, may be regarded as belonging to large or small groups of related species, which are often in geographical contact with each other and between which gene exchange still occurs. In these situations, many species appear to grade into one another, and intermediate forms are common. In other words, some species are relatively fixed genetically, as expressed in their morphology, while others have not diverged completely from their nearest relatives.Hybrid individuals have not always been recognised as such on first collection and some have been named as new species, such as E. chrysantha (E. preissiana × E. sepulcralis) and E. "rivalis" (E. marginata × E. megacarpa). Hybrid combinations are not particularly common in the field, but some other published species frequently seen in Australia have been suggested to be hybrid combinations. For example, E. erythrandra is believed to be E. angulosa × E. teraptera and due to its wide distribution is often referred to in texts.[13]Renantherin, a phenolic compound present in the leaves of some eucalyptus species, allows chemotaxonomic discrimination in the sections renantheroideae and renantherae[19] and the ratio of the amount of leucoanthocyanins varies considerably in certain species.[20]A small genus of similar trees, Angophora, has also been known since the 18th century. In 1995 new evidence, largely genetic, indicated that some prominent eucalyptus species were actually more closely related to Angophora than to the other eucalypts; they were split off into the new genus Corymbia. Although separate, the three groups are allied and it remains acceptable to refer to the members of all three genera, Angophora, Corymbia and Eucalyptus, as "eucalypts".Several eucalypt species are among the tallest trees in the world. Eucalyptus regnans, the Australian 'mountain ash', is the tallest of all flowering plants (angiosperms); today, the tallest measured specimen named Centurion is 99.6 m (327 ft) tall.[21] Coast Douglas-fir is about the same height; only coast redwood is taller, and they are conifers (gymnosperms). Six other eucalypt species exceed 80 metres in height: Eucalyptus obliqua, Eucalyptus delegatensis, Eucalyptus diversicolor, Eucalyptus nitens, Eucalyptus globulus and Eucalyptus viminalis.Most eucalypts are not tolerant of severe cold.[6][22][23] Eucalypts do well in a range of climates but are usually  damaged by anything beyond a light frost of −5 °C (23 °F);[6][22][23] the hardiest are the snow gums, such as Eucalyptus pauciflora, which is capable of withstanding cold and frost down to about −20 °C (−4 °F).[24] Two subspecies, E. pauciflora subsp. niphophila and E. pauciflora subsp. debeuzevillei in particular are even hardier and can tolerate even quite severe winters. Several other species, especially from the high plateau and mountains of central Tasmania such as Eucalyptus coccifera, Eucalyptus subcrenulata and Eucalyptus gunnii,[25] have also produced extreme cold-hardy forms and it is seed procured from these genetically hardy strains that are planted for ornament in colder parts of the world.An essential oil extracted from eucalyptus leaves contains compounds that are powerful natural disinfectants and can be toxic in large quantities. Several marsupial herbivores, notably koalas and some possums, are relatively tolerant of it. The close correlation of these oils with other more potent toxins called formylated phloroglucinol compounds (euglobals, macrocarpals and sideroxylonals)[26] allows koalas and other marsupial species to make food choices based on the smell of the leaves. For koalas, these compounds are the most important factor in leaf choice.Eucalyptus flowers produce a great abundance of nectar, providing food for many pollinators including insects, birds, bats and possums. Although eucalyptus trees are seemingly well-defended from herbivores by the oils and phenolic compounds, they have insect pests. These include the eucalyptus longhorn borer Phoracantha semipunctata and the aphid-like psyllids known as "bell lerps", both of which have become established as pests throughout the world wherever eucalypts are cultivated.The eusocial beetle Austroplatypus incompertus makes and defends its galleries exclusively inside Eucalyptus plants.The trunks and branches of the eucalyptus tree allow the largest known moth,  Zelotypia stacyi (the bentwing ghost moth, having a wingspan up to 250 mm) to feed and protect their larva and pupa, respectively.Eucalypts originated between 35 and 50 million years ago, not long after Australia-New Guinea separated from Gondwana, their rise coinciding with an increase in fossil charcoal deposits (suggesting that fire was a factor even then), but they remained a minor component of the Tertiary rainforest until about 20 million years ago, when the gradual drying of the continent and depletion of soil nutrients led to the development of a more open forest type, predominantly Casuarina and Acacia species.The aridification of Australia during the mid-tertiary period (25-40 million years ago), combined with the annual penetration of tropical convection storms, and associated lightning, deep into the continental interior stimulated the gradual evolution, diversification and geographic expansion of the flammable biota. The absence of great rivers or mountain chains meant that there were no geographic barriers to check the spread of fires. From the monsoonal 'cradle', fire-promoting species expanded into higher rainfall environments, where lightning was less frequent, gradually displacing the Gondwanan rainforest from all but the most fire-sheltered habitats.[27]The two valuable timber trees, alpine ash E. delegatensis and Australian mountain ash E. regnans, are killed by fire and only regenerate from seed. The same 2003 bushfire that had little impact on forests around Canberra resulted in thousands of hectares of dead ash forests. However, a small amount of ash survived and put out new ash trees as well. There has been some debate as to whether to leave the stands or attempt to harvest the mostly undamaged timber, which is increasingly recognised as a damaging practice.The two most common hazards of eucalyptus species to people are fire and falling branches.Eucalyptus oil is highly flammable; ignited trees have been known to explode.[11][28] Bushfires can travel easily through the oil-rich air of the tree crowns.[29][30]  Eucalypts obtain long-term fire survivability from their ability to regenerate from epicormic buds  situated deep within their thick bark, or from lignotubers,[31] or by producing serotinous fruits.In seasonally dry climates oaks are often fire-resistant, particularly in open grasslands, as a grass fire is insufficient to ignite the scattered trees. In contrast, a eucalyptus forest tends to promote fire because of the volatile and highly combustible oils produced by the leaves, as well as the production of large amounts of litter high in phenolics, preventing its breakdown by fungi and thus accumulating as large amounts of dry, combustible fuel.[31] Consequently, dense eucalypt plantings may be subject to catastrophic firestorms.  In fact, almost thirty years before the Oakland firestorm of 1991, a study of eucalyptus in the area warned that the litter beneath the trees builds up very rapidly and should be regularly monitored and removed.[32] It has been estimated that 70% of the energy released through the combustion of vegetation in the Oakland fire was due to eucalyptus.[33] In a National Park Service study, it was found that the fuel load (in tons per acre) of non-native eucalyptus woods is almost three times as great as native oak woodland.[33]Some species of gum trees drop branches unexpectedly. In Australia, Parks Victoria warns campers not to camp under river red gums.[34] Some councils in Australia such as Gosnells, Western Australia, have removed eucalypts after reports of damage from dropped branches, even in the face of lengthy, well publicised protests to protect particular trees.[35] A former Australian National Botanic Gardens director and consulting arborist, Robert Boden, has been quoted referring to "summer branch drop".[36] Dropping of branches is recognised in Australia literature through the fictional death of Judy in Seven Little Australians. Although all large trees can drop branches, the density of eucalyptus wood is high[37] due to its high resin content,[38] increasing the hazard.Eucalypts were introduced from Australia to the rest of the world following the Cook expedition in 1770. Collected by Sir Joseph Banks, botanist on the expedition, they were subsequently introduced to many parts of the world, notably California, Brazil, Ecuador, Colombia, Ethiopia, Morocco, Portugal, South Africa, Uganda, Israel-Palestine, Galicia and Chile. On the order of 250 species are under cultivation in California.[39] In Portugal and also Spain, eucalypts have been planted in plantations for the production of pulpwood. Eucalyptus are the basis for several industries, such as sawmilling, pulp, charcoal and others. Several species have become invasive and are causing major problems for local ecosystems, mainly due to the absence of wildlife corridors and rotations management.Eucalypts have many uses which have made them economically important trees, and have become a cash crop in poor areas such as Timbuktu, Mali[9]:22and the Peruvian Andes,[8] despite concerns that the trees are invasive in some countries like South Africa.[10] Best-known are perhaps the varieties karri and yellow box. Due to their fast growth, the foremost benefit of these trees is their wood. They can be chopped off at the root and grow back again. They provide many desirable characteristics for use as ornament, timber, firewood and pulpwood. It is also used in a number of industries, from fence posts and charcoal to cellulose extraction for biofuels. Fast growth also makes eucalypts suitable as windbreaks and to reduce erosion.Eucalypts draw a tremendous amount of water from the soil through the process of transpiration. They have been planted (or re-planted) in some places to lower the water table and reduce soil salination. Eucalypts have also been used as a way of reducing malaria by draining the soil in Algeria, Lebanon, Sicily,[40] elsewhere in Europe, in Caucasus (Western Georgia), and California.[41] Drainage removes swamps which provide a habitat for mosquito larvae, but can also destroy ecologically productive areas. This drainage is not limited to the soil surface, because the eucalyptus roots are up to 2.5 m (8.2 ft) in length and can, depending on the location, even reach the phreatic zone.[citation needed]Eucalyptus is the most common short fibre source for pulpwood to make pulp.[42] The types most often used in papermaking are Eucalyptus globulus (in temperate areas) and the Eucalyptus urophylla x Eucalyptus grandis hybrid (in the tropics).[43] The fibre length of Eucalyptus is relatively short and uniform with low coarseness compared with other hardwoods commonly used as pulpwood. The fibres are slender, yet relatively thick walled. This gives uniform paper formation and high opacity that are important for all types of fine papers. The low coarseness is important for high quality coated papers.[42]  Eucalyptus is suitable for many tissue papers as the short and slender fibres gives a high number of fibres per gram and low coarseness contributes to softness.[42]Eucalyptus oil is readily steam distilled from the leaves and can be used for cleaning and as an industrial solvent, as an antiseptic, for deodorising, and in very small quantities in food supplements, especially sweets, cough drops, toothpaste and decongestants. It has insect repellent properties (Jahn 1991 a, b; 1992), and is an active ingredient in some commercial mosquito repellents (Fradin & Day 2002). Eucalyptus globulus is the principal source of eucalyptus oil worldwide.The nectar of some eucalypts produces high-quality monofloral honey.Eucalypt wood is also commonly used to make didgeridoos, a traditional Australian Aboriginal wind instrument.[44] The trunk of the tree is hollowed out by termites, and then cut down if the bore is of the correct size and shape.[45]All parts of Eucalyptus may be used to make dyes that are substantive on protein fibres (such as silk and wool), simply by processing the plant part with water. Colours to be achieved range from yellow and orange through green, tan, chocolate and deep rust red.[46] The material remaining after processing can be safely used as mulch or fertiliser.[citation needed]Eucalyptus trees in the Australian outback draw up gold from tens of metres underground through their root system and deposit it as particles in their leaves and branches. A Maia detector for x-ray elemental imaging at the Australian Synchrotron clearly showed deposits of gold and other metals in the structure of eucalyptus leaves from the Kalgoorlie region of Western Australia that would have been untraceable using other methods. The microscopic leaf-bound "nuggets" are not worth collecting themselves, but may provide an environmentally benign way of locating subsurface mineral deposits.[47]In the 20th century, scientists around the world experimented with eucalyptus species. They hoped to grow them in the tropics, but most experimental results failed until breakthroughs in the 1960s-1980s in species selection, silviculture, and breeding programs "unlocked" the potential of eucalypts in the tropics. Prior to then, as Brett Bennett noted in a 2010 article, eucalypts were something of the "El Dorado" of forestry. Today, eucalyptus is the most widely planted type of tree in plantations around the world,[48] in South America (mainly in Brazil, Argentina, Paraguay and Uruguay), South Africa,  Australia, India, Galicia, Portugal and many more.[49]In the 1850s, Eucalyptus trees were introduced to California by Australians during the California Gold Rush. Much of California has a similar climate to parts of Australia. By the early 1900s, thousands of acres of eucalypts were planted with the encouragement of the state government. It was hoped that they would provide a renewable source of timber for construction, furniture making and railroad ties.  It was soon found that for the latter purpose eucalyptus was particularly unsuitable, as the ties made from eucalyptus had a tendency to twist while drying, and the dried ties were so tough that it was nearly impossible to hammer  rail spikes into them.They went on to note that the promise of eucalyptus in California was based on the old virgin forests of Australia. This was a mistake, as the young trees being harvested in California could not compare in quality to the centuries-old eucalyptus timber of Australia. It reacted differently to harvest. The older trees didn't split or warp as the infant California crop did. There was a vast difference between the two, and this would doom the California eucalyptus industry.[50]One way in which the eucalyptus, mainly the blue gum E. globulus, proved valuable in California was in providing windbreaks for highways, orange groves, and farms in the mostly treeless central part of the state. They are also admired as shade and ornamental trees in many cities and gardens.Eucalyptus plantations in California have been criticised, because they compete with native plants and do not support native animals. Fire is also a problem. The 1991 Oakland Hills firestorm, which destroyed almost 3,000 homes and killed 25 people, was partly fuelled by large numbers of eucalypts close to the houses.[51]In some parts of California, eucalypt plantations are being removed and native trees and plants restored. Individuals have also illegally destroyed some trees and are suspected of introducing insect pests from Australia which attack the trees.[52]Certain eucalyptus species may also be grown for ornament in warmer parts of the Pacific Northwest — western Washington, western Oregon and southwestern British Columbia.Antonio Lussich introduced Eucalyptus into Uruguay in approximately 1896, throughout what is now Maldonado Department, and it has spread all over the south-eastern and eastern coast. There had been no trees in the area because it consisted of dry sand dunes and stones. Lussich also introduced many other trees, particularly Acacia and pines, but they have not expanded so extensively.Uruguayan forestry crops using eucalyptus species have been promoted since 1989, when the new National Forestry Law established that 20% of the national territory would be dedicated to forestry. As the main landscape of Uruguay is grassland (140,000 km2, 87% of the national territory), most of the forestry plantations would be established in prairie regions.[53][54][55]The planting of Eucalyptus sp. has been criticised because of concerns that soil would be degraded by nutrient depletion and other biological changes.[54][55][56] During the last ten years, in the northwestern regions of Uruguay the Eucalyptus sp. plantations have reached annual forestation rates of 300%. That zone has a potential forested area of 1 million hectares, approximately 29% of the national territory dedicated to forestry, of which approximately 800,000 hectares are currently forested by monoculture of Eucalyptus spp.[57] It is expected that the radical and durable substitution of vegetation cover leads to changes in the quantity and quality of soil organic matter. Such changes may also influence soil fertility and soil physical and chemical properties. The soil quality effects associated with Eucalyptus sp. plantations could have adverse effects on soil chemistry;[56][58][59] for example: soil acidification,[60][61][62] iron leaching, allelopathic activities[61] and a high C:N ratio of litter.[58][63][64][65] Additionally, as most scientific understanding of land cover change effects is related to ecosystems where forests were replaced by grasslands or crops, or grassland was replaced by crops, the environmental effects of the current Uruguayan land cover changes are not well understood.[66] The first scientific publication on soil studies in western zone tree plantations (focused on pulp production) appeared in 2004 and described soil acidification and soil carbon changes,[67] similar to a podzolisation process, and destruction of clay (illite-like minerals), which is the main reservoir of potassium in the soil.[68] Although these studies were carried out in an important zone for forest cultivation, they cannot define the current situation in the rest of the land area under eucalyptus cultivation. Moreover, recently Jackson and Jobbagy have proposed another adverse environmental impact that may result from Eucalyptus culture on prairie soils — stream acidification.[69]The eucalyptus species most planted are E. grandis, E. globulus and E. dunnii; they are used mainly for pulp mills. Approximately 80,000 ha of E. grandis situated in the departments of Rivera, Tacuarembó and Paysandú is primarily earmarked for the solid wood market, although a portion of it is used for sawlogs and plywood. The current area under commercial forest plantation is 6% of the total. The main uses of the wood produced are elemental chlorine free pulp mill production (for cellulose and paper), sawlogs, plywood and bioenergy (thermoelectric generation). Most of the products obtained from sawmills and pulp mills, as well as plywood and logs, are exported. This has raised the income of this sector with respect to traditional products from other sectors. Uruguayan forestry plantations have rates of growth of 30 cubic metres per hectare per year and commercial harvesting occurs after nine years.Eucalypts were introduced to Brazil in 1910, for timber substitution and the charcoal industry. It has thrived in the local environment, and today there are around 7 million hectares planted. The wood is highly valued by the charcoal and pulp and paper industries. The short rotation allows a larger wood production and supplies wood for several other activities, helping to preserve the native forests from logging. When well managed, the plantation soils can sustain endless replanting. Eucalyptus plantings are also used as wind breaks. Brazil's plantations have world-record rates of growth, typically over 40 cubic metres per hectare per year,[70] and commercial harvesting occurs after years 5. Due to continual development and governmental funding, year-on-year growth is consistently being improved. Eucalyptus can produce up to 100 cubic metres per hectare per year. Brazil has become the top exporter and producer of Eucalyptus round wood and pulp, and has played an important role in developing the Australian market through the country's[clarification needed] committed research in this area. The local iron producers in Brazil rely heavily on sustainably grown Eucalyptus for charcoal; this has greatly pushed up the price of charcoal in recent years. The plantations are generally owned and operated for national and international industry by timber asset companies such as Thomson Forestry, Greenwood Management or cellulose producers such as Aracruz Cellulose and Stora Enso.[citation needed]Overall, South America was expected to produce 55% of the world's Eucalyptus round-wood by 2010. Many environmental NGOs have criticised the use of exotic tree species for forestry in Latin America.[71]Ethiopia. Eucalypts were introduced to Ethiopia in either 1894 or 1895, either by Emperor Menelik II's French advisor Mondon-Vidailhet or by the Englishman Captain O'Brian. Menelik II endorsed its planting around his new capital city of Addis Ababa because of the massive deforestation around the city for firewood. According to Richard R.K. Pankhurst, "The great advantage of the eucalypts was that they were fast growing, required little attention and when cut down grew up again from the roots; it could be harvested every ten years. The tree proved successful from the onset".[72] Plantations of eucalypts spread from the capital to other growing urban centres such as Debre Marqos. Pankhurst reports that the most common species found in Addis Ababa in the mid-1960s was E. globulus, although he also found E. melliodora and E. rostrata in significant numbers. David Buxton, writing of central Ethiopia in the mid-1940s, observed that eucalyptus trees "have become an integral -- and a pleasing -- element in the Shoan landscape and has largely displaced the slow-growing native 'cedar' Juniperus procera)."[73]It was commonly believed that the thirst of the Eucalyptus "tended to dry up rivers and wells", creating such opposition to the species that in 1913 a proclamation was issued ordering a partial destruction of all standing trees, and their replacement with mulberry trees. Pankhurst reports, "The proclamation however remained a dead letter; there is no evidence of eucalypts being uprooted, still less of mulberry trees being planted."[74] Eucalypts remain a defining feature of Addis Ababa.Madagascar. Much of Madagascar's original native forest has been replaced with Eucalyptus, threatening biodiversity by isolating remaining natural areas such as Andasibe-Mantadia National Park.South Africa. Numerous Eucalyptus species have been introduced into South Africa, mainly for timber and firewood but also for ornamental purposes. They are popular with beekeepers for the honey they provide.[75] However, in South Africa they are considered invasive, with their water-sucking capabilities threatening water supplies. They also release a chemical into the surrounding soil which kills native competitors.[10]Eucalyptus seedlings are usually unable to compete with the indigenous grasses, but after a fire when the grass cover has been removed, a seed-bed may be created. The following Eucalyptus species have been able to become naturalised in South Africa: E. camaldulensis, E. cladocalyx, E. diversicolor, E. grandis and E. lehmannii.[75]Zimbabwe. As in South Africa, many Eucalyptus species have been introduced into Zimbabwe, mainly for timber and firewood, and E. robusta and E. tereticornis have been recorded as having become naturalised there.[75]In continental Portugal, the Azores and continental Spain (especially in Cantabria, Biscay, Asturias and Galicia in the north, and Huelva in Andalusia) farmland has been replaced with eucalypt plantations since their introduction by Rosendo Salvado in the 19th century.[citation needed]In Italy, the eucalyptus only arrived at the turn of the 19th century and large scale plantations were started at the beginning of the 20th century with the aim of drying up swampy ground to defeat malaria.[citation needed] During the 1930s, Benito Mussolini had thousands of eucalyptus plants planted in the marshes around Rome.[76] This, their rapid growth in the Italian climate and excellent function as windbreaks, has made them a common sight in the south of the country, including the islands of Sardinia and Sicily.[citation needed] They are also valued for the characteristic smelling and tasting honey that is produced from them. The variety of eucalyptus most commonly found in Italy is E. camaldulensis.[77]In Greece, eucalypts are widely found, especially in southern Greece and Crete. They are cultivated and used for various purposes, including as an ingredient in pharmaceutical products (e.g., creams, elixirs and sprays) and for leather production. They were imported in 1862 by botanist Theodoros Georgios Orphanides. The principal species is Eucalyptus globulus.Eucalyptus has been grown in Ireland since trials in the 1930s and now grows wild in South Western Ireland in the mild climate.Eucalyptus seeds of the species E. globulus were imported into Palestine in the 1860s, but did not acclimatise well.[78] Later, E. camaldulensis was introduced more successfully and it is still a very common tree in Israel.[78] The use of eucalyptus trees to drain swampy land was a common practice in the late nineteenth and early twentieth centuries.[78][79]  The German Templer colony of Sarona had begun planting eucalyptus for this purpose by 1874, though it is not known where the seeds came from.[80]   Many Zionist colonies also adopted the practice in the following years under the guidance of the Mikveh Israel Agricultural School.[78][79]In India, the Institute of Forest Genetics and Tree Breeding, Coimbatore started a eucalyptus breeding program in the 1990s. The organisation released four varieties of conventionally bred, high yielding and genetically improved clones for commercial and research interests in 2010.[81][82][83]Eucalyptus trees were introduced to Sri Lanka in the late 19th century by tea and coffee planters, for wind protection, shade and fuel. Forestry replanting of eucalyptus began in the 1930s in deforested mountain areas, and currently there are about 10 species present in the island. They account for 20% of major reforestation plantings. They provide railway sleepers, utility poles, sawn timber and fuelwood, but are controversial because of their adverse effect on biodiversity, hydrology and soil fertility. They are associated with another invasive species, the eucalyptus gall wasp, Leptocybe invasa.[84][85]Hawaii Some 90 species of eucalyptus have been introduced to the islands, where they have displaced some native species due to their higher maximum height, fast growth and lower water needs. Particularly noticeable is the rainbow eucalyptus (Eucalyptus deglupta), native to Indonesia and the Philippines, whose bark falls off to reveal a trunk that can be green, red, orange, yellow, pink and purple.[86]Due to similar favourable climatic conditions, Eucalyptus plantations have often replaced oak woodlands, for example in California, Spain and Portugal. The resulting monocultures have raised concerns about loss of biological diversity, through loss of acorns that mammals and birds feed on, absence of hollows that in oak trees provide shelter and nesting sites for birds and small mammals and for bee colonies, as well as lack of downed trees in managed plantations. A study of the relationship between birds and eucalyptus in the San Francisco Bay Area found that bird diversity was similar in native forest vs. eucalyptus forest but the species were different.[87] One way in which the avifauna changes is that cavity nesting birds including woodpeckers, owls, chickadees, wood ducks, etc. are depauperate in eucalyptus groves because the decay-resistant wood of these trees prevents cavity formation by decay or excavation. Also those bird species that glean insects from foliage, such as warblers and vireos, have population declines when eucalyptus replace oak forest. Birds that do well in eucalyptus groves in California like tall vertical habitat like herons and egrets (possibly because redwood trees are less available), or have longer bills, which may play a role in preventing their nostrils from being clogged by eucalyptus resin/pitch.[88] The Point Reyes Bird Observatory observes that sometimes short-billed birds like the ruby-crowned kinglet are found dead beneath eucalyptus trees with their nostrils clogged with pitch.[33]Monarch butterflies use eucalyptus in California for over-wintering, but in some locations have a preference for Monterey pines.[33]Although eucalypts must have been seen by the very early European explorers and collectors, no botanical collections of them are known to have been made until 1770 when Joseph Banks and Daniel Solander arrived at Botany Bay with Captain James Cook. There they collected specimens of E. gummifera and later, near the Endeavour River in northern Queensland, E. platyphylla; neither of these species was named as such at the time.In 1777, on Cook's third expedition, David Nelson collected a eucalypt on Bruny Island in southern Tasmania. This specimen was taken to the British Museum in London, and was named Eucalyptus obliqua by the French botanist L'Héritier, who was working in London at the time.[89] He coined the generic name from the Greek roots eu and calyptos, meaning "well" and "covered" in reference to the operculum of the flower bud which protects the developing flower parts as the flower develops and is shed by the pressure of the emerging stamens at flowering. It was most likely an accident that L'Héritier chose a feature common to all eucalypts.The name obliqua was derived from the Latin obliquus, meaning "oblique", which is the botanical term describing a leaf base where the two sides of the leaf blade are of unequal length and do not meet the petiole at the same place.E. obliqua was published in 1788-89, which coincided with the first official European settlement of Australia. Between then and the turn of the 19th century, several more species of Eucalyptus were named and published. Most of these were by the English botanist James Edward Smith and most were, as might be expected, trees of the Sydney region. These include the economically valuable E. pilularis, E. saligna and E. tereticornis.The first endemic Western Australian Eucalyptus to be collected and subsequently named was the Yate (E. cornuta) by the French botanist Jacques Labillardière, who collected in what is now the Esperance area in 1792.[13]Several Australian botanists were active during the 19th century, particularly Ferdinand von Mueller, whose work on eucalypts contributed greatly to the first comprehensive account of the genus in George Bentham's Flora Australiensis in 1867. The account is the most important early systematic treatment of the genus. Bentham divided it into five series whose distinctions were based on characteristics of the stamens, particularly the anthers (Mueller, 1879–84), work elaborated by Joseph Henry Maiden (1903–33) and still further by William Faris Blakely (1934). The anther system became too complex to be workable and more recent systematic work has concentrated on the characteristics of buds, fruits, leaves and bark.Eucalyptus sideroxylon, showing fruit (capsules) and buds with operculum present.Eucalyptus forest in East Gippsland, Victoria. Mostly E. albens (white box).Eucalyptus forest in East Gippsland, Victoria. Mostly E.albens (white box).Eucalyptus forest in East Gippsland, Victoria. Mostly E. albens (white box).A Eucalyptus tree with the sun shining through its branches.Eucalyptus bridgesiana (apple box) on Red Hill, Australian Capital Territory.Eucalyptus gunnii planted in southern England. The lower part of the trunk is covered in ivy.Eucalyptus cinerea x pulverulenta - National Botanical Gardens CanberraEucalyptus gallEucalyptus grandis. Province of Buenos Aires, Argentina.Eucalyptus plantation near Viveiro, in Galicia in Northwest Spain. Mostly E. globulusA snow gum (E. pauciflora), in winter in the Australian AlpsEucalyptus rubida (candlebark gum) in Burra, New South Wales.Sydney blue gums west of Port Macquarie, New South WalesEucalyptus chapmaniana (bogong gum) in Kew Gardens, LondonEucalyptus regnans trees in Sherbrooke Forest, VictoriaEucalyptus deanei, Blue Mountains National Park, AustraliaEucalypt woodland area near Prospect Creek in western Sydney. Mostly  E. amplifolia and E. tereticornis.
Prunus
Prunus is a genus of trees and shrubs, which includes the plums, cherries, peaches, nectarines, apricots, and almonds.Native to the northern temperate regions,[2] there are 430 different species classified under Prunus.[3] Many members of the genus are widely cultivated for their fruit and for decorative purposes. Prunus fruit are defined as drupes, or stone fruits, because the fleshy mesocarp surrounding the endocarp (pit or stone) is edible.[4] Most Prunus fruit and seeds are commonly used in processing, such as jam production, canning, drying or roasting.[5]Members of the genus can be deciduous or evergreen. A few species have spiny stems. The leaves are simple, alternate, usually lanceolate, unlobed, and often with nectaries on the leaf stalk. The flowers are usually white to pink, sometimes red, with five petals and five sepals. There are numerous stamens. Flowers are borne singly, or in umbels of two to six or sometimes more on racemes. The fruit is a fleshy drupe (a "prune") with a single relatively large, hard-coated seed (a "stone").[6]Within the rose family Rosaceae, it was traditionally placed as a subfamily, the Amygdaloideae (incorrectly "Prunoideae"), but was sometimes placed in its own family, the Prunaceae (or Amygdalaceae). More recently, it has become apparent that Prunus evolved from within a much larger clade now called subfamily Amygdaloideae (incorrectly "Spiraeoideae").[1]In 1737, Carl Linnaeus used four genera to include the species of modern Prunus—Amygdalus, Cerasus, Prunus and Padus—but simplified it to Amygdalus and Prunus in 1758.[7] Since then, the various genera of Linnaeus and others have become subgenera and sections, as it is clearer that all the species are more closely related. Liberty Hyde Bailey says: "The numerous forms grade into each other so imperceptibly and inextricably that the genus cannot be readily broken up into species."[8]A recent DNA study of 48 species concluded that Prunus is monophyletic and is descended from some Eurasian ancestor.[9]Historical treatments break the genus into several different genera, but this segregation is not currently widely recognised other than at the subgeneric rank. ITIS recognises just the single genus Prunus, with an open list of species,[a] all of which are given at List of Prunus species.[b]One standard modern treatment of the subgenera derives from the work of Alfred Rehder in 1940. Rehder hypothesized five subgenera: Amygdalus, Prunus, Cerasus, Padus and Laurocerasus.[10] To them C. Ingram added Lithocerasus.[11] The six subgenera are described as follows:Another recent DNA study[10] found that there are two clades: Prunus-Maddenia, with Maddenia basal within Prunus, and Exochorda-Oemleria-Prinsepia, but further refinement[1] shows that Exochorda-Oemleria-Prinsepia is somewhat separate from Prunus-Maddenia-Pygeum, and that, like the traditional subfamily Maloideae with apple-like fruits, all of these genera appear to be best considered within the expanded subfamily Amygdaloideae. Prunus can be divided into two clades: Amygdalus-Prunus and Cerasus-Laurocerasus-Padus. Yet another study adds Emplectocladus as a subgenus to the former.[13]The lists below are incomplete, but include most of the better-known species.The genus Prunus includes the almond, the nectarine and peach, and several species of apricots, of cherries, and of plums, all of which have cultivars developed for commercial fruit and nut production. The almond is not a true nut, the edible part is the seed. Other species are occasionally cultivated or used for their seed and fruit.A number of species, hybrids, and cultivars are grown as ornamental plants, usually for their profusion of flowers, sometimes for ornamental foliage and shape, and occasionally for their bark.The Tree of 40 Fruit has forty varieties grafted on to one rootstock.[14][15]Species such as blackthorn (Prunus spinosa), are grown for hedging, game cover, and other utilitarian purposes.The wood of some species (notably black cherry) is prized as a furniture and cabinetry timber, especially in North America.Many species produce an aromatic resin from wounds in the trunk; this is sometimes used medicinally. Other minor uses include dye production.Pygeum, a herbal remedy containing extracts from the bark of Prunus africana, is used as to alleviate some of the discomfort caused by inflammation in patients suffering from benign prostatic hyperplasia.Prunus species are food plants for the larvae of a large number of Lepidoptera species (butterflies and moths); see List of Lepidoptera which feed on Prunus.Prunus species are included in the Tasmanian Fire Service's list of low flammability plants, indicating that it is suitable for growing within a building protection zone.[16]Because of their considerable value as both food and ornamental plants, many Prunus species have been introduced to parts of the world to which they are not native, some becoming naturalised.Ornamentals include the group that may be collectively called "flowering cherries" (including sakura, the Japanese flowering cherries).Many species are cyanogenic; that is, they contain compounds called cyanogenic glucosides, notably amygdalin, which, on hydrolysis, yield hydrogen cyanide.[17] Although the fruits of some may be edible by humans and livestock (in addition to the ubiquitous fructivory of birds), seeds, leaves and other parts may be toxic, some highly so.[18] The plants contain no more than trace amounts of hydrogen cyanide, but on decomposition after crushing and exposure to air or on digestion, poisonous amounts may be generated. The trace amounts may give a characteristic taste ("bitter almond") with increasing bitterness in larger quantities, less tolerable to people than to birds, which habitually feed on specific fruits.People are often encouraged to consume many fruits because they are rich in a variety of nutrients and phytochemicals which are supposedly beneficial to human health. The fruits of Prunus often contain many phytochemicals and antioxidants.[5][19][20] These compounds have properties that have been linked to preventing different diseases and disorders.[19][21][22]  Research suggests that the consumption of these fruits reduces the risk of developing diseases such as cardiovascular diseases, cancer, diabetes, and other age-related declines.[21][22] There are many factors that can affect the levels of bioactive compounds in the different fruits of the genus Prunus, including the environment, season, processing methods, orchard operations as well as postharvest management.[5]Cherries contain many different phenolic compounds and anthocyanins, which is an indicator of being rich in antioxidants.[23][21] There has been recent research linking the phenolic compounds of the sweet cherry (Prunus avium) with antitumor properties.[24]Reactive oxygen species (ROS) include superoxide radicals, hydrogen peroxide, hydroxyl radicals and singlet oxygen; they are the byproducts of metabolism. High levels of ROS lead to oxidative stress which causes damage to lipids, proteins, and nucleic acids. The oxidative damage results in cell death which ultimately leads to numerous diseases and disorders. Antioxidants act as a defensive mechanism against the oxidative stress.[21][22] They are used to remove the free radicals in a living system that are generated as reactive oxygen species.[25][21] Some of those antioxidants include gutathione S-transferase, glutathione peroxidase, superoxide dismutase, and catalase.[25] The antioxidants present in cherry extracts act as inhibitors of the free radicals.[19] However, the DNA and proteins can be damaged when there is an imbalance in the level of free radicals and the antioxidants. When there aren't enough antioxidants to remove the free radicals, there are many diseases that can occur, such as cancers, cardiovascular diseases, Parkinson's disease, etc.[22] Recent studies have shown that using natural antioxidants as a supplement in chemotherapy can decrease the amount of oxidative damage. Some of these natural antioxidants include ascorbic acid, tocopherol, and epigallocatechin gallate; they can be found in certain cherry extracts.[25]Similar to cherries, strawberries, and raspberries, almonds are also rich in phenolics. Almonds have a high oxygen radical absorbing capacity (ORAC), which is another indicator of being rich in antioxidants.[5][26] As stated before, high levels of the free radicals is harmful and thus, having the capacity to absorb those radicals is greatly beneficial. The bioactive compounds, polyphenols and anthocyanins, that are found in berries and cherries, are also present in almonds.[27][26] Almonds also contain nonflavonoid and flavonoid compounds, which contribute to the antioxidant properties of almonds.[5][28][26] Flavonoids are a group of structurally related compounds that are arranged in a specific manner and can be found in all vascular plants on land. They also contribute to the antioxidant properties of almonds.[28] Some of the nonflavonoid compounds present are protocatechuic, vanillic, and p-hydroxybenzoic acids. Flavonoid compounds that can be found in the skin of the almond are flavanols, dihydroflavonols, and flavanones.[28][26]Of all of the different species of stone fruits, plums are the most rich in antioxidants and phenolic compounds. The total antioxidant capacity (TAC) varies within each fruit, but in plums, TAC is much higher in the skin that in the flesh of the fruit.[5][29][20]Apricots are high in carotenoids, which play a key role in light absorption during development. Carotenoids are the pigments which give the pulp and peel of apricots and other Prunus fruits their yellow and orange colors. Moreover, it is an essential precursor for Vitamin A, which is especially important for vision and the immune system in humans.[5][30] Moreover, these fruits are quite rich in phenolic substances including, catechin, epicatechin, p-coumaric acid, caffeic acid, and ferulic acid.[30][31]Similar to the plum, peaches and nectarines also have higher TAC in the skin than in the flesh.[5][29] They also contain moderate levels of carotenoids and ascorbic acid.[32][29][20] Peaches and nectarines are orange and yellow in color which can be attributed to the carotenoids present.[5] Ascorbic acid is important in hydroxylation reactions, such as collagen synthesis and de novo synthesis of bone and cartilage, and wound healing. Ascorbic acid is also a precursor of Vitamin C, which is essential for repairing tissues and absorbing iron.[5][20]Various Prunus species are winter hosts of the Damson-hop aphid, Phorodon humuli, which is destructive to hops Humulus lupulus just at the time of their maturity,[33] so it is recommended that plum trees not be grown in the vicinity of hop fields.Corking is the drying or withering of fruit tissue.[34] In stone fruit, it is often caused by a lack of boron and/or calcium.[35]Gummosis is a nonspecific condition of stone fruits (peach, nectarine, plum and cherry) in which gum is exuded and deposited on the bark of trees. Gum is produced in response to any type of wound: insects, mechanical injury or disease.[36]The earliest known fossil Prunus specimens are wood, drupe and seed and a leaf from the middle Eocene of the Princeton Chert of British Columbia.[37] Using the known age as calibration data, a partial phylogeny of some Rosaceae from a number of nucleotide sequences was reconstructed.[38] Prunus and its sister clade Maloideae (apple subfamily) diverged 44.3 mya. This date is within the Lutetian, or older middle Eocene.[c] Stockey and Wehr report: "The Eocene was a time of rapid evolution and diversification in Angiosperm families such as the Rosaceae ...."[37]The Princeton finds are among a large number of angiosperm fossils from the Okanagan Highlands dating to the late early and middle Eocene. Crataegus is found at three locations: Mcabee Falls, Idaho; Republic, Washington and Princeton, British Columbia, while Prunus is found at those locations and Quilchena, British Columbia and Chu Chua, British Columbia. A recent recapitulation of research on the topic[39] reported that the Rosaceae were more diverse at higher altitudes. The Okanagan formations date to as early as 52 mya, but the 44.3 mya date, which is approximate, depending on assumptions, might still apply. The authors state: "... the McAbee flora records a diverse early middle Eocene angiosperm-dominated forest."[39]:165The Online Etymology Dictionary presents the customary derivations of plum[40] and prune[41] from Latin prūnum,[42] the plum fruit. The tree is prūnus;[43] and Pliny uses prūnus silvestris to mean the blackthorn. The word is not native Latin, but is a loan from Greek προῦνον (prounon), which is a variant of προῦμνον (proumnon),[44] origin unknown. The tree is προύμνη (proumnē).[45] Most dictionaries follow Hoffman, Etymologisches Wörterbuch des Griechischen, in making some form of the word a loan from a pre-Greek language of Asia Minor, related to Phrygian.The first use of Prunus as a genus name was by Carl Linnaeus in Hortus Cliffortianus of 1737,[46] which went on to become Species Plantarum. In that work,[clarification needed] Linnaeus attributes the word to "Varr.", who it is assumed must be Marcus Terentius Varro.[dubious  – discuss]
Pollination
Pollination is the transfer of pollen from a male part of a plant to a female part of a plant, enabling later fertilisation and the production of seeds, most often by an animal or by wind.[1] Pollinating agents are animals such as insects, birds, and bats; water; wind; and even plants themselves, when self-pollination occurs within a closed flower. Pollination often occurs within a species. When pollination occurs between species it can produce hybrid offspring in nature and in plant breeding work.In angiosperms, after the pollen grain has landed on the stigma, it develops a pollen tube which grows down the style until it reaches an ovary. Sperm cells from the pollen grain then move along the pollen tube, enter an ovum cell through the micropyle and fertilise it, resulting in the production of a seed.A successful angiosperm pollen grain (gametophyte) containing the male gametes is transported to the stigma, where it germinates and its pollen tube grows down the style to the ovary. Its two gametes travel down the tube to where the gametophyte(s) containing the female gametes are held within the carpel. One nucleus fuses with the polar bodies to produce the endosperm tissues, and the other with the ovule to produce the embryo[2][3] Hence the term: "double fertilization".In gymnosperms, the ovule is not contained in a carpel, but exposed on the surface of a dedicated support organ, such as the scale of a cone, so that the penetration of carpel tissue is unnecessary. Details of the process vary according to the division of gymnosperms in question. Two main modes of fertilization are found in gymnosperms. Cycads and Ginkgo have motile sperm that swim directly to the egg inside the ovule, whereas conifers and gnetophytes have sperm that are unable to swim but are conveyed to the egg along a pollen tube.The study of pollination brings together many disciplines, such as botany, horticulture, entomology, and ecology. The pollination process as an interaction between flower and pollen vector was first addressed in the 18th century by Christian Konrad Sprengel. It is important in horticulture and agriculture, because fruiting is dependent on fertilization: the result of pollination. The study of pollination by insects is known as anthecology.Pollen germination has three stages; hydration, activation and pollen tube emergence. The pollen grain is severely dehydrated so that its mass is reduced enabling it to be more easily transported from flower to flower. Germination only takes place after rehydration, ensuring that premature germination does not take place in the anther. Hydration allows the plasma membrane of the pollen grain to reform into its normal bilayer organization providing an effective osmotic membrane. Activation involves the development of actin filaments throughout the cytoplasm of the cell, which eventually become concentrated at the point from which the pollen tube will emerge. Hydration and activation continue as the pollen tube begins to grow.[4]In conifers, the reproductive structures are borne on cones. The cones are either pollen cones (male) or ovulate cones (female), but some species are monoecious and others dioecious.  A pollen cone contains hundreds of microsporangia carried on (or borne on) reproductive structures called sporophylls. Spore mother cells in the microsporangia divide by meiosis to form haploid microspores that  develop further by two mitotic divisions into immature male gametophytes (pollen grains).  The four resulting cells consist of a large tube cell that forms the pollen tube, a generative cell that will produce two sperm by mitosis, and two prothallial cells that degenerate.  These cells comprise a very reduced microgametophyte, that is contained within the resistant wall of the pollen grain.[5][6]The pollen grains are dispersed by the wind to the female,  ovulate cone that is made up of many overlapping scales (sporophylls, and thus megasporophylls), each protecting two ovules, each of which consists of a megasporangium  (the nucellus) wrapped in  two layers of tissue, the integument and the cupule, that were derived from highly modified branches of ancestral gymnosperms.  When a pollen grain lands close enough to the tip of an ovule, it is drawn in through the micropyle ( a pore in the integuments covering the tip of the ovule)  often by means of a drop of liquid known as a pollination drop. The pollen enters a  pollen chamber close to the nucellus, and there  it may wait for a year before it germinates and forms a pollen tube that grows through the wall of the megasporangium (=nucellus) where fertilisation takes place. During this time, the megaspore mother cell divides by meiosis to form four haploid cells, three of which degenerate. The surviving one develops as a megaspore and divides repeatedly to form an immature female gametophyte (egg sac). Two or three archegonia containing an egg then develop inside the gametophyte.  Meanwhile, in the spring of the second year two sperm cells are produced by mitosis of the body cell of the male gametophyte. The pollen tube elongates and pierces and grows through the megasporangium wall and delivers the sperm cells to the female gametophyte inside. Fertilisation takes place when the nucleus of one of the sperm cells enters the egg cell in the megagametophyte’s archegonium.[6]In flowering plants, the anthers of the flower produce microspores by meiosis. These undergo mitosis to form male gametophytes, each of which contains two haploid cells. Meanwhile, the ovules produce megaspores by meiosis, further division of these form the female gametophytes, which are very strongly reduced, each consisting only of a few cells, one of which is the egg. When a pollen grain adheres to the stigma of a carpel it germinates, developing a pollen tube that grows through the tissues of  the style, entering the ovule through the micropyle. When the tube reaches the egg sac, two sperm cells pass through it into the female gametophyte and fertilisation takes place.[5]Pollination may be biotic or abiotic. Biotic pollination relies on living pollinators to move the pollen from one flower to another. Abiotic pollination relies on wind, water or even rain. About 80% of angiosperms rely on biotic pollination.[7]Abiotic pollination uses nonliving methods such as wind and water to move pollen from one flower to another. This allows the plant to spend energy directly on pollen rather than on attracting pollinators with flowers and nectar.Some 98% of abiotic pollination is anemophily, pollination by wind. This probably arose from insect pollination, most likely due to changes in the environment or the availability of pollinators.[8][9][10] The transfer of pollen is more efficient than previously thought; wind pollinated plants have developed to have specific heights, in addition to specific floral, stamen and stigma positions that promote effective pollen dispersal and transfer.[11]Pollination by water, hydrophily, uses water to transport pollen, sometimes as whole anthers; these can travel across the surface of the water to carry dry pollen from one flower to another.[12] In Vallisneria spiralis, an unopened male flower floats to the surface of the water, and, upon reaching the surface, opens up and the fertile anthers project forward. The female flower, also floating, has its stigma protected from the water, while its sepals are slightly depressed into the water, allowing the male flowers to tumble in.[12]Rain pollination is used by a small percentage of plants. Heavy rain discourages insect pollination and damages unprotected flowers, but can itself disperse pollen of suitably adapted plants, such as Ranunculus flammula, Narthecium ossifragum, and Caltha palustris.[13] In these plants, excess rain drains allowing the floating pollen to come in contact with the stigma.[13] In rain pollination in orchids, the rain allows for the anther cap to be removed, allowing for the pollen to be exposed. After exposure, raindrops causes the pollen to be shot upward, when the stipe pulls them back, and then fall into the cavity of the stigma. Thus, for the orchid Acampe rigida, this allows the plant to self-pollinate, which is useful when biotic pollinators in the environment have decreased.[14]It is possible for a plant have varying pollination methods, including both biotic and abiotic pollination. The orchid Oeceoclades maculata uses both rain and butterflies, depending on its environmental conditions.[15]More commonly, pollination involves pollinators (also called pollen vectors): organisms that carry or move the pollen grains from the anther of one flower to the receptive part of the carpel or pistil (stigma) of another.[16] Between 100,000 and 200,000 species of animal act as pollinators of the world's 250,000 species of flowering plant.[17] The majority of these pollinators are insects, but about 1,500 species of birds and mammals visit flowers and may transfer pollen between them. Besides birds and bats which are the most frequent visitors, these include monkeys, lemurs, squirrels, rodents and possums.[17]Entomophily, pollination by insects, often occurs on plants that have developed colored petals and a strong scent to attract insects such as, bees, wasps and occasionally ants (Hymenoptera), beetles (Coleoptera), moths and butterflies (Lepidoptera), and flies (Diptera). The existence of insect pollination dates back to the dinosaur era.[18]In zoophily, pollination is performed by vertebrates such as birds and bats, particularly, hummingbirds, sunbirds, spiderhunters, honeyeaters, and fruit bats. Ornithophily or bird pollination is the pollination of flowering plants by birds. Chiropterophily or bat pollination is the pollination of flowering plants by bats. Plants adapted to use bats or moths as pollinators typically have white petals, strong scent and flower at night, whereas plants that use birds as pollinators tend to produce copious nectar and have red petals.[19]Insect pollinators such as honey bees (Apis spp.),[20]bumblebees (Bombus spp.),[21][22] and butterflies (e.g., Thymelicus flavus)[23] have been observed to engage in flower constancy, which means they are more likely to transfer pollen to other conspecific plants.[24][25] This can be beneficial for the pollinators, as flower constancy prevents the loss of pollen during interspecific flights and pollinators from clogging stigmas with pollen of other flower species. It also improves the probability that the pollinator will find productive flowers easily accessible and recognisable by familiar clues.[26]Some flowers have specialized mechanisms to trap pollinators to increase effectiveness.[27] Other flowers will attract pollinators by odor. For example, bee species such as Euglossa cordata are attracted to orchids this way, and it has been suggested that the bees will become intoxicated during these visits to the orchid flowers, which last up to 90 minutes.[28] However, in general, plants that rely on pollen vectors tend to be adapted to their particular type of vector, for example day-pollinated species tend to be brightly coloured, but if they are pollinated largely by birds or specialist mammals, they tend to be larger and have larger nectar rewards than species that are strictly insect-pollinated. They also tend to spread their rewards over longer periods, having long flowering seasons; their specialist pollinators would be likely to starve if the pollination season were too short.[27]As for the types of pollinators, reptile pollinators are known, but they form a minority in most ecological situations. They are most frequent and most ecologically significant in island systems, where insect and sometimes also bird populations may be unstable and less species-rich. Adaptation to a lack of animal food and of predation pressure, might therefore favour reptiles becoming more herbivorous and more inclined to feed on pollen and nectar.[29] Most species of lizards in the families that seem to be significant in pollination seem to carry pollen only incidentally, especially the larger species such as Varanidae  and Iguanidae, but especially several species of the Gekkonidae are active pollinators, and so is at least one species of the Lacertidae, Podarcis lilfordi, which pollinates various species, but in particular is the major pollinator of Euphorbia dendroides on various Mediterranean islands.[30]Mammals are not generally thought of as pollinators, but some rodents, bats and marsupials are significant pollinators and some even specialise in such activities. In South Africa certain species of Protea (in particular Protea humiflora, P. amplexicaulis, P. subulifolia, P. decurrens and P. cordata) are adapted to pollination by rodents (particularly Cape Spiny Mouse, Acomys subspinosus)[31] and elephant shrews (Elephantulus species).[32] The flowers are borne near the ground, are yeasty smelling, not colourful, and sunbirds reject the nectar with its high xylose content. The mice apparently can digest the xylose and they eat large quantities of the pollen.[33] In Australia pollination by flying, gliding and earthbound mammals has been demonstrated.[34]Examples of pollen vectors include many species of wasps, that transport pollen of many plant species, being potential or even efficient pollinators.[35]Pollination can be accomplished by cross-pollination or by self-pollination:Geranium incanum, like most geraniums and pelargoniums, sheds its anthers, sometimes its stamens as well, as a barrier to self-pollination. This young flower is about to open its anthers, but has not yet fully developed its pistil.These Geranium incanum flowers have opened their anthers, but not yet their stigmas. Note the change of colour that signals to pollinators that it is ready for visits.This Geranium incanum flower has shed its stamens, and deployed the tips of its pistil without accepting pollen from its own anthers. (It might of course still receive pollen from younger flowers on the same plant.)An estimated 48.7% of plant species are either dioecious or self-incompatible obligate out-crossers.[41]  It is also estimated that about 42% of flowering plants have a mixed mating system in nature.[42]  In the most common kind of mixed mating system, individual plants produce a single type of flower and fruits may contain self-pollinated, out-crossed or a mixture of progeny types.Pollination also requires consideration of pollenizers, the plants that serve as the pollen source for other plants. Some plants are self-compatible (self-fertile) and can pollinate and fertilize themselves. Other plants have chemical or physical barriers to self-pollination.In agriculture and horticulture pollination management, a good pollenizer is a plant that provides compatible, viable and plentiful pollen and blooms at the same time as the plant that is to be pollinated or has pollen that can be stored and used when needed to pollinate the desired flowers. Hybridization is effective pollination between flowers of different species, or between different breeding lines or populations. see also Heterosis.Peaches are considered self-fertile because a commercial crop can be produced without cross-pollination, though cross-pollination usually gives a better crop. Apples are considered self-incompatible, because a commercial crop must be cross-pollinated. Many commercial fruit tree varieties are grafted clones, genetically identical. An orchard block of apples of one variety is genetically a single plant. Many growers now consider this a mistake. One means of correcting this mistake is to graft a limb of an appropriate pollenizer (generally a variety of crabapple) every six trees or so.[citation needed]The first fossil record for abiotic pollination is from fern-like plants in the late Carboniferous period. Gymnosperms show evidence for biotic pollination as early as the Triassic period. Many fossilized pollen grains show characteristics similar to the biotically dispersed pollen today. Furthermore, the gut contents, wing structures, and mouthpart morphology of fossilized beetles and flies suggest that they acted as early pollinators. The association between beetles and angiosperms during the early Cretaceous period led to parallel radiations of angiosperms and insects into the late Cretaceous. The evolution of nectaries in late Cretaceous flowers signals the beginning of the mutualism between hymenopterans and angiosperms.Bees provide a good example of the mutualism that exists between hymenopterans and angiosperms. Flowers provide bees with nectar (an energy source) and pollen (a source of protein). When bees go from flower to flower collecting pollen they are also depositing pollen grains onto the flowers, thus pollinating them. While pollen and nectar, in most cases, are the most notable reward attained from flowers, bees also visit flowers for other resources such as oil, fragrance, resin and even waxes.[43] It has been estimated that bees originated with the origin or diversification of angiosperms.[44] In addition, cases of coevolution between bee species and flowering plants have been illustrated by specialized adaptations. For example, long legs are selected for in Rediviva neliana, a bee that collects oil from Diascia capsularis, which have long spur lengths that are selected for in order to deposit pollen on the oil-collecting bee, which in turn selects for even longer legs in R. neliana and again longer spur length in D. capsularis is selected for, thus, continually driving each other's evolution.[45]Pollination management is a branch of agriculture that seeks to protect and enhance present pollinators and often involves the culture and addition of pollinators in monoculture situations, such as commercial fruit orchards. The largest managed pollination event in the world is in Californian almond orchards, where nearly half (about one million hives) of the US honey bees are trucked to the almond orchards each spring. New York's apple crop requires about 30,000 hives; Maine's blueberry crop uses about 50,000 hives each year. The US solution to the pollinator shortage, so far, has been for commercial beekeepers to become pollination contractors and to migrate. Just as the combine harvesters follow the wheat harvest from Texas to Manitoba, beekeepers follow the bloom from south to north, to provide pollination for many different crops.[citation needed]In America, bees are brought to commercial plantings of cucumbers, squash, melons, strawberries, and many other crops. Honey bees are not the only managed pollinators: a few other species of bees are also raised as pollinators. The alfalfa leafcutter bee is an important pollinator for alfalfa seed in western United States and Canada. Bumblebees are increasingly raised and used extensively for greenhouse tomatoes and other crops.The ecological and financial importance of natural pollination by insects to agricultural crops, improving their quality and quantity, becomes more and more appreciated and has given rise to new financial opportunities. The vicinity of a forest or wild grasslands with native pollinators near agricultural crops, such as apples, almonds or coffee can improve their yield by about 20%. The benefits of native pollinators may result in forest owners demanding payment for their contribution in the improved crop results – a simple example of the economic value of ecological services. Farmers can also raise native crops in order to promote native bee pollinator species as shown with L. vierecki in Delaware[47] and L. leucozonium in southwest Virginia.[48]The American Institute of Biological Sciences reports that native insect pollination saves the United States agricultural economy nearly an estimated $3.1 billion annually through natural crop production;[49] pollination produces some $40 billion worth of products annually in the United States alone.[50]Pollination of food crops has become an environmental issue, due to two trends. The trend to monoculture means that greater concentrations of pollinators are needed at bloom time than ever before, yet the area is forage poor or even deadly to bees for the rest of the season. The other trend is the decline of pollinator populations, due to pesticide misuse and overuse, new diseases and parasites of bees, clearcut logging, decline of beekeeping, suburban development, removal of hedges and other habitat from farms, and public concern about bees. Widespread aerial spraying for mosquitoes due to West Nile fears is causing an acceleration of the loss of pollinators.In some situations, farmers or horticulturists may aim to restrict natural pollination to only permit breeding with the preferred individuals plants. This may be achieved through the use of pollination bags.In some instances growers’ demand for beehives far exceeds the available supply. The number of managed beehives in the US has steadily declined from close to 6 million after WWII, to less than 2.5 million today. In contrast, the area dedicated to growing bee-pollinated crops has grown over 300% in the same time period. Additionally, in the past five years there has been a decline in winter managed beehives, which has reached an unprecedented rate of colony losses at near 30%.[51][52][53][54] At present, there is an enormous demand for beehive rentals that cannot always be met. There is a clear need across the agricultural industry for a management tool to draw pollinators into cultivations and encourage them to preferentially visit and pollinate the flowering crop. By attracting pollinators like honey bees and increasing their foraging behavior, particularly in the center of large plots, we can increase grower returns and optimize yield from their plantings. ISCA Technologies,[55] from Riverside California, created a semiochemical formulation called SPLAT Bloom, that modifies the behavior of honey bees, inciting them to visit flowers in every portion of the field.Loss of pollinators, also known as Pollinator decline (of which colony collapse disorder is perhaps the most well known) has been noticed in recent years. These loss of pollinators have caused a disturbance in early plant regeneration processes such as seed dispersal and of course, pollination. Early processes of plant regeneration greatly depend on plant-animal interactions and because these interactions are interrupted, biodiversity and ecosystem functioning are threatened.[56] Pollination by animals aids in the genetic variability and diversity within plants because it allows for out-crossing instead for self-crossing. Without this genetic diversity there would be a lack of traits for natural selection to act on for the survival of the plant species. Seed dispersal is also important for plant fitness because it allows plants the ability to expand their populations. More than that, it permits plants to escape environments that have changed and have become difficult to reside in. All of these factors show the importance of pollinators for plants, which are the foundation for a stable ecosystem. If only a few species of plants depended on pollinators the overall effect would not be as devastating however, this is not the case. It is known that more than 87.5% of angiosperms, over 75% of tropical tree species, and 30-40% of tree species in temperate regions depend on pollination and seed dispersal.[56]Possible explanations for pollinator decline include habitat destruction, pesticide, parasitism/diseases, and climate change.[57] It has also been found that the more destructive forms of human disturbances are land use changes such as fragmentation, selective logging, and the conversion to secondary forest habitat.[56] Defaunation of frugivores has also been found to be an important driver.[58] These alterations are especially harmful due to the sensitivity of the pollination process of plants.[56] There was a study done on tropical palms and the researchers concluded that defaunation has caused a decline in seed dispersal, which causes a decrease in genetic variability in this species.[58] Habitat destruction such as fragmentation and selective logging remove area that are most optimal for the different types of pollinators, which removes pollinators food resources, nesting sites, and leads to isolation of populations.[59] The effect of pesticides on pollinators has been debated due to the difficulty to be confident that a single pesticide is the cause and not a mixture or other threats.[59] It is also not know if exposure alone causes damages, or if the duration and potency are also factors.[59] However, insecticides do have some negative effects, such as neonicotinoids that harm bee colonies. Many researchers believe it is the synergistic effects of these factors which are ultimately detrimental to pollinator populations.[57]The most known and understood pollinator, bees, have been used as the prime example of the decline in pollinators. Bees are essential in the pollination of agricultural crops and wild plants and are one of the main insects that perform this task.[60] Out of the bees species, the honey bee or Apis mellifera has been studied the most and in the United States, there has been a loss of 59% of colonies from 1947 to 2005.[60] The decrease in populations of the honey bee have been attributed to pesticides, genetically modified crops, fragmentation, parasites and diseases that have been introduced.[61] There has been a focus on neonicotinoids effects on honey bee populations. Neonicotinoids insecticides have been used due to its low mammalian toxicity, target specificity, low application rates, and broad spectrum activity. However, the insecticides are able to make its way throughout the plant, which includes the pollen and nectar. Due to this, it has been shown to effect on the nervous system and colony relations in the honey bee populations.[61]Butterflies too have suffered due to these modifications. Butterflies are helpful ecological indicators since they are sensitive to changes within the environment like the season, altitude, and above all, human impact on the environment. Butterfly populations were higher within the natural forest and were lower in open land. The reason for the difference in density is the fact that in open land the butterflies would be exposed to desiccation and predation. These open regions are caused by habitat destruction like logging for timber, livestock grazing, and firewood collection. Due to this destruction, butterfly species' diversity can decrease and it is known that there is a correlation in butterfly diversity and plant diversity.[62]Besides the imbalance of the ecosystem caused by the decline in pollinators, it may jeopardise food security. Pollination is necessary for plants to continue their populations and 3/4 of the world's food supply are plants that require pollinators.[63] Insect pollinators, like bees, are large contributors to crop production, over 200 billion dollars worth of crop species are pollinated by these insects.[59]  Pollinators are also essential because they improve crop quality and increase genetic diversity, which is necessary in producing fruit with nutritional value and various flavors.[64] Crops that do not depend on animals for pollination but on the wind or self-pollination, like corn and potatoes, have doubled in production and make up a large part of the human diet but do not provide the micronutrients that are needed.[65] The essential nutrients that are necessary in the human diet are present in plants that rely on animal pollinators.[65] There have been issues in vitamin and mineral deficiencies and it is believed that if pollinator populations continue to decrease these deficiencies will become even more prominent.[64]Wild pollinators often visit a large number of plant species and plants are visited by a large number of pollinator species. All these relations together form a network of interactions between plants and pollinators. Surprising similarities were found in the structure of networks consisting out of the interactions between plants and pollinators. This structure was found to be similar in very different ecosystems on different continents, consisting of entirely different species.[66]The structure of plant-pollinator networks may have large consequences for the way in which pollinator communities respond to increasingly harsh conditions. Mathematical models, examining the consequences of this network structure for the stability of pollinator communities suggest that the specific way in which plant-pollinator networks are organized minimizes competition between pollinators[67] and may even lead to strong indirect facilitation between pollinators when conditions are harsh.[68] This means that pollinator species together can survive under harsh conditions. But it also means that pollinator species collapse simultaneously when conditions pass a critical point. This simultaneous collapse occurs, because pollinator species depend on each other when surviving under difficult conditions.[68]Such a community-wide collapse, involving many pollinator species, can occur suddenly when increasingly harsh conditions pass a critical point and recovery from such a collapse might not be easy. The improvement in conditions needed for pollinators to recover, could be substantially larger than the improvement needed to return to conditions at which the pollinator community collapsed.[68]
Lumber
Lumber (American English; used only in North America) or timber (used in the rest of the English-speaking world) is a type of wood that has been processed into beams and planks, a stage in the process of wood production. Lumber is mainly used for structural purposes but has many other uses as well.There are two main types of lumber. It may be supplied either rough-sawn, or surfaced on one or more of its faces. Besides pulpwood, rough lumber is the raw material for furniture-making and other items requiring additional cutting and shaping. It is available in many species, usually hardwoods; but it is also readily available in softwoods, such as white pine and red pine, because of their low cost.[1]Finished lumber is supplied in standard sizes, mostly for the construction industry – primarily softwood, from coniferous species, including pine, fir and spruce (collectively spruce-pine-fir), cedar, and hemlock, but also some hardwood, for high-grade flooring. It is more commonly made from softwood than hardwoods, and 80% of lumber comes from softwood.[2]In the United States milled boards of wood are referred to as lumber. However, in Britain and other Commonwealth nations, the term timber is instead used to describe sawn wood products, like floor boards.In the United States and Canada, generally timber describes standing or felled trees. Specifically in Canada, lumber describes cut and surfaced wood.[3]In the United Kingdom, the word lumber is rarely used in relation to wood and has several other meanings, including unused or unwanted items. Referring to wood, Timber is almost universally used instead.Remanufactured lumber is the result of secondary or tertiary processing/cutting of previously milled lumber. Specifically, it is lumber cut for industrial or wood-packaging use. Lumber is cut by ripsaw or resaw to create dimensions that are not usually processed by a primary sawmill.Resawing is the splitting of 1-inch through 12-inch hardwood or softwood lumber into two or more thinner pieces of full-length boards. For example, splitting a ten-foot 2×4 into two ten-foot 1×4s is considered resawing.Structural lumber may also be produced from recycled plastic and new plastic stock. Its introduction has been strongly opposed by the forestry industry.[4] Blending fiberglass in plastic lumber enhances its strength, durability, and fire resistance.[5] Plastic fiberglass structural lumber can have a "class 1 flame spread rating of 25 or less, when tested in accordance with ASTM standard E 84," which means it burns slower than almost all treated wood lumber.[6]Logs are converted into timber by being sawn, hewn, or split. Sawing with a rip saw is the most common method, because sawing allows logs of lower quality, with irregular grain and large knots, to be used and is more economical. There are various types of sawing:Dimensional lumber is lumber that is cut to standardized width and depth, specified in inches. Carpenters extensively use dimensional lumber in framing wooden buildings. Common sizes include 2×4 (pictured) (also two-by-four and other variants, such as four-by-two in Australia, New Zealand, and the UK), 2×6, and 4×4. The length of a board is usually specified separately from the width and depth. It is thus possible to find 2×4s that are four, eight, and twelve feet in length. In Canada and the United States, the standard lengths of lumber are 6, 8, 10, 12, 14, 16, 18, 20, 22 and 24 feet (1.83, 2.44, 3.05, 3.66, 4.27, 4.88, 5.49, 6.10, 6.71 and 7.32 meters).  For wall framing, "stud" or "precut" sizes are available, and are commonly used. For an eight-, nine-, or ten-foot ceiling height, studs are available in 92 5⁄8 inches (235 cm), 104 5⁄8 inches (266 cm), and 116 5⁄8 inches (296 cm). The term "stud" is used inconsistently to specify length; where the exact length matters, one must specify the length explicitly.Under the prescription of the Method of Construction (營造法式) issued by the Southern Song government in the early 12th century, timbers were standardized to eight cross-sectional dimensions.[7]  Regardless of the actual dimensions of the timber, the ratio between width and height was maintained at 1:1.5.  Units are in Song Dynasty inches (3.12 cm).Timber smaller than the 8th class were called "unclassed" (等外).  The width of a timber is referred to as one "timber" (材), and the dimensions of other structural components were quoted in multiples of "timber"; thus, as the width of the actual timber varied, the dimensions of other components were easily calculated, without resorting to specific figures for each scale.  The dimensions of timbers in similar application show a gradual diminution from the Sui Dyansty (580~618) to the modern era; a 1st class timber during the Sui was reconstructed as 15×10 (Sui Dynasty inches, or 2.94 cm).[8]The length of a unit of dimensional lumber is limited by the height and girth of the tree it is milled from.  In general the maximum length is 24 ft (7.32 m). Engineered wood products, manufactured by binding the strands, particles, fibers, or veneers of wood, together with adhesives, to form composite materials, offer more flexibility and greater structural strength than typical wood building materials.[9]Pre-cut studs save a framer much time, because they are pre-cut by the manufacturer for use in 8-, 9-, and 10-ft (2.44, 2.74 and 3.05 m) ceiling applications, which means the manufacturer has removed a few inches or centimetres of the piece to allow for the sill plate and the double top plate with no additional sizing necessary.In the Americas, two-bys (2×4s, 2×6s, 2×8s, 2×10s, and 2×12s), named for traditional board thickness in inches, along with the 4×4 (89 mm × 89 mm), are common lumber sizes used in modern construction. They are the basic building blocks for such common structures as balloon-frame or platform-frame housing. Dimensional lumber made from softwood is typically used for construction, while hardwood boards are more commonly used for making cabinets or furniture.Lumber's nominal dimensions are larger than the actual standard dimensions of finished lumber.  Historically, the nominal dimensions were the size of the green (not dried), rough (unfinished) boards that eventually became smaller finished lumber through drying and planing (to smooth the wood).  Today, the standards specify the final finished dimensions and the mill cuts the logs to whatever size it needs to achieve those final dimensions.  Typically, that rough cut is smaller than the nominal dimensions because modern technology makes it possible and it uses the logs more efficiently.  For example, a "2×4" board historically started out as a green, rough board actually 2 by 4 inches (51 mm × 102 mm).  After drying and planing, it would be smaller, by a nonstandard amount.  Today, a "2×4" board starts out as something smaller than 2 inches by 4 inches and not specified by standards, and after drying and planing is reliably 1 1⁄2 by 3 1⁄2 inches (38 mm × 89 mm).[10]Early standards called for green rough lumber to be of full nominal dimension when dry. However, the dimensions have diminished over time.  In 1910, a typical finished 1-inch (25 mm) board was 13⁄16 in (21 mm).  In 1928, that was reduced by 4%, and yet again by 4% in 1956.  In 1961, at a meeting in Scottsdale, Arizona, the Committee on Grade Simplification and Standardization agreed to what is now the current U.S. standard: in part, the dressed size of a 1-inch (nominal) board was fixed at ​3⁄4 inch; while the dressed size of 2 inch (nominal) lumber was reduced from ​1 5⁄8 inch to the current ​1 1⁄2 inch.[11]Dimensional lumber is available in green, unfinished state, and for that kind of lumber, the nominal dimensions are the actual dimensions.Individual pieces of lumber exhibit a wide range in quality and appearance with respect to knots, slope of grain, shakes and other natural characteristics. Therefore, they vary considerably in strength, utility, and value.The move to set national standards for lumber in the United States began with publication of the American Lumber Standard in 1924, which set specifications for lumber dimensions, grade, and moisture content; it also developed  inspection and accreditation programs.  These standards have changed over the years to meet the changing needs of manufacturers and distributors, with the goal of keeping lumber competitive with other construction products. Current standards are set by the American Lumber Standard Committee, appointed by the U.S. Secretary of Commerce.[12]Design values for most species and grades of visually graded structural products are determined in accordance with ASTM standards, which consider the effect of strength reducing characteristics, load duration, safety and other influencing factors. The applicable standards are based on results of tests conducted in cooperation with the USDA Forest Products Laboratory. Design Values for Wood Construction, which is a supplement to the ANSI/AF&PA National Design Specification® for Wood Construction, provides these lumber design values, which are recognized by the model building codes.[13]Canada has grading rules that maintain a standard among mills manufacturing similar woods to assure customers of uniform quality. Grades standardize the quality of lumber at different levels and are based on moisture content, size, and manufacture at the time of grading, shipping, and unloading by the buyer. The National Lumber Grades Authority (NLGA)[14] is responsible for writing, interpreting and maintaining Canadian lumber grading rules and standards. The Canadian Lumber Standards Accreditation Board (CLSAB)[15] monitors the quality of Canada's lumber grading and identification system.Attempts to maintain lumber quality over time have been challenged by historical changes in the timber resources of the United States – from the slow-growing virgin forests common over a century ago to the fast-growing plantations now common in today's commercial forests.  Resulting declines in lumber quality have been of concern to both the lumber industry and consumers and have caused increased use of alternative construction products.[16][17]Machine stress-rated and machine-evaluated lumber is readily available for end-uses where high strength is critical, such as trusses, rafters, laminating stock, I-beams and web joints. Machine grading measures a characteristic such as stiffness or density that correlates with the structural properties of interest, such as bending strength. The result is a more precise understanding of the strength of each piece of lumber than is possible with visually graded lumber, which allows designers to use full-design strength and avoid overbuilding.[18]In Europe, strength grading of rectangular sawn timber (both softwood and hardwood) is done according to EN-14081 [19] and commonly sorted into classes defined by EN-338. For softwoods the common classes are (in increasing strength) C16, C18, C24 and C30.  There are also classes specifically for hardwoods and those in most common use (in increasing strength) are D24, D30, D40, D50, D60 and D70. For these classes, the number refers to the required 5th percentile bending strength in Newtons per square millimetre. There are other strength classes, including T-classes based on tension intended for use in glulam.Grading rules for African and South American sawn timber have been developed by ATIBT[21] according to the rules of the Sciages Avivés Tropicaux Africains (SATA) and is based on clear cuttings – established by the percentage of the clear surface.[22]In North America, market practices for dimensional lumber made from hardwoods[a] varies significantly from the regularized standardized 'dimension lumber' sizes used for sales and specification of softwoods – hardwood boards are often sold totally rough cut,[b] or machine planed only on the two (broader) face sides. When Hardwood Boards are also supplied with planed faces, it is usually both by random widths of a specified thickness (normally matching milling of softwood dimensional lumbers) and somewhat random lengths. But besides those older (traditional and normal) situations, in recent years some product lines have been widened to also market boards in standard stock sizes; these usually retail in big-box stores and using only a relatively small set of specified lengths;[c] in all cases hardwoods are sold to the consumer by the board-foot (144 cubic inches or 2,360 cubic centimetres), whereas that measure is not used for softwoods at the retailer (to the cognizance of the buyer).[d]Also in North America, hardwood lumber is commonly sold in a "quarter" system, when referring to thickness; 4/4 (four quarter) refers to a 1-inch-thick (25 mm) board, 8/4 (eight quarter) is a 2-inch-thick (51 mm) board, etc. This "quarter" system is rarely used for softwood lumber; although softwood decking is sometimes sold as 5/4, even though it is actually one-inch thick (from milling 1/8th inch off each side in a motorized planing step of production).The "quarter" system of reference is a traditional (cultural) North American lumber industry nomenclature used specifically to indicate the thickness of rough sawn hardwood lumber.The following paragraph is exactly backwards from North American cultural practices where finished retail and rough lumber share the same terminology, as is discussed in the paragraph after about 'architects, designers, and builders':In rough sawn lumber it immediately clarifies that the lumber is not yet milled, avoiding confusion with milled dimension lumber which is measured as actual thickness after machining. Examples – 3/4", 19mm, or 1x.In recent years architects, designers, and builders have begun to use the "quarter" system in specifications as a vogue of insider knowledge, though the materials being specified are finished lumber, thus conflating  the separate systems and causing confusion.Hardwoods cut for furniture are cut in the fall and winter, after the sap has stopped running in the trees. If hardwoods are cut in the spring or summer the sap ruins the natural color of the timber and decreases the value of the timber for furniture.Engineered lumber is lumber created by a manufacturer and designed for a certain structural purpose. The main categories of engineered lumber are:[23]In the United States, pilings are mainly cut from southern yellow pines and Douglas firs. Treated pilings are available in Chromated copper arsenate retentions of 0.60, 0.80 and 2.50 pounds per cubic foot (9.6, 12.8 and 40.0 kg/m3) if treatment is required.Defects occurring in lumber are grouped into the following four divisions:During the process of converting timber to commercial form the following defects may occur:Fungi attack timber when these conditions are all present:Wood with less than 25% moisture (dry weight basis) can remain free of decay for centuries. Similarly, wood submerged in water may not be attacked by fungi if the amount of oxygen is inadequate.Fungi timber defects:Following are the insects and molluscs which are usually responsible for the decay of timber:There are two main natural forces responsible for causing defects in timber: abnormal growth and rupture of tissues. Rupture of tissue includes cracks or splits in the wood called "shakes". "Ring shake", "wind shake", or "ring failure" is when the wood grain separates around the growth rings either while standing or during felling. Shakes may reduce the strength of a timber and the appearance thus reduce lumber grade and may capture moisture, promoting decay. Eastern hemlock is known for having ring shake.[24] A "check" is a crack on the surface of the wood caused by the outside of a timber shrinking as it seasons. Checks may extend to the pith and follow the grain. Like shakes, checks can hold water promoting rot. A "split" goes all the way through a timber. Checks and splits occur more frequently at the ends of lumber because of the more rapid drying in these locations.[24]The seasoning of lumber is typically either kiln- or air-dried. Defects due to seasoning are the main cause of splits, bowing and honeycombing.[25]Under proper conditions, wood provides excellent, lasting performance. However, it also faces several potential threats to service life, including fungal activity and insect damage – which can be avoided in numerous ways. Section 2304.11 of the International Building Code  addresses protection against decay and termites. This section provides requirements for non-residential construction applications, such as wood used above ground (e.g., for framing, decks, stairs, etc.), as well as other applications.There are four recommended methods to protect wood-frame structures against durability hazards and thus provide maximum service life for the building. All require proper design and construction:Wood is a hygroscopic material, which means it naturally absorbs and releases water to balance its internal moisture content with the surrounding environment. The moisture content of wood is measured by the weight of water as a percentage of the oven-dry weight of the wood fiber. The key to controlling decay is controlling moisture. Once decay fungi are established, the minimum moisture content for decay to propagate is 22 to 24 percent, so building experts recommend 19 percent as the maximum safe moisture content for untreated wood in service. Water by itself does not harm the wood, but rather, wood with consistently high moisture content enables fungal organisms to grow.The primary objective when addressing moisture loads is to keep water from entering the building envelope in the first place, and to balance the moisture content within the building itself. Moisture control by means of accepted design and construction details is a simple and practical method of protecting a wood-frame building against decay. For applications with a high risk of staying wet, designers specify durable materials such as naturally decay-resistant species or wood that has been treated with preservatives. Cladding, shingles, sill plates and exposed timbers or glulam beams are examples of potential applications for treated wood.For buildings in termite zones, basic protection practices addressed in current building codes include (but are not limited to) the following:• Grading the building site away from the foundation to provide proper drainage• Covering exposed ground in any crawl spaces with 6-mil polyethylene film and maintaining at least 12 to 18 inches (300 to 460 mm) of clearance between the ground and the bottom of framing members above (12 inches to beams or girders, 18 inches to joists or plank flooring members)• Supporting post columns by concrete piers so that there is at least 6 inches (150 mm) of clear space between the wood and exposed earth• Installing wood framing and sheathing in exterior walls at least eight inches above exposed earth; locating siding at least six inches from the finished grade• Where appropriate, ventilating crawl spaces according to local building codes• Removing building material scraps from the job site before backfilling.• If allowed by local regulation, treating the soil around the foundation with an approved termiticide to provide protection against subterranean termitesTo avoid decay and termite infestation, untreated wood is separated from the ground and other sources of moisture. These separations are required by many building codes and are considered necessary to maintain wood elements in permanent structures at a safe moisture content for decay protection. When it is not possible to separate wood from the sources of moisture, designers often rely on preservative-treated wood.[26]Wood can be treated with a preservative that improves service life under severe conditions without altering its basic characteristics. It can also be pressure-impregnated with fire-retardant chemicals that improve its performance in a fire.[27] One of the early treatments to "fireproof lumber", which retard fires, was developed in 1936 by the Protexol Corporation, in which lumber is heavily treated with salt.[28]Wood does not deteriorate simply because it gets wet. When wood breaks down, it is because an organism is eating it. Preservatives work by making the food source inedible to these organisms. Properly preservative-treated wood can have 5 to 10 times the service life of untreated wood. Preserved wood is used most often for railroad ties, utility poles, marine piles, decks, fences and other outdoor applications. Various treatment methods and types of chemicals are available, depending on the attributes required in the particular application and the level of protection needed.[29]There are two basic methods of treating: with and without pressure. Non-pressure methods are the application of preservative by brushing, spraying or dipping the piece to be treated. Deeper, more thorough penetration is achieved by driving the preservative into the wood cells with pressure. Various combinations of pressure and vacuum are used to force adequate levels of chemical into the wood. Pressure-treating preservatives consist of chemicals carried in a solvent.Chromated copper arsenate, once the most commonly used wood preservative in North America began being phased out of most residential applications in 2004. Replacing it are amine copper quat and copper azole.All wood preservatives used in the United States and Canada are registered and regularly re-examined for safety by the U.S. Environmental Protection Agency and Health Canada's Pest Management and Regulatory Agency, respectively.[29]Timber was used as a dominant building material in most of the ancient temples of Kerala and coastal Karnataka of India.[30]Timber framing is a style of construction which uses heavier framing elements than modern stick framing, which uses dimensional lumber. The timbers originally were tree boles squared with a broadaxe or adze and joined together with joinery without nails. Modern timber framing has been growing in popularity in the United States since the 1970s.[31]Green building minimizes the impact or "environmental footprint" of a building. Wood is a major building material that is renewable and replenishable in a continuous cycle.[29] Studies show manufacturing wood uses less energy and results in less air and water pollution than steel and concrete.[32] However, demand for lumber is blamed for deforestation.[33]The conversion from coal to biomass power is a growing trend in the United States.[34]The United Kingdom, Uzbekistan, Kazakhstan, Australia, Fiji, Madagascar, Mongolia, Russia, Denmark, Switzerland and Swaziland governments all support an increased role for energy derived from biomass, which are organic materials available on a renewable basis and include residues and/or byproducts of the logging, sawmilling and papermaking processes. In particular, they view it as a way to lower greenhouse gas emissions by reducing consumption of oil and gas while supporting the growth of forestry, agriculture and rural economies. Studies by the U.S. government have found the country’s combined forest and agriculture land resources have the power to sustainably supply more than one-third of its current petroleum consumption.[35]Biomass is already an important source of energy for the North American forest products industry. It is common for companies to have cogeneration facilities, also known as combined heat and power, which convert some of the biomass that results from wood and paper manufacturing to electrical and thermal energy in the form of steam. The electricity is used to, among other things, dry lumber and supply heat to the dryers used in paper-making.
Actinorhizal plant
Actinorhizal plants  are a group of angiosperms characterized by their ability to form a symbiosis with the nitrogen fixing actinobacteria Frankia. This association leads to the formation of nitrogen-fixing root nodules.Actinorhizal plants are dicotyledons distributed among three angiosperm orders, 8 families and 24 genera:[1]These three orders form a single clade within the Rosids, which is a sister taxon to the other major nitrogen-fixing order, the Fabales. All actinorhizal species are trees or shrubs, except for the genus Datisca.Many are common plants in temperate regions like alder, bayberry, sweetfern, Avens, mountain misery and Coriaria. Some Elaeagnus species and Sea-buckthorns produce edible fruit. In tropical regions, Casuarinas are widely cultivated.Actinorhizal plants are found on all continents except for Antarctica. Their ability to form nitrogen-fixing nodules confers a selective advantage in poor soils. Most actinorhizal plants are therefore pioneer species that colonize young soils where available nitrogen is scarce like moraines, volcanic flows or sand dunes.[2] Being among the first species to colonize these disturbed environments, actinorhizal shrubs and trees play a critical role, enriching the soil and enabling the establishment of other species in an ecological succession.[1][2] Actinorhizal plants like alders are also common in the riparian forest.[2]Actinorhizal plants are the major contributors to nitrogen fixation in broad areas of the world, and are particularly important in temperate forest.[1] The nitrogen fixation rate measured for some alder species is as high as 300 kg of N2/ha/year, close to the highest rate reported in legumes.[3]No fossil records are available concerning nodules, but fossil pollen of plants similar to modern actinorhizal species has been found in sediments deposited 87 million years ago. The origin of the symbiotic association remains uncertain. The ability to associate with Frankia is a polyphyletic character and has probably evolved independently in different clades.[4] Nevertheless, actinorhizal plants and Legumes, the two major nitrogen-fixing groups of plants share a relatively close ancestor, as they are all part of a clade within the rosids which is often called the nitrogen-fixing clade. This ancestor may have developed a "predisposition" to enter into symbiosis with nitrogen fixing bacteria and this led to the independent acquisition of symbiotic abilities by ancestors of the actinorhizal and Legume species. The genetic program used to establish the symbiosis has probably recruited elements of the arbuscular mycorrhizal symbioses, a much older and widely distributed symbiotic association between plants and fungi.[5]Evolutionary origin of nitrogen-fixing nodulationAs in legumes, nodulation is favored by nitrogen deprivation and is inhibited by high nitrogen concentrations. Depending on the plant species, two mechanisms of infection have been described: The first is observed in  casuarinas or alders and is called root hair infection. In this case the infection begins with an intracellular infection of a root hair and is followed by the formation of a primitive symbiotic organ lacking any particular organization, a prenodule.[6] The second mechanism of infection is called intercellular entry and is well described in Discaria species. In this case bacteria penetrate the root extracellularly, growing between epidermal cells then between cortical cells. Later on Frankia becomes intracellular but no prenodule is formed. In both cases the infection leads to cell divisions in the pericycle and the formation of a new organ consisting of several lobes anatomically similar to a lateral root. This organ is the actinorhizal nodule also called actinorhizae. Cortical cells of the nodule are invaded by Frankia filaments coming from the site of infection or the prenodule. Actinorhizal nodules have generally an indeterminate growth, new cells are therefore continually produced at the apex and successively become infected. Mature cells of the nodule are filled with bacterial filaments that actively fix nitrogen.Little information is available concerning the mechanisms leading to nodulation. No equivalent of the rhizobial Nod factors have been found, but several genes known to participate in the formation and functioning of Legume nodules (coding for heamoglobin and other nodulins)  are also found in actinorhizal plants where they are supposed to play similar roles.[7] The lack of genetic tools in Frankia and in actinorhizal species was the main factor explaining such a poor understating of this symbiosis,  but  the recent sequencing of 3 Frankia genomes [8] and the development of RNAi and genomic tools in actinorhizal species [9][10] should help to a far better understanding in the following years.
Kiln
A kiln (/kɪln/ or /kɪl/,[1] originally pronounced "kill", with the "n" silent) is a thermally insulated chamber, a type of oven, that produces temperatures sufficient to complete some process, such as hardening, drying, or chemical changes.  Kilns have been used for millennia to turn objects made from clay into pottery, tiles and bricks. Various industries use rotary kilns for pyroprocessing—to calcinate ores, to calcinate limestone to lime for cement, and to transform many other materials.The word kiln descends from the Old English cylene (/ˈkylene/, which was adapted from the Latin culīna "kitchen, cooking-stove, burning-place". During the Middle English Period, the "n" was not pronounced, as evidenced by kiln having frequently been spelled without the "n",[2] Another word, "miln", a place where wheat is ground, also had a silent "n". Whereas the spelling of "miln" was changed to "mill" to match its pronunciation, "kiln" maintained its spelling, which most likely led to a common mispronunciation, which has now become commonly used. However, there are small bastions where the original pronunciation has endured. Kiln, Mississippi, a small town known for its wood drying kilns that once served the timber industry, is still referred to as "the Kill" by locals.[3]Unwittingly adding the "n" sound at the end of "kiln" is due to people being introduced to the word through the written language before ever hearing the actual pronunciation.  Linguists call this phenomenon "reading pronunciation" where an incorrect pronunciation is read aloud, becomes widespread, eventually reported by dictionaries, and the "original pronunciation, passed from parent to child, mouth to ear, for many generations is lost."[4]Phonetically, the "ln" in "kiln" is categorized as a digraph: a combination of two letters that make only one sound, such as the "mn" in "hymn". From English Words as Spoken and Written for Upper Grades by James A. Bowen 1900: "The digraph ln, n silent, occurs in kiln. A fall down the kiln can kill you."[5] Bowen was pointing out the humorous fact that "kill" and "kiln" are homophones.[6]Pit fired pottery was produced for thousands of years before the earliest known kiln, which dates to around 6000 BC, and was found at the Yarim Tepe site in modern Iraq.[7] Neolithic kilns were able to produce temperatures greater than 900 °C (1652 °F).[8]Uses include:Kilns are an essential part of the manufacture of all ceramics. Ceramics require high temperatures so chemical and physical reactions will occur to permanently alter the unfired body. In the case of pottery, clay materials are shaped, dried and then fired in a kiln. The final characteristics are determined by the composition and preparation of the clay body and the temperature at which it is fired. After a first firing, glazes may be used and the ware is fired a second time to fuse the glaze into the body. A third firing at a lower temperature may be required to fix overglaze decoration. Modern kilns often have sophisticated electrical control systems to firing regime, although pyrometric devices are often also used.Clay consists of fine-grained particles, that are relatively weak and porous. Clay is combined with other minerals to create a workable clay body. Part of the firing process includes sintering. This heats the clay until the particles partially melt and flow together, creating a strong, single mass, composed of a glassy phase interspersed with pores and crystalline material. Through firing, the pores are reduced in size, causing the material to shrink slightly. This crystalline material predominantly consists of silicon and aluminium oxides.In the broadest terms, there are two types of kiln: intermittent and continuous, both sharing the same basic characteristics of being an insulated box with a controlled inner temperature and atmosphere.A continuous kiln, sometimes called a tunnel kiln, is a long structure in which only the central portion is directly heated. From the cool entrance, ware is slowly transported through the kiln, and its temperature is increased steadily as it approaches the central, hottest part of the kiln. From there, it continues through the kiln, and the surrounding temperature is reduced until it exits the kiln nearly at room temperature. A continuous kiln is energy-efficient, because heat given off during cooling is recycled to pre-heat the incoming ware. In some designs, the ware is left in one place, while the heating zone moves across it. Kilns in this type include:In the intermittent kiln. the ware to be fired is placed into the kiln. The kiln is closed, and the internal temperature increased according to a schedule. After the firing is completed, both the kiln and the ware are cooled. The ware is removed, the kiln is cleaned and the next cycle begins. Kilns in this type include:[10]Kiln technology is very old. The development of the kiln from a simple earthen trench filled with pots and fuel, pit firing, to modern methods happened in stages. One improvement was to build a firing chamber around pots with baffles and a stoking hole. This conserved heat. A chimney stack improves the air flow or draw of the kiln, thus burning the fuel more completely. Chinese kiln technology has always been a key factor in the development of Chinese pottery, and until recent centuries was far ahead of other parts of the world. The Chinese developed effective kilns capable of firing at around 1,000°C before 2000 BC.  These were updraft kilns, often built below ground.  Two main types of kiln were developed by about 200 AD and remained in use until modern times.  These are the dragon kiln of hilly southern China, usually fuelled by wood, long and thin and running up a slope, and the horseshoe-shaped mantou kiln of the north Chinese plains, smaller and more compact.  Both could reliably produce the temperatures of up to 1300°C or more needed for porcelain.  In the late Ming, the egg-shaped kiln or zhenyao was developed at Jingdezhen, but mainly used there. This was something of a compromise between the other types, and offered locations in the firing chamber with a range of firing conditions.[11]Both Ancient Roman pottery and medieval Chinese pottery could be fired in industrial quantities, with tens of thousands of pieces in a single firing.[12] Early examples of simpler kilns found in Britain include those that made roof-tiles during the Roman occupation. These kilns were built up the side of a slope, such that a fire could be lit at the bottom and the heat would rise up into the kiln.Traditional kilns include:With the industrial age, kilns were designed to use electricity and more refined fuels, including natural gas and propane.  Many large industrial pottery kilns use natural gas, as it is generally clean, efficient and easy to control. Modern kilns can be fitted with computerized controls allowing for fine adjustments during the firing.  A user may choose to control the rate of temperature climb or ramp, hold or soak the temperature at any given point, or control the rate of cooling.  Both electric and gas kilns are common for smaller scale production in industry and craft, handmade and sculptural work.The temperature of some kilns is controlled by pyrometric cones—devices that begin to melt at specific temperatures. Modern kilns include:Green wood coming straight from the felled tree has far too high a moisture content to be commercially useful and will rot, warp and split. Both hardwoods and softwood must be left to dry out until the moisture content is between 18% and 8%. This can be a long process, or it is speeded up by use of a kiln. A variety of kiln technologies exist today:  conventional, dehumidification, solar, vacuum and radio frequency.The economics of different wood drying technologies are based on the total energy, capital, insurance/risk, environmental impacts, labor, maintenance, and product degradation costs. These costs which can be a significant part of plant costs, involve the differential impact of the presence of drying equipment in a specific plant. Every piece of equipment from the green trimmer to the infeed system at the planer mill is part the "drying system". The true costs of the drying system can only be determined when comparing the total plant costs and risks with and without drying.Kiln dried firewood was pioneered during the 1980s, and was later adopted extensively in Europe due to the economic and practical benefits of selling wood with a lower moisture content.[14][15][16]The total (harmful) air emissions produced by wood kilns, including their heat source, can be significant. Typically, the higher the temperature at which the kiln operates, the larger the quantity of emissions that are produced (per pound of water removed). This is especially true in the drying of thin veneers and high-temperature drying of softwoods.Brickmaking kilns, Mekong delta. The cargo boat in the foreground is carrying the rice chaff used as fuel for the firing.A wood fired pottery kiln in Hoi An Vietnam.A Catenary Arch kiln used for firing high temperature electron tube grade aluminium oxide ceramicsA two-story porcelain kiln with furnaces á alandier in Sèvres, France circa 1880CAD representation of a Beehive KilnCAD representation of a Tunnel kiln
Khasi people
The Khasi people, endonym: Ki Khun U Hynñiewtrep ("Children of the Seven Huts"), are an indigenous ethnic group of Meghalaya in north-eastern India, with a significant population in the bordering state of Assam, and in certain parts of Bangladesh. The Khasi people are the native people of Meghalaya and are the largest ethnic group in the state. Their language, Khasi, is categorised as the northernmost Austroasiatic language. Primarily an oral language, they had no script of their own, they used the Bengali script until the arrival of the Welsh missionaries. Particularly significant in this regard was a Welsh evangelist, Thomas Jones, who had transcribed the Khasi language into the Roman/Latin script. The Khasi people form the majority of the population of the eastern part of Meghalaya, and is the state's largest community, with around 48% of the population of Meghalaya. Before the arrival of Christian missionaries the Khasi people practiced an indigenous tribal religion.[2][3] Contrary to popular belief, the Khasis were never Sanskritized (nor were they Islamicized). [4] Though around 85% of the Khasi populace have embraced Christianity, a substantial minority of the Khasi people still follow and practice their age old indigenous religion, which is known as Ka Niam Khasi. Other religions practised among the Khasis include Catholicism, Anglicanism, Unitarianism, Presbyterianism (largest Christian denomination among the Khasis), and others. A small number of Khasis, as a result of inter-community marriages, are also Muslims. The main crops produced by the Khasi people are betel leaf, areca nut, oranges, local rice, vegetables, etc.The War sub-tribe of the Khasi community designed and built the famous living root bridges of the Cherrapunji region. Under the Constitution of India, the Khasis have been granted the status of Scheduled Tribe. A unique feature of the Khasi people is that they follow the matrilineal system of descent and inheritance. However, it must not be wrongly thought that men are completely powerless and have no say in private affairs of the household whatsoever. In matters of inheritance, some families do give men shares of the ancestral property, though the daughters usually get bigger shares. The reason is that, since women are the ones to continue the family lineage, giving them larger shares is necessary for them to run the households. In the Khasi system of asset management, the Khasi maternal uncles (Kñi) of the household (usually under the authority of the eldest Kñi), are the managers of their sister's property. No decision can be taken without their consent. In their wife's household too, they provide for their children like a normal father would. In present times, many Khasis are well placed in government and corporate sectors. Many Khasis are well educated. The tribe has produced many IAS, IPS and IFS bureaucrats. Many Khasis are also settled abroad, particularly in the US and Great Britain.Khasi mythology traces the tribe's original abode to 'Ki Hynñiewtrep ("The Seven Huts").[5] According to the Khasi mythology, "U Blei Trai Kynrad" (God, the Lord Master) had originally distributed the human race into 16 heavenly families (Khadhynriew Trep).[6] However, seven out of these 16 families are stuck on earth while the other 9 are stuck in Heaven. According to the myth, a heavenly ladder resting on the sacred Lum Sohpetbneng Peak (located in the present-day Ri-Bhoi district)enabled people to go freely and frequently to heaven whenever they pleased until one day they were tricked into cutting a divine tree which was situated at Lum Diengiei Peak (also in present-day Ri-Bhoi district), a grave error which prevented them access to the heavens forever. This myth is often seen as a metaphor of how nature and trees in particular are the manifestation of the divine on Earth and destroying nature and trees means severing our ties with the Divine. The Khasi indigenous religion like many other tribal religions of the world which are all based on nature worship face a growing threat of disappearance with the widespread dominance of Christianity and draws its beliefs from earlier times.Like the Japanese, the Khasis use the Rooster as a symbol because they believe that it was he who aroused God and also humbly paved and cleared the path for God to create the Universe in the beginning of time. The rooster is the symbol of Morning marking a new beginning and a new sunrise.The Khasi language is classified as part of the Austroasiatic language family. According to Peter Wilhelm Schmidt, the Khasi people are related to the Mon-Khmer people of South East Asia. Multiple researches indicate that the Austroasiatic populations in India are derived from migrations from southeast Asia during the Holocene. Many of the words also originate from the Tibetan language. "Nga" meaning "I" is the same in Khasi. Meghalaya was a part of Burma much before India claimed it as a part of it after it's independence from British rule. Traces of connections with the Kachin tribe of North Burma have been also been in the Khasis. The Khasi people also have their own word for the Himalayan mountains which is "Ki Lum Makachang" which means that at one point of time, they did cross the mighty mountains. Therefore, all these records and their present culture, features and language strongly show that they also have a strong Tibeto-Himalayan- Burman influence. The word "Khas" means hills and they have always been people of cold and hilly regions and have never been connected to the plains or arid regions. This nature loving tribe call the wettest place on Earth their home. The village of Mawsynram in Meghalaya receives 467 inches of rain per year.The Khasis first came in contact with the British in 1823, after the latter captured Assam. The area inhabited by the Khasis became a part of the Assam province after the Khasi Hill States (which numbered to about 25 kingdoms) entered into a subsidiary alliance with the British.According to the 2011 Census of India, over 1.41 million Khasi lived in Meghalaya in the districts of East Khasi Hills, West Khasi Hills, South West Khasi Hills, Ri-Bhoi, West Jaintia Hills and East Jaintia Hills. In Assam, their population reached 35,000. It is generally considered by many Khasi sociologists that the Khasi Tribe consist of seven sub-tribes, hence the title 'Children of the Seven Huts': Khynriam, Pnar, Bhoi, War, Maram, Lyngngam and Diko. The Khynriam (or Nongphlang) inhabit the uplands of the East Khasi Hills District; the Pnar or Synteng live in the uplands of the Jaintia Hills. The Bhoi live in the lower hills to the north and north-east of the Khasi Hills. and Jaintia Hills towards the Brahmaputra valley, a vast area now under Ri Bhoi District. The War, usually divided into War-Jaintia and War-Khynriam in the south of the Khasi Hills, live on the steep southern slopes leading to Bangladesh. The Maram inhabit the uplands of the central parts of West Khasi Hills Districts. The Lyngngam people who inhabit the western parts of the West Khasi Hills bordering the Garo Hills display linguistic and cultural characteristics which show influences from both the Khasis to their east and the Garo people to the west. The last sub-group completing the "seven huts", are the Diko, an extinct group who once inhabited the lowlands of the West Khasi Hills.Khasi people are mostly of short stature, with some exceptions. Estimates show that the average height is 5 feet 2 inches for Khasi men and around 4 feet 11 inches for khasi women. A majority of Khasi people are short while  a very few among them are taller, such as some Khasi males with heights ranging from 5.5 feet to 5.6 feet respectively and for women heights ranging from 5 feet to 5.4 feet normally.The traditional Khasi male dress is a Jymphong, a longish sleeveless coat without collar, fastened by thongs in front. Nowadays, most male Khasis have adopted western attire. On ceremonial occasions they appear in a Jymphong and sarong with an ornamental waist-band and they may also wear a turban.The traditional Khasi female dress is called the Jainsem or Dhara, both of which are rather elaborate with several pieces of cloth, giving the body a cylindrical shape. On ceremonial occasions they may wear a crown of silver or gold. A spike or peak is fixed to the back of the crown, corresponding to the feathers worn by the menfolk. The Jainsem consists of two pieces of material fastened at each shoulder. The "Dhara" consists of a single piece of material also fastened at each shoulder.The Khasis are, for the most part, monogamous. Their social organisation does not favour other forms of marriage; therefore, deviation from this norm is quite rare. Young men and women are permitted considerable freedom in the choice of mates. Potential marriage partners are likely to have been acquainted before betrothal. Once a man has selected his desired spouse, he reports his choice to his parents. They then secure the services of a mediator to make the arrangements with the woman's family (provided that the man's clan agree with his choice). The parents of the woman ascertain her wishes and if she agrees to the arrangement her parents check to make certain that the man to be wed is not a member of their clan (since Khasi clans are exogamous, marital partners may not be from the same clan). If this is satisfactory then a wedding date is set.Divorce (with causes ranging from incompatibility to lack of offspring) is easily obtainable. This ceremony traditionally consists of the husband handing the wife 5 cowries or paisa which the wife then hands back to her husband along with 5 of her own. The husband then throws these away or gives them to a village elder who throws them away.  Present-day Khasis divorce through the Indian legal system.The type of marriage is the determining factor in marital residence. In short, post marital residence for a married man when an heiress (known as Ka Khadduh)  is involved must be matrilocal (that is, in his mother-in-law's house), while post marital residence when a non-heiress is involved is neolocal. Generally, Khasi men prefer to marry a non-heiress because it will allow them to form independent family units somewhat immune to pressures from the wife's kin. Traditionally (though nowadays rule is not absolutely true), a Khasi man returns to his Iing-Kur (maternal home) upon the death of his spouse (if she is a Khadduh and they both have no children). These practices are the result of rules governing inheritance and property ownership. These rules are themselves related to the structure of the Khasi Kur (clan system).Khasi names are known for their originality and elaborate nature.  The given names may be invented by parents for their children, and these can be based on traditional native names, Christian names, or other English words.  The family names, which they call "surnames," remain typically in the native Khasi language.The traditional political structure of the Khasi community is democratic in nature. In the past, the Khasis consisted of independent native states called Syiemships, where male elders of various clans under the leadership of the Chief (called U Syiem) would congregate during Durbars or sessions and come to a decision regarding any dispute or problem that would arise in the Syiemship. At the village level, there exists a similar arrangement where all the residents of the village or town come together under the leadership of an elected Headman (called U Rangbah Shnong), to decide on matters pertaining to the locality. This system of village administration is much like the Panchayati Raj prevalent in most Indian States. There were around 25 independent native states on record which were annexed and acceded to the Indian Union. The Syiems of these native states (called Hima)  were traditionally elected by the people or ruling clans of their respective domains. Famous among these Syiemships are Hima Mylliem, Hima Khyrim, Hima Nongkhlaw, amongst others. These Syiemships continue to exist and function till today under the purview of the Khasi Hills Autonomous District Council (KHADC), which draws its legal power and authority from the Sixth Schedule of the Constitution of India.[7]
Frond
A frond is a large, divided leaf.[1] In both common usage and botanical nomenclature, the leaves of ferns are referred to as fronds[2] and some botanists restrict the term to this group.[3] Other botanists allow the term frond to also apply to the large leaves of cycads and palms (Arecaceae).[4][5] "Frond" is commonly used to identify a large, compound leaf, but if the term is used botanically to refer to the leaves of ferns, it may be applied to smaller and undivided leaves.Fronds have particular terms describing their components.  Like all leaves, fronds usually have a stalk connecting them to the main stem.  In botany, this leaf stalk is generally called a petiole, but in regard to fronds specifically it is called a stipe, and it supports a flattened blade (which may be called a lamina), and the continuation of the stipe into this portion is called the rachis. The blades may be simple (undivided), pinnatifid (deeply incised, but not truly compound), pinnate (compound with the leaflets arranged along a rachis to resemble a feather), or further compound (subdivided).  If compound, a frond may be compound once, twice, or more.If a frond is pinnate, each leafy segment of the blade is called a pinna (plural pinnae), the stalk bearing the pinna a petiolule, and the main vein or mid-rib of the pinna a costa (plural costae).[6] If a frond is divided into pinnae, the frond is called once pinnate. In some fronds the pinna are further divided into segments, creating a bipinnate frond. The segments into which each pinna are divided are called pinnules. Rarely, a frond may even be tripinnate, in which case the pinnule divisions are known as ultimate segments. Pinnae may be arranged along the rachis either directly opposite one another or alternating up the stem. The arrangement may change from the base of a blade to the tip, as in the example of Blechnum shown below (from base to tip: pinnae opposite to alternate, and pinnatisect to pinnatifid). Some fronds are not pinnately compound (or simple), but may be palmate or bifurcate. Some ferns, like members of the group Ophioglossales have a unique arrangement.Fern fronds often bear sporangia, where the plant's spores are formed, usually on the underside (abaxial surface) of the pinnae, but sometimes marginally or scattered over the frond.  The sporangia are typically clustered into a sorus (pl., sori). Associated with each sorus in many species is a membranous protective structure called an indusium, which is an outgrowth of the blade surface that may partly cover the sporangia. Some fern species feature frond dimorphism, in which fertile and sterile fronds differ in appearance and structure.Fern fronds, as with all leaves, arise from the stem, either directly, or on an outgrowth from the stem termed a phyllopodium. The stem of a typical (leptosporangiate) fern is subterranean or horizontal on the surface of the ground. These stems are called rhizomes. Many fern fronds are initially coiled into a fiddle-head or crozier (see circinate vernation), although cycad and palm fronds do not have this pattern of new leaf growth.Fronds may bear hairs, scales, glands, and, in some species, bulblets for vegetative reproduction.
Panicle
A panicle is a much-branched inflorescence.[1] Some authors distinguish it from a compound spike, by requiring that the flowers (and fruit) be pedicellate. The branches of a panicle are often racemes. A panicle may have determinate or indeterminate growth.This type of inflorescence is largely characteristic of grasses such as oat and crabgrass,[2] as well as other plants such as pistachio and mamoncillo. Botanists use the term paniculate in two ways: "having a true panicle inflorescence" as well as "having an inflorescence with the form but not necessarily the structure of a panicle".A corymb may have a paniculate branching structure, with the lower flowers having longer pedicels than the upper, thus giving a flattish top superficially resembling an umbel. Many species in the subfamily Amygdaloideae, such as hawthorns and rowans, produce their flowers in corymbs.
Tamil Nadu
Tamil Nadu is the eleventh largest Indian state by area and the sixth largest state by population. It has high HDI ranking among Indian states as of 2015.[7] The economy of Tamil Nadu is the second-largest state economy in India with ₹15.96 lakh crore (US$220 billion) in gross domestic product after Maharashtra and a per capita GDP of ₹167,000 (US$2,300).[4][5] It was ranked as one of the top seven developed states in India based on a "Multidimensional Development Index" in a 2013 report published by the Reserve Bank of India.[11] Its official language is Tamil, which is one of the longest-surviving classical languages in the world.The state is home to a number of historic buildings, multi-religious pilgrimage sites, hill stations and eight World Heritage sites.[12][13] The people of Tamil Nadu have continued to develop their cultural heritage in terms of music, dance, literature, theatre, cuisine, and other art forms. [14]Archaeological evidence points to this area being one of the longest continuous habitations in the Indian peninsula.[15] In Attirampakkam, archaeologists from the Sharma Centre for Heritage Education excavated ancient stone tools which suggests that a humanlike population existed in the Tamil Nadu region somewhere around 300,000 years before homo sapiens arrived from Africa.[16][17] In Adichanallur, 24 km (15 mi) from Tirunelveli, archaeologists from the Archaeological Survey of India (ASI) unearthed 169 clay urns containing human skulls, skeletons, bones, husks, grains of rice, charred rice and celts of the Neolithic period, 3,800 years ago.[18] The ASI archaeologists have proposed that the script used at that site is "very rudimentary" Tamil Brahmi.[19] Adichanallur has been announced as an archaeological site for further excavation and studies.[20] About 60 per cent of the total epigraphical inscriptions found by the ASI in India are from Tamil Nadu, and most of these are in the Tamil language.[21][22][23][24] [25][26][27][28][29]A Neolithic stone celt (a hand-held axe) with the Indus script on it was discovered at Sembian-Kandiyur near Mayiladuthurai in Tamil Nadu.According to epigraphist Iravatham Mahadevan, this was the first datable artefact bearing the Indus script to be found in Tamil Nadu.According to Mahadevan, the find was evidence of the use of the Harappan language, and therefore that the "Neolithic people of the Tamil country spoke a Harappan language".The date of the celt was estimated at between 1500 BCE and 2000 BCE.[30] .[31]The early history of the people and rulers of Tamil Nadu is a topic in Tamil literary sources known as Sangam literature. Numismatic, archaeological and literary sources corroborate that the Sangam period lasted for about six centuries, from 300 BC to AD 300. The recent excavations in Alagankulam archaeological site suggests that Alagankulam is one of the important trade centre or port city in Sangam Era.[32]The Bhakti movement originated in Tamil speaking region of South India and spread northwards through India. The Bhakti Movement was a rapid growth of bhakti beginning in this region with the Saiva Nayanars (4th–10th centuries)[33] and the Vaisnava Alvars who spread bhakti poetry and devotion.[33][34] The Alwars and Nayanmars were instrumental in propagating the Bhakti tradition.During the 4th to 8th centuries, Tamil Nadu saw the rise of the Pallava dynasty under Mahendravarman I and his son Mamalla Narasimhavarman I.[39] The Pallavas ruled parts of South India with Kanchipuram as their capital. Tamil architecture reached its peak during Pallava rule. Narasimhavarman II built the Shore Temple which is a UNESCO World Heritage Site. Much later, the Pallavas were replaced by the Chola dynasty as the dominant kingdom in the 9th century and they in turn were replaced by the Pandyan Dynasty in the 13th century. The Pandyan capital Madurai was in the deep south away from the coast. They had extensive trade links with the south east Asian maritime empires of Srivijaya and their successors, as well as contacts, even formal diplomatic contacts, reaching as far as the Roman Empire. During the 13th century, Marco Polo mentioned the Pandyas as the richest empire in existence. Temples such as the Meenakshi Amman Temple at Madurai and Nellaiappar Temple at Tirunelveli are the best examples of Pandyan temple architecture.[40] The Pandyas excelled in both trade and literature. They controlled the pearl fisheries along the south coast of India, between Sri Lanka and India, which produced some of the finest pearls in the known ancient world.During the 9th century, the Chola dynasty was once again revived by Vijayalaya Chola, who established Thanjavur as Chola's new capital by conquering central Tamil Nadu from Mutharaiyar and the Pandya king Varagunavarman II. Aditya I and his son Parantaka I expanded the kingdom to the northern parts of Tamil Nadu by defeating the last Pallava king, Aparajitavarman. Parantaka Chola II expanded the Chola empire into what is now interior Andhra Pradesh and coastal Karnataka, while under the great Rajaraja Chola and his son Rajendra Chola, the Cholas rose to a notable power in south east Asia. Now the Chola Empire stretched as far as Bengal and Sri Lanka. At its peak, the empire spanned almost 3,600,000 km2 (1,400,000 sq mi). Rajaraja Chola conquered all of peninsular south India and parts of Sri Lanka. Rajendra Chola's navy went even further, occupying coasts from Burma (now ) to Vietnam, the Andaman and Nicobar Islands, Lakshadweep, Sumatra, Java, Malaya, Philippines[41] in South East Asia and Pegu islands. He defeated Mahipala, the king of Bengal, and to commemorate his victory he built a new capital and named it Gangaikonda Cholapuram.The Cholas were prolific temple builders right from the times of the first medieval king Vijayalaya Chola.  These are the earliest specimen of Dravidian temples under the Cholas. His son Aditya I built several temples around the Kanchi and Kumbakonam regions. The Cholas went on to becoming a great power and built some of the most imposing religious structures in their lifetime and they also renovated temples and buildings of the Pallavas, acknowledging their common socio-religious and cultural heritage. The celebrated Nataraja temple at Chidambaram and the Sri Ranganathaswami Temple at Srirangam held special significance for the Cholas which have been mentioned in their inscriptions as their tutelary deities. Rajaraja Chola I and his son Rajendra Chola built temples such as the Brihadeshvara Temple of Thanjavur and Brihadeshvara Temple of Gangaikonda Cholapuram, the Airavatesvara Temple of Darasuram and the Sarabeswara (Shiva) Temple, also called the Kampahareswarar Temple at Thirubhuvanam, the last two temples being located near Kumbakonam. The first three of the above four temples are titled Great Living Chola Temples among the UNESCO World Heritage Sites.The Muslim invasions of southern India triggered the establishment of the Hindu Vijayanagara Empire with Vijayanagara in modern Karnataka as its capital. The Vijayanagara empire eventually conquered the entire Tamil country by c. 1370 and ruled for almost two centuries until its defeat in the Battle of Talikota in 1565 by a confederacy of Deccan sultanates. Subsequently, as the Vijayanagara Empire went into decline after the mid-16th century, many local rulers, called Nayaks, succeeded in gaining the trappings of independence. This eventually resulted in the further weakening of the empire; many Nayaks declared themselves independent, among whom the Nayaks of Madurai and Tanjore were the first to declare their independence, despite initially maintaining loose links with the Vijayanagara kingdom.[40] The Nayaks of Madurai and Nayaks of Thanjavur were the most prominent of Nayaks in the 17th century. They reconstructed some of the well-known temples in Tamil Nadu such as the Meenakshi Temple.By the early 18th century, the political scene in Tamil Nadu saw a major change-over and was under the control of many minor rulers aspiring to be independent. The fall of the Vijayanagara empire and the Chandragiri Nayakas gave the sultanate of Golconda a chance to expand into the Tamil heartland. When the sultanate was incorporated into the Mughal Empire in 1688, the northern part of current-day Tamil Nadu was administrated by the nawab of the Carnatic, who had his seat in Arcot from 1715 onward. Meanwhile, to the south, the fall of the Thanjavur Nayaks led to a short-lived Thanjavur Maratha kingdom. The fall of the Madurai Nayaks brought up many small Nayakars of southern Tamil Nadu, who ruled small parcels of land called palayams. The chieftains of these Palayams were known as Palaiyakkarar (or 'polygar' as called by British) and were ruling under the nawabs of the Carnatic.Europeans started to establish trade centres during the 17th century in the eastern coastal regions. Around 1609, the Dutch established a settlement in Pulicat,[42] while the Danes had their establishment in Tharangambadi also known as Tranquebar.[43] In 1639, the British, under the East India Company, established a settlement further south of Pulicat, in present-day Chennai. British constructed Fort St. George[44] and established a trading post at Madras.[45] The office of mayoralty of Madras was established in 1688. The French established trading posts at Pondichéry by 1693. The British and French were competing to expand the trade in the northern parts of Tamil Nadu which also witnessed many battles like Battle of Wandiwash as part of the Seven Years' War.[46] British reduced the French dominions in India to Puducherry. Nawabs of the Carnatic bestowed tax revenue collection rights on the East India Company for defeating the Kingdom of Mysore. Muhammad Ali Khan Wallajah surrendered much of his territory to the East India Company which firmly established the British in the northern parts. In 1762, a tripartite treaty was signed between Thanjavur Maratha, Carnatic and the British by which Thanjavur became a vassal of the Nawab of the Carnatic which eventually ceded to British.In the south, Nawabs granted taxation rights to the British which led to conflicts between British and the Palaiyakkarar, which resulted in series of wars called Polygar war to establish independent states by the aspiring Palaiyakkarar. Puli Thevar was one of the earliest opponents of the British rule in South India.[47] Thevar's prominent exploits were his confrontations with Marudhanayagam, who later rebelled against the British in the late 1750s and early 1760s. Rani Velu Nachiyar, was the first woman freedom fighter of India and Queen of Sivagangai.[48] She was drawn to war after her husband Muthu Vaduganatha Thevar (1750–1772), King of Sivaganga was murdered at Kalayar Kovil temple by British. Before her death, Queen Velu Nachi granted powers to the Maruthu brothers to rule Sivaganga.[49] Kattabomman (1760–1799), Palaiyakkara chief of Panchalakurichi who fought the British in the First Polygar War.[50] He was captured by the British at the end of the war and hanged near Kayattar in 1799. Veeran Sundaralingam (1700–1800) was the General of Kattabomman Nayakan's palayam, who died in the process of blowing up a British ammunition dump in 1799 which killed more than 150 British soldiers to save Kattapomman Palace. Oomaithurai, younger brother of Kattabomman, took asylum under the Maruthu brothers, Periya Marudhu and Chinna Marudhu and raised an army[51]. They formed a coalition with Dheeran Chinnamalai and Kerala Varma Pazhassi Raja which fought the British in Second Polygar Wars. Dheeran Chinnamalai (1756–1805), Polygar chieftain of Kongu and feudatory of Tipu Sultan who fought the British in the Second Polygar War. After winning the Polygar wars in 1801, the East India Company consolidated most of southern India into the Madras Presidency.At the beginning of the 19th century, the British firmly established governance over entirety of Tamil Nadu. The Vellore mutiny on 10 July 1806 was the first instance of a large-scale mutiny by Indian sepoys against the British East India Company, predating the Indian Rebellion of 1857 by half a century.[52] The revolt, which took place in Vellore, was brief, lasting one full day, but brutal as mutineers broke into the Vellore fort and killed or wounded 200 British troops, before they were subdued by reinforcements from nearby Arcot.[53][54] The British crown took over the control governance from the Company and the remainder of the 19th century did not witness any native resistance until the beginning of 20th century Indian Independence movements. During the administration of Governor George Harris(1854–1859) measures were taken to improve education and increase representation of Indians in the administration. Legislative powers given to the Governor's council under the Indian Councils Act 1861 and 1909 Minto-Morley Reforms eventually led to the establishment of the Madras Legislative Council. Failure of the summer monsoons and administrative shortcomings of the Ryotwari system resulted in two severe famines in the Madras Presidency, the Great Famine of 1876–78 and the Indian famine of 1896–97. The famine led to migration of people as bonded labours for British to various countries which eventually formed the present Tamil diaspora.When India became independent in 1947, Madras presidency became Madras state, comprising present-day Tamil Nadu, coastal Andhra Pradesh up to Ganjam district in Odisha, South Canara district Karnataka, and parts of Kerala. The state was subsequently split up along linguistic lines. In 1969, Madras State was renamed Tamil Nadu, meaning "Tamil country".[55]Tamil Nadu covers an area of 130,058 km2 (50,216 sq mi), and is the eleventh largest state in India. The bordering states are Kerala to the west, Karnataka to the north west and Andhra Pradesh to the north. To the east is the Bay of Bengal and the state encircles the union territory of Puducherry. The southernmost tip of the Indian Peninsula is Kanyakumari which is the meeting point of the Arabian Sea, the Bay of Bengal, and the Indian Ocean.The western, southern and the north western parts are hilly and rich in vegetation. The Western Ghats and the Eastern Ghats meet at the Nilgiri hills. The Western Ghats traverse the entire western border with Kerala, effectively blocking much of the rain bearing clouds of the south west monsoon from entering the state. The eastern parts are fertile coastal plains and the northern parts are a mix of hills and plains. The central and the south central regions are arid plains and receive less rainfall than the other regions.Tamil Nadu has the country's third longest coastline at about 906.9 km (563.5 mi).[56] Tamil Nadu's coastline bore the brunt of the 2004 Indian Ocean tsunami when it hit India, which caused 7,793 direct deaths in the state. Tamil Nadu falls mostly in a region of low seismic hazard with the exception of the western border areas that lie in a low to moderate hazard zone; as per the 2002 Bureau of Indian Standards (BIS) map, Tamil Nadu falls in Zones II & III. Historically, parts of this region have experienced seismic activity in the M5.0 range.[57]Tamil Nadu is mostly dependent on monsoon rains, and thereby is prone to droughts when the monsoons fail. The climate of the state ranges from dry sub-humid to semi-arid. The state has two distinct periods of rainfall:The annual rainfall of the state is about 945 mm (37.2 in) of which 48 per cent is through the north east monsoon, and 32 per cent through the south west monsoon. Since the state is entirely dependent on rains for recharging its water resources, monsoon failures lead to acute water scarcity and severe drought.[58] Tamil Nadu is divided into seven agro-climatic zones: north east, north west, west, southern, high rainfall, high altitude hilly, and Kaveri Delta (the most fertile agricultural zone).There are about 2000 species of wildlife that are native to Tamil Nadu. Protected areas provide safe habitat for large mammals including elephants, tigers, leopards, wild dogs, sloth bears, gaurs, lion-tailed macaques, Nilgiri langurs, Nilgiri tahrs, grizzled giant squirrels and sambar deer, resident and migratory birds such as cormorants, darters, herons, egrets, open-billed storks, spoonbills and white ibises, little grebes, Indian moorhen, black-winged stilts, a few migratory ducks and occasionally grey pelicans, marine species such as the dugongs, turtles, dolphins, Balanoglossus and a wide variety of fish and insects.Indian Angiosperm diversity comprises 17,672 species with Tamil Nadu leading all states in the country, with 5640 species accounting for 1/3 of the total flora of India. This includes 1559 species of medicinal plants, 533 endemic species, 260 species of wild relatives of cultivated plants and 230 red-listed species. The Gymnosperm diversity of the country is 64 species of which Tamil Nadu has four indigenous species and about 60 introduced species. The Pteridophytes diversity of India includes 1022 species of which Tamil Nadu has about 184 species. Vast numbers of bryophytes, lichen, fungi, algae and bacteria are among the wild plant diversity of Tamil Nadu.Common plant species include the state tree: palmyra palm, eucalyptus, rubber, cinchona, clumping bamboos (Bambusa arundinacea), common teak, Anogeissus latifolia, Indian laurel, grewia, and blooming trees like Indian labumusum, ardisia, and solanaceae. Rare and unique plant life includes Combretum ovalifolium, ebony (Diospyros nilagrica), Habenaria rariflora (orchid), Alsophila, Impatiens elegans, Ranunculus reniformis, and royal fern.[59]Tamil Nadu has a wide range of Biomes extending east from the South Western Ghats montane rain forests in the Western Ghats through the South Deccan Plateau dry deciduous forests and Deccan thorn scrub forests to tropical dry broadleaf forests and then to the beaches, estuaries, salt marshes, mangroves, Seagrasses and coral reefs of the Bay of Bengal.The state has a range of flora and fauna with many species and habitats. To protect this diversity of wildlife there are Protected areas of Tamil Nadu as well as biospheres which protect larger areas of natural habitat often include one or more National Parks. The Gulf of Mannar Biosphere Reserve established in 1986 is a marine ecosystem with seaweed seagrassrass communities, coral reefs, salt marshes and mangrove forests. The Nilgiri Biosphere Reserve located in the Western Ghats and Nilgiri Hills comprises part of adjoining states of Kerala and Karnataka. The Agasthyamala Biosphere Reserve is in the south west of the state bordering Kerala in the Western Ghats. Tamil Nadu is home to five declared National parks located in Anamalai, Mudumalai, Mukurithi, Gulf of Mannar, Guindy located in the centre of Chennai city and Vandalur located in South Chennai. Sathyamangalam Tiger Reserve, Mukurthi National Park and Kalakkad Mundanthurai Tiger Reserve are the tiger reserves in the state.The Governor is the constitutional head of the state while the Chief Minister is the head of the government and the head of the council of ministers.[60] The Chief Justice of the Madras High Court is the head of the judiciary.[60] The present Governor, Chief Minister and the Chief Justice are Banwarilal Purohit (governor),[61] Edappadi K. Palaniswami[62] and Vijaya Kamlesh Tahilramani[63] respectively. Administratively the state is divided into 32 districts. Chennai (formerly known as Madras) is the state capital. It is the fourth largest urban agglomeration in India and is also one of the major Metropolitan cities of India. The state comprises 39 Lok Sabha constituencies and 234 Legislative Assembly constituencies.[64]Tamil Nadu had a bicameral legislature until 1986, when it was replaced with a unicameral legislature, like most other states in India. The term length of the government is five years. The present government is headed by Edapaadi K Palaniswami, after the demise of former Chief Minister of Tamil Nadu, J. Jayalalithaa of the All India Anna Dravida Munnetra Kazhagam. The Tamil Nadu legislative assembly is housed at the Fort St. George in Chennai. The state had come under the President's rule on four occasions – first from 1976 to 1977, next for a short period in 1980, then from 1988 to 1989 and the latest in 1991.Tamil Nadu has been a pioneering state of E-Governance initiatives in India. A large part of the government records like land ownership records are digitised and all major offices of the state government like Urban Local Bodies – all the corporations and municipal office activities – revenue collection, land registration offices, and transport offices have been computerised. Tamil Nadu is one of the states where law and order has been maintained largely successfully.[65] The Tamil Nadu Police Force is over 140 years old. It is the fifth largest state police force in India (as of 2015, total police force of TN is 1,11,448) and has the highest proportion of women police personnel in the country (total women police personnel of TN is 13,842 which is about 12.42%) to specifically handled violence against women in Tamil Nadu.[66][67] In 2003, the state had a total police population ratio of 1:668, higher than the national average of 1:717.Tamil Nadu is subdivided into 32 districts, which are listed below. A district is administered by a District Collector who is mostly an Indian Administrative Service (IAS) member, appointed by State Government. Districts are further divided into 226 Taluks administrated by Tahsildars comprising 1127 Revenue blocks. A District has also one or more Revenue Divisions (in total 76) constituted by many Revenue Blocks. 16,564 Revenue villages (Village Panchayat) are the primary grassroots level administrative units which in turn might include many villages and administered by a Village Administrative Officer (VAO), many of which form a Revenue Block. Cities and towns are administered by Municipal corporations and Municipalities respectively. The urban bodies include 12 city corporations, 125 municipalities and 529 town panchayats.[68][69][70] The rural bodies include 31 district panchayats, 385 panchayat unions and 12,524 village panchayats.[71][72][73]Prior to Indian independence Tamil Nadu was under British colonial rule as part of the Madras Presidency. The main party in Tamil Nadu at that time was the Indian National Congress (INC). Regional parties have dominated state politics since 1916. One of the earliest regional parties, the South Indian Welfare Association, a forerunner to Dravidian parties in Tamil Nadu, was started in 1916. The party was called after its English organ, Justice Party, by its opponents. Later, South Indian Liberal Federation was adopted as its official name. The reason for victory of the Justice Party in elections was the non-participation of the INC, demanding complete independence of India.The Justice Party which was under E.V.Ramaswamy was renamed Dravidar Kazhagam in 1944. It was a non-political party which demanded the establishment of an independent state called Dravida Nadu. However, due to the differences between its two leaders EVR and C.N. Annadurai, the party was split. Annadurai left the party to form the Dravida Munnetra Kazhagam (DMK). The DMK decided to enter politics in 1956.Tamil Nadu is the seventh most populous state in India. 48.4 per cent of the state's population live in urban areas, the second highest percentage among large states in India. The state has registered the lowest fertility rate in India in year 2005–06 with 1.7 children born for each woman, lower than required for population sustainability.[77][78]At the 2011 India census, Tamil Nadu had a population of 72,147,030.[79] The sex ratio of the state is 995 with 36,137,975 males and 36,009,055 females. There are a total of 23,166,721 households.[79] The total children under the age of 6 is 7,423,832. A total of 14,438,445 people constituting 20.01 per cent of the total population belonged to Scheduled Castes (SC) and 794,697 people constituting 1.10 per cent of the population belonged to Scheduled tribes (ST).[80][79]The state has 51,837,507 literates, making the literacy rate 80.33 per cent. There are a total of 27,878,282 workers, comprising 4,738,819 cultivators, 6,062,786 agricultural labourers, 1,261,059 in house hold industries, 11,695,119 other workers, 4,120,499 marginal workers, 377,220 marginal cultivators, 2,574,844 marginal agricultural labourers, 238,702 marginal workers in household industries and 929,733 other marginal workers.[81]List of most populous towns in Tamil NaduAmong the cities in 2011, the state capital, Chennai, was the most populous city in the state, followed by Coimbatore, Madurai, Trichy and Salem respectively.[82] India has a human development index calculated as 0.619, while the corresponding figure for Tamil Nadu is 0.736, placing it among the top states in the country.[83][84] The life expectancy at birth for males is 65.2 years and for females it is 67.6 years.[85] However, it has a high level of poverty especially in the rural areas. In 2004–2005, the poverty line was set at ₹ 351.86/month for rural areas and ₹ 547.42/month for urban areas. Poverty in the state dropped from 51.7 per cent in 1983 to 21.1 per cent in 2001[86] For the period 2004–2005, the Trend in Incidence of Poverty in the state was 22.5 per cent compared with the national figure of 27.5 per cent. The World Bank is currently assisting the state in reducing poverty, High drop-out and low completion of secondary schools continue to hinder the quality of training in the population. Other problems include class, gender, inter-district and urban-rural disparities. Based on URP – Consumption for the period 2004–2005, percentage of the state's population Below Poverty Line was 27.5 per cent. The Oxford Poverty & Human Development Initiative ranks Tamil Nadu to have a Multidimensional Poverty Index of 0.141, which is in the level of Ghana among the developing countries.[87] Corruption is a major problem in the state with Transparency International ranking it the second most corrupt among the states of India.[88]Religion in Tamil Nadu (2011)[89]As per the religious census of 2011, Tamil Nadu had 87.6% Hindus, 6.1% Christians, 5.9% Muslims, 0.1% Jains and 0.3% following other religions or no religion.[90]Tamil (தமிழ்) is the sole official language of Tamil Nadu,while English is declared an additional official language for communication purposes.[6] When India adopted national standards, Tamil was the first language to be recognised as a classical language of India.[91] As of 2001 census, Tamil is spoken as the first language by 89.41 percent of the state's population followed by Telugu (5.65%), Kannada (1.67%), Urdu (1.51%) and Malayalam (0.89%).[6] Other Languages spoken are Hindi, Gujarati, Marathi, Bengali etc which are mostly spoken by migrant people.Distribution of languages in Tamil Nadu[92]Tamil Nadu is one of the most literate states in India.[93] Tamil Nadu has performed reasonably well in terms of literacy growth during the decade 2001–2011. A survey conducted by the Industry body Assocham ranks Tamil Nadu top among Indian states with about 100 per cent Gross Enrolment Ratio (GER) in primary and upper primary education. One of the basic limitations for improvement in education in the state is the rate of absence of teachers in public schools, which at 21.4 per cent is significant.[94] The analysis of primary school education in the state by Pratham shows a low drop-off rate but poor quality of state education compared to other states.[95]Tamil Nadu has 37 universities, 552 engineering colleges[96] 449 Polytechnic Colleges[97] and 566 arts and science colleges, 34335 elementary schools, 5167 high schools, 5054 higher secondary schools and 5000 hospitals. Some of the notable educational institutes present in Tamil Nadu are Indian Institute of Technology Madras, College of Engineering, Guindy, Indian Institute of Management Tiruchirappalli, St. Joseph’s Institute of Management Tiruchirappalli, Indian Maritime University, National Institute of Technology, Tiruchirappalli, Tamil Nadu Dr. Ambedkar Law University, Madras Medical College, Stanley Medical College, Chennai, Loyola College, Chennai, Ethiraj College for Women, Stella Maris College, Chennai, Anna University, Government College of Technology, Coimbatore, Bharathiar University, Coimbatore and Tamil Nadu Agricultural University, Coimbatore, Sri Ramachandra Medical College and Research Institute. Tamil Nadu now has 69 per cent reservation in educational institutions for socially backward section of the society, the highest among all Indian states.[98] The Midday Meal Scheme programme in Tamil Nadu was first initiated by Kamaraj, then it was expanded by M G Ramachandran in 1983.Tamil Nadu has a long tradition of venerable culture.[100] Tamil Nadu is known for its rich tradition of literature, art, music and dance which continue to flourish today. Tamil Nadu is a land most known for its monumental ancient Hindu temples and classical form of dance Bharata Natyam.[101] Unique cultural features like Bharatanatyam[102] (dance), Tanjore painting,[103] and Tamil architecture were developed and continue to be practised in Tamil Nadu.[104]Tamil written literature has existed for over 2000 years.[105] The earliest period of Tamil literature, Sangam literature, is dated from ca. 300 BC – AD 300.[106][107] It is the oldest Indian literature amongst all others.[108] The earliest epigraphic records found on rock edicts and hero stones date from around the 3rd century BC.[109][110]Most early Tamil literary works are in verse form, with prose not becoming more common until later periods. The Sangam literature collection contains 2381 poems composed by 473 poets, some 102 of whom remain anonymous.[111] Sangam literature is primarily secular, dealing with everyday themes in a Tamilakam context.[112] The Sangam literature also deals with human relationship and emotions.[113] The available literature from this period was categorised and compiled in the 10th century into two categories based roughly on chronology. The categories are: Pathinenmaelkanakku (The Major Eighteen Anthology Series) comprising Eṭṭuttokai (The Eight Anthologies) and the Pattupattu (Ten Idylls) and Pathinenkilkanakku (The Minor Eighteen Anthology Series).Much of Tamil grammar is extensively described in the oldest known grammar book for Tamil, the Tolkāppiyam. Modern Tamil writing is largely based on the 1000 B.C grammar Naṉṉūl which restated and clarified the rules of the Tolkāppiyam, with some modifications. Traditional Tamil grammar consists of five parts, namely eḻuttu, sol, poruḷ, yāppu, aṇi. Of these, the last two are mostly applied in poetry.[114] Notable example of Tamil poetry include the Tirukkural written by Tiruvalluvar before 2000 years.In 1578, the Portuguese published a Tamil book in old Tamil script named 'Thambiraan Vanakkam', thus making Tamil the first Indian language to be printed and published.[115] Tamil Lexicon, published by the University of Madras, is the first among the dictionaries published in any Indian language.[116] During the Indian freedom struggle, many Tamil poets and writers sought to provoke national spirit, social equity and secularist thoughts among the common man, notably Subramanya Bharathy and Bharathidasan.Pongal, also called as Tamizhar Thirunaal (festival of Tamils) or Makara Sankranti elsewhere in India, a four-day harvest festival is one of the most widely celebrated festivals throughout Tamil Nadu.[117] The Tamil language saying Thai Pirandhal Vazhi Pirakkum – literally meaning, the birth of the month of Thai will pave way for new opportunities – is often quoted with reference to this festival. The first day, Bhogi Pongal, is celebrated by throwing away and destroying old clothes and materials by setting them on fire to mark the end of the old and emergence of the new. The second day, Surya Pongal, is the main day which falls on the first day of the tenth Tamil month Thai (14 January or 15 January in western calendar). The third day, Maattu Pongal, is meant to offer thanks to the cattle, as they provide milk and are used to plough the lands. Jallikattu, a bull taming contest, marks the main event of this day. Alanganallur is famous for its Jallikattu[118][119] contest usually held on 3rd day of Pongal. During this final day, Kaanum Pongal – the word "kaanum", means 'to view' in Tamil. In 2011 the Madras High Court Bench ordered the cockfight at Santhapadi and Modakoor Melbegam villages permitted during the Pongal festival while disposing of a petition filed attempting to ban the cockfight.[120]The first month in the Tamil calendar is Chittirai and the first day of this month in mid-April is celebrated as Tamil New Year. The Thiruvalluvar calendar is 31 years ahead of the Gregorian calendar, i.e. Gregorian 2000 is Thiruvalluvar 2031. Aadi Perukku is celebrated on the 18th day of the Tamil month Aadi, which celebrates the rising of the water level in the river Kaveri. Apart from the major festivals, in every village and town of Tamil Nadu, the inhabitants celebrate festivals for the local gods once a year and the time varies from place to place. Most of these festivals are related to the goddess Maariyamman, the mother goddess of rain. Other major Hindu festivals including Deepavali (Death of Narakasura), Ayudha Poojai, Saraswathi Poojai (Dasara), Krishna Jayanthi and Vinayaka Chathurthi are also celebrated. Eid ul-Fitr, Bakrid, Milad un Nabi, Muharram are celebrated by Muslims whereas Christmas, Good Friday, Easter are celebrated by Christians in the state. Mahamagam a bathing festival at Kumbakonam in Tamil Nadu is celebrated once in 12 years. People from all the corners of the country come to Kumbakonam for the festival. This festival is also called as Kumbamela of South.[121][122]In terms of modern cine-music, Ilaiyaraaja was a prominent composer of film music in Tamil cinema during the late 1970s and 1980s. His work highlighted Tamil folk lyricism and introduced broader western musical sensibilities to the south Indian musical mainstream. Tamil Nadu is also the home of the double Oscar Winner A.R. Rahman[123][124][125] who has composed film music in Tamil, Telugu, Hindi films, English and Chinese films. He was once referred to by Time magazine as "The Mozart of Madras".Tamil Nadu is also home to the Tamil film industry nicknamed as "Kollywood", which released the most number of films in India in 2013.[126] The term Kollywood is a portmanteau of Kodambakkam and Hollywood.[127] Tamil cinema is one of the largest industries of film production in India.[128] In Tamil Nadu, cinema ticket prices are regulated by the government. Single screen theatres may charge a maximum of ₹50, while theaters with more than three screens may charge a maximum of ₹120 per ticket.[129] The first silent film in Tamil Keechaka Vadham, was made in 1916.[130] The first talkie was a multi-lingual film, Kalidas, which released on 31 October 1931, barely 7 months after India's first talking picture Alam Ara.[131] Swamikannu Vincent, who had built the first cinema of South India in Coimbatore, introduced the concept of "Tent Cinema" in which a tent was erected on a stretch of open land close to a town or village to screen the films. The first of its kind was established in Madras, called "Edison's Grand Cinemamegaphone". This was due to the fact that electric carbons were used for motion picture projectors.[132]There are more than 30 television channels of various genre in Tamil. DD Podhigai, Doordarshan's Tamil language regional channel was launched on 14 April 1993.[133] The first private Tamil channel, Sun TV was founded in 1993 by Kalanidhi Maran. In Tamil Nadu, the television industry is influenced by politics and majority of the channels are owned by politicians or people with political links.[134] The government of Tamil Nadu distributed free televisions to families in 2006 at an estimated cost ₹3.6 billion (US$50 million) of which has led to high penetration of TV services.[135][136] Cable used to be the preferred mode of reaching homes controlled by government run operator Arasu Cable.[137] From the early 2010s, Direct to Home has become increasingly popular replacing cable television services.[138] Tamil television serials form a major prime time source of entertainment and are directed usually by one director unlike American television series, where often several directors and writers work together.[139]Items that are native to Tamil Nadu are Idli, Dosa, cooked rice, Sambhar, Variety of Chutney, Idiyappam, Vadai Santhakai/Sandhavai, Athirasam, Chakkarai Pongal and Kuli Paniyaram. Salem is renowned for its unique mangoes, Madurai is the place of origin of milk dessert Jigarthanda while Palani is known for its Panchamirtham.[140] Coffee and tea are the staple drinks.[141]For the year 2014–15 Tamil Nadu's GSDP was ₹9.767 trillion (US$140 billion), and growth was 14.86.[142] It ranks third in foreign direct investment (FDI) approvals (cumulative 1991–2002) of ₹ 225.826 billion ($5,000 million), next only to Maharashtra and Delhi constituting 9.12 per cent of the total FDI in the country.[143] The per capita income in 2007–2008 for the state was ₹ 72,993 ranking third among states with a population over 10 million and has steadily been above the national average.[144]According to the 2011 Census, Tamil Nadu is the most urbanised state in India (49 per cent), accounting for 9.6 per cent of the urban population while only comprising 6 per cent of India's total population.[146][147] Services contributes to 45 per cent of the economic activity in the state, followed by manufacturing at 34 per cent and agriculture at 21 per cent. Government is the major investor in the state with 51 per cent of total investments, followed by private Indian investors at 29.9 per cent and foreign private investors at 14.9 per cent. Tamil Nadu has a network of about 113 industrial parks and estates offering developed plots with supporting infrastructure. According to the publications of the Tamil Nadu government the Gross State Domestic Product at Constant Prices (Base year 2004–2005) for the year 2011–2012 is ₹4.281 trillion (US$60 billion), an increase of 9.39 per cent over the previous year. The per capita income at current price is ₹ 72,993.Tamil Nadu has six Nationalised Home Banks which originated in this state; Two government-sector banks Indian Bank and Indian Overseas Bank in Chennai, and Four private-sector banks City Union Bank in Kumbakonam, Karur Vysya Bank, Lakshmi Vilas Bank in Karur, and Tamilnad Mercantile Bank Limited in Tuticorin.Tamil Nadu has historically been an agricultural state and is a leading producer of agricultural products in India. In 2008, Tamil Nadu was India's fifth biggest producer of rice. The total cultivated area in the State was 5.60 million hectares in 2009–10.[148] The Cauvery delta region is known as the Rice Bowl of Tamil Nadu.[149][better source needed] In terms of production, Tamil Nadu accounts for 10 per cent in fruits and 6 per cent in vegetables, in India.[150] Annual food grains production in the year 2007–08 was 10035,000 mt.[148]The state is the largest producer of bananas, turmeric, flowers,[150] tapioca,[150] the second largest producer of mango,[150] natural rubber,[151] coconut, groundnut and the third largest producer of coffee, sapota,[150] Tea[152] and Sugarcane. Tamil Nadu's sugarcane yield per hectare is the highest in India. The state has 17,000 hectares of land under oil palm cultivation, the second highest in India.[153]Dr M.S. Swaminathan, known as the "father of the Indian Green Revolution" was from Tamil Nadu.[154] Tamil Nadu Agricultural University with its seven colleges and thirty two research stations spread over the entire state contributes to evolving new crop varieties and technologies and disseminating through various extension agencies. Among states in India, Tamil Nadu is one of the leaders in livestock, poultry and fisheries production. Tamil Nadu had the second largest number of poultry amongst all the states and accounted for 17.7 per cent of the total poultry population in India.[155] In 2003–2004, Tamil Nadu had produced 3783.6 million of eggs, which was the second highest in India representing 9.37 per cent of the total egg production in the country.[156] With the second longest coastline in India, Tamil Nadu represented 27.54 per cent of the total value of fish and fishery products exported by India in 2006. Namakkal is also one of the major centres of egg production in India. Coimbatore is one of the major centres for poultry production.[157][158]Tamil Nadu is one of the leading States in the textile sector and it houses the country's largest spinning industry accounting for almost 80 per cent of the total installed capacity in India. When it comes to yarn production, the State contributes 40 per cent of the total production in the country. There are 2,614 Hand Processing Units (25 per cent of total units in the country) and 985 Power Processing Units (40 per cent of total units in the country) in Tamil Nadu. According to official data, the textile industry in Tamil Nadu accounts for 17 per cent of the total invested capital in all the industries.[159] Coimbatore is often referred to as the "Manchester of South India" due to its cotton production and textile industries.[160] Tirupur is the country's largest exporter of knitwear.[161][162][163] for its cotton production. The region around Coimbatore, Tirupur, Palladam, Karur and Erode is referred to[by whom?] as the "Textile Valley of India" with the export from the Tirupur ₹ 50,000 million ($1,000 million) and Karur generates around ₹ 35,500 million ($750 million) a year in foreign exchange. Rajapalayam, Gobichettipalayam, Pollachi, Udumalpet, Theni and Vedasandur are known for its cotton spinning mills. Gobichettipalayam is a prominent producer of white silk with the country's first automated silk reeling unit present here. Kanchipuram and Arani are world-famous for their pure silk sarees and hand loom silk weaving industries. Aruppukottai, Salem, and Sathyamangalam are also famous for art-silk sarees. Rajapalayam, Srivilliputhur, Sankarankovil, Andipatti, Tiruchengodu, Paramakudi, Kurinjipadi, Chennimalai, Komarapalayam are major handloom centres. Rajapalayam, Srivilliputhur, Sankarankovil, Negamam, Cinnalapatti, Woraiyur, Pochampalli are famous for its soft cotton saree weaving. Madurai is known for its Chungidi cotton sarees and Bhavani for its cotton carpets.Tamil Nadu has seen major investments in the automobile industry over many decades manufacturing cars, railway coaches, battle-tanks, tractors, motorcycles, automobile spare parts and accessories, tyres and heavy vehicles. Chennai is known as the Detroit of India.[164] Major global automobile companies including BMW, Ford, Robert Bosch, Renault-Nissan, Caterpillar, Hyundai, Mitsubishi Motors, and Michelin as well as Indian automobile majors like Mahindra & Mahindra, Ashok Leyland, Hindustan Motors, TVS Motors, Irizar-TVS, Royal Enfield, MRF, Apollo Tyres, TAFE Tractors, DaimlerChrysler AG Company also invested (₹) 4 billion for establishing new plant in Tamil Nadu.[165] Tamil Nadu is one of the highly industrialised states in India. Over 11% of the S&P CNX 500 conglomerates have corporate offices in Tamil Nadu. Many heavy engineering and manufacturing companies are located in and around the suburbs of Chennai. Bharat Heavy Electricals, one of India's largest electrical equipment manufacturing companies, has manufacturing plants at Tiruchirapalli and Ranipet. India's leading steel producer, the state-owned Steel Authority of India has a steel plant in Salem. Sterlite Industries has a copper smelter at Tuticorin and an aluminium plant in Mettur. The Chennai Petroleum Corporation is a state-owned oil and gas corporation headquartered in Chennai, and owns refineries at Manali and Panangudi. The state government owns the Tamil Nadu Newsprint and Papers,[166] in Karur. Jointly with the Tata Group, the state owns the world's sixth largest manufacturer of watches, under the brand name of Titan, at Hosur. A number of large cement manufacturers, including the Chettinad Group, Ramco Cements, Tancem, the Dalmia Group, UltraTech Cements and ACC are present across the state.Coimbatore is also referred to as "the Pump City" as it supplies two-thirds of India's requirements of motors and pumps. The city is one of the largest exporters of wet grinders and auto components and the term "Coimbatore Wet Grinder" has been given a Geographical indication.[167]Electronics manufacturing is a growing industry in Tamil Nadu, with many international companies like Nokia, Flextronics, Motorola, Sony-Ericsson, Foxconn, Samsung, Cisco, Moser Baer, Lenovo, Dell, Sanmina-SCI, Texas Instruments having chosen Chennai as their south Asian manufacturing hub. Products manufactured include circuit boards and cellular phone handsets.[168]Tamil Nadu is the second largest software exporter by value in India. Software exports from Tamil Nadu grew from ₹ 76 billion ($1.6 billion) in 2003–04 to ₹ 207 billion {$5 billion} by 2006–07 according to NASSCOM[169] and to ₹ 366 billion in 2008–09 which shows 29 per cent growth in software exports according to STPI. Major national and global IT Companies such as Syntel, Infosys, Wipro, HCL, Tata Consultancy Services, Verizon, Hewlett-Packard, Bosch, Amazon.com, eBay, PayPal, IBM, Accenture, Ramco Systems, DXC Technology, Cognizant Technology solutions, Tech Mahindra, Polaris, Aricent, MphasiS, Mindtree, Hexaware Technologies and many others have offices in Tamil Nadu. The top engineering colleges in Tamil Nadu have been a major recruiting hub for the IT firms. According to estimates, about 50 per cent of the HR required for the IT and ITES industry was being sourced from the State.[170] Coimbatore is the second largest software producer in the state, next to Chennai.[171]Tamil Nadu has a transportation system that connects all parts of the state. Tamil Nadu is served by an extensive road network, providing links between urban centres, agricultural market-places and rural areas. There are 29 national highways in the state, covering a total distance of 5,006.14 km (3,110.67 mi).[172][173] The state is also a terminus for the Golden Quadrilateral project, that connects Indian metropolises like (New Delhi, Mumbai, Bengaluru, Chennai and Kolkata). The state has a total road length of 167,000 km (104,000 mi), of which 60,628 km (37,672 mi) are maintained by Highways Department. This is nearly 2.5 times higher than the density of all-India road network.[174] The major road junctions are Chennai, Vellore, Madurai, Trichy, Coimbatore, Salem, Tirunelveli, Tuticorin, Karur, Krishnagiri, Dindigul and Kanniyakumari. Road transport is provided by state owned Tamil Nadu State Transport Corporation and State Express Transport Corporation. Almost every part of state is well connected by buses 24 hours a day. The State accounted for 13.6 per cent of all accidents in the country With 66,238 accidents in 2013, 11.3 per cent of all road accident deaths and 15 per cent of all road-related injuries, according to data provided by the Ministry of Road Transport and Highways. Although Tamil Nadu accounts for the highest number of road accidents in India, it also leads in having reduced the number of fatalities in accident-prone areas with deployment of personnel and a sustained awareness campaign. The number of deaths at areas decreased from 1,053 in 2011 to 881 in 2012 and 867 in 2013.[175]Tamil Nadu has a well-developed rail network as part of Southern Railway. Headquartered at Chennai, the Southern Railway network extends over a large area of India's southern peninsula, covering the states of Tamil Nadu, Kerala, Puducherry, a small portion of Karnataka and a small portion of Andhra Pradesh. Express trains connect the state capital Chennai with Mumbai, Delhi and Kolkata. Chennai Central is gateway for train towards north whereas Chennai Egmore serves as gateway for south. Tamil Nadu has a total railway track length of 5,952 km (3,698 mi) and there are 532 railway stations in the state. The network connects the state with most major cities in India. The Nilgiri Mountain Railway is one of the UNESCO World Heritage Site connecting Ooty on the hills and Mettupalayam in the foot hills which is in turn connected to Coimbatore city. The centenary old Pamban Bridge over sea connecting Rameswaram in Pamban island to mainland is an engineering marvel. It is one of the oldest cantilever bridges still in operation, the double-leaf bascule bridge section can be raised to let boats and small ships pass through Palk Strait in Indian Ocean. Chennai has a well-established suburban railway network and is constructing a Chennai Metro with phase1 operational since July 2015 . Major railway junctions( 4 & above lines ) in the state are Chennai, Coimbatore, Katpadi, Madurai, Salem, Erode, Dindigul, Karur, Nagercoil, Tiruchirapalli and Tirunelveli. Chennai Central, Madurai Junction, Katpadi Junction, Chennai Egmore, Salem Junction, Tiruchirappalli Junction, Coimbatore Junction are upgraded to A1 grade level. Loco sheds are located at Erode, Arakkonam, Royapuram in Chennai and Tondaiyarpet in Chennai, Ponmalai (GOC) in Tiruchirappalli as Diesel Loco Shed. The loco shed at Erode is a huge composite Electric and Diesel Loco shed. MRTS which covers from Chennai Beach to Velachery, and metro rail also running between Alandur and koyambedu station.Tamil Nadu has four international airports namely Chennai International Airport, Coimbatore International Airport, Tiruchirapalli International Airport and Madurai International Airport. Salem Airport and Tuticorin Airport are domestic airports. Chennai International Airport is a major international airport and aviation hub in South Asia. Besides civilian airports, the state has four air bases of the Indian Air Force namely Thanjavur AFS, Tambram AFS, Coimbatore AFS and two naval air stations INS Rajali and INS Parundu of Indian Navy.Tamil Nadu has three major seaports located at Chennai, Ennore and Tuticorin, as well as seven other minor ports including Cuddalore and Nagapattinam.[148] Chennai Port is an artificial harbour situated on the Coromandel Coast and is the second principal port in the country for handling containers. Ennore Port handles all the coal and ore traffic in Tamil Nadu. The volume of cargo in the ports grew by 13 per cent during 2005.[176]Tamil Nadu has the third largest installed power generation capacity in the country. The Kalpakkam Nuclear Power Plant, Ennore Thermal Plant, Neyveli Lignite Power Plant, many hydroelectric plants including Mettur Dam, hundreds of windmills and the Narimanam Natural Gas Plants are major sources of Tamil Nadu's electricity. Tamil Nadu generates a significant proportion of its power needs from renewable sources with wind power installed capacity at over 7154 MW,[177] accounting for 38 per cent of total installed wind power in India .[178] It is presently adding the Koodankulam Nuclear Power Plant to its energy grid, which on completion would be the largest atomic power plant in the country with 2000MW installed capacity.[179] The total installed capacity of electricity in the State by January 2014 was 20,716 MW.[180] Tamil Nadu ranks first nationwide in diesel-based thermal electricity generation with a national market share of over 34 per cent.[181] From a power surplus state in 2005–06, Tamil Nadu has become a state facing severe power shortage over the recent years due to lack of new power generation projects and delay in the commercial power generation at Kudankulam Atomic Power Project. The Tuticorin Thermal Power Station has five 210 megawatt generators. The first generator was commissioned in July 1979. The thermal power plants under construction include the coal-based 1000 MW NLC TNEB Power Plant. From the current 17MW installed Solar power, Tamil Nadu government's new policy aims to increase the installed capacity to 3000MW by 2016.[182]Kabbadi, is recognised as the state game in Tamil Nadu.[183] The traditional sport of Tamil Nadu include Silambam,[184] a Tamil martial arts played with a long bamboo staff, Cockfight, Jallikattu,[185] a bull taming sport famous on festival occasions, ox-wagon racing known as Rekkala,[186][184] Kite flying also known as Pattam viduthal,[185] Goli, the game with marbles,[185] Aadu Puli, the "goat and tiger" game[185] and Kabaddi also known as Sadugudu.[185] Most of these traditional sports are associated with festivals of land like Thai Pongal and mostly played in rural areas.[185] In urban areas of Tamil Nadu, modern sports like bat and ball games are played.[185] S. Ilavazhagi carrom world champion from 2002–2016The M. A. Chidambaram Stadium in Chennai is an international cricket ground with a capacity of 50,000 and houses the Tamil Nadu Cricket Association.[187] Srinivasaraghavan Venkataraghavan,[188] Krishnamachari Srikkanth,[189] Laxman Sivaramakrishnan,[190] Sadagoppan Ramesh, Laxmipathy Balaji,[191] Murali Vijay,[192] Ravichandran Ashwin[193] and Dinesh Karthik are some prominent cricketers from Tamil Nadu. The MRF Pace Foundation in Chennai is a popular fast bowling academy for pace bowlers all over the world. Cricket contests between local clubs, franchises and teams are popular in the state. Chennai Super Kings represent the city of Chennai in the Indian Premier League, a popular Twenty20 league. The Super Kings are the most successful team in the league with three IPL titles at par with Mumbai Indians and two CLT20 titles.Tennis is also a popular sport in Tamil Nadu with notable international players including Ramesh Krishnan,[194] Ramanathan Krishnan,[194] Vijay Amritraj[195] and Mahesh Bhupathi. Nirupama Vaidyanathan, the first Indian women to play in a grandslam tournament also hails from the state. The ATP Chennai Open tournament is held in Chennai every January. The Sports Development Authority of Tamil Nadu (SDAT) owns Nungambakkam tennis stadium which hosts Chennai Open and Davis Cup play-off tournaments.The Tamil Nadu Hockey Association is the governing body of Hockey in the state. Vasudevan Baskaran was the captain of the Indian team that won gold medal in 1980 Olympics at Moscow. The Mayor Radhakrishnan Stadium in Chennai hosts international hockey events and is regarded by the International Hockey Federation as one of the best in the world for its infrastructure.[196]Tamil Nadu also has Golf ground in Coimbatore, The Coimbatore Golf Club is an 18-hole golf course located in a place called Chettipalayam in Coimbatore, located within the city limits in the state of Tamil Nadu in India. The Club is also a popular venue for major Golf Tournaments held in India.The Sports Development Authority of Tamil Nadu (SDAT), a government body, is vested with the responsibility of developing sports and related infrastructure in the state.[197] The SDAT owns and operates world class stadiums and organises sporting events.[198] It also accommodates sporting events, both at domestic and international level, organised by other sports associations at its venues. The YMCA College of Physical Education at Nandanam in Chennai was established in 1920 and was the first college for physical education in Asia. The Jawaharlal Nehru Stadium in Chennai is a multi-purpose stadium hosting football and track & field events. The Indian Triathlon Federation and the Volleyball Federation of India are headquartered in Chennai. Chennai hosted India's first ever International Beach Volleyball Championship in 2008. The SDAT – TNSRA Squash Academy in Chennai is one of the very few academies in south Asia hosting international squash events.Jawaharlal Nehru Stadium in Coimbatore, it is a football stadium and also a multi-purpose stadium in Coimbatore constructed in 1971.The tourism industry of Tamil Nadu is the largest in India, with an annual growth rate of 16 per cent. Tourism in Tamil Nadu is promoted by Tamil Nadu Tourism Development Corporation (TTDC), a Government of Tamil Nadu undertaking. According to Ministry of Tourism statistics, 4.68 million foreign (20.1% share of the country) and 333.5 million domestic tourists (23.3% share of the country) visited the state in 2015 making it the most visited state in India both domestic and foreign tourists.[199] The state boasts some of the grand Hindu temples built in Dravidian architecture. The Brihadishwara Temple in Thanjavur, Gangaikonda Cholapuram and the Airavatesvara Temple in Darasuram built by the Cholas and the Shore Temple along with the collection of other monuments in Mahabalipuram (also called Mamallapuram) have been declared as UNESCO World Heritage Sites.[200][201] Erwadi in Ramanathapuram district is one of the major Islamic tourist attraction site.
Inosculation
Inosculation is a natural phenomenon in which trunks, branches or roots of two trees grow together. It is biologically similar to grafting and such trees are referred to in forestry terms as Gemels from the Latin word meaning 'A pair'.[1]It is most common for branches of two trees of the same species to grow together, though inosculation may be noted across related species. The branches first grow separately in proximity to each other until they touch. At this point, the bark on the touching surfaces is gradually abraded away as the trees move in the wind. Once the cambium of two trees touches, they sometimes self-graft and grow together as they expand in diameter. Inosculation customarily results when tree limbs are braided or pleached.The term "inosculation" is also used in the context of plastic surgery, as one of the three mechanisms by which skin grafts take at the host site. Blood vessels from the recipient site are believed to connect with those of the graft in order to restore vascularity.Inosculation is most common among the following species of tree[citation needed] due to their thin bark.Two trees may grow to their mature size adjacent to each other and seemingly grow together or conjoin, demonstrating the process of inosculation. These may be of the same species or even trees of two different genera or families, depending on whether the two trees have become truly grafted together (once the cambium of two trees touches, they self-graft and grow together) or not. Usually grafting is only between two trees of the same or closely related species or genera, but the appearance of grafting can be given by two trees that are physically touching, rubbing, intertwined, or entangled.[2] Both conifers and deciduous trees can become conjoined. Beech trees in particular are frequent conjoiners, as is blackthorn (Prunus spinosa).Such trees are often colloquially referred to as "husband and wife" trees, or "marriage trees". The straightforward application of the term comes from the obvious unification of two separate individual trees, although a more humorous use of the term relates to the suggestive appearance of some natural examples. A degree of religious intent may also exist, as some cults are organized around beliefs that trees contain a hidden or sacred power to cure or to enhance fertility, or that they contain the souls of ancestors or of the unborn.[3]At the ruined Lynncraigs Farm, Dalry, North Ayrshire in Scotland, a blackthorn (Prunus spinosa), standing in the old farm garden, shows signs of having been deliberately grafted.On his Tour of Scotland, published in 1800, T. Garnett notes a tree near Inveraray that the locals called the Marriage tree, formed from a lime tree with two trunks that have been joined by a branch in the manner of a person putting an arm around another (see illustration) as would a married couple.[4]On the way to the Heavenly Lake near Urumqi in China are a pair of trees that local people have called the Husband and Wife trees because they are connected by a living branch.[5] The Tatajia Husband and Wife trees are in Taiwan[6] and in Yakushima, Kagoshima-ken, Japan, are a pair of Husband and Wife trees formed from conjoined cedars.[7]In Lambeg, Co. Down, slightly north of Wolfenden's Bridge, stand two beech trees (see 'Gallery') at the entrance to Chrome Hill, on the Lambeg to Ballyskeagh road. In the late 18th century, John Wesley was staying at Chrome Hill and decided to weave together two young beech trees to act as a symbol of unity between the Methodist Church and the Church of Ireland.The John Wesley tree.Conjoined sycamore maplesAcer pseudoplatanus showing inosculationBeech tree trunks conjoinedGarnett's 18th century 'Marriage tree'Inosculation of willow (Salix sp.)A fig treeConjoined beech tree rootsThe classic Husband and Wife tree with branches conjoinedFused chestnut trees showing a bark pocket (Castanea sativa)Fused ash trees (Fraxinus excelsior)Fused ash trees, side view (Fraxinus excelsior)Fused sycamore trees (Platanus occidentalis)(In Vermont)
Crown sprouting
Crown sprouting is the ability of a plant to regenerate its shoot system after destruction (usually by fire) by activating dormant vegetative structures to produce regrowth from the root crown (the junction between the root and shoot portions of a plant).[1] These dormant structures take the form of lignotubers or basal epicormic buds. Plant species that can accomplish crown sprouting are called crown resprouters (distinguishing them from stem or trunk resprouters) and, like them, are characteristic of fire-prone habitats such as chaparral.[2]In contrast to plant fire survival strategies that decrease the flammability of the plant, or by requiring heat to germinate, crown sprouting allows for the total destruction of the above ground growth. Crown sprouting plants typically have extensive root systems in which they store nutrients allowing them to survive during fires and sprout afterwards. Early researchers suggested that crown sprouting species might lack species genetic diversity; however, research on Gondwanan shrubland[clarification needed] suggests that crown sprouting species have similar genetic diversity to seed sprouters.[3] Some genera, such as Arctostaphylos and Ceanothus, have species that are both resprouters and not, both adapted to fire.California Buckeye, Aesculus californica, is an example of a western United States tree which can regenerate from its root crown after a fire event, but can also regenerate by seed.[4]
Cinchona
about 38 species; see textCinchona /sɪŋˈkoʊnə/[1] is a genus of flowering plants in the family Rubiaceae containing at least 23 species of trees and shrubs. They are native to the tropical Andean forests of western South America. A few species are reportedly naturalized in Central America, Jamaica, French Polynesia, Sulawesi, Saint Helena in the South Atlantic, and São Tomé and Príncipe off the coast of tropical Africa. Several species were sought after for their medicinal value and cultivated in India and Java where they also formed hybrids. The barks of several species yield quinine and other alkaloids that were the only effective treatments against malaria during the height of colonialism which made them of great economic and political importance. The synthesis of quinine in 1944, an increase in resistant forms of malaria, and alternate therapies ended the large-scale economic interest in their cultivation. Academic interest continues as cinchona alkaloids show promise in treating falciparum malaria which has evolved resistance to synthetic drugs.Carl Linnaeus named the genus in 1742 based on a claim, first recorded by the Italian physician Sebastiano Bado in 1663, that the plant had cured the wife of the Luis Jerónimo de Cabrera, 4th Count of Chinchón, Count of Chinchón, a viceroy in Lima. While the veracity of the claims and the details are highly debated leaving it best treated as a legend, the curative properties were known even earlier. The history of the plant, their extracts, and the cures are however highly confused and controversial. Suggestions that the plant went by the native name of Quina Quina which yielded Quina bark have been questioned. Other fever cures from South America were known as Jesuit's Bark and Jesuit's Powder in Europe earlier but although they have been traced to Cinchona, there is evidence of materials being derived from other species such as Myroxylon. The species that Linnaeus used to describe the genus was Cinchona officinalis which is found only in a small region in Ecuador and specimens of which were obtained by Charles Marie de La Condamine around 1735. This species is however of little medicinal significance. In the course of the quest for species yielding effective remedies, numerous species were described, some now considered invalid or synonyms of others. Linnaeus used the Italian spelling used by Bado but the name Chinchón (pronounced [tʃinˈtʃon] in Spanish) led to Clements Markham and others proposing a correction of the spelling to Chinchona and some prefer the pronunciation /tʃɪnˈtʃoʊnə/ for the common name of the plant.The national tree of Peru is in the genus Cinchona.[2]Cinchona plants belong to the family Rubiaceae and are large shrubs or small trees with evergreen foliage, growing 5–15 m (16–49 ft) in height. The leaves are opposite, rounded to lanceolate and 10–40 cm long. The flowers are white, pink or red, produced in terminal panicles. The fruit is a small capsule containing numerous seeds. A key character of the genus is that the flowers have marginally hairy corolla lobes. The tribe Cinchoneae includes other genera Cinchonopsis, Jossia, Ladenbergia, Remijia, Stilpnophyllum, and Ciliosemina.[3] In South America, the species had geographically distinct distributions. The introduction of several species into cultivation in the same areas in India and Java, respectively, by the English and Dutch East India Companies led to the formation of hybrids.[4]Linnaeus described the genus based on the species Cinchona officinalis.[5][6] Nearly 300 species have been described and named in the genus but a revision of the genus in 1998 identified 23 distinct species.[4][7]The febrifugal properties of bark from trees now known to be in the genus Cinchona were used by many South American cultures[8] but malaria was an Old World disease that was introduced into the Americas by Europeans only after 1492. The origins and claims to the use of febrifugal barks and powders in Europe, especially those used against malaria, were disputed even in the 17th century. Jesuits played a key role in the transfer of remedies from the New World. The traditional story,[9] first recorded by Sebastiano Bado in 1663, is that the wife of the fourth Count of Chinchon fell ill in Lima with a tertian fever. A Spanish governor advised a traditional remedy which was tried, resulting in a miraculous and rapid cure. The Countess then ordered a large quantity of the bark and took it back to Europe. Bado claimed to have received this information from an Italian named Antonius Bollus who was a merchant in Peru. Clements Markham identified the Countess as Ana de Osorio but this was shown to be incorrect by Haggis. Ana de Osorio married the Count in August 1621 and died in 1625, even before the Count was appointed Viceroy of Peru in 1628. It was his second wife, Francisca Henriques de Ribera, who accompanied him to Peru. Haggis further examined the diaries of the Count of Chinchon and found no mention of the Countess suffering from fever although the Count himself had many malarial attacks. On account of numerous other discrepancies this is best treated as a legend. Quina bark was mentioned by Fray Antonio de La Calancha in 1638 as coming from a tree in Loja (Loxa). He noted that bark powder weighing about two coins was cast into water and drunk to cure fevers and "tertians". Jesuit Father Bernabé Cobo (1582–1657) also wrote on the "fever tree" in 1653. The legend was popularized in English literature by Markham in his writings and in 1874 he also published a "plea for the correct spelling of the genus Chinchona".[10][11] A Spanish physician, Juan Fragoso wrote of bark powder from an unknown tree in 1600 that was used for treating various ills. Nicolas Monardes also wrote of a New World bark powder used in Spain in 1574. Both identify the sources as trees that do not bear fruit and having heart-shaped leaves  and it has been suggested that these references could be to Cinchona species.[12] The name Quina-Quina or Quinquina was suggested as an old name for Cinchona used in Europe and based on the native name used by the Quechua people. Italian sources spelt Quina as Cina which was a source of confusion with Smilax from China.[13] Haggis argued that Qina and Jesuit's bark actually referred to Myroxylon peruiferum or Peruvian balsam and that this was an item of importance in Spanish trade in the 1500s. Over time, the bark of the Myroxylon was adulterated with the similar looking bark of what we now know as Cinchona.[14] Gradually the adulterant became the main product that was the key therapeutic ingredient used in malarial therapy. The bark was included as Cortex Peruanus in the London Pharmacopoeia in 1677. The "fever tree" was finally described carefully by the astronomer Charles Marie de la Condamine who visited Quito in 1735 on a quest to measure an arc of the meridian. The species he described, Cinchona officinalis, was however found to be of little therapeutic value. The first living plants seen in Europe were C. calisaya plants grown at the Jardin des Plantes from seeds collected by Hugh Algernon Weddell from Bolivia in 1846.[15] José Celestino Mutis, physician to the Viceroy of Nueva Granada, Pedro Messia de la Cerda gathered information on cinchona in Colombia from 1760 and wrote a manuscript El Arcano de la Quina (1793) with illustrations. He proposed a Spanish expedition to search for plants of commercial value which was approved in 1783 and was continued after his death in 1808 by his nephew Sinforoso Mutis.[16] As demand for the bark increased the trees in the forests began to be destroyed. To maintain their monopoly on cinchona bark, Peru and surrounding countries began outlawing the export of cinchona seeds and saplings beginning in the early 19th century.[17]The Colonial European powers considered growing the plant in other tropical parts. The French mission of 1743, of which de la Condamine was member, lost their plants when a wave took them off their ship. The Dutch sent Justus Hasskarl who brought plants that were then cultivated in Java from 1854. The English explorer Clements Markham went to collect plants that were introduced in Sri Lanka and the Nilgiris of southern India in 1860.[18] The main species introduced were Cinchona succirubra or red bark, as its sap turned red on contact with air, and Cinchona calisaya. The alkaloids quinine and cinchonine were extracted by Pelletier and Caventou in 1820. Later two more key alkaloids, quinidine and cinchonidine were identified and it became a routine in quinology to examine the contents of these components in assays. The yields of quinine in the cultivated trees were low and it took a while to develop sustainable methods to extract bark. In the meantime Charles Ledger and his native assistant Manuel collected another species from Bolivia. Manuel was caught and beaten by Bolivian officials leading to his death but Ledger obtained seeds of high quality which were offered to the British who were uninterested, leading to the rest being sold to the Dutch. The Dutch saw its value and multiplied the stock. The species later named as Cinchona ledgeriana[19] had a yield of 8 to 13 percent quinine in bark grown in Dutch Indonesia which effectively out-competed the British Indian production. It was only later that the English saw the value and sought to obtain the seeds of C. ledgeriana from the Dutch.[20][21]During World War II, the Japanese conquered Java and the United States lost access to the cinchona plantations that supplied war-critical quinine medication. Botanical expeditions – called Cinchona Missions[22] – were launched in 1942-1944 to explore promising areas of South America in an effort to locate cinchona species that contained quinine and could be harvested for quinine production.[22] While ultimately successful in their primary aim, these expeditions also identified new species of plants[22] and created a new chapter in international relations between the United States and other nations in the Americas.[23]Francesco Torti used the response of fevers to treatment with cinchona as a system of classification of fevers or a means for diagnosis. The use of cinchona in the effective treatment of malaria brought an end to treatment by bloodletting and long-held ideas of humorism from Galen.[24]For his part in obtaining and helping the establishment of cinchona in British India Clements Markham was knighted. For the role in establishing cinchona in Indonesia, Hasskarl was knighted with the Dutch order of the Lion.[25]Cinchona species are used as food plants by the larvae of some Lepidoptera species, including the engrailed, the commander, and members of the genus Endoclita, including E. damor, E. purpurescens and E. sericeus.Cinchona pubescens has grown uncontrolled on some islands such as the Galapagos where it has posed the risk of outcompeting native plant species.[26]In herbalism, cinchona bark was used as an adulterant in Jesuit's bark or Peruvian bark which originally is thought to have referred to Myroxylon peruiferum, another fever remedy. The bark of cinchona can be harvested in a number of ways. One approach was to cut the tree but this and girdling are equally destructive and unsustainable so small strips were cut and various techniques such as "mossing", the application of moss to the cut areas, were used to allow the tree to heal. Other approaches involved coppicing and chopping of side branches which were then stripped of bark. The bark was dried into what were called quills and then powdered for medicinal uses. The bark contains alkaloids, including quinine and quinidine.[citation needed] Cinchona is the only economically practical source of quinine, a drug that is still recommended for the treatment of falciparum malaria.[27][28]The Italian botanist Pietro Castelli wrote a pamphlet noteworthy as being the first Italian publication to mention the cinchona. By the 1630s (or 1640s, depending on the reference), the bark was being exported to Europe. In the late 1640s, the method of use of the bark was noted in the Schedula Romana.English King Charles II called upon Robert Talbor, who had become famous for his miraculous malaria cure.[29] Because at that time the bark was in religious controversy, Talbor gave the king the bitter bark decoction in great secrecy. The treatment gave the king complete relief from the malaria fever. In return, Talbor was offered membership of the prestigious Royal College of Physicians.[30]In 1679, Talbor was called by the King of France, Louis XIV, whose son was suffering from malaria fever. After a successful treatment, Talbor was rewarded by the king with 3,000 gold crowns and a lifetime pension for this prescription. Talbor was asked to keep the entire episode secret. After Talbor's death, the French king published this formula: seven grams of rose leaves, two ounces of lemon juice and a strong decoction of the cinchona bark served with wine. Wine was used because some alkaloids of the cinchona bark are not soluble in water, but are soluble in the ethanol in wine.[30] In 1681 Água de Inglaterra was introduced into Portugal from England by Dr. Fernando Mendes who, similarly, “received a handsome gift from (King Pedro) on condition that he should reveal to him the secret of its composition and withhold it from the public”.[31]In 1738, Sur l'arbre du quinquina, a paper written by Charles Marie de La Condamine, lead member of the expedition, along with Pierre Godin and Louis Bouger that was sent to Ecuador to determine the length of a degree of the 1/4 of meridian arc in the neighbourhood of the equator, was published by the French Academy of Sciences. In it he identified three separate species.[32]The birth of homeopathy was based on cinchona bark testing. The founder of homeopathy, Samuel Hahnemann, when translating William Cullen's Materia medica, noticed Cullen had written that Peruvian bark was known to cure intermittent fevers.[33] Hahnemann took daily a large, rather than homeopathic, dose of Peruvian bark. After two weeks, he said he felt malaria-like symptoms. This idea of "like cures like" was the starting point of his writings on homeopathy. Hahnemann's symptoms have been suggested by researchers, both homeopaths and skeptics, as being an indicator of his hypersensitivity to quinine.[34]The bark was very valuable to Europeans in expanding their access to and exploitation of resources in distant colonies and at home. Bark gathering was often environmentally destructive, destroying huge expanses of trees for their bark, with difficult conditions for low wages that did not allow the indigenous bark gatherers to settle debts even upon death.[35]Further exploration of the Amazon Basin and the economy of trade in various species of the bark in the 18th century is captured by Lardner Gibbon:"...this bark was first gathered in quantities in 1849, though known for many years. The best quality is not quite equal to that of Yungas, but only second to it. There are four other classes of inferior bark, for some of which the bank pays fifteen dollars per quintal. The best, by law, is worth fifty-four dollars. The freight to Arica is seventeen dollars the mule load of three quintals. Six thousand quintals of bark have already been gathered from Yuracares. The bank was established in the year 1851. Mr. [Thaddäus] Haenke mentioned the existence of cinchona bark on his visit to Yuracares in 1796". (Source: Exploration of the Valley of the Amazon, by Lieut. Lardner Gibbon, USN. Vol.II, Ch.6, pp. 146-47.)The cultivation of cinchona led from the 1890s to a decline in the price of quinine but the quality and production of raw bark by the Dutch in Indonesia led them to dominate world markets. The producers of processed drugs in Europe (especially Germany[36]) however bargained and caused fluctuations in prices which led to a Dutch-led Cinchona Agreement in 1913 that ensured a fixed price for producers. A Kina Bureau in Amsterdam regulated this trade.[37]It was estimated that the British Empire incurred direct losses of 52 to 62 million pounds a year due to malaria sickness each year. It was therefore of great importance to secure the supply of the cure.[38] In 1860, a British expedition to South America led by Clements Markham smuggled back cinchona seeds and plants, which were introduced in several areas of British India and Sri Lanka. In India, it was planted in Ootacamund by William Graham McIvor. In Sri Lanka, it was planted in the Hakgala Botanical Garden in January 1861.[39] James Taylor, the pioneer of tea planting in Sri Lanka, was one of the pioneers of cinchona cultivation.[40] By 1883, about 64,000 acres (260 km2) were in cultivation in Sri Lanka, with exports reaching a peak of 15 million pounds in 1886. The cultivation (initially of Cinchona succirubra and later of C. calisaya[41]) was extended through the work of George King and others into the hilly terrain of Darjeeling District of Bengal. Cinchona factories were established at Naduvattam in the Nilgiris and at Mungpoo, Darjeeling, West Bengal. Quinologists were appointed to oversee the extraction of alkaloids with John Broughton in the Nilgiris and C.H. Wood at Darjeeling. Others in the position included David Hooper and John Eliot Howard.[21][42][43]In 1865, "New Virginia" and "Carlota Colony" were established in Mexico by Matthew Fontaine Maury, a former confederate in the American Civil War. Postwar confederates were enticed there by Maury, now the "Imperial Commissioner of Immigration" for  Emperor Maximillian of Mexico, and Archduke of Habsburg. All that survives of those two colonies are the flourishing groves of cinchonas established by Maury using seeds purchased from England. These seeds were the first to be introduced into Mexico.[44]The bark of trees in this genus is the source of a variety of alkaloids, the most familiar of which is quinine, an antipyretic (antifever) agent especially useful in treating malaria.[45][46]Cinchona alkaloids include:They find use in organic chemistry as organocatalysts in asymmetric synthesis.Alongside the alkaloids, many cinchona barks contain cinchotannic acid, a particular tannin, which by oxidation rapidly yields a dark-coloured phlobaphene[47] called red cinchonic,[48] cinchono-fulvic acid or cinchona red.[49]In 1934, efforts to make malaria drugs cheap and effective for use across countries led to the development of a standard called "totaquina" proposed by the Malaria Commission of the League of Nations. Totaquina required a minimum of 70% crystallizable alkaloids of which at least 15% was to be quinine with not more than 20% amorphous alkaloids.[50][51]There are at least 24 species recognized by botanists.[4][52] There are likely several unnamed species and many intermediate forms that have arisen due to the plants' tendency to hybridize.[7]
Maple syrup
Maple syrup is a syrup usually made from the xylem sap of sugar maple, red maple, or black maple trees, although it can also be made from other maple species. In cold climates, these trees store starch in their trunks and roots before winter; the starch is then converted to sugar that rises in the sap in late winter and early spring. Maple trees are tapped by drilling holes into their trunks and collecting the exuded sap, which is processed by heating to evaporate much of the water, leaving the concentrated syrup.Maple syrup was first collected and used by the indigenous peoples of North America, and the practice was adopted by European settlers, who gradually refined production methods. Technological improvements in the 1970s further refined syrup processing. The Canadian province of Quebec is by far the largest producer, responsible for 70 percent of the world's output; Canadian exports of maple syrup in 2016 were C$ 487 million (about US$ 360 million), with Quebec accounting for some 90 percent of this total.[1][2]Maple syrup is graded according to the Canada, United States, or Vermont scales based on its density and translucency. Sucrose is the most prevalent sugar in maple syrup. In Canada, syrups must be made exclusively from maple sap to qualify as maple syrup and must also be at least 66 percent sugar.[3] In the United States, a syrup must be made almost entirely from maple sap to be labelled as "maple", though states such as Vermont and New York have more restrictive definitions.Maple syrup is often used as a condiment for pancakes, waffles, French toast, oatmeal or porridge. It is also used as an ingredient in baking and as a sweetener or flavouring agent. Culinary experts have praised its unique flavour, although the chemistry responsible is not fully understood.[4]Three species of maple trees are predominantly used to produce maple syrup: the sugar maple (Acer saccharum), the black maple (A. nigrum), and the red maple (A. rubrum),[5] because of the high sugar content (roughly two to five percent) in the sap of these species.[6] The black maple is included as a subspecies or variety in a more broadly viewed concept of A. saccharum, the sugar maple, by some botanists.[7] Of these, the red maple has a shorter season because it buds earlier than sugar and black maples, which alters the flavour of the sap.[8]A few other (but not all) species of maple (Acer) are also sometimes used as sources of sap for producing maple syrup, including the box elder or Manitoba maple (Acer negundo),[9] the silver maple (A. saccharinum),[10] and the bigleaf maple (A. macrophyllum).[11] Similar syrups may also be produced from walnut, birch or palm trees, among other sources.[12][13][14]Indigenous peoples living in northeastern North America were the first groups known to have produced maple syrup and maple sugar. According to aboriginal oral traditions, as well as archaeological evidence, maple tree sap was being processed into syrup long before Europeans arrived in the region.[15][16] There are no authenticated accounts of how maple syrup production and consumption began,[17] but various legends exist; one of the most popular involves maple sap being used in place of water to cook venison served to a chief.[16] Other stories credit the development of maple syrup production to Nanabozho, Glooskap, or the squirrel. Aboriginal tribes developed rituals around sugar-making, celebrating the Sugar Moon (the first full moon of spring) with a Maple Dance.[18] Many aboriginal dishes replaced the salt traditional in European cuisine with maple sugar or syrup.[16]The Algonquians recognized maple sap as a source of energy and nutrition. At the beginning of the spring thaw, they used stone tools to make V-shaped incisions in tree trunks; they then inserted reeds or concave pieces of bark to run the sap into buckets, which were often made from birch bark.[17] The maple sap was concentrated either by dropping hot cooking stones into the buckets[19] or by leaving them exposed to the cold temperatures overnight and disposing of the layer of ice that formed on top.[17]In the early stages of European colonization in northeastern North America, local Indigenous peoples showed the arriving colonists how to tap the trunks of certain types of maples during the spring thaw to harvest the sap.[20] André Thevet, the "Royal Cosmographer of France", wrote about Jacques Cartier drinking maple sap during his Canadian voyages.[21] By 1680, European settlers and fur traders were involved in harvesting maple products.[22] However, rather than making incisions in the bark, the Europeans used the method of drilling tapholes in the trunks with augers. During the 17th and 18th centuries, processed maple sap was used primarily as a source of concentrated sugar, in both liquid and crystallized-solid form, as cane sugar had to be imported from the West Indies.[17][18]Maple sugaring parties typically began to operate at the start of the spring thaw in regions of woodland with sufficiently large numbers of maples.[20] Syrup makers first bored holes in the trunks, usually more than one hole per large tree; they then inserted wooden spouts into the holes and hung a wooden bucket from the protruding end of each spout to collect the sap. The buckets were commonly made by cutting cylindrical segments from a large tree trunk and then hollowing out each segment's core from one end of the cylinder, creating a seamless, watertight container.[17] Sap filled the buckets, and was then either transferred to larger holding vessels (barrels, large pots, or hollowed-out wooden logs), often mounted on sledges or wagons pulled by draft animals, or carried in buckets or other convenient containers.[23] The sap-collection buckets were returned to the spouts mounted on the trees, and the process was repeated for as long as the flow of sap remained "sweet". The specific weather conditions of the thaw period were, and still are, critical in determining the length of the sugaring season.[24] As the weather continues to warm, a maple tree's normal early spring biological process eventually alters the taste of the sap, making it unpalatable, perhaps due to an increase in amino acids.[10]The boiling process was very time-consuming. The harvested sap was transported back to the party's base camp, where it was then poured into large vessels (usually made from metal) and boiled to achieve the desired consistency.[17] The sap was usually transported using large barrels pulled by horses or oxen to a central collection point, where it was processed either over a fire built out in the open or inside a shelter built for that purpose (the "sugar shack").[17][25]Around the time of the American Civil War (1861-1865), syrup makers started using large, flat sheet metal pans as they were more efficient for boiling than heavy, rounded iron kettles, because of a greater surface area for evaporation.[25] Around this time, cane sugar replaced maple sugar as the dominant sweetener in the US; as a result, producers focused marketing efforts on maple syrup. The first evaporator, used to heat and concentrate sap, was patented in 1858. In 1872, an evaporator was developed that featured two pans and a metal arch or firebox, which greatly decreased boiling time.[17] Around 1900, producers bent the tin that formed the bottom of a pan into a series of flues, which increased the heated surface area of the pan and again decreased boiling time. Some producers also added a finishing pan, a separate batch evaporator, as a final stage in the evaporation process.[25]Buckets began to be replaced with plastic bags, which allowed people to see at a distance how much sap had been collected. Syrup producers also began using tractors to haul vats of sap from the trees being tapped (the sugarbush) to the evaporator. Some producers adopted motor-powered tappers and metal tubing systems to convey sap from the tree to a central collection container, but these techniques were not widely used.[17] Heating methods also diversified: modern producers use wood, oil, natural gas, propane, or steam to evaporate sap.[25] Modern filtration methods were perfected to prevent contamination of the syrup.[26]A large number of technological changes took place during the 1970s. Plastic tubing systems that had been experimental since the early part of the century were perfected, and the sap came directly from the tree to the evaporator house.[27] Vacuum pumps were added to the tubing systems, and preheaters were developed to recycle heat lost in the steam. Producers developed reverse-osmosis machines to take a portion of water out of the sap before it was boiled, increasing processing efficiency.[17]Improvements in tubing and vacuum pumps, new filtering techniques, "supercharged" preheaters, and better storage containers have since been developed. Research continues on pest control and improved woodlot management.[17] In 2009, researchers at the University of Vermont unveiled a new type of tap that prevents backflow of sap into the tree, reducing bacterial contamination and preventing the tree from attempting to heal the bore hole.[28]  Experiments show that it may be possible to use saplings in a plantation instead of mature trees, dramatically boosting productivity per acre.[29]Open pan evaporation methods have been streamlined since colonial days, but remain basically unchanged. Sap must first be collected and boiled down to obtain pure syrup without chemical agents or preservatives. Maple syrup is made by boiling between 20 and 50 volumes of sap (depending on its concentration) over an open fire until 1 volume of syrup is obtained, usually at a temperature 4.1 °C (7.4 °F) over the boiling point of water. As the boiling point of water varies with changes in air pressure the correct value for pure water is determined at the place where the syrup is being produced, each time evaporation is begun and periodically throughout the day.[25][30] Syrup can be boiled entirely over one heat source or can be drawn off into smaller batches and boiled at a more controlled temperature.[31]Boiling the syrup is a tightly controlled process, which ensures appropriate sugar content.  Syrup boiled too long will eventually crystallize, whereas under-boiled syrup will be watery, and will quickly spoil.  The finished syrup has a density of 66° on the Brix scale (a hydrometric scale used to measure sugar solutions).[32] The syrup is then filtered to remove sugar sand, crystals made up largely of sugar and calcium malate.[33] These crystals are not toxic, but create a "gritty" texture in the syrup if not filtered out.[34]In addition to  open pan evaporation methods, many large producers use the more fuel efficient reverse osmosis procedure to separate the water from the sap.[35]The higher the sugar content of the sap, the smaller the volume of sap is needed to obtain the same amount of syrup. 57 units of sap with 1.5 percent sugar content will yield 1 unit of syrup, but only 25 units of sap with a 3.5 percent sugar content are needed to obtain one unit of syrup.[36] The sap's sugar content is highly variable and will fluctuate even within the same tree.[37]The filtered syrup is graded and packaged while still hot, usually at a temperature of 82 °C (180 °F) or greater. The containers are turned over after being sealed to sterilize the cap with the hot syrup. Packages can be made of metal, glass, or coated plastic, depending on volume and target market.[38] The syrup can also be heated longer and further processed to create a variety of other maple products, including maple sugar, maple butter or cream, and maple candy or taffy.[39]Off-flavours can sometimes develop during the production of maple syrup, resulting from contaminants in the boiling apparatus (such as disinfectants), microorganisms, fermentation products, metallic can flavours, and "buddy sap", an off-flavour occurring late in the syrup season when tree budding has begun.[40] In some circumstances, it is possible to remove off-flavours through processing.[40][41]Maple syrup production is centred in northeastern North America; however, given the correct weather conditions, it can be made wherever suitable species of maple trees grow.A maple syrup production farm is called a "sugarbush" or "sugarwood". Sap is often boiled in a "sugar house" (also known as a "sugar shack", "sugar shanty", or cabane à sucre), a building louvered at the top to vent the steam from the boiling sap.[42]Maples are usually tapped beginning at 30 to 40 years of age. Each tree can support between one and three taps, depending on its trunk diameter. The average maple tree will produce 35 to 50 litres (9.2 to 13.2 US gal) of sap per season, up to 12 litres (3.2 US gal) per day.[43] This is roughly equal to seven percent of its total sap. Seasons last for four to eight weeks, depending on the weather.[44] During the day, starch stored in the roots for the winter rises through the trunk as sugary sap, allowing it to be tapped.[24] Sap is not tapped at night because the temperature drop inhibits sap flow, although taps are typically left in place overnight.[45] Some producers also tap in autumn, though this practice is less common than spring tapping. Maples can continue to be tapped for sap until they are over 100 years old.[43]Until the 1930s, the United States produced most of the world's maple syrup.[46]  Today, after rapid growth in the 1990s, Canada produces more than 80 percent of the world's maple syrup, producing about 73 million kg (80,000 short tons) in 2016.[1] The vast majority of this comes from the province of Quebec, which is the world's largest producer, with about 70 percent of global production.[1][2]As of 2016, Quebec had some 7,300 producers working with 13,500 farmers, collectively making over 8 million US gal (30 million L) of syrup.[1][47] Production in Quebec is controlled through a supply management system, with producers receiving quota allotments from the Federation of Quebec Maple Syrup Producers (Fédération des producteurs acéricoles du Québec, FPAQ), which also maintains reserves of syrup,[1][48] although there is a black-market trade in Quebec product.[1][49][50] In 2017, the FPAQ mandated increased output of maple syrup production, attempting to establish Quebec's dominance in the world market.[1][2] Canada exported more than C$362 million of maple syrup in 2016.[2] The provinces of Ontario, Nova Scotia, New Brunswick, and Prince Edward Island produce smaller amounts of syrup.[47]The Canadian provinces of Manitoba and Saskatchewan produce maple syrup using the sap of the box elder or Manitoba maple (Acer negundo).[9]  A Manitoba maple tree's yield is usually less than half that of a similar sugar maple tree.[51] Manitoba maple syrup has a slightly different flavour from sugar-maple syrup, because it contains less sugar and the tree's sap flows more slowly.  British Columbia is home to a growing maple sugar industry using sap from the bigleaf maple, which is native to the West Coast of the United States and Canada.[52]Vermont is the biggest US producer, with over 1,320,000 US gal (5.0 million L) during the 2013 season, followed by New York with 574,000 US gal (2.17 million L) and Maine with 450,000 US gal (1.7 million L). Wisconsin, Ohio, New Hampshire, Michigan, Pennsylvania, Massachusetts, and Connecticut all produced marketable quantities of maple syrup of less than 265,000 US gal (1.0 million L) each in 2013.[53] As of 2003, Vermont produced about 5.5 percent of the global syrup supply.[54]Maple syrup has been produced on a small scale in some other countries, notably Japan and South Korea.[55] However, in South Korea in particular, it is traditional to consume maple sap, called gorosoe, instead of processing it into syrup.[56]In 2015, 64 percent of Canadian maple syrup exports went to the United States (a value of C$229 million), 8 percent to Germany (C$31 million), 6 percent to Japan (C$26 million), and 5 percent to the United Kingdom (C$16 million).[47]Under Canadian Maple Product Regulations, containers of maple syrup must include the words "maple syrup", its grade name and net quantity in litres or millilitres, on the main display panel with a minimum font size of 1.6 mm.[57][58] If the maple syrup is of Canada Grade A level, the name of the colour class must appear on the label in both English and French.[57] Also, the lot number or production code, and either: (1) the name and address of the sugar bush establishment, packing or shipper establishment, or (2) the first dealer and the registration number of the packing establishment, must be labeled on any display panel other than the bottom.[57][58]Following an effort from the International Maple Syrup Institute (IMSI) and many maple syrup producer associations, both Canada and the United States have altered their laws regarding the classification of maple syrup to be uniform. Whereas in the past each state or province had their own laws on the classification of maple syrup, now those laws define a unified grading system. This had been a work in progress for several years, and most of the finalization of the new grading system was made in 2014. The Canadian Food Inspection Agency (CFIA) announced in the Canada Gazette on 28 June 2014 that rules for the sale of maple syrup would be amended to include new descriptors, at the request of the IMSI.[59]As of December 31, 2014, the CFIA[59] and as of March 2, 2015, the United States Department of Agriculture (USDA) Agricultural Marketing Service[60] issued revised standards intended to harmonize Canada-United States regulations on the classification of maple syrup as follows:As long as maple syrup does not have an off-flavour, is of a uniform colour, and is free from turbidity and sediment, it can be labelled as one of the A grades. If it exhibits any problems, it does not meet Grade A requirements, and then must be labelled as Processing Grade maple syrup and may not be sold in containers smaller than 5 gallons.[59][60] If maple syrup does not meet the requirements of Processing Grade maple syrup (including a fairly characteristic maple taste), it is classified as Substandard.[59][60]This grading system was accepted and made law by most maple-producing states and provinces, and became compulsory in Canada as of 13 December 2016.[61] Vermont, in an effort to "jump-start" the new grading regulations, adopted the new grading system as of January 1, 2014, after the grade changes passed the Senate and House in 2013. Maine passed a bill to take effect as soon as both Canada and the United States adopted the new grades. In New York, the new grade changes became law on January 1, 2015. New Hampshire did not require legislative approval and so the new grade laws became effective as of December 16, 2014, and producer compliance was required as of January 1, 2016.[62]Golden and Amber grades typically have a milder flavour than Dark and Very dark, which are both dark and have an intense maple flavour.[63] The darker grades of syrup are used primarily for cooking and baking, although some specialty dark syrups are produced for table use.[64] Syrup harvested earlier in the season tends to yield a lighter colour.[65] With the new grading system, the classification of maple syrup depends ultimately on its internal transmittance at 560 nm wavelength through a 10 mm sample. Golden has to have 75 percent or more transmittance, Amber has to have 50.0 to 74.9 percent transmittance, Dark has to have 25.0 to 49.9 percent transmittance, and Very Dark is any product less than 25.0 percent transmittance.[60]In Canada, maple syrup was classified prior to December 31, 2014, by the Canadian Food Inspection Agency (CFIA) as one of three grades, each with several colour classes:[59]Producers in Ontario or Quebec may have followed either federal or provincial grading guidelines.[59] Quebec's and Ontario's guidelines differed slightly from the federal:A typical year's yield for a maple syrup producer will be about 25 to 30 percent of each of the #1 colours, 10 percent #2 Amber, and 2 percent #3 Dark.[32]The United States used (some states still do, as they await state regulation) different grading standards. Maple syrup was divided into two major grades: In Massachusetts, the Grade B was renamed as Grade A Very Dark, Strong Taste.[68]The Vermont Agency of Agriculture Food and Markets used a similar grading system of colour, and is roughly equivalent, especially for lighter syrups, but using letters: "AA", "A", etc.[69][70] The Vermont grading system differed from the US system in maintaining a slightly higher standard of product density (measured on the Baumé scale). New Hampshire maintained a similar standard, but not a separate state grading scale. The Vermont-graded product had 0.9 percent more sugar and less water in its composition than US-graded. One grade of syrup not for table use, called commercial or Grade C, was also produced under the Vermont system.[63]In Canada, the packing of maple syrup must follow the "Packing" conditions stated in the Maple Products Regulations, or utilize the equivalent Canadian or imported grading system.[57]As stated in the Maple Products Regulations, Canadian maple syrup can be classified as "Canadian Grade A" and "Canadian Processing Grade". Any maple syrup container under these classifications should be filled to at least 90% of the bottle size while still containing the net quantity of syrup product as stated on the label. Every container of maple syrup must be new if it has a capacity of 5 litres or less or is marked with a grade name. Every container of maple sugar must also be new if it has a capacity of less than 5 kg or is either exported out of Canada or conveyed from one province to another.[57]Each maple syrup product must be verified clean if it follows a grade name or if it is exported out of the province in which it was originally manufactured.[57]The basic ingredient in maple syrup is the sap from the xylem of sugar maple or various other species of maple trees. It consists primarily of sucrose and water, with small amounts of the monosaccharides glucose and fructose from the invert sugar created in the boiling process.[71]In a 100g amount, maple syrup provides 260 calories and is composed of 32 percent water by weight, 67 percent carbohydrates (90 percent of which are sugars), and no appreciable protein or fat (table). Maple syrup is generally low in overall micronutrient content, although manganese and riboflavin are at high levels along with moderate amounts of zinc and calcium (right table). It also contains trace amounts of amino acids which increase in content as sap flow occurs.[72]Maple syrup contains a wide variety of volatile organic compounds, including vanillin, hydroxybutanone, and propionaldehyde. It is not yet known exactly what compounds are responsible for maple syrup's distinctive flavour,[33] however its primary flavour contributing compounds are maple furanone, strawberry furanone, and maltol.[73]New compounds have been identified in maple syrup, one of which is quebecol, a natural phenolic compound created when the maple sap is boiled to create syrup.[74]One author described maple syrup as "a unique ingredient, smooth- and silky-textured, with a sweet, distinctive flavour – hints of caramel with overtones of toffee will not do – and a rare colour, amber set alight. Maple flavour is, well, maple flavour, uniquely different from any other."[45] Agriculture Canada has developed a "flavour wheel" that details 91 unique flavours that can be present in maple syrup. These flavours are divided into 13 families: vanilla, empyreumatic (burnt), milky, fruity, floral, spicy, foreign (deterioration or fermentation), foreign (environment), maple, confectionery, plant (herbaceous), plant (forest, humus or cereals), and plant (ligneous).[75][76] These flavours are evaluated using a procedure similar to wine tasting.[77] Other culinary experts praise its unique flavour.[78][79][80][81]Maple syrup and its various artificial imitations are widely used as toppings for pancakes, waffles, and French toast in North America. They can also be used to flavour a variety of foods, including fritters, ice cream, hot cereal, fresh fruit, and sausages. It is also used as sweetener for granola, applesauce, baked beans, candied sweet potatoes, winter squash, cakes, pies, breads, tea, coffee, and hot toddies. Maple syrup can also be used as a replacement for honey in wine (mead).[82]In Canada, maple syrup must be made entirely from maple sap, and syrup must have a density of 66° on the Brix scale to be marketed as maple syrup.[32] In the United States, maple syrup must be made almost entirely from maple sap, although small amounts of substances such as salt may be added.[83] Labeling laws prohibit imitation syrups from having "maple" in their names unless the finished product contains 10 percent or more of natural maple syrup.[83]"Maple-flavoured" syrups include maple syrup, but may contain additional ingredients.[83] "Pancake syrup", "waffle syrup", "table syrup", and similarly named syrups are substitutes which are less expensive than maple syrup. In these syrups, the primary ingredient is most often high-fructose corn syrup flavoured with sotolon; they have little genuine maple content, and are usually thickened above the viscosity of maple syrup.[84]Imitation syrups are generally cheaper than maple syrup, with less natural flavour.[84] In the United States, consumers generally prefer imitation syrups, likely because of the significantly lower cost and sweeter flavour;[85][86] they typically cost about $8 per gallon (1 US gallon (3.8 litres)), whereas authentic maple syrup costs $40 to $60 per gallon (2015 prices).[86]In 2016, maple syrup producers from nine US states petitioned the Food and Drug Administration (FDA) to regulate labeling of products containing maple syrup or using the word "maple" in manufactured products, indicating that imitation maple products contained insignificant amounts of natural maple syrup.[87] In September 2016, the FDA published a consumer advisory to carefully inspect the ingredient list of products labeled as "maple".[88]Maple products are considered emblematic of Canada, in particular Quebec, and are frequently sold in tourist shops and airports as souvenirs from Canada. The sugar maple's leaf has come to symbolize Canada, and is depicted on the country's flag.[89] Several US states, including West Virginia, New York, Vermont and Wisconsin, have the sugar maple as their state tree.[90] A scene of sap collection is depicted on the Vermont state quarter, issued in 2001.[91]Maple syrup and maple sugar were used during the American Civil War and by abolitionists in the years before the war because most cane sugar and molasses were produced by Southern slaves.[85][92] Because of food rationing during the Second World War, people in the northeastern United States were encouraged to stretch their sugar rations by sweetening foods with maple syrup and maple sugar,[17] and recipe books were printed to help housewives employ this alternative source.[93]
Ammonia
Trihydrogen nitride0.769  kg/m3 (STP)[1]0.73 kg/m3 (1.013 bar at 15 °C)681.9 kg/m3 at −33.3 °C (liquid)[2] See also Ammonia (data page)Ammonia is a compound of nitrogen and hydrogen with the formula NH3. The simplest pnictogen hydride, ammonia is a colourless gas with a characteristic pungent smell. It is a common nitrogenous waste, particularly among aquatic organisms, and it contributes significantly to the nutritional needs of terrestrial organisms by serving as a precursor to food and fertilizers. Ammonia, either directly or indirectly, is also a building block for the synthesis of many pharmaceutical products and is used in many commercial cleaning products. It is mainly collected by downward displacement of both air and water. Ammonia is named for the Ammonians, worshipers of the Egyptian god Amun, who used ammonium chloride in their rituals.[10]Although common in nature and in wide use, ammonia is both caustic and hazardous in its concentrated form. It is classified as an extremely hazardous substance in the United States, and is subject to strict reporting requirements by facilities which produce, store, or use it in significant quantities.[11]The global industrial production of ammonia in 2014 was 176 million tonnes,[12] a 16% increase over the 2006 global industrial production of 152 million tonnes.[13] Industrial ammonia is sold either as ammonia liquor (usually 28% ammonia in water) or as pressurized or refrigerated anhydrous liquid ammonia transported in tank cars or cylinders.[14]NH3 boils at −33.34 °C (−28.012 °F) at a pressure of one atmosphere, so the liquid must be stored under pressure or at low temperature. Household ammonia or ammonium hydroxide is a solution of NH3 in water. The concentration of such solutions is measured in units of the Baumé scale (density), with 26 degrees baumé (about 30% (by weight) ammonia at 15.5 °C or 59.9 °F) being the typical high-concentration commercial product.[15]Ammonia is a chemical found in trace quantities in nature, being produced from nitrogenous animal and vegetable matter. Ammonia and ammonium salts are also found in small quantities in rainwater, whereas ammonium chloride (sal ammoniac), and ammonium sulfate are found in volcanic districts; crystals of ammonium bicarbonate have been found in Patagonian guano.[16] The kidneys secrete ammonia to neutralize excess acid.[17] Ammonium salts are found distributed through fertile soil and in seawater.Ammonia is also found throughout the Solar System on Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto, among other places: on smaller, icy planets such as Pluto, ammonia can act as a geologically important antifreeze, as a mixture of water and ammonia can have a melting point as low as 173 K (−100 °C; −148 °F) if the ammonia concentration is high enough and thus allow such planets to retain internal oceans and active geology at a far lower temperature than would be possible with water alone.[18][19] Substances containing ammonia, or those that are similar to it, are called ammoniacal.Ammonia is a colourless gas with a characteristic pungent smell. It is lighter than air, its density being 0.589 times that of air. It is easily liquefied due to the strong hydrogen bonding between molecules; the liquid boils at −33.3 °C (−27.94 °F), and freezes at −77.7 °C (−107.86 °F) to white crystals.[16]Ammonia may be conveniently deodorized by reacting it with either sodium bicarbonate or acetic acid. Both of these reactions form an odourless ammonium salt.The ammonia molecule has a trigonal pyramidal shape as predicted by the valence shell electron pair repulsion theory (VSEPR theory) with an experimentally determined bond angle of 106.7°.[21] The central nitrogen atom has five outer electrons with an additional electron from each hydrogen atom. This gives a total of eight electrons, or four electron pairs that are arranged tetrahedrally. Three of these electron pairs are used as bond pairs, which leaves one lone pair of electrons. The lone pair of electrons repel more strongly than bond pairs, therefore the bond angle is not 109.5°, as expected for a regular tetrahedral arrangement, but 106.7°.[21] The nitrogen atom in the molecule has a lone electron pair, which makes ammonia a base, a proton acceptor. This shape gives the molecule a dipole moment and makes it polar. The molecule's polarity, and especially, its ability to form hydrogen bonds, makes ammonia highly miscible with water. Ammonia is moderately basic, a 1.0 M aqueous solution has a pH of 11.6 and if a strong acid is added to such a solution until the solution is neutral (pH = 7), 99.4% of the ammonia molecules are protonated. Temperature and salinity also affect the proportion of NH4+. The latter has the shape of a regular tetrahedron and is isoelectronic with methane.The ammonia molecule readily undergoes nitrogen inversion at room temperature; a useful analogy is an umbrella turning itself inside out in a strong wind. The energy barrier to this inversion is 24.7 kJ/mol, and the resonance frequency is 23.79 GHz, corresponding to microwave radiation of a wavelength of 1.260 cm. The absorption at this frequency was the first microwave spectrum to be observed.[22]One of the most characteristic properties of ammonia is its basicity. Ammonia is considered to be a weak base. It combines with acids to form salts; thus with hydrochloric acid it forms ammonium chloride (sal ammoniac); with nitric acid, ammonium nitrate, etc. Perfectly dry ammonia will not combine with perfectly dry hydrogen chloride; moisture is necessary to bring about the reaction.[23][24] As a demonstration experiment, opened bottles of concentrated ammonia and hydrochloric acid produce clouds of ammonium chloride, which seem to appear "out of nothing" as the salt forms where the two diffusing clouds of molecules meet, somewhere between the two bottles.The salts produced by the action of ammonia on acids are known as the ammonium salts and all contain the ammonium ion (NH4+).[23]Although ammonia is well known as a weak base, it can also act as an extremely weak acid. It is a protic substance and is capable of formation of amides (which contain the NH2− ion). For example, lithium dissolves in liquid ammonia to give a solution of lithium amide:Like water, ammonia undergoes molecular autoionisation to form its acid and base conjugates:Ammonia often functions as a weak base, so it has some buffering ability. Shifts in pH will cause more or fewer ammonium cations (NH+4) and amide anions (NH−2) to be present in solution. At standard pressure and temperature, K=[NH+4][NH−2] = 10−30The combustion of ammonia to nitrogen and water is exothermic:The standard enthalpy change of combustion, ΔH°c, expressed per mole of ammonia and with condensation of the water formed, is −382.81 kJ/mol. Dinitrogen is the thermodynamic product of combustion: all nitrogen oxides are unstable with respect to N2 and O2, which is the principle behind the catalytic converter. Nitrogen oxides can be formed as kinetic products in the presence of appropriate catalysts, a reaction of great industrial importance in the production of nitric acid:A subsequent reaction leads to NO2The combustion of ammonia in air is very difficult in the absence of a catalyst (such as platinum gauze or warm chromium(III) oxide), because the temperature of the flame is usually lower than the ignition temperature of the ammonia–air mixture. The flammable range of ammonia in air is 16–25%.[25]In organic chemistry, ammonia can act as a nucleophile in substitution reactions. Amines can be formed by the reaction of ammonia with alkyl halides, although the resulting -NH2 group is also nucleophilic and secondary and tertiary amines are often formed as byproducts. An excess of ammonia helps minimise multiple substitution and neutralises the hydrogen halide formed. Methylamine is prepared commercially by the reaction of ammonia with chloromethane, and the reaction of ammonia with 2-bromopropanoic acid has been used to prepare racemic alanine in 70% yield. Ethanolamine is prepared by a ring-opening reaction with ethylene oxide: the reaction is sometimes allowed to go further to produce diethanolamine and triethanolamine.Amides can be prepared by the reaction of ammonia with carboxylic acid derivatives. Acyl chlorides are the most reactive, but the ammonia must be present in at least a twofold excess to neutralise the hydrogen chloride formed. Esters and anhydrides also react with ammonia to form amides. Ammonium salts of carboxylic acids can be dehydrated to amides so long as there are no thermally sensitive groups present: temperatures of 150–200 °C are required.The hydrogen in ammonia is capable of replacement by metals; thus, magnesium burns in the gas with the formation of magnesium nitride Mg3N2, and when the gas is passed over heated sodium or potassium, sodamide, NaNH2, and potassamide, KNH2, are formed.[23] Where necessary in substitutive nomenclature, IUPAC recommendations prefer the name "azane" to ammonia: hence chloramine would be named "chloroazane" in substitutive nomenclature, not "chloroammonia".Pentavalent ammonia is known as λ5-amine, or more commonly, ammonium hydride. This crystalline solid is only stable under high pressure and decomposes back into trivalent ammonia and hydrogen gas at normal conditions. This substance was once investigated as a possible solid rocket fuel in 1966.[26]Ammonia can act as a ligand in transition metal complexes. It is a pure σ-donor, in the middle of the spectrochemical series, and shows intermediate hard-soft behaviour. For historical reasons, ammonia is named ammine in the nomenclature of coordination compounds. Some notable ammine complexes include tetraamminediaquacopper(II) ([Cu(NH3)4(H2O)2]2+), a dark blue complex formed by adding ammonia to a solution of copper(II) salts. Tetraamminediaquacopper(II) hydroxide is known as Schweizer's reagent, and has the remarkable ability to dissolve cellulose. Diamminesilver(I) ([Ag(NH3)2]+) is the active species in Tollens' reagent. Formation of this complex can also help to distinguish between precipitates of the different silver halides: silver chloride (AgCl) is soluble in dilute (2M) ammonia solution, silver bromide (AgBr) is only soluble in concentrated ammonia solution, whereas silver iodide (AgI) is insoluble in aqueous ammonia.Ammine complexes of chromium(III) were known in the late 19th century, and formed the basis of Alfred Werner's revolutionary theory on the structure of coordination compounds. Werner noted only two isomers (fac- and mer-) of the complex [CrCl3(NH3)3] could be formed, and concluded the ligands must be arranged around the metal ion at the vertices of an octahedron. This proposal has since been confirmed by X-ray crystallography.An ammine ligand bound to a metal ion is markedly more acidic than a free ammonia molecule, although deprotonation in aqueous solution is still rare. One example is the Calomel reaction, where the resulting amidomercury(II) compound is highly insoluble.Ammonia and ammonium salts can be readily detected, in very minute traces, by the addition of Nessler's solution, which gives a distinct yellow colouration in the presence of the slightest trace of ammonia or ammonium salts. The amount of ammonia in ammonium salts can be estimated quantitatively by distillation of the salts with sodium or potassium hydroxide, the ammonia evolved being absorbed in a known volume of standard sulfuric acid and the excess of acid then determined volumetrically; or the ammonia may be absorbed in hydrochloric acid and the ammonium chloride so formed precipitated as ammonium hexachloroplatinate, (NH4)2PtCl6.[27]Sulfur sticks are burnt to detect small leaks in industrial ammonia refrigeration systems. Larger quantities can be detected by warming the salts with a caustic alkali or with quicklime, when the characteristic smell of ammonia will be at once apparent.[27] Ammonia is an irritant and irritation increases with concentration; the permissible exposure limit is 25 ppm, and lethal above 500 ppm.[28] Higher concentrations are hardly detected by conventional detectors, the type of detector is chosen according to the sensitivity required (e.g. semiconductor, catalytic, electrochemical). Holographic sensors have been proposed for detecting concentrations up to 12.5% in volume.[29]Ammoniacal nitrogen (NH3-N) is a measure commonly used for testing the quantity of ammonium ions, derived naturally from ammonia, and returned to ammonia via organic processes, in water or waste liquids. It is a measure used mainly for quantifying values in waste treatment and water purification systems, as well as a measure of the health of natural and man-made water reserves. It is measured in units of mg/L (milligram per litre).The ancient Greek historian Herodotus mentioned that there were outcrops of salt in an area of Libya that was inhabited by a people called the "Ammonians" (now:  the Siwa oasis in northwestern Egypt, where salt lakes still exist).[30][31]  The Greek geographer Strabo also mentioned the salt from this region.  However, the ancient authors Dioscorides, Apicius, Arrian, Synesius, and Aëtius of Amida described this salt as forming clear crystals that could be used for cooking and that were essentially rock salt.[32]  Hammoniacus sal appears in the writings of Pliny,[33] although it is not known whether the term is identical with the more modern sal ammoniac (ammonium chloride).[16][34][35]The fermentation of urine by bacteria produces a solution of ammonia; hence fermented urine was used in Classical Antiquity to wash cloth and clothing, to remove hair from hides in preparation for tanning, to serve as a mordant in dying cloth, and to remove rust from iron.[36]In the form of sal ammoniac (نشادر, nushadir) ammonia was important to the Muslim alchemists as early as the 8th century, first mentioned by the Persian-Arab chemist Jābir ibn Hayyān,[37] and to the European alchemists since the 13th century, being mentioned by Albertus Magnus.[16] It was also used by dyers in the Middle Ages in the form of fermented urine to alter the colour of vegetable dyes. In the 15th century, Basilius Valentinus showed that ammonia could be obtained by the action of alkalis on sal ammoniac.[38] At a later period, when sal ammoniac was obtained by distilling the hooves and horns of oxen and neutralizing the resulting carbonate with hydrochloric acid, the name "spirit of hartshorn" was applied to ammonia.[16][39]Gaseous ammonia was first isolated by Joseph Black in 1756 by reacting sal ammoniac (Ammonium Chloride) with calcined magnesia (Magnesium Oxide).[40][41] It was isolated again by Peter Woulfe in 1767,[42][43] by Carl Wilhelm Scheele in 1770[44] and by Joseph Priestley in 1773 and was termed by him "alkaline air".[16][45] Eleven years later in 1785, Claude Louis Berthollet ascertained its composition.[46][16]The Haber–Bosch process to produce ammonia from the nitrogen in the air was developed by Fritz Haber and Carl Bosch in 1909 and patented in 1910. It was first used on an industrial scale in Germany during World War I,[47] following the allied blockade that cut off the supply of nitrates from Chile. The ammonia was used to produce explosives to sustain war efforts.[48]Before the availability of natural gas, hydrogen as a precursor to ammonia production was produced via the electrolysis of water or using the chloralkali process.With the advent of the steel industry in the 20th century, ammonia became a byproduct of the production of coking coal.Globally, approximately 88% (as of 2014) of ammonia is used as fertilizers either as its salts, solutions or anhydrously.[12] When applied to soil, it helps provide increased yields of crops such as maize and wheat.[49] 30% of agricultural nitrogen applied in the USA is in the form of anhydrous ammonia and worldwide 110 million tonnes are applied each year.[50]Ammonia is directly or indirectly the precursor to most nitrogen-containing compounds. Virtually all synthetic nitrogen compounds are derived from ammonia. An important derivative is nitric acid. This key material is generated via the Ostwald process by oxidation of ammonia with air over a platinum catalyst at 700–850 °C (1,292–1,562 °F), ~9 atm. Nitric oxide is an intermediate in this conversion:[51]Nitric acid is used for the production of fertilizers, explosives, and many organonitrogen compounds.Ammonia is also used to make the following compounds:Ammonia can also be used to make compounds in reactions which are not specifically named. Examples of such compounds include: ammonium perchlorate, ammonium nitrate, formamide, dinitrogen tetroxide, alprazolam, ethanolamine, ethyl carbamate, hexamethylenetetramine, and ammonium bicarbonate.Household ammonia is a solution of NH3 in water (i.e., ammonium hydroxide) used as a general purpose cleaner for many surfaces. Because ammonia results in a relatively streak-free shine, one of its most common uses is to clean glass, porcelain and stainless steel. It is also frequently used for cleaning ovens and soaking items to loosen baked-on grime. Household ammonia ranges in concentration by weight from 5 to 10% ammonia.[52] United States manufacturers of cleaning products are required to provide the product's material safety data sheet which lists the concentration used.[53]Solutions of ammonia ranging from 16% to 25% are used in the fermentation industry as a source of nitrogen for microorganisms and to adjust pH during fermentation.As early as in 1895, it was known that ammonia was "strongly antiseptic ... it requires 1.4 grams per litre to preserve beef tea."[54] In one study, anhydrous ammonia destroyed 99.999% of zoonotic bacteria in 3 types of animal feed, but not silage.[55][56] Anhydrous ammonia is currently used commercially to reduce or eliminate microbial contamination of beef.[57][58]Lean finely textured beef in the beef industry is made from fatty beef trimmings (c. 50–70% fat) by removing the fat using heat and centrifugation, then treating it with ammonia to kill E. coli. The process was deemed effective and safe by the US Department of Agriculture based on a study that found that the treatment reduces E. coli to undetectable levels.[59] There have been safety concerns about the process as well as consumer complaints about the taste and smell of beef treated at optimal levels of ammonia.[60]  The level of ammonia in any final product has not come close to toxic levels to humans.Because of ammonia's vaporization properties, it is a useful refrigerant.[47] It was commonly used before the popularisation of chlorofluorocarbons (Freons). Anhydrous ammonia is widely used in industrial refrigeration applications and hockey rinks because of its high energy efficiency and low cost. It suffers from the disadvantage of toxicity, which restricts its domestic and small-scale use. Along with its use in modern vapor-compression refrigeration it is used in a mixture along with hydrogen and water in absorption refrigerators. The Kalina cycle, which is of growing importance to geothermal power plants, depends on the wide boiling range of the ammonia–water mixture. Ammonia coolant is also used in the S1 radiator aboard the International Space Station in two loops which are used to regulate the internal temperature and enable temperature dependent experiments.[61][62]The potential importance of ammonia as a refrigerant has increased with the discovery that vented CFCs and HFCs are extremely potent and stable greenhouse gases.[63] The contribution to the greenhouse effect of CFCs and HFCs in current use, if vented, would match that of all CO2 in the atmosphere.Ammonia is used to scrub SO2 from the burning of fossil fuels, and the resulting product is converted to ammonium sulfate for use as fertilizer. Ammonia neutralizes the nitrogen oxide (NOx) pollutants emitted by diesel engines. This technology, called SCR (selective catalytic reduction), relies on a vanadia-based catalyst.[64]Ammonia may be used to mitigate gaseous spills of phosgene.[65]The raw energy density of liquid ammonia is 11.5 MJ/L,[66] which is about a third that of diesel. Although it can be used as a fuel, for a number of reasons this has never been common or widespread. In addition to direct utilization of ammonia as a fuel in combustion engines, there is also the opportunity to convert ammonia back to hydrogen, where it can be used to power hydrogen fuel cells or directly within high-temperature fuel cells.[67]Ammonia engines or ammonia motors, using ammonia as a working fluid, have been proposed and occasionally used.[68] The principle is similar to that used in a fireless locomotive, but with ammonia as the working fluid, instead of steam or compressed air. Ammonia engines were used experimentally in the 19th century by Goldsworthy Gurney in the UK and the St. Charles Avenue Streetcar line in New Orleans in the 1870s and 1880s,[69] and during World War II ammonia was used to power buses in Belgium.[70]Ammonia is sometimes proposed as a practical alternative to fossil fuel for internal combustion engines.[70] Its high octane rating of 120[71] and low flame temperature allows the use of high compression ratios without a penalty of high NOx production.  Since ammonia contains no carbon, its combustion cannot produce carbon monoxide, hydrocarbons or soot.However ammonia cannot be easily used in existing Otto cycle engines because of its very narrow flammability range, and there are also other barriers to widespread automobile usage.  In terms of raw ammonia supplies, plants would have to be built to increase production levels, requiring significant capital and energy sources.  Although it is the second most produced chemical, the scale of ammonia production is a small fraction of world petroleum usage.  It could be manufactured from renewable energy sources, as well as coal or nuclear power. The 60 MW Rjukan dam in Telemark, Norway produced ammonia for many years from 1913, providing fertilizer for much of Europe.Despite this, several tests have been done. In 1981, a Canadian company converted a 1981 Chevrolet Impala to operate using ammonia as fuel.[72][73] In 2007, a University of Michigan pickup powered by ammonia drove from Detroit to San Francisco as part of a demonstration, requiring only one fill-up in Wyoming.[74]Compared to hydrogen as a fuel, ammonia is much more energy efficient, and hydrogen could be produced, stored, and delivered at a much lower cost as ammonia rather than as compressed and/or cryogenic hydrogen.[66] The conversion of ammonia to hydrogen via the sodium-amide process,[75] either as a catalyst for combustion or as fuel for a proton exchange membrane fuel cell,[66] is another possibility. Conversion to hydrogen would allow the storage of hydrogen at nearly 18 wt% compared to ~5% for gaseous hydrogen under pressure.Rocket engines have also been fueled by ammonia. The Reaction Motors XLR99 rocket engine that powered the X-15 hypersonic research aircraft used liquid ammonia. Although not as powerful as other fuels, it left no soot in the reusable rocket engine, and its density approximately matches the density of the oxidizer, liquid oxygen, which simplified the aircraft's design.Ammonia, as the vapor released by smelling salts, has found significant use as a respiratory stimulant.  Ammonia is commonly used in the illegal manufacture of methamphetamine through a Birch reduction.[77] The Birch method of making methamphetamine is dangerous because the alkali metal and liquid ammonia are both extremely reactive, and the temperature of liquid ammonia makes it susceptible to explosive boiling when reactants are added.[78]Liquid ammonia is used for treatment of cotton materials, giving properties like mercerisation, using alkalis. In particular, it is used for prewashing of wool.[79]At standard temperature and pressure, ammonia is less dense than atmosphere and has approximately 45-48% of the lifting power of hydrogen or helium. Ammonia has sometimes been used to fill weather balloons as a lifting gas. Because of its relatively high boiling point (compared to helium and hydrogen), ammonia could potentially be refrigerated and liquefied aboard an airship to reduce lift and add ballast (and returned to a gas to add lift and reduce ballast).Ammonia has been used to darken quartersawn white oak in Arts & Crafts and Mission-style furniture. Ammonia fumes react with the natural tannins in the wood and cause it to change colours.[80]Ammonia can be manufactured from solar energy, air and water. This is an efficient way to package hydrogen into a chemical that is much cheaper to store and transport than pure hydrogen be it as gas or as liquid. In fact, per volume ammonia holds more hydrogen than does liquid hydrogen. Ammonia may be the key to overcome not only the daily but also the seasonal fluctuations of renewable energy sources.This approach will solve many of the problems foreseen for the proposed Hydrogen economy, that instead could be replaced by an Ammonia economy, essentially still a hydrogen economy.In early August 2018, scientists from Australia’s Commonwealth Scientific and Industrial Research Organisation (CSIRO) announced the success of developing a process to release hydrogen from ammonia and harvest that at ultra-high purity as a fuel for cars. This uses a special membrane. Two demonstration fuel cell vehicles have the technology, a Hyundai Nexo and Toyota Mirai.[81]Small-scale, intermittent production of ammonia, for local agricultural use, may be a viable substitute for electrical grid attachment as a sink for power generated by wind turbines in isolated rural installations. Such production would necessarily depend on new, efficiently catalyzed methods to emerge from laboratories.The U. S. Occupational Safety and Health Administration (OSHA) has set a 15-minute exposure limit for gaseous ammonia of 35 ppm by volume in the environmental air and an 8-hour exposure limit of 25 ppm by volume.[82] NIOSH recently reduced the IDLH from 500 to 300 based on recent more conservative interpretations of original research in 1943. IDLH (Immediately Dangerous to Life and Health) is the level to which a healthy worker can be exposed for 30 minutes without suffering irreversible health effects. Other organizations have varying exposure levels. U.S. Navy Standards [U.S. Bureau of Ships 1962] maximum allowable concentrations (MACs):continuous exposure (60 days): 25 ppm / 1 hour: 400 ppm[83] Ammonia vapour has a sharp, irritating, pungent odour that acts as a warning of potentially dangerous exposure. The average odour threshold is 5 ppm, well below any danger or damage. Exposure to very high concentrations of gaseous ammonia can result in lung damage and death.[82] Although ammonia is regulated in the United States as a non-flammable gas, it still meets the definition of a material that is toxic by inhalation and requires a hazardous safety permit when transported in quantities greater than 13,248 L (3,500 gallons).[84] Household products containing ammonia (i.e., Windex) should never be used in conjunction with products containing bleach, as the resulting chemical reaction produces highly toxic fumes.[85]Liquid ammonia is dangerous because it is hygroscopic and because it can freeze flesh. See Gas carrier#Health effects of specific cargoes carried on gas carriers for more information.The toxicity of ammonia solutions does not usually cause problems for humans and other mammals, as a specific mechanism exists to prevent its build-up in the bloodstream. Ammonia is converted to carbamoyl phosphate by the enzyme carbamoyl phosphate synthetase, and then enters the urea cycle to be either incorporated into amino acids or excreted in the urine.[86] Fish and amphibians lack this mechanism, as they can usually eliminate ammonia from their bodies by direct excretion. Ammonia even at dilute concentrations is highly toxic to aquatic animals, and for this reason it is classified as dangerous for the environment.Ammonia is a constituent of tobacco smoke.[87]Ammonia is present in coking wastewater streams, as a liquid by-product of the production of coke from coal.[88] In some cases, the ammonia is discharged to the marine environment where it acts as a pollutant. The Whyalla steelworks in South Australia is one example of a coke-producing facility which discharges ammonia into marine waters.[89]Ammonia toxicity is believed to be a cause of otherwise unexplained losses in fish hatcheries. Excess ammonia may accumulate and cause alteration of metabolism or increases in the body pH of the exposed organism. Tolerance varies among fish species.[90] At lower concentrations, around 0.05 mg/L, un-ionised ammonia is harmful to fish species and can result in poor growth and feed conversion rates, reduced fecundity and fertility and increase stress and susceptibility to bacterial infections and diseases.[91] Exposed to excess ammonia, fish may suffer loss of equilibrium, hyper-excitability, increased respiratory activity and oxygen uptake and increased heart rate.[90] At concentrations exceeding 2.0 mg/L, ammonia causes gill and tissue damage, extreme lethargy, convulsions, coma, and death.[90][92] Experiments have shown that the lethal concentration for a variety of fish species ranges from 0.2 to 2.0 mg/l.[92]During winter, when reduced feeds are administered to aquaculture stock, ammonia levels can be higher. Lower ambient temperatures reduce the rate of algal photosynthesis so less ammonia is removed by any algae present. Within an aquaculture environment, especially at large scale, there is no fast-acting remedy to elevated ammonia levels. Prevention rather than correction is recommended to reduce harm to farmed fish[92] and in open water systems, the surrounding environment.Similar to propane, anhydrous ammonia boils below room temperature when at atmospheric pressure. A storage vessel capable of 250 psi (1.7 MPa) is suitable to contain the liquid.[93] Ammonium compounds should never be allowed to come in contact with bases (unless in an intended and contained reaction), as dangerous quantities of ammonia gas could be released.Solutions of ammonia (5–10% by weight) are used as household cleaners, particularly for glass. These solutions are irritating to the eyes and mucous membranes (respiratory and digestive tracts), and to a lesser extent the skin. Caution should be used that the chemical is never mixed into any liquid containing bleach, as a poisonous gas may result. Mixing with chlorine-containing products or strong oxidants, such as household bleach, can lead to hazardous compounds such as chloramines.[94]The hazards of ammonia solutions depend on the concentration: "dilute" ammonia solutions are usually 5–10% by weight (<5.62 mol/L); "concentrated" solutions are usually prepared at >25% by weight. A 25% (by weight) solution has a density of 0.907 g/cm3, and a solution that has a lower density will be more concentrated. The European Union classification of ammonia solutions is given in the table.The ammonia vapour from concentrated ammonia solutions is severely irritating to the eyes and the respiratory tract, and these solutions should only be handled in a fume hood. Saturated ("0.880" — see #Properties) solutions can develop a significant pressure inside a closed bottle in warm weather, and the bottle should be opened with care; this is not usually a problem for 25% ("0.900") solutions.Ammonia solutions should not be mixed with halogens, as toxic and/or explosive products are formed. Prolonged contact of ammonia solutions with silver, mercury or iodide salts can also lead to explosive products: such mixtures are often formed in qualitative inorganic analysis, and should be lightly acidified but not concentrated (<6% w/v) before disposal once the test is completed.Anhydrous ammonia is classified as toxic (T) and dangerous for the environment (N). The gas is flammable (autoignition temperature: 651 °C) and can form explosive mixtures with air (16–25%). The permissible exposure limit (PEL) in the United States is 50 ppm (35 mg/m3), while the IDLH concentration is estimated at 300 ppm. Repeated exposure to ammonia lowers the sensitivity to the smell of the gas: normally the odour is detectable at concentrations of less than 50 ppm, but desensitised individuals may not detect it even at concentrations of 100 ppm. Anhydrous ammonia corrodes copper- and zinc-containing alloys, and so brass fittings should not be used for handling the gas. Liquid ammonia can also attack rubber and certain plastics.Ammonia reacts violently with the halogens. Nitrogen triiodide, a primary high explosive, is formed when ammonia comes in contact with iodine. Ammonia causes the explosive polymerisation of ethylene oxide. It also forms explosive fulminating compounds with compounds of gold, silver, mercury, germanium or tellurium, and with stibine. Violent reactions have also been reported with acetaldehyde, hypochlorite solutions, potassium ferricyanide and peroxides.Because of its many uses, ammonia is one of the most highly produced inorganic chemicals. Dozens of chemical plants worldwide produce ammonia. Consuming more than 1% of all man-made power, ammonia production is a significant component of the world energy budget.[47] The USGS reports global ammonia production in 2014 was 176 million tonnes.[12] China accounted for 32.6% of that (increasingly from coal as part of urea synthesis), followed by Russia at 8.1%, India at 7.6%, and the United States at 6.4%.[12] About 88% of the ammonia produced was used for fertilizing agricultural crops.[12] As of 2012 the global production of ammonia produced from natural gas using the steam reforming process was 72 percent.[95]Before the start of World War I, most ammonia was obtained by the dry distillation[96] of nitrogenous vegetable and animal waste products, including camel dung, where it was distilled by the reduction of nitrous acid and nitrites with hydrogen; in addition, it was produced by the distillation of coal, and also by the decomposition of ammonium salts by alkaline hydroxides[97] such as quicklime, the salt most generally used being the chloride (sal ammoniac) thus:[16]Hydrogen for ammonia synthesis could also be produced economically by using the water gas reaction followed by the water gas shift reaction, produced by passing steam through red-hot coke, to give a mixture of hydrogen and carbon dioxide gases, followed by removal of the carbon dioxide "washing" the gas mixture with water under pressure (25 standard atmospheres (2,500 kPa));[98] or by using other sources like coal or coke gasification.Modern ammonia-producing plants depend on industrial hydrogen production to react with atmospheric nitrogen using a magnetite catalyst or over a promoted Fe catalyst under high pressure (100 standard atmospheres (10,000 kPa)) and temperature (450 °C) to form anhydrous liquid ammonia. This step is known as the ammonia synthesis loop (also referred to as the Haber–Bosch process):[99]Hydrogen required for ammonia synthesis could also be produced economically using other sources like coal or coke gasification or less economically from the electrolysis of water into oxygen + hydrogen and other alternatives that are presently impractical for large scale.At one time, most of Europe's ammonia was produced from the Hydro plant at Vemork, via the electrolysis route. Various renewable energy electricity sources are also potentially applicable.As a sustainable alternative to the relatively inefficient electrolysis, hydrogen can be generated from organic wastes (such as biomass or food-industry waste) using catalytic reforming. This releases hydrogen from carbonaceous substances at only 10–20% of energy used by electrolysis and may lead to hydrogen being produced from municipal wastes at below zero cost (allowing for the tipping fees and efficient catalytic reforming, such as cold-plasma). Catalytic (thermal) reforming is possible in small, distributed (even mobile) plants, to take advantage of low-value, stranded biomass/biowaste or natural gas deposits. Conversion of such wastes into ammonia solves the problem of hydrogen storage, as hydrogen can be released economically from ammonia on-demand, without the need for high-pressure or cryogenic storage.It is also easier to store ammonia onboard vehicles than to store hydrogen, as ammonia is less flammable than petrol or LPG.There is significant recent progress in synthesizing ammonia more efficiently from H2 and N2 than with the Haber process. In 2012, Masaaki Kitano (and 9 co-authors), working with an organic ruthenium catalyst, demonstrated "Ammonia Synthesis Using a Stable Electride as an Electron Donor and Reversible Hydrogen Store".[100] In January 2018, Yutong Gong (and 12 co-authors) demonstrated "Ternary intermetallic LaCoSi as a catalyst for N2 activation"[101], an equally efficient production process, not dependent on rare metal. In July 2018, Xiaoqian Wang (and 14 co-authors) demonstrated "Atomically dispersed Au 1 catalyst towards efficient electrochemical synthesis of ammonia"[102], an even more efficient process.For small scale laboratory synthesis, one can heat urea and Ca(OH)2Liquid ammonia is the best-known and most widely studied nonaqueous ionising solvent. Its most conspicuous property is its ability to dissolve alkali metals to form highly coloured, electrically conductive solutions containing solvated electrons. Apart from these remarkable solutions, much of the chemistry in liquid ammonia can be classified by analogy with related reactions in aqueous solutions. Comparison of the physical properties of NH3 with those of water shows NH3 has the lower melting point, boiling point, density, viscosity, dielectric constant and electrical conductivity; this is due at least in part to the weaker hydrogen bonding in NH3 and because such bonding cannot form cross-linked networks, since each NH3 molecule has only one lone pair of electrons compared with two for each H2O molecule. The ionic self-dissociation constant of liquid NH3 at −50 °C is about 10−33 mol2·l−2.Liquid ammonia is an ionising solvent, although less so than water, and dissolves a range of ionic compounds, including many nitrates, nitrites, cyanides, thiocyanates, metal cyclopentadienyl complexes and metal bis(trimethylsilyl)amides.[103] Most ammonium salts are soluble and act as acids in liquid ammonia solutions. The solubility of halide salts increases from fluoride to iodide. A saturated solution of ammonium nitrate (Divers' solution, named after Edward Divers) contains 0.83 mol solute per mole of ammonia and has a vapour pressure of less than 1 bar even at 25 °C (77 °F).Liquid ammonia will dissolve the alkali metals and other electropositive metals such as magnesium, calcium, strontium, barium, europium and ytterbium. At low concentrations (<0.06 mol/l), deep blue solutions are formed: these contain metal cations and solvated electrons, free electrons that are surrounded by a cage of ammonia molecules.These solutions are very useful as strong reducing agents. At higher concentrations, the solutions are metallic in appearance and in electrical conductivity. At low temperatures, the two types of solution can coexist as immiscible phases.The range of thermodynamic stability of liquid ammonia solutions is very narrow, as the potential for oxidation to dinitrogen, E° (N2 + 6NH4+ + 6e− ⇌ 8NH3), is only +0.04 V. In practice, both oxidation to dinitrogen and reduction to dihydrogen are slow. This is particularly true of reducing solutions: the solutions of the alkali metals mentioned above are stable for several days, slowly decomposing to the metal amide and dihydrogen. Most studies involving liquid ammonia solutions are done in reducing conditions; although oxidation of liquid ammonia is usually slow, there is still a risk of explosion, particularly if transition metal ions are present as possible catalysts.Ammonia is both a metabolic waste and a metabolic input throughout the biosphere. It is an important source of nitrogen for living systems. Although atmospheric nitrogen abounds (more than 75%), few living creatures are capable of using this atmospheric nitrogen in its diatomic form, N2 gas. Therefore, nitrogen fixation is required for the synthesis of amino acids, which are the building blocks of protein. Some plants rely on ammonia and other nitrogenous wastes incorporated into the soil by decaying matter. Others, such as nitrogen-fixing legumes, benefit from symbiotic relationships with rhizobia that create ammonia from atmospheric nitrogen.[105]In certain organisms, ammonia is produced from atmospheric nitrogen by enzymes called nitrogenases. The overall process is called nitrogen fixation. Intense effort has been directed toward understanding the mechanism of biological nitrogen fixation; the scientific interest in this problem is motivated by the unusual structure of the active site of the enzyme, which consists of an Fe7MoS9 ensemble.[106]Ammonia is also a metabolic product of amino acid deamination catalyzed by enzymes such as glutamate dehydrogenase 1. Ammonia excretion is common in aquatic animals. In humans, it is quickly converted to urea, which is much less toxic, particularly less basic. This urea is a major component of the dry weight of urine. Most reptiles, birds, insects, and snails excrete uric acid solely as nitrogenous waste.Ammonia also plays a role in both normal and abnormal animal physiology. It is biosynthesised through normal amino acid metabolism and is toxic in high concentrations. The liver converts ammonia to urea through a series of reactions known as the urea cycle. Liver dysfunction, such as that seen in cirrhosis, may lead to elevated amounts of ammonia in the blood (hyperammonemia). Likewise, defects in the enzymes responsible for the urea cycle, such as ornithine transcarbamylase, lead to hyperammonemia. Hyperammonemia contributes to the confusion and coma of hepatic encephalopathy, as well as the neurologic disease common in people with urea cycle defects and organic acidurias.[107]Ammonia is important for normal animal acid/base balance. After formation of ammonium from glutamine, α-ketoglutarate may be degraded to produce two molecules of bicarbonate, which are then available as buffers for dietary acids. Ammonium is excreted in the urine, resulting in net acid loss. Ammonia may itself diffuse across the renal tubules, combine with a hydrogen ion, and thus allow for further acid excretion.[108]Ammonium ions are a toxic waste product of metabolism in animals. In fish and aquatic invertebrates, it is excreted directly into the water. In mammals, sharks, and amphibians, it is converted in the urea cycle to urea, because it is less toxic and can be stored more efficiently. In birds, reptiles, and terrestrial snails, metabolic ammonium is converted into uric acid, which is solid, and can therefore be excreted with minimal water loss.[109]Ammonia has been detected in the atmospheres of the gas giant planets, including Jupiter, along with other gases like methane, hydrogen, and helium. The interior of Saturn may include frozen crystals of ammonia.[110] It is naturally found on Deimos and Phobos – the two moons of Mars.Ammonia was first detected in interstellar space in 1968, based on microwave emissions from the direction of the galactic core.[111] This was the first polyatomic molecule to be so detected.The sensitivity of the molecule to a broad range of excitations and the ease with which it can be observed in a number of regions has made ammonia one of the most important molecules for studies of molecular clouds.[112] The relative intensity of the ammonia lines can be used to measure the temperature of the emitting medium.The following isotopic species of ammonia have been detected:The detection of triply deuterated ammonia was considered a surprise as deuterium is relatively scarce. It is thought that the low-temperature conditions allow this molecule to survive and accumulate.[113]Since its interstellar discovery, NH3 has proved to be an invaluable spectroscopic tool in the study of the interstellar medium. With a large number of transitions sensitive to a wide range of excitation conditions, NH3 has been widely astronomically detected – its detection has been reported in hundreds of journal articles. Listed below is a sample of journal articles that highlights the range of detectors that have been used to identify ammonia.The study of interstellar ammonia has been important to a number of areas of research in the last few decades. Some of these are delineated below and primarily involve using ammonia as an interstellar thermometer.The interstellar abundance for ammonia has been measured for a variety of environments. The [NH3]/[H2] ratio has been estimated to range from 10−7 in small dark clouds[114] up to 10−5 in the dense core of the Orion Molecular Cloud Complex.[115] Although a total of 18 total production routes have been proposed,[116] the principal formation mechanism for interstellar NH3 is the reaction:All other proposed formation reactions have rate constants of between 2 and 13 orders of magnitude smaller, making their contribution to the abundance of ammonia relatively insignificant.[119] As an example of the minor contribution other formation reactions play, the reaction:has a rate constant of 2.2×10−15. Assuming H2 densities of 105 and [NH2]/[H2] ratio of 10−7, this reaction proceeds at a rate of 2.2×10−12, more than 3 orders of magnitude slower than the primary reaction above.Some of the other possible formation reactions are:There are 113 total proposed reactions leading to the destruction of NH3. Of these, 39 were tabulated in extensive tables of the chemistry among C, N, and O compounds.[120] A review of interstellar ammonia cites the following reactions as the principal dissociation mechanisms:[112] NH3 + H3+ → NH4+ + H2    (1) NH3 + HCO+ → NH4+ + CO    (2)with rate constants of 4.39×10−9[121] and 2.2×10−9,[122] respectively. The above equations (1, 2) run at a rate of 8.8×10−9 and 4.4×10−13, respectively. These calculations assumed the given rate constants and abundances of [NH3]/[H2] = 10−5, [H3+]/[H2] = 2×10−5, [HCO+]/[H2] = 2×10−9, and total densities of n = 105, typical of cold, dense, molecular clouds.[123] Clearly, between these two primary reactions, equation (1) is the dominant destruction reaction, with a rate ~10,000 times faster than equation (2). This is due to the relatively high abundance of H3+.Radio observations of NH3 from the Effelsberg 100-m Radio Telescope reveal that the ammonia line is separated into two components – a background ridge and an unresolved core. The background corresponds well with the locations previously detected CO.[124] The 25 m Chilbolton telescope in England detected radio signatures of ammonia in H II regions, HNH2O masers, H-H objects, and other objects associated with star formation. A comparison of emission line widths indicates that turbulent or systematic velocities do not increase in the central cores of molecular clouds.[125]Microwave radiation from ammonia was observed in several galactic objects including W3(OH), Orion A, W43, W51, and five sources in the galactic centre. The high detection rate indicates that this is a common molecule in the interstellar medium and that high-density regions are common in the galaxy.[126]VLA observations of NH3 in seven regions with high-velocity gaseous outflows revealed condensations of less than 0.1 pc in L1551, S140, and Cepheus A. Three individual condensations were detected in Cepheus A, one of them with a highly elongated shape. They may play an important role in creating the bipolar outflow in the region.[127]Extragalactic ammonia was imaged using the VLA in IC 342. The hot gas has temperatures above 70 K, which was inferred from ammonia line ratios and appears to be closely associated with the innermost portions of the nuclear bar seen in CO.[128] NH3 was also monitored by VLA toward a sample of four galactic ultracompact HII regions: G9.62+0.19, G10.47+0.03, G29.96-0.02, and G31.41+0.31. Based upon temperature and density diagnostics, it is concluded that in general such clumps are probably the sites of massive star formation in an early evolutionary phase prior to the development of an ultracompact HII region.[129]Absorption at 2.97 micrometres due to solid ammonia was recorded from interstellar grains in the Becklin-Neugebauer Object and probably in NGC 2264-IR as well. This detection helped explain the physical shape of previously poorly understood and related ice absorption lines.[130]A spectrum of the disk of Jupiter was obtained from the Kuiper Airborne Observatory, covering the 100 to 300 cm−1 spectral range. Analysis of the spectrum provides information on global mean properties of ammonia gas and an ammonia ice haze.[131]A total of 149 dark cloud positions were surveyed for evidence of 'dense cores' by using the (J,K) = (1,1) rotating inversion line of NH3. In general, the cores are not spherically shaped, with aspect ratios ranging from 1.1 to 4.4. It is also found that cores with stars have broader lines than cores without stars.[132]Ammonia has been detected in the Draco Nebula and in one or possibly two molecular clouds, which are associated with the high-latitude galactic infrared cirrus. The finding is significant because they may represent the birthplaces for the Population I metallicity B-type stars in the galactic halo that could have been borne in the galactic disk.[133]By balancing and stimulated emission with spontaneous emission, it is possible to construct a relation between excitation temperature and density. Moreover, since the transitional levels of ammonia can be approximated by a 2-level system at low temperatures, this calculation is fairly simple. This premise can be applied to dark clouds, regions suspected of having extremely low temperatures and possible sites for future star formation. Detections of ammonia in dark clouds show very narrow lines—indicative not only of low temperatures, but also of a low level of inner-cloud turbulence. Line ratio calculations provide a measurement of cloud temperature that is independent of previous CO observations. The ammonia observations were consistent with CO measurements of rotation temperatures of ~10 K. With this, densities can be determined, and have been calculated to range between 104 and 105 cm−3 in dark clouds. Mapping of NH3 gives typical clouds sizes of 0.1 pc and masses near 1 solar mass. These cold, dense cores are the sites of future star formation.Ultra-compact HII regions are among the best tracers of high-mass star formation. The dense material surrounding UCHII regions is likely primarily molecular. Since a complete study of massive star formation necessarily involves the cloud from which the star formed, ammonia is an invaluable tool in understanding this surrounding molecular material. Since this molecular material can be spatially resolved, it is possible to constrain the heating/ionising sources, temperatures, masses, and sizes of the regions. Doppler-shifted velocity components allow for the separation of distinct regions of molecular gas that can trace outflows and hot cores originating from forming stars.Ammonia has been detected in external galaxies,[134][135] and by simultaneously measuring several lines, it is possible to directly measure the gas temperature in these galaxies. Line ratios imply that gas temperatures are warm (~50 K), originating from dense clouds with sizes of tens of pc. This picture is consistent with the picture within our Milky Way galaxy—hot dense molecular cores form around newly forming stars embedded in larger clouds of molecular material on the scale of several hundred pc (giant molecular clouds; GMCs).
Secondary forest
A secondary forest (or second-growth forest) is a forest or woodland area which has re-grown after a timber harvest, until a long enough period has passed so that the effects of the disturbance are no longer evident. It is distinguished from an old-growth forest (primary or primeval forest), which has not recently undergone such disruption, and complex early seral forest, as well as third-growth forests that result from harvest in second growth forests. Secondary forest regrowing after timber harvest differs from forest regrowing after natural disturbances such as fire, insect infestation, or windthrow because the dead trees remain to provide nutrients, structure, and water retention after natural disturbances. However, often after natural disturbance the timber is harvested and removed from the system, in which case the system more closely resembles secondary forest rather than complex early seral forest.Depending on the forest, the development of primary characteristics may take anywhere from a century to several millennia. Hardwood forests of the eastern United States, for example, can develop primary characteristics in one or two generations of trees, or 150–500 years. Often the disruption is the result of human activity, such as logging, but natural phenomena that produce the same effect are often included in the definition. Secondary forests tend to have trees closer spaced than primary forests and contain less undergrowth than primary forests.  Secondary forests typically were thought to lack biodiversity compared to primary forests,[1] however this has been challenged in recent years.[citation needed] Usually, secondary forests have only one canopy layer, whereas primary forests have several.Secondary forestation is common in areas where forests have been lost by the slash-and-burn method, a component of some shifting cultivation systems of agriculture. Secondary forests may also arise from forest that has been harvested heavily or over a long period of time, forest that is naturally regenerating from fire and from abandoned pastures or areas of agriculture.  It takes a secondary forest typically forty to 100 years to begin to resemble the original old-growth forest; however, in some cases a secondary forest will not succeed, due to erosion or soil nutrient loss in certain tropical forests.Secondary forests re-establish by the process of succession.  Openings created in the forest canopy allow sunlight to reach the forest floor.  An area that has been cleared will first be colonized by pioneer species.  Even though some species loss may occur with primary forest removal, a secondary forest can protect the watershed from further erosion and provides habitat.  Secondary forests may also buffer edge effects around mature forest fragments and increase connectivity between them.  They may also be a source of wood and other forest products.Today most of the forest of the United States, the eastern part of North America and Europe consist of secondary forest.In the case of tropical rainforests, where soil nutrient levels are characteristically low, the soil quality may be significantly diminished following the removal of primary forest. In Panama, growth of new forests from abandoned farmland exceeded loss of primary rainforest in 1990.[2] However, due to the diminished quality of soil, among other factors, the presence of a significant majority of primary forest species fail to recover in these second-growth forests.
Hevea brasiliensis
Hevea brasiliensis, the Pará rubber tree, sharinga tree, seringueira, or, most commonly, the rubber tree or rubber plant, is a tree belonging to the family Euphorbiaceae. It is the most economically important member of the genus Hevea because the milky latex extracted from the tree is the primary source of natural rubber.H. brasiliensis is a tall deciduous tree growing to a height of up to  43 m (141 ft) in the wild, but cultivated trees are usually much smaller because drawing off the latex restricts the growth of the tree. The trunk is cylindrical and may have a swollen, bottle-shaped base. The bark is some shade of brown, and the inner bark oozes latex when damaged. The leaves have three leaflets and are spirally arranged. The inflorescence include separate male and female flowers. The flowers are pungent, creamy-yellow and have no petals. The fruit is a capsule that contains three large seeds; it opens explosively when ripe.[1]In the wild, the tree can reach a height of up to 140 feet (43 m). The white or yellow latex occurs in latex vessels in the bark, mostly outside the phloem. These vessels spiral up the tree in a right-handed helix which forms an angle of about 30 degrees with the horizontal, and can grow as high as 45 ft.[2]In plantations, the trees are generally smaller for two reasons: 1) trees grow more slowly when they are tapped for latex, and 2) trees are generally cut down after only 30 years, because latex production declines as trees age, and they are no longer economically productive. The tree requires a tropical or subtropical climate with a minimum of about 1,200 mm per year of rainfall, and no frost.[3] If frost does occur, the results can be disastrous for production. One frost can cause the rubber from an entire plantation to become brittle and break once it has been refined."[4]Harvesters make incisions across the latex vessels, just deep enough to tap the vessels without harming the tree's growth, and the latex is collected in small buckets. This process is known as rubber tapping. Latex production is highly variable from tree to tree and across clone types.[2]As latex production declines with age, rubber trees are generally felled when they reach the age of 25 to 30 years. The earlier practice was to burn the trees, but in recent decades, the wood has been harvested for furniture making.[2]The South American rubber tree grew only in the Amazon rainforest, and increasing demand and the discovery of the vulcanization procedure in 1839 led to the rubber boom in that region, enriching the cities of Belém, Santarém, Manaus and Iquitos, Peru, of 1840 to 1913. In Brazil, the initial name of the plant was pará rubber tree.  The name of the tree derives from Grão-Pará and Rio Negro or only Grão-Pará (Great-Pará), the largest Brazilian province until 1850, the capital of which is Belém, where most of the fluid, also called latex, was extracted and exported.  In Peru, in addition to the hispanic-speaking countries of the Amazon region, the name given was árbol del caucho, with the fluid extracted called caucho. These trees were used to obtain rubber by the natives who inhabited its geographical distribution. The Olmec people of Mesoamerica extracted and produced similar forms of primitive rubber from analogous latex-producing trees such as Castilla elastica as early as 3,600 years ago. The rubber was used, among other things, to make the balls used in the Mesoamerican ballgame.[5] Early attempts were made in 1873 to grow H. brasilensis outside Brazil. After some effort, 12 seedlings were germinated at the Royal Botanic Gardens, Kew. These were sent to India for cultivation, but died. A second attempt was then made, some 70,000 seeds being smuggled to Kew in 1875, by Henry Wickham, in the service of the British Empire.[6]:55[7][8] About four percent of these germinated, and in 1876, about 2,000 seedlings were sent, in Wardian cases, to Ceylon (modern day Sri Lanka) and 22 were sent to the Botanic Gardens in Singapore. Once established outside its native country, rubber was extensively propagated in the British colonies. Rubber trees were brought to the botanical gardens at Buitenzorg, Java, in 1883.[9] By 1898, a rubber plantation had been established in Malaya, with imported Chinese field workers being the dominant work force in rubber production in the early 20th-century.[10] Today, most rubber tree plantations are in South and Southeast Asia, the top rubber producing countries in 2011 being Thailand, Indonesia, Malaysia, India and Vietnam.[11]The cultivation of the tree in South America (Amazon) ended early in the 20th century because of blight.[3] The blight, called South American leaf blight, is caused by the ascomycetes, Microcyclus ulei[12] or Pseudocercospora ulei.[13]The toxicity of arsenic to insects, bacteria, and fungi has led to the heavy use of arsenic trioxide on rubber plantations, especially in Malaysia.[14]The majority of the rubber trees in Southeast Asia are clones of varieties highly susceptible to the South American leaf blight—Microcyclus ulei.  For these reasons, environmental historian Charles C. Mann, in his 2011 book, 1493: Uncovering the New World Columbus Created, predicted that the Southeast Asian rubber plantations will be ravaged by the blight in the not-too-distant future, thus creating a potential calamity for international industry.[15]The genus Hevea is also known as:
Sycamine
The sycamine tree (Greek: συκάμινος sykaminοs)[1] is a tree mentioned[n 1] in Luke 17:6 of the Bible. It is rendered by Luther as "mulberry tree", which is most probably the correct rendering. It is found in two species, the black mulberry (Morus nigra) and the white mulberry (Morus alba), which are common in Palestine.  It is in the same family as the fig-tree. Some contend, however, that this name denotes the sycamore fig (συκομορέα sykomorea)[2] of Luke 19:4.
Eucalyptus regnans
Eucalyptus regnans, known variously as mountain ash, swamp gum, or stringy gum, is a species of Eucalyptus native to Tasmania and the state of Victoria in southeastern Australia. It is the tallest flowering plant and one of the tallest trees in the world, second to the coast redwood (Sequoia sempervirens). A straight-trunked tree with smooth grey bark and a stocking of rough brown bark to 5–20 metres (16–66 ft) above the ground, it regularly grows to 85 metres (279 ft), with the tallest living specimen, the Centurion, standing 99.82 metres (327.5 feet) tall in Tasmania. The white flowers appear in autumn. Victorian botanist Ferdinand von Mueller described the species in 1871.Eucalyptus regnans grows in pure stands in tall wet forest, sometimes with rainforest understorey, in temperate areas receiving over 1,200 millimetres (47 in) rainfall a year on deep loam soils. Many of these have been logged, including trees higher than trees of any species now living—one specimen recorded at over 132 metres (433 ft) in Victoria. Killed by bushfire, Eucalyptus regnans regenerates from seed and has a lifespan of several hundred years. Mature Eucalyptus regnans-dominated forests have been found to store more carbon than any other forest known. Also known in the timber industry as Tasmanian oak, E. regnans is logged for its wood and grown in plantations in New Zealand and Chile as well as Australia.Victorian Botanist Ferdinand von Mueller described Eucalyptus regnans in 1871,[1] using the Latin regnans "ruling" as its species epithet.[2] He noted, "This species or variety, which might be called Eucalyptus regnans, represents the loftiest tree in British Territory." However, until 1882 he considered the tree to be a variety of Eucalyptus amygdalina and called it thus,[3] not using the binomial name Eucalyptus regnans until the Systematic Census of Australian Plants in 1882,[4] and giving it a formal diagnosis in 1888 in Volume 1 of the Key to the System of Victorian Plants, where he describes it as "stupendously tall".[5]  Von Mueller did not designate a type specimen, nor did he use the name Eucalyptus regnans on his many collections of "White Mountain Ash" at the Melbourne Herbarium. Victorian botanist Jim Willis selected a lectotype in 1967, one of the more complete collections of a specimen from the Dandenong Ranges, that von Mueller had noted was one "of the tall trees measured by Mr D. Boyle in March 1867."[3]Genetic testing across its range of chloroplast DNA by Paul Nevill and colleagues yielded 41 haplotypes, divided broadly into Victorian and Tasmanian groups, but also showing distinct profiles for some areas such as East Gippsland, northeastern and southeastern Tasmania, suggesting the species had persisted in these areas during the Last Glacial Maximum and recolonised others. There was some sharing of haplotype between populations of the Otway Ranges and northwestern Tasmania, suggesting this was the most likely area for gene flow between the mainland and Tasmania in the past.[6]Eucalyptus regnans is widely known as the mountain ash, due to the resemblance of its wood to that of the northern hemisphere ash (Fraxinus). Swamp gum is a name given to it in Tasmania, and stringy gum in northern Tasmania.[2] Other common names include white mountain ash, giant ash, stringy gum, swamp gum and Tasmanian oak.[1] Von Mueller called it the "Giant gum-tree" and "Spurious blackbutt" in his 1888 Key to the System of Victorian Plants.[5] The timber has been known as "Tasmanian oak", since early settlers likened the strength of its wood that of English oak (Quercus robur).[7]The brown barrel (Eucalyptus fastigata) is a close relative, the two sharing the rare trait of paired inflorescences arising from axillary buds. Botanist Ian Brooker classified the two in the series Regnantes.[2] The latter species differs in having brown fibrous bark all the way up its trunk, and was long classified as a subspecies of E. regnans.[8] The series lies in the section Eucalyptus of the subgenus Eucalyptus within the genus Eucalyptus.[9]Hybridisation with messmate (Eucalyptus obliqua) is not uncommon and has been recorded from several sites in Victoria and Tasmania.[8] Hybrids with red stringybark (Eucalyptus macrorhyncha) occur in the Cathedral Range in Victoria. These trees resemble E. regnans in appearance though lack the paired inflorescences. They have the oil composition of E. macrorhyncha.[10]An evergreen tree, Eucalyptus regnans is the tallest of the eucalypts, growing to 70–114.4 m (230–375 ft), with a straight, grey trunk, smooth-barked except for the rough basal 5–20 metres (16–66 ft).[11] Mature trees have long strips of bark hanging from the trunk.[12] The trunk typically reaches a diameter of 2.5 metres (8 ft) at breast height (dbh),[2] and eventually develops a large buttress.[12]  Some individuals attain much greater diameter; the largest known being "The Bulga Stump", a charred remnant near Tarra Bulga, South Gippsland district, Victoria, Australia which as a living tree had a DBH (diameter at breast height) of 35' 4" (10.74 metres),[13][14] making Eu. regnans the third thickest species of tree after the Baobab (Adansonia digitata) and the Montezuma Cypress (Taxodium mucronatum). As a consequence of being both the tallest and thickest Australian trees, Eu. regnans is also the most massive; that title being currently held by an individual called the "Kermandie Queen" discovered 2.4 miles (4 km) west of Geeveston, Tasmania which measures 252' 7" (77 metres) in height and has a diameter at breast height (DBH) of 22' 7" (21.65 metres girth).[15]  The crown is open and small in relation to the size of the rest of the tree.[2]  Arranged alternately along the stems,[9] the adult leaves are falcate (sickle-shaped) to lanceolate, 9–14 centimetres (3.5–5.5 in) long and 1.5–2.5 centimetres (0.6–1.0 in) broad, with a long acuminate apex and smooth margin, green to grey-green with a reddish petiole.[11] The upper and lower surfaces of the leaves are the same colour, and are dotted with numerous circular or irregularly-shaped tiny oil glands. Secondary leaf veins arise at an acute angle from the midvein and tertiary venation is sparse.[16]  The flowers are produced in clusters of 9–15 together, each flower about 1 centimetre (0.4 in) diameter with a ring of numerous white stamens. These appear between January and May.[9][11] On 4–7 millimetres (0.16–0.28 in) long pedicels,[9] the fruit is a capsule 5–9 millimetres (0.20–0.35 in) long and 4–7 millimetres (0.16–0.28 in) broad.[17] Roughly cone-shaped, it has a disc at the base. The disc usually has three valves, which open to release the 1.5–3 millimetres (0.059–0.118 in) brown seed. The hilum is at one end of the pyramid-shaped seed.[9]Seedlings have kidney shaped cotyledons,[9] and the first 2-3 pairs of leaves are oppositely arranged along the stem, before switching to an alternate arrangement. These juvenile leaves are 5–13 centimetres (2.0–5.1 in) long and 2.5–4.5 centimetres (1.0–1.8 in) broad and ovate in shape.[2]Eucalyptus regnans is the tallest of all flowering plants, and possibly the tallest of all plants, although no living specimens can make that claim. The tallest measured living specimen, named Centurion, stands 99.82 metres (327.5 feet) tall in Tasmania.[18][19] Before the discovery of Centurion, the tallest known specimen was Icarus Dream, which was rediscovered in Tasmania in January, 2005 and is 97 metres (318 ft) high. It was first measured by surveyors at 98.8 metres (324 ft) in 1962 but the documentation had been lost.[20]A total of 16 living trees in Tasmania have been reliably measured in excess of 90 metres (300 ft).[21] The Cumberland Scenic Reserve near Cambarville, became the site of Victoria's tallest trees, in 1939, including one measured at 92 metres high, following the extensive Black Friday bushfires. A severe storm in 1959 blew down 13 of the trees and the tallest tree was reduced to a height of 84 metres after it lost part of its crown. The height of this tree was cited as 81.5 metres in 2002 following further storm damage in 1973.[22] In 2000, a tree at Wallaby Catchment in Kinglake National Park was discovered to be 91.6 metres (301 ft)  tall in 2000,[22] however it perished in the Black Saturday bushfires of 2009.[23]Historically, the tallest individual is claimed to be the Ferguson Tree, at 132.6 metres (435 ft), found in the Watts River region of Victoria in 1871 or 1872. This record is often disputed as unreliable, despite first-hand documentary evidence of it being measured on the ground with surveyor's tape by a senior forestry official (see below). Widespread agreement exists, however, that an exceptionally tall individual was reliably measured at 112.8 metres (370 ft) by theodolite in 1880 by a surveyor, George Cornthwaite, at Thorpdale, Victoria (the tree is known both as the Cornthwaite or Thorpdale Tree). When it was felled in 1881, Cornthwaite remeasured it on the ground by chain at 114.3 metres (375 ft).[24] The stump is commemorated with a plaque. That tree was about 1 metre shorter than Hyperion, the world's current tallest living tree, a coast redwood measuring 379.1 feet (115.5 m).[25]Al Carder, notes that in 1888 a cash reward of 100 pounds was offered there for the discovery of any tree measuring more than 122 metres (400 ft).[24] The fact that such a considerable reward was never claimed is taken as evidence that such large trees did not exist. Carder's historical research, however, revealed that the reward was offered under conditions that made it highly unlikely to be collected. First, it was made in the depths of winter and applied only for a very short time. Next, the tree had to be measured by an accredited surveyor. Since loggers had already taken the largest trees from the most accessible Victorian forests, finding very tall trees then would have demanded an arduous trek into remote wilderness and at considerable altitude. In turn, that meant that searchers also needed the services of experienced bushmen to be able to guide them and conduct an effective search. Only one expedition actually penetrated one of the strongholds of E. regnans at Mount Baw Baw but its search was rendered ineffectual by cold and snow and managed to measure only a single living tree — the New Turkey Tree: 99.4 metres (326 ft) — before appalling conditions forced a retreat, Carder notes.Ferdinand von Mueller, claimed to have personally measured one tree near the headwaters of the Yarra River at 122 metres (400 ft). Nurseryman David Boyle, claimed in 1862 to have measured a fallen tree in a deep gully in the Dandenongs at 119.5 metres (392 ft), and with a diameter at its broken tip that indicated it might have lost another eight metres of trunk when it broke, for 128 metres (420 ft).[24][26]Von Mueller's early records also mention two trees on the nearby Black Spur Range, one alive and measuring 128 metres (420 ft) and another fallen tree said to measure 146 metres (479 ft), but these were either based on hearsay or uncertain reliability. David Boyle also reported that a tree at Cape Otway measured 170 metres (560 ft), but this too was based on hearsay.None, however, had been verified by direct documentation until 1982 when Ken Simpendorfer, a Special Projects Officer for the Forests Commission Victoria, directed a search of official Victorian archives. It unearthed a forgotten report from more than a century earlier, one that had not been referred to in other accounts of the species up to that time. It was written on 21 February 1872, by the Inspector of State Forests, William Ferguson, and was addressed to the Assistant Commissioner of Lands and Surveys, Clement Hodgkinson. Ferguson had been instructed to explore and inspect the watershed of the Watts River and reported trees in great number and exceptional size in areas where loggers had not yet reached. He wrote: "In one instance I measured with a tape line one huge specimen that lay prostrate across a tributary of the Watts, and found it to be 435 ft [133 m] from its root to the top of its trunk. At 5 feet from the ground it measures 18 feet in diameter, and at the extreme end where it has broken in its fall, it is 3 feet in diameter. This tree has been much burnt by fire, and I fully believe that before it fell it must have been more than 500 ft [150 m] high. As it now lies, it forms a complete bridge across a deep ravine."[24]It is also possible that individual trees will again attain such heights. Author Bob Beale has recorded that the tallest trees in the Black Spur Range now measure about 85 metres (279 ft) but — due to major bushfires in the 1920s and 30s — are less than 80 years old and have been growing consistently at the rate of about one metre a year.[27]A Eucalyptus regnans stand in the Orokonui Ecosanctuary near Dunedin, New Zealand (where E. regnans is an introduced species) contains that country's tallest measured tree, standing 80.5 metres high in 2012.[28]A Eucalyptus regnans in Greytown, New Zealand was measured at 107 feet in 2011 and its details can be found in the Notable Tree Register of New Zealand.[29]Eucalyptus regnans occurs in cool, mostly mountainous areas to 1,000 metres (3,300 ft) altitude with high rainfall of over 1,200 millimetres (47 in) per year. In Victoria, stands of tall trees are found in the Otway, Dandenong, Yarra and Strzelecki ranges as well as Mount Disappointment and East Gippsland.,[22] However, the distribution is much reduced. Most of the E. regnans forest across Gippsland was cleared for farmland between 1860 and 1880, and in the Otway Ranges between 1880 and 1900, while severe bushfires hit in 1851, 1898 and 1939.[8] In Tasmania, E. regnans is found in the Huon and Derwent River valleys in the southeast of the state.[2]In the Otways, the species is found in wet forest in pure stands or growing in association with mountain grey gum ( Eucalyptus cypellocarpa), messmate (E. obliqua) and Victorian blue gum (E. globulus subsp. bicostata).[30] Other trees it grows with include manna gum (Eucalyptus viminalis), shining gum (E. nitens), myrtle beech (Nothofagus cunninghamii) and silver wattle (Acacia dealbata)[2] The mountain ash-dominated forest can be interspersed with rainforest understory, with such species as southern sassafras (Atherosperma moschatum), celery-top pine (Phyllocladus aspleniifolius), leatherwood (Eucryphia lucida) and horizontal (Anodopetalum biglandulosum).[31] The mountain ash is most suited to deep friable clay loam soils, often of volcanic origin; in areas of poorer soils, it can be confined to watercourses and valleys.[2] The species grows very quickly, at more than a metre a year, and can reach 65 metres (213 ft) in 50 years, with an average life-span of 400 years. The fallen logs continue supporting a rich variety of life for centuries more on the forest floor.The majority of the endangered Leadbeater’s possum population lives in mountain ash forests (Eucalyptus regnans, E. delegatensis and E. nitens) in the Central Highlands of Victoria. The possums use hollows in old trees for nesting and shelter and forage for arboreal arthropods under bark.[30] The vegetation structure of these forests enables the possums to travel through them.[30] Both Leadbeaters possums and yellow-bellied gliders feed on the sap from the trunks and branches.[32] Koalas feed on the foliage, though it is not one of their preferred forage species.[33]Yellow-tailed black-cockatoos nest in the hollows of old trees,[34] in contrast to the Tasmanian wedge-tailed eagle (Aquila audax fleayi) that builds its nest of large sticks at the top of the trees.[35]In a small area of rainforest in Yarra Ranges National Park in Victoria, nine epiphyte species were observed growing on Eucalyptus regnans, the most prevalent of these being the liverwort Bazzania adnexa.[36]The spur-legged phasmid (Didymuria violescens) is a leaf-eating insect that can defoliate trees during major infestations such as one experienced at Powelltown in the early 1960s.[22] Leaves and buds are eaten by the larvae and adults of the chrysomelid leaf beetle Chrysophtharta bimaculata.[37] Stressed trees can be damaged by the eucalyptus longhorned borer (Phoracantha semipunctata), which burrows into the trunk, which exudes a red stain. Eucalypt weevils of the genus Gonipterus commonly damage E. regnans, while the tortoise beetle (Paropsis atomaria) is a common pest of plantations.[38]A study carried out by environmental scientist Professor Brendan Mackey of the Australian National University in 2009 identified that mountain ash forests in Victoria’s Central Highlands are the best in the world at locking up carbon.[39] Mackey and colleagues found the highest amount of carbon was contained in a forest located in the O'Shannassy River catchment, which held 1,867 tonnes of carbon per hectare. This area was a stand of unlogged mountain ash over 100 years old, which had had minimal human disturbance. They further calculated that a E. regnans-dominated forest with trees up to 250 years old and a well-established mid-storey and upper storey could store up to 2,844 tonnes of carbon per hectare.[40]Eucalyptus regnans lacks a lignotuber and hence cannot recover by reshooting after intense fire. Instead, it can only regenerate by seed, and is thus termed an obligate seeder.[41] The seeds are released from their woody capsules (gumnuts) by heat and for successful germination the seedlings require a high level of light, much more than reaches the forest floor when there is a mature tree canopy. Severe fires can kill all the trees in a forest, prompting a massive release of seed to take advantage of the nutrients in the ash bed. Seedling densities of up to 2.5 million per hectare have been recorded after a major fire. Competition and natural thinning eventually reduces the mature tree density to about 30 to 40 individuals per hectare. Because it takes roughly 20 years for seedlings to reach sexual maturity, repeated fires in the same area can cause local extinctions. Trees that escape severe fire may have a lifespan exceeding 500 years.[31] Cool temperate rainforest species that live in association with Eucalyptus regnans may gradually replace it in gullies or other areas where the trees succumb to age rather than fire.[12]Eucalyptus regnans is valued for its timber, and has been harvested in very large quantities. Aside from being logged in its natural range, it is grown in plantations in New Zealand and Chile, and to a limited extent, in South Africa and Zimbabwe.[38] Primary uses are sawlogging and woodchipping. It was a major source of newsprint in the 20th century. Much of the present woodchip harvest is exported to Japan. While the area of natural stands with large old trees is rapidly decreasing, substantial areas of regrowth exist and it is increasingly grown in plantations, the long, straight, fast growing trunks being much more commercially valuable than the old growth timber.[citation needed]It is a medium weight timber (about 680 kg/m³) and rather coarse (stringy) in texture. Gum veins are common. The wood is easy to work and the grain is straight with long, clear sections without knots. The wood works reasonably well for steam-bending.Primary uses for sawn wood are furniture, flooring (where its very pale blonde colour is highly prized), panelling, veneer, plywood, window frames, general construction. The wood has sometimes been used for wood wool and cooperage. However, the wood needs steam reconditioning for high value applications, due to a tendency to collapse on drying. This wood is highly regarded by builders, furniture makers and architects.[7]E. regnans forests are particularly susceptible to destruction by bushfire, and, to a lesser extent, timber harvesting. [42]Opposition to logging of wet forests by clearfelling has grown very strong in recent years (particularly opposition to woodchipping). It is a controversial debate with strong opinions both for and against timber harvesting.Several applications have been made to Victoria's Flora and Fauna Guarantee (FFG) Scientific Advisory Committee to list mountain ash forests as an endangered vegetation community. The committee rejected an application in 2017 as being ineligible and that it did not satisfy at least one of the criterion set out in the Flora and Fauna Guarantee Act 1988 and its Regulations of 2011. The assessment criteria included, was there a demonstrated state of decline, has there been a reduction in distribution or has vegetation community altered markedly.[43] Studies conducted by Murray Cunningham and David Ashton found that the re-growth habit of Eucalyptus regnans requires high light conditions, and the high nutrients contained in the ash layer. These conditions are found typically following a high intensity wildfire, which are an infrequent, yet periodic feature of mountain ash forests. For this reason clearfelling - with the complete removal of all trees, followed by a high intensity fire and seeding are used by the timber industry and forest scientists to ensure regeneration of harvested areas because it mimics the conditions found after high intensity wildfire.[44][45] Melbourne's forested water catchment areas, which provides water requiring little treatment, are composed of large areas E. regnans forest. The management of 157,000 hectares of Melbourne’s forested water catchments were vested in the Melbourne and Metropolitan Board of Works (MMBW) in 1891 with a closed catchment policy where timber harvesting and public access was not permitted. These areas are now included in the Yarra Ranges National Park. There has been a long running political campaign to add more areas to create the Great Forest National Park.[46]Water yields from catchments fall significantly for 20 to 40 years if trees are killed by bushfire or timber harvesting. The MMBW began research into forest cover on water supplies as early as 1948. In the early 1960s they set up a new series of paired catchment experiments in wet mountain forests near Healesville to measure the long term impacts of timber harvesting and bushfire on water quality and quantity. It took another 10 years for the results to emerge more clearly. It was found that while timber harvesting had an impact, the most dramatic threat to stream flows remained catastrophic bushfires like those on Black Friday in 1939 or Black Saturday in 2009.[47]In 2018, some researchers concluded that Mountain Ash forests in Victoria represent a collapsing ecosystems. They coined the term 'hidden collapse' meaning an ecosystems that give a superficial appearance of being intact but has lost key elements. At their research sites they found that between 1997 and 2011,  up to 50% of large old-cavity trees (trees with big holes that serve as nest sites for animals and birds) had been lost and there had also been a significant decline in the numbers of  tree dwelling marsupials such possums and gliders and birds. They identified fast and slow drivers of change: fire, logging and climate change and indicated that Mountain Ash forests would be replaced with Acacia-dominated woodlands [48]Eucalyptus regnans is too large for the majority of gardens, but may be suitable for parks.[49] Propagation is from seed, with the best germination rates being obtained by refrigerating for three weeks before sowing.[50] Seed may be stored for several years if refrigerated and kept dry. Seedlings are grown in containers but are more prone to damping off than other eucalypts; they are highly susceptible to Phytophthora cinnamomi and P. nicotianae Young plants are generally planted out once they are 8 or 9 months old. These are at risk of being eaten by grazing rabbits, wallabies and possums, which can destroy young plantations in severe cases.[38]American horticulturist and entrepreneur Ellwood Cooper noted its rapid growth but demanding soil requirements in his 1876 work Forest Culture and Eucalyptus Trees.[51] Eucalyptus regnans requires fertile soil with good drainage and annual rainfall of 1,000 millimetres (39 in) spread over the year, and has poor tolerance to temperatures below  −7 °C (19 °F) or drought.[38]Outside Australia, plantations have been successfully established in New Zealand, South Africa, Kenya and Tanzania.[52]
Floral formula
Floral formula is a means to represent the structure of a flower using numbers, letters and various symbols, presenting substantial information about the flower in a compact form. It can represent particular species, or can be generalized to characterize higher taxa, usually giving ranges of organ numbers. Floral formulae are one of the two ways of describing flower structure developed during the 19th century, the other being floral diagrams.[2] The format of floral formulae differs between authors, yet they tend to convey the same information.[1]Floral formulae were developed at the beginning of the 19th century.[2] The first authors using them were Cassel[3] (1820) and Martius[4] (1828). Grisebach[5] (1854) used them in his textbook to describe characteristics of floral families, stating numbers of different organs separated by commas and highlighting fusion. Sachs[6] (1873) used them together with floral diagrams, he noted their advantage of being composed of "ordinary typeface". Although Eichler widely used floral diagrams in his Blüthendiagramme,[7][8] he used floral formulae sparingly, mainly for families with simple flowers. Sattler's[9] Organogenesis of Flowers (1973) takes advantage of floral formulae and diagrams to describe the ontogeny of 50 plant species. Newer books containing formulae include Plant Systematics by Judd et al.[10] (2002) and Simpson[11] (2010). Prenner et al. devised an extension of the existing model to broaden the descriptive capability of the formula and argued that formulae should be included in formal taxonomic descriptions.[2] Ronse De Craene (2010)[1] partially utilized their way of writing the formulae in his book Floral Diagrams.The formula expresses counts of different floral organs,[note 1] these are usually preceded by letters or abbreviations according to the organ type. They are ordered corresponding to the arrangement of the parts of the flower from the outside to the inside:The labels with darker backgrounds are less common. "V" used by Prenner et al. for the number of ovules per gynoecium is followed by lowercase letter describing the type of placentation. For epicalyx/calyculus, the letter "k" is used.The numbers are inserted after the labels, they may be formatted as sub- or superscript. If an organ is absent, its number is written as "0" or it is omitted, if there are "many" (usually more than 10–12) instances, it can be written as "∞". Whorls of the same organ are separated by "+". Organ counts within a whorl can be separated by ":", for example when part of the whorl is morphologically different. A range can be given if the number is variable, e.g. when the formula summarizes a taxon.Groups of organs can be described by writing the number of instances in the group as superscript.The formula can also express organ fusion. Fusion of one organ type can be shown by enclosing the number in a circle, fusion of different organs can be represented by ties, as e.g. in Judd et al. Prenner et al. state that this method is difficult to achieve via standard typesetting.[2]:242 Joining of organs can be more readily written using parentheses "(…)" if instances of the same organ are fused. Fusion between different organs can be achieved by square "[…]", eventually curly brackets "{…}".Prenner et al. propose superscript zero for a lost organ, and superscript "r" for a reduced one. Ronse De Craene uses a degree symbol to mark a staminode (infertile stamen) or pistillode (infertile carpel).Ovary position is shown by alternating the "G" label. Simpson circumvents the intricate formatting by expressing the ovary position by words.Symmetry or arrangement may be described for the whole flower, in such case the corresponding symbol is usually placed at the beginning of the formula. It may be also outlined separately for different organs, placing it after their labels or numbers, or it may not be included in the formula at all. It is described by following symbols:Floral formula can also incorporate the fruit type, Judd et al.[10] place it at the very end.↯ K3 [C3 A1°–3°+½:2°] Ğ(3)[1]:39 – the formula of Canna edulis; asymmetric flower; calyx of three free sepals; corolla of three free petals joined with androecium; androecium in two whorls, the outer whorl contains 1–3 staminodes, the inner contains ½ of a stamen and 2 staminodes; gynoecium fused of 3 carpels, inferior ovaryB BtC K3:(2)C↓ C3:2r↓ A(3):2r↓+4r:10 G1↓ Vm8–10[2]:246 – the formula of Tamarindus indica; bract and petaloid bracteoles; monosymmetric calyx of three and two petaloid sepals; monosymmetric corolla of three and two reduced petals; two whorls of stamens, the outer monosymmetric from three fused and two reduced stamens, the inner of 4 reduced and 1 lost stamen; monosymmetric gynoecium of 1 carpel with superior ovary; marginal placentation with 8–10 ovules per gynoecium.
Wood-burning stove
A wood-burning stove (or wood burner or log burner in the UK) is a heating appliance capable of burning wood fuel and wood-derived biomass fuel, such as sawdust bricks. Generally the appliance consists of a solid metal (usually cast iron or steel) closed firebox, often lined by fire brick, and one or more air controls (which can be manually or automatically operated depending upon the stove). The first wood burning stove was patented in Strasbourg in 1557, two centuries before the Industrial Revolution, which would make iron an inexpensive and common material, so such stoves were high end consumer items and only gradually spread in use.[1][a]The stove is connected by ventilating stove pipe to a suitable flue, which will fill with hot combustion gases once the fuel is ignited. The chimney or flue gases must be hotter than the outside temperature to ensure combustion gases are drawn out of the fire chamber and up the chimney.Keeping the air flowing correctly through a wood-burning stove is essential for safe and efficient operation of the stove. Fresh air needs to enter the firebox to provide oxygen for the fire; as the fire burns, the smoke must be allowed to rise through the stove pipe, and exit through the chimney. To regulate air flow, there may be damper devices built into the stove, flue, and stove pipes.By opening or closing the dampers, air flow can be increased or decreased, which can fan the fire in the firebox, or "dampen" it by restricting airflow and reducing the flames. The dampers can usually be accessed by turning knobs or handles attached to the damper. Some stoves adjust their own airflow using mechanical or electronic thermostatic devices.The highest heating efficiencies on closed appliances can be attained by controlling the various supplies of air to the stove (operating the air controls correctly). On modern stoves, owner's manuals provide documented procedures. Fully open air controls may lead to more heat being sent straight up the chimney rather than into the room (which reduces efficiency). The biggest problem with leaving the air controls fully open on many stoves is “overfiring”. Overfiring is caused when too much heat is generated within the fire chamber, which will lead to warping, buckling and general damage to the stove and its internal components. Different stoves have different numbers and types of air controls.Modern building techniques have created more airtight homes, forcing many stove manufacturers to design their stoves to permit outside air intakes. Outside air can improve the overall efficiency of the stove as a heater by drawing cold combustion air directly from the outside instead of drawing preheated air from the room that the stove is in. Many modern stoves can optionally use an outside air intake. Many manufacturers supply the necessary parts in kit form (an Outside Air Kit, or OAK). When considering an outside air kit, it is important to know that the air must come in from below the level of the stove. For example, a basement stove may not safely use an outside air kit. This is to prevent a reversal of venting in which very hot flue gasses are exhausted through a (usually PVC) air intake pipe, which could lead to a structure fire and/or hot flue gasses being released into the structure.Firewood is usually measured in English-speaking countries in a quantity called a cord, measuring 128 cubic feet (3.6 m3) (a orderly 'tightly packed' stack 4' high x 4' deep x 8' wide). Firewood may be purchased by the cord, or by a fraction of a cord. The term "face cord" is commonly used to describe varying volumes of wood. Nominally it means 4' x 8' x an unspecified third measurement, but the term is often used by unscrupulous sellers to mean varying amounts.  Experienced firewood buyers and honest firewood sellers do not usually use the term "face cord".When purchasing, cutting, or collecting firewood, it is good to be aware of the difference between hardwood and softwood.  Both hardwood and softwood have similar energy contents by mass, but not by volume. In other words, a piece of hardwood would usually be heavier and have more available energy than the same sized piece of softwood. Hardwoods, derived from trees such as oak and ash, may burn at a slower rate, resulting in sustained output. Many softwoods are derived from conifers, which are fast growing and may burn at a faster rate. This is one reason why softwood pellets (for pellet stoves) are popular.The primary advantage of hardwoods are that they tend to contain more potential energy than the same volume of a softwood, thus increasing the amount of potential heat that can be stacked into one stoveload. Hardwood tends to form and maintain a bed of hot coals, which release lower amounts of heat for a long time. Hardwoods are ideal for long, low burns, especially in stoves with a poor ability to sustain a low burn, or in mild weather when high heat output is not required.Softwoods, in contrast, tend to burn hot and fast with little coaling. They may leave less ash than hardwoods. Softwoods are ideal for fast, hot burns. They produce excellent heat and do not fill the stove with coals, a frequent problem for those pushing their hardwood-fired stoves hard to get the maximum possible heat out of them.Not all hardwoods have a higher potential energy content than all softwoods. Wood varies by species and even individual trees (a tree with many years of slow growth will have a higher BTU content than a tree of the same species and same size than a tree with a few years of rapid growth). Osage orange, also known as hedge, is perhaps the highest-BTU wood that is common in North America.Many softwoods will season (dry) much more quickly than many hardwoods. For example, pine that has been cut, split, stacked and topcovered will usually be ready to burn in one year; oak may be expected to take three years under the same conditions.Softwood is often said to be dangerous to burn because it generates more dangerous creosote than hardwood. This myth is pervasive in the North American northeast, where both types of wood are commonly available. It is not common in the northwest, where most full-time wood burners burn pine and fir exclusively.  A basic understanding of what creosote is and how it accumulates in your flue is all you need to rid yourself of this byproduct.It is possible that this myth originated with old-fashioned stoves and fireplaces. These "appliances" did not require seasoned wood, and frequently did not receive it. As a result, they often experienced very low flue temperatures- usually in flues that were not insulated as modern flues are. The combination of low firebox temperatures due to high moisture content in the wood and low flue temperatures due to lack of insulation led to high levels of creosote accumulation. Burning a wood that emits a lot of sparks (such as pine) in an old-fashioned fireplace or stove will lead to sparks going directly into the flue, which can lead to a dangerous chimney fire if the flue is coated in creosote.Modern stoves which are operated properly do not cause this high level of creosote accumulation. While different wood species do contain varying levels of volatile organic compounds, the difference is academic to the wood burner. All woods produce creosote. All woods will cause creosote accumulation if burned improperly. So-called dangerous woods such as pine are in fact safer than woods such as oak, as they will burn hotter and thus help keep flue temperatures up, and their fast seasoning will help ensure that novice wood burners are burning reasonably dry wood.Dry wood produces more usable heat than wet wood, since the energy used to evaporate the water from the wood is lost up the chimney. Freshly cut wood (known as green wood) has a high moisture content. Different wood species have different moisture contents, which also vary tree to tree. Burning fuel that is mostly water uses much of the combustion energy to evaporate the water. This results in low firebox temperatures and low flue temperatures.Firewood with a moisture content below 20 per cent by weight can burn efficiently.  This is the "free" moisture content absorbed in the wood fibers, and does not include the chemically bound hydrogen and oxygen content. Moisture content can be reduced by outdoor air-drying ("seasoning"), for a period of several months in summer weather.  Solar-powered or fuel-fired kilns can accelerate the drying process.[2]The most common process of removing the excess moisture is called seasoning. Seasoning by air-drying the wood can take three years or more. Wood is dried in outdoor well-ventilated covered structures, or in a kiln.All wood will release creosote vapors when burned. Modern stoves will burn the vapors, either via direct secondary combustion or via a catalyst. Very little, if any, creosote will escape a properly operating modern stove's secondary combustion.Creosote that does escape may still not be harmful. It leaves the wood in gaseous form. It will not condense on surfaces above 250 degrees Fahrenheit. Modern flues are insulated to help ensure that they do not fall below this temperature during normal stove operation. Creosote accumulation can be dangerous, as it is flammable and burns hot. If a flue is coated with creosote and ignited, perhaps by a spark going up the flue, it can cause a serious chimney fire that can lead to a structure fire.  This can be avoided by using modern stoves and flue standards, burning dry wood, keeping your fires hot enough to maintain flue temperatures of at least 250 degrees F at the top of the flue, and proper chimney cleaning as needed.Multi-fuel stove designs are common in the United Kingdom, Ireland and Europe. They burn solid fuels only, including wood, wood pellets, coal and peat. They are typically made of steel or cast iron. Some models are also boiler stoves, with an attached water tank to provide hot water, and they can also be connected to radiators to add heat to the house, though they are usually not as efficient as a dedicated wood boiler.There are also stove models that can switch from wood fuel to oil or gas sources that are installed in the house to supply heat to a separate water boiler.[3] Stoves that readily convert to either oil or gas in addition to wood fuel have been manufactured in North America and Europe since the early 20th century, and are still manufactured. In some models, the oil or gas may fuel the stove through a pipe connection leading to a "pot burner" in the rear of the firewood compartment in the stove.\Multi-fuel stoves are versatile, but usually perform poorly compared to a stove that is designed to burn one specific fuel as well as possible.Modern wood stoves universally have some method of secondary combustion to burn unburned gasses for greatly improved efficiency and emissions.  One common method is via a catalyst.A catalytic wood stove will re-burn the gasses from the firebox in a catalyst- a matrix of steel or ceramic plated with a catalyst that allows combustion of these gasses at much lower temperatures than would ordinarily be possible.  This is why among modern stoves, catalytic models tend to be much better at achieving low, even heat output, which is desirable in warmer weather.Modern non-catalytic wood stoves will also reburn the gasses from the firebox, but require a much higher temperature for the secondary combustion. No catalyst is required.  These models lose a large amount of efficiency at low burn rates, as they cannot maintain secondary combustion, but can be very efficient at higher temperatures that allow secondary combustion.There also exist hybrid stoves that employ both catalytic and non-catalytic secondary combustion.Stoves that do not employ any secondary combustion still exist, but are markedly less efficient than a modern stove due to their lack of secondary combustion.In a conventional stove, when wood is added to a hot fire, a process of pyrolysis or destructive distillation begins. Gases (or volatiles) are evolved which are burned above the solid fuel. These are the two distinct processes going on in most solid fuel appliances. In obsolete stoves without secondary combustion, air had to be admitted both below and above the fuel to attempt to increase combustion and efficiency. The correct balance was difficult to achieve in practice, and many obsolete wood-burning stoves only admitted air above the fuel as a simplification. Often the volatiles were not completely burned, resulting in energy loss, chimney tarring, and atmospheric pollution.To overcome this, the pyrolyzing stove was developed. The two processes go on in separate parts of the stove with separately controlled air supplies. Most stoves designed to burn wood pellets fall into this category.Most pyrolyzing stoves regulate both fuel and air supply as opposed to controlling combustion of a mass of fuel by simple air regulation as in traditional stoves.The pelleted fuel is typically introduced into the pyrolyzing chamber with a screw conveyor. This leads to better and more efficient combustion of the fuel.The technology is not actually new; it has been used for decades in industrial coal-fired boilers intended to burn coal with high volatile content.Correct air flow and ventilation are also critical to efficient and safe wood burning. Specific requirements will be laid down by the stove manufacturer. Legal requirements for new installations in the UK can be found in Building Regulations Approved Document J, Section 2, Table 1 "Air Supply to solid fuel appliances".[4]The safe operation of a wood-burning stove requires regular maintenance such as emptying ash pans (containers) beneath the wood grate. Routine cleaning of the stove pipes and chimney is also needed to prevent chimney fires. Creosote and soot gradually build up in stovepipes and chimneys. This could damage the chimney and spread fire to the surrounding structure, especially the roof. When soot blocks the airflow through the stove pipes or chimney, smoke can build up in the stove pipes and in the house.The basic principle of controlling combustion by reducing the air supply means that very often there is a reduction zone/conditions within the stove. This means that carbon dioxide is often "reduced" to carbon monoxide, which is highly poisonous and must not be allowed to escape into the home. This can occur if the stove or chimney has not been cleaned or there is insufficient ventilation. Carbon monoxide detectors or alarms should always be installed according to manufacturers' recommendations where a wood stove is in use. Not all smoke detectors detect carbon monoxide.Fuel accelerants such as coal, grease, oil, gasoline, kerosene, plastics, and so on, also must never be added to firewood in a wood stove, since the flames produced may easily overwhelm the wood compartment and stove pipes and create a house fire.Under the United Kingdom's Clean Air Act, local authorities may declare the whole or part of the district of the authority to be a smoke control area. It is an offence to emit smoke from a chimney of a building, from a furnace or from any fixed boiler if located in a designated smoke control area. It is also an offence to acquire an “unauthorized fuel” for use within a smoke control area unless it is used in an “exempt” appliance (“exempted” from the controls which generally apply in the smoke control area). The current maximum level of fine is £1,000 for each offence.In order to comply with the Clean Air Act in "smoke control areas", an exempt appliance or fuel must be used.[5]The United States Clean Air Act requires that wood stoves be certified by the Environmental Protection Agency (EPA). These devices meet a particular emissions standard of no more than 7.5 grams per hour for non-catalytic wood stoves and 4.1 grams per hour for catalytic wood stoves.[6] Washington State has stricter requirements of a maximum of 4.5 grams per hour. However, the EPA has had no mandatory emission limits for pellet stoves, indoor or outdoor wood boilers, masonry stoves and certain types of wood stoves that are exempt from EPA regulation. EPA is developing new regulations and in 2015, these will begin to come into effect, establishing mandatory emission limits for almost all wood-burning appliances (fireplaces, chimeneas, and some other special appliances will still be exempt).[7]In some places, such as the Caribbean, Central America and South America, many houses have wood-burning stoves that are used indoors without any means of proper ventilation. Smoke stays in the house, where it is breathed in by the residents, harming their health. Nearly 2 million people are killed each year by indoor air pollution caused by open-fire cooking, mostly women and children, according to the World Health Organization (WHO). The cutting of large amounts of firewood also endangers local forests and ecosystems.[8]Non-governmental organizations (NGOs) such as Rotary International are actively assisting homeowners in constructing more fuel-efficient and safe wood-burning stoves. One design is called the Justa stove, Just stove, Ecostove, or La Estufa Justa. Justa stoves are made out of such materials as adobe, cement, and pumice, with chimneys. Other wood-burning stoves types are also being introduced to these communities, such as rocket stoves and haybox stoves. A rocket stove is up to 30% more fuel efficient than a Justa stove, but a small portable rocket stove (for cooking) does not have a chimney and is suitable for outdoor use only. Bigger rocket stoves are connected to chimney or flue-exhaust pipe. The haybox stove is another outdoor wood-burning stove. Haybox stoves use straw, wool, or foam as an insulator, reducing fuel use by up to 70%.[9]Italy is one of the biggest markets for pellet-burning stoves in Europe, having around 30% of all homes using wood for some heat. This means about 5 million homes have a wood fueled stove or cooker.[citation needed]Rocket mass heater in a tipi at Paul Wheaton's permaculture homestead in MontanaChimenea, burning wood for heatDaruma stove, a traditional Japanese wood-burning stoveCeramic-tiled kachelofen wood-burning stove in an Alsatian house, Strasbourg, France. Wooden laundry-drying racks hang over the stove.New Mexico woman cooking on a stove typical of North American kitchens, in 1941Wood-burning sauna stove, FinlandCustom-fitted fireplace insert with large glass doors, and a large heat exchanger for efficiencyWood stove used as an outdoor "evaporator" for producing maple syrup, New York StateSmall pot-bellied stove, Kabul, AfghanistanStove in the living room of a German farmStove in Massachusetts
Soil erosion
Soil erosion is the displacement of the upper layer of soil, one form of soil degradation. This natural process is caused by the dynamic activity of erosive agents, that is, water, ice (glaciers), snow, air (wind), plants, animals, and humans. In accordance with these agents, erosion is sometimes divided into water erosion, glacial erosion, snow erosion, wind (aeolean) erosion, zoogenic erosion, and anthropogenic erosion.[1] Soil erosion may be a slow process that continues relatively unnoticed, or it may occur at an alarming rate causing a serious loss of topsoil. The loss of soil from farmland may be reflected in reduced crop production potential, lower surface water quality and damaged drainage networks.Human activities have increased by 10–40 times the rate at which erosion is occurring globally. Excessive (or accelerated) erosion causes both "on-site" and "off-site" problems. On-site impacts include decreases in agricultural productivity and (on natural landscapes) ecological collapse, both because of loss of the nutrient-rich upper soil layers. In some cases, the eventual end result is desertification. Off-site effects include sedimentation of waterways and eutrophication of water bodies, as well as sediment-related damage to roads and houses. Water and wind erosion are the two primary causes of land degradation; combined, they are responsible for about 84% of the global extent of degraded land, making excessive erosion one of the most significant environmental problems worldwide.[2][3]Intensive agriculture, deforestation, roads, anthropogenic climate change and urban sprawl are amongst the most significant human activities in regard to their effect on stimulating erosion.[4] However, there are many prevention and remediation practices that can curtail or limit erosion of vulnerable soils.Rainfall, and the surface runoff which may result from rainfall, produces four main types of soil erosion: splash erosion, sheet erosion, rill erosion, and gully erosion. Splash erosion is generally seen as the first and least severe stage in the soil erosion process, which is followed by sheet erosion, then rill erosion and finally gully erosion (the most severe ohe'llof the four).[5][6]In splash erosion, the impact of a falling raindrop creates a small crater in the soil,[7] ejecting soil particles.[8] The distance these soil particles travel can be as much as 0.6 m (two feet) vertically and 1.5 m (five feet) horizontally on level ground.If the soil is saturated, or if the rainfall rate is greater than the rate at which water can infiltrate into the soil, surface runoff occurs. If the runoff has sufficient flow energy, it will transport loosened soil particles (sediment) down the slope.[9] Sheet erosion is the transport of loosened soil particles by overland flow.[9]Rill erosion refers to the development of small, ephemeral concentrated flow paths which function as both sediment source and sediment delivery systems for erosion on hillslopes. Generally, where water erosion rates on disturbed upland areas are greatest, rills are active. Flow depths in rills are typically of the order of a few centimeters (about an inch) or less and along-channel slopes may be quite steep. This means that rills exhibit hydraulic physics very different from water flowing through the deeper, wider channels of streams and rivers.[10]Gully erosion occurs when runoff water accumulates and rapidly flows in narrow channels during or immediately after heavy rains or melting snow, removing soil to a considerable depth.[11][12][13]Valley or stream erosion occurs with continued water flow along a linear feature. The erosion is both downward, deepening the valley, and headward, extending the valley into the hillside, creating head cuts and steep banks. In the earliest stage of stream erosion, the erosive activity is dominantly vertical, the valleys have a typical V cross-section and the stream gradient is relatively steep. When some base level is reached, the erosive activity switches to lateral erosion, which widens the valley floor and creates a narrow floodplain. The stream gradient becomes nearly flat, and lateral deposition of sediments becomes important as the stream meanders across the valley floor. In all stages of stream erosion, by far the most erosion occurs during times of flood, when more and faster-moving water is available to carry a larger sediment load. In such processes, it is not the water alone that erodes: suspended abrasive particles, pebbles and boulders can also act erosively as they traverse a surface, in a process known as traction.[14]Bank erosion is the wearing away of the banks of a stream or river. This is distinguished from changes on the bed of the watercourse, which is referred to as scour. Erosion and changes in the form of river banks may be measured by inserting metal rods into the bank and marking the position of the bank surface along the rods at different times.[15]Thermal erosion is the result of melting and weakening permafrost due to moving water.[16] It can occur both along rivers and at the coast. Rapid river channel migration observed in the Lena River of Siberia is due to thermal erosion, as these portions of the banks are composed of permafrost-cemented non-cohesive materials.[17] Much of this erosion occurs as the weakened banks fail in large slumps. Thermal erosion also affects the Arctic coast, where wave action and near-shore temperatures combine to undercut permafrost bluffs along the shoreline and cause them to fail. Annual erosion rates along a 100-kilometre (62-mile) segment of the Beaufort Sea shoreline averaged 5.6 metres (18 feet) per year from 1955 to 2002.[18]At extremely high flows, kolks, or vortices are formed by large volumes of rapidly rushing water. Kolks cause extreme local erosion, plucking bedrock and creating pothole-type geographical features called Rock-cut basins. Examples can be seen in the flood regions result from glacial Lake Missoula, which created the channeled scablands in the Columbia Basin region of eastern Washington.[19]Wind erosion is a major geomorphological force, especially in arid and semi-arid regions. It is also a major source of land degradation, evaporation, desertification, harmful airborne dust, and crop damage—especially after being increased far above natural rates by human activities such as deforestation, urbanization, and agriculture.[20][21]Wind erosion is of two primary varieties: deflation, where the wind picks up and carries away loose particles; and abrasion, where surfaces are worn down as they are struck by airborne particles carried by wind. Deflation is divided into three categories: (1) surface creep, where larger, heavier particles slide or roll along the ground; (2) saltation, where particles are lifted a short height into the air, and bounce and saltate across the surface of the soil; and (3) suspension, where very small and light particles are lifted into the air by the wind, and are often carried for long distances. Saltation is responsible for the majority (50–70%) of wind erosion, followed by suspension (30–40%), and then surface creep (5–25%).[22][23] Silty soils tend to be the most affected by wind erosion; silt particles are relatively easily detached and carried away.[24]Wind erosion is much more severe in arid areas and during times of drought. For example, in the Great Plains, it is estimated that soil loss due to wind erosion can be as much as 6100 times greater in drought years than in wet years.[25]Mass movement is the downward and outward movement of rock and sediments on a sloped surface, mainly due to the force of gravity.[26][27]Mass movement is an important part of the erosional process, and is often the first stage in the breakdown and transport of weathered materials in mountainous areas.[28] It moves material from higher elevations to lower elevations where other eroding agents such as streams and glaciers can then pick up the material and move it to even lower elevations. Mass-movement processes are always occurring continuously on all slopes; some mass-movement processes act very slowly; others occur very suddenly, often with disastrous results. Any perceptible down-slope movement of rock or sediment is often referred to in general terms as a landslide. However, landslides can be classijelloSlumping happens on steep hillsides, occurring along distinct fracture zones, often within materials like clay that, once released, may move quite rapidly downhill. They will often show a spoon-shaped isostatic depression, in which the material has begun to slide downhill. In some cases, the slump is caused by water beneath the slope weakening it. In many cases it is simply the result of poor engineering along highways where it is a regular occurrence.[citation needed]Surface creep is the slow movement of soil and rock debris by gravity which is usually not perceptible except through extended observation. However, the term can also describe the rolling of dislodged soil particles 0.5 to 1.0 mm (0.02 to 0.04 in) in diameter by wind along the soil surface.[citation needed]The amount and intensity of precipitation is the main climatic factor governing soil erosion by water. The relationship is particularly strong if heavy rainfall occurs at times when, or in locations where, the soil's surface is not well protected by vegetation. This might be during periods when agricultural activities leave the soil bare, or in semi-arid regions where vegetation is naturally sparse. Wind erosion requires strong winds, particularly during times of drought when vegetation is sparse and soil is dry (and so is more erodible). Other climatic factors such as average temperature and temperature range may also affect erosion, via their effects on vegetation and soil properties. In general, given similar vegetation and ecosystems, areas with more precipitation (especially high-intensity rainfall), more wind, or more storms are expected to have more erosion.In some areas of the world (e.g. the mid-western USA), rainfall intensity is the primary determinant of erosivity, with higher intensity rainfall generally resulting in more soil erosion by water. The size and velocity of rain drops is also an important factor. Larger and higher-velocity rain drops have greater kinetic energy, and thus their impact will displace soil particles by larger distances than smaller, slower-moving rain drops.[29]In other regions of the world (e.g. western Europe), runoff and erosion result from relatively low intensities of stratiform rainfall falling onto previously saturated soil. In such situations, rainfall amount rather than intensity is the main factor determining the severity of soil erosion by water.[30]The composition, moisture, and compaction of soil are all major factors in determining the erosivity of rainfall. Sediments containing more clay tend to be more resistant to erosion than those with sand or silt, because the clay helps bind soil particles together.[31] Soil containing high levels of organic materials are often more resistant to erosion, because the organic materials coagulate soil colloids and create a stronger, more stable soil structure.[32] The amount of water present in the soil before the precipitation also plays an important role, because it sets limits on the amount of water that can be absorbed by the soil (and hence prevented from flowing on the surface as erosive runoff). Wet, saturated soils will not be able to absorb as much rain water, leading to higher levels of surface runoff and thus higher erosivity for a given volume of rainfall.[32][33] Soil compaction also affects the permeability of the soil to water, and hence the amount of water that flows away as runoff. More compacted soils will have a larger amount of surface runoff than less compacted soils.[32]Vegetation acts as an interface between the atmosphere and the soil. It increases the permeability of the soil to rainwater, thus decreasing runoff. It shelters the soil from winds, which results in decreased wind erosion, as well as advantageous changes in microclimate. The roots of the plants bind the soil together, and interweave with other roots, forming a more solid mass that is less susceptible to both water and wind erosion. The removal of vegetation increases the rate of surface erosion.[34]The topography of the land determines the velocity at which surface runoff will flow, which in turn determines the erosivity of the runoff. Longer, steeper slopes (especially those without adequate vegetative cover) are more susceptible to very high rates of erosion during heavy rains than shorter, less steep slopes. Steeper terrain is also more prone to mudslides, landslides, and other forms of gravitational erosion processes.[35][36][37]Unsustainable agricultural practices are the single greatest contributor to the global increase in erosion rates.[38]The tillage of agricultural lands, which breaks up soil into finer particles, is one of the primary factors. The problem has been exacerbated in modern times, due to mechanized agricultural equipment that allows for deep plowing, which severely increases the amount of soil that is available for transport by water erosion. Others include mono-cropping, farming on steep slopes, pesticide and chemical fertilizer usage (which kill organisms that bind soil together), row-cropping, and the use of surface irrigation.[39][40] A complex overall situation with respect to defining nutrient losses from soils, could arise as a result of the size selective nature of soil erosion events. Loss of total phosphorus, for instance, in the finer eroded fraction is greater relative to the whole soil.[41] Extrapolating this evidence to predict subsequent behaviour within receiving aquatic systems, the reason is that this more easily transported material may support a lower solution P concentration compared to coarser sized fractions.[42] Tillage also increases wind erosion rates, by dehydrating the soil and breaking it up into smaller particles that can be picked up by the wind. Exacerbating this is the fact that most of the trees are generally removed from agricultural fields, allowing winds to have long, open runs to travel over at higher speeds.[43] Heavy grazing reduces vegetative cover and causes severe soil compaction, both of which increase erosion rates.[44]In an undisturbed forest, the mineral soil is protected by a layer of leaf litter and an humus that cover the forest floor. These two layers form a protective mat over the soil that absorbs the impact of rain drops. They are porous and highly permeable to rainfall, and allow rainwater to slow percolate into the soil below, instead of flowing over the surface as runoff.[45] The roots of the trees and plants[46] hold together soil particles, preventing them from being washed away.[45] The vegetative cover acts to reduce the velocity of the raindrops that strike the foliage and stems before hitting the ground, reducing their kinetic energy.[47] However it is the forest floor, more than the canopy, that prevents surface erosion. The terminal velocity of rain drops is reached in about 8 metres (26 feet). Because forest canopies are usually higher than this, rain drops can often regain terminal velocity even after striking the canopy. However, the intact forest floor, with its layers of leaf litter and organic matter, is still able to absorb the impact of the rainfall.[47][48]Deforestation causes increased erosion rates due to exposure of mineral soil by removing the humus and litter layers from the soil surface, removing the vegetative cover that binds soil together, and causing heavy soil compaction from logging equipment. Once trees have been removed by fire or logging, infiltration rates become high and erosion low to the degree the forest floor remains intact. Severe fires can lead to significant further erosion if followed by heavy rainfall.[49]Globally one of the largest contributors to erosive soil loss in the year 2006 is the slash and burn treatment of tropical forests. In a number of regions of the earth, entire sectors of a country have been rendered unproductive. For example, on the Madagascar high central plateau, comprising approximately ten percent of that country's land area, virtually the entire landscape is sterile of vegetation, with gully erosive furrows typically in excess of 50 metres (160 ft) deep and 1 kilometre (0.6 miles) wide. Shifting cultivation is a farming system which sometimes incorporates the slash and burn method in some regions of the world. This degrades the soil and causes the soil to become less and less fertile.[citation needed]Urbanization has major effects on erosion processes—first by denuding the land of vegetative cover, altering drainage patterns, and compacting the soil during construction; and next by covering the land in an impermeable layer of asphalt or concrete that increases the amount of surface runoff and increases surface wind speeds.[50] Much of the sediment carried in runoff from urban areas (especially roads) is highly contaminated with fuel, oil, and other chemicals.[51] This increased runoff, in addition to eroding and degrading the land that it flows over, also causes major disruption to surrounding watersheds by altering the volume and rate of water that flows through them, and filling them with chemically polluted sedimentation. The increased flow of water through local waterways also causes a large increase in the rate of bank erosion.[52]The warmer atmospheric temperatures observed over the past decades are expected to lead to a more vigorous hydrological cycle, including more extreme rainfall events.[53] The rise in sea levels that has occurred as a result of climate change has also greatly increased coastal erosion rates.[54][55]Studies on soil erosion suggest that increased rainfall amounts and intensities will lead to greater rates of soil erosion. Thus, if rainfall amounts and intensities increase in many parts of the world as expected, erosion will also increase, unless amelioration measures are taken. Soil erosion rates are expected to change in response to changes in climate for a variety of reasons. The most direct is the change in the erosive power of rainfall. Other reasons include: a) changes in plant canopy caused by shifts in plant biomass production associated with moisture regime; b) changes in litter cover on the ground caused by changes in both plant residue decomposition rates driven by temperature and moisture dependent soil microbial activity as well as plant biomass production rates; c) changes in soil moisture due to shifting precipitation regimes and evapo-transpiration rates, which changes infiltration and runoff ratios; d) soil erodibility changes due to decrease in soil organic matter concentrations in soils that lead to a soil structure that is more susceptible to erosion and increased runoff due to increased soil surface sealing and crusting; e) a shift of winter precipitation from non-erosive snow to erosive rainfall due to increasing winter temperatures; f) melting of permafrost, which induces an erodible soil state from a previously non-erodible one; and g) shifts in land use made necessary to accommodate new climatic regimes.[citation needed]Studies by Pruski and Nearing indicated that, other factors such as land use unconsidered, it is reasonable to expect approximately a 1.7% change in soil erosion for each 1% change in total precipitation under climate change.[56] In recent studies, there are predicted increasex of rainfall erosivity by 17% in the United States[57] and by 18% in Europe.[58]Due to the severity of its ecological effects, and the scale on which it is occurring, erosion constitutes one of the most significant global environmental problems we face today.[3]Water and wind erosion are now the two primary causes of land degradation; combined, they are responsible for 84% of degraded acreage.[2]Each year, about 75 billion tons of soil is eroded from the land—a rate that is about 13–40 times as fast as the natural rate of erosion.[61] Approximately 40% of the world's agricultural land is seriously degraded.[62] According to the United Nations, an area of fertile soil the size of Ukraine is lost every year because of drought, deforestation and climate change.[63] In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa.[64]Recent modeling developments have quantified rainfall erosivity at global scale using high temporal resolution(<30 min) and high fidelity rainfall recordings. The results is an extensive global data collection effort produced the Global Rainfall Erosivity Database (GloREDa) which includes rainfall erosivity for 3,625 stations and covers 63 countries. This first ever Global Rainfall Erosivity Database was used to develop a global erosivity map [65] at 30 arc-seconds(~1 km) based on sophisticated geostatistical process. According to a new study[66] published in Nature Communications, almost 36 billion tons of soil is lost every year due to water, and deforestation and other changes in land use make the problem worse. The study investigates global soil erosion dynamics by means of high-resolution spatially distributed modelling (ca. 250 × 250 m cell size). The geo-statistical approach allows, for the first time, the thorough incorporation into a global soil erosion model of land use and changes in land use, the extent, types, spatial distribution of global croplands and the effects of different regional cropping systems.The loss of soil fertility due to erosion is further problematic because the response is often to apply chemical fertilizers, which leads to further water and soil pollution, rather than to allow the land to regenerate.[67]Soil erosion (especially from agricultural activity) is considered to be the leading global cause of diffuse water pollution, due to the effects of the excess sediments flowing into the world's waterways. The sediments themselves act as pollutants, as well as being carriers for other pollutants, such as attached pesticide molecules or heavy metals.[68]The effect of increased sediments loads on aquatic ecosystems can be catastrophic. Silt can smother the spawning beds of fish, by filling in the space between gravel on the stream bed. It also reduces their food supply, and causes major respiratory issues for them as sediment enters their gills. The biodiversity of aquatic plant and algal life is reduced, and invertebrates are also unable to survive and reproduce. While the sedimentation event itself might be relatively short-lived, the ecological disruption caused by the mass die off often persists long into the future.[69]One of the most serious and long-running water erosion problems worldwide is in the People's Republic of China, on the middle reaches of the Yellow River and the upper reaches of the Yangtze River. From the Yellow River, over 1.6 billion tons of sediment flows into the ocean each year. The sediment originates primarily from water erosion in the Loess Plateau region of the northwest.[citation needed]Soil particles picked up during wind erosion of soil are a major source of air pollution, in the form of airborne particulates—"dust". These airborne soil particles are often contaminated with toxic chemicals such as pesticides or petroleum fuels, posing ecological and public health hazards when they later land, or are inhaled/ingested.[70][71][72][73]Dust from erosion acts to suppress rainfall and changes the sky color from blue to white, which leads to an increase in red sunsets[citation needed]. Dust events have been linked to a decline in the health of coral reefs across the Caribbean and Florida, primarily since the 1970s.[74] Similar dust plumes originate in the Gobi desert, which combined with pollutants, spread large distances downwind, or eastward, into North America.[75]Monitoring and modeling of erosion processes can help people better understand the causes of soil erosion, make predictions of erosion under a range of possible conditions, and plan the implementation of preventative and restorative strategies for erosion. However, the complexity of erosion processes and the number of scientific disciplines that must be considered to understand and model them (e.g. climatology, hydrology, geology, soil science, agriculture, chemistry, physics, etc.) makes accurate modelling challenging.[76][77][78] Erosion models are also non-linear, which makes them difficult to work with numerically, and makes it difficult or impossible to scale up to making predictions about large areas from data collected by sampling smaller plots.[79]The most commonly used model for predicting soil loss from water erosion is the Universal Soil Loss Equation (USLE). This was developed in the 1960s and 1970s. It estimates the average annual soil loss A on a plot-sized area as:[80]where R is the rainfall erosivity factor,[81][82] K is the soil erodibility factor,[83] L and S are topographic factors[84] representing length and slope,[85] C is the cover and management factor[86] and P is the support practices factor.[87]Despite the USLE's plot-scale spatial focus, the model has often been used to estimate soil erosion on much larger areas, such as watersheds or even whole continents. For example, RUSLE has recently been used to quantify soil erosion across the whole of Europe. One major problem is that the USLE cannot simulate gully erosion, and so erosion from gullies is ignored in any USLE-based assessment of erosion. Yet erosion from gullies can be a substantial proportion (10–80%) of total erosion on cultivated and grazed land.[88]During the 50 years since the introduction of the USLE, many other soil erosion models have been developed.[89] But because of the complexity of soil erosion and its constituent processes, all erosion models can give unsatisfactory results when validated i.e. when model predictions are compared with real-world measurements of erosion.[90][91] Thus new soil erosion models continue to be developed. Some of these remain USLE-based, e.g. the G2 model.[92][93] Other soil erosion models have largely (e.g. the Water Erosion Prediction Project model) or wholly (e.g. the Rangeland Hydrology and Erosion Model [94]) abandoned usage of USLE elements.The most effective known method for erosion prevention is to increase vegetative cover on the land, which helps prevent both wind and water erosion.[95] Terracing is an extremely effective means of erosion control, which has been practiced for thousands of years by people all over the world.[96] Windbreaks (also called shelterbelts) are rows of trees and shrubs that are planted along the edges of agricultural fields, to shield the fields against winds.[97] In addition to significantly reducing wind erosion, windbreaks provide many other benefits such as improved microclimates for crops (which are sheltered from the dehydrating and otherwise damaging effects of wind), habitat for beneficial bird species,[98] carbon sequestration,[99] and aesthetic improvements to the agricultural landscape.[100][101] Traditional planting methods, such as mixed-cropping (instead of monocropping) and crop rotation have also been shown to significantly reduce erosion rates.[102][103] Crop residues play a role in the mitigation of erosion, because they reduce the impact of raindrops breaking up the soil particles.[104] There is a higher potential for erosion when producing potatoes than when growing cereals, or oilseed crops.[105] Forages have a fibrous root system, which helps combat erosion by anchoring the plants to the top layer of the soil, and covering the entirety of the field, as it is a non-row crop.[106] In tropical coastal systems, properties of mangroves have been examined as a potential means to reduce soil erosion. Their complex root structures are known to help reduce wave damage from storms and flood impacts while binding and building soils. These roots can slow down water flow, leading to the deposition of sediments and reduced erosion rates. However, in order to maintain sediment balance, adequate mangrove forest width needs to be present.[107]
Xylem
Xylem is one of the two types of transport tissue in vascular plants, phloem being the other. The basic function of xylem is to transport water from roots to stems and leaves, but it also transports  nutrients.[1][2] The word "xylem" is derived from the Greek word ξύλον (xylon), meaning "wood"; the best-known xylem tissue is wood, though it is found throughout the plant.[3] The term was introduced by Carl Nägeli in 1858.[4][5]The most distinctive xylem cells are the long tracheary elements that transport water. Tracheids and vessel elements are distinguished by their shape;  vessel elements are shorter, and are connected together into long tubes that are called vessels.[6]Xylem also contains two other cell types: parenchyma and fibers.[7]Xylem can be found:In transitional stages of plants with secondary growth, the first two categories are not mutually exclusive, although usually a vascular bundle will contain primary xylem only.The branching pattern exhibited by xylem follows Murray's law.[8]Primary xylem is formed during primary growth from procambium. It includes protoxylem and metaxylem. Metaxylem develops after the protoxylem but before secondary xylem. Metaxylem has wider vessels and tracheids than protoxylem.Secondary xylem is formed during secondary growth from vascular cambium. Although secondary xylem is also found in members of the gymnosperm groups Gnetophyta and Ginkgophyta and to a lesser extent in members of the Cycadophyta, the two main groups in which secondary xylem can be found are:The xylem, vessels and tracheids of the roots, stems and leaves are interconnected to form a continuous system of water conducting channels reaching all parts of the plants. It transports water and soluble mineral nutrients from the roots throughout the plant. It is also used to replace water lost during transpiration and photosynthesis. Xylem sap consists mainly of water and inorganic ions, although it can also contain a number of organic chemicals as well. The transport is passive, not powered by energy spent by the tracheary elements themselves, which are dead by maturity and no longer have living contents. Transporting sap upwards becomes more difficult as the height of a plant increases and upwards transport of water by xylem is considered to limit the maximum height of trees.[10] Three phenomena cause xylem sap to flow:The primary force that creates the capillary action movement of water upwards in plants is the adhesion between the water and the surface of the xylem conduits.[13][14] Capillary action provides the force that establishes an equilibrium configuration, balancing gravity. When transpiration removes water at the top, the flow is needed to return to the equilibrium.Transpirational pull results from the evaporation of water from the surfaces of cells in the leaves. This evaporation causes the surface of the water to recess into the pores of the cell wall. By capillary action, the water forms concave menisci inside the pores. The high surface tension of water pulls the concavity outwards, generating enough force to lift water as high as a hundred meters from ground level to a tree's highest branches.Transpirational pull requires that the vessels transporting the water be very small in diameter; otherwise, cavitation would break the water column. And as water evaporates from leaves, more is drawn up through the plant to replace it. When the water pressure within the xylem reaches extreme levels due to low water input from the roots (if, for example, the soil is dry), then the gases come out of solution and form a bubble – an embolism forms, which will spread quickly to other adjacent cells, unless bordered pits are present (these have a plug-like structure called a torus, that seals off the opening between adjacent cells and stops the embolism from spreading).The cohesion-tension theory is a theory of intermolecular attraction that explains the process of water flow upwards (against the force of gravity) through the xylem of plants. It was proposed in 1894 by John Joly and Henry Horatio Dixon.[15][16]  Despite numerous objections,[17][18] this is the most widely accepted theory for the transport of water through a plant's vascular system based on the classical research of Dixon-Joly (1894),  Eugen Askenasy (1845–1903) (1895),[19][20] and Dixon (1914,1924).[21][22]Water is a polar molecule. When two water molecules approach one another, the slightly negatively charged oxygen atom of one forms a hydrogen bond with a slightly positively charged hydrogen atom in the other. This attractive force, along with other intermolecular forces, is one of the principal factors responsible for the occurrence of surface tension in liquid water. It also allows plants to draw water from the root through the xylem to the leaf.Water is constantly lost through transpiration from the leaf. When one water molecule is lost another is pulled along by the processes of cohesion and tension. Transpiration pull, utilizing capillary action and the inherent surface tension of water, is the primary mechanism of water movement in plants. However, it is not the only mechanism involved. Any use of water in leaves forces water to move into them.Transpiration in leaves creates tension (differential pressure) in the cell walls of mesophyll cells. Because of this tension, water is being pulled up from the roots into the leaves, helped by cohesion (the pull between individual water molecules, due to hydrogen bonds) and adhesion (the stickiness between water molecules and the hydrophilic cell walls of plants). This mechanism of water flow works because of water potential (water flows from high to low potential), and the rules of simple diffusion.[23]Over the past century, there has been a great deal of research regarding the mechanism of xylem sap transport; today, most plant scientists continue to agree that the cohesion-tension theory best explains this process, but multiforce theories that hypothesize several alternative mechanisms have been suggested, including longitudinal cellular and xylem osmotic pressure gradients, axial potential gradients in the vessels, and gel- and gas-bubble-supported interfacial gradients.[24][25]Until recently, the differential pressure (suction) of transpirational pull could only be measured indirectly, by applying external pressure with a pressure bomb to counteract it.[26] When the technology to perform direct measurements with a pressure probe was developed, there was initially some doubt about whether the classic theory was correct, because some workers were unable to demonstrate negative pressures. More recent measurements do tend to validate the classic theory, for the most part. Xylem transport is driven by a combination[27] of transpirational pull from above and root pressure from below, which makes the interpretation of measurements more complicated.Xylem appeared early in the history of terrestrial plant life. Fossil plants with anatomically preserved xylem are known from the Silurian (more than 400 million years ago), and trace fossils resembling individual xylem cells may be found in earlier Ordovician rocks. The earliest true and recognizable xylem consists of tracheids with a helical-annular reinforcing layer added to the cell wall. This is the only type of xylem found in the earliest vascular plants, and this type of cell continues to be found in the protoxylem (first-formed xylem) of all living groups of plants. Several groups of plants later developed pitted tracheid cells, it seems, through convergent evolution. In living plants, pitted tracheids do not appear in development until the maturation of the metaxylem (following the protoxylem).In most plants, pitted tracheids function as the primary transport cells. The other type of tracheary element, besides the tracheid, is the vessel element. Vessel elements are joined by perforations into vessels. In vessels, water travels by bulk flow, as in a pipe, rather than by diffusion through cell membranes. The presence of vessels in xylem has been considered to be one of the key innovations that led to the success of the angiosperms.[28] However, the occurrence of vessel elements is not restricted to angiosperms, and they are absent in some archaic or "basal" lineages of the angiosperms: (e.g., Amborellaceae, Tetracentraceae, Trochodendraceae, and Winteraceae), and their secondary xylem is described by Arthur Cronquist as "primitively vesselless". Cronquist considered the vessels of Gnetum to be convergent with those of angiosperms.[29] Whether the absence of vessels in basal angiosperms is a primitive condition is contested, the alternative hypothesis states that vessel elements originated in a precursor to the angiosperms and were subsequently lost.To photosynthesize, plants must absorb CO2 from the atmosphere. However, this comes at a price: while stomata are open to allow CO2 to enter, water can evaporate.[30] Water is lost much faster than CO2 is absorbed, so plants need to replace it, and have developed systems to transport water from the moist soil to the site of photosynthesis.[30] Early plants sucked water between the walls of their cells, then evolved the ability to control water loss (and CO2 acquisition) through the use of stomata. Specialized water transport tissues soon evolved in the form of hydroids, tracheids, then secondary xylem, followed by an endodermis and ultimately vessels.[30]The high CO2 levels of Silurian-Devonian times, when plants were first colonizing land, meant that the need for water was relatively low. As CO2 was withdrawn from the atmosphere by plants, more water was lost in its capture, and more elegant transport mechanisms evolved.[30] As water transport mechanisms, and waterproof cuticles, evolved, plants could survive without being continually covered by a film of water. This transition from poikilohydry to homoiohydry opened up new potential for colonization.[30] Plants then needed a robust internal structure that held long narrow channels for transporting water from the soil to all the different parts of the above-soil plant, especially to the parts where photosynthesis occurred.During the Silurian, CO2 was readily available, so little water needed expending to acquire it. By the end of the Carboniferous, when CO2 levels had lowered to something approaching today's, around 17 times more water was lost per unit of CO2 uptake.[30] However, even in these "easy" early days, water was at a premium, and had to be transported to parts of the plant from the wet soil to avoid desiccation. This early water transport took advantage of the cohesion-tension mechanism inherent in water. Water has a tendency to diffuse to areas that are drier, and this process is accelerated when water can be wicked along a fabric with small spaces. In small passages, such as that between the plant cell walls (or in tracheids), a column of water behaves like rubber – when molecules evaporate from one end, they pull the molecules behind them along the channels. Therefore, transpiration alone provided the driving force for water transport in early plants.[30] However, without dedicated transport vessels, the cohesion-tension mechanism cannot transport water more than about 2 cm, severely limiting the size of the earliest plants.[30] This process demands a steady supply of water from one end, to maintain the chains; to avoid exhausting it, plants developed a waterproof cuticle. Early cuticle may not have had pores but did not cover the entire plant surface, so that gas exchange could continue.[30] However, dehydration at times was inevitable; early plants cope with this by having a lot of water stored between their cell walls, and when it comes to it sticking out the tough times by putting life "on hold" until more water is supplied.[30]To be free from the constraints of small size and constant moisture that the parenchymatic transport system inflicted, plants needed a more efficient water transport system. During the early Silurian, they developed specialized cells, which were lignified (or bore similar chemical compounds)[30] to avoid implosion; this process coincided with cell death, allowing their innards to be emptied and water to be passed through them.[30] These wider, dead, empty cells were a million times more conductive than the inter-cell method, giving the potential for transport over longer distances, and higher CO2 diffusion rates.The earliest macrofossils to bear water-transport tubes are Silurian plants placed in the genus Cooksonia.[31] The early Devonian pretracheophytes Aglaophyton and Horneophyton have structures very similar to the hydroids of modern mosses.Plants continued to innovate new ways of reducing the resistance to flow within their cells, thereby increasing the efficiency of their water transport. Bands on the walls of tubes, in fact apparent from the early Silurian onwards,[32] are an early improvisation to aid the easy flow of water.[33] Banded tubes, as well as tubes with pitted ornamentation on their walls, were lignified[34] and, when they form single celled conduits, are considered to be tracheids. These, the "next generation" of transport cell design, have a more rigid structure than hydroids, allowing them to cope with higher levels of water pressure.[30] Tracheids may have a single evolutionary origin, possibly within the hornworts,[35] uniting all tracheophytes (but they may have evolved more than once).[30]Water transport requires regulation, and dynamic control is provided by stomata.[36]By adjusting the amount of gas exchange, they can restrict the amount of water lost through transpiration. This is an important role where water supply is not constant, and indeed stomata appear to have evolved before tracheids, being present in the non-vascular hornworts.[30]An endodermis probably evolved during the Silu-Devonian, but the first fossil evidence for such a structure is Carboniferous.[30] This structure in the roots covers the water transport tissue and regulates ion exchange (and prevents unwanted pathogens etc. from entering the water transport system). The endodermis can also provide an upwards pressure, forcing water out of the roots when transpiration is not enough of a driver.Once plants had evolved this level of controlled water transport, they were truly homoiohydric, able to extract water from their environment through root-like organs rather than relying on a film of surface moisture, enabling them to grow to much greater size.[30] As a result of their independence from their surroundings, they lost their ability to survive desiccation – a costly trait to retain.[30]During the Devonian, maximum xylem diameter increased with time, with the minimum diameter remaining pretty constant.[33] By the middle Devonian, the tracheid diameter of some plant lineages (Zosterophyllophytes) had plateaued.[33] Wider tracheids allow water to be transported faster, but the overall transport rate depends also on the overall cross-sectional area of the xylem bundle itself.[33] The increase in vascular bundle thickness further seems to correlate with the width of plant axes, and plant height; it is also closely related to the appearance of leaves[33] and increased stomatal density, both of which would increase the demand for water.[30]While wider tracheids with robust walls make it possible to achieve higher water transport pressures, this increases the problem of cavitation.[30] Cavitation occurs when a bubble of air forms within a vessel, breaking the bonds between chains of water molecules and preventing them from pulling more water up with their cohesive tension. A tracheid, once cavitated, cannot have its embolism removed and return to service (except in a few advanced angiosperms[37][38] which have developed a mechanism of doing so). Therefore, it is well worth plants' while to avoid cavitation occurring. For this reason, pits in tracheid walls have very small diameters, to prevent air entering and allowing bubbles to nucleate. Freeze-thaw cycles are a major cause of cavitation. Damage to a tracheid's wall almost inevitably leads to air leaking in and cavitation, hence the importance of many tracheids working in parallel.[30]Cavitation is hard to avoid, but once it has occurred plants have a range of mechanisms to contain the damage.[30] Small pits link adjacent conduits to allow fluid to flow between them, but not air – although ironically these pits, which prevent the spread of embolisms, are also a major cause of them.[30] These pitted surfaces further reduce the flow of water through the xylem by as much as 30%.[30] Conifers, by the Jurassic, developed an ingenious improvement, using valve-like structures to isolate cavitated elements. These torus-margo structures have a blob floating in the middle of a donut; when one side depressurizes the blob is sucked into the torus and blocks further flow.[30] Other plants simply accept cavitation; for instance, oaks grow a ring of wide vessels at the start of each spring, none of which survive the winter frosts. Maples use root pressure each spring to force sap upwards from the roots, squeezing out any air bubbles.Growing to height also employed another trait of tracheids – the support offered by their lignified walls. Defunct tracheids were retained to form a strong, woody stem, produced in most instances by a secondary xylem. However, in early plants, tracheids were too mechanically vulnerable, and retained a central position, with a layer of tough sclerenchyma on the outer rim of the stems.[30] Even when tracheids do take a structural role, they are supported by sclerenchymatic tissue.Tracheids end with walls, which impose a great deal of resistance on flow;[33] vessel members have perforated end walls, and are arranged in series to operate as if they were one continuous vessel.[33] The function of end walls, which were the default state in the Devonian, was probably to avoid embolisms. An embolism is where an air bubble is created in a tracheid. This may happen as a result of freezing, or by gases dissolving out of solution. Once an embolism is formed, it usually cannot be removed (but see later); the affected cell cannot pull water up, and is rendered useless.End walls excluded, the tracheids of prevascular plants were able to operate under the same hydraulic conductivity as those of the first vascular plant, Cooksonia.[33]The size of tracheids is limited as they comprise a single cell; this limits their length, which in turn limits their maximum useful diameter to 80 μm.[30] Conductivity grows with the fourth power of diameter, so increased diameter has huge rewards; vessel elements, consisting of a number of cells, joined at their ends, overcame this limit and allowed larger tubes to form, reaching diameters of up to 500 μm, and lengths of up to 10 m.[30]Vessels first evolved during the dry, low CO2 periods of the late Permian, in the horsetails, ferns and Selaginellales independently, and later appeared in the mid Cretaceous in angiosperms and gnetophytes.[30]Vessels allow the same cross-sectional area of wood to transport around a hundred times more water than tracheids![30] This allowed plants to fill more of their stems with structural fibers, and also opened a new niche to vines, which could transport water without being as thick as the tree they grew on.[30] Despite these advantages, tracheid-based wood is a lot lighter, thus cheaper to make, as vessels need to be much more reinforced to avoid cavitation.[30]Xylem development can be described by four terms: centrarch, exarch, endarch and mesarch.  As it develops in young plants, its nature changes from protoxylem to metaxylem (i.e. from first xylem to after xylem). The patterns in which protoxylem and metaxylem are arranged is important in the study of plant morphology.As a young vascular plant grows, one or more strands of primary xylem form in its stems and roots. The first xylem to develop is called 'protoxylem'. In appearance protoxylem is usually distinguished by narrower vessels formed of smaller cells. Some of these cells have walls which contain thickenings in the form of rings or helices. Functionally, protoxylem can extend: the cells are able to grow in size and develop while a stem or root is elongating. Later, 'metaxylem' develops in the strands of xylem. Metaxylem vessels and cells are usually larger; the cells have thickenings which are typically either in the form of ladderlike transverse bars (scalariform) or continuous sheets except for holes or pits (pitted). Functionally, metaxylem completes its development after elongation ceases when the cells no longer need to grow in size.[39][40]There are four main patterns to the arrangement of protoxylem and metaxylem in stems and roots.The other three terms are used where there is more than one strand of primary xylem.In his book De plantis libri XVI (On Plants, in 16 books) (1583), the Italian physician and botanist Andrea Cesalpino proposed that plants draw water from soil not by magnetism (ut magnes ferrum trahit, as magnetic iron attracts) nor by suction (vacuum), but by absorption, as occurs in the case of linen, sponges, or powders.[42]  The Italian biologist Marcello Malpighi was the first person to describe and illustrate xylem vessels, which he did in his book Anatome plantarum … (1675).[43][note 1] Although Malpighi believed that xylem contained only air, the British physician and botanist Nehemiah Grew, who was Malpighi's contemporary, believed that sap ascended both through the bark and through the xylem.[44] However, according to Grew, capillary action in the xylem would raise the sap by only a few inches; in order to raise the sap to the top of a tree, Grew proposed that the parenchymal cells become turgid and thereby not only squeeze the sap in the tracheids but force some sap from the parenchyma into the tracheids.[45]  In 1727, English clergyman and botanist Stephen Hales showed that transpiration by a plant's leaves causes water to move through its xylem.[46][note 2]  By 1891, the Polish-German botanist Eduard Strasburger had shown that the transport of water in plants did not require the xylem cells to be alive.[47]
Climax species
Climax species, also called late seral, late-successional, K-selected or equilibrium species, are plant species that will remain essentially unchanged in terms of species composition for as long as a site remains undisturbed.  They are the most shade-tolerant species of tree to establish in the process of forest succession.  The seedlings of climax species can grow in the shade of the parent trees, ensuring their dominance indefinitely.  A disturbance, such as fire, may kill the climax species, allowing pioneer or earlier successional species to re-establish for a time.  They are the opposite of pioneer species, also known as ruderal, fugitive, opportunistic or R-selected species, in the sense that climax species are good competitors but poor colonizers, whereas pioneer species are good colonizers but poor competitors.  Climax species dominate the climax community, when the pace of succession slows down, the result of ecological homeostasis, which features maximum permitted biodiversity, given the prevailing ecological conditions.  Their reproductive strategies and other adaptive characteristics can be considered more sophisticated than those of opportunistic species.  Through negative feedback, they adapt themselves to specific environmental conditions. Climax species are mostly found in forests. Climax species, closely controlled by carrying capacity, follow K strategies, wherein species produce fewer numbers of potential offspring, but invest more heavily in securing the reproductive success of each one to the micro-environmental conditions of its specific ecological niche.  Climax species might be iteroparous, energy consumption efficient and nutrient cycling.[1]The idea of a climax species has been criticized in recent ecological literature.[2]  Any assessment of successional states depends on assumptions about the natural fire regime.  But the idea of a dominant species is still widely used in silvicultural programs and California Department of Forestry literature.White spruce (Picea glauca) is an example of a climax species in the northern forests of North America.Other examples:
Phyllotaxis
In botany, phyllotaxis or phyllotaxy is the arrangement of leaves on a plant stem (from Ancient Greek phýllon "leaf" and táxis "arrangement").[1] Phyllotactic spirals form a distinctive class of patterns in nature.The basic arrangements of leaves on a stem are opposite and alternate (also known as spiral). Leaves may also be whorled if several leaves arise, or appear to arise, from the same level (at the same node) on a stem. With an opposite leaf arrangement, two leaves arise from the stem at the same level (at the same node), on opposite sides of the stem. An opposite leaf pair can be thought of as a whorl of two leaves.With an alternate (spiral) pattern, each leaf arises at a different point (node) on the stem.Distichous phyllotaxis, also called "two-ranked leaf arrangement" is a special case of either opposite or alternate leaf arrangement where the leaves on a stem are arranged in two vertical columns on opposite sides of the stem. Examples include various bulbous plants such as Boophone. It also occurs in other plant habits such as those of Gasteria or Aloe seedlings, and also in mature plants of related species such as Kumara plicatilis (Aloe plicatilis).In an opposite pattern, if successive leaf pairs are 90 degrees apart, this habit is called decussate. It is common in members of the family Crassulaceae[2] Decussate phyllotaxis also occurs in the Aizoaceae. In genera of the Aizoaceae, such as Lithops and Conophytum, many species have just two fully developed leaves at a time, the older pair folding back and dying off to make room for the decussately oriented new pair as the plant grows.[3]The whorled arrangement is fairly unusual on plants except for those with particularly short internodes. Examples of trees with whorled phyllotaxis are Brabejum stellatifolium[4] and the related genus Macadamia.[5]A whorl can occur as a basal structure where all the leaves are attached at the base of the shoot and the internodes are small or nonexistent. A basal whorl with a large number of leaves spread out in a circle is called a rosette.The rotational angle from leaf to leaf in a repeating spiral can be represented by a fraction of a full rotation around the stem.Alternate distichous leaves will have an angle of 1/2 of a full rotation. In beech and hazel the angle is 1/3, in oak and apricot it is 2/5, in sunflowers, poplar, and pear, it is 3/8, and in willow and almond the angle is 5/13.[6] The numerator and denominator normally consist of a  Fibonacci number and its second successor. The number of leaves is sometimes called rank, in the case of simple Fibonacci ratios, because the leaves line up in vertical rows. With larger Fibonacci pairs,  the pattern becomes complex and non-repeating. This tends to occur with a basal configuration. Examples can be found in composite flowers and seed heads.  The most famous example is the sunflower head.  This phyllotactic pattern creates an optical effect of criss-crossing spirals. In the botanical literature, these designs are described by the number of counter-clockwise spirals and the number of clockwise spirals. These also turn out to be Fibonacci numbers. In some cases, the numbers appear to be multiples of Fibonacci numbers because the spirals consist of whorls.The pattern of leaves on a plant is ultimately controlled by the local depletion of the plant hormone auxin in certain areas of the meristem.[7] Leaves become initiated in localized areas where auxin is absent.[disputed  – discuss] When a leaf is initiated and begins development, auxin begins to flow towards it, thus depleting auxin from another area on the meristem where a new leaf is to be initiated. This gives rise to a self-propagating system that is ultimately controlled by the ebb and flow of auxin in different regions of the meristematic topography.[8]Insight into the mechanism had to wait until Wilhelm Hofmeister proposed a model in 1868. A primordium, the nascent leaf, forms at the least crowded part of the shoot meristem. The golden angle between successive leaves is the blind result of this jostling. Since three golden arcs add up to slightly more than enough to wrap a circle, this guarantees that no two leaves ever follow the same radial line from center to edge. The generative spiral is a consequence of the same process that produces the clockwise and counter-clockwise spirals that emerge in densely packed plant structures, such as Protea flower disks or pinecone scales.In modern times, researchers such as Snow and Snow[9] have continued these lines of inquiry. Computer modeling and morphological studies have confirmed and refined Hoffmeister's ideas. Questions remain about the details. Botanists are divided on whether the control of leaf migration depends on chemical gradients among the primordia or purely mechanical forces. Lucas rather than Fibonacci numbers have been observed in a few plants[citation needed] and occasionally the leaf positioning appears to be random.Physical models of phyllotaxis date back to Airy's experiment of packing hard spheres. Gerrit van Iterson diagrammed grids imagined on a cylinder (Rhombic Lattices).[10] Douady et al. showed that phyllotactic patterns emerge as self-organizing processes in dynamic systems.[11] In 1991, Levitov proposed that lowest energy configurations of repulsive particles in cylindrical geometries reproduce the spirals of botanical phyllotaxis.[12] More recently, Nisoli et al. (2009) showed that to be true by constructing a "magnetic cactus" made of magnetic dipoles mounted on bearings stacked along a "stem".[13][14] They  demonstrated that these interacting particles can access novel dynamical phenomena beyond what botany yields: a "Dynamical Phyllotaxis" family of non local topological solitons emerge in the nonlinear regime of these systems, as well as purely classical rotons and maxons in the spectrum of linear excitations.Close packing of spheres generates a dodecahedral tessellation with pentaprismic faces. Pentaprismic symmetry is related to the Fibonacci series and the golden section of classical geometry.[15][16]Phyllotaxis has been used as an inspiration for a number of sculptures and architectural designs. Akio Hizume has built and exhibited several bamboo towers based on the Fibonacci sequence which exhibit phyllotaxis.[17] Saleh Masoumi has proposed a design for an apartment building where the apartment balconies project in a spiral arrangement around a central axis and each one does not shade the balcony of the apartment directly beneath.[18]
Kalpavriksha
Kalpavriksha (Devanagari: कल्पवृक्ष), also known as kalpataru, karpaga viruksham,kalpadruma or kalpapādapa, is a wish-fulfilling divine tree in Hindu mythology. It is mentioned in Sanskrit literature from the earliest sources. It is also a popular theme in Jain cosmology and Buddhism. Sage Durvasa and Adi Shankaracharya, meditated under the Kalpavriksha. The birth of Ashokasundari, the daughter of Shiva and Parvati, is attributed to the Kalpavriksha tree. Another daughter Aranyani was also gifted to Kalpavriksha for safekeeping.The Kalpavriksha originated during the Samudra manthan or "churning of the ocean of milk" along with the Kamadhenu, the divine cow providing for all needs. The king of the gods, Indra, returned with this tree to his paradise.Kalpavriksha is also identified with many trees such as Parijata (Erythrina variegata), Ficus benghalensis, coconut tree (Cocos nucifera), Acacia, Madhuca longifolia, Prosopis cineraria, Bassia butyracea, and mulberry tree (Morus nigra tree). The tree is also extolled in iconography and literature.Kalpavriksha is an artistic and literary theme common to the Hindu Bhagavatas, the Jains and the Buddhists.[1]Kalpavriksha, the tree of life, also meaning "World Tree" finds mention in the Vedic scriptures. In the earliest account of the Samudra manthan or "churning of the ocean of milk" Kalpavriksha emerged from the primal waters during the ocean churning process along with Kamadhenu, the divine cow that bestows all needs. The tree is also said to be the Milky way or the birthplace of the stars Sirius. The king of the gods, Indra returned with this Kalpavriksha to his abode, the paradise and planted it there. Tree also finds mention in the Sanskrit text Mānāsara, part of Shilpa Shastras.[2][3]Another myth says that Kalpavriksha was located on earth and was transported to Indra's abode after people started misusing it by wishing evil and wrong things.[4]  In Indra's "Devaloka" it is said that there are five Kalpavrikshas, which are called Mandana, Parijata, Santana, Kalpavriksha and Harichandana, all of which fulfill various wishes.[4] Kalpavriksha, in particular, is said to be planted at Mt. Meru peak in the middle of Indra's five paradise gardens. It is on account of these wish-granting trees that the asuras waged a perpetual war with the devas as the heavenly gods who exclusively benefited freely from the "divine flowers and fruits" from the Kalpavriksha, whereas the demigods lived comparatively in penury at the lower part of its "trunk and roots". The Parijata is often identified with its terrestrial  counterpart, the Indian coral tree (Eyrthrina indica), but is most often depicted like a magnolia or frangipani (Sanskrit: champaka) tree. It is described as having roots made of gold, a silver midriff, lapislazuli boughs, coral leaves, pearl flower, gemstone buds, and diamond fruit.[3] It is also said that Ashokasundari was created from a Kalpavriksha tree to provide relief to Parvati from her loneliness.[5]In Hindu mythology Shiva and Parvati after much painful discussions while parting with their daughter Aranyani gave her away to the divine Kalpavriksha for safe keeping when the demon Andhakasura waged war. Parvati requested Kalpavriksha to bring up her daughter with "safety, wisdom, health and happiness," and to make her Vana Devi, the protector of forests.[6]Kalpavrikshas are wish-granting trees which fulfill the desires of people in initial stages of worldly cycle as per Jain Cosmology. In initial times children are born in pairs (boy and girl) and don't do any karma.[7] There are 10 Kalpavrikshas which grant 10 distinct wishes such as an abode to reside, garments, utensils, nourishment including fruits and sweets, pleasant music, ornaments, fragrant flowers, shining lamps and a radiant light at night.[7]According to Jain cosmology, in the three Aras (unequal periods) of the descending arc (Avasarpini), Kalpavrikshas provided all that was needed, but towards the end of the third ara, the yield from them diminished. Eight types of these trees are described in some texts, each of which provided different objects. Thus from the "Madyanga tree" delicious and nutritious drinks could be obtained; from the "Bhojananga", delicious food; from "yotiranga", light more radiant than the sun and the moon; while from "Dopanga" came indoor light. Other trees provided homes, musical devices, table ware, fine garments, wreaths and scents.[4]The Tiloya Panatti give the following list: Pananga, Turiyanga, Bhusananga, Vatthanga, Bhoyanga, Alayanga, Diviyanga, Bhayananga, Malanga, Tejanga with excellent drinks, music, ornaments, garments, edibles and ready-made dishes, mansions to live in, lamps, utensils and garlands of flowers respectively while the last type, namely Tejanga, seems to be self-luminous, serving the purpose of heavenly luminaries.[8]In Buddhism a small wish granting tree is depicted decorating the upper part of the "long-life vase" held by "longevity deities" like Amitayus and Ushnishavijaya. The goddess Shramana devi holds jeweled branch of Kalpavriksha in her left hand.[3]Worship of the Nyagrodha tree as a form of non-human worship is depicted in a Buddhist sculpture at Besnagar.[9] This sculpture in Besnagar, also known as Vidisa (Bhilsa), is dated to third century BC and is exhibited in the Calcutta Museum.[10]In Myanmar, where Theravada Buddhism is practiced, the significance of the Kalpavriksha is in the form of an annual ritual known as Kathina (presenting a robe) in which the laity present gifts to the monks in the form of money trees.[11]In different states of India some trees are specifically referred to as the Kalpavriksha. These are stated below.The banyan tree (Ficus benghalensis), also called Nyagrodha tree, which grows throughout the country is referred to as Kalpavriksha or Kaplaptaru because of its ability to amply provide for human needs.[12][9]The coconut tree (Cocos nucifera) found in most regions of the country is called "Kalpavriksha", as every part of it is useful in one way or the other. The coconut water inside the nut is a delicious drink. In dried form it is called copra and is used to manufacture oil. The coconut husk, called coir, is used to make rope. Leaves are used to make huts, fans, mats. Palm sugar is made from budding flower. The dried midrib is used to make boats.[13]Ashwatha tree (sacred fig tree) is also known as Kalapvriksha where the deities and Brahma are stated to reside, and it is where sage Narada taught the rishis on the procedure for worshipping the tree and its usefulness.[14]Mahua tree (Madhuca longifolia) holds an important place in the day-to-day life of the tribal people. It is like the Kalpavriksha wish tree called madhu (Madhuca indica).[15]Shami tree (Prosopis cineraria), found in desert areas of the country, called in local dialect as khejari or jaant is called Kalpavriksha. In Rajasthan desert area its roots go deep to a depth of 17–25 metres (56–82 ft). This checks the erosion of the sandy soil of the desert. For this reason the tree stays green even drought conditions of weather. People of Rajasthan hence regard this tree as Kalpavriksha, because at the time of drought when no grass or fodder is found anywhere the animals are able to sustain by eating its green leaves.[16]Chyur tree in the high altitudes of the Himalayas growing at an altitude between 500 and 1000 m, known as the Indian butter tree (Diploknema butyracea), is called a Kalpavriskha, or tree of paradise by the people of the mountainous region as it yields honey, jaggery and ghee. It is in the shape of an umbrella.[17]In Joshimath in Uttarakhand a mulberry tree, which is said to be 2400 years old, is renowned and revered as the Kalpavriksha as it was the location where, in the 8th century, Adi Sankaracharya did "penance" under the tree as he considered it an incarnation of Lord Shiva.[18] It is also believed that sage Durvasa meditated under this tree,[5] in Urgam.[19] The mountain slopes of Kailasa are stated to have a profusion of Kalpavrikshas.[19]At Mangaliyawas near Ajmer, Rajasthan, there are two revered trees (Male and Female) which are more than 800 years old, known as Kalpavrikshas. They are worshipped on an Amavasya day in the Hindu month of Shraavana.[5]In Ranchi (Jharkhand, Ranchi, India) there are three Kalpavrikshas. They are at a locality called Hinoo. In Tamil Nadu's culture, tala (Borassus flabellifer) a variety of Palmyra palm (Borassus), also known as toddy, is referred to as Kalpataru as all its parts have a use. This tree is also native to Asia and South East Asia, has normally a life span of 100 years, grows up to 20 metres (66 ft) height; its leaves in the shape of a fan are rough texture. The leaves were used for writing in the ancient times.[20]In the Harivansh Puraan, the Parijata, baobab tree, is called a Kalpavriksha, or wish bearing tree, which apart from the village of Kintoor, near Barabanki, Uttar Pradesh, is only found in heaven. The tree has mythological link with prince Arjuna of the Pandava clan who is said to have brought it from heaven. His mother Kunti after whom the village Kintoor is named used to offer flowers from this tree to worship Lord Shiva. It is also said that Lord Krishna brought this tree from heaven to please his wife Satyabhama.[21]Kalpalatha is another wish fulfilling tree, a creeper, which was extolled during the later part of the Aryan period. It is said that a person standing below this tree would be blessed with beautiful ornaments, dresses and even unmarried girls.[22]In iconography, Kalpavriksha, the wish-fulfilling tree, is painted within a picture of a landscape, decorated with flowers, silks, and suspended with jewelry.[3] It is a pattern which has a prominent symbolic meaning.[1] Ornamental Kalpavriksha design was a feature that was adopted on the reverse of the coins and sculptures in the Gupta period.[23]Kalpavriksha is also dated to the Dharmachakra period of Buddhism. The paintings of this period depicting the tree with various branches and leaves have a female figure painted on its top part. The female figure is painted from mast upwards holding a bowl in her hand. Similar depiction of female figure with tree representing it as presiding deity was a notable feature during the Sunga period as seen in the image of "Salabhanvka" in the railing pillars.[24]In most paintings of Kalpavriksha Shiva and Parvati are a common feature. It forms a  canopy over Shiva. In one painting Paravati is paying obeisance to  Lord Shiva with her hands held up in adoration when she is blessed with a stream of water from the Kalpavriksha.[25]A Kalpavriksha is mentioned in the Sanskrit work Mānāsara as a royal insignia. In Hemādri's work Caturvargacīntama, the Kalpavriksha is said to be a tree of gold and gem stones.[26]In poetry Kalpavriksha is compared to Lakshmi as its sister emerging from the sea. It is born to the Naga King Kumuda, the fifth descendant of Takshaka, along with  his sister Kumudavati. It emerged from below the bed of the Sarayu River challenging Kusa considered an incarnation of Vishnu just in the disguise as a son.[27]Kalidasa, in his poetry Meghadūta epitomizing wish-fulfilling trees found in the capital of the Yaksha king extols the virtues of Kalpavriksha as "the dainties and fineries for the fair women of Alaka, coloured clothes for the body, intoxicating drinks for exciting glances of the eyes, and flowers for decorating the hair and ornaments of various designs".[28]
Penjing
Penjing, also known as penzai, is the ancient Chinese art of depicting artistically formed trees, other plants, and landscapes in miniature.Penjing generally fall into one of three categories:[1]Similar practices exist in other cultures, including the Japanese traditions of bonsai and saikei, as well as the miniature living landscapes of Vietnamese hòn non bộ. Generally speaking, tree penjing specimens differ from bonsai by allowing a wider range of tree shapes (more "wild-looking") and by planting them in bright-colored and creatively shaped pots. In contrast, bonsai are more simplified in shape (more "refined" in appearance) with larger-in-proportion trunks, and are planted in unobtrusive, low-sided containers with simple lines and muted colors.While saikei depicts living landscapes in containers, like water and land penjing, it does not use miniatures to decorate the living landscape. Hòn non bộ focuses on depicting landscapes of islands and mountains, usually in contact with water, and decorated with live trees and other plants. Like water and land penjing, hòn non bộ specimens can feature miniature figures, vehicles, and structures. Distinctions among these traditional forms have been blurred by some practitioners outside of Asia, as enthusiasts explore the potential of local plant and pot materials without strict adherence to traditional styling and display guidelines.Classical Chinese gardens often contain arrangements of miniature trees and rockeries known as penjing. These creations of carefully pruned trees and rocks are small-scale renditions of natural landscapes. They are often referred to as living sculptures or as three-dimensional poetry. Their artistic composition captures the spirit of nature and distinguishes them from ordinary potted plants.The container known as the pen originated in Neolithic China in the Yangshao culture as an earthenware shallow dish with a foot. It was later one of the vessels manufactured in bronze for use in court ceremonies and religious rituals during the Shang dynasty and Zhou dynasty.[2]When foreign trade introduced into China new herbal aromatics in the 2nd century BC, a unique incense burner was designed.[3]  The boshanlu stemmed cup was topped by a perforated lid in the shape of one of the sacred mountains/islands, such as Mount Penglai – focus of a strong contemporary belief – often with the images of mythical persons and beasts throughout the hillsides.  Smaller versions of the pen dish were sometimes used as bottom pieces either to catch hot embers or to be filled with water to represent the ocean out of which the sacred mountains/islands arose.  Originally made out of bronze, ceramic, or talc stone, some later versions were believed to be stones which occasionally were partly covered with moss and lichens to further heighten the miniature representation.[4] Since at least the 1st century AD, Daoist mysticism has included the recreating of magical sites in miniature to focus and increase the properties found in the full-size sites.   The  various schools of Buddhism introduced from India after the mid-2nd century included the meditative dhyana sect, whose translations of Sanskrit texts sometimes used Daoist terminology to convey non-physical concepts. Also, floral altar decorations were introduced and floral designs started to become a dominant force in Chinese art. Five centuries later the Chán school of Buddhism was established, in which renewed Indian dhyana Buddhist teachings were merged with native Chinese Daoism. Chán maintained its more active, vital spirit even as other Buddhist sects were becoming more rigidly formalized.[5]While there were legends dating from at least the 3rd and 4th centuries of Daoist persons said to have had the power to shrink whole landscapes down to small vessel size,[6] written descriptions of miniature landscapes are not known until Tang Dynasty times. As the information at that point shows a somewhat developed craft, (then called "punsai")[7] the making of dwarfed tree landscapes had to have been taking place for a while, either in China or possibly based on a form brought in from outside.[4]  The earliest-known graphic dates from 706 and is found in a wall mural on a corridor leading to the tomb of Prince Zhang Huai at the Qianling Mausoleum site.[8][9] Excavated in 1972, the frescoes show two maid servants carrying penjing with miniature rockeries and fruit trees.[9]The first highly prized trees are believed to have been collected in the wild and were full of twists, knots, and deformities. These were seen as sacred, of no practical profane value for timber or other ordinary purpose. These naturally dwarfed plants were held to be endowed with special concentrated energies due to age and origin away from human influence. The viewpoint of Chán Buddhism would continue to impact the creation of miniature landscapes. Smaller and younger plants which could be collected closer to civilization but still bore a resemblance to the rugged old treasures from the mountains would also have been chosen. Horticultural techniques to increase the appearance of age by emphasizing trunk, root, and branch size, texture, and shapes would eventually be employed with these specimens.[10]From Tang times onward, various poets and essayists praised dwarf potted landscapes.  A decorative tree guild from around 1276 is known to have supplied dwarf specimens for use in Suzhou restaurants in the province of Jiangsu.[11]Although imperial embassy personnel and Buddhist students from Japan had returned from the mainland with miniature landscape souvenirs since the 6th century, the oldest known depiction of a dwarfed tray landscape in Japan dates from 1309. The fifth of the twenty-scroll Kasuga-gongen-genki masterpiece depicts the household of a wealthy Japanese individual who has an outdoors slatted-workbench holding a shallow wooden tray and ceramic dish of Chinese origin with dwarf trees, grasses, and stones.[12]  By this time Chán Buddhism had been developed in Japan as Zen.  Its influence of "beauty in severe austerity" led native Japanese dwarf potted landscapes to be distilled into single, ideal trees being representatives of the universe.  What is termed bonsai derives from this.Since at least the 16th century, shops at the "Garden of Dragon Flowers" (Longhua) to the southwest of Shanghai, were engaged in cultivating miniature trees in containers. (These would continue to the present day.)  Meanwhile, Suzhou was still considered at century's end to be the source of the finest exponents of the art of penjing.[13][14]The earliest-known English observation of penjing in China/Macau dates from 1637.[15]    During the end of the 18th century, Yangzhou in central Jiangsu province boasted landscape penjing that contained water and soil.[16]In 1806, a very old dwarf tree from Canton (now Guangzhou) was gifted to Sir Joseph Banks and eventually presented to Queen Charlotte for Her Majesty's inspection.[17]  This tree and most others seen by Westerners in southeast China probably originated at the celebrated Fa Ti gardens near Canton.[18]By the first half of the 19th century, according to various Western accounts, air layering was the primary propagation method for penjing, which were then generally between one and two feet in height after two to twenty years of work. Elms were the main specimens used, along with pines, junipers, cypresses, and bamboos; plums were the favored fruit trees, along with peaches and oranges. The branches could be bent and shaped using various forms of bamboo scaffolding, twisted lead strips, and iron or brass wire to hold them in place; they could be also be cut, burnt, or grafted. The bark was sometimes lacerated at places or smeared with sugary substance to induce termites ("white ants") to roughen it or even to eat the similarly sweetened heartwood. Rocks with moss or lichens were also a frequent feature of these compositions.[19]The earliest known photograph from China which included penjing was made c.1868 by John Thomson.[20] He was particularly delighted by the collection in the garden of the Hoi Tong Monastery on Henan Island near Guangzhou.[21] A collection of dwarf trees and plants from China was also exhibited that year in Brooklyn, New York.[22]  In America, laws such as the Chinese Exclusion Act led to Japanese bonsai becoming more familiar to Americans. This led to the prevalence of knowledge of the Japanese forms of dwarf potted trees for the next several decades and prior to Chinese forms.[23]Near the end of the 19th century, the Lingnan or Cantonese school of "Clip and Grow" styling was developed at a monastery in southeast China. Fast-growing tropical trees and shrubs could be more easily and quickly shaped using these techniques.[24]Established in 1954, the Longhua nursery in Shanghai included the teaching of classical theory and all aspects of the practice of penjing, a process which could take student-gardeners ten years.[25]As late as the early 1960s, it is reported that some 60 characteristic regional forms of penjing could be distinguished by the expert eye.[26]  A few of these forms dated back to at least the 16th century.[27]During the upheaval of the Great Proletarian Cultural Revolution (May 1966-April 1969), one relatively small effect was that many collections of penjing in Mainland China, especially around Beijing, were damaged or neglected because they were seen as a bourgeois pastime. After their trees were gone, some Chinese penjing masters, men in their sixties and seventies, were forced to do something considered socially redemptive—many were sent to fields to plant rice. However, in other areas of China, especially in eastern and southern China, penjing were collected for safe keeping.[28][29][30]Wu Yee-sun (1905–2005), third generation penjing master and grandson of a Lingnan school founder, held the first exhibition of artistic pot plants jointly with Mr. Liu Fei Yat in Hong Kong in 1968. This was a display of traditional aristocratic penjing which had survived the 1949 Chinese Communist Revolution by leaving/being protected from Mainland China.  The two editions of Wu's Chinese/English book, Man Lung Garden Artistic Pot Plants, helped develop interest in this older form of what the West only knew as the later-refined Japanese art of bonsai.[30]The Yuk Sui Yuen Penzai Exhibition was held in Canton in 1978. This was the first public show in ten years with approximately 250 penjing from private collections displayed in a public park. Antique pots were also shown.[31] The Shanghai Botanical Garden opened that year and permanently displays 3,000 penjing.[32]  The First National Penjing Show was held the following year in Beijing with over 1,100 exhibits from 13 provinces, towns, and autonomies.[33]One division of the Hangzhou Flower Nursery by 1981 specialized in penjing, including over fifteen hundred once abandoned older specimens being maintained and in the initial stages of being retrained. The art of penjing would again become vastly popular in China, in part due to stability returning to most people's lives and the significantly improved economic conditions; growth would be most pronounced particularly in coastal provinces of Jiangsu, Zhejiang, Fujian, Guangdong as well as Shanghai. There would be increasing numbers of good public and private collections, the latter with anywhere from several hundred to several thousand pieces.By the end of 1981, the China Flower and Penjing Association was formed, and seven years later the China Penjing Artists Association was likewise established.[32]The Hong Kong Baptist University opened the Man Lung Garden in 2000 to promote the Chinese heritage of penjing.   Temporarily located on the University's Shaw Campus, in February 2005 a permanent site was set up at the Kam Shing Road Entrance of its Ho Sin Hang Campus.[34]Using artificially dwarfed trees and shrubs, these arrangements are created in special trays or pots which are placed on ornately carved wooden stands. Often, rocks, miniature ceramic structures (like buildings and bridges), and figurines are added to give the proper scales as part of the natural scenery. These miniatures add to the symbolism of a penjing specimen, by providing a social or historical context in which to interpret the overall penjing design.[29]These miniature landscapes include trees which are frequently over a hundred years old. Like the plants in the Chinese garden, they have been carefully selected and tended so that they develop into twisted and gnarled shapes reminiscent of their full-size counterparts in the wild. Like Chinese gardens, these miniature landscapes are designed to convey landscapes experienced from various viewpoints - a close-up view, a medium-range view or a panorama.As an art form, penjing is an extension of the garden, since it enables an artist to recreate parts of the natural landscape in miniature. Penjing is often used indoors as part of a garden's overall design, since it reiterates the landscape features found outside. Penjing pots grace pavilions, private studies or living rooms, and public buildings. They are either free-standing elements within the gardens or are placed on furniture such as a table or bookshelf. Sometimes a lattice display stand is built which adds particular prominence to the penjing specimen and exemplifies the interplay between architecture and nature.Penjing seeks to capture the essence and spirit of nature through contrasts. Philosophically, it is influenced by the principles of Taoism, specifically the concept of Yin and Yang: the idea of the universe as governed by two primal forces, opposing but complementary. Some of the contrasting concepts used in penjing include portrayal of "dominance and subordination, emptiness (void) and substance, denseness and sparseness, highness and lowness, largeness and smallness, life and death, dynamics and statics, roughness and meticulousness, firmness and gentleness, lightness and darkness, straightness and curviness, verticality and horizontality[,] and lightness and heaviness."[1]Design inspiration is not limited to observation or representation of nature, but is also influenced by Chinese poetry, calligraphy, and other visual arts. Common penjing designs include evocation of dragons and the strokes of well-omened characters. At its highest level, the artistic value of penjing is on par with that of poetry, calligraphy, brush painting and garden art.[35]Styles of the traditional Penjing in China are mainly classified by the most representative (dominant) plants used, and named after the regions of their origin.  Since different plants require different techniques to handle, different styles thus formed.  There are more than a dozen styles of traditional Penjing:The maintenance and care of penjing trees are similar to that of the bonsai.
Floral diagram
Floral diagram is a graphic representation of flower structure. It shows the number of floral organs, their arrangement and fusion. Different parts of the flower are represented by their respective symbols. Floral diagrams are useful for flower identification or can help in understanding angiosperm evolution. They were introduced in the late 19th century and are generally attributed to A. W. Eichler.[1]In the 19th century, two contrasting methods of describing the flower were introduced: the textual floral formulae and pictorial floral diagrams.[2] Floral diagrams are credited to A. W. Eichler, his extensive work Blüthendiagramme[3][4] (1875, 1878) remains a valuable source of information on floral morphology. Eichler inspired later generation of scientists, including John Henry Schaffner.[5] Diagrams were included e.g. in Types of Floral Mechanism[6] by Church (1908). They were used in different textbooks, e.g. Organogenesis of Flowers[7] by Sattler (1973), Botanische Bestimmungsübungen[8] by Stützel (2006) or Plant Systematics[9] by Simpson (2010). Floral Diagrams[1] (2010) by Ronse De Craene followed Eichler’s approach using the contemporary[Note 1] APG II system.A floral diagram is a schematic cross-section through a young flower.[1] It may be also defined as “projection of the flower perpendicular to its axis”.[3] It usually shows the number of floral parts,[Note 2] their sizes, relative positions and fusion. Different organs are represented by distinguishable symbols, which may be uniform for one organ type, or may reflect concrete morphology. The diagram may also include symbols that don’t represent physical structures, but carry additional information (e.g. symmetry plane orientation).There is no agreement on how floral diagrams should be drawn, it depends on the author whether it is just a rough representation, or whether structural details of the flower are included.Diagrams can describe the ontogeny of flowers, or can show evolutionary relationships. They can be generalized to show the typical floral structure of a taxon.[1]:37 It is also possible to represent (partial) inflorescences by diagrams.Substantial amount of information may be included in a good diagram. It can be useful for flower identification or comparison between angiosperm taxa. Paleontologists can take advantage of diagrams for reconstruction of fossil flowers. Floral diagrams are also of didactic value.[1]:xiiiDiagrams are usually depicted with the subtending bract below and the axis above the flower itself, both in the median line. The axis corresponds to the position of the main stem relative to a lateral flower.[10]:12 When a terminal flower is depicted, the axis is not present and therefore cannot be shown. Bracteoles, if they are present, are usually drawn on the sides of the diagram.Not only the information contained within diagrams, but also their appearance commonly varies between authors. Just some publications incorporate an overview of used symbols.Bracts and bracteoles are commonly shown as arcs. In Floral Diagrams by Ronse De Craene they consistently have a black fill and a little triangle on the outer side to distinguish them from the perianth. In Eichler’s Blüthendiagramme their representation alters between diagrams.The axis relative to the flower is shown as black circle in Floral Diagrams. When inflorescence is depicted, the position of its main stem is illustrated by a crossed circle. Eichler’s depiction of axes alternates between diagrams.Perianth parts are also shown as arcs. They may be colored according to their type. In Blüthendiagramme the tepals are usually white with black stroke, sepals are hatched and petals are black. Ronse De Craene implies that it may be sometimes impossible to classify the organs, he shows green perianth parts as black and pigmented as white. Estivation can be accurately shown in the diagram.Stamens are represented by a cross-section through anthers. In case there are many stamens in the flower, they can be simplified and drawn as circles. Staminodes have a small black circle inside or are painted black in Floral Diagrams, Eichler also fills them black.The pistil is shown as a sectional view of the ovary. Ovary position is highlighted by small triangles in Floral Diagrams. Ronse De Craene also incorporates ovule morphology or shows the position of stigmatic lobes by white shapes.In Floral Diagrams, nectaries are filled by grey color, Eichler fills them by hatching.Fusion can be shown in diagrams by full connecting lines between organs. Lost organs can be represented by a star (✶), lost perianth parts or bracts/bracteoles can be shown with dashed stroke. It is possible to show the direction of monosymmetry by a large arrow. Resupination may be illustrated by a curved arrow. Floral parts can be accompanied by numbers to show their sequence of inicialization.Each of these two concepts is better in expressing some information. Floral diagrams can show the size and relative position of the organs. On the other hand, floral formulae are capable of broader generalization. Prenner et al. view them as complementary methods and state they make an “identikit” flower when utilized together.[2]:248 Ronse De Craene also approves of their combined use.[1]:xiii
Fern
A fern is a member of a group of vascular plants (plants with xylem and phloem) that reproduce via spores and have neither seeds nor flowers. They differ from mosses by being vascular, i.e., having specialized tissues that conduct water and nutrients, in having branched stems and in having life cycles in which the sporophyte is the dominant phase. Like other vascular plants, ferns have complex leaves called megaphylls, that are more complex than the microphylls of clubmosses. Most ferns are leptosporangiate ferns, sometimes referred to as true ferns. They produce coiled fiddleheads that uncoil and expand into fronds.[3] The group includes about 10,560 known extant species.[4]Ferns are defined here in the broad sense, being all of the Polypodiopsida, comprising both the leptosporangiate (Polypodiidae) and eusporangiate ferns, the latter itself comprising ferns other than those denominated true ferns, including horsetails or scouring rushes, whisk ferns, marattioid ferns, and ophioglossoid ferns.Ferns first appear in the fossil record about 360 million years ago in the late Devonian period,[5] but many of the current families and species did not appear until roughly 145 million years ago in the early Cretaceous, after flowering plants came to dominate many environments. The fern Osmunda claytoniana is a paramount example of evolutionary stasis; paleontological evidence indicates it has remained unchanged, even at the level of fossilized nuclei and chromosomes, for at least 180 million years.[6]Ferns are not of major economic importance, but some are used for food, medicine, as biofertilizer, as ornamental plants and for remediating contaminated soil. They have been the subject of research for their ability to remove some chemical pollutants from the atmosphere. Some fern species are significant weeds.[citation needed] They also play certain roles in mythology and art.Like the sporophytes of seed plants, those of ferns consist of stems, leaves and roots.Stems: Fern stems are often referred to as rhizomes, even though they grow underground only in some of the species. Epiphytic species and many of the terrestrial ones have above-ground creeping stolons (e.g., Polypodiaceae), and many groups have above-ground erect semi-woody trunks (e.g., Cyatheaceae). These can reach up to 20 meters (66 ft) tall in a few species (e.g., Cyathea brownii on Norfolk Island and Cyathea medullaris in New Zealand).[7]Leaf: The green, photosynthetic part of the plant is technically a megaphyll and in ferns, it is often referred to as a frond. New leaves typically expand by the unrolling of a tight spiral called a crozier or fiddlehead fern. This uncurling of the leaf is termed circinate vernation. Leaves are divided into two types a trophophyll and a sporophyll. A trophophyll frond is a vegetative leaf analogous to the typical green leaves of seed plants that does not produce spores, instead only producing sugars by photosynthesis. A sporophyll frond is a fertile leaf that produces spores borne in sporangia that are usually clustered to form sori. In most ferns, fertile leaves are morphologically very similar to the sterile ones, and they photosynthesize in the same way. In some groups, the fertile leaves are much narrower than the sterile leaves, and may even have no green tissue at all (e.g., Blechnaceae, Lomariopsidaceae). The anatomy of fern leaves can either be simple or highly divided. In tree ferns, the main stalk that connects the leaf to the stem (known as the stipe), often has multiple leaflets. The leafy structures that grow from the stipe are known as pinnae and are often again divided into smaller pinnules.[8]Roots: The underground non-photosynthetic structures that take up water and nutrients from soil. They are always fibrous and structurally are very similar to the roots of seed plants.Like all other vascular plants, the diploid sporophyte is the dominant phase or generation in the life cycle. The gametophytes of ferns, however, are very different from those of seed plants. They are free-living and resemble liverworts, whereas those of seed plants develop within the spore wall and are dependent on the parent sporophyte for their nutrition. A fern gametophyte typically consists of:Ferns first appear in the fossil record in the early Carboniferous period. By the Triassic, the first evidence of ferns related to several modern families appeared. The great fern radiation occurred in the late Cretaceous, when many modern families of ferns first appeared.Ferns were traditionally classified in the class Filices, and later in a Division of the Plant Kingdom named Pteridophyta or Filicophyta. Pteridophyta is no longer recognised as a valid taxon because it is paraphyletic. The ferns are also referred to as Polypodiophyta or, when treated as a subdivision of Tracheophyta (vascular plants), Polypodiopsida, although this name sometimes only refers to leptosporangiate ferns. Traditionally, all of the spore producing vascular plants were informally denominated the pteridophytes, rendering the term synonymous with ferns and fern allies. This can be confusing because members of the division Pteridophyta were also denominated pteridophytes (sensu stricto).Traditionally, three discrete groups have been denominated ferns: two groups of eusporangiate ferns, the families Ophioglossaceae (adder's tongues, moonworts, and grape ferns) and Marattiaceae; and the leptosporangiate ferns. The Marattiaceae are a primitive group of tropical ferns with large, fleshy rhizomes and are now thought to be a sibling taxon to the leptosporangiate ferns. Several other groups of species were considered fern allies: the clubmosses, spikemosses, and quillworts in Lycopodiophyta; the whisk ferns of Psilotaceae; and the horsetails of Equisetaceae. Since this grouping is polyphyletic, the term fern allies should be abandoned, except in a historical context.[9] More recent genetic studies demonstrated that the Lycopodiophyta are more distantly related to other vascular plants, having radiated evolutionarily at the base of the vascular plant clade, while both the whisk ferns and horsetails are as much true ferns as the ophioglossoid ferns and Marattiaceae. In fact, the whisk ferns and ophioglossoid ferns are demonstrably a clade, and the horsetails and Marattiaceae are arguably another clade.Smith et al. (2006) carried out the first higher-level pteridophyte classification published in the molecular phylogenetic era, and considered the ferns as monilophytes, as follows:[10]Molecular data, which remain poorly constrained for many parts of the plants' phylogeny, have been supplemented by morphological observations supporting the inclusion of Equisetaceae in the ferns, notably relating to the construction of their sperm and peculiarities of their roots.[10] However, there remained differences of opinion about the placement of the genus Equisetum (see Equisetopsida for further discussion). One possible solution was to denominate only the leptosporangiate ferns as true ferns while denominating the other three groups as fern allies. In practice, numerous classification schemes have been proposed for ferns and fern allies, and there has been little consensus among them.The leptosporangiate ferns are sometimes called true ferns.[11] This group includes most plants familiarly known as ferns. Modern research supports older ideas based on morphology that the Osmundaceae diverged early in the evolutionary history of the leptosporangiate ferns; in certain ways this family is intermediate between the eusporangiate ferns and the leptosporangiate ferns. Rai and Graham (2010) broadly supported the primary groups, but queried their relationships, concluding that "at present perhaps the best that can be said about all relationships among the major lineages of monilophytes in current studies is that we do not understand them very well".[12] Grewe et al. (2013) confirmed the inclusion of horsetails within ferns sensu lato, but also suggested that uncertainties remained in their precise placement.[13] Other classifications have raised Ophioglossales to the rank of a fifth class, separating the whisk ferns and ophioglossoid ferns.[13]One problem with the classification of ferns is that of cryptic species. A cryptic species is a species that is morphologically similar to another species, but differs genetically in ways that prevent fertile interbreeding. A good example of this is the currently designated species Asplenium trichomanes (maidenhair spleenwort). This is actually a species complex that includes distinct diploid and tetraploid races. There are minor but unclear morphological differences between the two groups, which prefer distinctly differing habitats. In many cases such as this, the species complexes have been separated into separate species, thus raising the total number of species of ferns. Possibly many more cryptic species are yet to be discovered and designated.The ferns are related to other higher order taxa, as shown in the following cladogram:[9][14][15][2].mw-parser-output table.clade{border-spacing:0;margin:0;font-size:100%;line-height:100%;border-collapse:separate;width:auto}.mw-parser-output table.clade table.clade{width:100%}.mw-parser-output table.clade td{border:0;padding:0;vertical-align:middle;text-align:center}.mw-parser-output table.clade td.clade-label{width:0.8em;border:0;padding:0 0.2em;vertical-align:bottom;text-align:center}.mw-parser-output table.clade td.clade-slabel{border:0;padding:0 0.2em;vertical-align:top;text-align:center}.mw-parser-output table.clade td.clade-bar{vertical-align:middle;text-align:left;padding:0 0.5em}.mw-parser-output table.clade td.clade-leaf{border:0;padding:0;text-align:left;vertical-align:middle}.mw-parser-output table.clade td.clade-leafR{border:0;padding:0;text-align:right}Lycopodiophyta (Lycopodiopsida) - lycophytesPolypodiophyta (Polypodiopsida) - fernsGymnospermaeAngiospermae - flowering plantsSmith's 2006 classification treated the ferns as four classes:[16][17]In addition they defined 11 orders and 37 families.[16] That system was a consensus of a number of studies, and was further refined.[13][18] The phylogenetic relationships are shown in the following cladogram (to the level of orders).[19] This division into four major clades was then confirmed using morphology alone.[20]Lycopodiophytes (club mosses, spike mosses, quillworts)Spermatophytes (seed plants)Psilotales (whisk ferns)  Ophioglossales (grapeferns etc.)  Equisetales (horsetails)  Marattiales  Osmundales  Hymenophyllales (filmy ferns)  Gleicheniales  Schizaeales  Salviniales (heterosporous)  Cyatheales (tree ferns)  Polypodiales  Subsequently, Chase and Reveal considered both lycopods and ferns as subclasses of a class Equisetopsida (Embryophyta) encompassing all land plants. This is referred to as  Equisetopsida sensu lato to distinguish it from the narrower use to refer to horsetails alone, Equisetopsida sensu stricto. They placed the lycopods into subclass Lycopodiidae and the ferns, keeping the term monilophytes, into five subclasses, Equisetidae, Ophioglossidae, Psilotidae, Marattiidae and Polypodiidae, by dividing Smith's Psilotopsida into its two orders and elevating them to subclass (Ophioglossidae and Psilotidae).[15] Christenhusz et al.[a] (2011) followed this use of subclasses but recombined Smith's Psilotopsida as Ophioglossidae, giving four subclasses of ferns again.[21]Christenhusz and Chase (2014) developed a new classification of ferns and lycopods. They used the term Polypodiophyta for the ferns, subdivided like Smith et al. into four groups (shown with equivalents in the Smith system), with 21 families, approximately 212 genera and 10,535 species;[9]This was a considerable reduction in the number of families from the 37 in the system of Smith et al., since the approach was more that of lumping rather than splitting. For instance a number of families were reduced to subfamilies. Subsequently, a consensus group was formed, the Pteridophyte Phylogeny Group (PPG), analogous to the Angiosperm Phylogeny Group, publishing their first complete classification in November 2016. They recognise ferns as a class, the Polypodiopsida, with four subclasses as described by Christenhusz and Chase, and which are phylogenetically related as in this cladogram:[2]EquisetidaeOphioglossidaeMarattiidaePolypodiidaeIn the Pteridophyte Phylogeny Group classification the Polypodiopsida consist of four subclasses, 11 orders, 48 families, 319 genera, and an estimated 10,578 species. Thus Polypodiopsida in the broad sense (sensu lato) as used by the PPG (Polypodiopsida sensu PPG) needs to be distinguished from the narrower usage (sensu stricto) of Smith et al. (Polypodiopsida sensu Smith et al.)[2]The stereotypical image of ferns growing in moist shady woodland nooks is far from a complete picture of the habitats where ferns can be found growing. Fern species live in a wide variety of habitats, from remote mountain elevations, to dry desert rock faces, to bodies of water or in open fields. Ferns in general may be thought of as largely being specialists in marginal habitats, often succeeding in places where various environmental factors limit the success of flowering plants. Some ferns are among the world's most serious weed species, including the bracken fern growing in the Scottish highlands, or the mosquito fern (Azolla) growing in tropical lakes, both species forming large aggressively spreading colonies. There are four particular types of habitats that ferns are found in: moist, shady forests; crevices in rock faces, especially when sheltered from the full sun; acid wetlands including bogs and swamps; and tropical trees, where many species are epiphytes (something like a quarter to a third of all fern species[22]).Especially the epiphytic ferns have turned out to be hosts of a huge diversity of invertebrates. It is assumed that bird's-nest ferns alone contain up to half the invertebrate biomass within a hectare of rainforest canopy.[23]Many ferns depend on associations with mycorrhizal fungi. Many ferns grow only within specific pH ranges; for instance, the climbing fern (Lygodium palmatum) of eastern North America will grow only in moist, intensely acid soils, while the bulblet bladder fern (Cystopteris bulbifera), with an overlapping range, is found only on limestone.The spores are rich in lipids, protein and calories, so some vertebrates eat these. The European woodmouse (Apodemus sylvaticus) has been found to eat the spores of Culcita macrocarpa and the bullfinch (Pyrrhula murina) and the New Zealand lesser short-tailed bat (Mystacina tuberculata) also eat fern spores.[24]Ferns are vascular plants differing from lycophytes by having true leaves (megaphylls), which are often pinnate. They differ from seed plants (gymnosperms and angiosperms) in reproducing by means of spores and they lack flowers and seeds. Like all land plants, they have a life cycle referred to as alternation of generations, characterized by alternating diploid sporophytic and haploid gametophytic phases. The diploid sporophyte has 2n paired chromosomes, where n varies from species to species. The haploid gametophyte has n unpaired chromosomes, i.e. half the number of the sporophyte. The gametophyte of ferns is a free-living organism, whereas the gametophyte of the gymnosperms and angiosperms is dependent on the sporophyte.The life cycle of a typical fern proceeds as follows:Ferns are not as important economically as seed plants but have considerable importance in some societies. Some ferns are used for food, including the fiddleheads of Pteridium aquilinum (bracken), Matteuccia struthiopteris (ostrich fern), and Osmundastrum cinnamomeum (cinnamon fern). Diplazium esculentum is also used by some tropical persons (for example in budu pakis, a traditional dish of Brunei[25]) as food. Tubers from the "para", Ptisana salicina (king fern) are a traditional food in New Zealand and the South Pacific. Fern tubers were used for food 30,000 years ago in Europe.[26][27] Fern tubers were used by the Guanches to make gofio in the Canary Islands. Ferns are generally not known to be poisonous to humans.[28] Licorice fern rhizomes were chewed by the natives of the Pacific Northwest for their flavor.[citation needed]Ferns of the genus Azolla are very small, floating plants that do not resemble ferns. Called the mosquito fern, they are used as a biological fertilizer in the rice paddies of southeast Asia, taking advantage of their ability to fix nitrogen from the air into compounds that can then be used by other plants.[1]Many ferns are grown in horticulture as landscape plants, for cut foliage and as houseplants, especially the Boston fern (Nephrolepis exaltata) and other members of the genus Nephrolepis. The bird's nest fern (Asplenium nidus) is also popular, as are the staghorn ferns (genus Platycerium). Perennial (also known as hardy) ferns planted in gardens in the northern hemisphere also have a considerable following.[citation needed]Several ferns are noxious weeds or invasive species, including Japanese climbing fern (Lygodium japonicum), mosquito fern and sensitive fern (Onoclea sensibilis). Giant water fern (Salvinia molesta) is one of the world's worst aquatic weeds. The important fossil fuel coal consists of the remains of primitive plants, including ferns.[citation needed]Ferns have been studied and found to be useful in the removal of heavy metals, especially arsenic, from the soil. Other ferns with some economic significance include:[citation needed]The study of ferns and other pteridophytes is called pteridology. A pteridologist is a specialist in the study of pteridophytes in a broader sense that includes the more distantly related lycophytes.Pteridomania is a term for the Victorian era craze of fern collecting and fern motifs in decorative art including pottery, glass, metals, textiles, wood, printed paper, and sculpture "appearing on everything from christening presents to gravestones and memorials." The fashion for growing ferns indoors led to the development of the Wardian case, a glazed cabinet that would exclude air pollutants and maintain the necessary humidity.[29]The dried form of ferns was also used in other arts, being used as a stencil or directly inked for use in a design. The botanical work, The Ferns of Great Britain and Ireland, is a notable example of this type of nature printing. The process, patented by the artist and publisher Henry Bradbury, impressed a specimen on to a soft lead plate. The first publication to demonstrate this was Alois Auer's The Discovery of the Nature Printing-Process.Fern bars were popular in America in the 1970s and 80s.Ferns figure in folklore, for example in legends about mythical flowers or seeds.[30] In Slavic folklore, ferns are believed to bloom once a year, during the Ivan Kupala night. Although alleged to be exceedingly difficult to find, anyone who sees a fern flower is thought to be guaranteed to be happy and rich for the rest of their life. Similarly, Finnish tradition holds that one who finds the seed of a fern in bloom on Midsummer night will, by possession of it, be guided and be able to travel invisibly to the locations where eternally blazing Will o' the wisps called aarnivalkea mark the spot of hidden treasure. These spots are protected by a spell that prevents anyone but the fern-seed holder from ever knowing their locations.[31]Several non-fern plants (and even animals) are called ferns and are sometimes confused with true ferns. These include:Some flowering plants such as palms and members of the carrot family have pinnate leaves that somewhat resemble fern fronds. However, these plants have fully developed seeds contained in fruits, rather than the microscopic spores of ferns.Adiantum lunulatumFern leaf, probably Blechnum nudumA tree fern unrolling a new frondTree fern, probably Dicksonia antarcticaTree ferns, probably Dicksonia antarctica"Filicinae" from Ernst Haeckel's Kunstformen der Natur, 1904Unidentified tree fern in OaxacaTree Fern Spores San Diego, CALeaf of fernUnidentified fern with spores showing in Rotorua, NZ.Ferns in one of many natural Coast Redwood undergrowth settings Santa Cruz, CA.Nature prints in The Ferns of Great Britain and Ireland used fronds to produce the platesA young, newly formed fern frondFern bed under a forest canopy in woods near Franklin, VirginiaPyrrosia piloselloides, Dragon's Scale, in MalaysiaFern growing on a wallSpores of Dryopteris filix-mas
Hardboard
Hardboard, also called high-density fiberboard (HDF),[1] is a type of fiberboard, which is an engineered wood product.[2]It is similar to particle board and medium-density fiberboard, but is denser and much stronger and harder because it is made out of exploded wood fibers that have been highly compressed.[3] Consequently, the density of hardboard is 31 lbs or more per cubic foot (500 kg/m³)[4] and is usually about 50-65 lbs per cubic foot (800–1040 kg/m³).[citation needed] It differs from particle board in that the bonding of the wood fibers requires no additional materials,[5] although resin is often added. Unlike particle board, it will not split or crack.[citation needed]Hardboard has long been used in furniture, but it is also popular for use in the construction industry and with trades as a temporary floor protector. Hardboard has become less popular over recent years due to new environmental targets in the construction industry[6] to procure more sustainable temporary protection materials.Hardboard is produced in either a wet or dry process. The wet process, known as the Mason Method,[7] leaves only one smooth side while the dry processed hardboard is smooth on both sides. Masonite is produced using the wet process only.Perforated hardboard, also called pegboard, is tempered hardboard that has a uniform array of 1/8″ or 1/4″ holes in it, into which tool-hanging hooks or store fixtures can be placed.A product resembling hardboard was first made in England in 1898 by hot pressing waste paper.[8] In the 1900s, fiber building board of relatively low density was manufactured in Canada. In the early 1920s, improved methods of compressing wet wood pulp at high temperatures resulted in a higher density product.[8]Unlike solid wood, hardboard is very homogeneous with no grain. A wood veneer can be glued onto it to give the appearance of solid wood. Other overlays include Formica, laminated papers, ceramics,[citation needed] and vinyl. It has many uses, such as a substrate. It is used in construction, flooring, furniture, home appliances, automobiles and cabinetry, and is popular among acrylic and oil painters as a painting surface due to its economical price (though it must be coated with gesso or canvas before use).[9] Hardboard has often been used as the surface material in clipboards, especially older models. It is also used as the final layer in many skateboard ramps and the half-pipe.Tempered hardboard is hardboard that has been coated with a thin film of linseed oil and then baked; this gives it more water resistance, impact resistance, hardness, rigidity and tensile strength. An earlier tempering process involved immersing the board in linseed oil or tung oil until it was 5 to 6 percent saturated, and heating to 170 °C (340 °F).[10] Tempered hardboard is used in construction siding.
Convergent evolution
Convergent evolution is the independent evolution of similar features in species of different lineages. Convergent evolution creates analogous structures that have similar form or function but were not present in the last common ancestor of those groups. The cladistic term for the same phenomenon is homoplasy. The recurrent evolution of flight is a classic example, as flying insects, birds, pterosaurs, and bats have independently evolved the useful capacity of flight. Functionally similar features that have arisen through convergent evolution are analogous, whereas homologous structures or traits have a common origin but can have dissimilar functions.  Bird, bat, and pterosaur wings are analogous structures, but their forelimbs are homologous, sharing an ancestral state despite serving different functions.The opposite of convergence is divergent evolution, where related species evolve different traits. Convergent evolution is similar to parallel evolution, which occurs when two independent species evolve in the same direction and thus independently acquire similar characteristics; for instance, gliding frogs have evolved in parallel from multiple types of tree frog.Many instances of convergent evolution are known in plants, including the repeated development of C4 photosynthesis, seed dispersal by fleshy fruits adapted to be eaten by animals, and carnivory.In morphology, analogous traits arise when different species live in similar ways and/or a similar environment, and so face the same environmental factors. When occupying similar ecological niches (that is, a distinctive way of life) similar problems can lead to similar solutions.[1][2][3] The British anatomist Richard Owen was the first to identify the fundamental difference between analogies and homologies.[4]In biochemistry, physical and chemical constraints on mechanisms have caused some active site arrangements such as the catalytic triad to evolve independently in separate enzyme superfamilies.[5]In his 1989 book Wonderful Life, Stephen Jay Gould argued that if one could "rewind the tape of life [and] the same conditions were encountered again, evolution could take a very different course".[6] Simon Conway Morris disputes this conclusion, arguing that convergence is a dominant force in evolution, and given that the same environmental and physical constraints are at work, life will inevitably evolve toward an "optimum" body plan, and at some point, evolution is bound to stumble upon intelligence, a trait presently identified with at least primates, corvids, and cetaceans.[7]In cladistics, a homoplasy is a trait shared by two or more taxa for any reason other than that they share a common ancestry. Taxa which do share ancestry are part of the same clade; cladistics seeks to arrange them according to their degree of relatedness to describe their phylogeny. Homoplastic traits caused by convergence are therefore, from the point of view of cladistics, confounding factors which could lead to an incorrect analysis.[8][9][10][11]In some cases, it is difficult to tell whether a trait has been lost and then re-evolved convergently, or whether a gene has simply been switched off and then re-enabled later. Such a re-emerged trait is called an atavism. From a mathematical standpoint, an unused gene (selectively neutral) has a steadily decreasing probability of retaining potential functionality over time. The time scale of this process varies greatly in different phylogenies; in mammals and birds, there is a reasonable probability of remaining in the genome in a potentially functional state for around 6 million years.[12]When two species are similar in a particular character, evolution is defined as parallel if the ancestors were also similar, and convergent if they were not.[b] Some scientists have argued that there is a continuum between parallel and convergent evolution, while others maintain that despite some overlap, there are still important distinctions between the two.[13][14][15]When the ancestral forms are unspecified or unknown, or the range of traits considered is not clearly specified, the distinction between parallel and convergent evolution becomes more subjective.  For instance, the striking example of similar placental and marsupial forms is described by Richard Dawkins in The Blind Watchmaker as a case of convergent evolution, because mammals on each continent had a long evolutionary history prior to the extinction of the dinosaurs under which to accumulate relevant differences.[16]The enzymology of proteases provides some of the clearest examples of convergent evolution. These examples reflect the intrinsic chemical constraints on enzymes, leading evolution to converge on equivalent solutions independently and repeatedly.[5][17]Serine and cysteine proteases use different amino acid functional groups (alcohol or thiol) as a nucleophile. In order to activate that nucleophile, they orient an acidic and a basic residue in a catalytic triad. The chemical and physical constraints on enzyme catalysis have caused identical triad arrangements to evolve independently more than 20 times in different enzyme superfamilies.[5]Threonine proteases use the amino acid threonine as their catalytic nucleophile. Unlike cysteine and serine, threonine is a secondary alcohol (i.e. has a methyl group). The methyl group of threonine greatly restricts the possible orientations of triad and substrate, as the methyl clashes with either the enzyme backbone or the histidine base. Consequently, most threonine proteases use an N-terminal threonine in order to avoid such steric clashes.Several evolutionarily independent enzyme superfamilies with different protein folds use the N-terminal residue as a nucleophile. This commonality of active site but difference of protein fold indicates that the active site evolved convergently in those families.[5][18]Convergence occurs at the level of DNA and the amino acid sequences produced by translating structural genes into proteins. Studies have found convergence in amino acid sequences in echolocating bats and the dolphin;[19] among marine mammals;[20] between giant and red pandas;[21] and between the thylacine and canids.[22] Convergence has also been detected in a type of non-coding DNA, cis-regulatory elements, such as in their rates of evolution; this could indicate either positive selection or relaxed purifying selection.[23]Swimming animals including fish such as herrings, marine mammals such as dolphins, and ichthyosaurs (of the Mesozoic) all converged on the same streamlined shape.[24][25] The fusiform bodyshape (a tube tapered at both ends) adopted by many aquatic animals is an adaptation to enable them to travel at high speed in a high drag environment.[26] Similar body shapes are found in the earless seals and the eared seals: they still have four legs, but these are strongly modified for swimming.[27]The marsupial fauna of Australia and the placental mammals of the Old World have several strikingly similar forms, developed in two clades, isolated from each other.[7] The body and especially the skull shape of the thylacine (Tasmanian wolf) converged with those of Canidae such as the red fox, Vulpes vulpes.[28]Red fox skeletonSkulls of  thylacine (left),  timber wolf (right)Thylacine skeletonAs a sensory adaptation, echolocation has evolved separately in cetaceans (dolphins and whales) and bats, but from the same genetic mutations.[29][30]One of the best-known examples of convergent evolution is the camera eye of cephalopods (such as squid and octopus), vertebrates (including mammals) and cnidaria (such as jellyfish).[32]  Their last common ancestor had at most a simple photoreceptive spot, but a range of processes led to the progressive refinement of camera eyes — with one sharp difference: the cephalopod eye is "wired" in the opposite direction, with blood and nerve vessels entering from the back of the retina, rather than the front as in vertebrates. As a result, cephalopods lack a blind spot.[7]Birds and bats have homologous limbs because they are both ultimately derived from terrestrial tetrapods, but their flight mechanisms are only analogous, so their wings are examples of functional convergence. The two groups have powered flight, evolved independently.  Their wings differ substantially in construction. The bat wing is a membrane stretched across four extremely elongated fingers and the legs. The airfoil of the bird wing is made of feathers, strongly attached to the forearm (the ulna) and the highly fused bones of the wrist and hand (the carpometacarpus), with only tiny remnants of two fingers remaining, each anchoring a single feather. So, while the wings of bats and birds are functionally convergent, they are not anatomically convergent.[3][33] Birds and bats also share a high concentration of cerebrosides in the skin of their wings. This improves skin flexibility, a trait useful for flying animals; other mammals have a far lower concentration.[34] The extinct pterosaurs independently evolved wings from their fore- and hindlimbs, while insects have wings that evolved separately from different organs.[35]Flying squirrels and sugar gliders are much alike in their body plans, with gliding wings stretched between their limbs, but flying squirrels are placental mammals while sugar gliders are marsupials, widely separated within the mammal lineage.[36]Insect mouthparts show many examples of convergent evolution. The mouthparts of different insect groups consist of a set of homologous organs, specialised for the dietary intake of that insect group. Convergent evolution of many groups of insects led from original biting-chewing mouthparts to different, more specialised, derived function types. These include, for example, the proboscis of flower-visiting insects such as bees and flower beetles,[37][38][39] or the biting-sucking mouthparts of blood-sucking insects such as fleas and mosquitos.Opposable thumbs allowing the grasping of objects are most often associated with primates, like humans, monkeys, apes, and lemurs. Opposable thumbs also evolved in giant pandas, but these are completely different in structure, having six fingers including the thumb, which develops from a wrist bone entirely separately from other fingers.[40]Convergent evolution in humans includes blue eye colour and light skin colour. When humans migrated out of Africa, they moved to more northern latitudes with less intense sunlight. It was beneficial to them to reduce their skin pigmentation. It appears certain that there was some lightening of skin colour before European and East Asian lineages diverged, as there are some skin-lightening genetic differences that are common to both groups. However, after the lineages diverged and became genetically isolated, the skin of both groups lightened more, and that additional lightening was due to different genetic changes.[41]Lemurs and humans are both primates. Ancestral primates had brown eyes, as most primates do today. The genetic basis of blue eyes in humans has been studied in detail and much is known about it. It is not the case that one gene locus is responsible, say with brown dominant to blue eye colour. However, a single locus is responsible for about 80% of the variation. In lemurs, the differences between blue and brown eyes are not completely known, but the same gene locus is not involved.[42]While convergent evolution is often illustrated with animal examples, it has often occurred in plant evolution. For instance, C4 photosynthesis, one of the three major carbon-fixing biochemical processes, has arisen independently up to 40 times.[43][44] About 7,600 plant species of angiosperms use C4 carbon fixation, with many monocots including 46% of grasses such as maize and sugar cane,[45][46] and dicots including several species in the Chenopodiaceae and the Amaranthaceae.[47][48]A good example of convergence in plants is the evolution of edible fruits such as apples. These pomes incorporate (five) carpels and their accessory tissues forming the apple's core, surrounded by structures from outside the botanical fruit, the receptacle or hypanthium. Other edible fruits include other plant tissues;[49] for example, the fleshy part of a tomato is the walls of the pericarp.[50] This implies convergent evolution under selective pressure, in this case the competition for seed dispersal by animals through consumption of fleshy fruits.[51]Seed dispersal by ants (myrmecochory) has evolved independently more than 100 times, and is present in more than 11,000 plant species. It is one of the most dramatic examples of convergent evolution in biology.[52]Carnivory has evolved multiple times independently in plants in widely separated groups. In three species studied, Cephalotus follicularis, Nepenthes alata and Sarracenia purpurea, there has been convergence at the molecular level. Carnivorous plants secrete enzymes into the digestive fluid they produce. By studying phosphatase, glycoside hydrolase, glucanase,  RNAse and chitinase enzymes as well as a pathogenesis-related protein and a thaumatin-related protein,  the authors found many convergent amino acid substitutions. These changes were not at the enzymes' catalytic sites, but rather on the exposed surfaces of the proteins, where they might interact with other components of the cell or the digestive fluid. The authors also found that homologous genes in the non-carnivorous plant Arabidopsis thaliana tend to have their expression increased when the plant is stressed, leading the authors to suggest that stress-responsive proteins have often been co-opted[c] in the repeated evolution of carnivory.[53]Phylogenetic reconstruction and ancestral state reconstruction proceed by assuming that evolution has occurred without convergence. Convergent patterns may, however, appear at higher levels in a phylogenetic reconstruction, and are sometimes explicitly sought by investigators. The methods applied to infer convergent evolution depend on whether pattern-based or process-based convergence is expected. Pattern-based convergence is the broader term, for when two or more lineages independently evolve patterns of similar traits. Process-based convergence is when the convergence is due to similar forces of natural selection.[54]Earlier methods for measuring convergence incorporate ratios of phenotypic and phylogenetic distance by simulating evolution with a Brownian motion model of trait evolution along a phylogeny.[55][56] More recent methods also quantify the strength of convergence.[57] One drawback to keep in mind is that these methods can confuse long-term stasis with convergence due to phenotypic similarities. Stasis occurs when there is little evolutionary change among taxa.[54]Distance-based measures assess the degree of similarity between lineages over time. Frequency-based measures assess the number of lineages that have evolved in a particular trait space.[54]Methods to infer process-based convergence fit models of selection to a phylogeny and continuous trait data to determine whether the same selective forces have acted upon lineages. This uses the Ornstein-Uhlenbeck (OU) process to test different scenarios of selection. Other methods rely on an a priori specification of where shifts in selection have occurred.[58]
Afforestation
Afforestation is the establishment of a forest or stand of trees (forestation) in an area where there was no previous tree cover.[1]Many government and non-governmental organizations directly engage in programs of afforestation to create forests, increase carbon capture and carbon sequestration, and help to anthropogenically improve biodiversity. In the UK, afforestation may mean converting the legal status of some land to royal forest.Gap dynamics is the pattern of plant growth that occurs following the creation of a forest gap, a local area of natural disturbance that results in an opening in the canopy of a forest. Gap dynamics are a typical characteristic of temperate and tropical forests, and have a wide variety of causes and effects on forest life.Sometimes special tools, such as a tree planting bar, are used to make planting of trees easier and faster.[citation needed]In some places, forests need help to reestablish themselves because of environmental factors.  For a example, in arid zones, once forest cover is destroyed, the land may become dry and inhospitable for the growth of new trees .  Other factors include overgrazing by livestock, especially animals such as goats, cows, and over-harvesting of forest resources. Together these may lead to desertification and the loss of topsoil; without soil, forests cannot grow until the long process of soil creation has been completed - if erosion allows this. In some tropical areas, forest cover removal may result in a duricrust or duripan that effectively seal off the soil to water penetration and root growth.  In many areas, reforestation is impossible because people are using the land. In other areas, mechanical breaking up of duripans or duricrusts is necessary, careful and continued watering may be essential, and special protection, such as fencing, may be needed.Several new studies suggest that forests attract rain and this may explain why drought is occurring more frequently in parts of the world such as western Africa. A new study by Carol Rasmussen,NASA's Jet Propulsion Laboratory gives the first observational evidence that the southern Amazon rain forest triggers its own rainy season using water vapor from plant leaves. The finding helps explain why deforestation in this region is linked with reduced rainfall [2] A study by Douglas Sheil and Daniel Murdiyarso hypothesis suggests that forest cover plays a much greater role in determining rainfall than previously recognized. It explains how forested regions generate large-scale flows in atmospheric water vapor[3] Makarieva and Gorshkov have developed a hypothesis to explain how forests attract moist air and increase rainfall in area covered by trees [4]In Adelaide, South Australia (a city of 1.3 million as of June 2016),[5] Premier Mike Rann (2002 to 2011) launched an urban forest initiative in 2003 to plant 3 million native trees and shrubs by 2014 on 300 project sites across the metro area.  The projects range from large habitat restoration projects to local biodiversity projects. Thousands of Adelaide citizens have participated in community planting days. Sites include parks, reserves, transport corridors, schools, water courses and coastline. Only trees native to the local area are planted to ensure genetic integrity. Premier Rann said the project aimed to beautify and cool the city and make it more liveable; improve air and water quality and reduce Adelaide's greenhouse gas emissions by 600,000 tonnes of C02 a year. He said it was also about creating and conserving habitat for wildlife and preventing species loss.[6]There is extensive and ongoing Amazon deforestation.[7][page needed]). There is also ongoing afforestation effort in Brazil. In an afforestation hotspot outlined in Para, Brazil, 1 billion trees are intended to be planted to restore deforested lands by 2013.[8]Desertification is increasing along the Sahel, the strip of land between Africa's fertile tropics and the Sahara Desert. After a crippling famine in the 1970s caused by overgrazing and deforestation, a local community approach has been pioneered by Yacouba Sawadogo, a peasant farmer.[9] By replanting trees and crops together in holes filled with compost, whole villages have been able to move back to areas considered uninhabitable.China has deforested most of its historically wooded areas. China reached the point where timber yields declined far below historic levels, due to over-harvesting of trees beyond sustainable yield.[11] Although it has set official goals for reforestation, these goals are set over an 80-year time horizon and have not been significantly met by 2008. China is trying to correct these problems by projects like the Green Wall of China, which aims to replant a great deal of forests and halt the expansion of the Gobi desert. The Green Wall of China Project has historical precedences dating back to before the Common Era. However, in pre-modern periods, government sponsored afforestation projects along the historical frontier regions were mostly for military fortification.[12]A law promulgated in 1981 requires that every school student over the age of 11 plants at least one tree per year. As a result, China has the highest afforestation rate of any country or region in the world, with 47,000 square kilometers of afforestation in 2008.[13] However, the forest area per capita is still far lower than the international average.[14] There has also been considerable criticism regarding the effectiveness of planting so many trees especially in regions where they never grew prior. Studies reveal that the water table of those areas is becoming deeper indicating significant water loss.Europe has deforested the majority of its historical forests. The European Union (EU) has paid farmers for afforestation since 1990, offering grants to turn farmland back into forest and payments for the management of forest. Between 1993 and 1997, EU afforestation policies made possible the re-forestation of over 5,000 square kilometres of land. A second program, running between 2000 and 2006, afforested more than 1000 square kilometres of land (precise statistics not yet available). A third such program began in 2007. Europe's forests are growing by 0.8 million ha a year thanks to these programmes.[15]In Poland, the National Program of Afforestation was introduced by the government after World War II, when  area of forests shrank to 20% of country's territory. Consequently, forested areas of Poland grew year by year, and on December 31, 2006, forests covered 29% of the country (see: Polish forests). It is planned that by 2050, forests will cover 33% of Poland.According to Food and Agriculture Organization statistics, Spain had the third fastest afforestation rate in Europe in the 1990-2005 period, after Iceland and Ireland.[16][17] In those years, a total of 44,360 square kilometers were afforested, and the total forest cover rose from 13,5 to 17,9 million hectares. In 1990, forests covered 26.6% of the Spanish territory. As of 2007, that figure had risen to 36.6%. Spain today has the fifth largest forest area in the European Union.[18]In January 2013 the UK government set a target of 12% woodland cover in England by 2060, up from the then 10%.[19]  Government-backed initiatives such as the Woodland Carbon Code are intended to support this objective by encouraging corporations and landowners to create new woodland to offset their carbon emissions.Alpine and Subalpine regions have undergone a lot of deforestation and then forestation in the last 300 years. Out of this has emerged much practical experience. One example is the clustered group,[20] which is a method to bring in stable age mixed tree communities.Since the founding of the crown colony in the 19th century, afforestation has taken place to prevent soil erosion in the catchment areas of the reservoirs that were built. During the Japanese occupation in the Second World War, the countryside was deforested as the remaining population required fuel to survive. Most of the trees were cut down and extensive reafforestation was carried out after the war. Trees that were planted are mostly non-native species, such as: Pinus massoniana, Acacia confusa (Formosan acacia), Lophostemon confertus and the Paper Bark Tree.India has witnessed a minor increase in the percentage of the land area under forest cover from 1950 to 2006. In 1950 around 40.48 million hectares was covered by forest. In 1980 it increased to 67.47 million hectares and in 2006 it was found to be 69 million hectares. 23% of India is covered by forest.[21] The forests of India are grouped into 5 major categories and 16 types based on biophysical criteria. 38% of the forest is categorized as subtropical dry deciduous and 30% as tropical moist deciduous and other smaller groups. Only local species are planted in an area. Trees bearing fruits are preferred wherever possible due to their function as a food source.Iran is considered a low forest cover region of the world with present cover approximating seven percent of the land area. This is a value reduced by an estimated six million hectares of virgin forest, which includes oak, almond and pistachio.[22]  Due to soil substrates, it is difficult to achieve afforestation on a large scale compared to other temperate areas endowed with more fertile and less rocky and arid soil conditions.[22]  Consequently, most of the afforestation is conducted with non-native species,[22] leading to habitat destruction for native flora and fauna, and resulting in an accelerated loss of biodiversity.[7][page needed]Since 1996, with the preparation and testing of areas suitable for afforestation in the highlands of northern Tehran, the plan of flooding began with the name of the Southern Alborz Range forestry plan, which will cover approximately 2700 hectares of afforestation by 2017.By 2020, more than 2000 hectares will be added to the plan.(Amiri)Tree-planting is an ancient Jewish tradition, mentioned in the Talmud as being more important than greeting the Messiah.[23] With over 240 million planted trees, Israel is one of only two countries that entered the 21st century with a net gain in the number of trees, due to massive afforestation efforts.[24] Most Israeli forests are the product of a major afforestation campaign by the Jewish National Fund (JNF).[citation needed]Critics argue that many JNF lands inside the West Bank were illegally confiscated from Palestinian refugees, and that the JNF furthermore should not be involved with lands in the West Bank.[25] Shaul Ephraim Cohen has claimed that trees have been planted to restrict Bedouin herding.[26] Susan Nathan wrote that forests were planted on the site of abandoned Arab villages after the 1948 war.[27]Since 2009, the JNF has provided the Palestinian Authority with 3,000 tree seedlings for a forested area being developed on the edge of the new city of Rawabi, north of Ramallah.[28]In North Africa, the Sahara Forest Project coupled with the Seawater greenhouse has been proposed. Some projects have also been launched in countries as Senegal to revert desertification. As of 2010, African leaders are discussing the combining of national resources to increase effectiveness.[29] In addition, other projects as the Keita Project in Niger have been launched in the past, and have been able to locally revert damage done by desertification. See Development aid#EffectivenessThe United States is roughly one-third covered in forest and woodland.[citation needed] Nevertheless, areas in the US were subject to significant tree planting. In the 1800s people moving westward encountered the Great Plains – land with fertile soil, a growing population and a demand for timber but with few trees to supply it. So tree planting was encouraged along homesteads. Arbor Day was founded in 1872 by Julius Sterling Morton in Nebraska City, Nebraska. By the 1930s the Dust Bowl environmental disaster signified a reason for significant new tree cover. Public works programs under the New Deal saw the planting of 18,000 miles of windbreaks stretching from North Dakota to Texas to fight soil erosion (see Great Plains Shelterbelt).At their summit in Copenhagen in 2009, organised by the UK based The Climate Group, leaders of subnational governments – states, regions and provinces – unanimously supported a recommendation by Premier Rann to plant one billion trees across their varied jurisdictions.  The initiative was strongly supported by leaders present including Quebec Premier Jean Charest, California Governor Arnold Schwarzenegger and Scottish First Minister Alex Salmond.  At a subsequent meeting in Rio de Janeiro in June 2012, The Climate Group announced that it had already received commitments by member governments to plant more than 500 million trees.[30]The use of afforestation as strategy of conservation of forest biomes is seen as a menace to the conservation of natural grassland and savanna biomes, as the ideal would be the reforestation of areas where forest occurs naturally.[31]
