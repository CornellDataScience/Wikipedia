Function (mathematics)
In mathematics, a function[1] was originally the idealization of how a varying quantity depends on another quantity. For example, the position of a planet is a function of time. Historically, the concept was elaborated with the infinitesimal calculus at the end of the 17th century, and, until the 19th century, the functions that were considered were differentiable (that is, they had a high degree of regularity). The concept of function was formalized at the end of the 19th century in terms of set theory, and this greatly enlarged the domains of application of the concept. A function is a process or a relation that associates each element x of a set X,  the domain of the function, to a single element y of another set Y (possibly the same set), the codomain of the function. If the function is called f, this relation is denoted y = f (x) (read f of x), the element x is the argument or input of the function, and y is the value of the function, the output, or the image of x by f.[2] The symbol that is used for representing the input is the variable of the function (one often says that f is a function of the variable x).A function is uniquely represented by its graph which is the set of all pairs (x, f (x)). When the domain and the codomain are sets of numbers, each such pair may be considered as the Cartesian coordinates of a point in the plane. In general, these points form a curve, which is also called the graph of the function. This is a useful representation of the function, which is commonly used everywhere, for example in newspapers.Functions are widely used in science, and in most fields of mathematics. Their role is so important that it has been said that they are "the central objects of investigation" in most fields of mathematics.[3]Intuitively, a function is a process that associates to each element of a set X a unique element of a set Y.Formally, a function f from a set X to a set Y is defined by a set G of ordered pairs (x, y) such that x ∈ X, y ∈ Y, and every element of X is the first component of exactly one ordered pair in G.[4] In other words, for every x in X, there is exactly one element y such that the ordered pair (x, y) belongs to the set of pairs defining the function f. The set G is called the graph of the function. Formally speaking, it may be identified with the function, but this hides the usual interpretation of a function as a process. Therefore, in common usage, the function is generally distinguished from its graph. Functions are also called maps or mappings. However, some authors[5] reserve the word mapping to the case where the codomain Y belongs explicitly to the definition of the function. In this sense, the graph of the mapping recovers the function as the set of pairs.In the definition of function, X and Y are respectively called the domain and the codomain of the function f. If (x, y) belongs to the set defining f, then y is the image of x under f, or the value of f applied to the argument x. Especially in the context of numbers, one says also that y is the value of f for the value x of its variable, or, still shorter, y is the value of f of x, denoted as y = f(x).The range of a function is the set of the images of all elements in the domain. However, range is sometimes used as a synonym of codomain, generally in old textbooks.A univalent relation is a relation such thatUnivalent relations may be identified to functions whose domain is a subset of X.A left-total relation is a relation such that Formal functions may be strictly identified to relations that are both univalent and left total. Violating the left-totality is similar to giving a convenient encompassing set instead of the true domain, as explained above.This distinction in language and notation becomes important in cases where functions themselves serve as inputs for other functions.  (A function taking another function as an input is termed a functional.)  Other approaches to denoting functions, detailed below, avoid this problem but are less commonly used.First used by Leonhard Euler in 1734,[7] it is often useful to use a symbol for denoting a function. This symbol consists generally of a single letter in italic font, most often the lower-case letters f, g, h. Some widely used functions are represented by a symbol consisting of several letters (usually two or three, generally an abbreviation of their name). By convention, the symbol for standard functions is set in roman type, such as "sin" for the sine function, in contrast to functions defined on an ad hoc basis.The notation (read: "y equals f of x")means that the pair (x, y) belongs to the set of pairs defining the function f. If X is the domain of f, the set of pairs defining the function is thus, using set-builder notation,Often, a definition of the function is given by what f does to the explicit argument x.  For example, a function f can be defined by the equationFor explicitly expressing domain X and the codomain Y of a function f, the arrow notation is often used (read: "the function f from X to Y" or "the function f mapping elements of  X to elements of Y"):orThis is often used in relation with the arrow notation for elements (read: "f maps x to f (x)"), often stacked immediately below the arrow notation giving the function symbol, domain, and codomain: the latter line being more commonly written There are other, specialized notations for functions in sub-disciplines of mathematics.  For example, in linear algebra and functional analysis, linear forms and the vectors they act upon are denoted using a dual pair to show the underlying duality.  This is similar to the use of bra–ket notation in quantum mechanics.  In logic and the theory of computation, the function notation of lambda calculus is used to explicitly express the basic notions of function abstraction and application.  In category theory and homological algebra, networks of functions are described in terms of how they and their compositions commute with each other using commutative diagrams that extend and generalize the arrow notation for functions described above.According to the definition of a function, a specific function is, in general, defined by associating to every element of its domain one element of its codomain. When the domain and the codomain are sets of numbers, this association may take the form of a computation taking as input any element of the domain and producing an output in the codomain. This computation may be described by a formula. (This is the starting point of algebra, where many similar numerical computations can be replaced by a single formula that describes these computations by means of variables that represent computation inputs as unspecified numbers). This type of specification of a function frequently uses previously defined auxiliary functions. The above ways of defining functions define them "pointwise", that is, each value is defined independently of the other values. This is not necessarily the case.When the domain of a function is the set of nonnegative integers or, more generally, when the domain is a well ordered set, a function may be defined by induction or recursion, meaning (roughly) that the calculation of the value of the function for some given input requires values of the function for lesser inputs. For example, the Fibonacci sequence is a function from the natural numbers into themselves that is defined by two starting values and a formula, recurring to the two immediately preceding arguments (see above for the use of indices for the argument of a function): As functions may be complicated objects, it is often useful to draw the graph of a function for getting a global view of its properties. Some functions may also represented histogramsIt is possible to draw effectively the graph of a function only if the function is sufficiently regular, that is, either if the function is differentiable (or piecewise differentiable) or if its domain may be identified with the integers or a subset of the integers.Histograms are often used for representing functions whose domain is finite, or is the natural numbers or the integers. In this case, an element x of the domain is represented by an interval of the x-axis, and a point (x, y) of the graph is represented by a rectangle with basis the interval corresponding to x and height y.In statistic, histogram are often used for representing very irregular functions. For example, for representing the function that associates his weight to each member of some population, one draws the histogram of the function that associates to each weight interval the number of people, whose weights belong to this interval. There are many variants of this method, see Histogram for details.This section describes general properties of functions, that are independent of specific properties of the domain and the codomain.Some functions are uniquely defined by their domain and codomain, and are sometimes called canonical: Two functions f and g are equal if their domain and codomain sets agree and their output values agree on the whole domain. Formally, f=g if f(x)=g(x) for all x∈X, where f:X→Y and g:X→Y.A composite function g(f(x)) can be visualized as the combination of two "machines".A simple example of a function compositionAnother composition. In this example, (g ∘ f )(c) = #.The image of f is the image of the whole domain, that is f(X). It is also called the range of f, although the term may also refer to the codomain.[8]For example, the preimage of {4, 9} under the square function is the set {−3,−2,2,3}. The preimage by f of an element y of the codomain is sometimes called, in some contexts, the fiber of y under f.  "One-to-one" and "onto" are terms that were more common in the older English language literature; "injective", "surjective", and "bijective" were originally coined as French words in the second quarter of the 20th century by the Bourbaki group and imported into English.  As a word of caution, "a one-to-one function" is one that is injective, while a "one-to-one correspondence" refers to a bijective function.  Also, the statement "f maps X onto Y" differs from "f  maps X into B" in that the former implies that f is surjective), while the latter makes no assertion about the nature of f the mapping.  In a complicated reasoning, the one letter difference can easily be missed. Due to the confusing nature of this older terminology, these terms have declined in popularity relative to the Bourbakian terms, which have also the advantage to be more symmetrical.This often used for define partial inverse functions: if there is a subset S of a function f such that f|S is injective, then the canonical surjection of f|S on its image f|S(S) = f(S) is a bijection, which has an inverse function from f(S) to S. This is in this way that inverse trigonometric functions are defined. The cosine function, for example, is injective, when restricted to the interval (–0, π); the image of this restriction is the interval (–1, 1); this defines thus an inverse function from (–1, 1) to (–0, π), which is called arccosine and denoted arccos.An extension of a  function f is a function g such that f is a restriction of g. A typical use of this concept is the process of analytic continuation, that allows extending functions whose domain is a small part of the complex plane to functions whose domain is almost the whole complex plane.A multivariate function, or function of several variables is a function that depends on several arguments. Such functions are commonly encountered. For example, the position of a car on a road is a function of the time and its speed.More formally, a function of n variables is a function whose domain is a set of n-tuples.For example, multiplication of integers is a function of two variables, or bivariate function, whose domain is the set of all pairs (2-tuples) of integers, and whose codomain is the set of integers. The same is true for every binary operation. More generally, every mathematical operation is defined as a multivariate function.where the domain U has the formIt is common to also consider functions whose codomain is a product of sets. For example, Euclidean division maps every pair (a, b) of integers with b ≠ 0 to a pair of integers called the quotient and the remainder:The codomain may also be a vector space. In this case, one talks of a vector-valued function. If the domain is contained in a Euclidean space, or more generally a manifold, a vector-valued function is often called a vector field.The idea of function, starting in the 17th century, was fundamental to the new infinitesimal calculus (see History of the function concept). At that time, only real-valued functions of a real variable were considered, and all functions were assumed to be smooth. But the definition was soon extended to functions of several variables and to function of a complex variable.  In the second half of 19th century, the mathematically rigorous definition of a function was introduced, and functions with arbitrary domains and codomains were defined. Functions are now used throughout all areas of mathematics.  In introductory calculus, when the word function is used without qualification, it means a real-valued function of a single real variable.  The more general definition of a function is usually introduced to second or third year college students with STEM majors, and in their senior year they are introduced to calculus in a larger, more rigorous setting in courses such as real analysis and complex analysis.A real function is a real-valued function of a real variable, that is, a function whose codomain is the field of real numbers and whose domain is a set of real numbers that contains an interval. In this section, these functions are simply called functions.The functions that are most commonly considered in mathematics and its applications have some regularity, that is they are continuous, differentiable, and even analytic. This regularity insures that these functions can be visualized by their graphs. In this section, all functions are differentiable in some interval.Functions enjoy pointwise operations, that is, if f and g are functions, their sum, difference and product are functions defined  by The domains of the resulting functions are the intersection of the domains of f and g. The quotient of two functions is defined similarly by but the domain of the resulting function is obtained by removing the zeros of g from the intersection of the domains of f and g.Many other real functions are defined either by the implicit function theorem (the inverse function is a particular instance) or as solutions of differential equations. For example the sine and the cosine functions are the solutions of the linear differential equation such that When working with complex numbers different types of functions are used[9]: The study of complex functions is a vast subject in mathematics with many applications, and that can claim[10] to be an ancestor to many other areas of mathematics, like homotopy theory, and manifolds.In mathematical analysis, and more specifically in functional analysis, a function space is a set of scalar-valued or vector-valued functions, which share a specific property and form a topological vector space. For example, the real smooth functions with a compact support (that is, they are zero outside some compact set) form a function space that is at the basis of the theory of distributions.Function spaces play a fundamental role in advanced mathematical analysis, by allowing the use of their algebraic and topological properties for studying properties of functions. For example, all theorems of existence and uniqueness of solutions of ordinary or partial differential equations result of the study of function spaces.It is rather frequent that a function with domain X may be naturally extended to a function whose domain is a set Z that is built from X.where f (S) is the image by f of the subset S of X.Under slight abuse of notation this function on subsets is often denoted also by f.which is also a ring homomorphism.Usefulness of the concept of multi-valued functions is clearer when considering complex functions, typically analytic functions. The domain to which a complex function may be extended by analytic continuation generally consists of almost the whole complex plane. However, when extending the domain through two different paths, one often gets different values. For example, when extending the domain of the square root function, along a path of complex numbers with positive imaginary parts, one gets i for the square root of –1; while, when extending through complex numbers with negative imaginary parts, one gets –i. There are generally two ways of solving the problem. One may define a function that is not continuous along some curve, called a branch cut. Such a function is called the principal value of the function. The other way is to consider that one has a multi-valued function, which is analytic everywhere except for isolated singularities, but whose value may "jump" if one follows a closed loop around a singularity. This jump is called the monodromy.The definition of a function that is given in this article requires the concept of set, since the domain and the codomain of a function must be a set. This is not a problem in usual mathematics, as it is generally not difficult to consider only functions whose domain and codomain are sets, which are well defined, even if the domain is not explicitly defined. However, it is sometimes useful to consider more general functions. These generalized functions may be critical in the development of a formalization of foundations of mathematics. For example, the Von Neumann–Bernays–Gödel set theory, is an extension of the set theory in which the collection of all sets is a class. This theory includes the replacement axiom, which may be interpreted as "if X is a set, and F is a function, then F[X] is a set".
Quadruple product
In mathematics, the quadruple product is a product of four vectors in three-dimensional Euclidean space. The name "quadruple product" is used for two different products,[1] the scalar-valued scalar quadruple product and the vector-valued vector quadruple product or vector product of four vectors .The scalar quadruple product  is defined as the dot product of two cross products:where a, b, c, d are vectors in three-dimensional Euclidean space.[2] It can be evaluated using the identity:[2]or using the determinant:The vector quadruple product  is defined as the cross product of two cross products:where a, b, c, d are vectors in three-dimensional Euclidean space.[3] It can be evaluated using the identity:[4]This identity can also be written using tensor notation and the Einstein summation convention as follows:using the notation for the triple product:Equivalent forms can be obtained using the identity:[5]The quadruple products are useful for deriving various formulas in spherical and plane geometry.[3] For example, if four points are chosen on the unit sphere, A, B, C, D, and unit vectors drawn from the center of the sphere to the four points, a, b, c, d respectively, the identity:in conjunction with the relation for the magnitude of the cross product:and the dot product:where a = b = 1 for the unit sphere, results in the identity among the angles attributed to Gauss:where x is the angle between a × b and c × d, or equivalently, between the planes defined by these vectors.Josiah Willard Gibbs's pioneering work on vector calculus provides several other examples.[3]
MATLAB
MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, C#, Java, Fortran and Python.Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.As of 2018, MATLAB has more than 3 million users worldwide.[7] MATLAB users come from various backgrounds of engineering, science, and economics.Cleve Moler, the chairman of the computer science department at the University of New Mexico, started developing MATLAB in the late 1970s.[8] He designed it to give his students access to LINPACK and EISPACK without them having to learn Fortran. It soon spread to other universities and found a strong audience within the applied mathematics community. Jack Little, an engineer, was exposed to it during a visit Moler made to Stanford University in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in C and founded MathWorks in 1984 to continue its development. These rewritten libraries were known as JACKPAC.[9] In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, LAPACK.[10]MATLAB was first adopted by researchers and practitioners in control engineering, Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of linear algebra, numerical analysis, and is popular amongst scientists involved in image processing.[8]The MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical shell or executing text files containing MATLAB code.[11]Variables are defined using the assignment operator, =. MATLAB is a weakly typed programming language because types are implicitly converted.[12]  It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects,[13] and that their type can change. Values can come from constants, from computation involving values of other variables, or from the output of a function. For example:A simple array is defined using the colon syntax: initial:increment:terminator. For instance:defines a variable named array (or assigns a new value to an existing variable with the name array) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the initial value), increments with each step from the previous value by 2 (the increment value), and stops once it reaches (or to avoid exceeding) 9 (the terminator value).the increment value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.assigns to the variable named ari an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.Indexing is one-based,[14] which is the usual convention for matrices in mathematics, although not for some programming languages such as C, C++, and Java.Matrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).Sets of indices can be specified by expressions such as "2:4", which evaluates to [2, 3, 4].  For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:A square identity matrix of size n can be generated using the function eye, and matrices of any size with zeros or ones can be generated with the functions zeros and ones, respectively.Transposing a vector or a matrix is done either by the function transpose or by adding prime after a dot to the matrix. Without the dot MATLAB will perform conjugate transpose.Most MATLAB functions can accept matrices and will apply themselves to each element. For example, mod(2*J,n) will multiply every element in "J" by 2, and then reduce each element modulo "n". MATLAB does include standard "for" and "while" loops, but (as in other similar applications such as R), using the vectorized notation often produces code that is faster to execute. This code, excerpted from the function magic.m, creates a magic square M for odd values of n (MATLAB function meshgrid is used here to generate square matrices I and J containing 1:n).MATLAB has structure data types.[15] Since all variables in MATLAB are arrays, a more adequate name is "structure array", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names[16] (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.[17]When creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores.  Functions are often case sensitive.MATLAB supports elements of lambda calculus by introducing function handles,[18] or function references, which are implemented either in .m files or anonymous[19]/nested functions.[20]MATLAB supports object-oriented programming including classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics.[21] However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has handle as a super-class (for reference classes) or not (for value classes).[22]Method call behavior is different between value and reference classes. For example, a call to a methodcan alter any member of object only if object is an instance of a reference class.An example of a simple class is provided below.When put into a file named hello.m, this can be executed with the following commands:MATLAB supports developing applications with graphical user interface (GUI) features. MATLAB includes GUIDE[23] (GUI development environment) for graphically designing GUIs.[24] It also has tightly integrated graph-plotting features. For example, the function plot can be used to produce a graph from two vectors x and y. The code:produces the following figure of the sine function:A MATLAB program can produce three-dimensional graphics using the functions surf, plot3 or mesh.In MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.[25]MATLAB can call functions and subroutines written in the programming languages C or Fortran.[26] A wrapper function is created allowing MATLAB data types to be passed and returned. MEX files (MATLAB executables) are the dynamically loadable object files created by compiling such functions.[27][28] Since 2014 increasing two-way interfacing with Python was being added.[29][30]Libraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB,[31][32] and many MATLAB libraries (for example XML or SQL support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox[33] which is sold separately by MathWorks, or using an undocumented mechanism called JMI (Java-to-MATLAB  Interface),[34][35] (which should not be confused with the unrelated Java Metadata Interface that is also called JMI). Official MATLAB API for Java was added in 2016.[36]As alternatives to the MuPAD based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to Maple or Mathematica.[37][38]Libraries also exist to import and export MathML.[39]MATLAB is a proprietary product of MathWorks, so users are subject to vendor lock-in.[40][41]  Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with .NET[42] or Java[43] application building environment, future development will still be tied to the MATLAB language.Each toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.It has been reported that European Union (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor.[44]  The regulators dropped the investigation after the complainant withdrew its accusation and no evidence of wrongdoing was found.[45]MATLAB has a number of competitors.[46] Commercial competitors include Mathematica, TK Solver, Maple, and IDL. There are also free open source alternatives to MATLAB, in particular GNU Octave, Scilab, FreeMat, and SageMath, which are intended to be mostly compatible with the MATLAB language; the Julia programming language also initially used MATLAB-like syntax. Among other languages that treat arrays as basic entities (array programming languages) are APL, Fortran 90 and higher, S-Lang, as well as the statistical languages R and S. There are also libraries to add similar functionality to existing languages, such as IT++ for C++, Perl Data Language for Perl, ILNumerics for .NET, NumPy/SciPy/matplotlib for Python, SciLua/Torch for Lua, SciRuby for Ruby, and Numeric.js for JavaScript.GNU Octave is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see MATLAB Compatibility of GNU Octave), therefore, making GNU Octave a superset of the MATLAB language.Re-introduced for Mac (under Mac OS X)The number (or release number) is the version reported by Concurrent License Manager program FLEXlm.For a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.[97]Several easter eggs exist in MATLAB.[111] These include hidden pictures,[112] and jokes. For example, typing in "spy" used to generate a picture of the spies from Spy vs Spy, but now displays an image of a dog. Typing in "why" randomly outputs a philosophical answer. Other commands include "penny", "toilet", "image", and "life".  Not every Easter egg appears in every version of MATLAB.
Multi-core processor
A multi-core processor is a single computing component with two or more independent processing units called cores, which read and execute program instructions.[1] The instructions are ordinary CPU instructions (such as add, move data, and branch) but the single processor can run multiple instructions on separate cores at the same time, increasing overall speed for programs amenable to parallel computing.[2] Manufacturers typically integrate the cores onto a single integrated circuit die (known as a chip multiprocessor or CMP) or onto multiple dies in a single chip package.  The microprocessors currently used in almost all personal computers are multi-core.A multi-core processor implements multiprocessing in a single physical package. Designers may couple cores in a multi-core device tightly or loosely. For example, cores may or may not share caches, and they may implement message passing or shared-memory inter-core communication methods. Common network topologies to interconnect cores include bus, ring, two-dimensional mesh, and crossbar. Homogeneous multi-core systems include only identical cores; heterogeneous multi-core systems have cores that are not identical (e.g. big.LITTLE have heterogeneous cores that share the same instruction set, while AMD Accelerated Processing Units have cores that don't even share the same instruction set). Just as with single-processor systems, cores in multi-core systems may implement architectures such as VLIW, superscalar, vector, or multithreading.Multi-core processors are widely used across many application domains, including general-purpose, embedded, network, digital signal processing (DSP), and graphics (GPU).The improvement in performance gained by the use of a multi-core processor depends very much on the software algorithms used and their implementation. In particular, possible gains are limited by the fraction of the software that can run in parallel simultaneously on multiple cores; this effect is described by Amdahl's law. In the best case, so-called embarrassingly parallel problems may realize speedup factors near the number of cores, or even more if the problem is split up enough to fit within each core's cache(s), avoiding use of much slower main-system memory. Most applications, however, are not accelerated so much unless programmers invest a prohibitive amount of effort in re-factoring the whole problem.[3]The parallelization of software is a significant ongoing topic of research.The terms multi-core and dual-core most commonly refer to some sort of central processing unit (CPU), but are sometimes also applied to digital signal processors (DSP) and system on a chip (SoC). The terms are generally used only to refer to multi-core microprocessors that are manufactured on the same integrated circuit die; separate microprocessor dies in the same package are generally referred to by another name, such as multi-chip module. This article uses the terms "multi-core" and "dual-core" for CPUs manufactured on the same integrated circuit, unless otherwise noted.In contrast to multi-core systems, the term multi-CPU refers to multiple physically separate processing-units (which often contain special circuitry to facilitate communication between each other).The terms many-core and massively multi-core are sometimes used to describe multi-core architectures with an especially high number of cores (tens to thousands[4]).[5]Some systems use many soft microprocessor cores placed on a single FPGA. Each "core" can be considered a "semiconductor intellectual property core" as well as a CPU core.[citation needed]While manufacturing technology improves, reducing the size of individual gates, physical limits of semiconductor-based microelectronics have become a major design concern. These physical limitations can cause significant heat dissipation and data synchronization problems. Various other methods are used to improve CPU performance. Some instruction-level parallelism (ILP) methods such as superscalar pipelining are suitable for many applications, but are inefficient for others that contain difficult-to-predict code. Many applications are better suited to thread-level parallelism (TLP) methods, and multiple independent CPUs are commonly used to increase a system's overall TLP. A combination of increased available space (due to refined manufacturing processes) and the demand for increased TLP led to the development of multi-core CPUs.Several business motives drive the development of multi-core architectures. For decades, it was possible to improve performance of a CPU by shrinking the area of the integrated circuit (IC), which reduced the cost per device on the IC. Alternatively, for the same circuit area, more transistors could be used in the design, which increased functionality, especially for complex instruction set computing (CISC) architectures. Clock rates also increased by orders of magnitude in the decades of the late 20th century, from several megahertz in the 1980s to several gigahertz in the early 2000s.As the rate of clock speed improvements slowed, increased use of parallel computing in the form of multi-core processors has been pursued to improve overall processing performance. Multiple cores were used on the same CPU chip, which could then lead to better sales of CPU chips with two or more cores. For example, Intel has produced a 48-core processor for research in cloud computing; each core has an x86 architecture.[6][7]Since computer manufacturers have long implemented symmetric multiprocessing (SMP) designs using discrete CPUs, the issues regarding implementing multi-core processor architecture and supporting it with software are well known.Additionally:In order to continue delivering regular performance improvements for general-purpose processors, manufacturers such as Intel and AMD have turned to multi-core designs, sacrificing lower manufacturing-costs for higher performance in some applications and systems. Multi-core architectures are being developed, but so are the alternatives. An especially strong contender for established markets is the further integration of peripheral functions into the chip.The proximity of multiple CPU cores on the same die allows the cache coherency circuitry to operate at a much higher clock rate than what is possible if the signals have to travel off-chip. Combining equivalent CPUs on a single die significantly improves the performance of cache snoop (alternative: Bus snooping) operations. Put simply, this means that signals between different CPUs travel shorter distances, and therefore those signals degrade less. These higher-quality signals allow more data to be sent in a given time period, since individual signals can be shorter and do not need to be repeated as often.Assuming that the die can physically fit into the package, multi-core CPU designs require much less printed circuit board (PCB) space than do multi-chip SMP designs. Also, a dual-core processor uses slightly less power than two coupled single-core processors, principally because of the decreased power required to drive signals external to the chip. Furthermore, the cores share some circuitry, like the L2 cache and the interface to the front-side bus (FSB). In terms of competing technologies for the available silicon die area, multi-core design can make use of proven CPU core library designs and produce a product with lower risk of design error than devising a new wider-core design. Also, adding more cache suffers from diminishing returns.Multi-core chips also allow higher performance at lower energy. This can be a big factor in mobile devices that operate on batteries. Since each core in a multi-core CPU is generally more energy-efficient, the chip becomes more efficient than having a single large monolithic core. This allows higher performance with less energy. A challenge in this, however, is the additional overhead of writing parallel code.[9]Maximizing the usage of the computing resources provided by multi-core processors requires adjustments both to the operating system (OS) support and to existing application software. Also, the ability of multi-core processors to increase application performance depends on the use of multiple threads within applications.Integration of a multi-core chip can lower the chip production yields. They are also more difficult to manage thermally than lower-density single-core designs. Intel has partially countered this first problem by creating its quad-core designs by combining two dual-core ones on a single die with a unified cache, hence any two working dual-core dies can be used, as opposed to producing four cores on a single die and requiring all four to work to produce a quad-core CPU. From an architectural point of view, ultimately, single CPU designs may make better use of the silicon surface area than multiprocessing cores, so a development commitment to this architecture may carry the risk of obsolescence. Finally, raw processing power is not the only constraint on system performance. Two processing cores sharing the same system bus and memory bandwidth limits the real-world performance advantage. In a 2009 report, Dr Jun Ni showed that if a single core is close to being memory-bandwidth limited, then going to dual-core might give 30% to 70% improvement; if memory bandwidth is not a problem, then a 90% improvement can be expected; however, Amdahl's law makes this claim dubious.[10] It would be possible for an application that used two CPUs to end up running faster on a single-core one if communication between the CPUs was the limiting factor, which would count as more than 100% improvement.The trend in processor development has been towards an ever-increasing number of cores, as processors with hundreds or even thousands of cores become theoretically possible.[11] In addition, multi-core chips mixed with simultaneous multithreading, memory-on-chip, and special-purpose "heterogeneous" (or asymmetric) cores promise further performance and efficiency gains,[12] especially in processing multimedia, recognition and networking applications. For example, a big.LITTLE core includes a high-performance core (called 'big') and a low-power core (called 'LITTLE'). There is also a trend towards improving energy-efficiency by focusing on performance-per-watt with advanced fine-grain or ultra fine-grain power management and dynamic voltage and frequency scaling (i.e. laptop computers and portable media players).Chips designed from the outset for a large number of cores (rather than having evolved from single core designs) are sometimes referred to as manycore designs, emphasising qualitative differences.The composition and balance of the cores in multi-core architecture show great variety. Some architectures use one core design repeated consistently ("homogeneous"), while others use a mixture of different cores, each optimized for a different, "heterogeneous" role.The article "CPU designers debate multi-core future" by Rick Merritt, EE Times 2008,[13] includes these comments:Chuck Moore [...] suggested computers should be like cellphones, using a variety of specialty cores to run modular software scheduled by a high-level applications programming interface.[...] Atsushi Hasegawa, a senior chief engineer at Renesas, generally agreed. He suggested the cellphone's use of many specialty cores working in concert is a good model for future multi-core designs.[...] Anant Agarwal, founder and chief executive of startup Tilera, took the opposing view. He said multi-core chips need to be homogeneous collections of general-purpose cores to keep the software model simple.An outdated version of an anti-virus application may create a new thread for a scan process, while its GUI thread waits for commands from the user (e.g. cancel the scan). In such cases, a multi-core architecture is of little benefit for the application itself due to the single thread doing all the heavy lifting and the inability to balance the work evenly across multiple cores. Programming truly multithreaded code often requires complex co-ordination of threads and can easily introduce subtle and difficult-to-find bugs due to the interweaving of processing on data shared between threads (see thread-safety). Consequently, such code is much more difficult to debug than single-threaded code when it breaks. There has been a perceived lack of motivation for writing consumer-level threaded applications because of the relative rarity of consumer-level demand for maximum use of computer hardware. Although threaded applications incur little additional performance penalty on single-processor machines, the extra overhead of development has been difficult to justify due to the preponderance of single-processor machines. Also, serial tasks like decoding the entropy encoding algorithms used in video codecs are impossible to parallelize because each result generated is used to help create the next result of the entropy decoding algorithm.Given the increasing emphasis on multi-core chip design, stemming from the grave thermal and power consumption problems posed by any further significant increase in processor clock speeds, the extent to which software can be multithreaded to take advantage of these new chips is likely to be the single greatest constraint on computer performance in the future. If developers are unable to design software to fully exploit the resources provided by multiple cores, then they will ultimately reach an insurmountable performance ceiling.The telecommunications market had been one of the first that needed a new design of parallel datapath packet processing because there was a very quick adoption of these multiple-core processors for the datapath and the control plane. These MPUs are going to replace[14] the traditional Network Processors that were based on proprietary microcode or picocode.Parallel programming techniques can benefit from multiple cores directly. Some existing parallel programming models such as Cilk Plus, OpenMP, OpenHMPP, FastFlow, Skandium, MPI, and Erlang can be used on multi-core platforms. Intel introduced a new abstraction for C++ parallelism called TBB. Other research efforts include the Codeplay Sieve System, Cray's Chapel, Sun's Fortress, and IBM's X10.Multi-core processing has also affected the ability of modern computational software development. Developers programming in newer languages might find that their modern languages do not support multi-core functionality. This then requires the use of numerical libraries to access code written in languages like C and Fortran, which perform math computations faster than newer languages like C#. Intel's MKL and AMD's ACML are written in these native languages and take advantage of multi-core processing. Balancing the application workload across processors can be problematic, especially if they have different performance characteristics. There are different conceptual models to deal with the problem, for example using a coordination language and program building blocks (programming libraries or higher-order functions). Each block can have a different native implementation for each processor type. Users simply program using these abstractions and an intelligent compiler chooses the best implementation based on the context.[15]Managing concurrency acquires a central role in developing parallel applications. The basic steps in designing parallel applications are:On the other hand, on the server side, multi-core processors are ideal because they allow many users to connect to a site simultaneously and have independent threads of execution. This allows for Web servers and application servers that have much better throughput.Vendors may license some software "per processor". This can give rise to ambiguity, because a "processor" may consist either of a single core or of a combination of cores.Embedded computing operates in an area of processor technology distinct from that of "mainstream" PCs. The same technological drives towards multi-core apply here too. Indeed, in many cases the application is a "natural" fit for multi-core technologies, if the task can easily be partitioned between the different processors.In addition, embedded software is typically developed for a specific hardware release, making issues of software portability, legacy code or supporting independent developers less critical than is the case for PC or enterprise computing. As a result, it is easier for developers to adopt new technologies and as a result there is a greater variety of multi-core processing architectures and suppliers.As of  2010[update], multi-core network processing devices have become mainstream, with companies such as Freescale Semiconductor, Cavium Networks, Wintegra and Broadcom all manufacturing products with eight processors. For the system developer, a key challenge is how to exploit all the cores in these devices to achieve maximum networking performance at the system level, despite the performance limitations inherent in an SMP operating system. To address this issue, companies such as 6WIND provide portable packet processing software designed so that the networking data plane runs in a fast path environment outside the OS.[18]In digital signal processing the same trend applies: Texas Instruments has the three-core TMS320C6488 and four-core TMS320C5441, Freescale the four-core MSC8144 and six-core MSC8156 (and both have stated they are working on eight-core successors). Newer entries include the Storm-1 family from Stream Processors, Inc with 40 and 80 general purpose ALUs per chip, all programmable in C as a SIMD engine and Picochip with three-hundred processors on a single die, focused on communication applications.As of  2016[update] heterogeneous multi-core solutions are becoming more common: Xilinx Zynq UltraScale+ MPSoC has Quad-core ARM Cortex-A53 and Dual-core ARM Cortex-R5. Software solutions such as OpenAMP are being used to help with inter processor communication.The research and development of multicore processors often compares many options, and benchmarks are developed to help such evaluations. Existing benchmarks include SPLASH-2, PARSEC, and COSMIC for heterogeneous systems.[26]
Aitken's delta-squared process
In numerical analysis, Aitken's delta-squared process or Aitken Extrapolation is a series acceleration method, used for accelerating the rate of convergence of a sequence. It is named after Alexander Aitken, who introduced this method in 1926.[1] Its early form was known to Seki Kōwa (end of 17th century) and was found for rectification of the circle, i.e. the calculation of π. It is most useful for accelerating the convergence of a sequence that is converging linearly.which can, with improved numerical stability, also be written asor equivalently aswhereandAitken's delta-squared process is a method of acceleration of convergence, and a particular case of a nonlinear sequence transformation.It is worth noting here that Aitken's method does not save two iteration steps; computation of the first three Ax values required the first five x values.  Also, the second Ax value is decidedly inferior to the 4th x value, mostly due to the fact that Aitken's process assumes linear, rather than quadratic, convergence[citation needed].In this example, Aitken's method is applied to a sublinearly converging series, accelerating convergence considerably.  It is still sublinear, but much faster than the original convergence[citation needed]: the first Ax value, whose computation required the first three x values, is closer to the limit than the eighth x value.
Butcher group
In mathematics,  the Butcher group, named after the New Zealand mathematician John C. Butcher by Hairer & Wanner (1974), is an infinite-dimensional  Lie group[1] first introduced in numerical analysis to study solutions of non-linear ordinary differential equations by the Runge–Kutta method. It arose from an algebraic formalism involving rooted trees that provides formal power series solutions of the differential equation modeling the flow of a vector field. It was Cayley (1857), prompted by the work of Sylvester on change of variables in differential calculus, who first noted that the derivatives of a composition of functions can be conveniently expressed in terms of rooted trees and their combinatorics.Connes & Kreimer (1999) pointed out that the Butcher group is the group of characters of the Hopf algebra of rooted trees that had arisen independently in their own work on renormalization in quantum field theory and Connes' work with Moscovici on local index theorems. This Hopf algebra, often called the Connes-Kreimer algebra, is essentially equivalent to the Butcher group, since its dual can be identified with the universal enveloping algebra of the Lie algebra of the Butcher group.[2] As they commented:A rooted tree is a graph with a distinguished node, called the root, in which every other node is connected to the root by a unique path.  If the root of a tree t is removed and the nodes connected to the original node by a single bond are taken as new roots, the tree t breaks up into rooted trees t1, t2, ... Reversing this process a new tree t = [t1, t2, ...] can be constructed by joining the roots of the trees to a new common root. The number of nodes in a tree is denoted by |t|. A heap-ordering of a rooted tree t is an allocation of the numbers 1 through |t| to the nodes so that the numbers increase on any path going away from the root. Two heap orderings are equivalent, if there is an automorphism of rooted trees mapping one of them on the other. The number of equivalence classes of heap-orderings on a particular tree is denoted by α(t) and can be computed using the Butcher's formula:[3][4]where St denotes the symmetry group of t and the tree factorial is defined recursively bywith the tree factorial of an isolated root defined to be 1The ordinary differential equation for the flow of a vector field on an open subset U of RN can be writtenwhere x(s) takes values in U, f is a smooth function from U to  RN and x0 is the starting point of the flow at time s = 0.Cayley (1857) gave a method to compute the higher order derivatives x(m)(s) in terms of rooted trees. His formula can be conveniently expressed using the elementary differentials introduced by Butcher. These are defined inductively byWith this notationgiving the power series expansionAs an example when N = 1, so that x and f are real-valued functions of a single real variable, the formula yieldswhere the four terms correspond to the four rooted trees from left to right in Figure 3 above.In a single variable this formula is the same as Faà di Bruno's formula of 1855; however in several variables it has to be written more carefully in the formwhere the tree structure is crucial.The Hopf algebra H of rooted trees was defined by Connes & Kreimer (1998) in connection with Kreimer's previous work on renormalization in quantum field theory. It was later discovered that the Hopf algebra was the dual of a Hopf algebra defined earlier by Grossman & Larsen (1989) in a different context. The characters of H, i.e. the homomorphisms of the underlying commutative algebra into R, form a group, called the Butcher group. It corresponds to the formal group structure discovered in numerical analysis by Butcher (1972).The Hopf algebra of rooted trees H is defined to be the polynomial ring in the variables t, where t runs through rooted trees.The Butcher group is defined to be the set of algebra homomorphisms φ of H into R with group structureThe inverse in the Butcher group is given byand the identity by the counit ε.Using complex coefficients in the construction of the Hopf algebra of rooted trees one obtains the complex Hopf algebra of rooted trees.Its C-valued characters form a group, called the complex Butcher group GC. The complex Butcher group GC is an infinite-dimensional complex Lie group[1] which appears as a toy model in the § Renormalization of quantum field theories.The non-linear ordinary differential equationcan be solved approximately by the Runge-Kutta method. This iterative scheme requires an m x m matrixand a vectorwith m components.The scheme defines vectors xn by first finding a solution X1, ... , Xm ofand then settingButcher (1963) showed that the solution of the corresponding ordinary differential equationshas the power series expansionwhere φj and φ are determined recursively byandThe power series above are called B-series or Butcher series.[3][5] The corresponding assignment φ is an element of the Butcher group. The homomorphism corresponding to the actual flow hasHairer & Wanner (1974) proved that the Butcher group acts naturally on the functions f. Indeed, settingthey proved thatthe formal tangent space of G at the identity ε. This forms a Lie algebra with Lie bracketfor each rooted tree t.Connes & Kreimer (1998) provided a general context for using Hopf algebraic methods to give a simple mathematical formulation of renormalization in quantum field theory. Renormalization was interpreted as Birkhoff factorization of loops in the character group of the associated Hopf algebra.  The models considered by Kreimer (1999) had Hopf algebra H and character group G, the Butcher group. Brouder (2000) has given an account of this renormalization process in terms of Runge-Kutta data.In this simplified setting, a renormalizable model has two pieces of input data:[6]Note that R satisfies the Rota-Baxter identity if and only if id –  R does. An important example is the minimal subtraction schemeIn addition there is a projection P of H onto the augmentation ideal ker ε given byTo define the renormalized Feynman rules, note that the antipode S satisfiesso thatFor the minimal subtraction scheme, this process can be interpreted in terms of Birkhoff factorization in the complex Butcher group. Φ can be regarded as a map γ of the unit circle into the complexification GC of G (maps into C instead of R). As such it has a Birkhoff factorizationIn example, the Feynman rules depend on additional parameter μ, a "unit of mass". Connes & Kreimer (2001) showed thatso that γμ– is independent of μ.The complex Butcher group comes with a natural one-parameter group λw of automorphisms, dual to that on Hfor w ≠ 0 in C.The loops γμ and λw · γμ have the same negative part and, for t real,defines a one-parameter subgroup of the complex Butcher group GC called the  renormalization group flow (RG).Its infinitesimal generator β is an element of the Lie algebra of GC and is defined byIt is called the beta-function of the model.In any given model, there is usually a finite-dimensional space of complex coupling constants. The complex Butcher group acts by diffeomorphims on this space. In particular the renormalization group defines a flow on the space of coupling constants, with the beta function giving the corresponding vector field.More general models in quantum field theory require rooted trees to be replaced by Feynman diagrams with vertices decorated by symbols from a finite index set. Connes and Kreimer have also defined Hopf algebras in this setting and have shown how they can be used to systematize standard computations in renormalization theory.Kreimer (2007) has given a "toy model" involving dimensional regularization for H and the algebra V. If c is a positive integer and qμ = q / μ is a dimensionless constant, Feynman rules can be defined recursively bywhere z = 1 – D/2 is the regularization parameter. These integrals can be computed explicitly in terms of the Gamma function using the formulaIn particular
Vectorization (mathematics)
In mathematics, especially in linear algebra and matrix theory, the vectorization of a matrix is a linear transformation which converts the matrix into a column vector. Specifically, the vectorization of an m × n matrix A, denoted vec(A), is the mn × 1 column vector obtained by stacking the columns of the matrix A on top of one another:The vectorization is frequently used together with the Kronecker product to express matrix multiplication as a linear transformation on matrices. In particular,There are two other useful formulations:More generally, it has been shown that vectorization is a self-adjunction in the monoidal closed structure of any category of matrices.[1]Vectorization is an algebra homomorphism from the space of n × n matrices with the Hadamard (entrywise) product to Cn2 with its Hadamard product:Vectorization is a unitary transformation from the space of n×n matrices with the Frobenius (or Hilbert–Schmidt) inner product to Cn2 :where the superscript H denotes the conjugate transpose.Bi consists of n block matrices of size m × m, stacked column-wise, and all these matrices are all-zero except for the i-th one, which is a m × m identity matrix Im.Then the vectorized version of X can be expressed as follows:Multiplication of X by ei extracts the i-th column, while multiplication by Bi puts it into the desired position in the final vector.Alternatively, the linear sum can be expressed using the Kronecker product:For a symmetric matrix A, the vector vec(A) contains more information than is strictly necessary, since the matrix is completely determined by the symmetry together with the lower triangular portion, that is, the n(n + 1)/2 entries on and below the main diagonal. For such matrices, the half-vectorization is sometimes more useful than the vectorization. The half-vectorization, vech(A), of a symmetric n × n matrix A is the n(n + 1)/2 × 1 column vector obtained by vectorizing only the lower triangular part of A:There exist unique matrices transforming the half-vectorization of a matrix to its vectorization and vice versa called, respectively, the duplication matrix and the elimination matrix.Programming languages that implement matrices may have easy means for vectorization.In Matlab/GNU Octave a matrix A can be vectorized by A(:).GNU Octave also allows vectorization and half-vectorization with vec(A) and vech(A) respectively. Julia has the vec(A) function as well.In Python NumPy arrays implement the 'flatten' method[1], while in R the desired effect can be achieved via the c() or as.vector() functions. In R, function vec() of package 'ks' allows vectorization and function vech() implemented in both packages 'ks' and 'sn' allows half-vectorization.
Monte Carlo method
Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. Their essential idea is using randomness to solve problems that might be deterministic in principle. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three problem classes:[1] optimization, numerical integration, and generating draws from a probability distribution.In physics-related problems, Monte Carlo methods are useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean-Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals  with complicated boundary conditions.  In application to systems engineering problems (space, oil exploration, aircraft design, etc.) problems, Monte Carlo–based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative "soft" methods.[2]In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is parametrized, mathematicians often use a Markov chain Monte Carlo (MCMC) sampler.[3][4][5][6] The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. That is, in the limit, the samples being generated by the MCMC method will be samples from the desired (target) distribution.[7] By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler.In other problems, the objective is generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depend on the distributions of the current random states (see McKean-Vlasov processes, nonlinear filtering equation).[8][9] In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain.[9][10]  A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and MCMC methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.Monte Carlo methods vary, but tend to follow a particular pattern:For example, consider a quadrant inscribed in a unit square. Given that the ratio of their areas is π/4, the value of π can be approximated using a Monte Carlo method:[11]In this procedure the domain of inputs is the square that circumscribes the quadrant.  We generate random inputs by scattering grains over the square then perform a computation on each input (test whether it falls within the quadrant). Aggregating the results yields our final result, the approximation of π.There are two important points:Uses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of pseudorandom number generators, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling.Before the Monte Carlo method was developed, simulations tested a previously understood deterministic problem, and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a probabilistic analog (see Simulated annealing).An early variant of the Monte Carlo method can be seen in the Buffon's needle experiment, in which π can be estimated by dropping needles on a floor made of parallel and equidistant strips. In the 1930s, Enrico Fermi first experimented with the Monte Carlo method while studying neutron diffusion, but did not publish anything on it.[12]The modern version of the Markov Chain Monte Carlo method was invented in the late 1940s by Stanislaw Ulam, while he was working on nuclear weapons projects at the Los Alamos National Laboratory. Immediately after Ulam's breakthrough, John von Neumann understood its importance and programmed the ENIAC computer to carry out Monte Carlo calculations. In 1946, physicists at Los Alamos Scientific Laboratory were investigating radiation shielding and the distance that neutrons would likely travel through various materials. Despite having most of the necessary data, such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus, and how much energy the neutron was likely to give off following a collision, the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods. Ulam had the idea of using random experiments. He recounts his inspiration as follows:Being secret, the work of von Neumann and Ulam required a code name.[14] A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name Monte Carlo, which refers to the Monte Carlo Casino in Monaco where Ulam's uncle would borrow money from relatives to gamble.[12] Using lists of "truly random" random numbers was extremely slow, but von Neumann developed a way to calculate pseudorandom numbers, using the middle-square method. Though this method has been criticized as crude, von Neumann was aware of this: he justified it as being faster than any other method at his disposal, and also noted that when it went awry it did so obviously, unlike methods that could be subtly incorrect.Monte Carlo methods were central to the simulations required for the Manhattan Project, though severely limited by the computational tools at the time. In the 1950s they were used at Los Alamos for early work relating to the development of the hydrogen bomb, and became popularized in the fields of physics, physical chemistry, and operations research. The Rand Corporation and the U.S. Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.The theory of more sophisticated mean field type particle Monte Carlo methods had certainly started by the mid-1960s, with the work of Henry P. McKean Jr. on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics.[15][16] We also quote an earlier pioneering article by Theodore E. Harris and Herman Kahn, published in 1951, using mean field genetic-type Monte Carlo methods for estimating particle transmission energies.[17] Mean field genetic type Monte Carlo methodologies are also used as heuristic natural search algorithms (a.k.a. Metaheuristic) in evolutionary computing. The origins of these mean field computational techniques can be traced to 1950 and 1954 with the work of Alan Turing on genetic type mutation-selection learning machines[18] and the articles by Nils Aall Barricelli at the Institute for Advanced Study in Princeton, New Jersey.[19][20]Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods can also be interpreted as a mean field particle Monte Carlo approximation of Feynman-Kac path integrals.[21][22][23][24][25][26][27] The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions,[28] but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984[27] In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth.[29]The use of Sequential Monte Carlo in advanced signal processing and Bayesian inference is more recent. It was in 1993, that Gordon et al., published in their seminal work[30] the first application of a Monte Carlo resampling algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. We also quote another pioneering article in this field of Genshiro Kitagawa on a related "Monte Carlo filter",[31] and the ones by Pierre Del Moral[32] and Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut[33] on particle filters published in the mid-1990s. Particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the LAAS-CNRS (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems.[34][35][36][37][38][39] These Sequential Monte Carlo methodologies can be interpreted as an acceptance-rejection sampler equipped with an interacting recycling mechanism.From 1950 to 1996, all the publications on Sequential Monte Carlo methodologies including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these particle algorithms are due to Pierre Del Moral[32][40] in 1996. Branching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons,[41][42][43] and by Dan Crisan, Pierre Del Moral and Terry Lyons.[44] Further developments in this field were developed in 2000 by P. Del Moral, A. Guionnet and L. Miclo.[22][45][46]There is no consensus on how Monte Carlo should be defined. For example, Ripley[47] defines most probabilistic modeling as stochastic simulation, with Monte Carlo being reserved for Monte Carlo integration and Monte Carlo statistical tests. Sawilowsky[48] distinguishes between a simulation, a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to obtain the statistical properties of some phenomenon (or behavior). Examples:Kalos and Whitlock[11] point out that such distinctions are not always easy to maintain. For example, the emission of radiation from atoms is a natural stochastic process. It can be simulated directly, or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods. "Indeed, the same computer code can be viewed simultaneously as a 'natural simulation' or as a solution of the equations by natural sampling."The main idea behind this method is that the results are computed based on repeated random sampling and statistical analysis. The Monte Carlo simulation is in fact random experimentations, in the case that, the results of these experiments are not well known.Monte Carlo simulations are typically characterized by a large number of unknown parameters, many of which are difficult to obtain experimentally.[49] Monte Carlo simulation methods do not always require truly random numbers to be useful (although, for some applications such as primality testing, unpredictability is vital).[50] Many of the most useful techniques use deterministic, pseudorandom sequences, making it easy to test and re-run simulations. The only quality usually necessary to make good simulations is for the pseudo-random sequence to appear "random enough" in a certain sense.What this means depends on the application, but typically they should pass a series of statistical tests. Testing that the numbers are uniformly distributed or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest, and most common ones. Weak correlations between successive samples is also often desirable/necessary.Sawilowsky lists the characteristics of a high quality Monte Carlo simulation:[48]Pseudo-random number sampling algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given probability distribution.Low-discrepancy sequences are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences. Methods based on their use are called quasi-Monte Carlo methods.In an effort to assess the impact of random number quality on Monte Carlo simulation outcomes, astrophysical researchers tested cryptographically-secure pseudorandom numbers generated via Intel's RdRand instruction set, as compared to those derived from algorithms, like the Mersenne Twister, in Monte Carlo simulations of radio flares from brown dwarfs.  RdRand is the closest pseudorandom number generator to a true random number generator.  No statistically-significant difference was found between models generated with typical pseudorandom number generators and RdRand for trials consisting of the generation of 107 random numbers.[51]There are ways of using probabilities that are definitely not Monte Carlo simulations — for example, deterministic modeling using single-point estimates. Each uncertain variable within a model is assigned a “best guess” estimate.  Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded.[52]By contrast, Monte Carlo simulations sample from a probability distribution for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring.[53] For example, a comparison of a spreadsheet cost construction model run using traditional “what if” scenarios, and then running the comparison again with Monte Carlo simulation and triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the “what if” analysis.[example  needed]  This is because the “what if” analysis gives equal weight to all scenarios (see quantifying uncertainty in corporate finance), while the Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called "rare events".Monte Carlo methods are especially useful for simulating phenomena with significant uncertainty in inputs and systems with a large number of coupled degrees of freedom. Areas of application include:Numerical analysis · SimulationFinite element · Boundary element Lattice Boltzmann · Riemann solverDissipative particle dynamicsSmoothed particle hydrodynamicsMonte Carlo methods are very important in computational physics, physical chemistry, and related applied fields, and have diverse applications from complicated quantum chromodynamics calculations to designing heat shields and aerodynamic forms as well as in modeling radiation transport for radiation dosimetry calculations.[54][55][56]  In statistical physics Monte Carlo molecular modeling is an alternative to computational molecular dynamics, and Monte Carlo methods are used to compute statistical field theories of simple particle and polymer systems.[29][57]  Quantum Monte Carlo methods solve the many-body problem for quantum systems.[8][9][21] In radiation materials science, the binary collision approximation for simulating ion implantation is usually based on a Monte Carlo approach to select the next colliding atom.[58] In experimental particle physics, Monte Carlo methods are used for designing detectors, understanding their behavior and comparing experimental data to theory. In astrophysics, they are used in such diverse manners as to model both galaxy evolution[59] and microwave radiation transmission through a rough planetary surface.[60] Monte Carlo methods are also used in the ensemble models that form the basis of modern weather forecasting.Monte Carlo methods are widely used in engineering for sensitivity analysis and quantitative probabilistic analysis in process design. The need arises from the interactive, co-linear and non-linear behavior of typical process simulations. For example,The Intergovernmental Panel on Climate Change relies on Monte Carlo methods in probability density function analysis of radiative forcing.Probability density function (PDF) of ERF due to total GHG, aerosol forcing and total anthropogenic forcing. The GHG consists of WMGHG, ozone and stratospheric water vapour. The PDFs are generated based on uncertainties provided in Table 8.6. The combination of the individual RF agents to derive total forcing over the Industrial Era are done by Monte Carlo simulations and based on the method in Boucher and Haywood (2001). PDF of the ERF from surface albedo changes and combined contrails and contrail-induced cirrus are included in the total anthropogenic forcing, but not shown as a separate PDF. We currently do not have ERF estimates for some forcing mechanisms: ozone, land use, solar, etc.[68]Monte Carlo methods are used in various fields of computational biology, for example for Bayesian inference in phylogeny, or for studying biological systems such as genomes, proteins,[69] or membranes.[70]The systems can be studied in the coarse-grained or ab initio frameworks depending on the desired accuracy. Computer simulations allow us to monitor the local environment of a particular molecule to see if some chemical reaction is happening for instance. In cases where it is not feasible to conduct a physical experiment, thought experiments can be conducted (for instance: breaking bonds, introducing impurities at specific sites, changing the local/global structure, or introducing external fields).Path tracing, occasionally referred to as Monte Carlo ray tracing, renders a 3D scene by randomly tracing samples of possible light paths. Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the rendering equation, making it one of the most physically accurate 3D graphics rendering methods in existence.The standards for Monte Carlo experiments in statistics were set by Sawilowsky.[71][72] In applied statistics, Monte Carlo methods are generally used for three purposes:Monte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate randomization test is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice—or more frequently—for the efficiency of not having to track which permutations have already been selected).Monte Carlo methods have been developed into a technique called Monte-Carlo tree search that is useful for searching for the best move in a game.  Possible moves are organized in a search tree and a large number of random simulations are used to estimate the long-term potential of each move. A black box simulator represents the opponent's moves.[74]The Monte Carlo tree search (MCTS) method has four steps:[75]The net effect, over the course of many simulated games, is that the value of a node representing a move will go up or down, hopefully corresponding to whether or not that node represents a good move.Monte Carlo Tree Search has been used successfully to play games such as Go,[76] Tantrix,[77] Battleship,[78] Havannah,[79] and Arimaa.[80]Monte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport, and thus these methods have been used in global illumination computations that produce photo-realistic images of virtual 3D models, with applications in video games, architecture, design, computer generated films, and cinematic special effects.[81]The US Coast Guard utilizes Monte Carlo methods within its computer modeling software SAROPS in order to calculate the probable locations of vessels during search and rescue operations. Each simulation can generate as many as ten thousand data points that are randomly distributed based upon provided variables.[82] Search patterns are then generated based upon extrapolations of these data in order to optimize the probability of containment (POC) and the probability of detection (POD), which together will equal an overall probability of success (POS). Ultimately this serves as a practical application of probability distribution in order to provide the swiftest and most expedient method of rescue, saving both lives and resources.[83]Monte Carlo simulation is commonly used to evaluate the risk and uncertainty that would affect the outcome of different decision options. Monte Carlo simulation allows the business risk analyst to incorporate the total effects of uncertainty in variables like sales volume, commodity and labour prices, interest and exchange rates, as well as the effect of distinct risk events like the cancellation of a contract or the change of a tax law.Monte Carlo methods in finance are often used to evaluate investments in projects at a business unit or corporate level, or to evaluate financial derivatives. They can be used to model project schedules, where simulations aggregate estimates for worst-case, best-case, and most likely durations for each task to determine outcomes for the overall project. Monte Carlo methods are also used in option pricing, default risk analysis.[84][85][86]A Monte Carlo approach was used for evaluating the potential value of a proposed program to help female petitioners in Wisconsin be successful in their applications for harassment and domestic abuse restraining orders.  It was proposed to help women succeed in their petitions by providing them with greater advocacy thereby potentially reducing the risk of rape and physical assault.  However, there were many variables in play that could not be estimated perfectly, including the effectiveness of restraining orders, the success rate of petitioners both with and without advocacy, and many others.  The study ran trials that varied these variables to come up with an overall estimate of the success level of the proposed program as a whole.[87]In general, the Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers (see also Random number generation) and observing that fraction of the numbers that obeys some property or properties. The method is useful for obtaining numerical solutions to problems too complicated to solve analytically.  The most common application of the Monte Carlo method is Monte Carlo integration.Deterministic numerical integration algorithms work well in a small number of dimensions, but encounter two problems when the functions have many variables. First, the number of function evaluations needed increases rapidly with the number of dimensions. For example, if 10 evaluations provide adequate accuracy in one dimension, then 10100 points are needed for 100 dimensions—far too many to be computed. This is called the curse of dimensionality. Second, the boundary of a multidimensional region may be very complicated, so it may not be feasible to reduce the problem to an iterated integral.[88] 100 dimensions is by no means unusual, since in many physical problems, a "dimension" is equivalent to a degree of freedom.A refinement of this method, known as importance sampling in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as stratified sampling, recursive stratified sampling, adaptive umbrella sampling[89][90] or the VEGAS algorithm.A similar approach, the quasi-Monte Carlo method, uses low-discrepancy sequences. These sequences "fill" the area better and sample the most important points more frequently, so quasi-Monte Carlo methods can often converge on the integral more quickly.Another class of methods for sampling points in a volume is to simulate random walks over it (Markov chain Monte Carlo). Such methods include the Metropolis-Hastings algorithm, Gibbs sampling, Wang and Landau algorithm, and interacting type MCMC methodologies such as the sequential Monte Carlo samplers.[91]Another powerful and very popular application for random numbers in numerical simulation is in numerical optimization. The problem is to minimize (or maximize) functions of some vector that often has a large number of dimensions. Many problems can be phrased in this way: for example, a computer chess program could be seen as trying to find the set of, say, 10 moves that produces the best evaluation function at the end. In the traveling salesman problem the goal is to minimize distance traveled. There are also applications to engineering design, such as multidisciplinary design optimization. It has been applied with quasi-one-dimensional models to solve particle dynamics problems by efficiently exploring large configuration space. Reference [92] is a comprehensive review of many issues related to simulation and optimization.The traveling salesman problem is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines prior information with new information obtained by measuring some observable parameters (data).As, in the general case, the theory linking data with model parameters is nonlinear, the posterior probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.).When analyzing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available.The best-known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.[93][94]
Basis (linear algebra)
In mathematics, a set B of elements (vectors) in a vector space V is called a basis, if every element of V may be written in a unique way as a (finite) linear combination of elements of B. The coefficients of this linear combination are referred to as components or coordinates on B of the vector. The elements of a basis are called basis vectors.Equivalently B is a basis if its elements are linearly independent and every element of V is a linear combination of elements of B.[1]  In more general terms, a basis is a linearly independent spanning set.A vector space can have generally several bases; however all the bases have the same number of elements, called the dimension of the vector space.A basis B of a vector space V over a field F (such as the real numbers R or the complex numbers C) is a linearly independent subset of V that spans V.This means that, a subset B of V is a basis if it satisfies the two following conditions:The scalars vi are called the coordinates of the vector v with respect to the basis B, and by the first property they are uniquely determined.A vector space that has a finite basis is called finite-dimensional. In this case, the subset {b1, ..., bn} that is considered (twice) in the above definition may be chosen as B itself.It is often convenient to ordering the basis vectors, typically when one consider the coefficients of a vector on a basis, without referring explicitly to the basis elements. In this case, the ordering is necessary for associating each coefficient to the corresponding basis element. Generally, this ordering is implicitly done by numbering the basis elements. For example, when using matrices, the ith row, and ith column refer to the ith element of a basis of some vector space. For emphasizing that an order has been chosen, one speaks of an ordered basis, which is therefore a sequence rather than a set; see Ordered bases and coordinates below.Many properties of finite bases result from the Steinitz exchange lemma, which states that, given a finite spanning set S and a linearly independent subset L of n elements of S, one may replace n well chosen elements of S by the elements of L for getting a spanning set containing L, having its other elements in S, and having the same number of elements as S.Most properties resulting from Steinitz exchange lemma remain true when there is no finite spanning set, but their proof in the infinite case requires generally the axiom of choice or a weaker form of it, such as the ultrafilter lemma.If V is a vector space over a field F, then:If V is a vector space of dimension n, then:Let V be a vector space of finite dimension n over a field F, and be a basis of V. By definition of a basis, for every v in V may be written, in a unique way,Typically, the new basis vectors are given by their coordinates over the old basis, that is, The formula for changing the coordinates with respect to the other basis results from the uniqueness of the decomposition of a vector over a basis, and is thusfor i = 1, ..., n.be the column vectors of the coordinates of v in the old and the new basis respectively, then the formula for changing coordinates isIf one replaces the field occurring in the definition of a vector space by a ring, one gets the definition of a module. For modules, linear independence and spanning sets are defined exactly as for vector spaces, although "generating set" is more commonly used than that of "spanning set".Like for vector spaces, a basis of a module is a linearly independent subset that is also a generating set. A major difference with the theory of vector spaces is that not every module has a basis. A module that has a basis is called a free module. Free modules play a fundamental role in module theory, as they may be used for describing the structure of non-free modules through free resolutions.The common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces – a large class of vector spaces including e.g. Hilbert spaces, Banach spaces, or Fréchet spaces.In the study of Fourier series, one learns that the functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2π] that are square-integrable on this interval, i.e., functions f satisfyingThe functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are linearly independent, and every function f that is square-integrable on [0, 2π] is an "infinite linear combination" of them, in the sense thatfor suitable (real or complex) coefficients ak, bk.  But many[2] square-integrable functions cannot be represented as finite linear combinations of these basis functions, which therefore do not comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.For a probability distribution in Rn with a probability density function, such as the equidistribution  in a n-dimensional ball with respect to Lebesgue measure, it can be shown that n randomly and independently chosen vectors will form a basis with probability one, which is due to the fact that n linearly dependent vectors x1, ..., xn in Rn should satisfy the equation det[x1, ..., xn] = 0 (zero determinant of the matrix with columns xi), and the set of zeros of a non-trivial polynomial has zero measure. This observation has led to techniques for approximating random bases.[5][6]In high dimensions, two independent random vectors are with high probability almost orthogonal, and the number of independent random vectors, which all are with given high probability pairwise almost orthogonal, grows exponentially with dimension. More precisely, consider equidistribution in n-dimensional ball. Choose N independent random vectors from a ball (they are independent and identically distributed). Let θ be a small positive number. Then for    (Eq. 1)The figure (right) illustrates distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the n-dimensional cube [−1, 1]n as a function of dimension, n. A point is first randomly selected in the cube. The second point is randomly chosen in the same cube. If the angle between the vectors was within π/2 ± 0.037π/2 then the vector was retained. At the next step a new vector is generated in the same hypercube, and its angles with the previously generated vectors are evaluated. If these angles are within π/2 ± 0.037π/2 then the vector is retained. The process is repeated until the chain of almost orthogonality breaks, and the number of such pairwise almost orthogonal vectors (length of the chain) is recorded. For each n, 20 pairwise almost orthogonal chains where constructed numerically for each dimension. Distribution of the length of these chains is presented.Let V be any vector space over some field F.Let X be the set of all linearly independent subsets of V.The set X is nonempty since the empty set is an independent subset of V,and it is partially ordered by inclusion, which is denoted, as usual, by ⊆.Let Y be a subset of X that is totally ordered by ⊆,and let LY be the union of all the elements of Y (which are themselves certain subsets of V).Since (Y, ⊆) is totally ordered, every finite subset of LY is a subset of an element of Y,which is a linearly independent subset of V,and hence every finite subset of LY is linearly independent.Thus LY is linearly independent, so LY is an element of X.Therefore, LY is an upper bound for Y in (X, ⊆):it is an element of X, that contains every element Y.As X is nonempty, and every totally ordered subset of (X, ⊆) has an upper bound in X, Zorn's lemma asserts that X has a maximal element. In other words, there exists some element Lmax of X satisfying the condition that whenever Lmax ⊆ L for some element L of X, then L = Lmax.It remains to prove that Lmax is a basis of V.  Since Lmax belongs to X, we already know that Lmax is a linearly independent subset of V.If Lmax would not span V, there would exist some vector w of V that cannot be expressed as a linear combination of elements of Lmax (with coefficients in the field F). In particular, w cannot be an element of Lmax.Let Lw  =  Lmax ∪ {w}. This set is an element of X, that is, it is a linearly independent subset of V (because w is not in the span of Lmax, and Lmax is independent). As Lmax ⊆ Lw, and Lmax ≠ Lw (because Lw contains the vector w that is not contained in Lmax), this contradicts the maximality of Lmax. Thus this shows that Lmax spans V.Hence Lmax is linearly independent and spans V. It is thus a basis of V, and this proves that every vector space has a basis.This proof relies on Zorn's lemma, which is equivalent to the axiom of choice. Conversely, it may be proved that if every vector space has a basis, then the axiom of choice is true; thus the two assertions are equivalent.
Finite field
In mathematics, a finite field or Galois field (so-named in honor of Évariste Galois) is a field that contains a finite number of elements.  As with any field, a finite field is a set on which the operations of multiplication, addition, subtraction and division are defined and satisfy certain basic rules.  The most common examples of finite fields are given by the integers mod p when p is a prime number.Finite fields are fundamental in a number of areas of mathematics and computer science, including number theory, algebraic geometry, Galois theory, finite geometry, cryptography and coding theory.The number of elements of a finite field is called its order. A finite field of order q exists if and only if the order q is a prime power pk (where p is a prime number and k is a positive integer). All finite fields of a given order are isomorphic.[1] In a field of order pk, adding p copies of any element always results in zero; that is, the characteristic of the field is p. In a finite field of order q, the polynomial Xq − X has all q elements of the finite field as roots. The non-zero elements of a finite field form a multiplicative group. This group is cyclic, so all non-zero elements can be expressed as powers of a single element called a primitive element of the field.  (In general there will be several primitive elements for a given field.)The elements of the prime field of order p may be represented by integers in the range 0, ..., p − 1. The sum, the difference and the product are computed by taking the remainder by p of the integer result. The multiplicative inverse of an element may be computed by using the extended Euclidean algorithm (see Extended Euclidean algorithm § Modular integers).Let F be a finite field. For any element x in F and any integer n, denote by n ⋅ x the sum of n copies of x. The least positive n such that n ⋅ 1 = 0 exists and is a prime number; it is called the characteristic of the field.The identity(sometimes called the freshman's dream) is true in a field of characteristic p for every x and y.  This follows from the binomial theorem, as each binomial coefficient of the expansion of (x + y)p, except the first and the last, is a multiple of p.By Fermat's little theorem, if p is a prime number and x is in the field GF(p) then xp = x.  This implies the equalityfor polynomials over GF(p).  More generally, every element in GF(pn) satisfies the polynomial equation xpn − x = 0.Any finite field extension of a finite field is separable and simple.  That is, if E is a finite field and F is a subfield of E, then E is obtained from F by adjoining a single element whose minimal polynomial is separable. To use a jargon, finite fields are perfect.A more general algebraic structure that satisfies all the other axioms of a field, but whose multiplication is not required to be commutative, is called a division ring (or sometimes skew field). By Wedderburn's little theorem, any finite division ring is commutative, and hence is a finite field.Let q = pn be a prime power, and F be the splitting field of the polynomial over the prime field GF(p). This means that F is a finite field of lowest order, in which P has q distinct roots (the roots are distinct, as the formal derivative of P is equal to −1). The above identity shows that the sum and the product of two roots of P are roots of P, as well as the multiplicative inverse of a root of P. In other word, the roots of P form a field of order q, which is equal to F by the minimality of the splitting field.The uniqueness up to isomorphism of splitting fields implies thus that all fields of order q are isomorphic.In summary, we have the following classification theorem first proved in 1893 by E. H. Moore:[1]It follows that GF(pn) contains a subfield isomorphic to GF(pm) if and only if m is a divisor of n; in that case, this subfield is unique. In fact, the polynomial Xpm − X divides Xpn − X if and only if m is a divisor of n.Given a prime power q = pn with p prime and n > 1, the field GF(q) may be explicitly constructed in the following way. One chooses first an irreducible polynomial P in GF(p)[X] of degree n (such an irreducible polynomial always exists). Then the quotient ring of the polynomial ring GF(p)[X] by the ideal generated by P is a field of order q.More explicitly, the elements of GF(q) are the polynomials over GF(p) whose degree is strictly less than n. The addition and the subtraction are those of polynomials over GF(p). The product of two elements is the remainder of the Euclidean division by P of the product in GF(p)[X].The multiplicative inverse of a non-zero element may be computed with the extended Euclidean algorithm; see Extended Euclidean algorithm § Simple algebraic field extensions.Except in the construction of GF(4), there are several possible choices for P, which produce isomorphic results. To simplify the Euclidean division, for P one commonly chooses polynomials of the form which make the needed Euclidean divisions very efficient. However, for some fields, typically in characteristic 2, irreducible polynomials of the form Xn + aX + b may not exist. In characteristic 2, if the polynomial Xn + X + 1 is reducible, it is recommended to choose Xn + Xk + 1 with the lowest possible k that makes the polynomial irreducible. If all these trinomials are reducible, one chooses "pentanomials" Xn + Xa +  Xb +  Xc +  1, as polynomials of degree greater than 1, with an even number of terms, are never irreducible in characteristic 2, having 1 as a root.[3]In the next sections, we will show how this general construction method works for small finite fields.Over GF(2), there is only one irreducible polynomial of degree 2:Therefore, for GF(4) the construction of the preceding section must involve this polynomial, andIf one denotes a a root of this polynomial in GF(4), the tables of the operations in GF(4) are the following. There is no table for subtraction, because subtraction is identical to addition, as is the case for every field of characteristic 2. In the third table, for the division of x by y, x must be read on the left, and y on the top.For applying the above general construction of finite fields in the case of GF(p2), one has to find an irreducible polynomial of degree 2. For p = 2, this has been done in the preceding section. If p is an odd prime, there are always irreducible polynomials of the form X2 − r, with r in GF(p).Having chosen a quadratic non-residue r, let α be a symbolic square root of r, that is a symbol which has the property α2 = r, in the same way as the complex number i is a symbolic square root of −1. Then, the elements of GF(p2) are all the linear expressionswith a and b in GF(p). The operations on GF(p2) are defined as follows (the operations between elements of GF(p) represented by Latin letters are the operations in GF(p)):The polynomialis irreducible over GF(2) and GF(3), that is, it is irreducible modulo 2 and 3 (to show this it suffices to show that it has no root in GF(2) nor in GF(3)). It follows that the elements of GF(8) and GF(27) may be represented by expressionsThe addition, additive inverse and multiplication on GF(8) and GF(27) may thus be defined as follows; in following formulas, the operations between elements of GF(2) or GF(3), represented by Latin letters, are the operations in GF(2) or GF(3), respectively:The polynomialis irreducible over GF(2), that is, it is irreducible modulo 2. It follows that the elements of GF(16) may be represented by expressionswhere a, b, c, d are either 0 or 1 (elements of GF(2)), and α is a symbol such that As the characteristic of GF(2) is 2, each element is its additive inverse in GF(16).The addition and multiplication on GF(16) may be defined as follows; in following formulas, the operations between elements of GF(2), represented by Latin letters are the operations in GF(2).The set of non-zero elements in GF(q) is an abelian group under the multiplication, of order q – 1. By Lagrange's theorem, there exists a divisor k of q – 1 such that xk = 1 for every non-zero x in GF(q). As the equation Xk = 1 has at most k solutions in any field, q – 1 is the lowest possible value for k.The structure theorem of finite abelian groups implies that this multiplicative group is cyclic, that is, all non-zero elements are powers of a single element. In summary:Such an element a is called a primitive element. Unless q = 2, 3, the primitive element is not unique. The number of primitive elements is φ(q − 1) where φ is Euler's totient function.The result above implies that xq = x for every x in GF(q). The particular case where q is prime is Fermat's little theorem.If a is a primitive element in GF(q), then for any non-zero element x in F, there is a unique integer n with 0 ≤ n ≤ q − 2 such thatThis integer n is called the discrete logarithm of x to the base a.While an can be computed very quickly, for example using exponentiation by squaring, there is no known efficient algorithm for computing the inverse operation, the discrete logarithm. This has been used in various cryptographic protocols, see Discrete logarithm for details.When the nonzero elements of GF(q) are represented by their discrete logarithms, multiplication and division are easy, as they reduce to addition and subtraction modulo q – 1. However, addition amounts to computing the discrete logarithm of am + an. The identity allows one to solve this problem by constructing the table of the discrete logarithms of an + 1, called Zech's logarithms, for n = 0, ..., q − 2 (it is convenient to define the discrete logarithm of zero as being −∞).Zech's logarithms are useful for large computations, such as linear algebra over medium-sized fields, that is, fields that are sufficiently large for making natural algorithms inefficient, but not too large, as one has to pre-compute a table of the same size as the order of the field.Every nonzero element of a finite field is a root of unity, as xq−1 = 1 for every nonzero element of GF(q).If n is a positive integer, an nth primitive root of unity is a solution of the equation xn = 1 that is not a solution of the equation xm = 1 for any positive integer m < n. If a is a nth primitive root of unity in a field F, then F contains all the n roots of unity, which are 1, a, a2, ..., an−1.The field GF(q) contains a nth primitive root of unity if and only if n is a divisor of q − 1; if n is a divisor of q − 1, then the number of primitive nth roots of unity in GF(q) is φ(n) (Euler's totient function). The number of nth roots of unity in GF(q) is gcd(n, q − 1).In a field of characteristic p, every (np)th root of unity is also a nth root of unity. It follows that primitive (np)th roots of unity never exist in a field of characteristic p.The field GF(64) has several interesting properties that smaller fields do not share: it has two subfields such that neither is contained in the other; not all generators (elements with minimal polynomial of degree 6 over GF(2)) are primitive elements; and the primitive elements are not all conjugate under the Galois group.The order of this field being 26, and the divisors of 6 being 1, 2, 3, 6, the subfields of GF(64) are GF(2), GF(22) = GF(4), GF(23) = GF(8), and GF(64) itself. As 2 and 3 are coprime, the intersection of GF(4) and GF(8) in GF(64) is the prime field GF(2).The union of GF(4) and GF(8) has thus 10 elements. The remaining 54 elements of GF(64) generate GF(64) in the sense that no other subfield contains any of them. It follows that they are roots of irreducible polynomials of degree 6 over GF(2). This implies that, over GF(2), there are exactly 9 = 54/6 irreducible monic polynomials of degree 6. This may be verified by factoring X64 − X over GF(2).The elements of GF(64) are primitive nth roots of unity for some n dividing 63. As the 3rd and the 7th roots of unity belong to GF(4) and GF(8), respectively, the 54 generators are primitive nth roots of unity for some n in {9, 21, 63}. Euler's totient function shows that there are 6 primitive 9th roots of unity, 12 primitive 21st roots of unity, and 36 primitive 63rd roots of unity. Summing these numbers, one finds again 54 elements.By factoring the cyclotomic polynomials over GF(2), one finds that:This shows that the best choice to construct GF(64) is to define it as GF(2)[X]/(X6 + X + 1). In fact, this generator is a primitive element, and this polynomial is the irreducible polynomial that produces the easiest Euclidean division.In this section, p is a prime number, and q = pn is a power of p.In GF(q), the identity (x + y)p = xp + yp implies that the mapis a GF(p)-linear endomorphism and a field automorphism of GF(q), which fixes every element of the subfield GF(p). It is called the Frobenius automorphism, after Ferdinand Georg Frobenius.Denoting by φk the composition of φ with itself k times, we have It has been shown in the preceding section that φn is the identity. For 0 < k < n, the automorphism φk is not the identity, as, otherwise, the polynomial would have more than pk roots.There are no other GF(p)-automorphisms of GF(q). In other words, GF(pn) has exactly n GF(p)-automorphisms, which are In terms of Galois theory, this means that GF(pn) is a Galois extension of GF(p), which has a cyclic Galois group.The fact that the Frobenius map is surjective implies that every finite field is perfect.If F is a finite field, a non-constant monic polynomial with coefficients in F is irreducible over F, if it is not the product of two non-constant monic polynomials, with coefficients in F.As every polynomial ring over a field is a unique factorization domain, every monic polynomial over a finite field may be factored in a unique way (up to the order of the factors) into a product of irreducible monic polynomials.There are efficient algorithms for testing polynomial irreducibility and factoring polynomials over finite field. They are a key step for factoring polynomials over the integers or the rational numbers. At least for this reason, every computer algebra system has functions for factoring polynomials over finite fields, or, at least, over finite prime fields.The polynomialfactors into linear factors over a field of order q. More precisely, this polynomial is the product of all monic polynomials of degree one over a field of order q.This implies that, if q = pn then Xq − X is the product of all monic irreducible polynomials over GF(p), whose degree divides n. In fact, if P is an irreducible factor over GF(p) of Xq − X, its degree divides n, as its splitting field is contained in GF(pn). Conversely, if P is an irreducible monic polynomial over GF(p) of degree d dividing n, it defines a field extension of degree d, which is contained in GF(pn), and all roots of P belong to GF(pn), and are roots of Xq − X; thus P divides Xq − X. As Xq − X does not have any multiple factor, it is thus the product of all the irreducible monic polynomials that divide it.This property is used to compute the product of the irreducible factors of each degree of polynomials over GF(p); see Distinct degree factorization.The number N(q, n) of monic irreducible polynomials of degree n over GF(q) is given by[4]where μ is the Möbius function. This formula is almost a direct consequence of above property of Xq − X.By the above formula, the number of irreducible (not necessarily monic) polynomials of degree n over GF(q) is (q − 1)N(q, n).A (slightly simpler) lower bound for N(q, n) isOne may easily deduce that, for every q and every n, there is at least one irreducible polynomial of degree n over GF(q). This lower bound is sharp for q = n = 2.In cryptography, the difficulty of the discrete logarithm problem in finite fields or in elliptic curves is the basis of several widely used protocols, such as the Diffie–Hellman protocol. For example, in 2014 a secure internet connection to Wikipedia involved the elliptic curve Diffie–Hellman protocol (ECDHE) over a large finite field.[5] In coding theory, many codes are constructed as subspaces of vector spaces over finite fields.Finite fields are widely used in number theory, as many problems over the integers may be solved by reducing them modulo one or several prime numbers. For example, the fastest known algorithms for polynomial factorization and linear algebra over the field of rational numbers proceed by reduction modulo one or several primes, and then reconstruction of the solution by using Chinese remainder theorem, Hensel lifting or the LLL algorithm.Similarly many theoretical problems in number theory can be solved by considering their reductions modulo some or all prime numbers. See, for example, Hasse principle. Many recent developments of algebraic geometry were motivated by the need to enlarge the power of these modular methods. Wiles' proof of Fermat's Last Theorem is an example of a deep result involving many mathematical tools, including finite fields.A finite field F is not algebraically closed. To demonstrate this, consider the polynomialwhich has no roots in F, since f (α) = 1 for all α in F.The direct limit of the system:If we actually construct our finite fields in such a fashion that Fpn is contained in Fpm whenever n divides m, then this direct limit can be constructed as the union of all these fields. Even if we do not construct our fields this way, we can still speak of the algebraic closure, but some more delicacy is required in its construction.A division ring is a generalization of field. Division rings are not assumed to be commutative. There are no non-commutative finite division rings: Wedderburn's little theorem states that all finite division rings are commutative, hence finite fields. The result holds even if we relax associativity and consider alternative rings, by the Artin–Zorn theorem.[6]Finite fields appear in the following chain of inclusions:
Norm (mathematics)
In linear algebra, functional analysis, and related areas of mathematics, a norm is a function that assigns a strictly positive length or size to each vector in a vector space—except for the zero vector, which is assigned a length of zero. A seminorm, on the other hand, is allowed to assign zero length to some non-zero vectors (in addition to the zero vector).A norm must also satisfy certain properties pertaining to scalability and additivity which are given in the formal definition below.A simple example is two dimensional Euclidean space R2 equipped with the "Euclidean norm" (see below). Elements in this vector space (e.g., (3, 7)) are usually drawn as arrows in a 2-dimensional cartesian coordinate system starting at the origin (0, 0). The Euclidean norm assigns to each vector the length of its arrow. Because of this, the Euclidean norm is often known as the magnitude.A vector space on which a norm is defined is called a normed vector space. Similarly, a vector space with a seminorm is called a seminormed vector space. It is often possible to supply a norm for a given vector space in more than one way.Given a vector space V over a subfield F of the complex numbers, a norm on V is a nonnegative-valued scalar function p: V → [0,+∞) with the following properties:[1]For all a ∈ F and all u, v ∈ V,A seminorm on V is a function p : V → R with the properties 1 and 2 above.Every vector space V with seminorm p induces a normed space V/W, called the quotient space, where W is the subspace of V consisting of all vectors v in V with p(v) = 0. The induced norm on V/W is defined by:Two norms (or seminorms) p and q on a vector space V are equivalent if there exist two real constants c and C, with c > 0, such thatA topological vector space is called normable (seminormable) if the topology of the space can be induced by a norm (seminorm).If a norm p : V → R is given on a vector space V then the norm of a vector v ∈ V is usually denoted by enclosing it within double vertical lines: ‖v‖ = p(v). Such notation is also sometimes used if p is only a seminorm.For the length of a vector in Euclidean space (which is an example of a norm, as explained below), the notation |v| with single vertical lines is also widespread.In Unicode, the code point of the "double vertical line" character ‖ is U+2016. The double vertical line should not be confused with the "parallel to" symbol, Unicode U+2225 ( ∥ ). This is usually not a problem because the former is used in parenthesis-like fashion, whereas the latter is used as an infix operator. The double vertical line used here should also not be confused with the symbol used to denote lateral clicks in linguistics, Unicode U+01C1 ( ǁ ). The single vertical line | is called "vertical line" in Unicode and its code point is U+007C.In LaTeX and related markup languages, the macros '\|' and '\parallel' are often used to denote a norm.The absolute valueis a norm on the one-dimensional vector spaces formed by the real or complex numbers.The absolute value norm is a special case of the L1 norm.On an n-dimensional Euclidean space Rn, the intuitive notion of length of the vector x = (x1, x2, ..., xn) is captured by the formulaThis gives the ordinary distance from the origin to the point X, a consequence of the Pythagorean theorem.  This operation may also be referred to as "SRSS" which is an acronym for the square root of the sum of squares.[2]The Euclidean norm is by far the most commonly used norm on Rn, but there are other norms on this vector space as will be shown below. However, all these norms are equivalent in the sense that they all define the same topology.On an n-dimensional complex space Cn the most common norm isIn both cases the norm can be expressed as the square root of the inner product of the vector and itself:where x is represented as a column vector ([x1; x2; ...; xn]), and x∗ denotes its conjugate transpose.This formula is valid for any inner product space, including Euclidean and complex spaces. For Euclidean spaces, the inner product is equivalent to the dot product. Hence, in this specific case the formula can be also written with the following notation:The Euclidean norm is also called the Euclidean length, L2 distance, ℓ2 distance, L2 norm, or ℓ2 norm; see Lp space.The set of vectors in Rn+1 whose Euclidean norm is a given positive constant forms an n-sphere.The name relates to the distance a taxi has to drive in a rectangular street grid to get from the origin to the point x.The 1-norm is simply the sum of the absolute values of the columns.In contrast,is not a norm because it may yield negative results.The p-norm is related to the generalized mean or power mean.This definition is still of some interest for 0 < p < 1, but the resulting function does not define a norm,[3] because it violates the triangle inequality. What is true for this case of 0 < p < 1, even in the measurable analog, is that the corresponding Lp class is a vector space, and it is also true that the function(without pth root) defines a distance that makes Lp(X) into a complete metric topological vector space. These spaces are of great interest in functional analysis, probability theory, and harmonic analysis.However, outside trivial cases, this topological vector space is not locally convex and has no continuous nonzero linear forms. Thus the topological dual space contains only the zero functional.The partial derivative of the p-norm is given byThe derivative with respect to x, therefore, isFor the special case of p = 2, this becomesorThe set of vectors whose infinity norm is a given constant, c, forms the surface of a hypercube with edge length 2c.In metric geometry, the discrete metric takes the value one for distinct points and zero otherwise. When applied coordinate-wise to the elements of a vector space, the discrete distance defines the Hamming distance, which is important in coding and information theory. In the field of real or complex numbers, the distance of the discrete metric from zero is not homogeneous in the non-zero point; indeed, the distance from zero remains one as its non-zero argument approaches zero. However, the discrete distance of a number from zero does satisfy the other properties of a norm, namely the triangle inequality and positive definiteness. When applied component-wise to vectors, the discrete distance from zero behaves like a non-homogeneous "norm", which counts the number of non-zero components in its vector argument; again, this non-homogeneous "norm" is discontinuous.In signal processing and statistics, David Donoho referred to the zero "norm" with quotation marks. Following Donoho's notation, the zero "norm" of x is simply the number of non-zero coordinates of x, or the Hamming distance of the vector from zero. When this "norm" is localized to a bounded set, it is the limit of p-norms as p approaches 0. Of course, the zero "norm" is not truly a norm, because it is not positive homogeneous. Indeed, it is not even an F-norm in the sense described above, since it is discontinuous, jointly and severally, with respect to the scalar argument in scalar–vector multiplication and with respect to its vector argument. Abusing terminology, some engineers[who?] omit Donoho's quotation marks and inappropriately call the number-of-nonzeros function the L0 norm, echoing the notation for the Lebesgue space of measurable functions.Other norms on Rn can be constructed by combining the above; for exampleis a norm on R4.For any norm and any injective linear transformation A we can define a new norm of x, equal toIn 2D, with A a rotation by 45° and a suitable scaling, this changes the taxicab norm into the maximum norm. In 2D, each A applied to the taxicab norm, up to inversion and interchanging of axes, gives a different unit ball: a parallelogram of a particular shape, size and orientation. In 3D this is similar but different for the 1-norm (octahedrons) and the maximum norm (prisms with parallelogram base).There are examples of norms that are not defined by "entrywise" formulas. For instance, the Minkowski functional of a centrally-symmetric convex body in Rn (centered at zero) defines a norm on Rn.All the above formulas also yield norms on Cn without modification.There are also norms on spaces of matrices (with real or complex entries), the so-called matrix norms.The generalization of the above norms to an infinite number of components leads to ℓ p and L p spaces, with normsOther examples of infinite dimensional normed vector spaces can be found in the Banach space article.Two norms ‖•‖α and ‖•‖β on a vector space V are called equivalent if there exist positive real numbers C and D such that for all x in VIn particular,i.e., If the vector space is a finite-dimensional real or complex one, all norms are equivalent. On the other hand, in the case of infinite-dimensional vector spaces, not all norms are equivalent.Equivalent norms define the same notions of continuity and convergence and for many purposes do not need to be distinguished. To be more precise the uniform structure defined by equivalent norms on the vector space is uniformly isomorphic.Every (semi)-norm is a sublinear function, which implies that every norm is a convex function. As a result, finding a global optimum of a norm-based objective function is often tractable.Given a finite family of seminorms pi on a vector space the sumis again a seminorm.For any norm p on a vector space V, we have that for all u and v ∈ V:Thus, p(u ± v) ≥ |p(u) − p(v)|.For the Lp norms, we have Hölder's inequality[6]A special case of this is the Cauchy–Schwarz inequality:[6]All seminorms on a vector space V can be classified in terms of absolutely convex absorbing subsets A of V. To each such subset corresponds a seminorm pA called the gauge of A, defined aswith the property thatConversely:Any locally convex topological vector space has a local basis consisting of absolutely convex sets. A common method to construct such a basis is to use a family (p) of seminorms p that separates points: the collection of all finite intersections of sets {p < 1/n} turns the space into a locally convex topological vector space so that every p is continuous.Such a method is used to design weak and weak* topologies.norm case:There are several generalizations of norms and semi-norms. If p is absolute homogeneity but in place of subadditivity we require thatthen p satisfies the triangle inequality but is called a quasi-seminorm and the smallest value of b for which this holds is called the multiplier of p; if in addition p separates points then it is called a quasi-norm.On the other hand, if p satisfies the triangle inequality but in place of absolute homogeneity we require thatthen p is called a k-seminorm.We have the following relationship between quasi-seminorms and k-seminorms:The concept of norm in composition algebras does not share the usual properties of a norm. A composition algebra (A, *, N) consists of an algebra over a field A, an involution *, and a quadratic form N, which is called the "norm". In several cases N is an isotropic quadratic form so that A has at least one null vector, contrary to the separation of points required for the usual norm discussed in this article.
Gottfried Wilhelm Leibniz
Gottfried Wilhelm (von) Leibniz (sometimes spelled Leibnitz) (/ˈlaɪbnɪts/;[10] German: [ˈɡɔtfʁiːt ˈvɪlhɛlm fɔn ˈlaɪbnɪts][11][12] or [ˈlaɪpnɪts];[13] French: Godefroi Guillaume Leibnitz;[14] 1 July 1646 [O.S. 21 June] – 14 November 1716) was a prominent German polymath and philosopher in the history of mathematics and the history of philosophy.  His most notable accomplishment was conceiving the ideas of differential and integral calculus, independently of Isaac Newton's contemporaneous developments.[15] Mathematical works have always favored Leibniz's notation as the conventional expression of calculus, while Newton's notation became unused. It was only in the 20th century that Leibniz's law of continuity and transcendental law of homogeneity found mathematical implementation (by means of non-standard analysis). He became one of the most prolific inventors in the field of mechanical calculators. While working on adding automatic multiplication and division to Pascal's calculator, he was the first to describe a pinwheel calculator in 1685[16] and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, which is the foundation of all digital computers.In philosophy, Leibniz is most noted for his optimism, i.e. his conclusion that our universe is, in a restricted sense, the best possible one that God could have created, an idea that was often lampooned by others such as Voltaire. Leibniz, along with René Descartes and Baruch Spinoza, was one of the three great 17th-century advocates of rationalism. The work of Leibniz anticipated modern logic and analytic philosophy, but his philosophy also looks back to the scholastic tradition, in which conclusions are produced by applying reason to first principles or prior definitions rather than to empirical evidence.Leibniz made major contributions to physics and technology, and anticipated notions that surfaced much later in philosophy, probability theory, biology, medicine, geology, psychology, linguistics, and computer science. He wrote works on philosophy, politics, law, ethics, theology, history, and philology. Leibniz also contributed to the field of library science. While serving as overseer of the Wolfenbüttel library in Germany, he devised a cataloging system that would serve as a guide for many of Europe's largest libraries.[17]  Leibniz's contributions to this vast array of subjects were scattered in various learned journals, in tens of thousands of letters, and in unpublished manuscripts. He wrote in several languages, but primarily in Latin, French, and German.[18] There is no complete gathering of the writings of Leibniz translated into English.[19]Gottfried Leibniz was born on 1 July 1646, toward the end of the Thirty Years' War, in Leipzig, Saxony, to Friedrich Leibniz and Catharina Schmuck. Friedrich noted in his family journal:21. Juny am Sontag 1646 Ist mein Sohn Gottfried Wilhelm, post sextam vespertinam 1/4 uff 7 uhr abents zur welt gebohren, im Wassermann.In English:On Sunday 21 June [NS: 1 July] 1646, my son Gottfried Wilhelm is born into the world a quarter before seven in the evening, in Aquarius.[20][21]Leibniz was baptized on 3 July of that year at St. Nicholas Church, Leipzig; his godfather was the Lutheran theologian Martin Geier [de].[22] His father died when he was six years old, and from that point on he was raised by his mother.[23]Leibniz's father had been a Professor of Moral Philosophy at the University of Leipzig, and the boy later inherited his father's personal library. He was given free access to it from the age of seven. While Leibniz's schoolwork was largely confined to the study of a small canon of authorities, his father's library enabled him to study a wide variety of advanced philosophical and theological works—ones that he would not have otherwise been able to read until his college years.[24] Access to his father's library, largely written in Latin, also led to his proficiency in the Latin language, which he achieved by the age of 12.  He also composed 300 hexameters of Latin verse, in a single morning, for a special event at school at the age of 13.[25]In April 1661 he enrolled in his father's former university at age 14,[26][1][27] and completed his bachelor's degree in Philosophy in December 1662. He defended his Disputatio Metaphysica de Principio Individui (Metaphysical Disputation on the Principle of Individuation),[28] which addressed the principle of individuation, on 9 June 1663. Leibniz earned his master's degree in Philosophy on 7 February 1664. He published and defended a dissertation Specimen Quaestionum Philosophicarum ex Jure collectarum (An Essay of Collected Philosophical Problems of Right),[28] arguing for both a theoretical and a pedagogical relationship between philosophy and law, in December 1664. After one year of legal studies, he was awarded his bachelor's degree in Law on 28 September 1665.[29] His dissertation was titled De conditionibus (On Conditions).[28]In early 1666, at age 19, Leibniz wrote his first book, De Arte Combinatoria (On the Combinatorial Art), the first part of which was also his habilitation thesis in Philosophy, which he defended in March 1666.[28][30] His next goal was to earn his license and Doctorate in Law, which normally required three years of study. In 1666, the University of Leipzig turned down Leibniz's doctoral application and refused to grant him a Doctorate in Law, most likely due to his relative youth.[31][32] Leibniz subsequently left Leipzig.[33]Leibniz then enrolled in the University of Altdorf and quickly submitted a thesis, which he had probably been working on earlier in Leipzig.[34] The title of his thesis was Disputatio Inauguralis de Casibus Perplexis in Jure (Inaugural Disputation on Ambiguous Legal Cases).[28] Leibniz earned his license to practice law and his Doctorate in Law in November 1666. He next declined the offer of an academic appointment at Altdorf, saying that "my thoughts were turned in an entirely different direction".[35]As an adult, Leibniz often introduced himself as "Gottfried von Leibniz". Many posthumously published editions of his writings presented his name on the title page as "Freiherr G. W. von Leibniz." However, no document has ever been found from any contemporary government that stated his appointment to any form of nobility.[36]Leibniz's first position was as a salaried secretary to an alchemical society in Nuremberg.[37] He knew fairly little about the subject at that time but presented himself as deeply learned. He soon met Johann Christian von Boyneburg (1622–1672), the dismissed chief minister of the Elector of Mainz, Johann Philipp von Schönborn.[38] Von Boyneburg hired Leibniz as an assistant, and shortly thereafter reconciled with the Elector and introduced Leibniz to him. Leibniz then dedicated an essay on law to the Elector in the hope of obtaining employment. The stratagem worked; the Elector asked Leibniz to assist with the redrafting of the legal code for the Electorate.[39] In 1669, Leibniz was appointed assessor in the Court of Appeal. Although von Boyneburg died late in 1672, Leibniz remained under the employment of his widow until she dismissed him in 1674.[citation needed]Von Boyneburg did much to promote Leibniz's reputation, and the latter's memoranda and letters began to attract favorable notice. After Leibniz's service to the Elector there soon followed a diplomatic role. He published an essay, under the pseudonym of a fictitious Polish nobleman, arguing (unsuccessfully) for the German candidate for the Polish crown. The main force in European geopolitics during Leibniz's adult life was the ambition of Louis XIV of France, backed by French military and economic might. Meanwhile, the Thirty Years' War had left German-speaking Europe exhausted, fragmented, and economically backward. Leibniz proposed to protect German-speaking Europe by distracting Louis as follows. France would be invited to take Egypt as a stepping stone towards an eventual conquest of the Dutch East Indies. In return, France would agree to leave Germany and the Netherlands undisturbed. This plan obtained the Elector's cautious support. In 1672, the French government invited Leibniz to Paris for discussion,[40] but the plan was soon overtaken by the outbreak of the Franco-Dutch War and became irrelevant. Napoleon's failed invasion of Egypt in 1798 can be seen as an unwitting, late implementation of Leibniz's plan, after the Eastern hemisphere colonial supremacy in Europe had already passed from the Dutch to the British.[citation needed]Thus Leibniz went to Paris in 1672. Soon after arriving, he met Dutch physicist and mathematician Christiaan Huygens and realised that his own knowledge of mathematics and physics was patchy. With Huygens as his mentor, he began a program of self-study that soon pushed him to making major contributions to both subjects, including discovering his version of the differential and integral calculus. He met Nicolas Malebranche and Antoine Arnauld, the leading French philosophers of the day, and studied the writings of Descartes and Pascal, unpublished as well as published.[41] He befriended a German mathematician, Ehrenfried Walther von Tschirnhaus; they corresponded for the rest of their lives.When it became clear that France would not implement its part of Leibniz's Egyptian plan, the Elector sent his nephew, escorted by Leibniz, on a related mission to the English government in London, early in 1673.[42] There Leibniz came into acquaintance of Henry Oldenburg and John Collins. He met with the Royal Society where he demonstrated a calculating machine that he had designed and had been building since 1670. The machine was able to execute all four basic operations (adding, subtracting, multiplying, and dividing), and the society quickly made him an external member.The mission ended abruptly when news of the Elector's death (12 February 1673) reached them. Leibniz promptly returned to Paris and not, as had been planned, to Mainz.[43] The sudden deaths of his two patrons in the same winter meant that Leibniz had to find a new basis for his career.In this regard, a 1669 invitation from the John Frederick of Brunswick to visit Hanover proved to have been fateful. Leibniz had declined the invitation, but had begun corresponding with the duke in 1671. In 1673, the duke offered Leibniz the post of counsellor. Leibniz very reluctantly accepted the position two years later, only after it became clear that no employment in Paris, whose intellectual stimulation he relished, or with the Habsburg imperial court, was forthcoming.[44]In 1675 he tried to get admitted to the French Academy of Sciences as a foreign honorary member, but it was considered that there were already enough foreigners there and so no invitation came. He left Paris in October 1676.Leibniz managed to delay his arrival in Hanover until the end of 1676 after making one more short journey to London, where Newton accused him of having seen Newton's unpublished work on calculus in advance.[45]  This was alleged to be evidence supporting the accusation, made decades later, that he had stolen calculus from Newton. On the journey from London to Hanover, Leibniz stopped in The Hague where he met van Leeuwenhoek, the discoverer of microorganisms. He also spent several days in intense discussion with Spinoza, who had just completed his masterwork, the Ethics.[46]In 1677, he was promoted, at his request, to Privy Counselor of Justice, a post he held for the rest of his life. Leibniz served three consecutive rulers of the House of Brunswick as historian, political adviser, and most consequentially, as librarian of the ducal library. He thenceforth employed his pen on all the various political, historical, and theological matters involving the House of Brunswick; the resulting documents form a valuable part of the historical record for the period.Leibniz began promoting a project to use windmills to improve the mining operations in the Harz Mountains. This project did little to improve mining operations and was shut down by Duke Ernst August in 1685.[44]Among the few people in north Germany to accept Leibniz were the Electress Sophia of Hanover (1630–1714), her daughter Sophia Charlotte of Hanover (1668–1705), the Queen of Prussia and his avowed disciple, and Caroline of Ansbach, the consort of her grandson, the future George II. To each of these women he was correspondent, adviser, and friend. In turn, they all approved of Leibniz more than did their spouses and the future king George I of Great Britain.[47]The population of Hanover was only about 10,000, and its provinciality eventually grated on Leibniz. Nevertheless, to be a major courtier to the House of Brunswick was quite an honor, especially in light of the meteoric rise in the prestige of that House during Leibniz's association with it. In 1692, the Duke of Brunswick became a hereditary Elector of the Holy Roman Empire. The British Act of Settlement 1701 designated the Electress Sophia and her descent as the royal family of England, once both King William III and his sister-in-law and successor, Queen Anne, were dead. Leibniz played a role in the initiatives and negotiations leading up to that Act, but not always an effective one. For example, something he published anonymously in England, thinking to promote the Brunswick cause, was formally censured by the British Parliament.The Brunswicks tolerated the enormous effort Leibniz devoted to intellectual pursuits unrelated to his duties as a courtier, pursuits such as perfecting calculus, writing about other mathematics, logic, physics, and philosophy, and keeping up a vast correspondence. He began working on calculus in 1674; the earliest evidence of its use in his surviving notebooks is 1675. By 1677 he had a coherent system in hand, but did not publish it until 1684. Leibniz's most important mathematical papers were published between 1682 and 1692, usually in a journal which he and Otto Mencke founded in 1682, the Acta Eruditorum. That journal played a key role in advancing his mathematical and scientific reputation, which in turn enhanced his eminence in diplomacy, history, theology, and philosophy.The Elector Ernest Augustus commissioned Leibniz to write a history of the House of Brunswick, going back to the time of Charlemagne or earlier, hoping that the resulting book would advance his dynastic ambitions. From 1687 to 1690, Leibniz traveled extensively in Germany, Austria, and Italy, seeking and finding archival materials bearing on this project. Decades went by but no history appeared; the next Elector became quite annoyed at Leibniz's apparent dilatoriness. Leibniz never finished the project, in part because of his huge output on many other fronts, but also because he insisted on writing a meticulously researched and erudite book based on archival sources, when his patrons would have been quite happy with a short popular book, one perhaps little more than a genealogy with commentary, to be completed in three years or less. They never knew that he had in fact carried out a fair part of his assigned task: when the material Leibniz had written and collected for his history of the House of Brunswick was finally published in the 19th century, it filled three volumes.Leibniz was appointed Librarian of the Herzog August Library in Wolfenbüttel, Lower Saxony, in 1691.In 1708, John Keill, writing in the journal of the Royal Society and with Newton's presumed blessing, accused Leibniz of having plagiarised Newton's calculus.[48] Thus began the calculus priority dispute which darkened the remainder of Leibniz's life. A formal investigation by the Royal Society (in which Newton was an unacknowledged participant), undertaken in response to Leibniz's demand for a retraction, upheld Keill's charge. Historians of mathematics writing since 1900 or so have tended to acquit Leibniz, pointing to important differences between Leibniz's and Newton's versions of calculus.In 1711, while traveling in northern Europe, the Russian Tsar Peter the Great stopped in Hanover and met Leibniz, who then took some interest in Russian matters for the rest of his life. In 1712, Leibniz began a two-year residence in Vienna, where he was appointed Imperial Court Councillor to the Habsburgs. On the death of Queen Anne in 1714, Elector George Louis became King George I of Great Britain, under the terms of the 1701 Act of Settlement. Even though Leibniz had done much to bring about this happy event, it was not to be his hour of glory. Despite the intercession of the Princess of Wales, Caroline of Ansbach, George I forbade Leibniz to join him in London until he completed at least one volume of the history of the Brunswick family his father had commissioned nearly 30 years earlier. Moreover, for George I to include Leibniz in his London court would have been deemed insulting to Newton, who was seen as having won the calculus priority dispute and whose standing in British official circles could not have been higher. Finally, his dear friend and defender, the Dowager Electress Sophia, died in 1714.Leibniz died in Hanover in 1716: at the time, he was so out of favor that neither George I (who happened to be near Hanover at that time) nor any fellow courtier other than his personal secretary attended the funeral. Even though Leibniz was a life member of the Royal Society and the Berlin Academy of Sciences, neither organization saw fit to honor his death. His grave went unmarked for more than 50 years. Leibniz was eulogized by Fontenelle, before the French Academy of Sciences in Paris, which had admitted him as a foreign member in 1700. The eulogy was composed at the behest of the Duchess of Orleans, a niece of the Electress Sophia.Leibniz never married. He complained on occasion about money, but the fair sum he left to his sole heir, his sister's stepson, proved that the Brunswicks had, by and large, paid him well. In his diplomatic endeavors, he at times verged on the unscrupulous, as was all too often the case with professional diplomats of his day. On several occasions, Leibniz backdated and altered personal manuscripts, actions which put him in a bad light during the calculus controversy. On the other hand, he was charming, well-mannered, and not without humor and imagination.[49] He had many friends and admirers all over Europe. On Leibniz's religious views, though he was a protestant, Leibniz learned to appreciate aspects of Catholicism through his patrons and colleagues. He never admitted the Protestant view of Pope as an Antichrist.[50] Leibniz was claimed as a philosophical theist.[51][52][53][54] Leibniz remained committed to Trinitarian Christianity throughout his life.[55]Leibniz's philosophical thinking appears fragmented, because his philosophical writings consist mainly of a multitude of short pieces: journal articles, manuscripts published long after his death, and many letters to many correspondents. He wrote only two book-length philosophical treatises, of which only the Théodicée of 1710 was published in his lifetime.Leibniz dated his beginning as a philosopher to his Discourse on Metaphysics, which he composed in 1686 as a commentary on a running dispute between Nicolas Malebranche and Antoine Arnauld. This led to an extensive and valuable correspondence with Arnauld;[56] it and the Discourse were not published until the 19th century. In 1695, Leibniz made his public entrée into European philosophy with a journal article titled "New System of the Nature and Communication of Substances".[57] Between 1695 and 1705, he composed his New Essays on Human Understanding, a lengthy commentary on John Locke's 1690 An Essay Concerning Human Understanding, but upon learning of Locke's 1704 death, lost the desire to publish it, so that the New Essays were not published until 1765. The Monadologie, composed in 1714 and published posthumously, consists of 90 aphorisms.Leibniz met Spinoza in 1676, read some of his unpublished writings, and has since been suspected of appropriating some of Spinoza's ideas. While Leibniz admired Spinoza's powerful intellect, he was also forthrightly dismayed by Spinoza's conclusions,[58] especially when these were inconsistent with Christian orthodoxy.Unlike Descartes and Spinoza, Leibniz had a thorough university education in philosophy. He was influenced by his Leipzig professor Jakob Thomasius, who also supervised his BA thesis in philosophy.[9] Leibniz also eagerly read Francisco Suárez, a Spanish Jesuit respected even in Lutheran universities. Leibniz was deeply interested in the new methods and conclusions of Descartes, Huygens, Newton, and Boyle, but viewed their work through a lens heavily tinted by scholastic notions. Yet it remains the case that Leibniz's methods and concerns often anticipate the logic, and analytic and linguistic philosophy of the 20th century.Leibniz variously invoked one or another of seven fundamental philosophical Principles:[59]Leibniz would on occasion give a rational defense of a specific principle, but more often took them for granted.[65]Leibniz's best known contribution to metaphysics is his theory of monads, as exposited in Monadologie. He proposes his theory that the universe is made of an infinite number of simple substances known as monads.[66] Monads can also be compared to the corpuscles of the Mechanical Philosophy of René Descartes and others. These simple substances or monads are the "ultimate units of existence in nature". Monads have no parts but still exist by the qualities that they have. These qualities are continuously changing over time, and each monad is unique. They are also not affected by time and are subject to only creation and annihilation.[67] Monads are centers of force; substance is force, while space, matter, and motion are merely phenomenal.Leibniz's proof of God can be summarized in the Théodicée.[68]  Reason is governed by the principle of contradiction and the principle of sufficient reason. Using the principle of reasoning, Leibniz concluded that the first reason of all things is God.[68] All that we see and experience are subject to change, and the fact that this world is contingent can be explained by the possibility of the world being arranged differently in space and time. The contingent world must have some necessary reason for its existence. Leibniz uses a geometry book as an example to explain his reasoning. If this book was copied from an infinite chain of copies, there must be a some reason for the content of the book.[69] Leibniz concluded that there must be the "monas monadum" or God.The ontological essence of a monad is its irreducible simplicity. Unlike atoms, monads possess no material or spatial character. They also differ from atoms by their complete mutual independence, so that interactions among monads are only apparent. Instead, by virtue of the principle of pre-established harmony, each monad follows a preprogrammed set of "instructions" peculiar to itself, so that a monad "knows" what to do at each moment. By virtue of these intrinsic instructions, each monad is like a little mirror of the universe. Monads need not be "small"; e.g., each human being constitutes a monad, in which case free will is problematic.Monads are purported to have gotten rid of the problematic:The Theodicy[70] tries to justify the apparent imperfections of the world by claiming that it is optimal among all possible worlds. It must be the best possible and most balanced world, because it was created by an all powerful and all knowing God, who would not choose to create an imperfect world if a better world could be known to him or possible to exist. In effect, apparent flaws that can be identified in this world must exist in every possible world, because otherwise God would have chosen to create the world that excluded those flaws.Leibniz asserted that the truths of theology (religion) and philosophy cannot contradict each other, since reason and faith are both "gifts of God" so that their conflict would imply God contending against himself. The Theodicy is Leibniz's attempt to reconcile his personal philosophical system with his interpretation of the tenets of Christianity.[71] This project was motivated in part by Leibniz's belief, shared by many conservative philosophers and theologians during the Enlightenment, in the rational and enlightened nature of the Christian religion as compared to its purportedly less advanced non-Western counterparts. It was also shaped by Leibniz's belief in the perfectibility of human nature (if humanity relied on correct philosophy and religion as a guide), and by his belief that metaphysical necessity must have a rational or logical foundation, even if this metaphysical causality seemed inexplicable in terms of physical necessity (the natural laws identified by science).Because reason and faith must be entirely reconciled, any tenet of faith which could not be defended by reason must be rejected. Leibniz then approached one of the central criticisms of Christian theism:[72] if God is all good, all wise and all powerful, how did evil come into the world? The answer (according to Leibniz) is that, while God is indeed unlimited in wisdom and power, his human creations, as creations, are limited both in their wisdom and in their will (power to act). This predisposes humans to false beliefs, wrong decisions and ineffective actions in the exercise of their free will. God does not arbitrarily inflict pain and suffering on humans; rather he permits both moral evil (sin) and physical evil (pain and suffering) as the necessary consequences of metaphysical evil (imperfection), as a means by which humans can identify and correct their erroneous decisions, and as a contrast to true good.Further, although human actions flow from prior causes that ultimately arise in God, and therefore are known as a metaphysical certainty to God, an individual's free will is exercised within natural laws, where choices are merely contingently necessary, to be decided in the event by a "wonderful spontaneity" that provides individuals an escape from rigorous predestination.For Leibniz, "God is an absolutely perfect being." He describes this perfection later in section VI as the simplest form of something with the most substantial outcome (VI). Along these lines, he declares that every type of perfection "pertains to him (God) in the highest degree" (I). Even though his types of perfections are not specifically drawn out, Leibniz highlights the one thing that, to him, does certify imperfections and proves that God is perfect: "that one acts imperfectly if he acts with less perfection than he is capable of", and since God is a perfect being, he cannot act imperfectly (III). Because God cannot act imperfectly, the decisions he makes pertaining to the world must be perfect. Leibniz also comforts readers, stating that because he has done everything to the most perfect degree; those who love him cannot be injured. However, to love God is a subject of difficulty as Leibniz believes that we are "not disposed to wish for that which God desires" because we have the ability to alter our disposition (IV). In accordance with this, many act as rebels, but Leibniz says that the only way we can truly love God is by being content "with all that comes to us according to his will" (IV).Because God is "an absolutely perfect being" (I), Leibniz argues that God would be acting imperfectly if he acted with any less perfection than what he is able of (III). His syllogism then ends with the statement that God has made the world perfectly in all ways. This also affects how we should view God and his will. Leibniz states that, in lieu of God’s will, we have to understand that God "is the best of all masters" and he will know when his good succeeds, so we, therefore, must act in conformity to his good will—or as much of it as we understand (IV). In our view of God, Leibniz declares that we cannot admire the work solely because of the maker, lest we mar the glory and love God in doing so. Instead, we must admire the maker for the work he has done (II). Effectively, Leibniz states that if we say the earth is good because of the will of God, and not good according to some standards of goodness, then how can we praise God for what he has done if contrary actions are also praiseworthy by this definition (II). Leibniz then asserts that different principles and geometry cannot simply be from the will of God, but must follow from his understanding.[73]Leibniz wrote: "Why is there something rather than nothing? The sufficient reason ... is found in a substance which ... is a necessary being bearing the reason for its existence within itself."[74] Martin Heidegger called this question "the fundamental question of metaphysics".[75][76]Leibniz believed that much of human reasoning could be reduced to calculations of a sort, and that such calculations could resolve many differences of opinion:The only way to rectify our reasonings is to make them as tangible as those of the Mathematicians, so that we can find our error at a glance, and when there are disputes among persons, we can simply say: Let us calculate [calculemus], without further ado, to see who is right.[77]Leibniz's calculus ratiocinator, which resembles symbolic logic, can be viewed as a way of making such calculations feasible. Leibniz wrote memoranda[78] that can now be read as groping attempts to get symbolic logic—and thus his calculus—off the ground. These writings remained unpublished until the appearance of a selection edited by C.I. Gerhardt (1859). L. Couturat published a selection in 1901; by this time the main developments of modern logic had been created by Charles Sanders Peirce and by Gottlob Frege.Leibniz thought symbols were important for human understanding. He attached so much importance to the development of good notations that he attributed all his discoveries in mathematics to this. His notation for calculus is an example of his skill in this regard. Peirce, a 19th-century pioneer of semiotics, shared Leibniz's passion for symbols and notation, and his belief that these are essential to a well-running logic and mathematics.But Leibniz took his speculations much further. Defining a character as any written sign, he then defined a "real" character as one that represents an idea directly and not simply as the word embodying the idea. Some real characters, such as the notation of logic, serve only to facilitate reasoning. Many characters well known in his day, including Egyptian hieroglyphics, Chinese characters, and the symbols of astronomy and chemistry, he deemed not real.[79] Instead, he proposed the creation of a characteristica universalis or "universal characteristic", built on an alphabet of human thought in which each fundamental concept would be represented by a unique "real" character:It is obvious that if we could find characters or signs suited for expressing all our thoughts as clearly and as exactly as arithmetic expresses numbers or geometry expresses lines, we could do in all matters insofar as they are subject to reasoning all that we can do in arithmetic and geometry. For all investigations which depend on reasoning would be carried out by transposing these characters and by a species of calculus.[80]Complex thoughts would be represented by combining characters for simpler thoughts. Leibniz saw that the uniqueness of prime factorization suggests a central role for prime numbers in the universal characteristic, a striking anticipation of Gödel numbering. Granted, there is no intuitive or mnemonic way to number any set of elementary concepts using the prime numbers. Because Leibniz was a mathematical novice when he first wrote about the characteristic, at first he did not conceive it as an algebra but rather as a universal language or script. Only in 1676 did he conceive of a kind of "algebra of thought", modeled on and including conventional algebra and its notation. The resulting characteristic included a logical calculus, some combinatorics, algebra, his analysis situs (geometry of situation), a universal concept language, and more. What Leibniz actually intended by his characteristica universalis and calculus ratiocinator, and the extent to which modern formal logic does justice to calculus, may never be established.[81] Leibniz's idea of reasoning through a universal language of symbols and calculations remarkably foreshadows great 20th-century developments in formal systems, such as Turing completeness, where computation was used to define equivalent universal languages (see Turing degree).Leibniz has been noted as one of the most important logicians between the times of Aristotle and Gottlob Frege.[82] Leibniz enunciated the principal properties of what we now call conjunction, disjunction, negation, identity, set inclusion, and the empty set. The principles of Leibniz's logic and, arguably, of his whole philosophy, reduce to two:The formal logic that emerged early in the 20th century also requires, at minimum, unary negation and quantified variables ranging over some universe of discourse.Leibniz published nothing on formal logic in his lifetime; most of what he wrote on the subject consists of working drafts. In his book History of Western Philosophy, Bertrand Russell went so far as to claim that Leibniz had developed logic in his unpublished writings to a level which was reached only 200 years later.Russell's principal work on Leibniz found that many of Leibniz's most startling  philosophical ideas and claims (e.g., that each of the fundamental monads mirrors the whole universe) follow logically from Leibniz's conscious choice to reject relations between things as unreal. He regarded such relations as (real) qualities of things (Leibniz admitted unary predicates only): For him "Mary is the mother of John" describes separate qualities of Mary and of John. This view contrasts with the relational logic of De Morgan, Peirce, Schröder and Russell himself, now standard in predicate logic. Notably, Leibniz also declared space and time to be inherently relational.[83]Although the mathematical notion of function was implicit in trigonometric and logarithmic tables, which existed in his day, Leibniz was the first, in 1692 and 1694, to employ it explicitly, to denote any of several geometric concepts derived from a curve, such as abscissa, ordinate, tangent, chord, and the perpendicular.[84] In the 18th century, "function" lost these geometrical associations. Leibniz also believed that the sum of an infinite number of zeros would equal to one half using the analogy of the creation of the world from nothing.[85] Leibniz was also one of the pioneers in actuarial science, calculating the purchase price of life annuities and the liquidation of a state's debt.[86]Leibniz's discoveries of Boolean algebra and of symbolic logic, also relevant to mathematics, are discussed in the preceding section. The best overview of Leibniz's writings on calculus may be found in Bos (1974).[87]The Leibniz formula for π states thatLeibniz wrote that circles "can most simply be expressed by this series, that is, the aggregate of fractions alternately added and subtracted."[95] However this formula is only accurate with a large number of terms, using 10,000,000 terms to obtain the correct value of π/4 to 8 decimal places.[96] Leibniz attempted to create a definition for a straight line while attempting to prove the parallel postulate.[97] While most mathematicians defined a straight line as the shortest line between two points, Leibniz believed that this was merely a property of a straight line rather than the definition.[98]Leibniz is credited, along with Sir Isaac Newton, with the discovery of calculus (differential and integral calculus). According to Leibniz's notebooks, a critical breakthrough occurred on 11 November 1675, when he employed integral calculus for the first time to find the area under the graph of a function y = f(x).[99] He introduced several notations used to this day, for instance the integral sign ∫, representing an elongated S, from the Latin word summa, and the d used for differentials, from the Latin word differentia. Leibniz did not publish anything about his calculus until 1684.[100] Leibniz expressed the inverse relation of integration and differentiation, later called the fundamental theorem of calculus, by means of a figure[101] in his 1693 paper Supplementum geometriae dimensoriae....[102] However, James Gregory is credited for the theorem's discovery in geometric form, Isaac Barrow proved a more generalized geometric version, and Newton developed supporting theory. The concept became more transparent as developed through Leibniz's formalism and new notation.[103] The product rule of differential calculus is still called "Leibniz's law". In addition, the theorem that tells how and when to differentiate under the integral sign is called the Leibniz integral rule.Leibniz exploited infinitesimals in developing calculus, manipulating them in ways suggesting that they had paradoxical algebraic properties. George Berkeley, in a tract called The Analyst and also in De Motu, criticized these. A recent study argues that Leibnizian calculus was free of contradictions, and was better grounded than Berkeley's empiricist criticisms.[104]From 1711 until his death, Leibniz was engaged in a dispute with John Keill, Newton and others, over whether Leibniz had invented calculus independently of Newton. This subject is treated at length in the article Leibniz–Newton calculus controversy.The use of infinitesimals in mathematics was frowned upon by followers of Karl Weierstrass,[citation needed] but survived in science and engineering, and even in rigorous mathematics, via the fundamental computational device known as the differential. Beginning in 1960, Abraham Robinson worked out a rigorous foundation for Leibniz's infinitesimals, using model theory, in the context of a field of hyperreal numbers. The resulting non-standard analysis can be seen as a belated vindication of Leibniz's mathematical reasoning. Robinson's transfer principle is a mathematical implementation of Leibniz's heuristic law of continuity, while the standard part function implements the Leibnizian transcendental law of homogeneity.Leibniz was the first to use the term analysis situs,[105] later used in the 19th century to refer to what is now known as topology. There are two takes on this situation. On the one hand, Mates, citing a 1954 paper in German by Jacob Freudenthal, argues:Although for Leibniz the situs of a sequence of points is completely determined by the distance between them and is altered if those distances are altered, his admirer Euler, in the famous 1736 paper solving the Königsberg Bridge Problem and its generalizations, used the term geometria situs in such a sense that the situs remains unchanged under topological deformations. He mistakenly credits Leibniz with originating this concept. ... [It] is sometimes not realized that Leibniz used the term in an entirely different sense and hence can hardly be considered the founder of that part of mathematics.[106]But Hideaki Hirano argues differently, quoting Mandelbrot:[107]To sample Leibniz' scientific works is a sobering experience. Next to calculus, and to other thoughts that have been carried out to completion, the number and variety of premonitory thrusts is overwhelming. We saw examples in "packing", ... My Leibniz mania is further reinforced by finding that for one moment its hero attached importance to geometric scaling. In Euclidis Prota ..., which is an attempt to tighten Euclid's axioms, he states ...: "I have diverse definitions for the straight line. The straight line is a curve, any part of which is similar to the whole, and it alone has this property, not only among curves but among sets." This claim can be proved today.[108]Thus the fractal geometry promoted by Mandelbrot drew on Leibniz's notions of self-similarity and the principle of continuity: Natura non facit saltus.[62] We also see that when Leibniz wrote, in a metaphysical vein, that "the straight line is a curve, any part of which is similar to the whole", he was anticipating topology by more than two centuries. As for "packing", Leibniz told his friend and correspondent Des Bosses to imagine a circle, then to inscribe within it three congruent circles with maximum radius; the latter smaller circles could be filled with three even smaller circles by the same procedure. This process can be continued infinitely, from which arises a good idea of self-similarity. Leibniz's improvement of Euclid's axiom contains the same concept.Leibniz's writings are currently discussed, not only for their anticipations and possible discoveries not yet recognized, but as ways of advancing present knowledge. Much of his writing on physics is included in Gerhardt's Mathematical Writings.Leibniz contributed a fair amount to the statics and dynamics emerging around him, often disagreeing with Descartes and Newton. He devised a new theory of motion (dynamics) based on kinetic energy and potential energy, which posited space as relative, whereas Newton was thoroughly convinced that space was absolute. An important example of Leibniz's mature physical thinking is his Specimen Dynamicum of 1695.[109]Until the discovery of subatomic particles and the quantum mechanics governing them, many of Leibniz's speculative ideas about aspects of nature not reducible to statics and dynamics made little sense. For instance, he anticipated Albert Einstein by arguing, against Newton, that space, time and motion are relative, not absolute: "As for my own opinion, I have said more than once, that I hold space to be something merely relative, as time is, that I hold it to be an order of coexistences, as time is an order of successions."[110]Leibniz held a relationist notion of space and time, against Newton's substantivalist views.[111][112][113] According to Newton's substantivalism, space and time are entities in their own right, existing independently of things. Leibniz's relationism, on the other hand, describes space and time as systems of relations that exist between objects. The rise of general relativity and subsequent work in the history of physics has put Leibniz's stance in a more favorable light.One of Leibniz's projects was to recast Newton's theory as a vortex theory.[114] However, his project went beyond vortex theory, since at its heart there was an attempt to explain one of the most difficult problems in physics, that of the origin of the cohesion of matter.[114]The principle of sufficient reason has been invoked in recent cosmology, and his identity of indiscernibles in quantum mechanics, a field some even credit him with having anticipated in some sense. Those who advocate digital philosophy, a recent direction in cosmology, claim Leibniz as a precursor. In addition to his theories about the nature of reality, Leibniz's contributions to the development of calculus have also had a major impact on physics.Leibniz's vis viva (Latin for "living force") is mv2, twice the modern kinetic energy. He realized that the total energy would be conserved in certain mechanical systems, so he considered it an innate motive characteristic of matter.[115] Here too his thinking gave rise to another regrettable nationalistic dispute. His vis viva was seen as rivaling the conservation of momentum championed by Newton in England and by Descartes in France; hence academics in those countries tended to neglect Leibniz's idea. In reality, both energy and momentum are conserved, so the two approaches are equally valid.By proposing that the earth has a molten core, he anticipated modern geology. In embryology, he was a preformationist, but also proposed that organisms are the outcome of a combination of an infinite number of possible microstructures and of their powers. In the life sciences and paleontology, he revealed an amazing transformist intuition, fueled by his study of comparative anatomy and fossils. One of his principal works on this subject, Protogaea, unpublished in his lifetime, has recently been published in English for the first time. He worked out a primal organismic theory.[116] In medicine, he exhorted the physicians of his time—with some results—to ground their theories in detailed comparative observations and verified experiments, and to distinguish firmly scientific and metaphysical points of view.Psychology had been a central interest of Leibniz.[117][118] He appears to be an "underappreciated pioneer of psychology" [119] He wrote on topics which are now regarded as fields of psychology: attention and consciousness, memory, learning (association), motivation (the act of "striving"), emergent individuality, the general dynamics of development (evolutionary psychology)[citation needed]. His discussions in the New Essays and Monadology often rely on everyday observations such as the behaviour of a dog or the noise of the sea, and he develops intuitive analogies (the synchronous running of clocks or the balance spring of a clock). He also devised postulates and principles that apply to psychology: the continuum of the unnoticed petite perceptions to the distinct, self-aware apperception, and psychophysical parallelism from the point of view of causality and of purpose:  “Souls act according to the laws of final causes, through aspirations, ends and means. Bodies act according to the laws of efficient causes, i.e. the laws of motion. And these two realms, that of efficient causes and that of final causes, harmonize with one another.” [120] This idea refers to the mind-body problem, stating that the mind and brain do not act upon each other, but act alongside each other separately but in harmony.[121] Leibniz, however, did not use the term psychologia.[122]Leibniz’ epistemological position—against John Locke and English empiricism (sensualism)—was made clear: “Nihil est in intellectu quod non fuerit in sensu, nisi intellectu ipse.” – “Nothing is in the intellect that was not first in the senses, except the intellect itself.” [123] Principles that are not present in sensory impressions can be recognised in human perception and consciousness: logical inferences, categories of thought, the principle of causality and the principle of purpose (teleology). Leibniz found his most important interpreter in Wilhelm Wundt, founder of psychology as a discipline. Wundt used the "… nisi intellectu ipse" quotation 1862 on the title page of his Beiträge zur Theorie der Sinneswahrnehmung (Contributions on the Theory of Sensory Perception) and published a detailed and aspiring monograph on Leibniz[124] Wundt shaped the term apperception, introduced by Leibniz, into an experimental psychologically based apperception psychology that included neuropsychological modelling – an excellent example of how a concept created by a great philosopher could stimulate a psychological research program. One principle in the thinking of Leibniz played a fundamental role: “the principle of equality of separate but corresponding viewpoints.” Wundt characterized this style of thought (perspectivism) in a way that also applied for him—viewpoints that "supplement one another, while also being able to appear as opposites that only resolve themselves when considered more deeply."[125][126]    Much of Leibniz's work went on to have a great impact on the field of psychology.[127] Leibniz thought that there are many petites perceptions, or small perceptions of which we perceive but of which we are unaware. He believed that by the principle that phenomena found in nature were continuous by default, it was likely that the transition between conscious and unconscious states had intermediary steps.[128]  For this to be true, there must also be a portion of the mind of which we are unaware at any given time. His theory regarding consciousness in relation to the principle of continuity can be seen as an early theory regarding the stages of sleep. In this way, Leibniz's theory of perception can be viewed as one of many theories leading up to the idea of the unconscious. Leibniz was a direct influence on Ernst Platner, who is credited with originally coining the term Unbewußtseyn (unconscious).[129] Additionally, the idea of subliminal stimuli can be traced back to his theory of small perceptions.[130] Leibniz's ideas regarding music and tonal perception went on to influence the laboratory studies of Wilhelm Wundt.[131]In public health, he advocated establishing a medical administrative authority, with powers over epidemiology and veterinary medicine. He worked to set up a coherent medical training program, oriented towards public health and preventive measures. In economic policy, he proposed tax reforms and a national insurance program, and discussed the balance of trade. He even proposed something akin to what much later emerged as game theory. In sociology he laid the ground for communication theory.In 1906, Garland published a volume of Leibniz's writings bearing on his many practical inventions and engineering work. To date, few of these writings have been translated into English. Nevertheless, it is well understood that Leibniz was a serious inventor, engineer, and applied scientist, with great respect for practical life. Following the motto theoria cum praxi, he urged that theory be combined with practical application, and thus has been claimed as the father of applied science. He designed wind-driven propellers and water pumps, mining machines to extract ore, hydraulic presses, lamps, submarines, clocks, etc. With Denis Papin, he invented a steam engine. He even proposed a method for desalinating water. From 1680 to 1685, he struggled to overcome the chronic flooding that afflicted the ducal silver mines in the Harz Mountains, but did not succeed.[132]Leibniz may have been the first computer scientist and information theorist.[133] Early in life, he documented the binary numeral system (base 2), then revisited that system throughout his career.[134] While Leibniz was examining other cultures to compare his metaphysical views, he encountered an ancient Chinese book I Ching. Leibniz interpreted a diagram which showed yin and yang and corresponded it to a zero and one.[135] More information can be found in the Sinophile section. Leibniz may have plagiarized Juan Caramuel y Lobkowitz and Thomas Harriot, who independently developed the binary system,  as he was familiar with their works on the binary system.[136] Juan Caramuel y Lobkowitz worked extensively on logarithms including logarithms with base 2.[137] Thomas Harriot's manuscripts contained a table of binary numbers and their notation, which demonstrated that any number could be written on a base 2 system.[138] Regardless, Leibniz simplified the binary system and articulated logical properties such as conjunction, disjunction, negation, identity, inclusion, and the empty set.[139] He anticipated Lagrangian interpolation and algorithmic information theory. His calculus ratiocinator anticipated aspects of the universal Turing machine. In 1961, Norbert Wiener suggested that Leibniz should be considered the patron saint of cybernetics.[140]In 1671, Leibniz began to invent a machine that could execute all four arithmetic operations, gradually improving it over a number of years. This "stepped reckoner" attracted fair attention and was the basis of his election to the Royal Society in 1673. A number of such machines were made during his years in Hanover by a craftsman working under his supervision. They were not an unambiguous success because they did not fully mechanize the carry operation. Couturat reported finding an unpublished note by Leibniz, dated 1674, describing a machine capable of performing some algebraic operations.[141] Leibniz also devised a (now reproduced) cipher machine, recovered by Nicholas Rescher in 2010.[142] In 1693, Leibniz described a design of a machine which could, in theory, integrate differential equations, which he called "integraph".[143]Leibniz was groping towards hardware and software concepts worked out much later by Charles Babbage and Ada Lovelace. In 1679, while mulling over his binary arithmetic, Leibniz imagined a machine in which binary numbers were represented by marbles, governed by a rudimentary sort of punched cards.[144][145] Modern electronic digital computers replace Leibniz's marbles moving by gravity with shift registers, voltage gradients, and pulses of electrons, but otherwise they run roughly as Leibniz envisioned in 1679.Later in Leibniz’s career (after the death of von Boinburg), Leibniz moved to Paris and accepted a position as a librarian in the Hanoverian court of Johann Friedrich, Duke of Brunswick-Luneburg. Leibniz’s predecessor, Tobias Fleischer, had already created a cataloging system for the Duke’s library but it was a clumsy attempt. At this library, Leibniz focused more on advancing the library than on the cataloging. For instance, within a month of taking the new position, he developed a comprehensive plan to expand the library. He was one of the first to consider developing a core collection for a library and felt “that a library for display and ostentation is a luxury and indeed superfluous, but a well-stocked and organized library is important and useful for all areas of human endeavor and is to be regarded on the same level as schools and churches”.[146] Unfortunately, Leibniz lacked the funds to develop the library in this manner. After working at this library, by the end of 1690 Leibnez was appointed as privy-councilor and librarian of the Bibliotheca Augusta at Wolfenbuettel. It was an extensive library with at least 25,946 printed volumes.[146] At this library, Leibniz sought to improve the catalog. He was not allowed to make complete changes to the existing closed catalog, but was allowed to improve upon it so he started on that task immediately. He created an alphabetical author catalog and had also created other cataloging methods that were not implemented. While serving as librarian of the ducal libraries in Hanover and Wolfenbuettel, Leibniz effectively became one of the founders of library science. He also designed a book indexing system in ignorance of the only other such system then extant, that of the Bodleian Library at Oxford University. He also called on publishers to distribute abstracts of all new titles they produced each year, in a standard form that would facilitate indexing. He hoped that this abstracting project would eventually include everything printed from his day back to Gutenberg. Neither proposal met with success at the time, but something like them became standard practice among English language publishers during the 20th century, under the aegis of the Library of Congress and the British Library.He called for the creation of an empirical database as a way to further all sciences. His characteristica universalis, calculus ratiocinator, and a "community of minds"—intended, among other things, to bring political and religious unity to Europe—can be seen as distant unwitting anticipations of artificial languages (e.g., Esperanto and its rivals), symbolic logic, even the World Wide Web.Leibniz emphasized that research was a collaborative endeavor. Hence he warmly advocated the formation of national scientific societies along the lines of the British Royal Society and the French Academie Royale des Sciences. More specifically, in his correspondence and travels he urged the creation of such societies in Dresden, Saint Petersburg, Vienna, and Berlin. Only one such project came to fruition; in 1700, the Berlin Academy of Sciences was created. Leibniz drew up its first statutes, and served as its first President for the remainder of his life. That Academy evolved into the German Academy of Sciences, the publisher of the ongoing critical edition of his works.[147]With the possible exception of Marcus Aurelius, no philosopher has ever had as much experience with practical affairs of state as Leibniz. Leibniz's writings on law, ethics, and politics[148] were long overlooked by English-speaking scholars, but this has changed of late.[149]While Leibniz was no apologist for absolute monarchy like Hobbes, or for tyranny in any form, neither did he echo the political and constitutional views of his contemporary John Locke, views invoked in support of liberalism, in 18th-century America and later elsewhere. The following excerpt from a 1695 letter to Baron J. C. Boyneburg's son Philipp is very revealing of Leibniz's political sentiments:As for ... the great question of the power of sovereigns and the obedience their peoples owe them, I usually say that it would be good for princes to be persuaded that their people have the right to resist them, and for the people, on the other hand, to be persuaded to obey them passively. I am, however, quite of the opinion of Grotius, that one ought to obey as a rule, the evil of revolution being greater beyond comparison than the evils causing it. Yet I recognize that a prince can go to such excess, and place the well-being of the state in such danger, that the obligation to endure ceases. This is most rare, however, and the theologian who authorizes violence under this pretext should take care against excess; excess being infinitely more dangerous than deficiency.[150]In 1677, Leibniz called for a European confederation, governed by a council or senate, whose members would represent entire nations and would be free to vote their consciences;[151] this is sometimes considered an anticipation of the European Union. He believed that Europe would adopt a uniform religion. He reiterated these proposals in 1715.But at the same time, he arrived to propose an interreligious and multicultural project to create a universal system of justice, which required from him a broad interdisciplinary perspective. In order to propose it, he combined linguistics, especially sinology, moral and law philosophy, management, economics, and politics.[152]Leibniz devoted considerable intellectual and diplomatic effort to what would now be called ecumenical endeavor, seeking to reconcile first the Roman Catholic and Lutheran churches, and later the Lutheran and Reformed churches. In this respect, he followed the example of his early patrons, Baron von Boyneburg and the Duke John Frederick—both cradle Lutherans who converted to Catholicism as adults—who did what they could to encourage the reunion of the two faiths, and who warmly welcomed such endeavors by others. (The House of Brunswick remained Lutheran because the Duke's children did not follow their father.) These efforts included corresponding with the French bishop Jacques-Bénigne Bossuet, and involved Leibniz in some theological controversy. He evidently thought that the thoroughgoing application of reason would suffice to heal the breach caused by the Reformation.Leibniz the philologist was an avid student of languages, eagerly latching on to any information about vocabulary and grammar that came his way. He refuted the belief, widely held by Christian scholars in his day, that Hebrew was the primeval language of the human race. He also refuted the argument, advanced by Swedish scholars in his day, that a form of proto-Swedish was the ancestor of the Germanic languages. He puzzled over the origins of the Slavic languages and was fascinated by classical Chinese. Leibniz was also an expert in the Sanskrit language.[85]He published the princeps editio (first modern edition) of the late medieval Chronicon Holtzatiae, a Latin chronicle of the County of Holstein.Leibniz was perhaps the first major European intellectual to take a close interest in Chinese civilization, which he knew by corresponding with, and reading other works by, European Christian missionaries posted in China. Having read Confucius Sinarum Philosophus on the first year of its publication,[154] he concluded that Europeans could learn much from the Confucian ethical tradition. He mulled over the possibility that the Chinese characters were an unwitting form of his universal characteristic. He noted with fascination how the I Ching hexagrams correspond to the binary numbers from 000000 to 111111, and concluded that this mapping was evidence of major Chinese accomplishments in the sort of philosophical mathematics he admired.[155] Leibniz communicated his ideas of the binary system representing Christianity to the Emperor of China hoping it would convert him.[85] Leibniz may be the only major Western philosopher who attempted to accommodate Confucian ideas to prevailing European beliefs.[156]Leibniz's attraction to Chinese philosophy originates from his perception that Chinese philosophy was similar to his own.[154] The historian E.R. Hughes suggests that Leibniz's ideas of "simple substance" and "pre-established harmony" were directly influenced by Confucianism, pointing to the fact that they were conceived during the period that he was reading Confucius Sinarum Philosophus.[154]While making his grand tour of European archives to research the Brunswick family history that he never completed, Leibniz stopped in Vienna between May 1688 and February 1689, where he did much legal and diplomatic work for the Brunswicks. He visited mines, talked with mine engineers, and tried to negotiate export contracts for lead from the ducal mines in the Harz mountains. His proposal that the streets of Vienna be lit with lamps burning rapeseed oil was implemented. During a formal audience with the Austrian Emperor and in subsequent memoranda, he advocated reorganizing the Austrian economy, reforming the coinage of much of central Europe, negotiating a Concordat between the Habsburgs and the Vatican, and creating an imperial research library, official archive, and public insurance fund. He wrote and published an important paper on mechanics.Leibniz also wrote a short paper, Primae veritates, first published by Louis Couturat in 1903 (pp. 518–523)[157] summarizing his views on metaphysics. The paper is undated; that he wrote it while in Vienna in 1689 was determined only in 1999, when the ongoing critical edition finally published Leibniz's philosophical writings for the period 1677–90.[158] Couturat's reading of this paper was the launching point for much 20th-century thinking about Leibniz, especially among analytic philosophers. But after a meticulous study of all of Leibniz's philosophical writings up to 1688—a study the 1999 additions to the critical edition made possible—Mercer (2001) begged to differ with Couturat's reading; the jury is still out.When Leibniz died, his reputation was in decline. He was remembered for only one book, the Théodicée,[159] whose supposed central argument Voltaire lampooned in his popular book Candide, which concludes with the character Candide saying, "Non liquet" (it is not clear), a term that was applied during the Roman Republic to a legal verdict of "not proven". Voltaire's depiction of Leibniz's ideas was so influential that many believed it to be an accurate description. Thus Voltaire and his Candide bear some of the blame for the lingering failure to appreciate and understand Leibniz's ideas. Leibniz had an ardent disciple, Christian Wolff, whose dogmatic and facile outlook did Leibniz's reputation much harm. He also influenced David Hume who read his Théodicée and used some of his ideas.[160] In any event, philosophical fashion was moving away from the rationalism and system building of the 17th century, of which Leibniz had been such an ardent proponent. His work on law, diplomacy, and history was seen as of ephemeral interest. The vastness and richness of his correspondence went unrecognized.Much of Europe came to doubt that Leibniz had discovered calculus independently of Newton, and hence his whole work in mathematics and physics was neglected. Voltaire, an admirer of Newton, also wrote Candide at least in part to discredit Leibniz's claim to having discovered calculus and Leibniz's charge that Newton's theory of universal gravitation was incorrect.[citation needed]Leibniz's long march to his present glory began with the 1765 publication of the Nouveaux Essais, which Kant read closely. In 1768, Louis Dutens edited the first multi-volume edition of Leibniz's writings, followed in the 19th century by a number of editions, including those edited by Erdmann, Foucher de Careil, Gerhardt, Gerland, Klopp, and Mollat. Publication of Leibniz's correspondence with notables such as Antoine Arnauld, Samuel Clarke, Sophia of Hanover, and her daughter Sophia Charlotte of Hanover, began.In 1900, Bertrand Russell published a critical study of Leibniz's metaphysics.[161] Shortly thereafter, Louis Couturat published an important study of Leibniz, and edited a volume of Leibniz's heretofore unpublished writings, mainly on logic. They made Leibniz somewhat respectable among 20th-century analytical and linguistic philosophers in the English-speaking world (Leibniz had already been of great influence to many Germans such as Bernhard Riemann). For example, Leibniz's phrase salva veritate, meaning interchangeability without loss of or compromising the truth, recurs in Willard Quine's writings. Nevertheless, the secondary literature on Leibniz did not really blossom until after World War II. This is especially true of English speaking countries; in Gregory Brown's bibliography fewer than 30 of the English language entries were published before 1946. American Leibniz studies owe much to Leroy Loemker (1904–1985) through his translations and his interpretive essays in LeClerc (1973).Nicholas Jolley has surmised that Leibniz's reputation as a philosopher is now perhaps higher than at any time since he was alive.[162] Analytic and contemporary philosophy continue to invoke his notions of identity, individuation, and possible worlds. Work in the history of 17th- and 18th-century ideas has revealed more clearly the 17th-century "Intellectual Revolution" that preceded the better-known Industrial and commercial revolutions of the 18th and 19th centuries.In 1985, the German government created the Leibniz Prize, offering an annual award of 1.55 million euros for experimental results and 770,000 euros for theoretical ones. It was the worlds largest prize for scientific achievement prior to the Fundamental Physics Prize.The collection of manuscript papers of Leibniz at the Gottfried Wilhelm Leibniz Bibliothek – Niedersächische Landesbibliothek were inscribed on UNESCO's Memory of the World Register in 2007.[163]Leibniz mainly wrote in three languages: scholastic Latin, French and German. During his lifetime, he published many pamphlets and scholarly articles, but only two "philosophical" books, the Combinatorial Art and the Théodicée. (He published numerous pamphlets, often anonymous, on behalf of the House of Brunswick-Lüneburg, most notably the "De jure suprematum" a major consideration of the nature of sovereignty.) One substantial book appeared posthumously, his Nouveaux essais sur l'entendement humain, which Leibniz had withheld from publication after the death of John Locke. Only in 1895, when Bodemann completed his catalogue of Leibniz's manuscripts and correspondence, did the enormous extent of Leibniz's Nachlass become clear: about 15,000 letters to more than 1000 recipients plus more than 40,000 other items. Moreover, quite a few of these letters are of essay length. Much of his vast correspondence, especially the letters dated after 1700, remains unpublished, and much of what is published has been so only in recent decades. The amount, variety, and disorder of Leibniz's writings are a predictable result of a situation he described in a letter as follows:I cannot tell you how extraordinarily distracted and spread out I am. I am trying to find various things in the archives; I look at old papers and hunt up unpublished documents. From these I hope to shed some light on the history of the [House of] Brunswick. I receive and answer a huge number of letters. At the same time, I have so many mathematical results, philosophical thoughts, and other literary innovations that should not be allowed to vanish that I often do not know where to begin.[164]The extant parts of the critical edition[165] of Leibniz's writings are organized as follows:The systematic cataloguing of all of Leibniz's Nachlass began in 1901. It was hampered by two world wars and decades of German division in two states with the cold war's "iron curtain" in between, separating scholars, and also scattering portions of his literary estates. The ambitious project has had to deal with seven languages contained in some 200,000 pages of written and printed paper. In 1985 it was reorganized and included in a joint program of German federal and state (Länder) academies. Since then the branches in Potsdam, Münster, Hanover and Berlin have jointly published 57 volumes of the critical edition, with an average of 870 pages, and prepared index and concordance works.The year given is usually that in which the work was completed, not of its eventual publication.Meditationes de cognitione, veritate et ideisSix important collections of English translations are Wiener (1951), Parkinson (1966), Loemker (1969), Ariew and Garber (1989), Woolhouse and Francks (1998), and Strickland (2006). The ongoing critical edition of all of Leibniz's writings is Sämtliche Schriften und Briefe.[165]Leibniz is receiving popular attention. The Google Doodle for July 1, 2018 celebrated Leibniz's 372nd birthday.[167][168] [169] Using a quill, his hand is shown writing "Google" in binary ASCII code.One of the earliest popular but indirect exposition of Leibniz was Voltaire's satire Candide, published in 1759. Leibniz was lampooned as Professor Pangloss, described as "the greatest philosopher of the Holy Roman Empire".Leibniz also appears as one of the main historical figures in Neal Stephenson's series of novels The Baroque Cycle. Stephenson credits readings and discussions concerning Leibniz for inspiring him to write the series.[170]An updated bibliography of more than 25.000 titles is available at Leibniz Bibliographie.
Compressed sensing
Compressed sensing (also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Shannon-Nyquist sampling theorem. There are two conditions under which recovery is possible.[1] The first one is sparsity which requires the signal to be sparse in some domain. The second one is incoherence which is applied through the isometric property which is sufficient for sparse signals.[2][3]A common goal of the engineering field of signal processing is to reconstruct a signal from a series of sampling measurements. In general, this task is impossible because there is no way to reconstruct a signal during the times that the signal is not measured. Nevertheless, with prior knowledge or assumptions about the signal, it turns out to be possible to perfectly reconstruct a signal from a series of measurements (acquiring this series of measurements is called sampling). Over time, engineers have improved their understanding of which assumptions are practical and how they can be generalized.An early breakthrough in signal processing was the Nyquist–Shannon sampling theorem. It states that if a real signal's highest frequency is less than half of the sampling rate (or less than the sampling rate, if the signal is complex), then the signal can be reconstructed perfectly by means of sinc interpolation. The main idea is that with prior knowledge about constraints on the signal's frequencies, fewer samples are needed to reconstruct the signal.Around 2004, Emmanuel Candès, Justin Romberg, Terence Tao, and David Donoho proved that given knowledge about a signal's sparsity, the signal may be reconstructed with even fewer samples than the sampling theorem requires.[4][5] This idea is the basis of compressed sensing.At first glance, compressed sensing might seem to violate the sampling theorem, because compressed sensing depends on the sparsity of the signal in question and not its highest frequency. This is a misconception, because the sampling theorem guarantees perfect reconstruction given sufficient, not necessary, conditions. A sampling method fundamentally different from classical fixed-rate sampling cannot "violate" the sampling theorem. Sparse signals with high frequency components can be highly under-sampled using compressed sensing compared to classical fixed-rate sampling.[10]In order to choose a solution to such a system, one must impose extra constraints or conditions (such as smoothness) as appropriate. In compressed sensing, one adds the constraint of sparsity, allowing only solutions which have a small number of nonzero coefficients. Not all underdetermined systems of linear equations have a sparse solution. However, if there is a unique sparse solution to the underdetermined system, then the compressed sensing framework allows the recovery of that solution.Compressed sensing takes advantage of the redundancy in many interesting signals—they are not pure noise. In particular, many signals are sparse, that is, they contain many coefficients close to or equal to zero, when represented in some domain.[11] This is the same insight used in many forms of lossy compression.Compressed sensing typically starts with taking a weighted linear combination of samples also called compressive measurements in a basis different from the basis in which the signal is known to be sparse. The results found by  Emmanuel Candès, Justin Romberg,  Terence Tao and  David Donoho, showed that the number of these compressive measurements can be small and still contain nearly all the useful information. Therefore, the task of converting the image back into the intended domain involves solving an underdetermined matrix equation since the number of compressive measurements taken is smaller than the number of pixels in the full image. However, adding the constraint that the initial signal is sparse enables one to solve this underdetermined system of linear equations.Total variation can be seen as a non-negative real-valued functional defined on the space of real-valued functions (for the case of functions of one variable) or on the space of integrable functions (for the case of functions of several variables). For signals, especially, total variation refers to the integral of the absolute gradient of the signal. In signal and image reconstruction, it is applied as total variation regularization where the underlying principle is that signals with excessive details have high total variation and that removing these details, while retaining important information such as edges, would reduce the total variation of the signal and make the signal subject closer to the original signal in the problem.Recent progress on this problem involves using an iteratively directional TV refinement for CS reconstruction.[15] This method would have 2 stages: the first stage would estimate and refine the initial orientation field - which is defined as a noisy point-wise initial estimate, through edge-detection, of the given image. In the second stage, the CS reconstruction model is presented by utilizing directional TV regularizer. More details about these TV-based approaches - iteratively reweighted l1 minimization, edge-preserving TV and iterative model using directional orientation field and TV- are provided below.Early iterations may find inaccurate sample estimates, however this method will down-sample these at a later stage to give more weight to the smaller non-zero signal estimates. One of the disadvantages is the need for defining a valid starting point as a global minimum might not be obtained every time due to the concavity of the function. Another disadvantage is that this method tends to uniformly penalize the image gradient irrespective of the underlying image structures. This causes over-smoothing of edges, especially those of low contrast regions, subsequently leading to loss of low contrast information. The advantages of this method include: reduction of the sampling rate for sparse signals; reconstruction of the image while being robust to the removal of noise and other artifacts; and use of very few iterations. This can also help in recovering images with sparse gradients.Some of the disadvantages of this method are the absence of smaller structures in the reconstructed image and degradation of image resolution. This edge preserving TV algorithm, however, requires fewer iterations than the conventional TV algorithm.[14] Analyzing the horizontal and vertical intensity profiles of the reconstructed images, it can be seen that there are sharp jumps at edge points and negligible, minor fluctuation at non-edge points. Thus, this method leads to low relative error and higher correlation as compared to the TV method. It also effectively suppresses and removes any form of image noise and image artifacts such as streaking.To overcome this drawback, a refined orientation model is defined in which the data term reduces the effect of noise and improves accuracy while the second penalty term with the L2-norm is a fidelity term which ensures accuracy of initial coarse estimation.For the orientation field refinement model, the Lagrangian multipliers are updated in the iterative process as follows:For the iterative directional total variation refinement model, the Lagrangian multipliers are updated as follows:Based on Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics and known ground-truth images for testing performance, it is concluded that iterative directional total variation has a better reconstructed performance than the non-iterative methods in preserving edge and texture areas. The orientation field refinement model plays a major role in this improvement in performance as it increases the number of directionless pixels in the flat area while enhancing the orientation field consistency in the regions with edges.The field of compressive sensing is related to several topics in signal processing and computational mathematics, such as underdetermined linear-systems, group testing, heavy hitters, sparse coding, multiplexing, sparse sampling, and finite rate of innovation. Its broad scope and generality has enabled several innovative CS-enhanced approaches in signal processing and compression, solution of inverse problems, design of radiating systems, radar and through-the-wall imaging, and antenna characterization.[22]  Imaging techniques having a strong affinity with compressive sensing include coded aperture and computational photography. Implementations of compressive sensing in hardware at different technology readiness levels is available.[23]Compressed sensing is used in a mobile phone camera sensor. The approach allows a reduction in image acquisition energy per image by as much as a factor of 15 at the cost of complex decompression algorithms; the computation may require an off-device implementation.[26]Compressed sensing is used in single-pixel cameras from Rice University.[27] Bell Labs employed the technique in a lensless single-pixel camera that takes stills using repeated snapshots of randomly chosen apertures from a grid. Image quality improves with the number of snapshots, and generally requires a small fraction of the data of conventional imaging, while eliminating lens/focus-related aberrations.[28][29]Compressed sensing can be used to improve image reconstruction in holography by increasing the number of voxels one can infer from a single hologram.[30][31][32] It is also used for image retrieval from undersampled measurements in optical [33][34] and millimeter-wave [35] holography.Compressed sensing is being used in facial recognition applications.[36]Compressed sensing has been used [37][38]  to shorten magnetic resonance imaging scanning sessions on conventional hardware.[39][40][41] Reconstruction methods includeCompressed sensing addresses the issue of high scan time by enabling faster acquisition by measuring fewer Fourier coefficients. This produces a high-quality image with relatively lower scan time. Another application (also discussed ahead) is for CT reconstruction with fewer X-ray projections. Compressed sensing, in this case, removes the high spatial gradient parts - mainly, image noise and artifacts. This holds tremendous potential as one can obtain high-resolution CT images at low radiation doses (through lower current-mA settings).[45]Compressed sensing has showed outstanding results in the application of network tomography to network management. Network delay estimation and network congestion detection can both be modeled as underdetermined systems of linear equations where the coefficient matrix is the network routing matrix. Moreover, in the Internet, network routing matrices usually satisfy the criterion for using compressed sensing.[46]Commercial shortwave-infrared cameras based upon compressed sensing are available.[47] These cameras have light sensitivity from 0.9 µm to 1.7 µm, which are wavelengths invisible to the human eye.In the field of radio astronomy, compressed sensing has been proposed for deconvolving an interferometric image.[48] In fact, the Högbom CLEAN algorithm that has been in use for the deconvolution of radio images since 1974, is similar to compressed sensing's matching pursuit algorithm.Compressed sensing combined with a moving aperture has been used to increase the acquisition rate of images in a transmission electron microscope.[49] In scanning mode, compressive sensing combined with random scanning of the electron beam has enabled both faster acquisition and less electron dose, which allows for imaging of electron beam sensitive materials.[50]
Plane (geometry)
In mathematics, a plane is a flat, two-dimensional surface that extends infinitely far. A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. Planes can arise as subspaces of some higher-dimensional space, as with a room's walls extended infinitely far, or they may enjoy an independent existence in their own right, as in the setting of Euclidean geometry.When working exclusively in two-dimensional Euclidean space, the definite article is used, so, the plane refers to the whole space. Many fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional space, or, in other words, in the plane.Euclid set forth the first great landmark of mathematical thought, an axiomatic treatment of geometry.[1] He selected a small core of undefined terms (called common notions) and postulates (or axioms) which he then used to prove various geometrical statements. Although the plane in its modern sense is not directly given a definition anywhere in the Elements, it may be thought of as part of the common notions.[2] Euclid never used numbers to measure length, angle, or area. In this way the Euclidean plane is not quite the same as the Cartesian plane.A plane is a ruled surface.This section is solely concerned with planes embedded in three dimensions: specifically, in R3.In a Euclidean space of any number of dimensions, a plane is uniquely determined by any of the following:The following statements hold in three-dimensional Euclidean space but not in higher dimensions, though they have higher-dimensional analogues:In a manner analogous to the way lines in a two-dimensional space are described using a point-slope form for their equations, planes in a three dimensional space have a natural description using a point in the plane and a vector orthogonal to it (the normal vector) to indicate its "inclination".Specifically, let r0 be the position vector of some point P0 = (x0, y0, z0), and let n = (a, b, c) be a nonzero vector. The plane determined by the point P0 and the vector n consists of those points P, with position vector r, such that the vector drawn from P0 to P is perpendicular to n. Recalling that two vectors are perpendicular if and only if their dot product is zero, it follows that the desired plane can be described as the set of all points r such that(The dot here means a dot (scalar) product.)Expanded this becomeswhich is the point-normal form of the equation of a plane.[3] This is just a linear equationwhereConversely, it is easily shown that if a, b, c and d are constants and a, b, and c are not all zero, then the graph of the equationis a plane having the vector n = (a, b, c) as a normal.[4] This familiar equation for a plane is called the general form of the equation of the plane.[5]Thus for example a regression equation of the form y = d + ax + cz (with b = −1) establishes a best-fit plane in three-dimensional space when there are two explanatory variables.Alternatively, a plane may be described parametrically as the set of all points of the formwhere s and t range over all real numbers, v and w are given linearly independent vectors defining the plane, and r0 is the vector representing the position of an arbitrary (but fixed) point on the plane. The vectors v and w can be visualized as vectors starting at r0 and pointing in different directions along the plane. Note that v and w can be perpendicular, but cannot be parallel.Let p1=(x1, y1, z1), p2=(x2, y2, z2), and p3=(x3, y3, z3) be non-collinear points.The plane passing through p1, p2, and p3 can be described as the set of all points (x,y,z) that satisfy the following determinant equations:This system can be solved using Cramer's rule and basic matrix manipulations. LetIf D is non-zero (so for planes not through the origin) the values for a, b and c can be calculated as follows:These equations are parametric in d. Setting d equal to any non-zero number and substituting it into these equations will yield one solution set.This plane can also be described by the "point and a normal vector" prescription above. A suitable normal vector is given by the cross productand the point r0 can be taken to be any of the given points p1,p2 or p3[6] (or any other point in the plane).Another vector form for the equation of a plane, known as the Hesse normal form relies on the parameter D. This form is:[5]whereIn addition to its familiar geometric structure, with isomorphisms that are isometries with respect to the usual inner product, the plane may be viewed at various other levels of abstraction. Each level of abstraction corresponds to a specific category.At one extreme, all geometrical and metric concepts may be dropped to leave the topological plane, which may be thought of as an idealized homotopically trivial infinite rubber sheet, which retains a notion of proximity, but has no distances. The topological plane has a concept of a linear path, but no concept of a straight line. The topological plane, or its equivalent the open disc, is the basic topological neighborhood used to construct surfaces (or 2-manifolds) classified in low-dimensional topology. Isomorphisms of the topological plane are all continuous bijections. The topological plane is the natural context for the branch of graph theory that deals with planar graphs, and results such as the four color theorem.The plane may also be viewed as an affine space, whose isomorphisms are combinations of translations and non-singular linear maps. From this viewpoint there are no distances, but collinearity and ratios of distances on any line are preserved.Differential geometry views a plane as a 2-dimensional real manifold, a topological plane which is provided with a differential structure. Again in this case, there is no notion of distance, but there is now a concept of smoothness of maps, for example a differentiable or smooth path (depending on the type of differential structure applied). The isomorphisms in this case are bijections with the chosen degree of differentiability.In the opposite direction of abstraction, we may apply a compatible field structure to the geometric plane, giving rise to the complex plane and the major area of complex analysis. The complex field has only two isomorphisms that leave the real line fixed, the identity and conjugation.In the same way as in the real case, the plane may also be viewed as the simplest, one-dimensional (over the complex numbers) complex manifold, sometimes called the complex line. However, this viewpoint contrasts sharply with the case of the plane as a 2-dimensional real manifold. The isomorphisms are all conformal bijections of the complex plane, but the only possibilities are maps that correspond to the composition of a multiplication by a complex number and a translation.In addition, the Euclidean geometry (which has zero curvature everywhere) is not the only geometry that the plane may have. The plane may be given a spherical geometry by using the stereographic projection. This can be thought of as placing a sphere on the plane (just like a ball on the floor), removing the top point, and projecting the sphere onto the plane from this point). This is one of the projections that may be used in making a flat map of part of the Earth's surface. The resulting geometry has constant positive curvature.Alternatively, the plane can also be given a metric which gives it constant negative curvature giving the hyperbolic plane. The latter possibility finds an application in the theory of special relativity in the simplified case where there are two spatial dimensions and one time dimension. (The hyperbolic plane is a timelike hypersurface in three-dimensional Minkowski space.)The one-point compactification of the plane is homeomorphic to a sphere (see stereographic projection); the open disk is homeomorphic to a sphere with the "north pole" missing; adding that point completes the (compact) sphere. The result of this compactification is a manifold referred to as the Riemann sphere or the complex projective line. The projection from the Euclidean plane to a sphere without a point is a diffeomorphism and even a conformal map.The plane itself is homeomorphic (and diffeomorphic) to an open disk. For the hyperbolic plane such diffeomorphism is conformal, but for the Euclidean plane it is not.
Woodbury matrix identity
In mathematics (specifically linear algebra), the Woodbury matrix identity, named after Max A. Woodbury[1][2] says that the inverse of a rank-k correction of some matrix can be computed by doing a rank-k correction to the inverse of the original matrix. Alternative names for this formula are the matrix inversion lemma, Sherman–Morrison–Woodbury formula or just Woodbury formula. However, the identity appeared in several papers before the Woodbury report.[3]The Woodbury matrix identity is[4]where A, U, C and V all denote matrices of the correct (conformable) sizes. Specifically, A is n-by-n, U is n-by-k, C is k-by-k and V is k-by-n. This can be derived using blockwise matrix inversion.For a more general formula for which the matrix C need not be invertible or even square, see Binomial inverse theorem.First consider these useful identities,Now, Deriving the Woodbury matrix identity is easily done by solving the following block matrix inversion problemExpanding, we can see that the above reduces to We have derived the Woodbury matrix identity.We start by the matrixBy eliminating the entry under the A (given that A is invertible) we getLikewise, eliminating the entry above C givesNow combining the above two, we getMoving to the right side giveswhich is the LDU decomposition of the block matrix into an upper triangular, diagonal, and lower triangular matrices.Now inverting both sides givesWe could equally well have done it the other way (provided that C is invertible) i.e.Now again inverting both sides,Now comparing elements (1,1) of the RHS of (1) and (2) above gives the Woodbury formulaThis identity is useful in certain numerical computations where A−1 has already been computed and it is desired to compute (A + UCV)−1.  With the inverse of A available, it is only necessary to find the inverse of C−1 + VA−1U in order to obtain the result using the right-hand side of the identity.  If C has a much smaller dimension than A, this is more efficient than inverting A + UCV directly. A common case is finding the inverse of a low-rank update A + UCV of A (where U only has a few columns and V only a few rows), or finding an approximation of the inverse of the matrix A + B where the matrix B can be approximated by a low-rank matrix UCV, for example using the singular value decomposition.This is applied, e.g., in the Kalman filter and recursive least squares methods, to replace the parametric solution, requiring inversion of a state vector sized matrix, with a condition equations based solution. In case of the Kalman filter this matrix has the dimensions of the vector of observations, i.e., as small as 1 in case only one new observation is processed at a time. This significantly speeds up the often real time calculations of the filter.The binomial inverse theorem is a more general form of the Woodbury matrix identity.If A, U, B, V are matrices of sizes p×p, p×q, q×q, q×p, respectively, thenprovided A and B + BVA−1UB are nonsingular. Nonsingularity of the latter requires that B−1 exist since it equals B(I+VA-1UB) and the rank of the latter cannot exceed the rank of B.[5]Since B is invertible, the two B terms flanking the parenthetical quantity inverse in the right-hand side can be replaced with (B−1)−1, which results inThis is the Woodbury matrix identity, which can also be derived using matrix blockwise inversion.A more general formula exists when B is singular and possibly even non-square:[5]Formulas also exist for certain cases in which A is singular.[6]First notice that Now multiply the matrix we wish to invert by its alleged inverse:which verifies that it is the inverse.If p = q and U = V = Ip is the identity matrix, thenRemembering the identitywe can also express the previous equation in the simpler form asContinuing with the merging of the terms of the far right-hand side of the above equation results in Hua's identityAnother useful form of the same identity iswhich has a recursive structure that yieldsThis form can be used in perturbative expansions where B is a perturbation of A.If B = Iq is the identity matrix and q = 1, then U is a column vector, written u, and V is a row vector, written vT.  Then the theorem implies the Sherman-Morrison formula:This is useful if one has a matrix A with a known inverse A−1 and one needs to invert matrices of the form A+uvT quickly for various u and v.If we set A = Ip and B = Iq, we get In particular, if q = 1, thenwhich is a particular case of the Sherman-Morrison formula given above.
Linear approximation
In mathematics, a linear approximation is an approximation of a general function using a linear function (more precisely, an affine function). They are widely used in the method of finite differences to produce first order methods for solving or approximating solutions to equations.In the more general case of Banach spaces, one hasGaussian optics is a technique in geometrical optics that describes the behaviour of light rays in optical systems by using the paraxial approximation, in which only rays which make small angles with the optical axis of the system are considered.[2] In this approximation, trigonometric functions can be expressed as linear functions of the angles. Gaussian optics applies to systems in which all the optical surfaces are either flat or are portions of a sphere. In this case, simple explicit formulae can be given for parameters of an imaging system such as focal distance, magnification and brightness, in terms of the geometrical shapes and material properties of the constituent elements.The period of swing of a simple gravity pendulum depends on its length, the local strength of gravity, and to a small extent on the maximum angle that the pendulum swings away from vertical, θ0, called the amplitude.[3]  It is independent of the mass of the bob. The true period T of a simple pendulum, the time taken for a complete cycle of an ideal simple gravity pendulum, can be written in several different forms (see Pendulum (mathematics) ), one example being the infinite series:[4][5]where L is the length of the pendulum and g is the local acceleration of gravity.However, if one takes the linear approximation (i.e. if the amplitude is limited to small swings,[Note 1] ) the period is:[6]In the linear approximation, the period of swing is approximately the same for different size swings: that is, the period is independent of amplitude. This property, called isochronism, is the reason pendulums are so useful for timekeeping.[7]  Successive swings of the pendulum, even if changing in amplitude, take the same amount of time.The electrical resistivity of most materials changes with temperature. If the temperature T does not vary too much, a linear approximation is typically used:
Homogeneous coordinates
In mathematics, homogeneous coordinates or projective coordinates, introduced by August Ferdinand Möbius in his 1827 work Der barycentrische Calcül,[1][2] are a system of coordinates used in projective geometry, as Cartesian coordinates are used in Euclidean geometry. They have the advantage that the coordinates of points, including points at infinity, can be represented using finite coordinates. Formulas involving homogeneous coordinates are often simpler and more symmetric than their Cartesian counterparts. Homogeneous coordinates have a range of applications, including computer graphics and 3D computer vision, where they allow affine transformations and, in general, projective transformations to be easily represented by a matrix.If the homogeneous coordinates of a point are multiplied by a non-zero scalar then the resulting coordinates represent the same point. Since homogeneous coordinates are also given to points at infinity, the number of coordinates required to allow this extension is one more than the dimension of the projective space being considered. For example, two homogeneous coordinates are required to specify a point on the projective line and three homogeneous coordinates are required to specify a point in the projective plane.The real projective plane can be thought of as the Euclidean plane with additional points added, which are called points at infinity, and are considered to lie on a new line, the line at infinity. There is a point at infinity corresponding to each direction (numerically given by the slope of a line), informally defined as the limit of a point that moves in that direction away from the origin. Parallel lines in the Euclidean plane are said to intersect at a point at infinity corresponding to their common direction. Given a point (x, y) on the Euclidean plane, for any non-zero real number Z, the triple (xZ, yZ, Z) is called a set of homogeneous coordinates for the point. By this definition, multiplying the three homogeneous coordinates by a common, non-zero factor gives a new set of homogeneous coordinates for the same point. In particular, (x, y, 1) is such a system of homogeneous coordinates for the point (x, y).For example, the Cartesian point (1, 2) can be represented in homogeneous coordinates as (1, 2, 1) or (2, 4, 2). The original Cartesian coordinates are recovered by dividing the first two positions by the third. Thus unlike Cartesian coordinates, a single point can be represented by infinitely many homogeneous coordinates.The equation of a line through the origin (0, 0) may be written nx + my = 0 where n and m are not both 0. In parametric form this can be written x = mt, y = −nt. Let Z = 1/t, so the coordinates of a point on the line may be written (m/Z,  −n/Z). In homogeneous coordinates this becomes (m,  −n, Z). In the limit, as t approaches infinity, in other words, as the point moves away from the origin, Z approaches 0 and the homogeneous coordinates of the point become (m, −n, 0). Thus we define (m, −n, 0) as the homogeneous coordinates of the point at infinity corresponding to the direction of the line nx + my = 0. As any line of the Euclidean plane is parallel to a line passing through the origin, and since parallel lines have the same point at infinity, the infinite point on every line of the Euclidean plane has been given homogeneous coordinates.To summarize:Note that the triple (0, 0, 0) is omitted and does not represent any point. The origin is represented by (0, 0, 1).[3]Some authors use different notations for homogeneous coordinates which help distinguish them from Cartesian coordinates. The use of colons instead of commas, for example (x:y:z) instead of (x, y, z), emphasizes that the coordinates are to be considered ratios.[4] Square brackets, as in [x, y, z] emphasize that multiple sets of coordinates are associated with a single point.[5] Some authors use a combination of colons and square brackets, as in [x:y:z].[6]The discussion in the preceding section applies analogously to projective spaces other than the plane. So the points on the projective line may be represented by pairs of coordinates (x, y), not both zero. In this case, the point at infinity is (1, 0). Similarly the points in projective n-space are represented by (n + 1)-tuples.[7]The use of real numbers gives the homogeneous coordinates of points in the classical case of the real projective spaces, however any field may be used, in particular, the complex numbers may be used for complex projective space. For example, the complex projective line uses two homogeneous complex coordinates and is known as the Riemann sphere. Other fields, including finite fields, can be used.Homogeneous coordinates for projective spaces can also be created with elements from a division ring (skewfield). However, in this case, care must be taken to account for the fact that multiplication may not be commutative.[8]Another definition of the real projective plane can be given in terms of equivalence classes. For non-zero elements of R3, define (x1, y1, z1) ~ (x2, y2, z2) to mean there is a non-zero λ so that (x1, y1, z1) = (λx2, λy2, λz2). Then ~ is an equivalence relation and the projective plane can be defined as the equivalence classes of R3 ∖ {0}. If (x, y, z) is one of the elements of the equivalence class p then these are taken to be homogeneous coordinates of p.Lines in this space are defined to be sets of solutions of equations of the form ax + by + cz = 0 where not all of a, b and c are zero. The condition ax + by + cz = 0 depends only on the equivalence class of (x, y, z) so the equation defines a set of points in the projective plane. The mapping (x, y) → (x, y, 1) defines an inclusion from the Euclidean plane to the projective plane and the complement of the image is the set of points with z = 0. This is the equation of a line according to the definition and the complement is called the line at infinity.The equivalence classes, p, are the lines through the origin with the origin removed. The origin does not really play an essential part in the previous discussion so it can be added back in without changing the properties of the projective plane. This produces a variation on the definition, namely the projective plane is defined as the set of lines in R3 that pass through the origin and the coordinates of a non-zero element (x, y, z) of a line are taken to be homogeneous coordinates of the line. These lines are now interpreted as points in the projective plane.Again, this discussion applies analogously to other dimensions. So the projective space of dimension n can be defined as the set of lines through the origin in Rn+1.[9]Homogeneous coordinates are not uniquely determined by a point, so a function defined on the coordinates, say f(x, y, z), does not determine a function defined on points as with Cartesian coordinates. But a condition f(x, y, z) = 0 defined on the coordinates, as might be used to describe a curve, determines a condition on points if the function is homogeneous. Specifically, suppose there is a k such thatIf a set of coordinates represent the same point as (x, y, z) then it can be written (λx, λy, λz) for some non-zero value of λ. ThenA polynomial g(x, y) of degree k can be turned into a homogeneous polynomial by replacing x with x/z, y with y/z and multiplying by zk, in other words by definingThe resulting function f is a polynomial so it makes sense to extend its domain to triples where z = 0. The process can be reversed by setting z = 1, orThe equation f(x, y, z) = 0 can then be thought of as the homogeneous form of g(x, y) = 0 and it defines the same curve when restricted to the Euclidean plane. For example, the homogeneous form of the equation of the line ax + by + c = 0 is ax + by + cz = 0.[10]The equation of a line in the projective plane may be given as sx + ty + uz = 0 where s, t and u are constants. Each triple (s, t, u) determines a line, the line determined is unchanged if it is multiplied by a non-zero scalar, and at least one of s, t and u must be non-zero. So the triple (s, t, u) may be taken to be homogeneous coordinates of a line in the projective plane, that is line coordinates as opposed to point coordinates. If in sx + ty + uz = 0 the letters s, t and u are taken as variables and x, y and z are taken as constants then the equation becomes an equation of a set of lines in the space of all lines in the plane. Geometrically it represents the set of lines that pass through the point (x, y, z) and may be interpreted as the equation of the point in line-coordinates. In the same way, planes in 3-space may be given sets of four homogeneous coordinates, and so on for higher dimensions.[11]The same relation, sx + ty + uz = 0, may be regarded as either the equation of a line or the equation of a point. In general, there is no difference either algebraically or logically between the homogeneous coordinates of points and lines. So plane geometry with points as the fundamental elements and plane geometry with lines as the fundamental elements are equivalent except for interpretation. This leads to the concept of duality in projective geometry, the principle that the roles of points and lines can be interchanged in a theorem in projective geometry and the result will also be a theorem. Analogously, the theory of points in projective 3-space is dual to the theory of planes in projective 3-space, and so on for higher dimensions.[12]Assigning coordinates to lines in projective 3-space is more complicated since it would seem that a total of 8 coordinates, either the coordinates of two points which lie on the line or two planes whose intersection is the line, are required. A useful method, due to Julius Plücker, creates a set of six coordinates as the determinants xiyj − xjyi (1 ≤ i < j ≤ 4) from the homogeneous coordinates of two points (x1, x2, x3, x4) and (y1, y2, y3, y4) on the line. The Plücker embedding is the generalization of this to create homogeneous coordinates of elements of any dimension m in a projective space of dimension n.[13][14]Bézout's theorem predicts that the number of points of intersection of two curves is equal to the product of their degrees (assuming an algebraically closed field and with certain conventions followed for counting intersection multiplicities). Bézout's theorem predicts there is one point of intersection of two lines and in general this is true, but when the lines are parallel the point of intersection is infinite. Homogeneous coordinates are used to locate the point of intersection in this case. Similarly, Bézout's theorem predicts that a line will intersect a conic at two points, but in some cases one or both of the points is infinite and homogeneous coordinates must be used to locate them. For example, y = x2 and x = 0 have only one point of intersection in the finite (affine) plane. To find the other point of intersection, convert the equations into homogeneous form, yz = x2 and x = 0. This produces x = yz = 0 and, assuming not all of x, y and z are 0, the solutions are x = y = 0, z ≠ 0 and x = z = 0, y ≠ 0. This first solution is the point (0, 0) in Cartesian coordinates, the finite point of intersection. The second solution gives the homogeneous coordinates (0, 1, 0) which corresponds to the direction of the y-axis. For the equations xy = 1 and x = 0 there are no finite points of intersection. Converting the equations into homogeneous form gives xy = z2 and x = 0. Solving produces the equation z2 = 0 which has a double root at z = 0. From the original equation, x = 0, so y ≠ 0 since at least one coordinate must be non-zero. Therefore, (0, 1, 0) is the point of intersection counted with multiplicity 2 in agreement with the theorem.[15]The homogeneous form for the equation of a circle in the real or complex projective plane is x2 + y2 + 2axz + 2byz + cz2 = 0. The intersection of this curve with the line at infinity can be found by setting z = 0. This produces the equation x2 + y2 = 0 which has two solutions over the complex numbers, giving rise to the points with homogeneous coordinates (1, i, 0) and (1, −i, 0) in the complex projective plane. These points are called the circular points at infinity and can be regarded as the common points of intersection of all circles. This can be generalized to curves of higher order as circular algebraic curves.[16]Just as the selection of axes in the Cartesian coordinate system is somewhat arbitrary, the selection of a single system of homogeneous coordinates out of all possible systems is somewhat arbitrary. Therefore, it is useful to know how the different systems are related to each other.Let (x, y, z) be the homogeneous coordinates of a point in the projective plane. A fixed matrixwith nonzero determinant, defines a new system of coordinates (X, Y, Z) by the equationMultiplication of (x, y, z) by a scalar results in the multiplication of (X, Y, Z) by the same scalar, and X, Y and Z cannot be all 0 unless x, y and z are all zero since A is nonsingular. So (X, Y, Z) are a new system of homogeneous coordinates for the same point of the projective plane.Möbius' original formulation of homogeneous coordinates specified the position of a point as the center of mass (or barycenter) of a system of three point masses placed at the vertices of a fixed triangle. Points within the triangle are represented by positive masses and points outside the triangle are represented by allowing negative masses. Multiplying the masses in the system by a scalar does not affect the center of mass, so this is a special case of a system of homogeneous coordinates.Let l, m, n be three lines in the plane and define a set of coordinates X, Y and Z of a point p as the signed distances from p to these three lines. These are called the trilinear coordinates of p with respect to the triangle whose vertices are the pairwise intersections of the lines. Strictly speaking these are not homogeneous, since the values of X, Y and Z are determined exactly, not just up to proportionality. There is a linear relationship between them however, so these coordinates can be made homogeneous by allowing multiples of (X, Y, Z) to represent the same point. More generally, X, Y and Z can be defined as constants p, r and q times the distances to l, m and n, resulting in a different system of homogeneous coordinates with the same triangle of reference. This is, in fact, the most general type of system of homogeneous coordinates for points in the plane if none of the lines is the line at infinity.[17]Homogeneous coordinates are ubiquitous in computer graphics because they allow common vector operations such as translation, rotation, scaling and perspective projection to be represented as a matrix by which the vector is multiplied. By the chain rule, any sequence of such operations can be multiplied out into a single matrix, allowing simple and efficient processing. By contrast, using Cartesian coordinates, translations and perspective projection cannot be expressed as matrix multiplications, though other operations can. Modern OpenGL and Direct3D graphics cards take advantage of homogeneous coordinates to implement a vertex shader efficiently using vector processors with 4-element registers.[18][19]For example, in perspective projection, a position in space is associated with the line from it to a fixed point called the center of projection. The point is then mapped to a plane by finding the point of intersection of that plane and the line. This produces an accurate representation of how a three-dimensional object appears to the eye. In the simplest situation, the center of projection is the origin and points are mapped to the plane z = 1, working for the moment in Cartesian coordinates. For a given point in space, (x, y, z), the point where the line and the plane intersect is (x/z, y/z, 1). Dropping the now superfluous z coordinate, this becomes (x/z, y/z). In homogeneous coordinates, the point (x, y, z) is represented by (xw, yw, zw, w) and the point it maps to on the plane is represented by  (xw, yw, zw), so projection can be represented in matrix form as[clarification needed]Matrices representing other geometric transformations can be combined with this and each other by matrix multiplication. As a result, any perspective projection of space can be represented as a single matrix.[20][21]
Euclidean vector
A vector is what is needed to "carry" the point A to the point B; the Latin word vector means "carrier".[4] It was first used by 18th century astronomers investigating planet rotation around the Sun.[5]  The magnitude of the vector is the distance between the two points and the direction refers to the direction of displacement from A to B. Many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. These operations and associated laws qualify Euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space.Vectors play an important role in physics: the velocity and acceleration of a moving object and the forces acting on it can all be described with vectors. Many other physical quantities can be usefully thought of as vectors. Although most of them do not represent distances (except, for example, position or displacement), their magnitude and direction can still be represented by the length and direction of an arrow. The mathematical representation of a physical vector depends on the coordinate system used to describe it. Other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include pseudovectors and tensors.The concept of vector, as we know it today, evolved gradually over a period of more than 200 years. About a dozen people made significant contributions.[6]Giusto Bellavitis abstracted the basic idea in 1835 when he established the concept of equipollence. Working in a Euclidean plane, he made equipollent any pair of line segments of the same length and orientation. Essentially he realized an equivalence relation on the pairs of points (bipoints) in the plane and thus erected the first space of vectors in the plane.[6]:52–4The term vector was introduced by William Rowan Hamilton as part of a quaternion, which is a sum q = s + v of a Real number s (also called scalar) and a 3-dimensional vector. Like Bellavitis, Hamilton viewed vectors as representative of classes of equipollent directed segments. As complex numbers use an imaginary unit to complement the real line, Hamilton considered the vector v to be the imaginary part of a quaternion:Several other mathematicians developed vector-like systems in the middle of the nineteenth century,  including Augustin Cauchy, Hermann Grassmann, August Möbius, Comte de Saint-Venant, and Matthew O'Brien. Grassmann's 1840 work Theorie der Ebbe und Flut (Theory of the Ebb and Flow) was the first system of spatial analysis similar to today's system and had ideas corresponding to the cross product, scalar product and vector differentiation. Grassmann's work was largely neglected until the 1870s.[6]Peter Guthrie Tait carried the quaternion standard after Hamilton. His 1867 Elementary Treatise of Quaternions included extensive treatment of the nabla or del operator ∇.In 1878 Elements of Dynamic was published by William Kingdon Clifford. Clifford simplified the quaternion study by isolating the dot product and cross product of two vectors from the complete quaternion product. This approach made vector calculations available to engineers and others working in three dimensions and skeptical of the fourth.Josiah Willard Gibbs, who was exposed to quaternions through James Clerk Maxwell's Treatise on Electricity and Magnetism, separated off their vector part for independent treatment. The first half of Gibbs's Elements of Vector Analysis, published in 1881, presents what is essentially the modern system of vector analysis.[6] In 1901 Edwin Bidwell Wilson published Vector Analysis, adapted from Gibb's lectures, which banished any mention of quaternions in the development of vector calculus.In physics and engineering, a vector is typically regarded as a geometric entity characterized by a magnitude and a direction. It is formally defined as a directed line segment, or arrow, in a Euclidean space.[8] In pure mathematics, a vector is defined more generally as any element of a vector space. In this context, vectors are abstract entities which may or may not be characterized by a magnitude and a direction. This generalized definition implies that the above-mentioned geometric entities are a special kind of vectors, as they are elements of a special kind of vector space called Euclidean space.This article is about vectors strictly defined as arrows in Euclidean space. When it becomes necessary to distinguish these special vectors from vectors as defined in pure mathematics, they are sometimes referred to as geometric, spatial, or Euclidean vectors.The term vector also has generalizations to higher dimensions and to more formal approaches with much wider applications.Since the physicist's concept of force has a direction and a magnitude, it may be seen as a vector. As an example, consider a rightward force F of 15 newtons. If the positive axis is also directed rightward, then F is represented by the vector 15 N, and if positive points leftward, then the vector for F is −15 N. In either case, the magnitude of the vector is 15 N. Likewise, the vector representation of a displacement Δs of 4 meters would be 4 m or −4 m, depending on its direction, and its magnitude would be 4 m regardless.Vectors are fundamental in the physical sciences. They can be used to represent any quantity that has magnitude, has direction, and which adheres to the rules of vector addition. An example is velocity, the magnitude of which is speed. For example, the velocity 5 meters per second upward could be represented by the vector (0, 5) (in 2 dimensions with the positive y-axis as 'up'). Another quantity represented by a vector is force, since it has a magnitude and direction and follows the rules of vector addition. Vectors also describe many other physical quantities, such as linear displacement, displacement, linear acceleration, angular acceleration, linear momentum, and angular momentum. Other physical vectors, such as the electric and magnetic field, are represented as a system of vectors at each point of a physical space; that is, a vector field. Examples of quantities that have magnitude and direction but fail to follow the rules of vector addition are angular displacement and electric current. Consequently, these are not vectors.In Cartesian coordinates a free vector may be thought of in terms of a corresponding bound vector, in this sense, whose initial point has the coordinates of the origin O = (0, 0, 0). It is then determined by the coordinates of that bound vector's terminal point. Thus the free vector represented by (1, 0, 0) is a vector of unit length pointing along the direction of the positive x-axis.This coordinate representation of free vectors allows their algebraic features to be expressed in a convenient numerical fashion. For example, the sum of the two (free) vectors (1, 2, 3) and (−2, 0, 4) is the (free) vectorIn the geometrical and physical settings, sometimes it is possible to associate, in a natural way, a length or magnitude and a direction to vectors. In addition, the notion of direction is strictly associated with the notion of an angle between two vectors. If the dot product of two vectors is defined—a scalar-valued product of two vectors—then it is also possible to define a length; the dot product gives a convenient algebraic characterization of both angle (a function of the dot product between any two non-zero vectors) and length (the square root of the dot product of a vector by itself). In three dimensions, it is further possible to define the cross product, which supplies an algebraic characterization of the area and orientation in space of the parallelogram defined by two vectors (used as sides of the parallelogram). In any dimension (and, in particular, higher dimensions), it's possible to define the exterior product, which (among other things) supplies an algebraic characterization of the area and orientation in space of the n-dimensional parallelotope defined by n vectors.However, it is not always possible or desirable to define the length of a vector in a natural way. This more general type of spatial vector is the subject of vector spaces (for free vectors) and affine spaces (for bound vectors, as each represented by an ordered pair of "points"). An important example is Minkowski space that is important to our understanding of special relativity, where there is a generalization of length that permits non-zero vectors to have zero length. Other physical examples come from thermodynamics, where many of the quantities of interest can be considered vectors in a space with no notion of length or angle.[10]In physics, as well as mathematics, a vector is often identified with a tuple of components, or list of numbers, that act as scalar coefficients for a set of basis vectors. When the basis is transformed, for example by rotation or stretching, then the components of any vector in terms of that basis also transform in an opposite sense. The vector itself has not changed, but the basis has, so the components of the vector must change to compensate. The vector is called covariant or contravariant depending on how the transformation of the vector's components is related to the transformation of the basis. In general, contravariant vectors are "regular vectors" with units of distance (such as a displacement) or distance times some other unit (such as velocity or acceleration); covariant vectors, on the other hand, have units of one-over-distance such as gradient. If you change units (a special case of a change of basis) from meters to millimeters, a scale factor of 1/1000, a displacement of 1 m becomes 1000 mm—a contravariant change in numerical value. In contrast, a gradient of 1 K/m becomes 0.001 K/mm—a covariant change in value. See covariance and contravariance of vectors. Tensors are another type of quantity that behave in this way; a vector is one type of tensor.In pure mathematics, a vector is any element of a vector space over some field and is often represented as a coordinate vector. The vectors described in this article are a very special case of this general definition because they are contravariant with respect to the ambient space. Contravariance captures the physical intuition behind the idea that a vector has "magnitude and direction".Vectors are usually shown in graphs or other diagrams as arrows (directed line segments), as illustrated in the figure. Here the point A is called the origin, tail, base, or initial point; point B is called the head, tip, endpoint, terminal point or final point. The length of the arrow is proportional to the vector's magnitude, while the direction in which the arrow points indicates the vector's direction.On a two-dimensional diagram, sometimes a vector perpendicular to the plane of the diagram is desired. These vectors are commonly shown as small circles. A circle with a dot at its centre (Unicode U+2299 ⊙) indicates a vector pointing out of the front of the diagram, toward the viewer. A circle with a cross inscribed in it (Unicode U+2297 ⊗) indicates a vector pointing into and behind the diagram. These can be thought of as viewing the tip of an arrow head on and viewing the flights of an arrow from the back.In order to calculate with vectors, the graphical representation may be too cumbersome. Vectors in an n-dimensional Euclidean space can be represented as coordinate vectors in a Cartesian coordinate system. The endpoint of a vector can be identified with an ordered list of n real numbers (n-tuple). These numbers are the coordinates of the endpoint of the vector, with respect to a given Cartesian coordinate system, and are typically called the scalar components (or scalar projections) of the vector on the axes of the coordinate system.As an example in two dimensions (see figure), the vector from the origin O = (0, 0) to the point A = (2, 3) is simply written asIn three dimensional Euclidean space (or R3), vectors are identified with triples of scalar components:This can be generalised to n-dimensional Euclidean space (or Rn).These numbers are often arranged into a column vector or row vector, particularly when dealing with matrices, as follows:Another way to represent a vector in n-dimensions is to introduce the standard basis vectors. For instance, in three dimensions, there are three of them:These have the intuitive interpretation as vectors of unit length pointing up the x-, y-, and z-axis of a Cartesian coordinate system, respectively. In terms of these, any vector a in R3 can be expressed in the form:orwhere a1, a2, a3 are called the vector components (or vector projections) of a on the basis vectors or, equivalently, on the corresponding Cartesian axes x, y, and z (see figure), while a1, a2, a3 are the respective scalar components (or scalar projections).The notation ei is compatible with the index notation and the summation convention commonly used in higher level mathematics, physics, and engineering.As explained above a vector is often described by a set of vector components that add up to form the given vector. Typically, these components are the projections of the vector on a set of mutually perpendicular reference axes (basis vectors). The vector is said to be decomposed or resolved with respect to that set.The decomposition or resolution[11] of a vector into components is not unique, because it depends on the choice of the axes on which the vector is projected.The choice of a basis does not affect the properties of a vector or its behaviour under transformations.A vector can also be broken up with respect to "non-fixed" basis vectors that change their orientation as a function of time or space. For example, a vector in three-dimensional space can be decomposed with respect to two axes, respectively normal, and tangent to a surface (see figure). Moreover, the radial and tangential components of a vector relate to the radius of rotation of an object. The former is parallel to the radius and the latter is orthogonal to it.[12]In these cases, each of the components may be in turn decomposed with respect to a fixed coordinate system or basis set (e.g., a global coordinate system, or inertial reference frame).The following section uses the Cartesian coordinate system with basis vectorsand assumes that all vectors have the origin as a common base point. A vector a will be written asTwo vectors are said to be equal if they have the same magnitude and direction. Equivalently they will be equal if their coordinates are equal. So two vectorsandare equal ifTwo vectors are opposite if they have the same magnitude but opposite direction.  So two vectorsandare opposite ifTwo vectors are parallel if they have the same direction but not necessarily the same magnitude, or antiparallel if they have opposite direction but not necessarily the same magnitude.Assume now that a and b are not necessarily equal vectors, but that they may have different magnitudes and directions. The sum of a and b isThe addition may be represented graphically by placing the tail of the arrow b at the head of the arrow a, and then drawing an arrow from the tail of a to the head of b. The new arrow drawn represents the vector a + b, as illustrated below:This addition method is sometimes called the parallelogram rule because a and b form the sides of a parallelogram and a + b is one of the diagonals. If a and b are bound vectors that have the same base point, this point will also be the base point of a + b. One can check geometrically that a + b = b + a and (a + b) + c = a + (b + c).The difference of a and b isSubtraction of two vectors can be geometrically defined as follows: to subtract b from a, place the tails of a and b at the same point, and then draw an arrow from the head of b to the head of a. This new arrow represents the vector a − b, as illustrated below:A vector may also be multiplied, or re-scaled, by a real number r. In the context of conventional vector algebra, these real numbers are often called scalars (from scale) to distinguish them from vectors. The operation of multiplying a vector by a scalar is called scalar multiplication. The resulting vector isIntuitively, multiplying by a scalar r stretches a vector out by a factor of r. Geometrically, this can be visualized (at least in the case when r is an integer) as placing r copies of the vector in a line where the endpoint of one vector is the initial point of the next vector.If r is negative, then the vector changes direction: it flips around by an angle of 180°. Two examples (r = −1 and r = 2) are given below:Scalar multiplication is distributive over vector addition in the following sense: r(a + b) = ra + rb for all vectors a and b and all scalars r. One can also show that a − b = a + (−1)b.The length or magnitude or norm of the vector a is denoted by ‖a‖ or, less commonly, |a|, which is not to be confused with the absolute value (a scalar "norm").The length of the vector a can be computed with the Euclidean normwhich is a consequence of the Pythagorean theorem since the basis vectors e1, e2, e3 are orthogonal unit vectors.This happens to be equal to the square root of the dot product, discussed below, of the vector with itself:A unit vector is any vector with a length of one; normally unit vectors are used simply to indicate direction. A vector of arbitrary length can be divided by its length to create a unit vector. This is known as normalizing a vector. A unit vector is often indicated with a hat as in â.To normalize a vector a = (a1, a2, a3), scale the vector by the reciprocal of its length ‖a‖. That is:The dot product of two vectors a and b (sometimes called the inner product, or, since its result is a scalar, the scalar product) is denoted by a ∙ b and is defined as:where θ is the measure of the angle between a and b (see trigonometric function for an explanation of cosine). Geometrically, this means that a and b are drawn with a common start point and then the length of a is multiplied with the length of the component of b that points in the same direction as a.The dot product can also be defined as the sum of the products of the components of each vector asThe cross product (also called the vector product or outer product) is only meaningful in three or seven dimensions. The cross product differs from the dot product primarily in that the result of the cross product of two vectors is a vector. The cross product, denoted a × b, is a vector perpendicular to both a and b and is defined aswhere θ is the measure of the angle between a and b, and n is a unit vector perpendicular to both a and b which completes a right-handed system. The right-handedness constraint is necessary because there exist two unit vectors that are perpendicular to both a and b, namely, n and (–n).The cross product a × b is defined so that a, b, and a × b also becomes a right-handed system (but note that a and b are not necessarily orthogonal). This is the right-hand rule.The length of a × b can be interpreted as the area of the parallelogram having a and b as sides.The cross product can be written asFor arbitrary choices of spatial orientation (that is, allowing for left-handed as well as right-handed coordinate systems) the cross product of two vectors is a pseudovector instead of a vector (see below).The scalar triple product (also called the box product or mixed triple product) is not really a new operator, but a way of applying the other two multiplication operators to three vectors. The scalar triple product is sometimes denoted by (a b c) and defined as:It has three primary uses. First, the absolute value of the box product is the volume of the parallelepiped which has edges that are defined by the three vectors. Second, the scalar triple product is zero if and only if the three vectors are linearly dependent, which can be easily proved by considering that in order for the three vectors to not make a volume, they must all lie in the same plane. Third, the box product is positive if and only if the three vectors a, b and c are right-handed.In components (with respect to a right-handed orthonormal basis), if the three vectors are thought of as rows (or columns, but in the same order), the scalar triple product is simply the determinant of the 3-by-3 matrix having the three vectors as rowsThe scalar triple product is linear in all three entries and anti-symmetric in the following sense:All examples thus far have dealt with vectors expressed in terms of the same basis, namely, the e basis {e1, e2, e3}. However, a vector can be expressed in terms of any number of different bases that are not necessarily aligned with each other, and still remain the same vector.  In the e basis, a vector a is expressed, by definition, asThe scalar components in the e basis are, by definition,In another orthnormal basis n = {n1, n2, n3} that is not necessarily aligned with e, the vector a is expressed asand the scalar components in the n basis are, by definition,The values of p, q, r, and u, v, w relate to the unit vectors in such a way that the resulting vector sum is exactly the same physical vector a in both cases.  It is common to encounter vectors known in terms of different bases (for example, one basis fixed to the Earth and a second basis fixed to a moving vehicle).  In such a case it is necessary to develop a method to convert between bases so the basic vector operations such as addition and subtraction can be performed.  One way to express u, v, w in terms of p, q, r is to use column matrices along with a direction cosine matrix containing the information that relates the two bases.  Such an expression can be formed by substitution of the above equations to formDistributing the dot-multiplication givesReplacing each dot product with a unique scalar givesand these equations can be expressed as the single matrix equationThis matrix equation relates the scalar components of a in the n basis (u,v, and w) with those in the e basis (p, q, and r).  Each matrix element cjk is the direction cosine relating nj to ek.[13] The term direction cosine refers to the cosine of the angle between two unit vectors, which is also equal to their dot product.[13] Therefore,By referring collectively to e1, e2, e3 as the e basis and to n1, n2, n3 as the n basis, the matrix containing all the cjk is known as the "transformation matrix from e to n", or the "rotation matrix from e to n" (because it can be imagined as the "rotation" of a vector from one basis to another), or the "direction cosine matrix from e to n"[13] (because it contains direction cosines).  The properties of a rotation matrix are such that its inverse is equal to its transpose. This means that the "rotation matrix from e to n" is the transpose of "rotation matrix from n to e".The properties of a direction cosine matrix, C are[14]:The advantage of this method is that a direction cosine matrix can usually be obtained independently by using Euler angles or a quaternion to relate the two vector bases, so the basis conversions can be performed directly, without having to work out all the dot products described above.By applying several matrix multiplications in succession, any vector can be expressed in any basis so long as the set of direction cosines is known relating the successive bases.[13]With the exception of the cross and triple products, the above formulae generalise to two dimensions and higher dimensions. For example, addition generalises to two dimensions asand in four dimensions asThe cross product does not readily generalise to other dimensions, though the closely related exterior product does, whose result is a bivector. In two dimensions this is simply a pseudoscalarA seven-dimensional cross product is similar to the cross product in that its result is a vector orthogonal to the two arguments; there is however no natural way of selecting one of the possible such products.Vectors have many uses in physics and other sciences.In abstract vector spaces, the length of the arrow depends on a dimensionless scale. If it represents, for example, a force, the "scale" is of physical dimension length/force. Thus there is typically consistency in scale among quantities of the same dimension, but otherwise scale ratios may vary; for example, if "1 newton" and "5 m" are both represented with an arrow of 2 cm, the scales are 1 m:50 N and 1:250 respectively. Equal length of vectors of different dimension has no particular significance unless there is some proportionality constant inherent in the system that the diagram represents. Also length of a unit vector (of dimension length, not length/force, etc.) has no coordinate-system-invariant significance.Often in areas of physics and mathematics, a vector evolves in time, meaning that it depends on a time parameter t. For instance, if r represents the position vector of a particle, then r(t) gives a parametric representation of the trajectory of the particle. Vector-valued functions can be differentiated and integrated by differentiating or integrating the components of the vector, and many of the familiar rules from calculus continue to hold for the derivative and integral of vector-valued functions.The position of a point x = (x1, x2, x3) in three-dimensional space can be represented as a position vector whose base point is the originThe position vector has dimensions of length.Given two points x = (x1, x2, x3), y = (y1, y2, y3) their displacement is a vectorwhich specifies the position of y relative to x. The length of this vector gives the straight-line distance from x to y. Displacement has the dimensions of length.The velocity v of a point or particle is a vector, its length gives the speed. For constant velocity the position at time t will bewhere x0 is the position at time t = 0. Velocity is the time derivative of position. Its dimensions are length/time.Acceleration a of a point is vector which is the time derivative of velocity. Its dimensions are length/time2.Force is a vector with dimensions of mass×length/time2 and Newton's second law is the scalar multiplicationWork is the dot product of force and displacementTherefore, any directional derivative can be identified with a corresponding vector, and any vector can be identified with a corresponding directional derivative. A vector can therefore be defined precisely asIn the language of differential geometry, the requirement that the components of a vector transform according to the same matrix of the coordinate transition is equivalent to defining a contravariant vector to be a tensor of contravariant rank one. Alternatively, a contravariant vector is defined to be a tangent vector, and the rules for transforming a contravariant vector follow from the chain rule.Some vectors transform like contravariant vectors, except that when they are reflected through a mirror, they flip and gain a minus sign. A transformation that switches right-handedness to left-handedness and vice versa like a mirror does is said to change the orientation of space. A vector which gains a minus sign when the orientation of space changes is called a pseudovector or an axial vector. Ordinary vectors are sometimes called true vectors or polar vectors to distinguish them from pseudovectors. Pseudovectors occur most frequently as the cross product of two ordinary vectors.One example of a pseudovector is angular velocity. Driving in a car, and looking forward, each of the wheels has an angular velocity vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the reflection of this angular velocity vector points to the right, but the actual angular velocity vector of the wheel still points to the left, corresponding to the minus sign. Other examples of pseudovectors include magnetic field, torque, or more generally any cross product of two (true) vectors.This distinction between vectors and pseudovectors is often ignored, but it becomes important in studying symmetry properties. See parity (physics).
Linear programming
Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.Linear programs are problems that can be expressed in canonical form asLinear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.The problem of solving a system of linear inequalities dates back at least as far as Fourier, who in 1827 published a method for solving them,[1] and after whom the method of Fourier–Motzkin elimination is named.In 1939 a linear programming formulation of a problem that is equivalent to the general linear programming problem was given by the Soviet economist Leonid Kantorovich, who also proposed a method for solving it.[2] It is a way he developed, during World War II, to plan expenditures and returns in order to reduce costs of the army and to increase losses imposed on the enemy.[citation needed] Kantorovich's work was initially neglected in the USSR.[3] About the same time as Kantorovich, the Dutch-American economist T. C. Koopmans formulated classical economic problems as linear programs. Kantorovich and Koopmans later shared the 1975 Nobel prize in economics.[1] In 1941, Frank Lauren Hitchcock also formulated transportation  problems as linear programs and gave a solution very similar to the later simplex method.[2] Hitchcock had died in 1957 and the Nobel prize is not awarded posthumously.During 1946–1947, George B. Dantzig independently developed general linear programming formulation to use for planning problems in US Air Force[citation needed]. In 1947, Dantzig also invented the simplex method that for the first time efficiently tackled the linear programming problem in most cases[citation needed]. When Dantzig arranged a meeting with John von Neumann to discuss his simplex method, Neumann immediately conjectured the theory of duality by realizing that the problem he had been working in game theory was equivalent[citation needed]. Dantzig provided formal proof in an unpublished report "A Theorem on Linear Inequalities" on January 5, 1948.[3] In the post-war years, many industries applied it in their daily planning.Dantzig's original example was to find the best assignment of 70 people to 70 jobs. The computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the number of particles in the observable universe. However, it takes only a moment to find the optimum solution by posing the problem as a linear program and applying the simplex algorithm. The theory behind linear programming drastically reduces the number of possible solutions that must be checked.The linear programming problem was first shown to be solvable in polynomial time by Leonid Khachiyan in 1979,[4] but a larger theoretical and practical breakthrough in the field came in 1984 when Narendra Karmarkar introduced a new interior-point method for solving linear-programming problems.[5]Linear programming is a widely used field of optimization for several reasons. Many practical problems in operations research can be expressed as linear programming problems.[3] Certain special cases of linear programming, such as network flow problems and multicommodity flow problems are considered important enough to have generated much research on specialized algorithms for their solution. A number of algorithms for other types of optimization problems work by solving LP problems as sub-problems. Historically, ideas from linear programming have inspired many of the central concepts of optimization theory, such as duality, decomposition, and the importance of convexity and its generalizations. Likewise, linear programming was heavily used in the early formation of microeconomics and it is currently utilized in company management, such as planning, production, transportation, technology and other issues. Although the modern management issues are ever-changing, most companies would like to maximize profits and minimize costs with limited resources. Therefore, many issues can be characterized as linear programming problems.Standard form is the usual and most intuitive form of describing a linear programming problem. It consists of the following three parts:The problem is usually expressed in matrix form, and then becomes:Other forms, such as minimization problems, problems with constraints on alternative forms, as well as problems involving negative variables can always be rewritten into an equivalent problem in standard form.Suppose that a farmer has a piece of farm land, say L km2, to be planted with either wheat or barley or some combination of the two. The farmer has a limited amount of fertilizer, F kilograms, and pesticide, P kilograms. Every square kilometer of wheat requires F1 kilograms of fertilizer and P1 kilograms of pesticide, while every square kilometer of barley requires F2 kilograms of fertilizer and P2 kilograms of pesticide. Let S1 be the selling price of wheat per square kilometer, and S2 be the selling price of barley. If we denote the area of land planted with wheat and barley by x1 and x2 respectively, then profit can be maximized by choosing optimal values for x1 and x2. This problem can be expressed with the following linear programming problem in the standard form:In matrix form this becomes:Linear programming problems can be converted into an augmented form in order to apply the common form of the simplex algorithm. This form introduces non-negative slack variables to replace inequalities with equalities in the constraints. The problems can then be written in the following block matrix form:The example above is converted into the following augmented form:In matrix form this becomes:Every linear programming problem, referred to as a primal problem, can be converted into a dual problem, which provides an upper bound to the optimal value of the primal problem. In matrix form, we can express the primal problem as:An alternative primal formulation is:There are two ideas fundamental to duality theory. One is the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program. Additionally, every feasible solution for a linear program gives a bound on the optimal value of the objective function of its dual.  The weak duality theorem states that the objective function value of the dual at any feasible solution is always greater than or equal to the objective function value of the primal at any feasible solution. The strong duality theorem states that if the primal has an optimal solution, x*, then the dual also has an optimal solution, y*, and cTx*=bTy*.A linear program can also be unbounded or infeasible. Duality theory tells us that if the primal is unbounded then the dual is infeasible by the weak duality theorem. Likewise, if the dual is unbounded, then the primal must be infeasible. However, it is possible for both the dual and the primal to be infeasible. As an example, consider the linear program:Revisit the above example of the farmer who may grow wheat and barley with the set provision of some L land, F fertilizer and P pesticide. Assume now that y unit prices for each of these means of production (inputs) are set by a planning board. The planning board's job is to minimize the total cost of procuring the set amounts of inputs while providing the farmer with a floor on the unit price of each of his crops (outputs), S1 for wheat and S2 for barley. This corresponds to the following linear programming problem:In matrix form this becomes:The primal problem deals with physical quantities. With all inputs available in limited quantities, and assuming the unit prices of all outputs is known, what quantities of outputs to produce so as to maximize total revenue? The dual problem deals with economic values. With floor guarantees on all output unit prices, and assuming the available quantity of all inputs is known, what input unit pricing scheme to set so as to minimize total expenditure?To each variable in the primal space corresponds an inequality to satisfy in the dual space, both indexed by output type. To each inequality to satisfy in the primal space corresponds a variable in the dual space, both indexed by input type.The coefficients that bound the inequalities in the primal space are used to compute the objective in the dual space, input quantities in this example. The coefficients used to compute the objective in the primal space bound the inequalities in the dual space, output unit prices in this example.Both the primal and the dual problems make use of the same matrix. In the primal space, this matrix expresses the consumption of physical quantities of inputs necessary to produce set quantities of outputs. In the dual space, it expresses the creation of the economic values associated with the outputs from set input unit prices.Since each inequality can be replaced by an equality and a slack variable, this means each primal variable corresponds to a dual slack variable, and each dual variable corresponds to a primal slack variable.  This relation allows us to speak about complementary slackness.Sometimes, one may find it more intuitive to obtain the dual program without looking at the program matrix. Consider the following linear program:We have m + n conditions and all variables are non-negative. We shall define m + n dual variables: yj and si. We get:Since this is a minimization problem, we would like to obtain a dual program that is a lower bound of the primal. In other words, we would like the sum of all right hand side of the constraints to be the maximal under the condition that for each primal variable the sum of its coefficients do not exceed its coefficient in the linear function. For example, x1 appears in n + 1 constraints. If we sum its constraints' coefficients we get a1,1y1 + a1,2y2 + ... + a1,nyn + f1s1. This sum must be at most c1. As a result, we get:Note that we assume in our calculations steps that the program is in standard form. However, any linear program may be transformed to standard form and it is therefore not a limiting factor.A covering LP is a linear program of the form:such that the matrix A and the vectors b and c are non-negative.The dual of a covering LP is a packing LP, a linear program of the form:such that the matrix A and the vectors b and c are non-negative.Covering and packing LPs commonly arise as a linear programming relaxation of a combinatorial problem and are important in the study of approximation algorithms.[6] For example, the LP relaxations of the set packing problem, the independent set problem, and the matching problem are packing LPs. The LP relaxations of the set cover problem, the vertex cover problem, and the dominating set problem are also covering LPs.Finding a fractional coloring of a graph is another example of a covering LP. In this case, there is one constraint for each vertex of the graph and one variable for each independent set of the graph.It is possible to obtain an optimal solution to the dual when only an optimal solution to the primal is known using the complementary slackness theorem. The theorem states:Suppose that x = (x1, x2, ... , xn) is primal feasible and that y = (y1, y2, ... , ym) is dual feasible. Let (w1, w2, ..., wm) denote the corresponding primal slack variables, and let (z1, z2, ... , zn) denote the corresponding dual slack variables. Then x and y are optimal for their respective problems if and only ifSo if the i-th slack variable of the primal is not zero, then the i-th variable of the dual is equal to zero. Likewise, if the j-th slack variable of the dual is not zero, then the j-th variable of the primal is equal to zero.This necessary condition for optimality conveys a fairly simple economic principle.  In standard form (when maximizing), if there is slack in a constrained primal resource (i.e., there are "leftovers"), then additional quantities of that resource must have no value.  Likewise, if there is slack in the dual (shadow) price non-negativity constraint requirement, i.e., the price is not zero, then there must be scarce supplies (no "leftovers").Geometrically, the linear constraints define the feasible region, which is a convex polyhedron. A linear function is a convex function, which implies that every local minimum is a global minimum; similarly, a linear function is a concave function, which implies that every local maximum is a global maximum.An optimal solution need not exist, for two reasons. First, if two constraints are inconsistent, then no feasible solution exists: For instance, the constraints x ≥ 2 and x ≤ 1 cannot be satisfied jointly; in this case, we say that the LP is infeasible. Second, when the polytope is unbounded in the direction of the gradient of the objective function (where the gradient of the objective function is the vector of the coefficients of the objective function), then no optimal value is attained because it is always possible to do better than any finite value of the objective function.Otherwise, if a feasible solution exists and if the constraint set is bounded, then the optimum value is always attained on the boundary of the constraint set, by the maximum principle for convex functions (alternatively, by the minimum principle for concave functions) since linear functions are both convex and concave. However, some problems have distinct optimal solutions; for example, the problem of finding a feasible solution to a system of linear inequalities is a linear programming problem in which the objective function is the zero function (that is, the constant function taking the value zero everywhere). For this feasibility problem with the zero-function for its objective-function, if there are two distinct solutions, then every convex combination of the solutions is a solution.The vertices of the polytope are also called basic feasible solutions. The reason for this choice of name is as follows. Let d denote the number of variables. Then the fundamental theorem of linear inequalities implies (for feasible problems) that for every vertex x* of the LP feasible region, there exists a set of d (or fewer) inequality constraints from the LP such that, when we treat those d constraints as equalities, the unique solution is x*. Thereby we can study these vertices by means of looking at certain subsets of the set of all constraints (a discrete set), rather than the continuum of LP solutions. This principle underlies the simplex algorithm for solving linear programs.The simplex algorithm, developed by George Dantzig in 1947, solves LP problems by constructing a feasible solution at a vertex of the polytope and then walking along a path on the edges of the polytope to vertices with non-decreasing values of the objective function until an optimum is reached for sure. In many practical problems, "stalling" occurs: many pivots are made with no increase in the objective function.[7][8] In rare practical problems, the usual versions of the simplex algorithm may actually "cycle".[8] To avoid cycles, researchers developed new pivoting rules.[9][10][7][8][11][12]In practice, the simplex algorithm is quite efficient and can be guaranteed to find the global optimum if certain precautions against cycling are taken. The simplex algorithm has been proved to solve "random" problems efficiently, i.e. in a cubic number of steps,[13] which is similar to its behavior on practical problems.[7][14]However, the simplex algorithm has poor worst-case behavior: Klee and Minty constructed a family of linear programming problems for which the simplex method takes a number of steps exponential in the problem size.[7][10][11] In fact, for some time it was not known whether the linear programming problem was solvable in polynomial time, i.e. of complexity class P.Like the simplex algorithm of Dantzig, the criss-cross algorithm is a basis-exchange algorithm that pivots between bases. However, the criss-cross algorithm need not maintain feasibility, but can pivot rather from a feasible basis to an infeasible basis. The criss-cross algorithm does not have polynomial time-complexity for linear programming. Both algorithms visit all 2D corners of a (perturbed) cube in dimension D, the Klee–Minty cube, in the worst case.[12][15]In contrast to the simplex algorithm, which finds an optimal solution by traversing the edges between vertices on a polyhedral set, interior-point methods move through the interior of the feasible region.This is the first worst-case polynomial-time algorithm ever found for linear programming.  To solve a problem which has n variables and can be encoded in L input bits, this algorithm uses O(n4L) pseudo-arithmetic operations on numbers with O(L) digits. Leonid Khachiyan solved this long-standing complexity issue in 1979 with the introduction of the ellipsoid method. The convergence analysis has (real-number) predecessors, notably the iterative methods developed by Naum Z. Shor and the approximation algorithms by Arkadi Nemirovski and D. Yudin.Khachiyan's algorithm was of landmark importance for establishing the polynomial-time solvability of linear programs.  The algorithm was not a computational break-through, as the simplex method is more efficient for all but specially constructed families of linear programs.Affine scaling is one of the oldest interior point methods to be developed. It was developed in the Soviet Union in the mid-1960s, but didn't receive much attention until the discovery of Karmarkar's algorithm, after which affine scaling was reinvented multiple times and presented as a simplified version of Karmarkar's. Affine scaling amounts to doing gradient descent steps within the feasible region, while rescaling the problem to make sure the steps move toward the optimum faster.[17]For both theoretical and practical purposes, barrier function or path-following methods have been the most popular interior point methods since the 1990s.[18]The current opinion is that the efficiencies of good implementations of simplex-based methods and interior point methods are similar for routine applications of linear programming.[18]  However, for specific types of LP problems, it may be that one type of solver is better than another (sometimes much better), and that the structure of the solutions generated by interior point methods versus simplex-based methods are significantly different with the support set of active variables being typically smaller for the later one.[19]Covering and packing LPs can be solved approximately in nearly-linear time. That is, if matrix A is of dimension n×m and has N non-zero entries, then there exist algorithms that run in time O(N·(log N)O(1)/εO(1)) and produce O(1±ε) approximate solutions to given covering and packing LPs. The best known sequential algorithm of this kind runs in time O(N + (log N)·(n+m)/ε2),[20] and the best known parallel algorithm of this kind runs in O((log N)2/ε3) iterations, each requiring only a matrix-vector multiplication which is highly parallelizable.[21]There are several open problems in the theory of linear programming, the solution of which would represent fundamental breakthroughs in mathematics and potentially major advances in our ability to solve large-scale linear programs.This closely related set of problems has been cited by Stephen Smale as among the 18 greatest unsolved problems of the 21st century.  In Smale's words, the third version of the problem "is the main unsolved problem of linear programming theory."  While algorithms exist to solve linear programming in weakly polynomial time, such as the ellipsoid methods and interior-point techniques, no algorithms have yet been found that allow strongly polynomial-time performance in the number of constraints and the number of variables.  The development of such algorithms would be of great theoretical interest, and perhaps allow practical gains in solving large LPs as well.Although the Hirsch conjecture was recently disproved for higher dimensions, it still leaves the following questions open.These questions relate to the performance analysis and development of simplex-like methods.  The immense efficiency of the simplex algorithm in practice despite its exponential-time theoretical performance hints that there may be variations of simplex that run in polynomial or even strongly polynomial time.  It would be of great practical and theoretical significance to know whether any such variants exist, particularly as an approach to deciding if LP can be solved in strongly polynomial time.The simplex algorithm and its variants fall in the family of edge-following algorithms, so named because they solve linear programming problems by moving from vertex to vertex along edges of a polytope.  This means that their theoretical performance is limited by the maximum number of edges between any two vertices on the LP polytope.  As a result, we are interested in knowing the maximum graph-theoretical diameter of polytopal graphs.  It has been proved that all polytopes have subexponential diameter. The recent disproof of the Hirsch conjecture is the first step to prove whether any polytope has superpolynomial diameter. If any such polytopes exist, then no edge-following variant can run in polynomial time. Questions about polytope diameter are of independent mathematical interest.Simplex pivot methods preserve primal (or dual) feasibility.  On the other hand, criss-cross pivot methods do not preserve (primal or dual) feasibility—they may visit primal feasible, dual feasible or primal-and-dual infeasible bases in any order.  Pivot methods of this type have been studied since the 1970s.  Essentially, these methods attempt to find the shortest pivot path on the arrangement polytope under the linear programming problem.  In contrast to polytopal graphs, graphs of arrangement polytopes are known to have small diameter, allowing the possibility of strongly polynomial-time criss-cross pivot algorithm without resolving questions about the diameter of general polytopes.[12]If all of the unknown variables are required to be integers, then the problem is called an integer programming (IP) or integer linear programming (ILP) problem.  In contrast to linear programming, which can be solved efficiently in the worst case, integer programming problems are in many practical situations (those with bounded variables) NP-hard. 0–1 integer programming or binary integer programming (BIP) is the special case of integer programming where variables are required to be 0 or 1 (rather than arbitrary integers). This problem is also classified as NP-hard, and in fact the decision version was one of Karp's 21 NP-complete problems.If only some of the unknown variables are required to be integers, then the problem is called a mixed integer programming (MIP) problem.  These are generally also NP-hard because they are even more general than ILP programs.There are however some important subclasses of IP and MIP problems that are efficiently solvable, most notably problems where the constraint matrix is totally unimodular and the right-hand sides of the constraints are integers or – more general – where the system has the total dual integrality (TDI) property.Advanced algorithms for solving integer linear programs include:Such integer-programming algorithms are discussed by Padberg and in Beasley.Integral linear programs are of central importance in the polyhedral aspect of combinatorial optimization since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective. Conversely, if we can prove that a linear programming relaxation is integral, then it is the desired description of the convex hull of feasible (integral) solutions.Note that terminology is not consistent throughout the literature, so one should be careful to distinguish the following two concepts,One common way of proving that a polyhedron is integral is to show that it is totally unimodular. There are other general methods including the integer decomposition property and total dual integrality. Other specific well-known integral LPs include the matching polytope, lattice polyhedra, submodular flow polyhedra, and the intersection of 2 generalized polymatroids/g-polymatroids – e.g. see Schrijver 2003.A bounded integral polyhedron is sometimes called a convex lattice polytope, particularly in two dimensions.Permissive licenses:Copyleft (reciprocal) licenses:MINTO (Mixed Integer Optimizer, an integer programming solver which uses branch and bound algorithm) has publicly available source code[24] but is not open source.Proprietary licenses:A reader may consider beginning with Nering and Tucker, with the first volume of Dantzig and Thapa, or with Williams.
Discrete wavelet transform
In numerical analysis and functional analysis, a discrete wavelet transform (DWT) is any wavelet transform for which the wavelets are discretely sampled. As with other wavelet transforms, a key advantage it has over Fourier transforms is temporal resolution: it captures both frequency and location information (location in time).The most commonly used set of discrete wavelet transforms was formulated by the Belgian mathematician Ingrid Daubechies in 1988. This formulation is based on the use of recurrence relations to generate progressively finer discrete samplings of an implicit mother wavelet function; each resolution is twice that of the previous scale. In her seminal paper, Daubechies derives a family of wavelets, the first of which is the Haar wavelet. Interest in this field has exploded since then, and many variations of Daubechies' original wavelets were developed.[1][2]Other forms of discrete wavelet transform include the non- or undecimated wavelet transform (where downsampling is omitted), the Newland transform (where an orthonormal basis of wavelets is formed from appropriately constructed top-hat filters in frequency space). Wavelet packet transforms are also related to the discrete wavelet transform.  Complex wavelet transform is another form.Due to the rate-change operators in the filter bank, the discrete WT is not time-invariant but actually very sensitive to the alignment of the signal in time. To address the time-varying problem of wavelet transforms, Mallat and Zhong proposed a new algorithm for wavelet representation of a signal, which is invariant to time shifts.[4] According to this algorithm, which is called a TI-DWT, only the scale parameter is sampled along the dyadic sequence 2^j (j∈Z) and the wavelet transform is calculated for each point in time.[5][6]The discrete wavelet transform has a huge number of applications in science, engineering, mathematics and computer science. Most notably, it is used for signal coding, to represent a discrete signal in a more redundant form, often as a preconditioning for data compression. Practical applications can also be found in signal processing of accelerations for gait analysis,[7] image processing,[8] in digital communications and many others.[9][10][11]It is shown that discrete wavelet transform (discrete in scale and shift, and continuous in time) is successfully implemented as analog filter bank in biomedical signal processing for design of low-power pacemakers and also in ultra-wideband (UWB) wireless communications.[12]Wavelets are often used to denoise two dimensional signals, such as images.  The following example provides three steps to remove unwanted white Gaussian noise from the noisy image shown. Matlab was used to import and filter the image.The first step is to choose a wavelet type, and a level N of decomposition.  In this case biorthogonal 3.5 wavelets were chosen with a level N of 10. Biorthogonal wavelets are commonly used in image processing to detect and filter white Gaussian noise,[13] due to their high contrast of neighboring pixel intensity values. Using this wavelets a wavelet transformation is performed on the two dimensional image.Following the decomposition of the image file, the next step is to determine threshold values for each level from 1 to N. Birgé-Massart strategy[14] is a fairly common method for selecting these thresholds.  Using this process individual thresholds are made for N = 10 levels. Applying these thresholds are the majority of the actual filtering of the signal.The final step is to reconstruct the image from the modified levels. This is accomplished using an inverse wavelet transform. The resulting image, with white Gaussian noise removed is shown below the original image. When filtering any form of data it is important to quantify the signal-to-noise-ratio of the result.[15]  In this case, the SNR of the noisy image in comparison to the original was 30.4958%, and the SNR of the denoised image is 32.5525%.  The resulting improvement of the wavelet filtering is a SNR gain of 2.0567%.[16]It is important to note that choosing other wavelets, levels, and thresholding strategies can result in different types of filtering.  In this example, white Gaussian noise was chosen to be removed. Although, with different thresholding, it could just as easily have been amplified.To illustrate the differences and similarities between the discrete wavelet transform with the discrete Fourier transform, consider the DWT and DFT of the following sequence: (1,0,0,0), a unit impulse.The DFT has orthogonal basis (DFT matrix):while the DWT with Haar wavelets for length 4 data has orthogonal basis in the rows of:(To simplify notation, whole numbers are used, so the bases are orthogonal but not orthonormal.)Preliminary observations include:Decomposing the sequence with respect to these bases yields:The DWT demonstrates the localization: the (1,1,1,1) term gives the average signal value, the (1,1,–1,–1) places the signal in the left side of the domain, and the (1,–1,0,0) places it at the left side of the left side, and truncating at any stage yields a downsampled version of the signal:The DFT, by contrast, expresses the sequence by the interference of waves of various frequencies – thus truncating the series yields a low-pass filtered version of the series:This illustrates the kinds of trade-offs between these transforms, and how in some respects the DWT provides preferable behavior, particularly for the modeling of transients.This decomposition has halved the time resolution since only half of each filter output characterises the signal.  However, each output has half the frequency band of the input, so the frequency resolution has been doubled.the above summation can be written more concisely.The Lifting scheme is an optimization where these two computations are interleaved.This decomposition is repeated to further increase the frequency resolution and the approximation coefficients decomposed with high and low pass filters and then down-sampled.  This is represented as a binary tree with nodes representing a sub-space with a different time-frequency localisation.  The tree is known as a filter bank.The filterbank implementation of the Discrete Wavelet Transform takes only O(N) in certain cases, as compared to O(N log N) for the fast Fourier transform.which leads to an O(N) time for the entire operation, as can be shown by a geometric series expansion of the above relation.The Adam7 algorithm, used for interlacing in the Portable Network Graphics (PNG) format, is a multiscale model of the datawhich is similar to a DWT with Haar wavelets.Unlike the DWT, it has a specific scale – it starts from an 8×8 block, and it downsamples the image, rather than decimating (low-pass filtering, then downsampling). It thus offers worse frequency behavior, showing artifacts (pixelation) at the early stages, in return for simpler implementation.In its simplest form, the DWT is remarkably easy to compute.The Haar wavelet in Java:Complete Java code for a 1-D and 2-D DWT using Haar, Daubechies, Coiflet, and Legendre wavelets is available from the open source project: JWave.Furthermore, a fast lifting implementation of the discrete biorthogonal CDF 9/7 wavelet transform in C, used in the JPEG 2000 image compression standard can be found here (archived 5 March 2012).This figure shows an example of applying the above code to compute the Haar wavelet coefficients on a sound waveform.  This example highlights two key properties of the wavelet transform:
Savitzky–Golay filter
A Savitzky–Golay filter is a digital filter that can be applied to a set of digital data points for the purpose of smoothing the data, that is, to increase the signal-to-noise ratio without greatly distorting the signal. This is achieved, in a process known as convolution, by fitting successive sub-sets of adjacent data points with a low-degree polynomial by the method of linear least squares. When the data points are equally spaced, an analytical solution to the least-squares equations can be found, in the form of a single set of "convolution coefficients" that can be applied to all data sub-sets, to give estimates of the smoothed signal, (or derivatives of the smoothed signal) at the central point of each sub-set. The method, based on established mathematical procedures,[1][2] was popularized by Abraham Savitzky and Marcel J. E. Golay who published tables of convolution coefficients for various polynomials and sub-set sizes in 1964.[3][4] Some errors in the tables have been corrected.[5] The method has been extended for the treatment of 2- and 3-dimensional data.Savitzky and Golay's paper is one of the most widely cited papers in the journal Analytical Chemistry[6] and is classed by that journal as one of its "10 seminal papers" saying "it can be argued that the dawn of the computer-controlled analytical instrument can be traced to this article".[7]The data consists of a set of n  {xj, yj} points (j = 1, ..., n), where x is an independent variable and yj is an observed value. They are treated with a set of m convolution coefficients, Ci, according to the expression  It is easy to apply this formula in a spreadsheet. Selected convolution coefficients are shown in the tables, below. For example, for smoothing by a 5-point quadratic polynomial, m = 5, i = −2, −1, 0, 1, 2 and the jth smoothed data point, Yj, is given byA moving average filter is commonly used with time series data to smooth out short-term fluctuations and highlight longer-term trends or cycles. It is often used in technical analysis of financial data, like stock prices, returns or trading volumes. It is also used in economics to examine gross domestic product, employment or other macroeconomic time series.An unweighted moving average filter is the simplest convolution filter. Each subset of the data set is fitted by a straight horizontal line. It was not included in the Savitzsky-Golay tables of convolution coefficients as all the coefficient values are simply equal to 1/m.When the data points are equally spaced, an analytical solution to the least-squares equations can be found.[2] This solution forms the basis of the convolution method of numerical smoothing and differentiation. Suppose that the data consists of a set of n  points (xj, yj)  (j = 1, ..., n), where x is an independent variable and yj is a datum value. A polynomial will be fitted by linear least squares  to a set of m (an odd number) adjacent data points, each separated by an interval h. Firstly, a change of variable is madeThe coefficients a0, a1 etc. are obtained by solving the normal equations (bold a represents a vector, bold J represents a matrix).For example, for a cubic polynomial fitted to 5 points, z= −2, −1, 0, 1, 2 the normal equations are solved as follows.Now, the normal equations can be factored into two separate sets of equations, by rearranging rows and columns, withExpressions for the inverse of each of these matrices can be obtained using Cramer's ruleThe normal equations becomeandMultiplying out and removing common factors,The coefficients of y in these expressions are known as convolution coefficients. They are elements of the matrix In general,In matrix notation this example is written asTables of convolution coefficients, calculated in the same way for m up to 25, were published for the Savitzky–Golay smoothing filter in 1964,[3][5] The value of the central point, z = 0, is obtained from a single set of coefficients, a0 for smoothing, a1 for 1st. derivative etc. The numerical derivatives are obtained by differentiating Y. This means that the derivatives are calculated for the smoothed data curve.  For a cubic polynomialIn general, polynomials of degree (0 and 1),[note 3] (2 and 3), (4 and 5) etc. give the same coefficients for smoothing and even derivatives. Polynomials of degree (1 and 2), (3 and 4) etc. give the same coefficients for odd derivatives.It is not necessary always to use the Savitzky–Golay tables. The summations in the matrix JTJ can be evaluated in closed form, so that algebraic formulae can be derived for the convolution coefficients.[13][note 4] Functions that are suitable for use with a curve that has an inflection point are:Simpler expressions that can be used with curves that don't have an inflection point are:  Higher derivatives can be obtained. For example, a fourth derivative can be obtained by performing two passes of a second derivative function.[14]An alternative to fitting m data points by a simple polynomial in the subsidiary variable, z, is to use orthogonal polynomials. where P0, ..., Pk is a set of mutually orthogonal polynomials of degree 0, ..., k. Full details on how to obtain expressions for the orthogonal polynomials and the relationship between the coefficients b and a are given by Guest.[2]  Expressions for the convolution coefficients are easily obtained because the normal equations matrix, JTJ, is a diagonal matrix as the product of any two orthogonal polynomials is zero by virtue of their mutual orthogonality. Therefore, each non-zero element of its inverse is simply the reciprocal the corresponding element in the normal equation matrix. The calculation is further simplified by using recursion to build orthogonal Gram polynomials. The whole calculation can be coded in a few lines of PASCAL, a computer language well-adapted for calculations involving recursion.[15]Savitzky–Golay filters are most commonly used to obtain the smoothed or derivative value at the central point, z = 0, using a single set of convolution coefficients. (m − 1)/2 points at the start and end of the series cannot be calculated using this process. Various strategies can be employed to avoid this inconvenience.It is implicit in the above treatment that the data points are all given equal weight. Technically, the objective function being minimized in the least-squares process has unit weights, wi = 1. When weights are not all the same the normal equations becomeIf the same set of diagonal weights is used for all data subsets, W = diag(w1,w2,...,wm), an analytical solution to the normal equations can be written down. For example, with a quadratic polynomial,An explicit expression for the inverse of this matrix can be obtained using Cramer's rule. A set of convolution coefficients may then be derived as Alternatively the coefficients, C, could be calculated in a spreadsheet, employing a built-in matrix inversion routine to obtain the inverse of the normal equations matrix. This set of coefficients, once calculated and stored, can be used with all calculations in which the same weighting scheme applies. A different set of coefficients is needed for each different weighting scheme.Two-dimensional smoothing and differentiation can also be applied to tables of data values, such as intensity values in a photographic image which is composed of a rectangular grid of pixels.[16] [17] Such a grid is referred as a kernel, and the data points that constitute the kernel are referred as nodes. The trick is to transform the rectangular kernel into a single row by a simple ordering of the indices of the nodes.  Whereas the one-dimensional filter coefficients are found by fitting a polynomial in the subsidiary variable z to a set of m data points, the two-dimensional coefficients are found by fitting a polynomial in subsidiary variables v and w to a set of the values at the m × n kernel nodes. The following example, for a bi-cubic polynomial, m = 7, and n = 5, illustrates the process, which parallels the process for the one dimensional case, above.[18]The rectangular kernel of 35 data values, d1 − d35becomes a vector when the rows are placed one after another.The Jacobian has 10 columns, one for each of the parameters a00 − a03, and 35 rows, one for each pair of v and w values. Each row has the formThe convolution coefficients are calculated asNikitas and Pappa-Louisi showed that, depending on the format of the used polynomial, the quality of smoothing may vary significantly.[19] They recommend using the polynomial of the formbecause such polynomials can achieve good smoothing both in the central and in the near-boundary regions of a kernel, and therefore they can be confidently used in smoothing both at the internal and at the near-boundary data points of a sampled domain. In order to avoid ill-conditioning when solving the least-squares problem, p < m and q < n. For a software which calculates the two-dimensional coefficients and for a database of such C's, see the section on multi-dimensional convolution coefficients, below.The idea of two-dimensional convolution coefficients can be extended to the higher spatial dimensions as well, in a straightforward manner,[16][20] by arranging multidimensional distribution of the kernel nodes in a single row. Following the aforementioned finding by Nikitas and Pappa-Louisi[19] in two-dimensional cases, usage of the following form of the polynomial is recommended in multidimensional cases:Accurate computation of C in multidimensional cases becomes challenging, as precision of standard floating point numbers available in computer programming languages no longer remain sufficient. The insufficient precision causes the floating point truncation errors to become comparable to the magnitudes of some C elements, which, in turn, severely degrades its accuracy and renders it useless. Chandra Shekhar has brought forth two open source softwares, Advanced Convolution Coefficient Calculator (ACCC) and Precise Convolution Coefficient Calculator (PCCC), which handle these accuracy issues adequately. ACCC performs the computation by using floating point numbers, in an iterative manner.[21] The precision of the floating-point numbers is gradually increased in each iteration, by using GNU MPFR. Once the obtained C's in two consecutive iterations start having same significant digits until a pre-specified distance, the convergence is assumed to have reached. If the distance is sufficiently large, the computation yields a highly accurate C. PCCC employs rational number calculations, by using GNU Multiple Precision Arithmetic Library, and yields a fully accurate C, in the rational number format.[22] In the end, these rational numbers are converted into floating point numbers, until a pre-specified number of significant digits.A database of C's that are calculated by using ACCC, for symmetric kernels and both symmetric and asymmetric polynomials, on unity-spaced kernel nodes, in the 1, 2, 3, and 4 dimensional spaces, is made available.[23] Chandra Shekhar has also laid out a mathematical framework that describes usage of C calculated on unity-spaced kernel nodes to perform filtering and partial differentiations (of various orders) on non-uniformly spaced kernel nodes,[20] allowing usage of C provided in the aforementioned database. Although this method yields approximate results only, they are acceptable in most engineering applications, provided that non-uniformity of the kernel nodes is weak.It is inevitable that the signal will be distorted in the convolution process. From property 3 above, when data which has a peak is smoothed the peak height will be reduced and the half-width will be increased. Both the extent of the distortion and S/N (signal-to-noise ratio) improvement:For example, If the noise in all data points is uncorrelated and has a constant standard deviation, σ, the standard deviation on the noise will be decreased by convolution with an m-point smoothing function to[25][note 5]These functions are shown in the plot at the right. For example, with a 9-point linear function (moving average) two thirds of the noise is removed and with a 9-point quadratic/cubic smoothing function only about half the noise is removed. Most of the noise remaining is low-frequency noise(see Frequency characteristics of convolution filters, below).Although the moving average function gives the best noise reduction it is unsuitable for smoothing data which has curvature over m points. A quadratic filter function is unsuitable for getting a derivative of a data curve with an inflection point because a quadratic polynomial does not have one. The optimal choice of polynomial order and number of convolution coefficients will be a compromise between noise reduction and distortion.[27]One way to mitigate distortion and improve noise removal is to use a filter of smaller width and perform more than one convolution with it. For two passes of the same filter this is equivalent to one pass of a filter obtained by convolution of the original filter with itself.[28] For example, 2 passes of the filter with coefficients (1/3, 1/3, 1/3) is equivalent to 1 pass of the filter with coefficients(1/9, 2/9, 3/9, 2/9, 1/9).The disadvantage of multipassing is that the equivalent filter width for n passes of an m-point function is n(m − 1) + 1 so multipassing is subject to greater end-effects. Nevertheless, multipassing has been used to great advantage. For instance, some 40–80 passes on data with a signal-to-noise ratio of only 5 gave useful results.[29]  The noise reduction formulae given above do not apply because correlation between calculated data points increases with each pass.Convolution maps to multiplication in the Fourier co-domain. The discrete Fourier transform of a convolution filter is a real-valued function which can be represented asθ runs from 0 to 180 degrees, after which the function merely repeats itself. The plot for a 9-point quadratic/cubic smoothing function is typical. At very low angle, the plot is almost flat, meaning that low-frequency components of the data will be virtually unchanged by the smoothing operation. As the angle increases the value decreases so that higher frequency components are more and more attenuated. This shows that the convolution filter can be described as a low-pass filter: the noise that is removed is primarily high-frequency noise and low-frequency noise passes through the filter.[30] Some high-frequency noise components are attenuated more than others, as shown by undulations in the Fourier transform at large angles. This can give rise to small oscillations in the smoothed data.[31]Convolution affects the correlation between errors in the data. The effect of convolution can be expressed as a linear transformation.By the law of error propagation, the variance-covariance matrix of the data, A  will be transformed into B according toTo see how this applies in practice, consider the effect of a 3-point moving average on the first three calculated points, Y2 − Y4, assuming that the data points have equal variance and that there is no correlation between them. A will be an identity matrix multiplied by a constant, σ2, the variance at each point.In this case the correlation coefficients, between calculated points i and j will beIn general, the calculated values are correlated even when the observed values are not correlated.  The correlation extends over m − 1 calculated points at a time.[32]To illustrate the effect of multipassing on the noise and correlation of a set of data, consider the effects of a second pass of a 3-point moving average filter. For the second pass[note 6]Correlation now extends over a span of 4 sequential points with correlation coefficients The advantage obtained by performing two passes with the narrower smoothing function is that it introduces less distortion into the calculated data.Selected values of the convolution coefficients for polynomials of degree 1,2,3, 4 and 5 are given in the following tables. The values were calculated using the PASCAL code provided in Gorry.[15]
Matrix (mathematics)
In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 × 3 (read "two by three"), because there are two rows and three columns:The individual items in an m × n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n × Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6]  Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m × n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 × 2 matrix.Matrices with a single row are called row vectors, and those with a single column are called column vectors. A matrix with the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.Matrices are commonly written in box brackets or parentheses:The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i − j. In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m × n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m × n as a subscript. For instance, the matrix A above is 3 × 4 and can be defined as A = [i − j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i − j]3×4.Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-×-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n − 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,∗ refers to the ith row of A, and a∗,j refers to the jth column of A. The set of all m-by-n matrices is denoted 𝕄(m, n).There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]This operation is called scalar multiplication, but its result is not named "scalar product" to avoid confusion, since "scalar product" is sometimes used as a synonym for "inner product".Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A + B = B + A.[12]The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:where 1 ≤ i ≤ m and 1 ≤ j ≤ p.[13] For example, the underlined entry 2340 in the product is calculated as (2 × 1000) + (3 × 100) + (4 × 10) = 2340:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A + B)C = AC + BC as well as C(A + B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they need not be equal, that is, generally that is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:whereasBesides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.There are three types of row operations:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A principal submatrix is a square submatrix obtained by removing certain rows and columns.  The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix as one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]Matrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n×1-matrix) of n variables x1, x2, ..., xn, and b is an m×1-column vector, then the matrix equationis equivalent to the system of linear equations[24]Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingwhere A−1 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Matrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn → Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.For example, the 2×2 matrixThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g : Rm → Rk, then the composition g ∘ f is represented by BA sinceThe last equality follows from the above-mentioned associativity of matrix multiplication.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank–nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line that runs from the top left corner to the bottom right corner of the matrix.If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged: A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix.  If instead, A is equal to the negative of its transpose, that is, A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A is called invertible or non-singular if there exists a matrix B such thatwhere In is the n×n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A−1.A symmetric n×n-matrix A is called positive-definite if the associated quadratic form has a positive value for every nonzero vector x in Rn. If f (x) takes only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.Allowing as input two different vectors instead yields the bilinear form associated to A:An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:which entailswhere I is the identity matrix of size n.An orthogonal matrix A is necessarily invertible (with inverse A−1 = AT), unitary (A−1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or −1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.The complex analogue of an orthogonal matrix is a unitary matrix.The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors: This is immediate from the definition of matrix multiplication:It follows that the trace of the product of more than two matrices is independent of cyclic permutations of the matrices, however this does not in general apply for arbitrary permutations (for example, tr(ABC) ≠ tr(BAC), in general). Also, the trace of a matrix is equal to that of its transpose, that is,The determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.The determinant of 2-by-2 matrices is given byThe determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of a product of square matrices equals the product of their determinants: Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]A number λ and a non-zero vector v satisfyingare called an eigenvalue and an eigenvector of A, respectively.[40][41] The number λ is an eigenvalue of an n×n-matrix A if and only if A−λIn is not invertible, which is equivalent toThe polynomial pA in an indeterminate X given by evaluation the determinant det(XIn−A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(λ) = 0 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]To choose the most appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace expansion (Adj (A) denotes the adjugate matrix of A)may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.The eigendecomposition or diagonalization expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix.[52]  If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaand the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings  known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.More generally, matrices with entries in a ring R are widely used in mathematics.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]Matrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be square matrices, and thus need not be members of any ring; but their sizes must fulfil certain compatibility conditions.Linear maps Rn → Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V → W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatIn other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]More generally, the set of m×n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity.  When n = m composition of these maps is possible, and this gives rise to the matrix ring of n×n matrices representing the endomorphism ring of Rn.A group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element must be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionform the orthogonal group.[67] Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication, and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns does. Products of two matrices of the given type is well defined (provided that the column-index and row-index sets match), is of the same type, and corresponds to the composition of linear maps.If R is a normed ring, then the condition of row or column finiteness can be relaxed.  With the norm in place, absolutely convergent series can be used instead of finite sums.  For example, the matrices whose column sums are absolutely convergent sequences form a ring.  Analogously, the matrices whose row sums are absolutely convergent series also form a ring.Infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that must be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]Complex numbers can be represented by particular real 2-by-2 matrices viaunder which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.The Hessian matrix of a differentiable function ƒ: Rn → R consists of the second derivatives of ƒ with respect to the several coordinate directions, that is,[81]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn → Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Stochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), …, (xN, yN), by a linear functionwhich can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group.  Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states.  The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies.The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by  B = H · A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104]  The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105]  Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The term "matrix" (Latin for "womb", derived from mater—mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103]  Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley–Hamilton theorem.[103]An English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy − 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialwhere Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen über die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.Many theorems were first established for small matrices only, for example the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra, [115] partially due to their use in classification of the hypercomplex number systems of the previous century.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.The word has been used in unusual ways by at least two authors of historical importance.Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910–1913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:For example, a function Φ(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ∀ai: Φ(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]
Tensor operator
In pure and applied mathematics, quantum mechanics and computer graphics, a tensor operator generalizes the notion of operators which are scalars and vectors. A special class of these are spherical tensor operators which apply the notion of the spherical basis and spherical harmonics. The spherical basis closely relates to the description of angular momentum in quantum mechanics and spherical harmonic functions. The coordinate-free generalization of a tensor operator  is known as a  representation operator.[1]In the same way, tensor quantities must be represented by tensor operators. An example of a tensorquantity (of rank two) is the electrical quadrupole moment of the above molecule. Likewise, the octupole and hexadecapole moments would be tensors of rank three and four, respectively.The rotation operator about the unit vector n (defining the axis of rotation) through angle θ iswhere J = (Jx, Jy, Jz) are the rotation generators (also the angular momentum matrices):Using the completeness condition:we haveIntroducing the Wigner D matrix elements:gives the matrix multiplication:For one basis ket:where Pℓm is an associated Legendre polynomial, ℓ is the orbital angular momentum quantum number, and m is the orbital magnetic quantum number which takes the values −ℓ, −ℓ + 1, ... ℓ − 1, ℓ The formalism of spherical harmonics have wide applications in applied mathematics, and are closely related to the formalism of spherical tensors, as shown below.Spherical harmonics are functions of the polar and azimuthal angles, ϕ and θ respectively, which can be conveniently collected into a unit vector n(θ, ϕ) pointing in the direction of those angles, in the Cartesian basis it is:Now as,we have,A scalar operator is invariant under rotations:[2]and we have a simple result, that the scalar operator commutes with the rotation generators:Examples of a scalar operators includeVector operators (as well as pseudovector operators) are a set of 3 operators that can be rotated according to:[2]where εijk is the Levi-Civita symbol, which all vector operators must satisfy, by construction. As the symbol εijk is a pseudotensor, pseudovector operators are invariant up to a sign: +1 for proper rotations and −1 for improper rotations.Vector operators includeand peusodovector operators includeIn Dirac notation:and since | Ψ > is any quantum state, the same result follows:Note that here, the term "vector" is used two different ways: kets such as |ψ⟩ are elements of abstract Hilbert spaces, while the vector operator is defined as a quantity whose components transform in a certain way under rotations.A vector operator in the spherical basis is V = (V+1, V0, V−1) where the components are:[2]and the commutators with the rotation generators are:where q is a placeholder for the spherical basis labels (+1, 0, −1), and:(some authors may place a factor of 1/2 on the left hand side of the equation) and raise (J+) or lower (J−) the total magnetic quantum number m by one unit. In the spherical basis the generators are:The rotation transformation in the spherical basis (originally written in the Cartesian basis) is then:One can generalize the vector operator concept easily to tensorial operators, shown next.A tensor operator can be rotated according to:[2]Consider a dyadic tensor with components Tij = aibj, this rotates infinitesimally according to:Cartesian dyadic tensors of the formwhere a and b are two vector operators:are reducible, which means they can be re-expressed in terms of a and b as a rank 0 tensor (scalar), plus a rank 1 tensor (an antisymmetric tensor), plus a rank 2 tensor (a symmetric tensor with zero trace):where the first termincludes just one component, a scalar equivalently written (a·b)/3,  the secondincludes three independent components, equivalently the components of (a×b)/2, and the thirdincludes five independent components. Throughout, δij is the Kronecker delta, the components of the identity matrix. The number in the superscripted brackets denotes the tensor rank. These three terms are irreducible, which means they cannot be decomposed further and still be tensors satisfying the defining transformation laws under which they must be invariant. These also correspond to the number of spherical harmonic functions 2ℓ + 1 for ℓ = 0, 1, 2, the same as the ranks for each tensor. Each of the irreducible representations T(1), T(2) ...  transform like angular momentum eigenstates according to the number of independent components.Example of a Tensor operator,in general,Note: This is just an example, in general, a tensor operator cannot be written as the product of two Tensor operators as given in the above example.Continuing the previous example of the second order dyadic tensor T = a ⊗ b, casting each of a and b into the spherical basis and substituting into T gives the spherical tensor operators of the second order, which are:Using the infinitesimal rotation operator and its Hermitian conjugate, one can derive the commutation relation in the spherical basis:and the finite rotation transformation in the spherical basis is:In general, tensor operators can be constructed from two perspectives.[3]One way is to specify how spherical tensors transform under a physical rotation - a group theoretical definition. A rotated angular momentum eigenstate can be decomposed into a linear combination of the initial eigenstates: the coefficients in the linear combination consist of Wigner rotation matrix entries. Spherical tensor operators are sometimes defined as the set of operators that transform just like the eigenkets under a rotation.A spherical tensor Tq(k) of rank k is defined to rotate into Tq′(k) according to:where q = k, k − 1, ..., −k + 1, −k. For spherical tensors, k and q are analogous labels to ℓ and m respectively, for spherical harmonics. Some authors write Tkq instead of Tq(k), with or without the parentheses enclosing the rank number k.Another related procedure requires that the spherical tensors satisfy certain commutation relations with respect to the rotation generators Jx, Jy, Jz - an algebraic definition.The commutation relations of the angular momentum components with the tensor operators are:For any 3d vector, not just a unit vector, and not just the position vector:a spherical tensor is a spherical harmonic as a function of this vector a, and in Dirac notation:(the super and subscripts switch places for the corresponding labels ℓ ↔ k and m ↔ q which spherical tensors and spherical harmonics use).Spherical harmonic states and spherical tensors can also be constructed out of the Clebsch–Gordan coefficients. Irreducible spherical tensors can build higher rank spherical tensors; if Aq1(k1) and Bq2(k2) are two spherical tensors of ranks k1 and k2 respectively, then:is a spherical tensor of rank k.The Hermitian adjoint of a spherical tensor may be defined asThere is some arbitrariness in the choice of the phase factor: any factor containing (−1)±q will satisfy the commutation relations.[4]  The above choice of phase has the advantages of being real and that the tensor product two commuting Hermitian operators is still Hermitian.[5]  Some authors define it with a different sign on q, without the k, or use only the floor of k[6].Orbital angular momentum operators have the ladder operators:which raise or lower the orbital magnetic quantum number mℓ by one unit. This has almost exactly the same form as the spherical basis, aside from constant multiplicative factors.Spherical tensors can also be formed from algebraic combinations of the spin operators Sx, Sy, Sz, as matrices, for a spin system with total quantum number j = ℓ + s (and ℓ = 0). Spin operators have the ladder operators:which raise or lower the spin magnetic quantum number ms by one unit.Spherical bases have broad applications in pure and applied mathematics and physical sciences where spherical geometries occur.The transition amplitude is proportional to matrix elements of the dipole operator between the initial and final states. We use an electrostatic, spinless model for the atom and we consider the transition from the initial energy level Enℓ to final level En′ℓ′. These levels are degenerate, since the energy does not depend on the magnetic quantum number m or m′. The wave functions have the form,The dipole operator is proportional to the position operator of the electron, so we must evaluate matrix elements of the form,where, the initial state is on the right and the final one on the left. The position operator r has three components, and the initial and final levels consist of 2ℓ + 1 and 2ℓ′ + 1 degenerate states, respectively. Therefore if we wish to evaluate the intensity of a spectral line as it would be observed, we really have to evaluate 3(2ℓ′+ 1)(2ℓ+ 1) matrix elements, for example, 3×3×5 = 45 in a 3d → 2p transition. This is actually an exaggeration, as we shall see, because many of the matrix elements vanish, but there are still many non-vanishing matrix elements to be calculated.A great simplification can be achieved by expressing the components of r, not with respect to the Cartesian basis, but with respect to the spherical basis. First we define,Next, by inspecting a table of the Yℓm′s, we find that for ℓ = 1 we have,where, we have multiplied each Y1m by the radius r. On the right hand side we see the spherical components rq of the position vector r. The results can be summarized by,for q = 1, 0, −1, where q appears explicitly as a magnetic quantum number. This equation reveals a relationship between vector operators and the angular momentum value ℓ = 1, something we will have more to say about presently. Now the matrix elements become a product of a radial integral times an angular integral,We see that all the dependence on the three magnetic quantum numbers (m′,q,m) is contained in the angular part of the integral. Moreover, the angular integral can be evaluated by the three-Yℓm formula, whereupon it becomes proportional to the Clebsch-Gordan coefficient,The radial integral is independent of the three magnetic quantum numbers (m′, q, m), and the trick we have just used does not help us to evaluate it. But it is only one integral, and after it has been done, all the other integrals can be evaluated just by computing or looking up Clebsch-Gordancoefficients.The selection rule m′ = q + m in the Clebsch-Gordan coefficient means that many of the integrals vanish, so we have exaggerated the total number of integrals that need to be done. But had we worked with the Cartesian components ri of r, this selection rule might not have been obvious. In any case, even with the selection rule, there may still be many nonzero integrals to be done (nine, in the case 3d → 2p).The example we have just given of simplifying the calculation of matrix elements for a dipole transition is really an application of the Wigner-Eckart theorem, which we take up later in these notes.The spherical tensor formalism provides a common platform for treating coherence and relaxation in nuclear magnetic resonance. In NMR and EPR, spherical tensor operators are employed to express the quantum dynamics of particle spin, by means of an equation of motion for the density matrix entries, or to formulate dynamics in terms of an equation of motion in Liouville space. The Liouville space equation of motion governs the observable averages of spin variables. When relaxation is formulated using a spherical tensor basis in Liouville space, insight is gained because the relaxation matrix exhibits the cross-relaxation of spin observables directly.[3]
Remez algorithm
The Remez algorithm  or Remez exchange algorithm, published by Evgeny Yakovlevich Remez in 1934,[1] is an iterative algorithm used to find simple approximations to functions, specifically, approximations by functions in a Chebyshev space that are the best in the uniform norm L∞ sense.A typical example of a Chebyshev space is the subspace of Chebyshev polynomials of order n in the space of real continuous functions on an interval, C[a, b].The polynomial of best approximation within a given subspace is defined to be the one that minimizes the maximum absolute difference between the polynomial and the function. In this case, the form of the solution is precised by the equioscillation theorem.The result is called the polynomial of best approximation or the minimax approximation algorithm.A review of technicalities in implementing the Remez algorithm is given by W. Fraser.[2]The Chebyshev nodes are a common choice for the initial approximation because of their role in the theory of polynomial interpolation. For the initialization of the optimization problem for function f by the Lagrange interpolant Ln(f), it can be shown that this initial approximation is bounded bywith the norm or Lebesgue constant of the Lagrange interpolation operator Ln of the nodes (t1, ..., tn + 1) beingT being the zeros of the Chebyshev polynomials, and the Lebesgue functions beingFor Chebyshev nodes, which provides a suboptimal, but analytically explicit choice, the asymptotic behavior is known as[5](γ being the Euler-Mascheroni constant) withand upper bound[6]This section provides more information on the steps outlined above. In this section, the index i runs from 0 to n+1.The error at the given n+2 ordered nodes is positive and negative in turn becauseSometimes more than one sample point is replaced at the same time with the locations of nearby maximum absolute differences.Sometimes relative error is used to measure the difference between the approximation and the function, especially if the approximation will be used to compute the function on a computer which uses floating point arithmetic.
Generalizations of Pauli matrices
In mathematics and physics, in particular quantum information, the term generalized Pauli matrices refers to families of matrices which generalize the (linear algebraic) properties of the Pauli matrices. Here,  a few classes of such matrices are summarized.Let Ejk be the matrix with 1 in the jk-th entry and 0 elsewhere. Consider the space of d×d  complex matrices,   ℂd×d,  for a fixed d.Define the following matrices,The collection of matrices defined above without the identity matrix are called the generalized Gell-Mann matrices, in dimension d.[1]The symbol ⊕  (utilized in the Cartan subalgebra above) means matrix direct sum.In dimensions d=2 and 3, the above construction recovers the Pauli and Gell-Mann matrices, respectively.The so-called Walsh–Hadamard conjugation matrix isThe goal now is to extend the above to higher dimensions, d, a problem solved by J. J. Sylvester (1882).Fix the dimension d as before. Let  ω = exp(2πi/d),  a root of unity. Since  ωd = 1  and  ω  ≠ 1,  the sum  of all roots annuls:Integer indices may then be cyclically identified mod d.Now define, with Sylvester, the shift matrix[2]and the clock matrix, These matrices generalize σ1 and σ3, respectively.Note that the unitarity and tracelessness of the two Pauli matrices  is preserved, but not Hermiticity in dimensions higher than two. Since Pauli matrices describe Quaternions, Sylvester dubbed the higher-dimensional analogs "nonions", "sedenions", etc.These two matrices are also the cornerstone of quantum mechanical dynamics in finite-dimensional vector spaces[3][4][5]  as formulated by Hermann Weyl, and find routine applications in numerous areas of mathematical physics.[6]  The clock matrix amounts to the exponential of position in a "clock" of d hours, and the shift matrix is just the translation operator in that cyclic vector space, so the exponential of the momentum. They are (finite-dimensional)  representations of the corresponding elements of the Weyl-Heisenberg on a d-dimensional Hilbert space.The following relations echo and generalize those of the Pauli matrices:and the braiding relation,the Weyl formulation of the CCR, and can be rewritten as On the other hand, to generalize the Walsh–Hadamard matrix W, noteDefine, again with Sylvester,  the following analog matrix,[7] still denoted by W in a slight abuse of notation,It is evident that  W is no longer Hermitian, but is still unitary. Direct calculation  yieldswhich is the desired analog result. Thus, W , a Vandermonde matrix, arrays the eigenvectors of   Σ1, which has the same eigenvalues as   Σ3.When d = 2k,   W * is precisely the matrix of the discrete Fourier transform,converting position coordinates to momentum coordinates and vice versa.The complete family of d2 unitary (but non-Hermitian) independent matrices
Signal-flow graph
A signal-flow graph or signal-flowgraph (SFG), invented by Claude Shannon,[1] but often called a Mason graph after Samuel Jefferson Mason who coined the term,[2] is a specialized flow graph, a directed graph in which nodes represent system variables, and branches (edges, arcs, or arrows) represent functional connections between pairs of nodes. Thus, signal-flow graph theory builds on that of directed graphs (also called digraphs), which includes as well that of oriented graphs. This mathematical theory of digraphs exists, of course, quite apart from its applications.[3][4]SFGs are most commonly used to represent signal flow in a physical system and its controller(s), forming a cyber-physical system.  Among their other uses are the representation of signal flow in various electronic networks and amplifiers, digital filters, state-variable filters and some other types of analog filters.  In nearly all literature, a signal-flow graph is associated with a set of linear equations.Wai-Kai Chen wrote: "The concept of a signal-flow graph was originally worked out by Shannon [1942][1] in dealing with analog computers. The greatest credit for the formulation of signal-flow graphs is normally extended to Mason [1953],[2] [1956].[5] He showed how to use the signal-flow graph technique to solve some difficult electronic problems in a relatively simple manner. The term signal flow graph was used because of its original application to electronic problems and the association with electronic signals and flowcharts of the systems under study."[6]Lorens wrote: "Previous to Mason's work, C. E. Shannon[1]  worked out a number of the properties of what are now known as flow graphs. Unfortunately, the paper originally had a restricted classification and very few people had access to the material."[7]"The rules for the evaluation of the graph determinant of a Mason Graph were first given and proven by Shannon [1942] using mathematical induction. His work remained essentially unknown even after Mason published his classical work in 1953. Three years later, Mason [1956] rediscovered the rules and proved them by considering the value of a determinant and how it changes as variables are added to the graph. [...]"[8]Robichaud et al. identify the domain of application of SFGs as follows:[9]The following illustration and its meaning were introduced by Mason to illustrate basic concepts:[2]In the simple flow graphs of the figure, a functional dependence of a node is indicated by an incoming arrow, the node originating this influence is the beginning of this arrow, and in its most general form the signal flow graph indicates by incoming arrows only those nodes that influence the processing at the receiving node, and at each node, i, the incoming variables are processed according to a function associated with that node, say Fi. The flowgraph in (a) represents a set of explicit relationships:Node x1 is an isolated node because no arrow is incoming; the equations for x2 and x3 have the graphs shown in parts (b) and (c) of the figure.These relationships define for every node a function that processes the input signals it receives. Each non-source node combines the input signals in some manner, and broadcasts a resulting signal along each outgoing branch. "A flow graph, as defined originally by Mason, implies a set of functional relations, linear or not."[9]However, the commonly used Mason graph is more restricted, assuming that each node simply sums its incoming arrows, and that each branch involves only the initiating node involved. Thus, in this more restrictive approach, the node x1 is unaffected while:and now the functions fij can be associated with the signal-flow branches ij joining the pair of nodes xi, xj, rather than having general relationships associated with each node. A contribution by a node to itself like f33 for x3  is called a self-loop. Frequently these functions are simply multiplicative factors (often called transmittances or gains), for example, fij(xj)=cijxj, where c is a scalar, but possibly a function of some parameter like the Laplace transform variable s. Signal-flow graphs are very often used with Laplace-transformed signals, and in this case the transmittance, c(s), often is called a transfer function.In general, there are several ways of choosing the variables in a complex system. Corresponding to each choice, a system of equations can be written and each system of equations can be represented in a graph. This formulation of the equations becomes direct and automatic if one has at his disposal techniques which permit the drawing of a graph directly from the schematic diagram of the system under study. The structure of the graphs thus obtained is related in a simple manner to the topology of the schematic diagram, and it becomes unnecessary to consider the equations, even implicitly, to obtain the graph. In some cases, one has simply to imagine the flow graph in the schematic diagram and the desired answers can be obtained without even drawing the flow graph.Robichaud et al. wrote: "The signal flow graph contains the same information as the equations from which it is derived; but there does not exist a one-to-one correspondence between the graph and the system of equations. One system will give different graphs according to the order in which the equations are used to define the variable written on the left-hand side."[9] If all equations relate all dependent variables, then there are n! possible SFGs to choose from.[12]Linear signal-flow graph methods only apply to linear time-invariant systems, as studied by their associated theory.  When modeling a system of interest, the first step is often to determine the equations representing the system's operation without assigning causes and effects (this is called acausal modeling).[13] A SFG is then derived from this system of equations.A linear SFG consists of nodes indicated by dots and weighted directional branches indicated by arrows. The nodes are the variables of the equations and the branch weights are the coefficients. Signals may only traverse a branch in the direction indicated by its arrow.  The elements of a SFG can only represent the operations of multiplication by a coefficient and addition, which are sufficient to represent the constrained equations. When a signal traverses a branch in its indicated direction, the signal is multiplied the weight of the branch. When two or more branches direct into the same node, their outputs are added.For systems described by linear algebraic or differential equations, the signal-flow graph is mathematically equivalent to the system of equations describing the system, and the equations governing the nodes are discovered for each node by summing incoming branches to that node. These incoming branches convey the contributions of the other nodes, expressed as the connected node value multiplied by the weight of the connecting branch, usually a real number or function of some parameter (for example a Laplace transform variable s).For linear active networks, Choma writes:[14] "By a 'signal flow representation' [or 'graph', as it is commonly referred to] we mean a diagram that, by displaying the algebraic relationships among relevant branch variables of network, paints an unambiguous picture of the way an applied input signal ‘flows’ from input-to-output ... ports."A motivation for a SFG analysis is described by Chen:[15]A linear signal flow graph is related to a system of linear equations[16] of the following form:The figure to the right depicts various elements and constructs of a signal flow graph (SFG).[17]Terms used in linear SFG theory also include:[17]A signal-flow graph may be simplified by graph transformation rules.[19][20][21]  These simplification rules are also referred to as signal-flow graph algebra.[22]The purpose of this reduction is to relate the dependent variables of interest (residual nodes, sinks) to its independent variables (sources).The systematic reduction of a linear signal-flow graph is a graphical method equivalent to the Gauss-Jordan elimination method for solving linear equations.[23]The rules presented below may be applied over and over until the signal flow graph is reduced to its "minimal residual form".  Further reduction can require loop elimination or the use of a "reduction formula" with the goal to directly connect sink nodes representing the dependent variables to the source nodes representing the independent variables. By these means, any signal-flow graph can be simplified by successively removing internal nodes until only the input and output and index nodes remain.[24][25] Robichaud described this process of systematic flow-graph reduction:The reduction of a graph proceeds by the elimination of certain nodes to obtain a residual graph showing only the variables of interest. This elimination of nodes is called "node absorption". This method is close to the familiar process of successive eliminations of undesired variables in a system of equations. One can eliminate a variable by removing the corresponding node in the graph. If one reduces the graph sufficiently, it is possible to obtain the solution for any variable and this is the objective which will be kept in mind in this description of the different methods of reduction of the graph. In practice, however, the techniques of reduction will be used solely to transform the graph to a residual graph expressing some fundamental relationships. Complete solutions will be more easily obtained by application of Mason's rule.[26]The graph itself programs the reduction process. Indeed a simple inspection of the graph readily suggests the different steps of the reduction which are carried out by elementary transformations, by loop elimination, or by the use of a reduction formula.[26]For digitally reducing a flow graph using an algorithm, Robichaud extends the notion of a simple flow graph to a generalized flow graph:Before describing the process of reduction...the correspondence between the graph and a system of linear equations ... must be generalized...The generalized graphs will represent some operational relationships between groups of variables...To each branch of the generalized graph is associated a matrix giving the relationships between the variables represented by the nodes at the extremities of that branch...[27]The elementary transformations [defined by Robichaud in his Figure 7.2, p. 184] and the loop reduction permit the elimination of any node j of the graph by the reduction formula:[described in Robichaud's Equation 7-1]. With the reduction formula, it is always possible to reduce a graph of any order... [After reduction] the final graph will be a cascade graph in which the variables of the sink nodes are explicitly expressed as functions of the sources. This is the only method for reducing the generalized graph since Mason's rule is obviously inapplicable.[28]The definition of an elementary transformation varies from author to author:Parallel edges. Replace parallel edges with a single edge having a gain equal to the sum of original gains.The graph on the left has parallel edges between nodes. On the right, these parallel edges have been replaced with a single edge having a gain equal to the sum of the gains on each original edge.The equations corresponding to the reduction between N and node I1 are:Outflowing edges. Replace outflowing edges with edges directly flowing from the node's sources.The graph on the left has an intermediate node N between nodes from which it has inflows, and nodes to which it flows out.The graph on the right shows direct flows between these node sets, without transiting via N.For the sake of simplicity, N and its inflows are not represented. The outflows from N are eliminated.The equations corresponding to the reduction directly relating N's input signals to its output signals are:Zero-signal nodes.Eliminate outflowing edges from a node determined to have a value of zero.If the value of a node is zero, its outflowing edges can be eliminated.Nodes without outflows.Eliminate a node without outflows.In this case, N is not a variable of interest, and it has no outgoing edges; therefore, N, and its inflowing edges, can be eliminated.Self-looping edge. Replace looping edges by adjusting the gains on the incoming edges.The graph on the left has a looping edge at node N, with a gain of  g. On the right, the looping edge has been eliminated, and all inflowing edges have their gain divided by (1-g).The equations corresponding to the reduction between N and all its input signals are:The above procedure for building the SFG from an acausal system of equations and for solving the SFG's gains have been implemented[31] as an add-on to MATHLAB 68,[32] an on-line system providing machine aid for the mechanical symbolic processes encountered in analysis.Signal flow graphs can be used to solve sets of simultaneous linear equations.[33] The set of equations must be consistent and all equations must be linearly independent.For M equations with N unknowns where each yj is a known value and each xj is an unknown value, there is equation for each known of the following form.Although it is feasible, particularly for simple cases, to establish a signal flow graph using the equations in this form, some rearrangement allows a general procedure that works easily for any set of equations, as now is presented. To proceed, first the equations are rewritten asand further rewritten asand finally rewritten as The signal-flow graph is now arranged by selecting one of these equations and addressing the node on the right-hand side. This is the node for which the node connects to itself with the branch of weight including a '+1', making a self-loop in the flow graph. The other terms in that equation connect this node first to the source in this equation and then to all the other branches incident on this node. Every equation is treated this way, and then each incident branch is joined to its respective emanating node. For example, the case of three variables is shown in the figure, and the first equation is:where the right side of this equation is the sum of the weighted arrows incident on node x1.As there is a basic symmetry in the treatment of every node, a simple starting point is an arrangement of nodes with each node at one vertex of a regular polygon. When expressed using the general coefficients {cin}, the environment of each node is then just like all the rest apart from a permutation of indices. Such an implementation for a set of three simultaneous equations is seen in the figure.[34]Often the known values, yj are taken as the primary causes and the unknowns values, xj to be effects, but regardless of this interpretation, the last form for the set of equations can be represented as a signal-flow graph. This point is discussed further in the subsection Interpreting 'causality'.In the most general case, the values for all the xk variables can be calculated by computing Mason's gain formula for the path from each yj to each xk and using superposition.In general, there are N-1 paths from yj to variable xk so the computational effort to calculated Gkj is proportional to N-1.Since there are M values of yj, Gkj must be computed M times for a single value of xk.  The computational effort to calculate a single xk variable is proportional to (N-1)(M).  The effort to compute all the xk variables is proportional to (N)(N-1)(M).  If there are N equations and N unknowns, then the computation effort is on the order of N3.For some authors, a linear signal-flow graph is more constrained than a block diagram,[35] in that the SFG rigorously describes linear algebraic equations represented by a directed graph.For other authors, linear block diagrams and linear signal-flow graphs are equivalent ways of depicting a system, and either can be used to solve the gain.[36]A tabulation of the comparison between block diagrams and signal-flow graphs is provided by Bakshi & Bakshi,[37] and another tabulation by Kumar.[38]  According to Barker et al.:[39]In the figure, a simple block diagram for a feedback system is shown with two possible interpretations as a signal-flow graph. The input R(s) is the Laplace-transformed input signal; it is shown as a source node in the signal-flow graph (a source node has no input edges). The output signal C(s) is the Laplace-transformed output variable. It is represented as a sink node in the flow diagram (a sink has no output edges). G(s) and H(s) are transfer functions, with H(s) serving to feed back a modified version of the output to the input, B(s). The two flow graph representations are equivalent.The term "cause and effect" was applied by Mason to SFGs:[2] and has been repeated by many later authors:[40]However, Mason's paper is concerned to show in great detail how a set of equations is connected to an SFG, an emphasis unrelated to intuitive notions of "cause and effect". Intuitions can be helpful for arriving at an SFG or for gaining insight from an SFG, but are inessential to the SFG. The essential connection of the SFG is to its own set of equations, as described, for example, by Ogata:[41]There is no reference to "cause and effect" here, and as said by Barutsky:[42]The term "cause and effect" may be misinterpreted as it applies to the SFG, and taken incorrectly to suggest a system view of causality,[43] rather than a computationally based meaning. To keep discussion clear, it may be advisable to use the term "computational causality",  as is suggested for bond graphs:[44]The term "computational causality" is explained using the example of current and voltage in a resistor:[45]A computer program or algorithm can be arranged to solve a set of equations using various strategies. They differ in how they prioritize finding some of the variables in terms of the others, and these algorithmic decisions, which are simply about solution strategy, then set up the variables expressed as dependent variables earlier in the solution to be "effects", determined by the remaining variables that now are "causes", in the sense of "computational causality".Using this terminology, it is computational causality, not system causality, that is relevant to the SFG. There exists a wide-ranging philosophical debate, not concerned specifically with the SFG, over connections between computational causality and system causality.[46]Signal-flow graphs can be used for analysis, that is for understanding a model of an existing system, or for synthesis, that is for determining the properties of a design alternative.When building a model of a dynamic system, a list of steps is provided by Dorf & Bishop:[47]In this workflow, equations of the physical system's mathematical model are used to derive the signal-flow graph equations.Signal-flow graphs have been used in Design Space Exploration (DSE), as an intermediate representation towards a physical implementation.  The DSE process seeks a suitable solution among different alternatives. In contrast with the typical analysis workflow, where a system of interest is first modeled with the physical equations of its components, the specification for synthesizing a design could be a desired transfer function. For example, different strategies would create different signal-flow graphs, from which implementations are derived.[48]Another example uses an annotated SFG as an expression of the continuous-time behavior, as input to an architecture generator[49]Shannon's formula is an analytic expression for calculating the gain of an interconnected set of amplifiers in an analog computer.  During World War II, while investigating the functional operation of an analog computer, Claude Shannon developed his formula.  Because of wartime restrictions, Shannon's work was not published at that time, and, in 1952, Mason rediscovered the same formula.Happ generalized the Shannon formula for topologically closed systems.[50] The Shannon-Happ formula can be used for deriving transfer functions, sensitivities, and error functions.[51]For a consistent set of linear unilateral relations, the Shannon-Happ formula expresses the solution using direct substitution (non-iterative).[51][52]NASA's electrical circuit software NASAP is based on the Shannon-Happ formula.[51][52]The amplification of a signal V1 by an amplifier with gain a12 is described mathematically byThis relationship represented by the signal-flow graph of Figure 1. is that V2 is dependent on V1 but it implies no dependency of V1 on V2.  See Kou page 57.[53]A possible SFG for the asymptotic gain model for a negative feedback amplifier is shown in Figure 3, and leads to the equation for the gain of this amplifier asThe interpretation of the parameters is as follows: T = return ratio, G∞ = direct amplifier gain, G0 = feedforward (indicating the possible bilateral nature of the feedback, possibly deliberate as in the case of feedforward compensation). Figure 3 has the interesting aspect that it resembles Figure 2 for the two-port network with the addition of the extra feedback relation x2 =  T y1.From this gain expression an interpretation of the parameters G0 and G∞ is evident, namely:There are many possible SFG's associated with any particular gain relation. Figure 4 shows another SFG for the asymptotic gain model that can be easier to interpret in terms of a circuit. In this graph, parameter  β is interpreted as a feedback factor and A as a "control parameter", possibly related to a dependent source in the circuit. Using this graph, the gain isTo connect to the asymptotic gain model, parameters A and β cannot be arbitrary circuit parameters, but must relate to the return ratio T by:and to the asymptotic gain as:Substituting these results into the gain expression,which is the formula of the asymptotic gain model.The figure to the right depicts a circuit that contains a y-parameter two-port network. Vin is the input of the circuit and V2 is the output.  The two-port equations impose a set of linear constraints between its port voltages and currents.  The terminal equations impose other constraints.  All these constraints are represented in the SFG (Signal Flow Graph) below the circuit.  There is only one path from input to output which is shown in a different color and has a (voltage) gain of -RLy21.  There are also three loops: -Riny11, -RLy22, Riny21RLy12. Sometimes a loop indicates intentional feedback but it can also indicate a constraint on the relationship of two variables.  For example, the equation that describes a resistor says that the ratio of the voltage across the resistor to the current through the resistor is a constant which is called the resistance.  This can be interpreted as the voltage is the input and the current is the output, or the current is the input and the voltage is the output, or merely that the voltage and current have a linear relationship.  Virtually all passive two terminal devices in a circuit will show up in the SFG as a loop.The SFG and the schematic depict the same circuit, but the schematic also suggests the circuit's purpose.  Compared to the schematic, the SFG is awkward but it does have the advantage that the input to output gain can be written down by inspection using Mason's rule.This example is representative of a SFG (signal-flow graph) used to represent a servo control system and illustrates several features of SFGs.  Some of the loops (loop 3, loop 4 and loop 5) are extrinsic intentionally designed feedback loops.  These are shown with dotted lines.  There are also intrinsic loops (loop 0, loop1, loop2) that are not intentional feedback loops, although they can be analyzed as though they were.  These loops are shown with solid lines.  Loop 3 and loop 4 are also known as minor loops because they are inside a larger loop.See Mason's rule for development of Mason's Gain Formula for this example.There is some confusion in literature about what a signal-flow graph is; Henry Paynter, inventor of bond graphs, writes: "But much of the decline of signal-flow graphs [...] is due in part to the mistaken notion that the branches must be linear and the nodes must be summative. Neither assumption was embraced by Mason, himself !"[55]A state transition SFG or state diagram is a simulation diagram for a system of equations, including the initial conditions of the states.[56] Closed flowgraphs describe closed systems and have been utilized to provide a rigorous theoretical basis for topological techniques of circuit analysis.[50]Mason introduced both nonlinear and linear flow graphs.  To clarify this point, Mason wrote : "A linear flow graph is one whose associated equations are linear."[2]It we denote by xj the signal at node j, the following are examples of node functions that do not pertain to a linear time-invariant system:
Identifiability analysis
Identifiability analysis is a group of methods found in mathematical statistics that are used to determine how well the parameters of a model are estimated by the quantity and quality of experimental data.[1] Therefore, these methods explore not only identifiability of a model, but also the relation of the model to particular experimental data or, more generally, the data collection process.Assuming a model is fit to experimental data, the goodness of fit does not reveal how reliable the parameter estimates are. The goodness of fit is also not sufficient to prove the model was chosen correctly. For example, if the experimental data is noisy or if there is an insufficient amount of data points, it could be that the estimated parameter values could vary drastically without significantly influencing the goodnes of fit. To address this issues the identifiability analysis could be applied as an important step to ensure correct choice of model, and sufficient amount of experimental data. The purpose of this analysis is either a quantified proof of correct model choice and integrality of experimental data acquired or such analysis can serve as an instrument for the detection of non-identifiable and sloppy parameters, helping planning the experiments and in building and improvement of the model at the early stages.Structural identifiability analysis is a particular type of analysis in which the model structure itself is investigated for non-identifiability. Recognized non-identifiabilities may be removed analytically through substitution of the non-identifiable parameters with their combinations or by another way. The model overloading with number of independent parameters after its application to simulate finite experimental dataset may provide the good fit to experimental data by the price of making fitting results not sensible to the changes of parameters values, therefore leaving parameter values undetermined. Structural methods are also referred to as a priori, because non-identifiability analysis in this case could also be performed prior to the calculation of the fitting score functions, by exploring the number degrees of freedom (statistics) for the model and the number of independent experimental conditions to be varied.Practical identifiability analysis can be performed by exploring the fit of existing model to experimental data. Once the fitting in any measure was obtained, parameter identifiability analysis can be performed either locally near a given point (usually near the parameter values provided the best model fit) or globally over the extended parameter space. The common example of the practical identifiability analysis is profile likelihood method.
Dimension (vector space)
In mathematics, the dimension of a vector space V is the cardinality (i.e. the number of vectors) of a basis of V over its base field.[1] It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension.For every vector space there exists a basis,[a] and all bases of a vector space have equal cardinality;[b] as a result, the dimension of a vector space is uniquely defined. We say V is finite-dimensional if the dimension of V is finite, and infinite-dimensional if its dimension is infinite.The dimension of the vector space V over the field F can be written as dimF(V) or as [V : F], read "dimension of V over F". When F can be inferred from context, dim(V) is typically written.The vector space R3 has as a basis, and therefore we have dimR(R3) = 3. More generally, dimR(Rn) = n, and even more generally, dimF(Fn) = n for any field F.The complex numbers C are both a real and complex vector space; we have dimR(C) = 2 and dimC(C) = 1. So the dimension depends on the base field.The only vector space with dimension 0 is {0}, the vector space consisting only of its zero element.If W is a linear subspace of V, then dim(W) ≤ dim(V).To show that two finite-dimensional vector spaces are equal, one often uses the following criterion: if V is a finite-dimensional vector space and W is a linear subspace of V with dim(W) = dim(V), then W = V.Rn has the standard basis {e1, ..., en}, where ei is the i-th column of the corresponding identity matrix. Therefore Rn has dimension n.Any two vector spaces over F having the same dimension are isomorphic. Any bijective map between their bases can be uniquely extended to a bijective linear map between the vector spaces. If B is some set, a vector space with dimension |B| over F can be constructed as follows: take the set F(B) of all functions f : B → F such that f(b) = 0 for all but finitely many b in B. These functions can be added and multiplied with elements of F, and we obtain the desired F-vector space. An important result about dimensions is given by the rank–nullity theorem for linear maps.If F/K is a field extension, then F is in particular a vector space over K. Furthermore, every F-vector space V is also a K-vector space. The dimensions are related by the formulaIn particular, every complex vector space of dimension n is a real vector space of dimension 2n.Some simple formulae relate the dimension of a vector space with the cardinality of the base field and the cardinality of the space itself.If V is a vector space over a field F then, denoting the dimension of V by dim V, we have:One can see a vector space as a particular case of a matroid, and in the latter there is a well-defined notion of dimension. The length of a module and the rank of an abelian group both have several properties similar to the dimension of vector spaces.The Krull dimension of a commutative ring, named after Wolfgang Krull (1899–1971), is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring.Alternatively, one may be able to take the trace of operators on an infinite-dimensional space; in this case a (finite) trace is defined, even though no (finite) dimension exists, and gives a notion of "dimension of the operator". These fall under the rubric of "trace class operators" on a Hilbert space, or more generally nuclear operators on a Banach space.
Partial differential equation
In mathematics, a partial differential equation (PDE) is a differential equation that contains beforehand unknown multivariable functions and their partial derivatives. PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a computer model. A special case is ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.PDEs can be used to describe a wide variety of phenomena such as sound, heat, diffusion, electrostatics, electrodynamics, fluid dynamics, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.Partial differential equations (PDEs) are equations that involve rates of change with respect to continuous variables. The position of a rigid body is specified by six parameters,[1] but the configuration of a fluid is given by the continuous distribution of several parameters, such as the temperature, pressure, and so forth. The dynamics for the rigid body take place in a finite-dimensional configuration space; the dynamics for the fluid occur in an infinite-dimensional configuration space. This distinction usually makes PDEs much harder to solve than ordinary differential equations (ODEs), but here again, there will be simple solutions for linear problems. Classic domains where PDEs are used include acoustics, fluid dynamics, electrodynamics, and heat transfer.A partial differential equation (PDE) for the function u(x1,… xn) is an equation of the formIf f is a linear function of u and its derivatives, then the PDE is called linear. Common examples of linear PDEs include the heat equation, the wave equation, Laplace's equation, Helmholtz equation, Klein–Gordon equation, and Poisson's equation.A relatively simple PDE isThis relation implies that the function u(x,y) is independent of x. However, the equation gives no information on the function's dependence on the variable y. Hence the general solution of this equation iswhere f is an arbitrary function of y. The analogous ordinary differential equation iswhich has the solutionwhere c is any constant value. These two examples illustrate that general solutions of ordinary differential equations (ODEs) involve arbitrary constants, but solutions of PDEs involve arbitrary functions. A solution of a PDE is generally not unique; additional conditions must generally be specified on the boundary of the region where the solution is defined. For instance, in the simple example above, the function f(y) can be determined if u is specified on the line x = 0.Although the issue of existence and uniqueness of solutions of ordinary differential equations has a very satisfactory answer with the Picard–Lindelöf theorem, that is far from the case for partial differential equations. The Cauchy–Kowalevski theorem states that the Cauchy problem for any partial differential equation whose coefficients are analytic in the unknown function and its derivatives, has a locally unique analytic solution. Although this result might appear to settle the existence and uniqueness of solutions, there are examples of linear partial differential equations whose coefficients have derivatives of all orders (which are nevertheless not analytic) but which have no solutions at all: see Lewy (1957). Even if the solution of a partial differential equation exists and is unique, it may nevertheless have undesirable properties.  The mathematical study of these questions is usually in the more powerful context of weak solutions.An example of pathological behavior is the sequence (depending upon n) of Cauchy problems for the Laplace equationwith boundary conditionswhere n is an integer. The derivative of u with respect to y approaches zero uniformly in x as n increases, but the solution isThis solution approaches infinity if nx is not an integer multiple of π for any non-zero value of y. The Cauchy problem for the Laplace equation is called ill-posed or not well-posed, since the solution does not continuously depend on the data of the problem. Such ill-posed problems are not usually satisfactory for physical applications.The existence of solutions for the Navier–Stokes equations, a partial differential equation, is part of one of the Millennium Prize Problems.In PDEs, it is common to denote partial derivatives using subscripts. That is:Especially in physics, del or nabla (∇) is often used to denote spatial derivatives, and u̇, ü for time derivatives. For example, the wave equation (described below) can be written asorwhere Δ is the Laplace operator.Some linear, second-order partial differential equations can be classified as parabolic, hyperbolic and elliptic. Others, such as the Euler–Tricomi equation, have different types in different regions. The classification provides a guide to appropriate initial and boundary conditions and to the smoothness of the solutions.Assuming uxy = uyx, the general linear second-order PDE in two independent variables has the formwhere the coefficients A, B, C... may depend upon x and y. If A2 + B2 + C2 > 0 over a region of the xy-plane, the PDE is second-order in that region. This form is analogous to the equation for a conic section:More precisely, replacing ∂x by X, and likewise for other variables (formally this is done by a Fourier transform), converts a constant-coefficient PDE into a polynomial of the same degree, with the top degree (a homogeneous polynomial, here a quadratic form) being most significant for the classification.Just as one classifies conic sections and quadratic forms into parabolic, hyperbolic, and elliptic based on the discriminant B2 − 4AC, the same can be done for a second-order PDE at a given point.  However, the discriminant in a PDE is given by B2 − AC due to the convention of the xy term being 2B rather than B; formally, the discriminant (of the associated quadratic form) is (2B)2 − 4AC = 4(B2 − AC), with the factor of 4 dropped for simplicity.If there are n independent variables x1, x2 ,… xn, a general linear partial differential equation of second order has the formThe classification depends upon the signature of the eigenvalues of the coefficient matrix ai,j.The classification of partial differential equations can be extended to systems of first-order equations, where the unknown u is now a vector with m components, and the coefficient matrices Aν are m by m matrices for ν = 1, 2,… n. The partial differential equation takes the formwhere the coefficient matrices Aν and the vector B may depend upon x and u. If a hypersurface S is given in the implicit formwhere φ has a non-zero gradient, then S is a characteristic surface for the operator L at a given point if the characteristic form vanishes:The geometric interpretation of this condition is as follows: if data for u are prescribed on the surface S, then it may be possible to determine the normal derivative of u on S from the differential equation. If the data on S and the differential equation determine the normal derivative of u on S, then S is non-characteristic. If the data on S and the differential equation do not determine the normal derivative of u on S, then the surface is characteristic, and the differential equation restricts the data on S: the differential equation is internal to S.If a PDE has coefficients that are not constant, it is possible that it will not belong to any of these categories but rather be of mixed type. A simple but important example is the Euler–Tricomi equationwhich is called elliptic-hyperbolic because it is elliptic in the region x < 0, hyperbolic in the region x > 0, and degenerate parabolic on the line x = 0.In the phase space formulation of quantum mechanics,  one may consider the quantum Hamilton's equations for trajectories of quantum particles. These equations are infinite-order PDEs. However, in the semiclassical expansion, one has a finite system of ODEs at any fixed order of ħ.  The evolution equation of the Wigner function is also an infinite-order PDE. The quantum trajectories are quantum characteristics, with the use of which one could calculate the evolution of the Wigner function.Linear PDEs can be reduced to systems of ordinary differential equations by the important technique of separation of variables. This technique rests on a characteristic of solutions to differential equations: if one can find any solution that solves the equation and satisfies the boundary conditions, then it is the solution (this also applies to ODEs). We assume as an ansatz that the dependence of a solution on the parameters space and time can be written as a product of terms that each depend on a single parameter, and then see if this can be made to solve the problem.[2]In the method of separation of variables, one reduces a PDE to a PDE in fewer variables, which is an ordinary differential equation if in one variable – these are in turn easier to solve.This is possible for simple PDEs, which are called separable partial differential equations, and the domain is generally a rectangle (a product of intervals). Separable PDEs correspond to diagonal matrices – thinking of "the value for fixed x" as a coordinate, each coordinate can be understood separately.This generalizes to the method of characteristics, and is also used in integral transforms.In special cases, one can find characteristic curves on which the equation reduces to an ODE – changing coordinates in the domain to straighten these curves allows separation of variables, and is called the method of characteristics.More generally, one may find characteristic surfaces.An integral transform may transform the PDE to a simpler one, in particular, a separable PDE. This corresponds to diagonalizing an operator.An important example of this is Fourier analysis, which diagonalizes the heat equation using the eigenbasis of sinusoidal waves.If the domain is finite or periodic, an infinite sum of solutions such as a Fourier series is appropriate, but an integral of solutions such as a Fourier integral is generally required for infinite domains. The solution for a point source for the heat equation given above is an example of the use of a Fourier integral.Often a PDE can be reduced to a simpler form with a known solution by a suitable change of variables.  For example, the Black–Scholes PDEis reducible to the heat equationby the change of variables (for complete details see Solution of the Black Scholes Equation at the Wayback Machine (archived April 11, 2008))Inhomogeneous equations can often be solved (for constant coefficient PDEs, always be solved) by finding the fundamental solution (the solution for a point source), then taking the convolution with the boundary conditions to get the solution.This is analogous in signal processing to understanding a filter by its impulse response.The superposition principle applies to any linear system, including linear systems of PDEs. A common visualization of this concept is the interaction of two waves in phase being combined to result in a greater amplitude, for example sin x + sin x = 2 sin x. The same principle can be observed in PDEs where the solutions may be real or complex and additive. superpositionIf u1 and u2 are solutions of linear PDE in some function space R, then u = c1u1 + c2u2 with any constants c1 and c2 are also a solution of that PDE in the same function space.There are no generally applicable methods to solve nonlinear PDEs. Still, existence and uniqueness results (such as the Cauchy–Kowalevski theorem) are often possible, as are proofs of important qualitative and quantitative properties of solutions (getting these results is a major part of analysis). Computational solution to the nonlinear PDEs, the split-step method, exist for specific equations like nonlinear Schrödinger equation.Nevertheless, some techniques can be used for several types of equations. The h-principle is the most powerful method to solve underdetermined equations. The Riquier–Janet theory is an effective method for obtaining information about many analytic overdetermined systems.The method of characteristics (similarity transformation method) can be used in some very special cases to solve partial differential equations.In some cases, a PDE can be solved via perturbation analysis in which the solution is considered to be a correction to an equation with a known solution. Alternatives are numerical analysis techniques from simple finite difference schemes to the more mature multigrid and finite element methods. Many interesting problems in science and engineering are solved in this way using computers, sometimes high performance supercomputers.From 1870 Sophus Lie's work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called Lie groups, be referred to a common source; and that ordinary differential equations which admit the same infinitesimal transformations present comparable difficulties of integration. He also emphasized the subject of transformations of contact.A general approach to solving PDEs uses the symmetry property of differential equations, the continuous infinitesimal transformations of solutions to solutions (Lie theory). Continuous group theory, Lie algebras and differential geometry are used to understand the structure of linear and nonlinear partial differential equations for generating integrable equations, to find its Lax pairs, recursion operators, Bäcklund transform and finally finding exact analytic solutions to the PDE.Symmetry methods have been recognized to study differential equations arising in mathematics, physics, engineering, and many other disciplines.The adomian decomposition method, the Lyapunov artificial small parameter method, and He's homotopy perturbation method are all special cases of the more general homotopy analysis method. These are series expansion methods, and except for the Lyapunov method, are independent of small physical parameters as compared to the well known perturbation theory, thus giving these methods greater flexibility and solution generality.The three most widely used numerical methods to solve PDEs are the finite element method (FEM), finite volume methods (FVM) and finite difference methods (FDM), as well other kind of methods called Meshfree methods, which were made to solve problems where the before mentioned methods are limited. The FEM has a prominent position among these methods and especially its exceptionally efficient higher-order version hp-FEM. Other hybrid versions of FEM and Meshfree methods include the generalized finite element method (GFEM), extended finite element method (XFEM), spectral finite element method (SFEM), meshfree finite element method, discontinuous Galerkin finite element method (DGFEM), Element-Free Galerkin Method (EFGM), Interpolating Element-Free Galerkin Method (IEFGM), etc.The finite element method (FEM) (its practical application often known as finite element analysis (FEA)) is a numerical technique for finding approximate solutions of partial differential equations (PDE) as well as of integral equations. The solution approach is based either on eliminating the differential equation completely (steady state problems), or rendering the PDE into an approximating system of ordinary differential equations, which are then numerically integrated using standard techniques such as Euler's method, Runge–Kutta, etc.Finite-difference methods are numerical methods for approximating the solutions to differential equations using finite difference equations to approximate derivatives.Similar to the finite difference method or finite element method, values are calculated at discrete places on a meshed geometry. "Finite volume" refers to the small volume surrounding each node point on a mesh. In the finite volume method, surface integrals in a partial differential equation that contain a divergence term are converted to volume integrals, using the divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods conserve mass by design.
Vector space
A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.Euclidean vectors are an example of a vector space.  They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.The concept of vector space will first be explained by describing two particular examples:The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (−1)v = −v has the opposite direction and the same length as v (blue vector pointing down in the right image).A second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:and The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]A vector space over a field F is a set V together with two operations that satisfy the eight axioms listed below.Elements of V are commonly called vectors. Elements of F are commonly called scalars.In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.To qualify as a vector space, the set V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:Likewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.Subtraction of two vectors and division by a (non-zero) scalar can be defined asWhen the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.Vector addition and scalar multiplication are operations, satisfying the closure property:  u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example, the zero vector 0 of V and the additive inverse −v of any vector v are unique. Further properties follow by employing also the distributive law for the scalar multiplication, for example av equals 0 if and only if a equals 0 or v equals 0.Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by Möbius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.An important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed ofn-tuples (sequences of length n) of elements of F, such asA vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c ⋅ (x + iy) = (c ⋅ x) + i(c ⋅ y) for real numbers x, y, a, b and c.  The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.In fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.Functions from any fixed set Ω to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given byand similarly for multiplication. Such function spaces occur in many geometric situations, when Ω is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15]  Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:Systems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions of are given by triples with arbitrary a, b = a/2, and c = −5a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namelyyields f(x) = a e−x + bx e−x, where a and b are arbitrary constants, and ex is the natural exponential function.Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i ∈ I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. "Spanning the whole space" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:    (1)where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo–Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e−x and xe−x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(α) over Q depends on α. If α satisfies some polynomial equationThe relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure—i.e., they preserve sums and scalar multiplication:An isomorphism is a linear map f : V → W such that there exists an inverse map g : W → V, which is a map such that the two possible compositions f ∘ g : W → W and g ∘ f : V → V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26]  If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.For example, the "arrows in the plane" and "ordered pairs of numbers" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.Linear maps V → W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V∗.[28] Via the injective natural map V → V∗∗, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]Once a basis of V is chosen, linear maps f : V → W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no "canonical" or preferred isomorphism; actually an isomorphism φ : Fn → V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via φ. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context; see below.Matrices are a useful notion to encode linear maps.[32]  They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the followingor, using the matrix multiplication of the matrix A with the coordinate vector x:Moreover, after choosing bases of V and W, any linear map f : V → W is uniquely represented by a matrix via this assignment.[33]The determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.Endomorphisms, linear maps f : V → V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying λv = f(v), where λ is a scalar, is called an eigenvector of f with eigenvalue λ.[nb 5][35] Equivalently, v is an element of the kernel of the difference f − λ · Id (where Id is the identity map V → V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue λ is equivalent toBy spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in λ, called the characteristic polynomial of f.[36]  If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[37][nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[38][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[39]A linear subspace of dimension 1 is a vector line.  A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n – 1.The counterpart to subspaces are quotient vector spaces.[40] Given any subspace W ⊂ V, the quotient space V/W ("V modulo W") is defined as follows: as a set, it consists of v + W = {v + w : w ∈ W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a · (v + W) = (a · v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space "forgets" information that is contained in the subspace W.The kernel ker(f) of a linear map f : V → W consists of vectors v that are mapped to 0 in W.[41] Both kernel and image im(f) = {f(v) : v ∈ V} are subspaces of V and W, respectively.[42] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[43] Because of this, many statements such as the first isomorphism theorem (also called rank–nullity theorem in matrix-related terms)and the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.An important example is the kernel of a linear map x ↦ Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equationsIn the corresponding mapthe derivatives of the function f appear linearly (as opposed to f′′(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)′ = f′ + g ′ and (c·f)′ = c·f′ for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.The tensor product V ⊗F W, or simply V ⊗ W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g : V × W → X is called bilinear if g is linear in both variables v and w.  That is to say, for fixed w the map v ↦ g(v, w) is linear in the sense above and likewise for fixed v.The tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensorssubject to the rules These rules ensure that the map f from the V × W to V ⊗ W that maps a tuple (v, w) to v ⊗ w is bilinear. The universality states that given any vector space X and any bilinear map g : V × W → X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v ⊗ w) = g(v, w).[46] This is called the universal property of the tensor product, an instance of the method—much used in advanced abstract algebra—to indirectly define objects by specifying maps from or to this object.From the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question—crucial to analysis—whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.A vector space may be given a partial order ≤, under which some vectors can be compared.[47] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functionswhere f+ denotes the positive part of f and f− the negative part.[48]Coordinate space Fn can be equipped with the standard dot product:In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:Convergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[51][52] Compatible here means that addition and scalar multiplication have to be continuous maps.  Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.In such topological vector spaces one can consider series of vectors.  The infinite sumdenotes the limit of the corresponding finite partial sums of the sequence (fi)i∈N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[53]  In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[54] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only ifBanach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study—a key piece of functional analysis—focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[55] The image at the right shows the equivalence of the 1-norm and ∞-norm on R2: as the unit "balls" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.From a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V → W, maps between topological vector spaces are required to be continuous.[56] In particular, the (topological) dual space V∗ consists of continuous functionals V → R (or to C). The fundamental Hahn–Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[57]Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[58] Imposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[60]Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[61]The Hilbert space L2(Ω), with inner product given byBy definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[63] By the Stone–Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[64] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what "basic functions", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram–Schmidt process, it enables one to construct a basis of orthogonal vectors.[65] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[66] As an example from physics, the time-dependent Schrödinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[67] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[68]General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[69] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone–Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[70]Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):Examples include the vector space of n-by-n matrices, with [x, y] = xy − yx, the commutator of two matrices, and R3, endowed with the cross product.The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[72] As a vector space, it is spanned by symbols, called simple tensorsThe multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product ⊗, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 ⊗ v2 and v2 ⊗ v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 ⊗ v2 = − v2 ⊗ v1 yields the exterior algebra.[73]When a field, F is explicitly stated, a common term used is F-algebra.Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[74] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[75]A distribution (or generalized function) is a linear map assigning a number to each "test" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[76] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain Ω:When Ω = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by δ, which associates to a test function f its value at the p: δ(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax–Milgram theorem, a consequence of the Riesz representation theorem).[77]Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][78] The underlying vector space is usually the Hilbert space L2(0, 2π), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[79] The Fourier expansion of an L2 function f isThe coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[80]In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[81] A complex-number form of Fourier series is also commonly used.[80] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[82] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[83]Fourier series are used to solve boundary value problems in partial differential equations.[84] In 1822, Fourier first used this technique to solve the heat equation.[85] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[86] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[87] The JPEG image format is an application of the closely related discrete cosine transform.[88]The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[89] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[90] They in turn are applied in digital filters[91] and as a rapid multiplication algorithm for polynomials and large integers (Schönhage–Strassen algorithm).[92][93]The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact.  The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[94]Riemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[95] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[96][97] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[98]A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[94] More precisely, a vector bundle over X is a topological space E equipped with a continuous map such that for every x in X, the fiber π−1(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X × V → X makes the product X × V into a "trivial" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of π to π−1(U) is isomorphic[nb 16] to the trivial bundle U × V → U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be "twisted" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X × V). For example, the Möbius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 × R, because the latter is orientable whereas the former is not.[99]Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 × R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[100] K-theory studies the isomorphism classes of all vector bundles over some topological space.[101] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.Modules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[102] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors.  Some authors use the term vector space to mean modules over a division ring.[103]  The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.Roughly, affine spaces are vector spaces whose origins are not specified.[104] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the mapIf W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x ∈ W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v ∈ V. An important example is the space of solutions of a system of inhomogeneous linear equationsgeneralizing the homogeneous case b = 0 above.[105] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[106] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.   
Cross product
The cross product of two vectors a and b is defined only in three-dimensional space and is denoted by a × b. In physics, sometimes the notation a ∧ b is used,[2] though this is avoided in mathematics to avoid confusion with the exterior product.The cross product a × b is defined as a vector c that is perpendicular (orthogonal) to both a and b, with a direction given by the right-hand rule and a magnitude equal to the area of the parallelogram that the vectors span.The cross product is defined by the formula[3][4]where θ is the angle between a and b in the plane containing them (hence, it is between 0° and 180°), ‖a‖ and ‖b‖ are the magnitudes of vectors a and b, and n is a unit vector perpendicular to the plane containing a and b in the direction given by the right-hand rule (illustrated). If the vectors a and b are parallel (i.e., the angle θ between them is either 0° or 180°), by the above formula, the cross product of a and b is the zero vector 0.By convention, the direction of the vector n is given by the right-hand rule, where one simply points the forefinger of the right hand in the direction of a and the middle finger in the direction of b. Then, the vector n is coming out of the thumb (see the adjacent picture). Using this rule implies that the cross product is anti-commutative, i.e., b × a = −(a × b). By pointing the forefinger toward b first, and then pointing the middle finger toward a, the thumb will be forced in the opposite direction, reversing the sign of the product vector.Using the cross product requires the handedness of the coordinate system to be taken into account (as explicit in the definition above). If a left-handed coordinate system is used, the direction of the vector n is given by the left-hand rule and points in the opposite direction.This, however, creates a problem because transforming from one arbitrary reference system to another (e.g., a mirror image transformation from a right-handed to a left-handed coordinate system), should not change the direction of n. The problem is clarified by realizing that the cross product of two vectors is not a (true) vector, but rather a pseudovector.  See cross product and handedness for more detail.In 1881, Josiah Willard Gibbs, and independently Oliver Heaviside, introduced both the dot product and the cross product using a period (a . b) and an "x" (a x b), respectively, to denote them.[5]In 1877, to emphasize the fact that the result of a dot product is a scalar while the result of a cross product is a vector, William Kingdon Clifford coined the alternative names scalar product and vector product for the two operations.[5] These alternative names are still widely used in the literature.Both the cross notation (a × b) and the name cross product were possibly inspired by the fact that each scalar component of a × b is computed by multiplying non-corresponding components of a and b. Conversely, a dot product a ⋅ b involves multiplications between corresponding components of a and b. As explained below, the cross product can be expressed in the form of a determinant of a special 3 × 3 matrix. According to Sarrus's rule, this involves multiplications between matrix elements identified by crossed diagonals.The standard basis vectors i, j, and k satisfy the following equalities in a right hand coordinate system:which imply, by the anticommutativity of the cross product, that The definition of the cross product also implies thatThese equalities, together with the distributivity and linearity of the cross product (but both do not follow easily from the definition given above), are sufficient to determine the cross product of any two vectors a and b. Each vector can be defined as the sum of three orthogonal components parallel to the standard basis vectors:Their cross product a × b can be expanded using distributivity:This can be interpreted as the decomposition of a × b into the sum of nine simpler cross products involving vectors aligned with i, j, or k. Each one of these nine cross products operates on two vectors that are easy to handle as they are either parallel or orthogonal to each other. From this decomposition, by using the above-mentioned equalities and collecting similar terms, we obtain:meaning that the three scalar components of the resulting vector s = s1i + s2j + s3k = a × b areUsing column vectors, we can represent the same result as follows:The cross product can also be expressed as the formal[note 1] determinant:This determinant can be computed using Sarrus's rule or cofactor expansion. Using Sarrus's rule, it expands toUsing cofactor expansion along the first row instead, it expands to[6]which gives the components of the resulting vector directly.The magnitude of the cross product can be interpreted as the positive area of the parallelogram having a and b as sides (see Figure 1):Indeed, one can also compute the volume V of a parallelepiped having a, b and c as edges by using a combination of a cross product and a dot product, called scalar triple product (see Figure 2):Since the result of the scalar triple product may be negative, the volume of the parallelepiped is given by its absolute value. For instance,Because the magnitude of the cross product goes by the sine of the angle between its arguments, the cross product can be thought of as a measure of perpendicularity in the same way that the dot product is a measure of parallelism. Given two unit vectors, their cross product has a magnitude of 1 if the two are perpendicular and a magnitude of zero if the two are parallel. The dot product of two unit vectors behaves just oppositely: it is zero when the unit vectors are perpendicular and 1 if the unit vectors are parallel.Unit vectors enable two convenient identities: the dot product of two unit vectors yields the cosine (which may be positive or negative) of the angle between the two unit vectors. The magnitude of the cross product of the two unit vectors yields the sine (which will always be positive).If the cross product of two vectors is the zero vector (i.e. a × b = 0), then either one or both of the inputs is the zero vector, (a = 0 or b = 0) or else they are parallel or antiparallel (a ∥ b) so that the sine of the angle between them is zero (θ = 0° or θ = 180° and sinθ = 0).The self cross product of a vector is the zero vector:The cross product is anticommutative,distributive over addition,and compatible with scalar multiplication so thatIt is not associative, but satisfies the Jacobi identity:Distributivity, linearity and Jacobi identity show that the R3 vector space together with vector addition and the cross product forms a Lie algebra, the Lie algebra of the real orthogonal group in 3 dimensions, SO(3).The cross product does not obey the cancellation law: that is, a × b = a × c with a ≠ 0 does not imply b = c, but only that:This can be the case where b and c cancel, but additionally where a and b − c are parallel; that is, they are related by a scale factor t, leading to:for some scalar t.If, in addition to a × b = a × c and a ≠ 0 as above, it is the case that a ⋅ b = a ⋅ c thenAs b − c cannot be simultaneously parallel (for the cross product to be 0) and perpendicular (for the dot product to be 0) to a, it must be the case that b and c cancel: b = c.From the geometrical definition, the cross product is invariant under proper rotations about the axis defined by a × b. In formulae:More generally, the cross product obeys the following identity under matrix transformations:The cross product of two vectors lies in the null space of the 2 × 3 matrix with the vectors as rows:For the sum of two cross products, the following identity holds:The product rule of differential calculus applies to any bilinear operation, and therefore also to the cross product:where a and b are vectors that depend on the real variable t.The cross product is used in both forms of the triple product. The scalar triple product of three vectors is defined asIt is the signed volume of the parallelepiped with edges a, b and c and as such the vectors can be used in any order that's an even permutation of the above ordering. The following therefore are equal:The vector triple product is the cross product of a vector with the result of another cross product, and is related to the dot product by the following formulaThe mnemonic "BAC minus CAB" is used to remember the order of the vectors in the right hand member. This formula is used in physics to simplify vector calculations. A special case, regarding gradients and useful in vector calculus, iswhere ∇2 is the vector Laplacian operator.Other identities relate the cross product to the scalar triple product:where I is the identity matrix.The cross product and the dot product are related by:The right-hand side is the Gram determinant of a and b, the square of the area of the parallelogram defined by the vectors. This condition determines the magnitude of the cross product. Namely, since the dot product is defined, in terms of the angle θ between the two vectors, as:the above given relationship can be rewritten as follows:Invoking the Pythagorean trigonometric identity one obtains:which is the magnitude of the cross product expressed in terms of θ, equal to the area of the parallelogram defined by a and b (see definition above).The combination of this requirement and the property that the cross product be orthogonal to its constituents a and b provides an alternative definition of the cross product.[8]The relation:can be compared with another relation involving the right-hand side, namely Lagrange's identity expressed as:[9]where a and b may be n-dimensional vectors. This also shows that the Riemannian volume form for surfaces is exactly the surface element from vector calculus. In the case where n = 3, combining these two equations results in the expression for the magnitude of the cross product in terms of its components:[10]The same result is found directly using the components of the cross product found from:In R3, Lagrange's equation is a special case of the multiplicativity |vw| = |v||w| of the norm in the quaternion algebra.It is a special case of another formula, also sometimes called Lagrange's identity, which is the three dimensional case of the Binet–Cauchy identity:[11][12]If a = c and b = d this simplifies to the formula above.The cross product conveniently describes the infinitesimal generators of rotations in R3. Specifically, if n is a unit vector in R3 and R(φ, n) denotes a rotation about the axis through the origin specified by n, with angle φ (measured in radians, counterclockwise when viewed from the tip of n), thenfor every vector x in R3. The cross product with n therefore describes the infinitesimal generator of the rotations about n. These infinitesimal generators form the Lie algebra so(3) of the rotation group SO(3), and we obtain the result that the Lie algebra R3 with cross product is isomorphic to the Lie algebra so(3).The vector cross product also can be expressed as the product of a skew-symmetric matrix and a vector:[11]where superscript T refers to the transpose operation, and [a]× is defined by:The columns [a]×,i of the skew-symmetric matrix for a vector a can be also obtained by calculating the cross product with unit vectors, i.e.:orAlso, if a is itself expressed as a cross product:thenHence, the left hand side equalsNow, for the right hand side,And its transpose isEvaluation of the right hand side givesComparison shows that the left hand side equals the right hand side.This result can be generalized to higher dimensions using geometric algebra. In particular in any dimension bivectors can be identified with skew-symmetric matrices, so the product between a skew-symmetric matrix and vector is equivalent to the grade-1 part of the product of a bivector and vector.[13]  In three dimensions bivectors are dual to vectors so the product is equivalent to the cross product, with the bivector instead of its vector dual. In higher dimensions the product can still be calculated but bivectors have more degrees of freedom and are not equivalent to vectors.[13]This notation is also often much easier to work with, for example, in epipolar geometry.From the general properties of the cross product follows immediately thatand from fact that [a]× is skew-symmetric it follows thatThe above-mentioned triple product expansion (bac–cab rule) can be easily proven using this notation.As mentioned above, the Lie algebra R3 with cross product is isomorphic to the Lie algebra so(3), whose elements can be identified with the 3×3 skew-symmetric matrices. The map a → [a]× provides an isomorphism between R3 and so(3). Under this map, the cross product of 3-vectors corresponds to the commutator of 3x3 skew-symmetric matrices.These matrices share the following properties:For other properties of orthogonal projection matrices, see projection (linear algebra).The cross product can alternatively be defined in terms of the Levi-Civita symbol εijk and a dot product ηmi (= δmi for an orthonormal basis), which are useful in converting vector notation for tensor applications:in which repeated indices are summed over the values 1 to 3. Note that this representation is another form of the skew-symmetric representation of the cross product:In classical mechanics: representing the cross product by using the Levi-Civita symbol can cause mechanical symmetries to be obvious when physical systems are isotropic. (An example: consider a particle in a Hooke's Law potential in three-space, free to oscillate in three dimensions; none of these dimensions are "special" in any sense, so symmetries lie in the cross-product-represented angular momentum, which are made clear by the abovementioned Levi-Civita representation).[citation needed]The word "xyzzy" can be used to remember the definition of the cross product.Ifwhere:then:The second and third equations can be obtained from the first by simply vertically rotating the subscripts, x → y → z → x. The problem, of course, is how to remember the first equation, and two options are available for this purpose: either to remember the relevant two diagonals of Sarrus's scheme (those containing i), or to remember the xyzzy sequence.Since the first diagonal in Sarrus's scheme is just the main diagonal of the above-mentioned 3×3 matrix, the first three letters of the word xyzzy can be very easily remembered.Similarly to the mnemonic device above, a "cross" or X can be visualized between the two vectors in the equation. This may be helpful for remembering the correct cross product formula.Ifthen:The cross product has applications in various contexts:  e.g. it is used in computational geometry, physics and engineering.A non-exhaustive list of examples follows.The cross product appears in the calculation of the distance of two skew lines (lines not in the same plane) from each other in three-dimensional space.The cross product can be used to calculate the normal for a triangle or polygon, an operation frequently performed in computer graphics. For example, the winding of a polygon (clockwise or anticlockwise) about a point within the polygon can be calculated by triangulating the polygon (like spoking a wheel) and summing the angles (between the spokes) using the cross product to keep track of the sign of each angle.which is the signed length of the cross product of the two vectors.The cross product is used in calculating the volume of a polyhedron such as a tetrahedron or parallelepiped.The cross product frequently appears in the description of rigid motions. Two points P  and Q on a rigid body can be related by:In vector calculus, the cross product is used to define the formula for the vector operator curl.The trick of rewriting a cross product in terms of a matrix multiplication appears frequently in epipolar and multi-view geometry, in particular when deriving matching constraints.The cross product can be defined in terms of the exterior product. In this context, it is an external product.[14] This view allows for a natural geometric interpretation of the cross product. In exterior algebra the exterior product of two vectors is a bivector. A bivector is an oriented plane element, in much the same way that a vector is an oriented line element. Given two vectors a and b, one can view the bivector a ∧ b as the oriented parallelogram spanned by a and b. The cross product is then obtained by taking the Hodge star of the bivector a ∧ b, mapping 2-vectors to vectors:This can be thought of as the oriented multi-dimensional element "perpendicular" to the bivector. Only in three dimensions is the result an oriented line element – a vector – whereas, for example, in 4 dimensions the Hodge dual of a bivector is two-dimensional – another oriented plane element. So, only in three dimensions is the cross product of a and b the vector dual to the bivector a ∧ b: it is perpendicular to the bivector, with orientation dependent on the coordinate system's handedness, and has the same magnitude relative to the unit normal vector as a ∧ b has relative to the unit bivector; precisely the properties described above.When measurable quantities involve cross products, the handedness of the coordinate systems used cannot be arbitrary. However, when physics laws are written as equations, it should be possible to make an arbitrary choice of the coordinate system (including handedness). To avoid problems, one should be careful to never write down an equation where the two sides do not behave equally under all transformations that need to be considered. For example, if one side of the equation is a cross product of two vectors, one must take into account that when the handedness of the coordinate system is not fixed a priori, the result is not a (true) vector but a pseudovector. Therefore, for consistency, the other side must also be a pseudovector.[citation needed]More generally, the result of a cross product may be either a vector or a pseudovector, depending on the type of its operands (vectors or pseudovectors). Namely, vectors and pseudovectors are interrelated in the following ways under application of the cross product:So by the above relationships, the unit basis vectors i, j and k of an orthonormal, right-handed (Cartesian) coordinate frame must all be pseudovectors (if a basis of mixed vector types is disallowed, as it normally is) since i × j = k, j × k = i and k × i = j.Because the cross product may also be a (true) vector, it may not change direction with a mirror image transformation. This happens, according to the above relationships, if one of the operands is a (true) vector and the other one is a pseudovector (e.g., the cross product of two vectors). For instance, a vector triple product involving three (true) vectors is a (true) vector.A handedness-free approach is possible using exterior algebra.There are several ways to generalize the cross product to the higher dimensions.The cross product can be seen as one of the simplest Lie products, and is thus generalized by Lie algebras, which are axiomatized as binary products satisfying the axioms of multilinearity, skew-symmetry, and the Jacobi identity. Many Lie algebras exist, and their study is a major field of mathematics, called Lie theory.The cross product can also be described in terms of quaternions, and this is why the letters i, j, k are a convention for the standard basis on R3. The unit vectors i, j, k correspond to "binary" (180 deg) rotations about their respective axes (Altmann, S. L., 1986, Ch. 12), said rotations being represented by "pure" quaternions (zero real part) with unit norms.For instance, the above given cross product relations among i, j, and k agree with the multiplicative relations among the quaternions i, j, and k. In general, if a vector [a1, a2, a3] is represented as the quaternion a1i + a2j + a3k, the cross product of two vectors can be obtained by taking their product as quaternions and deleting the real part of the result. The real part will be the negative of the dot product of the two vectors.Alternatively, using the above identification of the 'purely imaginary' quaternions with R3, the cross product may be thought of as half of the commutator of two quaternions.A cross product for 7-dimensional vectors can be obtained in the same way by using the octonions instead of the quaternions. The nonexistence of nontrivial vector-valued cross products of two vectors in other dimensions is related to the result from Hurwitz's theorem that the only normed division algebras are the ones with dimension 1, 2, 4, and 8.In general dimension, there is no direct analogue of the binary cross product that yields specifically a vector. There is however the exterior product, which has similar properties, except that the exterior product of two vectors is now a 2-vector instead of an ordinary vector. As mentioned above, the cross product can be interpreted as the exterior product in three dimensions by using the Hodge star operator to map 2-vectors to vectors.  The Hodge dual of the exterior product yields an (n − 2)-vector, which is a natural generalization of the cross product in any number of dimensions.The exterior product and dot product can be combined (through summation) to form the geometric product.As mentioned above, the cross product can be interpreted in three dimensions as the Hodge dual of the exterior product. In any finite n dimensions, the Hodge dual of the exterior product of n-1 vectors is a vector. So, instead of a binary operation, in arbitrary finite dimensions, the cross product is generalized as the Hodge dual of the exterior product of some given n-1 vectors. This generalization is called external product[15]In the context of multilinear algebra, the cross product can be seen as the (1,2)-tensor (a mixed tensor, specifically a bilinear map) obtained from the 3-dimensional volume form,[note 2] a (0,3)-tensor, by raising an index.These products are all multilinear and skew-symmetric, and can be defined in terms of the determinant and parity.If the cross product is defined as a binary operation, it takes as input exactly two vectors. If its output is not required to be a vector or a pseudovector but instead a matrix, then it can be generalized in an arbitrary number of dimensions.[16][17][18]See § Alternative ways to compute the cross product for numerical details.In 1773, the Italian mathematician Joseph Louis Lagrange, (born Giuseppe Luigi Lagrangia), introduced the component form of both the dot and cross products in order to study the tetrahedron in three dimensions.[19]  In 1843 the Irish mathematical physicist Sir William Rowan Hamilton introduced the quaternion product, and with it the terms "vector" and "scalar". Given two quaternions [0, u] and [0, v], where u and v are vectors in R3, their quaternion product can be summarized as [−u ⋅ v, u × v]. James Clerk Maxwell used Hamilton's quaternion tools to develop his famous electromagnetism equations, and for this and other reasons quaternions for a time were an essential part of physics education.In 1878 William Kingdon Clifford published his Elements of Dynamic which was an advanced text for its time. He defined the product of two vectors[20] to have magnitude equal to the area of the parallelogram of which they are two sides, and direction perpendicular to their plane.Oliver Heaviside in England and Josiah Willard Gibbs, a professor at Yale University in Connecticut, also felt that quaternion methods were too cumbersome, often requiring the scalar or vector part of a result to be extracted. Thus, about forty years after the quaternion product, the dot product and cross product were introduced—to heated opposition. Pivotal to (eventual) acceptance was the efficiency of the new approach, allowing Heaviside to reduce the equations of electromagnetism from Maxwell's original 20 to the four commonly seen today.[21]Largely independent of this development, and largely unappreciated at the time, Hermann Grassmann created a geometric algebra not tied to dimension two or three, with the exterior product playing a central role. In 1853 Augustin-Louis Cauchy, a contemporary of Grassmann, published a paper on algebraic keys which were used to solve equations and had the same multiplication properties as the cross product.[22][23] William Kingdon Clifford combined the algebras of Hamilton and Grassmann to produce Clifford algebra, where in the case of three-dimensional vectors the bivector produced from two vectors dualizes to a vector, thus reproducing the cross product.The cross notation and the name "cross product" began with Gibbs. Originally they appeared in privately published notes for his students in 1881 as Elements of Vector Analysis. The utility for mechanics was noted by Aleksandr Kotelnikov. Gibbs's notation and the name "cross product" later reached a wide audience through Vector Analysis, a textbook by Edwin Bidwell Wilson, a former student. Wilson rearranged material from Gibbs's lectures, together with material from publications by Heaviside, Föpps, and Hamilton. He divided vector analysis into three parts:First, that which concerns addition and the scalar and vector products of vectors. Second, that which concerns the differential and integral calculus in its relations to scalar and vector functions. Third, that which contains the theory of the linear vector function.Two main kinds of vector multiplications were defined, and they were called as follows:Several kinds of triple products and products of more than three vectors were also examined. The above-mentioned triple product expansion was also included.
Dimension theorem for vector spaces
In mathematics, the dimension theorem for vector spaces states that all bases of a vector space have equally many elements. This number of elements may be finite or infinite (in the latter case, it is a cardinal number), and defines the dimension of the vector space.Formally, the dimension theorem for vector spaces states thatAs a basis is a generating set that is linearly independent, the theorem is a consequence of the following theorem, which is also useful:In particular if V is finitely generated, then all its bases are finite and have the  same number of elements.While the proof of the existence of a basis for any vector space in the general case requires Zorn's lemma and is in fact equivalent to the axiom of choice, the uniqueness of the cardinality of the basis requires only the ultrafilter lemma,[1] which is strictly weaker (the proof given below, however, assumes trichotomy, i.e., that all cardinal numbers are comparable, a statement which is also equivalent to the axiom of choice). The theorem can be generalized to arbitrary R-modules for rings R having invariant basis number.In the finitely generated case the proof uses only elementary arguments of algebra, and does not require the axiom of choice nor its weaker variants.Let V be a vector space, {ai: i ∈ I } be a linearly independent set of elements of V, and {bj: j ∈ J } be a generating set. One has to prove that the cardinality of I is not larger than that of J.If J is finite, this results from the Steinitz exchange lemma. (Indeed, the Steinitz exchange lemma implies every finite subset of I has cardinality not larger than that of J, hence I is finite with cardinality not larger than that of J.) If J is finite, a proof based on matrix theory is also possible.[2] Assume that J is infinite. If I is finite, there is nothing to prove. Thus, we may assume that I is also infinite. Let us suppose that the cardinality of I is larger than that of J.[note 1] We have to prove that this leads to a contradiction. By Zorn's lemma, every linearly independent set is contained in a maximal linearly independent set K. This maximality implies that K spans V and is therefore a basis (the maximality implies that every element of V is linearly dependent from the elements of K, and therefore is a linear combination of elements of K. As the cardinality of K is greater or equal with the cardinality of I, one may replace I with K, that is, one may suppose, without loss of generality, that I is a basis. Thus, every bj can be written as a finite sumThis application of the dimension theorem is sometimes itself called the dimension theorem. Letbe a linear transformation. Thenthat is, the dimension of U is equal to the dimension of the transformation's range plus the dimension of the kernel. See rank–nullity theorem for a fuller discussion.
Skew-Hermitian matrix
In linear algebra, a square matrix with complex entries is said to be skew-Hermitian or antihermitian if its conjugate transpose is the negative of the original matrix.[1] That is, the matrix A is skew-Hermitian if it satisfies the relationfor all i and j, where ai,j is the i,j-th entry of A, and the overline denotes complex conjugation.Skew-Hermitian matrices can be understood as the complex versions of real skew-symmetric matrices, or as the matrix analogue of the purely imaginary numbers.[2]  The set of all skew-Hermitian n×n matrices forms the u(n) Lie algebra, which corresponds to the Lie group U(n).The concept can be generalized to include linear transformations of any complex vector space with a sesquilinear norm.Imaginary numbers can be thought of as skew-adjoint (since they are like 1-by-1 matrices), whereas real numbers correspond to self-adjoint operators.For example, the following matrix is skew-Hermitian:
General linear group
In mathematics, the general linear group of degree n is the set of n×n invertible matrices, together with the operation of ordinary matrix multiplication. This forms a group, because the product of two invertible matrices is again invertible, and the inverse of an invertible matrix is invertible. The group is so named because the columns of an invertible matrix are linearly independent, hence the vectors/points they define are in general linear position, and matrices in the general linear group take points in general linear position to points in general linear position.To be more precise, it is necessary to specify what kind of objects may appear in the entries of the matrix. For example, the general linear group over R (the set of real numbers) is the group of n×n invertible matrices of real numbers, and is denoted by GLn(R) or GL(n, R).More generally, the general linear group of degree n over any field F (such as the complex numbers), or a ring R (such as the ring of integers), is the set of n×n invertible matrices with entries from F (or R), again with matrix multiplication as the group operation.[1] Typical notation is GLn(F) or GL(n, F), or simply GL(n) if the field is understood.More generally still, the general linear group of a vector space GL(V) is the abstract automorphism group, not necessarily written as matrices.The special linear group, written SL(n, F) or SLn(F), is the subgroup of GL(n, F) consisting of matrices with a determinant of 1.The group GL(n, F) and its subgroups are often called linear groups or matrix groups (the abstract group GL(V) is a linear group but not a matrix group). These groups are important in the theory of group representations, and also arise in the study of spatial symmetries and symmetries of vector spaces in general, as well as the study of polynomials. The modular group may be realised as a quotient of the special linear group SL(2, Z).If n ≥ 2, then the group GL(n, F) is not abelian.If V is a vector space over the field F, the general linear group of V, written GL(V) or Aut(V), is the group of all automorphisms of V, i.e. the set of all bijective linear transformations V → V, together with functional composition as group operation. If V has finite dimension n, then GL(V) and GL(n, F) are isomorphic. The isomorphism is not canonical; it depends on a choice of basis in V. Given a basis (e1, ..., en) of V and an automorphism T in GL(V), we have then for every basis vector ei thatfor some constants aij in F; the matrix corresponding to T is then just the matrix with entries given by the aij.In a similar way, for a commutative ring R the group GL(n, R) may be interpreted as the group of automorphisms of a free R-module M of rank n. One can also define GL(M) for any R-module, but in general this is not isomorphic to GL(n, R) (for any n).Over a field F, a matrix is invertible if and only if its determinant is nonzero. Therefore, an alternative definition of GL(n, F) is as the group of matrices with nonzero determinant.Over a commutative ring R, more care is needed: a matrix over R is invertible if and only if its determinant is a unit in R, that is, if its determinant is invertible in R. Therefore, GL(n, R) may be defined as the group of matrices whose determinants are units.Over a non-commutative ring R, determinants are not at all well behaved. In this case, GL(n, R) may be defined as the unit group of the matrix ring M(n, R).The general linear group GL(n, R) over the field of real numbers is a real Lie group of dimension n2. To see this, note that the set of all n×n real matrices, Mn(R), forms a real vector space of dimension n2. The subset GL(n, R) consists of those matrices whose determinant is non-zero. The determinant is a polynomial map, and hence GL(n, R) is an open affine subvariety of Mn(R) (a non-empty open subset of Mn(R) in the Zariski topology), and therefore[2]a smooth manifold of the same dimension.As a manifold, GL(n, R) is not connected but rather has two connected components: the matrices with positive determinant and the ones with negative determinant. The identity component, denoted by GL+(n, R), consists of the real n×n matrices with positive determinant. This is also a Lie group of dimension n2; it has the same Lie algebra as GL(n, R).The group GL(n, R) is also noncompact. "The"[3] maximal compact subgroup of GL(n, R) is the orthogonal group O(n), while "the" maximal compact subgroup of GL+(n, R) is the special orthogonal group SO(n). As for SO(n), the group GL+(n, R) is not simply connected (except when n = 1), but rather has a fundamental group isomorphic to Z for n = 2 or Z2 for n > 2.The general linear group over the field of complex numbers, GL(n, C), is a complex Lie group of complex dimension n2. As a real Lie group (through realification) it has dimension 2n2. The set of all real matrices forms a real Lie subgroup. These correspond to the inclusionswhich have real dimensions n2, 2n2, and 4n2 = (2n)2. Complex n-dimensional matrices can be characterized as real 2n-dimensional matrices that preserve a linear complex structure — concretely, that commute with a matrix J such that J2 = −I, where J corresponds to multiplying by the imaginary unit i.The Lie algebra corresponding to GL(n, C) consists of all n×n complex matrices with the commutator serving as the Lie bracket.Unlike the real case, GL(n, C) is connected. This follows, in part, since the multiplicative group of complex numbers C∗ is connected. The group manifold GL(n, C) is not compact; rather its maximal compact subgroup is the unitary group U(n). As for U(n), the group manifold GL(n, C) is not simply connected but has a fundamental group isomorphic to Z.If F is a finite field with q elements, then we sometimes write GL(n, q) instead of GL(n, F). When p is prime, GL(n, p) is the outer automorphism group  of the group Zpn, and also the automorphism group, because Zpn is abelian, so the inner automorphism group is trivial.The order of GL(n, q) is: For example, GL(3, 2) has order (8 − 1)(8 − 2)(8 − 4) = 168. It is the automorphism group of the Fano plane and of the group Z23, and is also known as PSL(2, 7).More generally, one can count points of Grassmannian over F: in other words the number of subspaces of a given dimension k. This requires only finding the order of the stabilizer subgroup of one such subspace and dividing into the formula just given, by the orbit-stabilizer theorem.These formulas are connected to the Schubert decomposition of the Grassmannian, and are q-analogs of the Betti numbers of complex Grassmannians. This was one of the clues leading to the Weil conjectures.Note that in the limit q ↦ 1 the order of GL(n, q) goes to 0! – but under the correct procedure (dividing by (q − 1)n) we see that it is the order of the symmetric group (See Lorscheid's article) – in the philosophy of the field with one element, one thus interprets the symmetric group as the general linear group over the field with one element: Sn ≅ GL(n, 1).The general linear group over a prime field, GL(ν, p), was constructed and its order computed by Évariste Galois in 1832, in his last letter (to Chevalier) and second (of three) attached manuscripts, which he used in the context of studying the Galois group of the general equation of order pν.[4]The special linear group, SL(n, F), is the group of all matrices with determinant 1. They are special in that they lie on a subvariety – they satisfy a polynomial equation (as the determinant is a polynomial in the entries). Matrices of this type form a group as the determinant of the product of two matrices is the product of the determinants of each matrix. SL(n, F) is a normal subgroup of GL(n, F).If we write F× for the multiplicative group of F (excluding 0), then the determinant is a group homomorphismthat is surjective and its kernel is the special linear group. Therefore, by the first isomorphism theorem, GL(n, F)/SL(n, F) is isomorphic to F×. In fact, GL(n, F) can be written as a semidirect product:When F is R or C, SL(n, F) is a Lie subgroup of GL(n, F) of dimension n2 − 1. The Lie algebra of SL(n, F) consists of all n×n matrices over F with vanishing trace. The Lie bracket is given by the commutator.The special linear group SL(n, R) can be characterized as the group of volume and orientation preserving linear transformations of Rn.The group SL(n, C) is simply connected, while SL(n, R) is not. SL(n, R) has the same fundamental group as GL+(n, R), that is, Z for n = 2 and Z2 for n > 2.The set of all invertible diagonal matrices forms a subgroup of GL(n, F) isomorphic to (F×)n. In fields like R and C, these correspond to rescaling the space; the so-called dilations and contractions.A scalar matrix is a diagonal matrix which is a constant times the identity matrix. The set of all nonzero scalar matrices forms a subgroup of GL(n, F) isomorphic to F× . This group is the center of GL(n, F). In particular, it is a normal, abelian subgroup.The center of SL(n, F) is simply the set of all scalar matrices with unit determinant, and is isomorphic to the group of nth roots of unity in the field F.The so-called classical groups are subgroups of GL(V) which preserve some sort of bilinear form on a vector space V. These include theThese groups provide important examples of Lie groups.The projective linear group PGL(n, F) and the projective special linear group PSL(n, F) are the quotients of GL(n, F) and SL(n, F) by their centers (which consist of the multiples of the identity matrix therein); they are the induced action on the associated projective space.The affine group Aff(n, F) is an extension of GL(n, F) by the group of translations in Fn. It can be written as a semidirect product:where GL(n, F) acts on Fn in the natural manner. The affine group can be viewed as the group of all affine transformations of the affine space underlying the vector space Fn.One has analogous constructions for other subgroups of the general linear group: for instance, the special affine group is the subgroup defined by the semidirect product, SL(n, F) ⋉ Fn, and the Poincaré group is the affine group associated to the Lorentz group, O(1, 3, F) ⋉ Fn.The general semilinear group ΓL(n, F) is the group of all invertible semilinear transformations, and contains GL. A semilinear transformation is a transformation which is linear "up to a twist", meaning "up to a field automorphism under scalar multiplication". It can be written as a semidirect product:where Gal(F) is the Galois group of F (over its prime field), which acts on GL(n, F) by the Galois action on the entries.The main interest of ΓL(n, F) is that the associated projective semilinear group PΓL(n, F) (which contains PGL(n, F)) is the collineation group of projective space, for n > 2, and thus semilinear maps are of interest in projective geometry.If one removes the restriction of the determinant being non-zero, the resulting algebraic structure is a monoid, usually called the full linear monoid,[6][7][8] but occasionally also full linear semigroup,[9] general linear monoid[10][11] etc. It is actually a regular semigroup.[7]The infinite general linear group or stable general linear group is the direct limit of the inclusions GL(n, F) → GL(n + 1, F) as the upper left block matrix. It is denoted by either GL(F) or GL(∞, F), and can also be interpreted as invertible infinite matrices which differ from the identity matrix in only finitely many places.[12]It is used in algebraic K-theory to define K1, and over the reals has a well-understood topology, thanks to Bott periodicity.It should not be confused with the space of (bounded) invertible operators on a Hilbert space, which is a larger group, and topologically much simpler, namely contractible – see Kuiper's theorem.
Wilkinson's polynomial
In numerical analysis, Wilkinson's polynomial is a specific polynomial which was used by James H. Wilkinson in 1963 to illustrate a difficulty when finding the root of a polynomial: the location of the roots can be very sensitive to perturbations in the coefficients of the polynomial.The polynomial isSometimes, the term Wilkinson's polynomial is also used to refer to some other polynomials appearing in Wilkinson's discussion.Wilkinson's polynomial arose in the study of algorithms for finding the roots of a polynomialIt is a natural question in numerical analysis to ask whether the problem of finding the roots of p from the coefficients ci is well-conditioned.  That is, we hope that a small change in the coefficients will lead to a small change in the roots.  Unfortunately, this is not the case here.The problem is ill-conditioned when the polynomial has a multiple root. For instance, the polynomial x2 has a double root at x = 0. However, the polynomial x2 − ε (a perturbation of size ε) has roots at ±√ε, which is much bigger than ε when ε is small.It is therefore natural to expect that ill-conditioning also occurs when the polynomial has zeros which are very close. However, the problem may also be extremely ill-conditioned for polynomials with well-separated zeros. Wilkinson used the polynomial w(x) to illustrate this point (Wilkinson 1963).In 1984, he described the personal impact of this discovery:Wilkinson's polynomial is often used to illustrate the undesirability of naively computing eigenvalues of a matrix by first calculating the coefficients of the matrix's characteristic polynomial and then finding its roots, since using the coefficients as an intermediate step may introduce an extreme ill-conditioning even if the original problem was well conditioned.[2]Wilkinson's polynomialclearly has 20 roots, located at x = 1, 2, …, 20. These roots are far apart. However, the polynomial is still very ill-conditioned.Expanding the polynomial, one findsIf the coefficient of x19 is decreased from −210 by 2−23 to −210.0000001192, then the polynomial value w(20) decreases from 0 to −2−232019 = −6.25×1017, and the root at x = 20 grows to x ≈ 20.8 . The roots at x = 18 and x = 19 collide into a double root at  x ≈ 18.62 which turns into a pair of complex conjugate roots at x ≈ 19.5 ± 1.9i as the perturbation increases further. The 20 roots become (to 5 decimals)Some of the roots are greatly displaced, even though the change to the coefficient is tiny and the original roots seem widely spaced. Wilkinson showed by the stability analysis discussed in the next section that this behavior is related to the fact that some roots α (such as α = 15) have many roots β that are "close" in the sense that |α − β| is smaller than |α|.Wilkinson chose the perturbation of 2−23 because his Pilot ACE computer had 30-bit floating point significands, so for numbers around 210, 2−23 was an error in the first bit position not represented in the computer. The two real numbers, −210 and −210 − 2−23, are represented by the same floating point number, which means that 2−23 is the unavoidable error in representing a real coefficient close to −210 by a floating point number on that computer. The perturbation analysis shows that 30-bit coefficient precision is insufficient for separating the roots of Wilkinson's polynomial.Suppose that we perturb a polynomial p(x) = Π (x − αj)with roots αj by adding a small multiple t·c(x) of a polynomial c(x), and ask how this affects the roots αj. To first order, the change in the roots will be controlled by the derivativeWhen the derivative is large, the roots will be less stable under variations of t, and conversely if this derivative is small the roots will be stable. In particular,if αj is a multiple root, then the denominator vanishes. In this case, αj is usually not differentiable with respect to t (unless c happens to vanish there), and the roots will be extremely unstable.For small values of t the perturbed root is given by the power series expansion in tand one expects problems when |t| is larger than the radius of convergence of this power series, which is given by the smallest value of |t| such that the root αj becomes multiple. A very crude estimate for this radius takes half the distance from αj to the nearest root, and divides by the derivative above.In the example of Wilkinson's polynomial of degree 20, the roots are given by αj = j for j = 1, …, 20, and c(x) is equal to x19.So the derivative is given byThis shows that the root αj will be less stable if there are many rootsαk close to αj, in the sense that the distance|αj − αk| between them is smaller than |αj|.Example. For the root α1 = 1, the derivative is equal to1/19! which is very small; this root is stable even for large changes in t. This is because all the other roots β are a long way from it, in the sense that |α1 − β| = 1, 2, 3, ..., 19 is larger than |α1| = 1.For example even if t is as large as –10000000000, the root α1 only changes from 1 to about  0.99999991779380 (which is very close to the first order approximation 1 + t/19! ≈ 0.99999991779365). Similarly, the other small roots of Wilkinson's polynomial are insensitive to changes in t.Example. On the other hand, for the root α20 = 20, the derivative is equal to −2019/19!  which is huge (about 43000000), so this root is very sensitive to small changes in t. The other roots β are close to α20, in the sense that |β − α20| = 1, 2, 3, ..., 19 is less than |α20| = 20. For t = −2 − 23 the first-order approximation 20 − t·2019/19! = 25.137... to the perturbed root 20.84... is terrible; this is even more obvious for the root α19 where the perturbed root has a large imaginary part but the first-order approximation (and for that matter all higher-order approximations) are real. The reason for this discrepancy is that |t|  ≈ 0.000000119 is greater than the radius of convergence of the power series mentioned above (which is about 0.0000000029, somewhat smaller than the value  0.00000001 given by the crude estimate) so the linearized theory does not apply. For a value such as t = 0.000000001 that is significantly smaller than this radius of convergence, the first-order approximation  19.9569... is reasonably close to the root  19.9509...At first sight the roots α1 = 1 and α20 = 20 of Wilkinson's polynomial appear to be similar, as they are on opposite ends of a symmetric line of roots, and have the same set of distances 1, 2, 3, ..., 19 from other roots. However the analysis above shows that this is grossly misleading: the root α20 = 20 is less stable than α1 = 1  (to small perturbations in the coefficient of x19) by a factor of 2019 = 5242880000000000000000000.The second example considered by Wilkinson isThe twenty zeros of this polynomial are in a geometric progression with common ratio 2, and hence the quotientcannot be large. Indeed, the zeros of w2 are quite stable to large relative changes in the coefficients.The expansionexpresses the polynomial in a particular basis, namely that of the monomials. If the polynomial is expressed in another basis, then the problem of finding its roots may cease to be ill-conditioned. For example, in a Lagrange form, a small change in one (or several) coefficients need not change the roots too much.  Indeed, the basis polynomials for interpolation at the points 0, 1, 2, …, 20 areEvery polynomial (of degree 20 or less) can be expressed in this basis:For Wilkinson's polynomial, we findGiven the definition of the Lagrange basis polynomial ℓ0(x), a change in the coefficient d0 will produce no change in the roots of w.  However, a perturbation in the other coefficients (all equal to zero) will slightly change the roots. Therefore, Wilkinson's polynomial is well-conditioned in this basis.Wilkinson discussed "his" polynomial inIt is mentioned in standard text books in numerical analysis, likeOther references:A high-precision numerical computation is presented in:
Reflection (mathematics)
In mathematics, a reflection (also spelled reflexion)[1] is a mapping from a Euclidean space to itself that is an isometry with a hyperplane as a set of fixed points; this set is called the axis (in dimension 2) or plane (in dimension 3) of reflection. The image of a figure by a reflection is its mirror image in the axis or plane of reflection. For example the mirror image of the small Latin letter p for a reflection with respect to a vertical axis would look like q. Its image by reflection in a horizontal axis would look like b. A reflection is an involution: when applied twice in succession, every point returns to its original location, and every geometrical object is restored to its original state.The term reflection is sometimes used for a larger class of mappings from a Euclidean space to itself, namely the non-identity isometries that are involutions. Such isometries have a set of fixed points (the "mirror") that is an affine subspace, but is possibly smaller than a hyperplane. For instance a reflection through a point is an involutive isometry with just one fixed point; the image of the letter p under itwould look like a d. This operation is also known as a central inversion (Coxeter 1969, §7.2), and exhibits Euclidean space as a symmetric space. In a Euclidean vector space, the reflection in the point situated at the origin is the same as vector negation. Other examples include reflections in a line in three-dimensional space. Typically, however, unqualified use of the term "reflection" means reflection in a hyperplane. A figure that does not change upon undergoing a reflection is said to have reflectional symmetry.Some mathematicians use "flip" as a synonym for "reflection".[2][3][4]In a plane (or, respectively, 3-dimensional) geometry, to find the reflection of a point drop a perpendicular from the point to the line (plane) used for reflection, and extend it the same distance on the other side. To find the reflection of a figure, reflect each point in the figure.To reflect point P through the line AB using compass and straightedge, proceed as follows (see figure):Point Q is then the reflection of point P through line AB.The matrix for a reflection is orthogonal with determinant −1 and eigenvalues −1, 1, 1, ..., 1. The product of two such matrices is a special orthogonal matrix that represents a rotation. Every rotation is the result of reflecting in an even number of reflections in hyperplanes through the origin, and every improper rotation is the result of reflecting in an odd number. Thus reflections generate the orthogonal group, and this result is known as the Cartan–Dieudonné theorem.Similarly the Euclidean group, which consists of all isometries of Euclidean space, is generated by reflections in affine hyperplanes.  In general, a group generated by reflections in affine hyperplanes is known as a reflection group. The finite groups generated in this way are examples of Coxeter groups.Reflection across a line through the origin in two dimensions can be described by the following formulawhere v denotes the vector being reflected, l denotes any vector in the line being reflected in, and v·l denotes the dot product of v with l. Note the formula above can also be described aswhere the reflection of line l on v is equal to 2 times the projection of v on line l minus v.  Reflections in a line have the eigenvalues of 1, and −1.Given a vector v in Euclidean space ℝn, the formula for the reflection in the hyperplane through the origin, orthogonal to a, is given bywhere v ⋅ a denotes the dot product of v with a. Note that the second term in the above equation is just twice the vector projection of v onto a. One can easily check thatUsing the geometric product, the formula isSince these reflections are isometries of Euclidean space fixing the origin they may be represented by orthogonal matrices. The orthogonal matrix corresponding to the above reflection is the matrix whose entries arewhere δij is the Kronecker delta.The formula for the reflection in the affine hyperplane v⋅ a = c not through the origin is
Row and column spaces
In linear algebra, the column space (also called the range or image) of a matrix A is the span (set of all possible linear combinations) of its column vectors. The column space of a matrix is the image or range of the corresponding matrix transformation.The row space is defined similarly.This article considers matrices of real numbers.  The row and column spaces are subspaces of the real spaces Rn and Rm respectively.[2]Let A be an m-by-n matrix. ThenIf one considers the matrix as a linear transformation from Rn to Rm, then the column space of the matrix equals the image of this linear transformation.The column space of a matrix A is the set of all linear combinations of the columns in A.  If A = [a1, ...., an], then colsp(A) = span {a1, ...., an}.The concept of row space generalizes to matrices over C, the field of complex numbers, or over any field.Intuitively, given a matrix A, the action of the matrix A on a vector x will return a linear combination of the columns of A weighted by the coordinates of x as coefficients. Another way to look at this is that it will (1) first project x into the row space of A, (2) perform an invertible transformation, and (3) place the resulting vector y in the column space of A.  Thus the result y = A x must reside in the column space of A. See singular value decomposition for more details on this second interpretation.[clarification needed]Given a matrix J:the rows arer1 = (2,4,1,3,2),r2 = (−1,−2,1,0,5),r3 = (1,6,2,2,2),r4 = (3,6,2,5,1).Consequently, the row space of J is the subspace of R5 spanned by { r1, r2, r3, r4 }.   Since these four row vectors are linearly independent, the row space is 4-dimensional.  Moreover, in this case it can be seen that they are all orthogonal to the vector n = (6,−1,4,−4,0), so it can be deduced that the row space consists of all vectors in R5 that are orthogonal to n.Let K be a field of scalars. Let A be an m × n matrix, with column vectors v1, v2, ..., vn.  A linear combination of these vectors is any vector of the formwhere c1, c2, ..., cn are scalars.  The set of all possible linear combinations of v1, ... ,vn is called the column space of A.  That is, the column space of A is the span of the vectors v1, ... , vn.Any linear combination of the column vectors of a matrix A can be written as the product of A with a column vector:Therefore, the column space of A consists of all possible products Ax, for x ∈ Cn.  This is the same as the image (or range) of the corresponding matrix transformation.The columns of A span the column space, but they may not form a basis if the column vectors are not linearly independent.  Fortunately, elementary row operations do not affect the dependence relations between the column vectors.  This makes it possible to use row reduction to find a basis for the column space.For example, consider the matrixThe columns of this matrix span the column space, but they may not be linearly independent, in which case some subset of them will form a basis.  To find this basis, we reduce A to reduced row echelon form:At this point, it is clear that the first, second, and fourth columns are linearly independent, while the third column is a linear combination of the first two.  (Specifically, v3 = –2v1 + v2.)  Therefore, the first, second, and fourth columns of the original matrix are a basis for the column space:Note that the independent columns of the reduced row echelon form are precisely the columns with pivots.  This makes it possible to determine which columns are linearly independent by reducing only to echelon form.The above algorithm can be used in general to find the dependence relations between any set of vectors, and to pick out a basis from any spanning set.  A different algorithm for finding a basis from a spanning set is given in the row space article;  finding a basis for the column space of A is equivalent to finding a basis for the row space of the transpose matrix AT.To find the basis in a practical setting (e.g., for large matrices), the singular-value decomposition is typically used.The dimension of the column space is called the rank of the matrix.  The rank is equal to the number of pivots in the reduced row echelon form, and is the maximum number of linearly independent columns that can be chosen from the matrix.  For example, the 4 × 4 matrix in the example above has rank three.Because the column space is the image of the corresponding matrix transformation, the rank of a matrix is the same as the dimension of the image.  For example, the transformation R4 → R4 described by the matrix above maps all of R4 to some three-dimensional subspace.The nullity of a matrix is the dimension of the null space, and is equal to the number of columns in the reduced row echelon form that do not have pivots.[6]  The rank and nullity of a matrix A with n columns are related by the equation:This is known as the rank–nullity theorem.The left null space of A is the set of all vectors x such that xTA = 0T.  It is the same as the null space of the transpose of A. The product of the matrix AT and the vector x can be written in terms of the dot product of vectors:because row vectors of AT are transposes of column vectors vk of A.  Thus ATx = 0 if and only if x is orthogonal (perpendicular) to each of the column vectors of A.It follows that the left null space (the null space of AT) is the orthogonal complement to the column space of A.For a matrix A, the column space, row space, null space, and left null space are sometimes referred to as the four fundamental subspaces.Similarly the column space (sometimes disambiguated as right column space) can be defined for matrices over a ring K asfor any c1, ..., cn, with replacement of the vector m-space with "right free module", which changes the order of scalar multiplication of the vector vk to the scalar ck such that it is written in an unusual order vector–scalar.[7]Let K be a field of scalars. Let A be an m × n matrix, with row vectors r1, r2, ... , rm.  A linear combination of these vectors is any vector of the formwhere c1, c2, ... , cm are scalars.  The set of all possible linear combinations of r1, ... , rm is called the row space of A.  That is, the row space of A is the span of the vectors r1, ... , rm.For example, ifthen the row vectors are r1 = (1, 0, 2) and r2 = (0, 1, 0).  A linear combination of r1 and r2 is any vector of the formThe set of all such vectors is the row space of A.  In this case, the row space is precisely the set of vectors (x, y, z) ∈ K3 satisfying the equation z = 2x (using Cartesian coordinates, this set is a plane through the origin in three-dimensional space).For a matrix that represents a homogeneous system of linear equations, the row space consists of all linear equations that follow from those in the system.The column space of A is equal to the row space of AT.The row space is not affected by elementary row operations.  This makes it possible to use row reduction to find a basis for the row space.For example, consider the matrixThe rows of this matrix span the row space, but they may not be linearly independent, in which case the rows will not be a basis.  To find a basis, we reduce A to row echelon form:r1, r2, r3 represents the rows.Once the matrix is in echelon form, the nonzero rows are a basis for the row space.  In this case, the basis is { (1, 3, 2), (0, 1, 0) }. Another possible basis { (1, 0, 2), (0, 1, 0) } comes from a further reduction.[8]This algorithm can be used in general to find a basis for the span of a set of vectors.  If the matrix is further simplified to reduced row echelon form, then the resulting basis is uniquely determined by the row space.It is sometimes convenient to find a basis for the row space from among the rows of the original matrix instead (for example, this result is useful in giving an elementary proof that the determinantal rank of a matrix is equal to its rank). Since row operations can affect linear dependence relations of the row vectors, such a basis is instead found indirectly using the fact that the column space of AT is equal to the row space of A. Using the example matrix A above, find AT and reduce it to row echelon form:The pivots indicate that the first two columns of AT form a basis of the column space of  AT. Therefore, the first two rows of A (before any row reductions) also form a basis of the row space of A.The dimension of the row space is called the rank of the matrix.  This is the same as the maximum number of linearly independent rows that can be chosen from the matrix, or equivalently the number of pivots.  For example, the 3 × 3 matrix in the example above has rank two.[8]The rank of a matrix is also equal to the dimension of the column space.  The dimension of the null space is called the nullity of the matrix, and is related to the rank by the following equation:where n is the number of columns of the matrix A.  The equation above is known as the rank–nullity theorem.The null space of matrix A is the set of all vectors x for which Ax = 0.  The product of the matrix A and the vector x can be written in terms of the dot product of vectors:where r1, ... , rm are the row vectors of A.  Thus Ax = 0 if and only if x is orthogonal (perpendicular) to each of the row vectors of A.It follows that the null space of A is the orthogonal complement to the row space.  For example, if the row space is a plane through the origin in three dimensions, then the null space will be the perpendicular line through the origin.  This provides a proof of the rank–nullity theorem (see dimension above).The row space and null space are two of the four fundamental subspaces associated with a matrix A (the other two being the column space and left null space).If V and W are vector spaces, then the kernel of a linear transformation T: V → W is the set of vectors v ∈ V for which T(v) = 0.  The kernel of a linear transformation is analogous to the null space of a matrix.If V is an inner product space, then the orthogonal complement to the kernel can be thought of as a generalization of the row space.  This is sometimes called the coimage of T.  The transformation T is one-to-one on its coimage, and the coimage maps isomorphically onto the image of T.When V is not an inner product space, the coimage of T can be defined as the quotient space V / ker(T).
Trace (linear algebra)
In linear algebra, the trace of an n-by-n square matrix A is defined to be the sum of the elements on the main diagonal (the diagonal from the upper left to the lower right) of A, i.e.,where aii denotes the entry on the ith row and ith column of A. The trace of a matrix is the sum of the (complex) eigenvalues, and it is invariant with respect to a change of basis. This characterization can be used to define the trace of a linear operator in general. Note that the trace is only defined for a square matrix (i.e., n × n).The trace (often abbreviated to tr) is related to the derivative of the determinant (see Jacobi's formula).Let A be a matrix, withThenThe trace is a linear mapping. That is,for all square matrices A and B, and all scalars c.A matrix and its transpose have the same trace:This follows immediately from the fact that transposing a square matrix does not affect elements along the main diagonal.The trace of a product can be rewritten as the sum of entry-wise products of elements:This means that the trace of a product of matrices functions similarly to a dot product of vectors.  For this reason, generalizations of vector operations to matrices (e.g. in matrix calculus and statistics) often involve a trace of matrix products.For real matrices, the trace of a product can also be written in the following forms:The matrices in a trace of a product can be switched without changing the result: If A is an m × n matrix and B is an n × m matrix, thenMore generally, the trace is invariant under cyclic permutations, i.e.,This is known as the cyclic property.Note that arbitrary permutations are not allowed: in general,However, if products of three symmetric matrices are considered, any permutation is allowed. (Proof: tr(ABC) = tr(AT BT CT) = tr(AT(CB)T) = tr((CB)TAT) = tr((ACB)T) = tr(ACB), where the last equality is because the traces of a matrix and its transpose are equal.) For more than three factors this is not true.Unlike the determinant, the trace of the product is not the product of traces, that is:What is true is that the trace of the Kronecker product of two matrices is the product of their traces:The following three properties:characterize the trace completely in the sense that follows. Let f be a linear functional on the space of square matrices satisfying f(x y) = f(y x). Then f and tr are proportional.[2]The trace is similarity-invariant, which means that A and P−1AP have the same trace. This is becauseIf A is symmetric and B is antisymmetric, thenThe trace of the identity matrix is the dimension of the space; this leads to generalizations of dimension using trace. The trace of an idempotent matrix A (for which A2 = A) is the rank of A. The trace of a nilpotent matrix is zero.More generally, if  f(x) = (x − λ1)d1···(x − λk)dk is the characteristic polynomial of a matrix A, thenConversely, any square matrix with zero trace is a linear combinations of the commutators of pairs of matrices.[3] Moreover, any square matrix with zero trace is unitarily equivalent to a square matrix with diagonal consisting of all zeros.The trace of a Hermitian matrix is real, because the elements on the diagonal are real.The trace of a projection matrix is the dimension of the target space.Expressions like tr(exp(A)), where A is a square matrix, occur so often in some fields (e.g. multivariate statistical theory), that a shorthand notation has become common:This is sometimes referred to as the exponential trace function; it is used in the Golden–Thompson inequality.Given some linear map f : V → V (where V is a finite-dimensional vector space) generally, we can define the trace of this map by considering the trace of matrix representation of f, that is, choosing a basis for V and describing f as a matrix relative to this basis, and taking the trace of this square matrix. The result will not depend on the basis chosen, since different bases will give rise to similar matrices, allowing for the possibility of a basis-independent definition for the trace of a linear map.Such a definition can be given using the canonical isomorphism between the space End(V) of linear maps on V and V ⊗ V∗, where V∗ is the dual space of V. Let v be in V and let f be in V∗. Then the trace of the indecomposable element v ⊗ f is defined to be f(v); the trace of a general element is defined by linearity. Using an explicit basis for V and the corresponding dual basis for V∗, one can show that this gives the same definition of the trace as given above.If A is a linear operator represented by a square n-by-n matrix with real or complex entries and if λ1, ..., λn are the eigenvalues of A (listed according to their algebraic multiplicities), thenThis follows from the fact that A is always similar to its Jordan form, an upper triangular matrix having λ1, ..., λn on the main diagonal. In contrast, the determinant of A is the product of its eigenvalues; i.e.,More generally,The trace corresponds to the derivative of the determinant: it is the Lie algebra analog of the (Lie group) map of the determinant. This is made precise in Jacobi's formula for the derivative of the determinant.For example, consider the one-parameter family of linear transformations given by rotation through angle θ,These transformations all have determinant 1, so they preserve area. The derivative of this family at θ = 0, the identity rotation, is the antisymmetric matrixwhich clearly has trace zero, indicating that this matrix represents an infinitesimal transformation which preserves area.A related characterization of the trace applies to linear vector fields. Given a matrix A, define a vector field F on ℝn by F(x) = Ax. The components of this vector field are linear functions (given by the rows of A). Its divergence div F is a constant function, whose value is equal to tr(A).By the divergence theorem, one can interpret this in terms of flows: if F(x) represents the velocity of a fluid at location x and U is a region in ℝn, the net flow of the fluid out of U is given by tr(A) ⋅ vol(U), where vol(U) is the volume of U.The trace is a linear operator, hence it commutes with the derivative:The trace of a 2-by-2 complex matrix is used to classify Möbius transformations. First the matrix is normalized to make its determinant equal to one. Then, if the square of the trace is 4, the corresponding transformation is parabolic. If the square is in the interval [0,4), it is elliptic. Finally, if the square is greater than 4, the transformation is loxodromic. See classification of Möbius transformations.The trace also plays a central role in the distribution of quadratic forms.The kernel of this map, a matrix whose trace is zero, is often said to be traceless or tracefree, and these matrices form the simple Lie algebra sln, which is the Lie algebra of the special linear group of matrices with determinant 1. The special linear group consists of the matrices which do not change volume, while the special linear Lie algebra is the matrices which infinitesimally do not change volume.In terms of short exact sequences, one haswhich is analogous toThe bilinear formis called the Killing form, which is used for the classification of Lie algebras.The trace defines a bilinear form:(x, y square matrices).The form is symmetric, non-degenerate[4] and associative in the sense that:Two matrices x  and y are said to be trace orthogonal ifFor an m-by-n matrix A with complex (or real) entries and * being the conjugate transpose, we havewith equality if and only if A = 0.  The assignmentyields an inner product on the space of all complex (or real) m-by-n matrices.The norm derived from the above inner product is called the Frobenius norm, which satisfies submultiplicative property as matrix norm. Indeed, it is simply the Euclidean norm if the matrix is considered as a vector of length m n.It follows that if A and B are real positive semi-definite matrices of the same size thenThe concept of trace of a matrix is generalized to the trace class of compact operators on Hilbert spaces, and the analog of the Frobenius norm is called the Hilbert–Schmidt norm.and is finite and independent of the orthonormal basis.[6] If A is a general associative algebra over a field k, then a trace on A is often defined to be any map tr: A → k which vanishes on commutators: tr([a, b]) = 0 for all a, b in A. Such a trace is not uniquely defined; it can always at least be modified by multiplication by a nonzero scalar.A supertrace is the generalization of a trace to the setting of superalgebras.The operation of tensor contraction generalizes the trace to arbitrary tensors.
Affine space
In mathematics, an affine space is a geometric structure that generalizes some of the properties of Euclidean spaces in such a way that these are independent of the concepts of distance and measure of angles, keeping only the properties related to parallelism and ratio of lengths for parallel line segments.In an affine space, there is no distinguished point that serves as an origin. Hence, no vector has a fixed origin and no vector can be uniquely associated to a point. In an affine space, there are instead displacement vectors, also called translation vectors or simply translations, between two points of the space.[1] Thus it makes sense to subtract two points of the space, giving a translation vector, but it does not make sense to add two points of the space. Likewise, it makes sense to add a displacement vector to a point of an affine space, resulting in a new point translated from the starting point by that vector.Any vector space may be considered as an affine space, and this amounts to forgetting the special role played by the zero vector. In this case, the elements of the vector space may be viewed either as points of the affine space or as displacement vectors or translations. When considered as a point, the zero vector is called the origin. Adding a fixed vector to the elements of a linear subspace of a vector space produces an affine subspace. One commonly says that this affine subspace has been obtained by translating (away from the origin) the linear subspace by the translation vector. In finite dimensions, such an affine subspace is the solution set of an inhomogeneous linear system. The displacement vectors for that affine space are the solutions of the corresponding homogeneous linear system, which is a linear subspace. Linear subspaces, in contrast, always contain the origin of the vector space.The dimension of an affine space is defined as the dimension of the vector space of its translations. An affine space of dimension one is an affine line. An affine space of dimension 2 is an affine plane. An affine subspace of dimension n – 1 in an affine space or a vector space of dimension n is an affine hyperplane.The following characterization may be easier to understand than the usual formal definition: an affine space is what is left of a vector space after you've forgotten which point is the origin (or, in the words of the French mathematician Marcel Berger, "An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps"[2]). Imagine that Alice knows that a certain point is the actual origin, but Bob believes that another point — call it p — is the origin. Two vectors, a and b, are to be added. Bob draws an arrow from point p to point a and another arrow from point p to point b, and completes the parallelogram to find what Bob thinks is a + b, but Alice knows that he has actually computedSimilarly, Alice and Bob may evaluate any linear combination of a and b, or of any finite set of vectors, and will generally get different answers. However, if the sum of the coefficients in a linear combination is 1, then Alice and Bob will arrive at the same answer.If Alice travels tothen Bob can similarly travel toUnder this condition, for all coefficients λ + (1 − λ) = 1, Alice and Bob describe the same point with the same linear combination, despite using different origins.While only Alice knows the "linear structure", both Alice and Bob know the "affine structure"—i.e. the values of affine combinations, defined as linear combinations in which the sum of the coefficients is 1. A set with an affine structure is an affine space.that has the following properties.[4][5][6]The first two properties are simply defining properties of a (right) group action. The third property characterizes free and transitive actions, the onto character coming from transitivity, and then the injective character follows from the action being free. There is a fourth property that follows from 1, 2, 3 above:Property 3 is often used in the following equivalent form.Another way to express the definition is that an affine space is a principal homogeneous space for the action of the additive group of a vector space. Homogeneous spaces are by definition endowed with a transitive group action, and for a principal homogeneous space such a transitive action is by definition free.Existence follows from the transitivity of the action, and uniqueness follows because the action is free.This subtraction has the two following properties, called Weyl's axioms:[7]In Euclidean geometry, the second Weyl's axiom is commonly called the parallelogram rule.The affine subspaces of A are the subsets of A of the formThe linear subspace associated with an affine subspace is often called its direction, and two subspaces that share the same direction are said to be parallel.This implies the following generalization of  Playfair's axiom: Given a direction V, for any point a of A there is one and only one affine subspace of direction V, which passes through a, namely the subspace a + V.The term parallel is also used for two affine subspaces such that the direction of one is included in the direction of the other.such thatEvery vector space V may be considered as an affine space over itself. This means that every element of V may be considered either as a point or as a vector. This affine set is sometimes denoted (V, V) for emphasizing the double role of the elements of V. When considered as a point, the zero vector is commonly denoted o (or O, when upper-case letters are used for points) and called the origin.Euclidean spaces (including the one-dimensional line, two-dimensional plane, and three-dimensional space commonly studied in elementary geometry, as well as higher-dimensional analogues) are affine spaces.Indeed, in most modern definitions, a Euclidean space is defined to be an affine space, such that the associated vector space is a real inner product space of finite dimension, that is a vector space over the reals with a positive-definite quadratic form q(x). The inner product of two vectors x and y is the value of the symmetric bilinear formThe usual Euclidean distance between two points A and B is In older definition of Euclidean spaces through synthetic geometry, vectors are defined as equivalence classes of ordered pairs of points under equipollence (the pairs (A, B) and (C, D) are equipollent if the points A, B, D, C (in this order) form a parallelogram). It is straightforward to verify that the vectors form a vector space, the square of the Euclidean distance is a quadratic form on the space of vectors, and the two definitions of Euclidean spaces are equivalent.In Euclidean geometry, the common phrase "affine property" refers to a property that can be proved in affine spaces, that is, it can be proved without using the quadratic form and its associated inner product. In other words, an affine property is a property that does not involve lengths and angles. Typical examples are parallelism, and the definition of a tangent. A non-example is the definition of a normal.Equivalently, an affine property is a property that is invariant under affine transformations of the Euclidean space.Thus this sum is independent of the choice of the origin, and the resulting vector is denotedone writesFor any subset X of an affine space A, there is a smallest affine subspace that contains it, called the affine span of X. It is the intersection of all affine subspaces containing X, and its direction is the intersection of the directions of the affine subspaces that contain X.The affine span of X is the set of all (finite) affine combinations of points of X, and its direction is the linear span of the x − y for x and y in X. If one chooses a particular point x0, the direction of the affine span of X is also the linear span of the x – x0 for x in X.One says also that the affine span of X is generated by X and that X is a generating set of its affine span.A set X of points of an affine space is said affinely independent or, simply, independent, if the affine span of any strict subset of X is a strict subset of the affine span of X. An affine basis, or barycentric frame (see § Barycentric coordinates, below) of an affine space is a generating set that is also independent (that is a minimal generating set).Recall the dimension of an affine space is the dimension of its associated vector space. The bases of an affine space of finite dimension n are the independent subsets of n + 1 elements, or, equivalently, the generating subsets of n + 1 elements. Equivalently, {x0, …, xn} is an affine basis of an affine space if and only if {x1 − x0, …, xn − x0} is a linear basis of the associated vector space.There are two strongly related kinds of coordinate systems that may be defined on affine spaces.andFor affine spaces of infinite dimension, the same definition applies, using only finite sums. This means that for each point, only a finite number of coordinates are non-zero.or equivalentlyExample: In Euclidean geometry, Cartesian coordinates are affine coordinates relative to an orthonormal frame, that is an affine frame (o, v1, …, vn) such that (v1, …, vn) is an orthonormal basis.Barycentric coordinates and affine coordinates are strongly related, and may be considered as equivalent.In fact, given a barycentric frameone deduces immediately the affine frameand, ifare the barycentric coordinates of a point over the barycentric frame, then the affine coordinates of the same point over the affine frame areConversely, ifis an affine frame, thenis a barycentric frame. Ifare the affine coordinates of a point over the affine frame, then its barycentric coordinates over the barycentric frame areTherefore, barycentric and affine coordinates are almost equivalent. In most applications, affine coordinates are preferred, as involving less coordinates that are independent. However, in the situations where the important points of the studied problem are affinity independent, barycentric coordinates may lead to simpler computation, as in the following example.The vertices of a non-flat triangle form an affine basis of the Euclidean plane. The barycentric coordinates allows easy characterization of the elements of the triangle that do not involve angles or distance:The vertices are the points of barycentric coordinates (1, 0, 0),  (0, 1, 0) and  (0, 0, 1). The lines supporting the edges are the points that have a zero coordinate. The edges themselves are the points that have a zero coordinate and two nonnegative coordinates. The interior of the triangle are the points whose all coordinates are positive. The medians are the points that have two equal coordinates, and the centroid is the point of coordinates (1/3, 1/3, 1/3).Letbe an affine homomorphism, withas associated linear map.An important example is the projection parallel to some direction onto an affine subspace. The importance of this example lies in the fact that Euclidean spaces are affine spaces, and that this kind of projections is fundamental in Euclidean geometry.for x and y in E.The image of this projection is  F, and its fibers are the subspaces of direction  D.Although kernels are not defined for affine spaces, quotient spaces are defined. This results from the fact that "belonging to the same fiber of an affine homomorphism" is an equivalence relation.Affine space is usually studied as analytic geometry using coordinates, or equivalently vector spaces. It can also be studied as synthetic geometry by writing down axioms, though this approach is much less common. There are several different systems of axioms for affine space.Coxeter (1969, p. 192) axiomatizes affine geometry (over the reals) as ordered geometry together with an affine form of Desargues's theorem and an axiom stating that in a plane there is at most one line through a given point not meeting a given line.Affine planes satisfy the following axioms (Cameron 1991, chapter 2):(in which two lines are called parallel if they are equal ordisjoint):As well as affine planes over fields (or division rings), there are also many non-Desarguesian planes satisfying these axioms. (Cameron 1991, chapter 3) gives axioms for higher-dimensional affine spaces.Affine spaces are subspaces of projective spaces: an affine plane can be obtained from any projective plane by removing a line and all the points on it, and conversely any affine plane can be used to construct a projective plane as a closure by adding a line at infinity whose points correspond to equivalence classes of parallel lines.Further, transformations of projective space that preserve affine space (equivalently, that leave the hyperplane at infinity invariant as a set) yield transformations of affine space. Conversely, any affine linear transformation extends uniquely to a projective linear transformation, so the affine group is a subgroup of the projective group. For instance, Möbius transformations (transformations of the complex projective line, or Riemann sphere) are affine (transformations of the complex plane) if and only if they fix the point at infinity.In algebraic geometry, an affine variety (or, more generally, an affine algebraic set) is defined as the subset of an affine space that is the set of the common zeros of a set of so-called polynomial functions over the affine space. For defining a polynomial function over the affine space, one has to choose an affine coordinate system. Then, a polynomial function is a function such that the image of any point is the value of some multivariate polynomial function of the coordinates of the point. As a change of affine coordinates may be expressed by linear functions (more precisely affine functions) of the coordinates, this definition is independent of a particular choice of coordinates.As the whole affine space is the set of the common zeros of the zero polynomial, affine spaces are affine algebraic varieties.Affine spaces over topological fields, such as the real or the complex numbers, have a natural topology. The Zariski topology, which is defined for affine spaces over any field, allows use of topological methods in any case. Zariski topology is the unique topology on an affine space whose closed sets are affine algebraic sets (that is sets of the common zeros of polynomials functions over the affine set). As, over a topological field, polynomial functions are continuous, every Zariski closed set is closed for the usual topology, if any. In other words, over a topological field, Zariski topology is coarser than the natural topology.The case of an algebraically closed ground field is especially important in algebraic geometry, because, in this case, the homeomorphism above is a map between the affine space and the set of all maximal ideals of the ring of functions (this is Hilbert's Nullstellensatz).This is the starting idea of scheme theory of Grothendieck, which consists, for studying algebraic varieties, of considering as "points", not only the points of the affine space, but also all the prime ideals of the spectrum. This allows gluing together algebraic varieties in a similar way as, for manifolds, charts are glued together for building a manifold.
Real number
In mathematics, a real number is a value of a continuous quantity that can represent a distance along a line. The adjective real in this context was introduced in the 17th century by René Descartes, who distinguished between real and imaginary roots of polynomials. The real numbers include all the rational numbers, such as the integer −5 and the fraction 4/3, and all the irrational numbers, such as √2 (1.41421356..., the square root of 2, an irrational algebraic number).  Included within the irrationals are the transcendental numbers, such as π (3.14159265...).   In addition to measuring distance, real numbers can be used to measure quantities such as time, mass, energy, velocity, and many more.Real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. Any real number can be determined by a possibly infinite decimal representation, such as that of 8.632, where each consecutive digit is measured in units one tenth the size of the previous one. The real line can be thought of as a part of the complex plane, and complex numbers include real numbers.These descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. The discovery of a suitably rigorous definition of the real numbers – indeed, the realization that a better definition was needed – was one of the most important developments of 19th-century mathematics. The current standard axiomatic definition is that real numbers form the unique Dedekind-complete ordered field (R ; + ; · ; <), up to an isomorphism,[a] whereas popular constructive definitions of real numbers include declaring them as equivalence classes of Cauchy sequences of rational numbers, Dedekind cuts, or infinite decimal representations, together with precise interpretations for the arithmetic operations and the order relation. All these definitions satisfy the axiomatic definition and are thus equivalent.Simple fractions were used by the Egyptians around 1000 BC; the Vedic "Sulba Sutras" ("The rules of chords") in, c. 600 BC, include what may be the first "use" of irrational numbers. The concept of irrationality was implicitly accepted by early Indian mathematicians since Manava (c. 750–690 BC), who were aware that the square roots of certain numbers such as 2 and 61 could not be exactly determined.[1] Around 500 BC, the Greek mathematicians led by Pythagoras realized the need for irrational numbers, in particular the irrationality of the square root of 2.The Middle Ages brought the acceptance of zero, negative, integral, and fractional numbers, first by Indian and Chinese mathematicians, and then by Arabic mathematicians, who were also the first to treat irrational numbers as algebraic objects,[2] which was made possible by the development of algebra. Arabic mathematicians merged the concepts of "number" and "magnitude" into a more general idea of real numbers.[3] The Egyptian mathematician Abū Kāmil Shujā ibn Aslam (c. 850–930) was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots.[4]In the 16th century, Simon Stevin created the basis for modern decimal notation, and insisted that there is no difference between rational and irrational numbers in this regard.In the 17th century, Descartes introduced the term "real" to describe roots of a polynomial, distinguishing them from "imaginary" ones.In the 18th and 19th centuries, there was much work on irrational and transcendental numbers. Johann Heinrich Lambert (1761) gave the first flawed proof that π cannot be rational; Adrien-Marie Legendre (1794) completed the proof,[5] and showed that π is not the square root of a rational number.[6] Paolo Ruffini (1799) and Niels Henrik Abel (1842) both constructed proofs of the Abel–Ruffini theorem: that the general quintic or higher equations cannot be solved by a general formula involving only arithmetical operations and roots.Évariste Galois (1832) developed techniques for determining whether a given equation could be solved by radicals, which gave rise to the field of Galois theory. Joseph Liouville (1840) showed that neither e nor e2 can be a root of an integer quadratic equation, and then established the existence of transcendental numbers; Georg Cantor (1873) extended and greatly simplified this proof.[7] Charles Hermite (1873) first proved that e is transcendental, and Ferdinand von Lindemann (1882), showed that π is transcendental. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and has finally been made elementary by Adolf Hurwitz[8] and Paul Gordan.[9]The development of calculus in the 18th century used the entire set of real numbers without having defined them cleanly. The first rigorous definition was published by Georg Cantor in 1871. In 1874, he showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite. Contrary to widely held beliefs, his first method was not his famous diagonal argument, which he published in 1891. See Cantor's first uncountability proof.Let R denote the set of all real numbers. Then:The last property is what differentiates the reals from the rationals (and from other, more exotic ordered fields). For example, the set of rationals with square less than 2 has a rational upper bound (e.g., 1.5) but no rational least upper bound, because the square root of 2 is not rational.These properties imply Archimedean property (which is not implied by other definitions of completeness). That is, the set of integers is not upper-bounded in the reals. In fact, if this were false, then the integers would have a least upper bound N; then, N – 1 would not be an upper bound, and there would be an integer n such that n > N – 1, and thus n + 1 > N, which is a contradiction with the upper-bound property of N.The real numbers are uniquely specified by the above properties. More precisely, given any two Dedekind-complete ordered fields R1 and R2, there exists a unique field isomorphism from R1 to R2, allowing us to think of them as essentially the same mathematical object.For another axiomatization of ℝ, see Tarski's axiomatization of the reals.The real numbers can be constructed as a completion of the rational numbers in such a way that a sequence defined by a decimal or binary expansion like (3; 3.1; 3.14; 3.141; 3.1415; ...) converges to a unique real number, in this case π. For details and other constructions of real numbers, see construction of the real numbers.More formally, the real numbers have the two basic properties of being an ordered field, and having the least upper bound property. The first says that real numbers comprise a field, with addition and multiplication as well as division by non-zero numbers, which can be totally ordered on a number line in a way compatible with addition and multiplication. The second says that, if a non-empty set of real numbers has an upper bound, then it has a real least upper bound. The second condition distinguishes the real numbers from the rational numbers: for example, the set of rational numbers whose square is less than 2 is a set with an upper bound (e.g. 1.5) but no (rational) least upper bound: hence the rational numbers do not satisfy the least upper bound property.A main reason for using real numbers is that the reals contain all limits. More precisely, a sequence of real numbers has a limit, which is a real number, if (and only if) its elements eventually come and remain arbitrarily close to each other.This is formally defined in the following, and means that the reals are complete (in the sense of metric spaces or uniform spaces, which is a different sense than the Dedekind completeness of the order in the previous section). :A sequence (xn) of real numbers is called a Cauchy sequence if for any ε > 0 there exists an integer N (possibly depending on ε) such that the distance |xn − xm| is less than ε for all n and m that are both greater than N. This definition, originally provided by Cauchy, formalizes the fact that the xn eventually come and remain arbitrarily close to each other.A sequence (xn) converges to the limit x if its elements eventually come and remain arbitrarily close to x, that is, if for any ε > 0 there exists an integer N (possibly depending on ε) such that the distance |xn − x| is less than ε for n greater than N.Every convergent sequence is a Cauchy sequence, and the converse is true for real numbers, and this means that the topological space of the real numbers is complete.The set of rational numbers is not complete. For example, the sequence (1; 1.4; 1.41; 1.414; 1.4142; 1.41421; ...), where each term adds a digit of the decimal expansion of the positive square root of 2, is Cauchy but it does not converge to a rational number (in the real numbers, in contrast, it converges to the positive square root of 2).The completeness property of the reals is the basis on which calculus, and, more generally mathematical analysis are built. In particular, the test that a sequence is a Cauchy sequence allows proving that a sequence has a limit, without computing it, and even without knowing it.For example, the standard series of the exponential functionconverges to a real number for every x, because the sumsThe real numbers are often described as "the complete ordered field", a phrase that can be interpreted in several ways.First, an order can be lattice-complete. It is easy to see that no ordered field can be lattice-complete, because it can have no largest element (given any element z, z + 1 is larger), so this is not the sense that is meant.Additionally, an order can be Dedekind-complete, as defined in the section Axioms. The uniqueness result at the end of that section justifies using the word "the" in the phrase "complete ordered field" when this is the sense of "complete" that is meant. This sense of completeness is most closely related to the construction of the reals from Dedekind cuts, since that construction starts from an ordered field (the rationals) and then forms the Dedekind-completion of it in a standard way.These two notions of completeness ignore the field structure. However, an ordered group (in this case, the additive group of the field) defines a uniform structure, and uniform structures have a notion of completeness (topology); the description in the previous section Completeness is a special case. (We refer to the notion of completeness in uniform spaces rather than the related and better known notion for metric spaces, since the definition of metric space relies on already having a characterization of the real numbers.) It is not true that R is the only uniformly complete ordered field, but it is the only uniformly complete Archimedean field, and indeed one often hears the phrase "complete Archimedean field" instead of "complete ordered field". Every uniformly complete Archimedean field must also be Dedekind-complete (and vice versa), justifying using "the" in the phrase "the complete Archimedean field". This sense of completeness is most closely related to the construction of the reals from Cauchy sequences (the construction carried out in full in this article), since it starts with an Archimedean field (the rationals) and forms the uniform completion of it in a standard way.But the original use of the phrase "complete Archimedean field" was by David Hilbert, who meant still something else by it. He meant that the real numbers form the largest Archimedean field in the sense that every other Archimedean field is a subfield of R. Thus R is "complete" in the sense that nothing further can be added to it without making it no longer an Archimedean field. This sense of completeness is most closely related to the construction of the reals from surreal numbers, since that construction starts with a proper class that contains every ordered field (the surreals) and then selects from it the largest Archimedean subfield.The reals are uncountable; that is: there are strictly more real numbers than natural numbers, even though both sets are infinite. In fact, the cardinality of the reals equals that of the set of subsets (i.e. the power set) of the natural numbers, and Cantor's diagonal argument states that the latter set's cardinality is strictly greater than the cardinality of N. Since the set of algebraic numbers is countable, almost all real numbers are transcendental. The non-existence of a subset of the reals with cardinality strictly between that of the integers and the reals is known as the continuum hypothesis. The continuum hypothesis can neither be proved nor be disproved; it is independent from the axioms of set theory.As a topological space, the real numbers are separable. This is because the set of rationals, which is countable, is dense in the real numbers. The irrational numbers are also dense in the real numbers, however they are uncountable and have the same cardinality as the reals.The real numbers form a metric space: the distance between x and y is defined as the absolute value |x − y|. By virtue of being a totally ordered set, they also carry an order topology; the topology arising from the metric and the one arising from the order are identical, but yield different presentations for the topology – in the order topology as ordered intervals, in the metric topology as epsilon-balls. The Dedekind cuts construction uses the order topology presentation, while the Cauchy sequences construction uses the metric topology presentation. The reals are a contractible (hence connected and simply connected), separable and complete metric space of Hausdorff dimension 1. The real numbers are locally compact but not compact. There are various properties that uniquely specify them; for instance, all unbounded, connected, and separable order topologies are necessarily homeomorphic to the reals.Every nonnegative real number has a square root in R, although no negative number does. This shows that the order on R is determined by its algebraic structure. Also, every polynomial of odd degree admits at least one real root: these two properties make R the premier example of a real closed field. Proving this is the first half of one proof of the fundamental theorem of algebra.The reals carry a canonical measure, the Lebesgue measure, which is the Haar measure on their structure as a topological group normalized such that the unit interval [0;1] has measure 1. There exist sets of real numbers that are not Lebesgue measurable, e.g. Vitali sets.The supremum axiom of the reals refers to subsets of the reals and is therefore a second-order logical statement. It is not possible to characterize the reals with first-order logic alone: the Löwenheim–Skolem theorem implies that there exists a countable dense subset of the real numbers satisfying exactly the same sentences in first-order logic as the real numbers themselves. The set of hyperreal numbers satisfies the same first order sentences as R. Ordered fields that satisfy the same first-order sentences as R are called nonstandard models of R. This is what makes nonstandard analysis work; by proving a first-order statement in some nonstandard model (which may be easier than proving it in R), we know that the same statement must also be true of R.The field R of real numbers is an extension field of the field Q of rational numbers, and R can therefore be seen as a vector space over Q. Zermelo–Fraenkel set theory with the axiom of choice guarantees the existence of a basis of this vector space: there exists a set B of real numbers such that every real number can be written uniquely as a finite linear combination of elements of this set, using rational coefficients only, and such that no element of B is a rational linear combination of the others. However, this existence theorem is purely theoretical, as such a base has never been explicitly described.The well-ordering theorem implies that the real numbers can be well-ordered if the axiom of choice is assumed: there exists a total order on R with the property that every non-empty subset of R has a least element in this ordering. (The standard ordering ≤ of the real numbers is not a well-ordering since e.g. an open interval does not contain a least element in this ordering.) Again, the existence of such a well-ordering is purely theoretical, as it has not been explicitly described. If V=L is assumed in addition to the axioms of ZF, a well ordering of the real numbers can be shown to be explicitly definable by a formula.[10]A real number may be either computable or uncomputable; either algorithmically random or not; and either arithmetically random or not.The real numbers are most often formalized using the Zermelo–Fraenkel axiomatization of set theory, but some mathematicians study the real numbers with other logical foundations of mathematics. In particular, the real numbers are also studied in reverse mathematics and in constructive mathematics.[11]The hyperreal numbers as developed by Edwin Hewitt, Abraham Robinson and others extend the set of the real numbers by introducing infinitesimal and infinite numbers, allowing for building infinitesimal calculus in a way closer to the original intuitions of Leibniz, Euler, Cauchy and others.Edward Nelson's internal set theory enriches the Zermelo–Fraenkel set theory syntactically by introducing a unary predicate "standard". In this approach, infinitesimals are (non-"standard") elements of the set of the real numbers (rather than being elements of an extension thereof, as in Robinson's theory).In the physical sciences, most physical constants such as the universal gravitational constant, and physical variables, such as position, mass, speed, and electric charge, are modeled using real numbers. In fact, the fundamental physical theories such as classical mechanics, electromagnetism, quantum mechanics, general relativity and the standard model are described using mathematical structures, typically smooth manifolds or Hilbert spaces, that are based on the real numbers, although actual measurements of physical quantities are of finite accuracy and precision.Physicists have occasionally suggested that a more fundamental theory would replace the real numbers with quantities that do not form a continuum, but such proposals remain speculative.[12]With some exceptions, most calculators do not operate on real numbers. Instead, they work with finite-precision approximations called floating-point numbers. In fact, most scientific computation uses floating-point arithmetic. Real numbers satisfy the usual rules of arithmetic, but floating-point numbers do not.A real number is called computable if there exists an algorithm that yields its digits. Because there are only countably many algorithms,[14] but an uncountable number of reals, almost all real numbers fail to be computable. Moreover, the equality of two computable numbers is an undecidable problem. Some constructivists accept the existence of only those reals that are computable. The set of definable numbers is broader, but still only countable.In set theory, specifically descriptive set theory, the Baire space is used as a surrogate for the real numbers since the latter have some topological properties (connectedness) that are a technical inconvenience. Elements of Baire space are referred to as "reals".Mathematicians use the symbol R, or, alternatively, ℝ, the letter "R" in blackboard bold (encoded in Unicode as .mw-parser-output .monospaced{font-family:monospace,monospace}U+211D ℝ .mw-parser-output .smallcaps{font-variant:small-caps}DOUBLE-STRUCK CAPITAL R (HTML &#8477;)), to represent the set of all real numbers. As this set is naturally endowed with the structure of a field, the expression field of real numbers is frequently used when its algebraic properties are under consideration.The sets of positive real numbers and negative real numbers are often noted R+ and R−,[15] respectively; R+ and R− are also used.[16] The non-negative real numbers can be noted R≥0 but one often sees this set noted R+ ∪ {0}.[15] In French mathematics, the positive real numbers and negative real numbers commonly include zero, and these sets are noted respectively ℝ+ and ℝ−.[16] In this understanding, the respective sets without zero are called strictly positive real numbers and strictly negative real numbers, and are noted ℝ+* and ℝ−*.[16]The notation Rn refers to the cartesian product of n copies of R, which is an n-dimensional vector space over the field of the real numbers; this vector space may be identified to the n-dimensional space of Euclidean geometry as soon as a coordinate system has been chosen in the latter. For example, a value from R3 consists of three real numbers and specifies the coordinates of a point in 3‑dimensional space.In mathematics, real is used as an adjective, meaning that the underlying field is the field of the real numbers (or the real field). For example, real matrix, real polynomial and real Lie algebra. The word is also used as a noun, meaning a real number (as in "the set of all reals").The real numbers can be generalized and extended in several different directions:
System of linear equations
In mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1]  For example,is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given bysince it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.The simplest kind of linear system involves two equations and two variables:Now substitute this expression for x into the bottom equation:A general system of m linear equations with n unknowns can be written asOften the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.The vector equation is equivalent to a matrix equation of the formwhere A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The number of vectors in a basis for the span is now expressed as the rank of the matrix.A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.A linear system may behave in any one of three possible ways:For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.The following pictures illustrate this trichotomy in the case of two variables:The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.For example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For a more complicated example, the equationsare not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.There are several algorithms for solving a system of linear equations.To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.For example, consider the following system:The solution set to this system can be described by the following equations:Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Here x is the free variable, and y and z are dependent.The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:For example, consider the following system:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsSolving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:The last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemis given byFor each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.)Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.There is also a quantum algorithm for linear systems of equations.[3]A system of linear equations is homogeneous if all of the constant terms are zero:A homogeneous system is equivalent to a matrix equation of the formwhere A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution.  If the system has a singular matrix then there is a solution set with an infinite number of solutions.  This solution set has the following additional properties:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A.Numerical solutions to a homogeneous system can be found with a singular value decomposition.There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asGeometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.
Horner's method
In mathematics, Horner's rule (also known as Horner scheme or Horner's method)[1][2] is a way of expressing and evaluating polynomials, which optimizes the needed number of arithmetic operations. More precisely Horner's rule consists of expressing the polynomialasThis allows evaluating a polynomial of degree n with only n – 1 multiplications and n – 1 additions. This is optimal, since there are polynomials of degree n that cannot be evaluated with fewer arithmetic operations.[citation needed]Horner's method may also refer to the use of Horner's rule in the process of solving a polynomial equation with Newton's method.[3] These methods are named after the British mathematician William George Horner, although they were known before him by Paolo Ruffini[4] , six hundred years earlier, by the Chinese mathematician Qin Jiushao[5] and seven hundred years earlier, by the Persian mathematician Sharaf al-Dīn al-Ṭūsī.[6]Given the polynomialTo accomplish this, we define a new sequence of constants as follows:To see why this works, note that the polynomial can be written in the formWe use synthetic division as follows:The third row is the sum of the first two rows, divided by 2. Each entry in the second row is the product of 1 with the third-row entry to the left. The answer isHorner's method is a fast, code-efficient method for multiplication and division of binary numbers on a microcontroller with no hardware multiplier.  One of the binary numbers to be multiplied is represented as a trivial polynomial, where (using the above notation) ai = 1, and x = 2.  Then, x (or x to some power) is repeatedly factored out.  In this binary numeral system (base 2), x = 2, so powers of 2 are repeatedly factored out.For example, to find the product of two numbers (0.15625) and m:To find the product of two binary numbers d and m:At this stage in the algorithm, it is required that terms with zero-valued coefficients are dropped, so that only binary coefficients equal to one are counted, thus the problem of multiplication or division by zero is not an issue, despite this implication in the factored equation:The denominators all equal one (or the term is absent), so this reduces toor equivalently (as consistent with the "method" described above)In binary (base-2) math, multiplication by a power of 2 is merely a register shift operation.  Thus, multiplying by 2 is calculated in base-2 by an arithmetic shift.  The factor (2−1) is a right arithmetic shift, a (0) results in no operation (since 20 = 1 is the multiplicative identity element), and a (21) results in a left arithmetic shift.The multiplication product can now be quickly calculated using only arithmetic shift operations, addition and subtraction.The method is particularly fast on processors supporting a single-instruction shift-and-addition-accumulate.  Compared to a C floating-point library, Horner's method sacrifices some accuracy, however it is nominally 13 times faster (16 times faster when the "canonical signed digit" (CSD) form is used) and uses only 20% of the code space.[7]These two steps are repeated until all real zeros are found for the polynomial. If the approximated zeros are not precise enough, the obtained values can be used as initial guesses for Newton's method but using the full polynomial rather than the reduced polynomials.[8]Consider the polynomialwhich can be expanded towhich is shown in yellow. The zero for this polynomial is found at 2 again using Newton's method and is circled in yellow. Horner's method is now used to obtainwhich is shown in green and found to have a zero at −3. This polynomial is further reduced toThe following Octave code was used in the example above to implement Horner's method.The following Python code implements Horner's method.The following C code implements Horner's method.Here is a slightly optimized version using explicit fused Multiply–accumulate operation, often execute faster than the above when running on a computer built with a processor supporting FMA instruction:The following C# code implements Horner's method.Evaluation using the monomial form of a degree-n polynomial requires at most n additions and (n2 + n)/2 multiplications, if powers are calculated by repeated multiplication and each monomial is evaluated individually.  (This can be reduced to n additions and 2n − 1 multiplications by evaluating the powers of x iteratively.)  If numerical data are represented in terms of digits (or bits), then the naive algorithm also entails storing approximately 2n times the number of bits of x (the evaluated polynomial has approximate magnitude xn, and one must also store xn itself).  By contrast, Horner's method requires only n additions and n multiplications, and its storage requirements are only n times the number of bits of x. Alternatively, Horner's method can be computed with n fused multiply–adds.  Horner's method can also be extended to evaluate the first k derivatives of the polynomial with kn additions and multiplications.[10]Horner's method is optimal, in the sense that any algorithm to evaluate an arbitrary polynomial must use at least as many operations. Alexander Ostrowski proved in 1954 that the number of additions required is minimal.[11] Victor Pan proved in 1966 that the number of multiplications is minimal.[12] However, when x is a matrix, Horner's method is not optimal.[citation needed]This assumes that the polynomial is evaluated in monomial form and no preconditioning of the representation is allowed, which makes sense if the polynomial is evaluated only once. However, if preconditioning is allowed and the polynomial is to be evaluated many times, then faster algorithms are possible. They involve a transformation of the representation of the polynomial. In general, a degree-n polynomial can be evaluated using only ⌊n/2⌋+2 multiplications and n additions.[13]A disadvantage of Horner's rule is that all of the operations are sequentially dependent, so it is not possible to take advantage of instruction level parallelism on modern computers.  In most applications where the efficiency of polynomial evaluation matters, many low-order polynomials are evaluated simultaneously (for each pixel or polygon in computer graphics, or for each grid square in a numerical simulation), so it is not necessary to find parallelism within a single polynomial evaluation.If, however, one is evaluating a single polynomial of very high order, it may be useful to break it up as follows:More generally, the summation can be broken into k parts:where the inner summations may be evaluated using separate parallel instances of Horner's method.  This requires slightly more operations than the basic Horner's method, but allows k-way SIMD execution of most of them.proceed as follows[14]At completion, we haveHorner's paper entitled "A new method of solving numerical equations of all orders, by continuous approximation"[16] was read before the Royal Society of London, at its meeting on July 1, 1819, with Davies Gilbert, Vice-President and Treasurer, in the chair; this was the final meeting of the session before the Society adjorned for its Summer recess. When a sequel was read before the Society in 1823, it was again at the final meeting of the session. On both occasions, papers by James Ivory, FRS, were also read. In 1819, it was Horner's paper that got through to publication in the "Philosophical Transactions".[16] later in the year, Ivory's paper falling by the way, despite Ivory being a Fellow; in 1823, when a total of ten papers were read, fortunes as regards publication, were reversed. But Gilbert, who had strong connections with the West of England and may have had social contact with Horner, resident as Horner was in Bristol and Bath, published his own survey of Horner-type methods earlier in 1823.Horner's paper in Part II of Philosophical Transactions of the Royal Society of London for 1819 was warmly and expansively welcomed by a reviewer in the issue of The Monthly Review: or, Literary Journal for April, 1820; in comparison, a technical paper by Charles Babbage is dismissed curtly in this review. However, the reviewer noted that another, similar method had also recently been published by the architect and mathematical expositor, Peter Nicholson. This theme is developed in a further review of some of Nicholson's books in the issue of The Monthly Review for December, 1820, which in turn ends with notice of the appearance of a booklet by Theophilus Holdred, from whom Nicholson acknowledges he obtained the gist of his approach in the first place, although claiming to have improved upon it. The sequence of reviews is concluded in the issue of The Monthly Review for September, 1821, with the reviewer concluding that whereas Holdred was the first person to discover a direct and general practical solution of numerical equations, he had not reduced it to its simplest form by the time of Horner's publication, and saying that had Holdred published forty years earlier when he first discovered his method, his contribution could be more easily recognized. The reviewer is exceptionally well-informed, even having sighted Horner's preparatory correspondence with Peter Barlow in 1818, seeking work of Budan. The Bodlean Library, Oxford has the Editor's annotated copy of The Monthly Review from which it is clear that the most active reviewer in mathematics in 1814 and 1815 (the last years for which this information has been published) was none other than Peter Barlow,one of the foremost specialists on approximation theory of the period, suggesting that it was Barlow, who wrote this sequence of reviews. As it also happened, Henry Atkinson, of Newcastle, devised a similar approximation scheme in 1809; he had consulted his fellow Geordie, Charles Hutton, another specialist and a senior colleague of Barlow at the Royal Military Academy, Woolwich, only to be advised that, while his work was publishable, it was unlikely to have much impact. J. R. Young, writing in the mid-1830s, concluded that Holdred's first method replicated Atkinson's while his improved method was only added to Holdred's booklet some months after its first appearance in 1820, when Horner's paper was already in circulation.The feature of Horner's writing that most distinguishes it from his English contemporaries is the way he draws on the Continental literature, notably the work of Arbogast. The advocacy, as well as the detraction, of Horner's Method has this as an unspoken subtext. Quite how he gained that familiarity has not been determined. Horner is known to have made a close reading of John Bonneycastle's book on algebra. Bonneycastle recognizes that Arbogast has the general, combinatorial expression for the reversion of series, a project going back at least to Newton. But Bonneycastle's main purpose in mentioning Arbogast is not to praise him, but to observe that Arbogast's notation is incompatible with the approach he adopts. The gap in Horner's reading was the work of Paolo Ruffini, except that, as far as awareness of Ruffini goes, citations of Ruffini's work by authors, including medical authors, in Philosophical Transactions speak volumes: there are none - Ruffini's name only appears in 1814, recording a work he donated to the Royal Society. Ruffini might have done better if his work had appeared in French, as had Malfatti's Problem in the reformulation of Joseph Diaz Gergonne, or had he written in French, as had Antonio Cagnoli, a source quoted by Bonneycastle on series reversion (today, Cagnoli is in the Italian Wikipedia, as shown, but has yet to make it into either French or English).Fuller[17] showed that the method in Horner's 1819 paper differs from what afterwards became known as 'Horner's method' and that in consequence the priority for this method should go to Holdred (1920). This view may be compared with the remarks concerning the works of Horner and Holdred in the previous paragraph. Fuller also takes aim at Augustus De Morgan. Precocious though Augustus de Morgan was, he was not the reviewer for The Monthly Review, while several others - Thomas Stephens Davies, J. R. Young, Stephen Fenwick, T. T. Wilkinson - wrote Horner firmly into their records, not least Horner himself, as he published extensively up until the year of his death in 1837. His paper in 1819 was one that would have been difficult to miss. In contrast, the only other mathematical sighting of Holdred is a single named contribution to The Gentleman's Mathematical Companion, an answer to a problem.It is questionable to what extent it was De Morgan's advocacy of Horner's priority in discovery[4][18] that led to "Horner's method" being so called in textbooks, but it is true that those suggesting this tend themselves to know of Horner largely through intermediaries, of whom De Morgan made himself a prime example. However, this method qua method was known long before Horner. In reverse chronological order, Horner's method was already known to:However, this observation on its own masks significant differences in conception and also, as noted with Ruffini's work, issues of accessibility.Qin Jiushao, in his Shu Shu Jiu Zhang (Mathematical Treatise in Nine Sections; 1247), presents a portfolio of methods of Horner-type for solving polynomial equations, which was based on earlier works of the 11th century Song dynasty mathematician Jia Xian; for example, one method is specifically suited to bi-quintics, of which Qin gives an instance, in keeping with the then Chinese custom of case studies. The first person writing in English to note the connection with Horner's method was Alexander Wylie, writing in The North China Herald in 1852; perhaps conflating and misconstruing different Chinese phrases, Wylie calls the method Harmoniously Alternating Evolution (which does not agree with his Chinese, linglong kaifang, not that at that date he uses pinyin), working the case of one of Qin's quartics and giving, for comparison, the working with Horner's method. Yoshio Mikami in Development of Mathematics in China and Japan published in Leipzig in 1913, gave a detailed description of Qin's method, using the quartic illustrated to the above right in a worked example; he wrote: "who can deny the fact of Horner's illustrious process being used in China at least nearly six long centuries earlier than in Europe ... We of course don't intend in any way to ascribe Horner's invention to a Chinese origin, but the lapse of time sufficiently makes it not altogether impossible that the Europeans could have known of the Chinese method in a direct or indirect way.".[21] However, as Mikami is also aware, it was not altogether impossible that a related work, Si Yuan Yu Jian (Jade Mirror of the Four Unknowns; 1303) by Zhu Shijie might make the shorter journey across to Japan, but seemingly it never did, although another work of Zhu, Suan Xue Qi Meng, had a seminal influence on the development of traditional mathematics in the Edo period, starting in the mid-1600s. Ulrich Libbrecht (at the time teaching in school, but subsequently a professor of comparative philosophy) gave a detailed description in his doctoral thesis of Qin's method, he concluded: It is obvious that this procedure is a Chinese invention....the method was not known in India. He said, Fibonacci probably learned of it from Arabs, who perhaps borrowed from the Chinese.[22] Here, the problems is that there is no more evidence for this speculation than there is of the method being known in India. Of course, the extraction of square and cube roots along similar lines is already discussed by Liu Hui in connection with Problems IV.16 and 22 in Jiu Zhang Suan Shu, while Wang Xiaotong in the 7th century supposes his readers can solve cubics by an approximation method described in his book Jigu Suanjing.
Bra–ket notation
In quantum mechanics, bra–ket notation is a standard notation for describing quantum states. It can also be used to denote abstract vectors and linear functionals in mathematics. The notation uses angle brackets (the ⟨ and ⟩ symbols) and a vertical bar (the | symbol), to denote the scalar product of vectors or the action of a linear functional on a vector in a complex vector space.  The scalar product or action is written asThe right part is called the ket /kɛt/; it is a vector, typically represented as a column vector and written The left part is called the bra, /brɑː/; it is the Hermitian conjugate of the ket with the same label, typically represented as a row vector and is writtenA combination of bras, kets, and operators is interpreted using matrix multiplication. A bra and a ket with the same label are Hermitian conjugates of each other.Bra-ket notation was introduced in 1939 by Paul Dirac[1][2] and is also known as the Dirac notation.  Bra–ket notation is a notation for linear algebra, particularly focused on vectors, inner products, linear operators, Hermitian conjugation, and the dual space, for both finite-dimensional and infinite-dimensional complex vector spaces. It is specifically designed to ease the types of calculations that frequently come up in quantum mechanics.Its use in quantum mechanics is quite widespread. Many phenomena that are explained using quantum mechanics are usually explained using bra–ket notation.In simple cases, a ket |m⟩ can be described as a column vector, a bra with the same label ⟨m| is its conjugate transpose (which is a row vector), and writing bras, kets, and linear operators next to each other implies matrix multiplication.[4] However, kets may also exist in uncountably-infinite-dimensional vector spaces, such that they cannot be literally written as a column vector. Also, writing a column vector as a list of numbers requires picking a basis, whereas one can write "|m⟩" without committing to any particular basis. This is helpful because quantum mechanics calculations involve frequently switching between different bases (e.g. position basis, momentum basis, energy eigenbasis, etc.), so it is better to have the basis vectors (if any) written out explicitly.  In some situations involving two important basis vectors they will be referred to simply as "|-⟩" and "|+⟩".The standard mathematical notation for the inner product, preferred as well by some physicists, expresses exactly the same thing as the bra–ket notation,Bras and kets can also be configured in other ways, such as the outer productwhich can also be represented as a matrix multiplication (i.e., a column vector times a row vector equals a matrix).If the ket is an element of a vector space, the bra is technically an element of its dual space—see Riesz representation theorem.In mathematics, the term "vector" is used to refer generally to any element of any vector space. In physics, however, the term "vector" is much more specific: "Vector" refers almost exclusively to quantities like displacement or velocity, which have three components that relate directly to the three dimensions of the real world. Such vectors are typically denoted with over arrows (r→) or boldface (r).In quantum mechanics, a quantum state is typically represented as an element of an abstract complex vector space—for example the infinite-dimensional vector space of all possible wavefunctions (functions mapping each point of 3D space to a complex number). Since the term "vector" is already used for something else (see previous paragraph), it is very common to refer to these elements of abstract complex vector spaces as "kets", and to write them using ket notation.Ket notation, invented by Dirac, uses vertical bars and angular brackets: |A⟩. When this notation is used, these quantities are called "kets", and |A⟩ is read as "ket-A".[5] These kets can be manipulated using the usual rules of linear algebra, for example:Note how any symbols, letters, numbers, or even words—whatever serves as a convenient label—can be used as the label inside a ket. For example, the last line above involves infinitely many different kets, one for each real number x. In other words, the symbol "|A⟩" has a specific and universal mathematical meaning, while just the "A" by itself does not. For example, |1⟩ + |2⟩ might or might not be equal to |3⟩. Nevertheless, for convenience, there is usually some logical scheme behind the labels inside kets, such as the common practice of labeling energy eigenkets in quantum mechanics through a listing of their quantum numbers.An inner product is a generalization of the dot product. The inner product of two vectors is a scalar. In neutral notation (notation dedicated to the inner product only), this might be written (A, B), where A and B are elements of the abstract vector space, i.e. both are kets.Bra–ket notation uses a specific notation for inner products:Bra–ket notation splits this inner product (also called a "bracket") into two pieces, the "bra" and the "ket":where ⟨A| is called a bra, read as "bra-A", and |B⟩ is a ket as above.The purpose of "splitting" the inner product into a bra and a ket is that both the bra ⟨A| and the ket |B⟩ are meaningful on their own, and can be used in other contexts besides within an inner product. There are two main ways to think about the meanings of separate bras and kets. Accordingly, the interpretation of the expression ⟨A|B⟩ has a second interpretation, namely that of the action of a linear functional per below.For a finite-dimensional vector space, using a fixed orthonormal basis, the inner product can be written as a matrix multiplication of a row vector with a column vector:Based on this, the bras and kets can be defined as:and then it is understood that a bra next to a ket implies matrix multiplication.The conjugate transpose (also called Hermitian conjugate) of a bra is the corresponding ket and vice versa:because if one starts with the brathen performs a complex conjugation, and then a matrix transpose, one ends up with the ketA more abstract definition, which is equivalent but more easily generalized to infinite-dimensional spaces, is to say that bras are linear functionals on the space of kets, i.e. linear transformations that input a ket and output a complex number. The bra linear functionals are defined to be consistent with the inner product. Thus, if ⟨A| is the linear functional corresponding to |A⟩ under the Riesz representation theorem, theni.e. it produces the same complex number as the inner product does. The terminology for the right hand side is though not inner product, which always involves two kets. Confusing this is harmless, since the same number is produced in the end.In mathematics terminology, the vector space of bras is the dual space to the vector space of kets, and corresponding bras and kets are related by the Riesz representation theorem.Bra–ket notation can be used even if the vector space is not a Hilbert space.In quantum mechanics, it is common practice to write down kets which have infinite norm, i.e. non-normalizable wavefunctions. Examples include states whose wavefunctions are Dirac delta functions or infinite plane waves. These do not, technically, belong to the Hilbert space itself. However, the definition of "Hilbert space" can be broadened to accommodate these states (see the Gelfand–Naimark–Segal construction or rigged Hilbert spaces). The bra–ket notation continues to work in an analogous way in this broader context.Banach spaces are a different generalization of Hilbert spaces. In a Banach space B, the vectors may be notated by kets and the continuous linear functionals by bras. Over any vector space without topology, we may also notate the vectors by kets and the linear functionals by bras. In these more general contexts, the bracket does not have the meaning of an inner product, because the Riesz representation theorem does not apply.The mathematical structure of quantum mechanics is based in large part on linear algebra:Since virtually every calculation in quantum mechanics involves vectors and linear operators, it can involve, and often does involve, bra–ket notation. A few examples follow:Starting from any ket |Ψ⟩ in this Hilbert space,  one may define a complex scalar function of r, known as a wavefunction,On the left-hand side, Ψ(r) is a function mapping any point in space to a complex number; on the right-hand side, |Ψ⟩ = ∫ d3r Ψ(r) |r⟩ is a ket consisting of a superposition of kets with relative coefficients specified by that function.It is then customary to define linear operators acting on wavefunctions in terms of linear operators acting on kets, byFor instance, the momentum operator p has the following form,One occasionally encounters an expression such asthough this is something of an abuse of notation. The differential operator must be understood to be an abstract operator, acting on kets, that has the effect of differentiating wavefunctions once the expression is projected into the position basis,even though, in the momentum basis, the operator amounts to a mere multiplication operator (by iħp).In quantum mechanics the expression ⟨φ|ψ⟩ is typically interpreted as the probability amplitude for the state ψ to collapse into the state φ. Mathematically, this means the coefficient for the projection of ψ onto φ.  It is also described as the projection of state ψ onto state φ.A stationary spin-1/2 particle has a two-dimensional Hilbert space. One orthonormal basis is:where |↑z⟩ is the state with a definite value of the spin operator Sz equal to +1/2 and  |↓z⟩ is the state with a definite value of the spin operator Sz equal to −1/2.Since these are a basis, any quantum state of the particle can be expressed as a linear combination (i.e., quantum superposition) of these two states:where aψ and bψ are complex numbers.A different basis for the same Hilbert space is:defined in terms of Sx rather than Sz.Again, any state of the particle can be expressed as a linear combination of these two:In vector form, you might writedepending on which basis you are using. In other words, the "coordinates" of a vector depend on the basis used.There is a mathematical relationship between aψ, bψ, cψ and dψ; see change of basis.There are a few conventions and abuses of notation that are generally accepted by the physics community, but which might confuse the non-initiated.It is common to use the same symbol for labels and constants in the same equation. For example, α̂ |α⟩ = α |α⟩, where the symbol α is used simultaneously as the name of the operator α̂, its eigenvector |α⟩ and the associated eigenvalue α.Something similar occurs in component notation of vectors. While Ψ (uppercase) is traditionally associated with wavefunctions, ψ (lowercase) may be used to denote a label, a wave function or complex constant in the same context, usually differentiated only by a subscript.The main abuses are including operations inside the vector labels. This is done for a fast notation of scaling vectors. E.g. if the vector |α⟩ is scaled by √2, it might be denoted by |α/√2⟩, which makes no sense since α is a label, not a function or a number, so you can't perform operations on it.This is especially common when denoting vectors as tensor products, where part of the labels are moved outside the designed slot, e.g. |α⟩ = |α/√21⟩ ⊗ |α/√22⟩. Here part of the labeling that should state that all three vectors are different was moved outside the kets, as subscripts 1 and 2. And a further abuse occurs, since α is meant to refer to the norm of the first vector—which is a label denoting a value.A linear operator is a map that inputs a ket and outputs a ket. (In order to be called "linear", it is required to have certain properties.) In other words, if A is a linear operator and |ψ⟩ is a ket, then A|ψ⟩ is another ket.In an N-dimensional Hilbert space, |ψ⟩ can be written as an N × 1 column vector, and then A is an N × N matrix with complex entries. The ket A|ψ⟩ can be computed by normal matrix multiplication.Linear operators are ubiquitous in the theory of quantum mechanics. For example, observable physical quantities are represented by self-adjoint operators, such as energy or momentum, whereas transformative processes are represented by unitary linear operators such as rotation or the progression of time.Operators can also be viewed as acting on bras from the right hand side. Specifically, if A is a linear operator and ⟨φ| is a bra, then ⟨φ|A is another bra defined by the rule(in other words, a function composition). This expression is commonly written as (cf. energy inner product)In an N-dimensional Hilbert space, ⟨φ| can be written as a 1 × N row vector, and A (as in the previous section) is an N × N matrix. Then the bra ⟨φ|A can be computed by normal matrix multiplication.If the same state vector appears on both bra and ket side,then this expression gives the expectation value, or mean or average value, of the observable represented by operator A for the physical system in the state |ψ⟩.A convenient way to define linear operators on a Hilbert space H is given by the outer product: if ⟨ϕ| is a bra and |ψ⟩ is a ket, the outer productdenotes the rank-one operator with the rule For a finite-dimensional vector space, the outer product can be understood as simple matrix multiplication:The outer product is an N × N matrix, as expected for a linear operator.One of the uses of the outer product is to construct projection operators. Given a ket |ψ⟩ of norm 1, the orthogonal projection onto the subspace spanned by |ψ⟩ isJust as kets and bras can be transformed into each other (making |ψ⟩ into ⟨ψ|), the element from the dual space corresponding to A|ψ⟩ is ⟨ψ|A†,  where A† denotes the Hermitian conjugate (or adjoint) of the operator A. In other words,If A is expressed as an N × N matrix, then A† is its conjugate transpose.Self-adjoint operators, where A = A†, play an important role in quantum mechanics; for example, an observable is always described by a self-adjoint operator. If A is a self-adjoint operator, then ⟨ψ|A|ψ⟩ is always a real number (not complex). This implies that expectation values of observables are real.Bra–ket notation was designed to facilitate the formal manipulation of linear-algebraic expressions. Some of the properties that allow this manipulation are listed herein. In what follows, c1 and c2 denote arbitrary complex numbers, c* denotes the complex conjugate of c, A and B denote arbitrary linear operators, and these properties are to hold for any choice of bras and kets.Given any expression involving complex numbers, bras, kets, inner products, outer products, and/or linear operators (but not addition), written in bra–ket notation, the parenthetical groupings do not matter (i.e., the associative property holds). For example:and so forth. The expressions on the right (with no parentheses whatsoever) are allowed to be written unambiguously because of the equalities on the left. Note that the associative property does not hold for expressions that include nonlinear operators, such as the antilinear time reversal operator in physics.Bra–ket notation makes it particularly easy to compute the Hermitian conjugate (also called dagger, and denoted †) of expressions. The formal rules are:These rules are sufficient to formally write the Hermitian conjugate of any such expression; some examples are as follows:Two Hilbert spaces V and W may form a third space V ⊗ W by a tensor product. In quantum mechanics, this is used for describing composite systems. If a system is composed of two subsystems described in V and W respectively, then the Hilbert space of the entire system is the tensor product of the two spaces. (The exception to this is if the subsystems are actually identical particles. In that case, the situation is a little more complicated.)If |ψ⟩ is a ket in V and |φ⟩ is a ket in W, the direct product of the two kets is a ket in V ⊗ W. This is written in various notations:See quantum entanglement and the EPR paradox for applications of this product.Consider a complete orthonormal system (basis),for a Hilbert space H, with respect to the norm from an inner product ⟨·,·⟩. From basic functional analysis, it is  known that any ket |ψ⟩ can also be written aswith ⟨·|·⟩ the inner product on the Hilbert space.From the commutativity of kets with (complex) scalars, it follows thatmust be the identity operator, which sends each vector to itself. This, then,  can be inserted in any expression without affecting its value; for examplewhere, in the last identity, the Einstein summation convention has been used.In quantum mechanics, it often occurs that little or no information about the inner product ⟨ψ|φ⟩ of two arbitrary (state) kets is present, while it is still possible to say something about the expansion coefficients ⟨ψ|ei⟩ = ⟨ei|ψ⟩* and ⟨ei|φ⟩ of those vectors with respect to a specific (orthonormalized) basis. In this case, it is particularly useful to insert the unit operator into the bracket one time or more.For more information, see Resolution of the identity, Since ⟨x′|x⟩ = δ(x − x′), plane waves follow, ⟨x|p⟩ = eixp/ħ/√2πħ.[7]Typically, when all matrix elements of an operator such as are available,this resolution serves to reconstitute the full operator,The object physicists are considering when using bra–ket notation is a Hilbert space (a complete inner product space).Let H be a Hilbert space and h ∈ H a vector in H. What physicists would denote by |h⟩ is the vector itself. That is,Let H* be the dual space of H. This is the space of linear functionals on H. The isomorphism Φ : H → H* is defined by Φ(h) = φh, where for every g ∈ H we definewhere IP(·,·), (·,·), ⟨·,·⟩ and ⟨·|·⟩ are just different notations for expressing an inner product between two elements in a Hilbert space (or for the first three, in any inner product space). Notational confusion arises when identifying φh and g with ⟨h| and |g⟩ respectively. This is because of literal symbolic substitutions. Let φh = H = ⟨h| and let g = G = |g⟩. This givesOne ignores the parentheses and removes the double bars. Some properties of this notation are convenient since we are dealing with linear operators and composition acts like a ring multiplication.Moreover, mathematicians usually write the dual entity not at the first place, as the physicists do, but at the second one, and they usually use not an asterisk but an overline (which the physicists reserve for averages and the Dirac spinor adjoint) to denote complex conjugate numbers; i.e., for scalar products mathematicians usually writewhereas physicists would write for the same quantity
Eigenvalues and eigenvectors
In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v.  This condition can be written as the equationwhere λ is a scalar in the field F, known as the eigenvalue,  characteristic value, or characteristic root associated with the eigenvector v.If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equationThere is a direct correspondence between n-by-n square matrices and linear transformations from an n-dimensional vector space to itself, given any basis of the vector space. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]Geometrically, an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched.  If the eigenvalue is negative, the direction is reversed.[3]Eigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value λ, called an eigenvalue. This condition can be written as the equationreferred to as the eigenvalue equation or eigenequation. In general, λ may be any scalar. For example, λ may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.The Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplicationwhere the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caractéristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Théorie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm–Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.[16]At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]Eigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]Consider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectorsThese vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar λ such thatIn this case λ = −1/20.Now consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,orwhere, for each row,If it occurs that v and w are scalar multiples, that is if    (1)then v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.Equation (1) can be stated equivalently as    (2)where I is the n by n identity matrix.Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A − λI) is zero. Therefore, the eigenvalues of A are values of λ that satisfy the equation    (3)Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable λ and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (−1)nλn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A.The fundamental theorem of algebra implies that the characteristic polynomial of an n-by-n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,    (4)where each λi may be real but in general is a complex number. The numbers λ1, λ2, … λn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.As a brief example, which is described in more detail in the examples section later, consider the matrixTaking the determinant of (M − λI), the characteristic polynomial of M isSetting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = λv. In this example, the eigenvectors are any non-zero scalar multiples ofIf the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.Let λi be an eigenvalue of an n by n matrix A. The algebraic multiplicity μA(λi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (λ − λi)k divides evenly that polynomial.[8][26][27]Suppose a matrix A has dimension n and d ≤ n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n asIf μA(λi) = 1, then λi is said to be a simple eigenvalue.[27] If μA(λi) equals the geometric multiplicity of λi, γA(λi), defined in the next section, then λi is said to be a semisimple eigenvalue.Given a particular eigenvalue λ of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),On one hand, this set is precisely the kernel or nullspace of the matrix (A − λI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with λ. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with λ, and E equals the nullspace of (A − λI). E is called the eigenspace or characteristic space of A associated with λ.[7][8] In general λ is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of ℂn.Because the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) ∈ E, then (u + v) ∈ E or equivalently A(u + v) = λ(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v ∈ E and α is a complex number,  (αv) ∈ E or equivalently A(αv) = λ(αv). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and αv are not zero, they are also eigenvectors of A associated with λ.The dimension of the eigenspace E associated with λ, or equivalently the maximum number of linearly independent eigenvectors associated with λ, is referred to as the eigenvalue's geometric multiplicity γA(λ). Because E is also the nullspace of (A − λI), the geometric multiplicity of λ is the dimension of the nullspace of (A − λI), also called the nullity of (A − λI), which relates to the dimension and rank of (A − λI) asBecause of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.Suppose A has d ≤ n distinct eigenvalues λ1, λ2, …, λd, where the geometric multiplicity of λi is γA(λi). The total geometric multiplicity of A,is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If γA = n, thenLet A be an arbitrary n by n matrix of complex numbers with eigenvalues λ1, λ2, ..., λn. Each eigenvalue appears μA(λi) times in this list, where μA(λi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:Many disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation iswhere κ is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and κ is its associated eigenvalue. Taking the transpose of this equation,Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, …, vn with associated eigenvalues λ1, λ2, …, λn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,Since each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,With this in mind, define a diagonal matrix Λ where each diagonal element Λii is the eigenvalue associated with the ith column of Q. ThenBecause the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q−1,or by instead left multiplying both sides by Q−1,A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix Λ or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as Λ.Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.A matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.Consider the matrixThe figure on the right shows the effect of this transformation on point coordinates in the plane.The eigenvectors v of this transformation satisfy Equation (1), and the values of λ for which the determinant of the matrix (A − λI) equals zero are the eigenvalues.Taking the determinant to find characteristic polynomial of A,Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of A.For λ = 1, Equation (2) becomes,Any non-zero vector with v1 = −v2 solves this equation. Therefore,is an eigenvector of A corresponding to λ = 1, as is any scalar multiple of this vector.For λ = 3, Equation (2) becomesAny non-zero vector with v1 = v2 solves this equation. Therefore,is an eigenvector of A corresponding to λ = 3, as is any scalar multiple of this vector.Thus, the vectors vλ=1 and vλ=3 are eigenvectors of A associated with the eigenvalues λ = 1 and λ = 3, respectively.Consider the matrixThe characteristic polynomial of A isConsider the cyclic permutation matrixThis matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1 − λ3, whose roots areFor the real eigenvalue λ1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,For the complex conjugate pair of imaginary eigenvalues, note thatThenandMatrices with entries only along the main diagonal are called diagonal matrices.  The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrixThe characteristic polynomial of A iswhich has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,respectively, as well as scalar multiples of these vectors.A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix.  As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.Consider the lower triangular matrix,The characteristic polynomial of A iswhich has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.These eigenvalues correspond to the eigenvectors,respectively, as well as scalar multiples of these vectors.As in the previous example, the lower triangular matrixhas a characteristic polynomial that is the product of its diagonal elements,The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is μA = 4 = n, the order of the characteristic polynomial and the dimension of A.The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C∞ of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equationThe functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential functionis the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for λ = 0 the eigenfunction f(t) is a constant.The main eigenfunction article gives other examples.The concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,We say that a non-zero vector v ∈ V is an eigenvector of T if and only if there exists a scalar λ ∈ K such that    ( 5)This equation is called the eigenvalue equation for T, and the scalar λ is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while λv is the product of the scalar λ with v.[33]Given an eigenvalue λ, consider the setwhich is the union of the zero vector with the set of all eigenvectors associated with λ. E is called the eigenspace or characteristic space of T associated with λ.By definition of a linear transformation,for (x,y) ∈ V and α ∈ K. Therefore, if u and v are eigenvectors of T associated with eigenvalue λ, namely (u,v) ∈ E, thenSo, both u + v and αv are either zero or eigenvectors of T associated with λ, namely (u + v, αv) ∈ E, and E is closed under addition and scalar multiplication. The eigenspace E associated with λ is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]The geometric multiplicity γT(λ) of an eigenvalue λ is the dimension of the eigenspace associated with λ, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, γT(λ) ≥ 1 because every eigenvalue has at least one eigenvector.The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar λ ∈ K such that there exists a non-zero vector v ∈ V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue λ to be any vector that, given λ, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.If λ is an eigenvalue of T, then the operator (T − λI) is not one-to-one, and therefore its inverse (T − λI)−1 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T − λI) may not have an inverse even if λ is not an eigenvalue.For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars λ for which the operator (T − λI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation – an associative algebra acting on a module. The study of such actions is the field of representation theory.The representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.The simplest difference equations have the formThe solution of this equation for x in terms of t is found by using its characteristic equationA similar procedure is used for solving a differential equation of the formThe calculation of eigenvalues and eigenvectors is a topic where theory, as presented in elementary linear algebra textbooks, is often very far from practice.The classical method is to first find the eigenvalues, and then calculate the eigenvectors for each eigenvalue. It is in several ways poorly suited for non-exact arithmetics such as floating-point.Once the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrixThis matrix equation is equivalent to two linear equationsEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961.[39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]Most numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation, although sometimes the implementors choose to discard the eigenvector information as soon as it is not needed anymore.The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.Principal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.Eigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed byorleads to a so-called quadratic eigenvalue problem,This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.
Binary operation
In mathematics, a binary operation on a set is a calculation that combines two elements of the set (called operands) to produce another element of the set. More formally, a binary operation is an operation of arity of two whose two domains and one codomain are the same set.  Examples include the familiar elementary arithmetic operations of addition, subtraction, multiplication and division.  Other examples are readily found in different areas of mathematics, such as vector addition, matrix multiplication and conjugation in groups.More precisely, a binary operation on a set S is a map which sends elements of the Cartesian product S × S to S:[1][2][3]Because the result of performing the operation on a pair of elements of S is again an element of S, the operation is called a closed binary operation on S (or sometimes expressed as having the property of closure).[4]  If f is not a function, but is instead a partial function, it is called a partial binary operation.  For instance, division of real numbers is a partial binary operation, because one can't divide by zero: a/0 is not defined for any real a.  Note however that both in algebra and model theory the binary operations considered are defined on all of S × S.Sometimes, especially in computer science, the term is used for any binary function.Binary operations are the keystone of algebraic structures studied in abstract algebra: they are essential in the definitions of groups, monoids, semigroups, rings, and more.  Most generally, a magma is a set together with some binary operation defined on it.Typical examples of binary operations are the addition (+) and multiplication (×) of numbers and matrices as well as composition of functions on a single set.For instance,Many binary operations of interest in both algebra and formal logic are commutative, satisfying f(a, b) = f(b, a) for all elements a and b in S, or associative, satisfying f(f(a, b), c) = f(a, f(b, c)) for all a, b and c in S.  Many also have identity elements and inverse elements.The first three examples above are commutative and all of the above examples are associative.On the set of real numbers R, subtraction, that is, f(a, b) = a − b, is a binary operation which is not commutative since, in general, a − b ≠ b − a.  It is also not associative, since, in general, a − (b − c) ≠ (a − b) − c; for instance, 1 − (2 − 3) = 2 but (1 − 2) − 3 = −4.On the set of natural numbers N, the binary operation exponentiation, f(a,b) = ab, is not commutative since, in general, ab ≠ ba and is also not associative since f(f(a, b), c) ≠ f(a, f(b, c)).  For instance, with a = 2, b = 3 and c = 2, f(23,2) = f(8,2) = 82 = 64, but f(2,32) = f(2,9) = 29 = 512.  By changing the set N to the set of integers Z, this binary operation becomes a partial binary operation since it is now undefined when a = 0 and b is any negative integer.  For either set, this operation has a right identity (which is 1) since f(a, 1) = a for all a in the set, which is not an identity (two sided identity) since f(1, b) ≠ b in general.Division (/), a partial binary operation on the set of real or rational numbers, is not commutative or associative.  Tetration (↑↑), as a binary operation on the natural numbers, is not commutative or associative and has no identity element.Binary operations are often written using infix notation such as a ∗ b, a + b, a · b or (by juxtaposition with no symbol) ab rather than by functional notation of the form f(a, b).  Powers are usually also written without operator, but with the second argument as superscript.Binary operations sometimes use prefix or (probably more often) postfix notation, both of which dispense with parentheses.  They are also called, respectively, Polish notation and reverse Polish notation.A binary operation, ab, depends on the ordered pair (a, b) and so (ab)c (where the parentheses here mean first operate on the ordered pair (a, b) and then operate on the result of that using the ordered pair ((ab), c)) depends in general on the ordered pair ((a, b), c).  Thus, for the general, non-associative case, binary operations can be represented with binary trees.However:A binary operation f on a set S may be viewed as a ternary relation on S, that is, the set of triples (a, b, f(a,b)) in S × S × S for all a and b in S.An external binary operation is a binary function from K × S to S.  This differs from a binary operation in the strict sense in that K need not be S; its elements come from outside.An example of an external binary operation is scalar multiplication in linear algebra.  Here K is a field and S is a vector space over that field.An external binary operation may alternatively be viewed as an action; K is acting on S.Note that the dot product of two vectors is not a binary operation, external or otherwise, as it maps from S × S to K, where K is a field and S is a vector space over K.
Triple product
In vector algebra, a branch of mathematics, the triple product is a product of three 3-dimensional vectors, usually Euclidean vectors. The name "triple product" is used for two different products, the scalar-valued scalar triple product and, less often, the vector-valued vector triple product.The scalar triple product (also called the mixed product, box product, or triple scalar product) is defined as the dot product of one of the vectors with the cross product of the other two.Geometrically, the scalar triple productis the (signed) volume of the parallelepiped defined by the three vectors given. Here, the parentheses may be omitted without causing ambiguity, since the dot product cannot be evaluated first. If it were, it would leave the cross product of a scalar and a vector, which is not defined.Although the scalar triple product gives the volume of the parallelepiped, it is the signed volume, the sign depending on the orientation of the frame or the parity of the permutation of the vectors. This means the product is negated if the orientation is reversed, for example by a parity transformation, and so is more properly described as a pseudoscalar if the orientation can change.This also relates to the handedness of the cross product; the cross product transforms as a pseudovector under parity transformations and so is properly described as a pseudovector. The dot product of two vectors is a scalar but the dot product of a pseudovector and a vector is a pseudoscalar, so the scalar triple product must be pseudoscalar-valued.If T is a rotation operator, thenbut if T is an improper rotation, thenIn exterior algebra and geometric algebra the exterior product of two vectors is a bivector, while the exterior product of three vectors is a trivector. A bivector is an oriented plane element and a trivector is an oriented volume element, in the same way that a vector is an oriented line element. Given vectors a, b and c, the productis a trivector with magnitude equal to the scalar triple product, and is the Hodge dual of the scalar triple product. As the exterior product is associative brackets are not needed as it does not matter which of  a ∧ b or  b ∧ c is calculated first, though the order of the vectors in the product does matter. Geometrically the trivector a ∧ b ∧ c corresponds to the parallelepiped spanned by a, b, and c, with bivectors a ∧ b, b ∧ c and a ∧ c matching the parallelogram faces of the parallelepiped.The triple product is identical to the volume form of the Euclidean 3-space applied to the vectors via interior product. It also can be expressed as a contraction of vectors with a rank-3 tensor equivalent to the form (or a pseudotensor equivalent to the volume pseudoform); see below.The vector triple product is defined as the cross product of one vector with the cross product of the other two. The following relationship holds:This is known as triple product expansion, or Lagrange's formula,[2][3] although the latter name is also used for several other formulas. Its right hand side can be remembered by using the mnemonic "BAC − CAB", provided one keeps in mind which vectors are dotted together. A proof is provided below.Since the cross product is anticommutative, this formula may also be written (up to permutation of the letters) as:From Lagrange's formula it follows that the vector triple product satisfies:which is the Jacobi identity for the cross product.  Another useful formula follows:These formulas are very useful in simplifying vector calculations in physics. A related identity regarding gradients and useful in vector calculus is Lagrange's formula of vector cross-product identity:[4]By combining these three components we obtain:If geometric algebra is used the cross product b × c of vectors is expressed as their exterior product  b∧c, a bivector. The second cross product cannot be expressed as an exterior product, otherwise the scalar triple product would result. Instead a left contraction[6] can be used, so the formula becomes[7]The proof follows from the properties of the contraction.[6] The result is the same vector as calculated using a × (b × c).In tensor notation the triple product is expressed using the Levi-Civita symbol:[8]andReturning to the triple cross product,
Sequence
In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed.  Like a set, it contains members (also called elements, or terms).  The number of elements (possibly infinite) is called the length of the sequence.  Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order matters.  Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first n natural numbers (for a sequence of finite length n).  The position of an element in a sequence is its rank or index; it is the natural number from which the element is the image. It depends on the context or a specific convention, if the first element has index 0 or 1.  When a symbol has been chosen for denoting a sequence, the nth element of the sequence is denoted by this symbol with n as subscript; for example, the nth element of the Fibonacci sequence is generally denoted Fn.For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last.  This sequence differs from (A, R, M, Y).  Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence.  Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...).  In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams.  The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.A sequence can be thought of as a list of elements with a particular order.  Sequences are useful in a number of mathematical disciplines for studying functions, spaces, and other mathematical structures using the convergence properties of sequences.  In particular, sequences are the basis for series, which are important in differential equations and analysis.  Sequences are also of interest in their own right and can be studied as patterns or puzzles, such as in the study of prime numbers.There are a number of ways to denote a sequence, some of which are more useful for specific types of sequences.  One way to specify a sequence is to list the elements.  For example, the first four odd numbers form the sequence (1, 3, 5, 7).  This notation can be used for infinite sequences as well.  For instance, the infinite sequence of positive odd integers can be written (1, 3, 5, 7, ...).  Listing is most useful for infinite sequences with a pattern that can be easily discerned from the first few elements.  Other ways to denote a sequence are discussed after the examples.The prime numbers are the natural numbers bigger than 1 that have no divisors but 1 and themselves.  Taking these in their natural order gives the sequence (2, 3, 5, 7, 11, 13, 17, ...).  The prime numbers are widely used in mathematics and specifically in number theory.The Fibonacci numbers are the integer sequence whose elements are the sum of the previous two elements.  The first two elements are either 0 and 1 or 1 and 1 so that the sequence is (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...).For a large list of examples of integer sequences, see On-Line Encyclopedia of Integer Sequences.Other examples of sequences include ones made up of rational numbers, real numbers, and complex numbers.  The sequence (.9, .99, .999, .9999, ...) approaches the number 1.  In fact, every real number can be written as the limit of a sequence of rational numbers, e.g. via its decimal expansion.  For instance, π is the limit of the sequence (3, 3.1, 3.14, 3.141, 3.1415, ...).  A related sequence is the sequence of decimal digits of π, i.e. (3, 1, 4, 1, 5, 9, ...).  This sequence does not have any pattern that is easily discernible by eye, unlike the preceding sequence, which is increasing.In some cases the elements of the sequence are related naturally to a sequence of integers whose pattern can be easily inferred.  In these cases the index set may be implied by a listing of the first few abstract elements.  For instance, the sequence of squares of odd numbers could be denoted in any of the following ways.Sequences whose elements are related to the previous elements in a straightforward way are often defined using recursion.  This is in contrast to the definition of sequences of elements as functions of their positions.To define a sequence by recursion, one needs a rule to construct each element in terms of the ones before it.  In addition, enough initial elements must be provided so that all subsequent elements of the sequence can be computed by the rule.  The principle of mathematical induction can be used to prove that in this case, there is exactly one sequence that satisfies both the recursion rule and the initial conditions.  Induction can also be used to prove properties about a sequence, especially for sequences whose most natural description is recursive.The Fibonacci sequence can be defined using a recursive rule along with two initial elements.  The rule is that each element is the sum of the previous two elements, and the first two elements are 0 and 1.The first ten terms of this sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34.  A more complicated example of a sequence that is defined recursively is Recaman's sequence.[1]  We can define Recaman's sequence byNot all sequences can be specified by a rule in the form of an equation, recursive or not, and some can be quite complicated.  For example, the sequence of prime numbers is the set of prime numbers in their natural order, i.e. (2, 3, 5, 7, 11, 13, 17, ...).There are many different notions of sequences in mathematics, some of which (e.g., exact sequence) are not covered by the definitions and notations introduced below.For the purposes of this article, we define a sequence to be a function whose domain is an interval of integers.  This definition covers several different uses of the word "sequence", including one-sided infinite sequences, bi-infinite sequences, and finite sequences (see below for definitions).  However, many authors use a narrower definition by requiring the domain of a sequence to be the set of natural numbers.  The narrower definition has the disadvantage that it rules out finite sequences and bi-infinite sequences, both of which are usually called sequences in standard mathematical practice.   In some contexts, to shorten exposition, the codomain of the sequence is fixed by context, for example by requiring it to be the set R of real numbers,[2] the set C of complex numbers,[3] or a topological space.[4]Sequences and their limits (see below) are important concepts for studying topological spaces.  An important generalization of sequences is the concept of nets.  A net is a function from a (possibly uncountable) directed set to a topological space.  The notational conventions for sequences normally apply to nets as well.The length of a sequence is defined as the number of terms in the sequence.A sequence of a finite length n is also called an n-tuple.  Finite sequences include the empty sequence ( ) that has no elements.The terms nondecreasing and nonincreasing are often used in place of increasing and decreasing in order to avoid any possible confusion with strictly increasing and strictly decreasing, respectively.If the sequence of real numbers (an) is such that all the terms are less than some real number M, then the sequence is said to be bounded from above.  In other words, this means that there exists M such that for all n, an ≤ M.  Any such M is called an upper bound.  Likewise, if, for some real m, an ≥ m for all n greater than some N, then the sequence is bounded from below and any such m is called a lower bound.  If a sequence is both bounded from above and bounded from below, then the sequence is said to be bounded.A subsequence of a given sequence is a sequence formed from the given sequence by deleting some of the elements without disturbing the relative positions of the remaining elements.  For instance, the sequence of positive even integers (2, 4, 6, ...) is a subsequence of the positive integers (1, 2, 3, ...).  The positions of some elements change when other elements are deleted.  However, the relative positions are preserved.Some other types of sequences that are easy to define include:An important property of a sequence is convergence.  If a sequence converges, it converges to a particular value known as the limit. If a sequence converges to some limit, then it is convergent. A sequence that does not converge is divergent.Moreover:A Cauchy sequence is a sequence whose terms become arbitrarily close together as n gets very large.  The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis.  One particularly important result in real analysis is Cauchy characterization of convergence for sequences:In contrast, there are Cauchy sequences of rational numbers that are not convergent in the rationals, e.g. the sequence defined byx1 = 1 and xn+1 = xn + 2/xn/2is Cauchy, but has no rational limit, cf. here.  More generally, any sequence of rational numbers that converges to an irrational number is Cauchy, but not convergent when interpreted as a sequence in the set of rational numbers.Metric spaces that satisfy the Cauchy characterization of convergence for sequences are called complete metric spaces and are particularly nice for analysis.In this case we say that the sequence diverges, or that it converges to infinity. An example of such a sequence is an = n.and say that the sequence diverges or converges to negative infinity.Sequences play an important role in topology, especially in the study of metric spaces.  For instance:Sequences can be generalized to nets or filters.  These generalizations allow one to extend some of the above theorems to spaces without metrics.The topological product of a sequence of topological spaces is the cartesian product of those spaces, equipped with a natural topology called the product topology.In analysis, when talking about sequences, one will generally consider sequences of the formwhich is to say, infinite sequences of elements indexed by natural numbers.It may be convenient to have the sequence start with an index different from 1 or 0.  For example, the sequence defined by xn = 1/log(n) would be defined only for n ≥ 2.  When talking about such infinite sequences, it is usually sufficient (and does not change much for most considerations) to assume that the members of the sequence are defined at least for all indices large enough, that is, greater than some given N.The most elementary type of sequences are numerical ones, that is, sequences of real or complex numbers.  This type can be generalized to sequences of elements of some vector space.  In analysis, the vector spaces considered are often function spaces.  Even more generally, one can study sequences with elements in some topological space.A sequence space is a vector space whose elements are infinite sequences of real or complex numbers.  Equivalently, it is a function space whose elements are functions from the natural numbers to the field K, where K is either the field of real numbers or the field of complex numbers.  The set of all such functions is naturally identified with the set of all possible infinite sequences with elements in K, and can be turned into a vector space under the operations of pointwise addition of functions and pointwise scalar multiplication.  All sequence spaces are linear subspaces of this space.  Sequence spaces are typically equipped with a norm, or at least the structure of a topological vector space.The most important sequences spaces in analysis are the ℓp spaces, consisting of the p-power summable sequences, with the p-norm.  These are special cases of Lp spaces for the counting measure on the set of natural numbers.  Other important classes of sequences like convergent sequences or null sequences form sequence spaces, respectively denoted c and c0, with the sup norm.  Any sequence space can also be equipped with the topology of pointwise convergence, under which it becomes a special kind of Fréchet space called an FK-space.Sequences over a field may also be viewed as vectors in a vector space.  Specifically, the set of F-valued sequences (where F is a field) is a function space (in fact, a product space) of F-valued functions over the set of natural numbers.Abstract algebra employs several types of sequences, including sequences of mathematical objects such as groups or rings.If A is a set, the free monoid over A (denoted A*, also called Kleene star of A) is a monoid containing all the finite sequences (or strings) of zero or more elements of A, with the binary operation of concatenation.  The free semigroup A+ is the subsemigroup of A* containing all elements except the empty sequence.In the context of group theory, a sequenceof groups and group homomorphisms is called exact, if the image (or range) of each homomorphism is equal to the kernel of the next:Note that the sequence of groups and homomorphisms may be either finite or infinite.A similar definition can be made for certain other algebraic structures.  For example, one could have an exact sequence of vector spaces and linear maps, or of modules and module homomorphisms.In homological algebra and algebraic topology, a spectral sequence is a means of computing homology groups by taking successive approximations.  Spectral sequences are a generalization of exact sequences, and since their introduction by Jean Leray (1946), they have become an important research tool, particularly in homotopy theory.An ordinal-indexed sequence is a generalization of a sequence.  If α is a limit ordinal and X is a set, an α-indexed sequence of elements of X is a function from α to X.  In this terminology an ω-indexed sequence is an ordinary sequence.Automata or finite state machines can typically be thought of as directed graphs, with edges labeled using some specific alphabet, Σ.  Most familiar types of automata transition from state to state by reading input letters from Σ, following edges with matching labels; the ordered input for such an automaton forms a sequence called a word (or input word).  The sequence of states encountered by the automaton when processing a word is called a run.  A nondeterministic automaton may have unlabeled or duplicate out-edges for any state, giving more than one successor for some input letter.  This is typically thought of as producing multiple possible runs for a given word, each being a sequence of single states, rather than producing a single run that is a sequence of sets of states; however, 'run' is occasionally used to mean the latter.Infinite sequences of digits (or characters) drawn from a finite alphabet are of particular interest in theoretical computer science.  They are often referred to simply as sequences or streams, as opposed to finite strings.  Infinite binary sequences, for instance, are infinite sequences of bits (characters drawn from the alphabet {0, 1}).  The set C = {0, 1}∞ of all infinite binary sequences is sometimes called the Cantor space.An infinite binary sequence can represent a formal language (a set of strings) by setting the n th bit of the sequence to 1 if and only if the n th string (in shortlex order) is in the language.  This representation is useful in the diagonalization method for proofs.[8]
Principal ideal domain
In abstract algebra, a principal ideal domain, or PID, is an integral domain in which every ideal is principal, i.e., can be generated by a single element. More generally, a principal ideal ring is a nonzero commutative ring whose ideals are principal, although some authors (e.g., Bourbaki) refer to PIDs as principal rings. The distinction is that a principal ideal ring may have zero divisors whereas a principal ideal domain cannot.Principal ideal domains are thus mathematical objects that behave somewhat like the integers, with respect to divisibility: any element of a PID has a unique decomposition into prime elements (so an analogue of the fundamental theorem of arithmetic holds); any two elements of a PID have a greatest common divisor (although it may not be possible to find it using the Euclidean algorithm). If x and y are elements of a PID without common divisors, then every element of the PID can be written in the form ax + by.Principal ideal domains are noetherian, they are  integrally closed, they are unique factorization domains and Dedekind domains.  All Euclidean domains and all fields are principal ideal domains.Principal ideal domains appear in the following chain of class inclusions:Examples include:Examples of integral domains that are not PIDs:In a principal ideal domain, any two elements a,b have a greatest common divisor, which may be obtained as a generator of the ideal (a,b).The previous three statements give the definition of a Dedekind domain, and hence every principal ideal domain is a Dedekind domain.Let A be an integral domain. Then the following are equivalent.A field norm is a Dedekind-Hasse norm; thus, (5) shows that a Euclidean domain is a PID. (4) compares to:An integral domain is a Bézout domain if and only if any two elements in it have a gcd that is a linear combination of the two. A Bézout domain is thus a GCD domain, and (4) gives yet another proof that a PID is a UFD.
Module homomorphism
If M, N are right modules, then the second condition is replaced withThe pre-image of the zero element under f is called the kernel of f. The set of all module homomorphisms from M to N is denoted by HomR(M, N). It is an abelian group (under pointwise addition) but is not necessarily a module unless R is commutative.The composition of module homomorphisms is again a module homomorphism. Thus, all the (say left) modules together with all the module homomorphisms between them form the category of modules.A module homomorphism is called an isomorphism if it admits an inverse homomorphism; in particular, it is a bijection. One can show a bijective module homomorphism is an isomorphism; i.e., the inverse is a module homomorphism. In particular, a module homomorphism is an isomorphism if and only if it is an isomorphism between the underlying abelian groups.The isomorphism theorems hold for module homomorphisms.Schur's lemma says that a homomorphism between simple modules (a module having only two submodules) must be either zero or an isomorphism. In particular, the endomorphism ring of a simple module is a division ring.In the language of the category theory, an injective homomorphism is also called a monomorphism and a surjective homomorphism an epimorphism.In short, Hom inherits a ring action that was not used up to form Hom. More precise, let M, N be left R-modules. Suppose M has a right action of a ring S that commutes with the R-action; i.e., M is an (R, S)-module. Thenhas the structure of a left S-module defined by: for s in S and x in M,Note: the above verification would "fail" if one used the left R-action in place of the right S-action. In this sense, Hom is often said to "use up" the R-action.The relationship between matrices and linear transformations in linear algebra generalizes in a natural way to module homomorphisms. Precisely, given a right R-module U, there is the canonical isomorphism of the abelian groupswhich turns out to be a ring isomorphism.and their tensor product iswhich is the image of the graph morphism[disambiguation needed] M → M ⊕ N, x → (x, f(x)).The transpose of f isIf f is an isomorphism, then the transpose of the inverse of f is called the contragredient of f.A short sequence of modules over a commutative ringconsists of modules A, B, C and homomorphisms f, g. It is exact if f is injective, the kernel of g is the image of f and g is surjective. A longer exact sequence is defined in the similar way. A sequence of modules is exact if and only if it is exact as a sequence of abelian groups. Also the sequence is exact if and only if it is exact at all the maximal ideals:Any module homomorphism f fits intowhere K is the kernel of f and C is the cokernel, the quotien of N by the image of f.See also: Herbrand quotient (which can be defined for any endomorphism with some finiteness conditions.)A transgression that arises from a spectral sequence is an example of an additive relation.
Linear complementarity problem
In mathematical optimization theory, the linear complementarity problem (LCP) arises frequently in computational mechanics and encompasses the well-known quadratic programming as a special case.  It was proposed by Cottle and Dantzig in 1968.[1][2][3]Given a real matrix M and vector q, the linear complementarity problem LCP(M, q) seeks vectors z and w which satisfy the following constraints:A sufficient condition for existence and uniqueness of a solution to this problem is that M be symmetric positive-definite. If M is such that LCP(M, q) have a solution for every q, then M is a Q-matrix. If M is such that LCP(M, q) have a unique solution for every q, then M is a P-matrix. Both of these characterizations are sufficient and necessary.[4]The vector w is a slack variable,[5] and so is generally discarded after z is found. As such, the problem can also be formulated as:Finding a solution to the linear complementarity problem is associated with minimizing the quadratic functionsubject to the constraintsThese constraints ensure that f is always non-negative. The minimum of f is 0 at z if and only if z solves the linear complementarity problem.If M is positive definite, any algorithm for solving (strictly) convex QPs can solve the LCP.  Specially designed basis-exchange pivoting algorithms, such as Lemke's algorithm and a variant of the simplex algorithm of Dantzig have been used for decades. Besides having polynomial time complexity, interior-point methods are also effective in practice.is the same as solving the LCP withThis is because the Karush–Kuhn–Tucker conditions of the QP problem can be written as:with v the Lagrange multipliers on the non-negativity constraints, λ the multipliers on the inequality constraints, and s the slack variables for the inequality constraints. The fourth condition derives from the complementarity of each group of variables (x, s) with its set of KKT vectors (optimal Lagrange multipliers) being (v, λ). In that case,If the non-negativity constraint on the x is relaxed, the dimensionality of the LCP problem can be reduced to the number of the inequalities, as long as Q is non-singular (which is guaranteed if it is positive definite). The multipliers v are no longer present, and the first KKT conditions can be rewritten as:or:pre-multiplying the two sides by A and subtracting b we obtain:The left side, due to the second KKT condition, is s. Substituting and reordering:Calling nowwe have an LCP, due to the relation of complementarity between the slack variables s and their Lagrange multipliers λ. Once we solve it, we may obtain the value of x from λ  through the first KKT condition.Finally, it is also possible to handle additional equality constraints:From λ we can now recover the values of both x and the Lagrange multiplier of equalities μ:In fact, most QP solvers work on the LCP formulation, including the interior point method, principal / complementarity pivoting, and active set methods.[1][2] LCP problems can be solved also by the criss-cross algorithm,[6][7][8][9] conversely, for linear complementarity problems, the criss-cross algorithm terminates finitely only if the matrix is a sufficient matrix.[8][9] A sufficient matrix is a generalization both of a positive-definite matrix and of a P-matrix, whose principal minors are each positive.[8][9][10]Such LCPs can be solved when they are formulated abstractly using oriented-matroid theory.[11][12][13]
Identity matrix
In linear algebra, the identity matrix, or sometimes ambiguously called a unit matrix, of size n is the n × n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. (In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I.) Less frequently, some mathematics books use U or E to represent the identity matrix, meaning "unit matrix"[1] and the German word "Einheitsmatrix",[2] respectively.When A is m×n, it is a property of matrix multiplication thatIn particular, the identity matrix serves as the unit of the ring of all n×n matrices, and as the identity element of the general linear group GL(n) consisting of all invertible n×n matrices. (The identity matrix itself is invertible, being its own inverse.)Where n×n matrices are used to represent linear transformations from an n-dimensional vector space to itself, In represents the identity function, regardless of the basis.The ith column of an identity matrix is the unit vector ei.  It follows that the determinant of the identity matrix is 1 and the trace is n.Using the notation that is sometimes used to concisely describe diagonal matrices, we can write:It can also be written using the Kronecker delta notation:The identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.The identity matrix of a given size is the only idempotent matrix of that size having full rank.  That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are linearly independent.The principal square root of an identity matrix is itself, and this is its only positive definite square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.[3]
Discrete Fourier transform
In mathematics, the discrete Fourier transform (DFT) converts a finite sequence of equally-spaced samples of a function into a same-length sequence of equally-spaced samples of the discrete-time Fourier transform (DTFT), which is a complex-valued function of frequency. The interval at which the DTFT is sampled is the reciprocal of the duration of the input sequence.  An inverse DFT is a Fourier series, using the DTFT samples as coefficients of complex sinusoids at the corresponding DTFT frequencies.  It has the same sample-values as the original input sequence.  The DFT is therefore said to be a frequency domain representation of the original input sequence.  If the original sequence spans all the non-zero values of a function, its DTFT is continuous (and periodic), and the DFT provides discrete samples of one cycle.  If the original sequence is one cycle of a periodic function, the DFT provides all the non-zero values of one DTFT cycle.The DFT is the most important discrete transform, used to perform Fourier analysis in many practical applications.[1]  In digital signal processing, the function is any quantity or signal that varies over time, such as the pressure of a sound wave, a radio signal, or daily temperature readings, sampled over a finite time interval (often defined by a window function[2]). In image processing, the samples can be the values of pixels along a row or column of a raster image. The DFT is also used to efficiently solve partial differential equations, and to perform other operations such as convolutions or multiplying large integers.Since it deals with a finite amount of data, it can be implemented in computers by numerical algorithms or even dedicated hardware. These implementations usually employ efficient fast Fourier transform (FFT) algorithms;[3] so much so that the terms "FFT" and "DFT" are often used interchangeably.  Prior to its current usage, the "FFT" initialism may have also been used for the ambiguous term "finite Fourier transform".    (Eq.1)where the last expression follows from the first one by Euler's formula.Eq.1 can be interpreted or derived in various ways, for example:    (Eq.2)The discrete Fourier transform is an invertible, linear transformationThe inverse transform is given by:where the star denotes complex conjugation.  Plancherel theorem is a special case of the Parseval's theorem and states:These theorems are also equivalent to the unitary condition below.The periodicity can be shown directly from the definition:Similarly, it can be shown that the IDFT formula leads to a periodic extension.It can also be shown that:The trigonometric interpolation polynomialAnother way of looking at the DFT is to note that in the above discussion, the DFT can be expressed as the DFT matrix, a Vandermonde matrix, introduced by Sylvester in 1867,whereis a primitive Nth root of unity.The inverse transform is then given by the inverse of the above matrix,The orthogonality of the DFT is now expressed as an orthonormality condition (which arises in many areas of mathematics as described in root of unity):If X  is defined as the unitary DFT of the vector x, thenand the Plancherel theorem is expressed asA consequence of the circular convolution theorem is that the DFT matrix F diagonalizes any circulant matrix.A useful property of the DFT is that the inverse DFT can be easily expressed in terms of the (forward) DFT, via several well-known "tricks".  (For example, in computations, it is often convenient to only implement a fast Fourier transform corresponding to one transform direction and then to get the other transform direction from the first.)First, we can compute the inverse DFT by reversing all but one of the inputs (Duhamel et al., 1988):Second, one can also conjugate the inputs and outputs:That is, the inverse transform is the same as the forward transform with the real and imaginary parts swapped for both input and output, up to a normalization (Duhamel et al., 1988).The eigenvalues of the DFT matrix are simple and well-known, whereas the eigenvectors are complicated, not unique, and are the subject of ongoing research.This matrix satisfies the matrix polynomial equation:The problem of their multiplicity was solved by McClellan and Parks (1972), although it was later shown to have been equivalent to a problem solved by Gauss (Dickinson and Steiglitz, 1982).  The multiplicity depends on the value of N modulo 4, and is given by the following table:No simple analytical formula for general eigenvectors is known.   Moreover, the eigenvectors are not unique because any linear combination of eigenvectors for the same eigenvalue is also an eigenvector for that eigenvalue.  Various researchers have proposed different choices of eigenvectors, selected to satisfy useful properties like orthogonality and to have "simple" forms (e.g., McClellan and Parks, 1972; Dickinson and Steiglitz, 1982; Grünbaum, 1982; Atakishiyev and Wolf, 1997; Candan et al., 2000; Hanna et al., 2004; Gurevich and Hadani, 2008).A straightforward approach is to discretize an eigenfunction of the continuous Fourier transform,of which the most famous is the Gaussian function.Since periodic summation of the function means discretizing its frequency spectrumand discretization means periodic summation of the spectrum,the discretized and periodically summed Gaussian function yields an eigenvector of the discrete transform:The closed form expression for the series can be expressed byJacobi theta functions asTwo other simple closed-form analytical eigenvectors for special DFT period N were found (Kong, 2008):For DFT period N = 2L + 1 = 4K +1, where K is an integer, the following is an eigenvector of DFT:For DFT period N = 2L = 4K, where K is an integer, the following is an eigenvector of DFT:The choice of eigenvectors of the DFT matrix has become important in recent years in order to define a discrete analogue of the fractional Fourier transform—the DFT matrix can be taken to fractional powers by exponentiating the eigenvalues (e.g., Rubio and Santhanam, 2005).  For the continuous Fourier transform, the natural orthogonal eigenfunctions are the Hermite functions, so various discrete analogues of these have been employed as the eigenvectors of the DFT, such as the Kravchuk polynomials (Atakishiyev and Wolf, 1997).  The "best" choice of eigenvectors to define a fractional discrete Fourier transform remains an open question, however.If the random variable Xk is constrained bythen may be considered to represent a discrete probability mass function of n, with an associated probability mass function constructed from the transformed variable,However, the Hirschman entropic uncertainty will have a useful analog for the case of the DFT.[7] The Hirschman uncertainty principle is expressed in terms of the Shannon entropy of the two probability functions.In the discrete case, the Shannon entropies are defined asandand the entropic uncertainty principle becomes[7]It is possible to shift the transform sampling in time and/or frequency domain by some real shifts a and b, respectively. This is sometimes known as a generalized DFT (or GDFT), also called the shifted DFT or offset DFT, and has analogous properties to the ordinary DFT:The term GDFT is also used for the non-linear phase extensions of DFT. Hence, GDFT method provides a generalization for constant amplitude orthogonal block transforms including linear and non-linear phase types. GDFT is a framework to improve time and frequency domain properties of the traditional DFT, e.g. auto/cross-correlations, by the addition of the properly designed phase shaping function (non-linear, in general) to the original linear phase functions (Akansu and Agirman-Tosun, 2010).[10]The discrete Fourier transform can be viewed as a special case of the z-transform, evaluated on the unit circle in the complex plane; more general z-transforms correspond to complex shifts a and b above.The inverse of the multi-dimensional DFT is, analogous to the one-dimensional case, given by:An algorithm to compute a one-dimensional DFT is thus sufficient to efficiently compute a multidimensional DFT.  This approach is known as the row-column algorithm. There are also intrinsically multidimensional FFT algorithms.The DFT has seen wide usage across a large number of fields; we only sketch a few examples below (see also the references at the end). All applications of the DFT depend crucially on the availability of a fast algorithm to compute discrete Fourier transforms and their inverses, a fast Fourier transform.A final source of distortion (or perhaps illusion) is the DFT itself, because it is just a discrete sampling of the DTFT, which is a function of a continuous frequency domain.  That can be mitigated by increasing the resolution of the DFT.  That procedure is illustrated at Sampling the DTFT.See FFT filter banks and Sampling the DTFT.The field of digital signal processing relies heavily on operations in the frequency domain (i.e. on the Fourier transform). For example, several lossy image and sound compression methods employ the discrete Fourier transform: the signal is cut into short segments, each is transformed, and then the Fourier coefficients of high frequencies, which are assumed to be unnoticeable, are discarded. The decompressor computes the inverse transform based on this reduced number of Fourier coefficients. (Compression applications often use a specialized form of the DFT, the discrete cosine transform or sometimes the modified discrete cosine transform.)Some relatively recent compression algorithms, however, use wavelet transforms, which give a more uniform compromise between time and frequency domain than obtained by chopping data into segments and transforming each segment.  In the case of JPEG2000, this avoids the spurious image features that appear when images are highly compressed with the original JPEG.Suppose we wish to compute the polynomial product c(x) = a(x) · b(x).  The ordinary product expression for the coefficients of c involves a linear (acyclic) convolution, where indices do not "wrap around."  This can be rewritten as a cyclic convolution by taking the coefficient vectors for a(x) and b(x) with constant term first, then appending zeros so that the resultant coefficient vectors a and b have dimension d > deg(a(x)) + deg(b(x)).  Then,But convolution becomes multiplication under the DFT:Here the vector product is taken elementwise.  Thus the coefficients of the product polynomial c(x) are just the terms 0, ..., deg(a(x)) + deg(b(x)) of the coefficient vectorWith a fast Fourier transform, the resulting algorithm takes O (N log N) arithmetic operations.  Due to its simplicity and speed, the Cooley–Tukey FFT algorithm, which is limited to composite sizes, is often chosen for the transform operation.  In this case, d should be chosen as the smallest integer greater than the sum of the input polynomial degrees that is factorizable into small prime factors (e.g. 2, 3, and 5, depending upon the FFT implementation).The fastest known algorithms for the multiplication of very large integers use the polynomial multiplication method outlined above.  Integers can be treated as the value of a polynomial evaluated specifically at the number base, with the coefficients of the polynomial corresponding to the digits in that base.  After polynomial multiplication, a relatively low-complexity carry-propagation step completes the multiplication.When data is convolved with a function with wide support, such as for downsampling by a large sampling ratio, because of the Convolution theorem and the FFT algorithm, it may be faster to transform it, multiply pointwise by the transform of the filter and then reverse transform it.  Alternatively, a good filter is obtained by simply truncating the transformed data and re-transforming the shortened data set.From this point of view, one may generalize the DFT to representation theory generally, or more narrowly to the representation theory of finite groups.More narrowly still, one may generalize the DFT by either changing the target (taking values in a field other than the complex numbers), or the domain (a group other than a finite cyclic group), as detailed in the sequel.The standard DFT acts on a sequence x0, x1, …, xN−1 of complex numbers, which can be viewed as a function {0, 1, …, N − 1} → C. The multidimensional DFT acts on multidimensional sequences, which can be viewed as functionsThis suggests the generalization to Fourier transforms on arbitrary finite groups, which act on functions G → C where G is a finite group. In this framework, the standard DFT is seen as the Fourier transform on a cyclic group, while the multidimensional DFT is a Fourier transform on a direct sum of cyclic groups.Further, Fourier transform can be on cosets of a group. There are various alternatives to the DFT for various applications, prominent among which are wavelets. The analog of the DFT is the discrete wavelet transform (DWT). From the point of view of time–frequency analysis, a key limitation of the Fourier transform is that it does not include location information, only frequency information, and thus has difficulty in representing transients. As wavelets have location as well as frequency, they are better able to represent location, at the expense of greater difficulty representing frequency. For details, see comparison of the discrete wavelet transform with the discrete Fourier transform.
Shear matrix
In mathematics, a shear matrix or transvection is an elementary matrix that represents the addition of a multiple of one row or column to another. Such a matrix may be derived by taking the identity matrix and replacing one of the zero elements with a non-zero value.A typical shear matrix is shown below:The name shear reflects the fact that the matrix represents a shear transformation. Geometrically, such a transformation takes pairs of points in a linear space, that are purely axially separated along the axis whose row in the matrix contains the shear element, and effectively replaces those pairs by pairs whose separation is no longer purely axial but has two vector components. Thus, the shear axis is always an eigenvector of S.If S is an n×n shear matrix, then:
Benjamin Peirce
Benjamin Peirce (/ˈpɜːrs/;[1]) FRSFor HFRSE  April 4, 1809 – October 6, 1880) was an American mathematician who taught at Harvard University for approximately 50 years. He made contributions to celestial mechanics, statistics, number theory, algebra, and the philosophy of mathematics.He was born in Salem, Massachusetts, the son of Benjamin Peirce (1778–1831), later librarian of Harvard, and Lydia Ropes Nichols Peirce (1781–1868).[2]After graduating from Harvard University in 1829, he remained as a tutor, and was subsequently appointed professor of mathematics in 1831. He added astronomy to his portfolio in 1842, and remained as Harvard professor until his death. In addition, he was instrumental in the development of Harvard's science curriculum, served as the college  librarian, and was director of the U.S. Coast Survey from 1867 to 1874.He was elected a Foreign Member of the Royal Society of London in 1852.[3]Benjamin Peirce is often regarded as the earliest American scientist whose research was recognized as world class.[4] He was an apologist for slavery, opining that it should be condoned if it was used to allow an elite to pursue scientific enquiry.[5]In number theory, he proved there is no odd perfect number with fewer than four prime factors.In algebra, he was notable for the study of associative algebras. He first introduced the terms idempotent and nilpotent in 1870 to describe elements of these algebras, and he also introduced the Peirce decomposition.In the philosophy of mathematics, he became known for the statement that "Mathematics is the science that draws necessary conclusions".[6] Peirce's definition of mathematics was credited by his son, Charles Sanders Peirce, as helping to initiate the consequence-oriented philosophy of pragmatism.  Like George Boole, Peirce believed that mathematics could be used to study logic. These ideas were further developed by son Charles , who noted that logic also includes the study of faulty reasoning.  In contrast, the later logicist program of Gottlob Frege and Bertrand Russell attempted to base mathematics on logic.Peirce proposed what came to be known as Peirce's Criterion for the statistical treatment of outliers, that is, of apparently extreme observations. His ideas were further developed by his son Charles.[7]Peirce was an expert witness in the Howland will forgery trial, where he was assisted by his son Charles. Their analysis of the questioned signature showed that it resembled another particular handwriting example so closely that the chances of such a match were statistically extremely remote.[citation needed]He was devoutly religious, though he seldom published his theological thoughts.[8] Peirce credited God as shaping nature in ways that account for the efficacy of pure mathematics in describing empirical phenomena.[9] Peirce viewed "mathematics as study of God's work by God's creatures", according to an encyclopedia.[8]He married Sarah Hunt Mills, the daughter of U.S. Senator Elijah Hunt Mills.[10] Peirce and his wife had four sons and one daughter:[11]The lunar crater Peirce is named for Peirce.Post-doctoral positions in Harvard University's mathematics department are named in his honor as Benjamin Peirce Fellows and Lecturers.The United States Coast Survey ship USCS Benjamin Peirce, in commission from 1855 to 1868, was named for him.[12]
Orthant
In geometry, an orthant[1] or hyperoctant[2] is the analogue in n-dimensional Euclidean space of a quadrant in the plane or an octant in three dimensions.In general an orthant in n-dimensions can be considered the intersection of n mutually orthogonal half-spaces. By independent selections of half-space signs, there are 2n orthants in n-dimensional space.More specifically, a closed orthant in Rn is a subset defined by constraining each Cartesian coordinate to be nonnegative or nonpositive.  Such a subset is defined by a system of inequalities:where each εi is +1 or −1.Similarly, an open orthant in Rn is a subset defined by a system of strict inequalitieswhere each εi is +1 or −1.By dimension:John Conway defined the term n-orthoplex from orthant complex as a regular polytope in n-dimensions with 2n simplex facets, one per orthant.[3]The nonnegative orthant is the generalization of the first quadrant to n dimensions and is important in many constrained optimization problems.[[Mohammad Amin Ahmadi]
Linear subspace
In linear algebra and related fields of mathematics, a linear subspace, also known as a vector subspace, or, in the older literature, a linear manifold,[1][2] is a vector space that is a subset of some other (higher-dimension) vector space. A linear subspace is usually called simply a subspace when the context serves to distinguish it from other kinds of subspace.Let K be a field (such as the real numbers), V be a vector space over K, and let W be a subset of V.Then W is a subspace if:Example I:Let the field K be the set R of real numbers, and let the vector space V be the real coordinate space R3.Take W to be the set of all vectors in V whose last component is 0.Then W is a subspace of V.Proof:Example II:Let the field be R again, but now let the vector space be the Cartesian plane R2.Take W to be the set of points (x, y) of R2 such that x = y.Then W is a subspace of R2.Proof:In general, any subset of the real coordinate space Rn that is defined by a system of homogeneous linear equations will yield a subspace.(The equation in example I was z = 0, and the equation in example II was x = y.)Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.Example III:Again take the field to be R, but now let the vector space V be the set RR of all functions from R to R.Let C(R) be the subset consisting of continuous functions.Then C(R) is a subspace of RR.Proof:Example IV:Keep the same field and vector space as before, but now consider the set Diff(R) of all differentiable functions.The same sort of argument as before shows that this is a subspace too.Examples that extend these themes are common in functional analysis.A way to characterize subspaces is that they are closed under linear combinations.That is, a nonempty set W is a subspace if and only if every linear combination of (finitely many) elements of W also belongs to W.Conditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.In a topological vector space X, a subspace W need not be closed in general, but a finite-dimensional subspace is always closed.[3] The same is true for subspaces of finite codimension, i.e. determined by a finite number of continuous linear functionals.Descriptions of subspaces include the solution set to a homogeneous system of linear equations, the subset of Euclidean space described by a system of homogeneous linear parametric equations, the span of a collection of vectors, and the null space, column space, and row space of a matrix. Geometrically (especially, over the field of real numbers and its subfields), a subspace is a flat in an n-space that passes through the origin.A natural description of an 1-subspace is the scalar multiplication of one non-zero vector v to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:This idea is generalized for higher dimensions with linear span, but criteria for equality of k-spaces specified by sets of k vectors are not so simple.A dual description is provided with linear functionals (usually implemented as linear equations). One non-zero linear functional F specifies its kernel subspace F = 0 of codimension 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the dual space):It is generalized for higher codimensions with a system of equations. The following two subsections will present this latter description in details, and the remaining four subsections further describe the idea of linear span.The solution set to any homogeneous system of linear equations with n variables is a subspace in the coordinate space Kn:For example (over real or rational numbers), the set of all vectors (x, y, z) satisfying the equationsis a one-dimensional subspace. More generally, that is to say that given a set of n independent functions, the dimension of the subspace in Kk will be the dimension of the null set of A, the composite matrix of the n functions.In a finite-dimensional space, a homogeneous system of linear equations can be written as a single matrix equation:The set of solutions to this equation is known as the null space of the matrix. For example, the subspace described above is the null space of the matrixEvery subspace of Kn can be described as the null space of some matrix (see algorithms, below).The subset of Kn described by a system of homogeneous linear parametric equations is a subspace:For example, the set of all vectors (x, y, z) parameterized by the equationsis a two-dimensional subspace of K3, if K is a number field (such as real or rational numbers).[4]In linear algebra, the system of parametric equations can be written as a single vector equation:The expression on the right is called a linear combination of the vectors(2, 5, −1) and (3, −4, 2). These two vectors are said to span the resulting subspace.In general, a linear combination of vectors v1, v2, ... , vk is any vector of the formThe set of all possible linear combinations is called the span:If the vectors v1, ... , vk have n components, then their span is a subspace of Kn. Geometrically, the span is the flat through the origin in n-dimensional space determined by the points v1, ... , vk.A system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:In this case, the subspace consists of all possible values of the vector x. In linear algebra, this subspace is known as the column space (or image) of the matrix A. It is precisely the subspace of Kn spanned by the column vectors of A.The row space of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the orthogonal complement of the null space (see below).In general, a subspace of Kn determined by k parameters (or spanned by k vectors) has dimension k. However, there are exceptions to this rule. For example, the subspace of K3 spanned by the three vectors (1, 0, 0), (0, 0, 1), and (2, 0, 3) is just the xz-plane, with each point on the plane described by infinitely many different values of  t1, t2, t3.In general, vectors v1, ... , vk are called linearly independent iffor(t1, t2, ... , tk) ≠ (u1, u2, ... , uk).[5]If  v1, ..., vk  are linearly independent, then the coordinates  t1, ..., tk for a vector in the span are uniquely determined.A basis for a subspace S is a set of linearly independent vectors whose span is S. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see algorithms, below).The set-theoretical inclusion binary relation specifies a partial order on the set of all subspaces (of any dimension).A subspace cannot lie in any subspace of lesser dimension. If dim U = k, a finite number, and U ⊂ W, then dim W = k if and only if U = W.Given subspaces U and W of a vector space V, then their intersection U ∩ W := {v ∈ V : v is an element of both U and W} is also a subspace of V.[6]Proof:For every vector space V, the set {0} and V itself are subspaces of V.[7]If U and W are subspaces, their sum is the subspaceFor example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequalityHere the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:The operations intersection and sum make the set of all subspaces a bounded modular lattice, where the {0} subspace, the least element, is an identity element of the sum operation, and the identical subspace V, the greatest element, is an identity element of the intersection operation.If V is an inner product space, then the orthogonal complement ⊥ of any subspace of V is again a subspace. This operation, understood as negation (¬), makes the lattice of subspaces a (possibly infinite) orthocomplemented lattice (it is not a distributive lattice).In a pseudo-Euclidean space there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a Heyting algebra) because of null subspaces, for which N ∩ N⊥ = N ≠ {0}. The same case presents the ⊥ operation in symplectic vector spaces.Most algorithms for dealing with subspaces involve row reduction. This is the process of applying elementary row operations to a matrix until it reaches either row echelon form or reduced row echelon form. Row reduction has the following important properties:See the article on row space for an example.If we instead put the matrix A into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of Kn are equal.See the article on column space for an example.This produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.If the final column of the reduced row echelon form contains a pivot, then the input vector v does not lie in S.See the article on null space for an example.
Linear function
In mathematics, the term linear function refers to two distinct but related notions:[1]In calculus, analytic geometry and related areas, a linear function is a polynomial of degree one or less, including the zero polynomial (the latter not being considered to have degree zero).When the function is of only one variable, it is of the formwhere a and b are constants, often real numbers. The graph of such a function of one variable is a nonvertical line. a is frequently referred to as the slope of the line, and b as the intercept.and the graph is a hyperplane of dimension k.A constant function is also considered linear in this context, as it is a polynomial of degree zero or is the zero polynomial. Its graph, when there is only one independent variable, is a horizontal line.In this context, the other meaning (a linear map) may be referred to as a homogeneous linear function or a linear form.  In the context of linear algebra, this meaning (polynomial functions of degree 0 or 1) is a special kind of affine map.In linear algebra, a linear function is a map f between two vector spaces that preserves vector addition and scalar multiplication:Here a denotes a constant belonging to some field K of scalars (for example, the real numbers) and x and y are elements of a vector space, which might be K itself.Some authors use "linear function" only for linear maps that take values in the scalar field;[6] these are also called linear functionals.
Mathematics
Mathematics (from Greek μάθημα máthēma, "knowledge, study, learning") includes the study of such topics as quantity,[1] structure,[2] space,[1] and change.[3][4][5]Mathematicians seek and use patterns[6][7] to formulate new conjectures; they resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.[8]Mathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics, or mathematics for its own sake, without having any application in mind. Practical applications for what began as pure mathematics are often discovered.[9][10]The history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals,[11] was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.[12]Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[13] The most ancient mathematical texts from Mesopotamia and Egypt are from 2000–1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication and division) first appear in the archaeological record. The Babylonians also possessed a place-value system, and used a sexagesimal numeral system, still in use today for measuring angles and time.[14]Beginning in the 6th century BC with the Pythagoreans, the Ancient Greeks began a systematic study of mathematics as a subject in its own right with Greek mathematics.[15] Around 300 BC, Euclid introduced the axiomatic method still used in mathematics today, consisting of definition, axiom, theorem, and proof. His textbook Elements is widely considered the most successful and influential textbook of all time.[16] The greatest mathematician of antiquity is often held to be Archimedes (c. 287–212 BC) of Syracuse.[17] He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[18] Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC),[19] trigonometry (Hipparchus of Nicaea (2nd century BC),[20] and the beginnings of algebra (Diophantus, 3rd century AD).[21]The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition of sine and cosine, and an early form of infinite series.During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other notable achievements of the Islamic period are advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī. During the early modern period, mathematics began to develop at an accelerating pace in Western Europe. The development of calculus by Newton and Leibniz in the 17th century revolutionized mathematics. Leonhard Euler was the most notable mathematician of the 18th century, contributing numerous theorems and discoveries. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Friedrich Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory,number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show that any axiomatic system that is consistent will contain unprovable propositions.Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."[22]The word mathematics comes from Ancient Greek μάθημα (máthēma), meaning "that which is learnt",[23] "what one gets to know", hence also "study" and "science". The word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times.[24] Its adjective is μαθηματικός (mathēmatikós), meaning "related to learning" or "studious", which likewise further came to mean "mathematical". In particular, μαθηματικὴ τέχνη (mathēmatikḗ tékhnē), Latin: ars mathematica, meant "the mathematical art".Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant "teachers" rather than "mathematicians" in the modern sense.In Latin, and in English until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of mathematici, meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.[25]The apparent plural form in English, like the French plural form les mathématiques (and the less commonly used singular derivative la mathématique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural τὰ μαθηματικά (ta mathēmatiká), used by Aristotle (384–322 BC), and meaning roughly "all things mathematical"; although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from Greek.[26] In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in North America, math.[27]Mathematics has no generally accepted definition.[28][29] Aristotle defined mathematics as "the science of quantity", and this definition prevailed until the 18th century.[30]  Galileo Galilei (1564–1642) said, "The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth."[31] Carl Friedrich Gauss (1777–1855) referred to mathematics as "the Queen of the Sciences".[32] Benjamin Peirce (1809–1880) called mathematics "the science that draws necessary conclusions".[33] David Hilbert said of mathematics: "We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise."[34] Albert Einstein (1879–1955) stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[35]Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions.[36] Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals.[28] There is not even consensus on whether mathematics is an art or a science.[29] A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable.[28] Some just say, "Mathematics is what mathematicians do."[28]Three leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought.[37] All have severe problems, none has widespread acceptance, and no reconciliation seems possible.[37]An early definition of mathematics in terms of logic was Benjamin Peirce's "the science that draws necessary conclusions" (1870).[38] In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's "All Mathematics is Symbolic Logic" (1903).[39]Intuitionist definitions, developing from the philosophy of mathematician L. E. J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other."[37] A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems".[40] A formal system is a set of symbols, or tokens, and some rules telling how the tokens may be combined into formulas. In formal systems, the word axiom has a special meaning, different from the ordinary meaning of "a self-evident truth". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.The German mathematician Carl Friedrich Gauss referred to mathematics as "the Queen of the Sciences".[32] More recently, Marcus du Sautoy has called mathematics "the Queen of Science ... the main driving force behind scientific discovery".[41] In the original Latin Regina Scientiarum, as well as in German Königin der Wissenschaften, the word corresponding to science means a "field of knowledge", and this was the original meaning of "science" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of "science" to natural science follows the rise of Baconian science, which contrasted "natural science" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[35]Many philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper.[42] However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians[who?] that mathematics cannot be reduced to logic alone, and Karl Popper concluded that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."[43] Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.[44][45]An alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.The opinions of mathematicians on this matter are varied. Many mathematicians[46] feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others[who?] feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). It is common to see universities divided into sections that include a division of Science and Mathematics, indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.[citation needed]Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.[47]Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what Eugene Wigner has called "the unreasonable effectiveness of mathematics".[10] As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages.[48] Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G. H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic.[49] Mathematicians often strive to find proofs that are particularly elegant, proofs from "The Book" of God according to Paul Erdős.[50][51] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.Most of the mathematical notation in use today was not invented until the 16th century.[52] Before that, mathematics was written out in words, limiting mathematical discovery.[53] Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more abstract and more encrypted than those of natural language.[54] Unlike natural language, where people can often equate a word (such as cow) with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog.[55] Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.[56]Mathematical language can be difficult to understand for beginners because even common terms, such as or and only, have a more precise meaning than they have in everyday speech, and other terms such as open and field refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as homeomorphism and integrable that have no meaning outside of mathematics. Additionally, shorthand phrases such as iff for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject.[b] The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.[57]Axioms in traditional thought were "self-evident truths", but that conception is problematic.[58] At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.[59]Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase "crisis of foundations" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930.[60] Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proved are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science,[citation needed] as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the "P = NP?" problem, one of the Millennium Prize Problems.[61] Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.The study of quantity starts with numbers, first the familiar natural numbers and integers ("whole numbers") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.As the number system is further developed, the integers are recognized as a subset of the rational numbers ("fractions"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of "infinity". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.The study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments;[62] the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.[c]Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[63] Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.[64]Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.Arguably the most prestigious award in mathematics is the Fields Medal,[65][66] established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a $1 million reward.
Translation of axes
In mathematics, a translation of axes in two dimensions is a mapping from an xy-Cartesian coordinate system to an x'y'-Cartesian coordinate system in which the x' axis is parallel to the x axis and k units away, and the y' axis is parallel to the y axis and h units away.  This means that the origin O' of the new coordinate system has coordinates (h, k) in the original system.  The positive x' and y' directions are taken to be the same as the positive x and y directions.  A point P has coordinates (x, y) with respect to the original system and coordinates (x', y') with respect to the new system, where    (1)or equivalently    (2)In the new coordinate system, the point P will appear to have been translated in the opposite direction.  For example, if the xy-system is translated a distance h to the right and a distance k upward, then P will appear to have been translated a distance h to the left and a distance k downward in the x'y'-system .  A translation of axes in more than two dimensions is defined similarly.[3]   A translation of axes is a rigid transformation, but not a linear map.  (See Affine transformation.)Coordinate systems are essential for studying the equations of curves using the methods of analytic geometry.  To use the method of coordinate geometry, the axes are placed at a convenient position with respect to the curve under consideration.  For example, to study the equations of ellipses and hyperbolas, the foci are usually located on one of the axes and are situated symmetrically with respect to the origin.  If the curve (hyperbola, parabola, ellipse, etc.) is not situated conveniently with respect to the axes, the coordinate system should be changed to place the curve at a convenient and familiar location and orientation.  The process of making this change is called a transformation of coordinates.[4]The solutions to many problems can be simplified by translating the coordinate axes to obtain new axes parallel to the original ones.[5]Through a change of coordinates, the equation of a conic section can be put into a standard form, which is usually easier to work with.   For the most general equation of the second degree, it is always possible to perform a rotation of axes in such a way that in the new system the equation takes the form    (3)that is, there is no xy term.[6]  Next, a translation of axes can reduce an equation of the form (3) to an equation of the same form but with new variables (x', y') as coordinates, and with D and E both equal to zero (with certain exceptions—for example, parabolas).  The principal tool in this process is "completing the square."[7]  In the examples that follow, it is assumed that a rotation of axes has already been performed.Given the equationby using a translation of axes, determine whether the locus of the equation is a parabola, ellipse, or hyperbola.  Determine foci (or focus), vertices (or vertex), and eccentricity.Solution:  To complete the square in x and y, write the equation in the formComplete the squares and obtainDefine    (4)Divide equation (4) by 225 to obtainFor an xyz-Cartesian coordinate system in three dimensions, suppose that a second Cartesian coordinate system is introduced, with axes x', y' and z' so located that the x' axis is parallel to the x axis and h units from it, the y' axis is parallel to the y axis and k units from it, and the z' axis is parallel to the z axis and l units from it.  A point P in space will have coordinates in both systems.  If its coordinates are (x, y, z) in the original system and (x', y', z') in the second system, the equations    (5)hold.[9]  Equations (5) define a translation of axes in three dimensions where (h, k, l) are the xyz-coordinates of the new origin.[10]  A translation of axes in any finite number of dimensions is defined similarly.In three-space, the most general equation of the second degree in x, y and z has the form    (6)As in the case of plane analytic geometry, the method of translation of axes may be used to simplify second-degree equations, thereby making evident the nature of certain quadric surfaces.  The principal tool in this process is "completing the square."[12]Use a translation of coordinates to identify the quadric surfaceSolution:  Write the equation in the formComplete the square to obtainIntroduce the translation of coordinatesThe equation of the surface takes the formwhich is recognizable as the equation of an ellipsoid.[13]
Skew-Hamiltonian matrix
In linear algebra, skew-Hamiltonian matrices are special matrices which correspond to skew-symmetric bilinear forms on a symplectic vector space.The square of a Hamiltonian matrix is skew-Hamiltonian. The converse is also true: every skew-Hamiltonian matrix can be obtained as the square of a Hamiltonian matrix.[1][2]
Free module
In mathematics, a free module is a module that has a basis – that is, a generating set consisting of linearly independent elements. Every vector space is a free module,[1] but, if the ring of the coefficients is not a division ring (not a field in the commutative case), then there exist non-free modules.Given any set S and ring R, there is a free R-module with basis S, which is called free module on S or module of formal linear combinations of the elements of S.A free abelian group is precisely a free module over the ring Z of integers.A free module is a module with a basis.[2]An immediate consequence of the second half of the definition is that the coefficients in the first half are unique for each element of M.Let R be a ring.Given a set E and ring R, there is a free R-module that has E as a basis: namely, the direct sum of copies of R indexed by EA similar argument shows that every free left (resp. right) R-module is isomorphic to a direct sum of copies of R as left (resp. right) module.The free module R(E) may also be constructed in the following equivalent way.Given a ring R and a set E, first as a set we letWe equip it with a structure of a left module such that the addition is defined by: for x in E,and the scalar multiplication by: for r in R and x in E,Many statements about free modules, which are wrong for general modules over rings, are still true for certain generalisations of free modules. Projective modules are direct summands of free modules, so one can choose an injection in a free module and use the basis of this one to prove something for the projective module. Even weaker generalisations are flat modules, which still have the property that tensoring with them preserves exact sequences, and torsion-free modules. If the ring has special properties, this hierarchy may collapse, e.g., for any perfect local Dedekind ring, every torsion-free module is flat, projective and free as well. A finitely generated torsion-free module of a commutative PID is free. A finitely generated Z-module is free if and only if it is flat.See local ring, perfect ring and Dedekind ring.This article incorporates material from free vector space over a set on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
Flat (geometry)
In geometry, a flat is a subset of a Euclidean space that is congruent to a Euclidean space of lower dimension.  The flats in two-dimensional space are points and lines, and the flats in three-dimensional space are points, lines, and planes.In a n-dimensional space, there are flats of every dimension from 0 to n − 1.[1] Flats of dimension n − 1 are called hyperplanes.Flats are the affine subspaces of Euclidean spaces, which means that they are similar to linear subspaces, except that they need not pass through the origin. Flats occurs in linear algebra, as geometric realizations of solution sets of systems of linear equations.A flat is manifold and an algebraic variety, and is sometimes called linear manifold or linear variety for distinguishing it from other manifolds or varieties.A flat can be described by a system of linear equations.  For example, a line in two-dimensional space can be described by a single linear equation involving x and y:In three-dimensional space, a single linear equation involving x, y, and z defines a plane, while a pair of linear equations can be used to describe a line.  In general, a linear equation in n variables describes a hyperplane, and a system of linear equations describes the intersection of those hyperplanes.  Assuming the equations are consistent and linearly independent, a system of k equations describes a flat of dimension n − k.A flat can also be described by a system of linear parametric equations.  A line can be described by equations involving one parameter:while the description of a plane would require two parameters:In general, a parameterization of a flat of dimension k would require parameters t1, … , tk.An intersection of flats is either a flat or the empty set.[2]If every line from the first flat is parallel to some line from the second flat, then these flats are parallel. Two parallel flats of the same dimension either coincide or do not intersect; they can be described by two systems of linear equations which differ only in their right-hand sides.If flats do not intersect, and no line from the first flat is parallel to a line from the second flat, then these are skew flats. It is possible only if sum of their dimensions is less than dimension of the ambient space.For two flats of dimensions k1 and k2 there exists the minimal flat which contains them, of dimension at most k1 + k2 + 1. If two flats intersect, then the dimension of the containing flat equals to k1 + k2 minus the dimension of the intersection.These two operations (referred to as meet and join) make the set of all flats in the Euclidean n-space a lattice and can build systematic coordinates for flats in any dimension, leading to Grassmann coordinates or dual Grassmann coordinates. For example, a line in three-dimensional space is determined by two distinct points or by two distinct planes.However, the lattice of all flats is not a distributive lattice.If two lines ℓ1 and ℓ2 intersect, then ℓ1 ∩ ℓ2 is a point. If p is a point not lying on the same plane, then (ℓ1 ∩ ℓ2) + p = (ℓ1 + p) ∩ (ℓ2 + p), both representing a line. But when ℓ1 and ℓ2 are parallel, this distributivity fails, giving p on the left-hand side and a third parallel line on the right-hand side.The aforementioned facts do not depend on the structure being that of Euclidean space (namely, involving  Euclidean distance) and are correct in any affine space. In a Euclidean space:
Kernel (linear algebra)
In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L : V → W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W.  That is, in set-builder notation,The kernel of L is a linear subspace of the domain V.[1]In the linear map L : V → W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:It follows that the image of L is isomorphic to the quotient of V by the kernel:This implies the rank–nullity theorem:where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L).  This is the generalization to linear operators of the row space, or coimage, of a matrix.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring.The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L: V → W is continuous if and only if the kernel of L is a closed subspace of V.Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K.The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,The matrix equation is equivalent to a homogeneous system of linear equations:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The kernel of an m × n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:The product Ax can be written in terms of the dot product of vectors as follows:Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank–nullity theoremThe left null space, or cokernel, of a matrix A consists of all vectors x such that xTA = 0T, where T denotes the transpose of a column vector.  The left null space of A is the same as the kernel of AT.  The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation.  The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The kernel also plays a role in the solution to a nonhomogeneous system of linear equations:If u and v are two possible solutions to the above equation, thenThus, the difference of any two solutions to the equation Ax = b lies in the kernel of A.It follows that any solution to the equation Ax = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel.  That is, the solution set to the equation Ax = b isGeometrically, this says that the solution set to Ax = b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).We give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Consider the matrixThe kernel of this matrix consists of all vectors (x, y, z) ∈ R3 for whichwhich can be expressed as a homogeneous system of linear equations involving x, y, and z:which can be written in matrix form as:Gauss–Jordan elimination reduces this to:Rewriting yields:Now we can express an element of the kernel:for c a scalar.Since c is a free variable, this can be expressed equally well as,The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (−1,−26,16)T constitutes a basis of the kernel of A.Thus, the nullity of A is 1.Note also that the following dot products are zero:which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (−1,−26,16)T.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.A basis of the kernel of a matrix may be computed by Gaussian elimination.In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.For example, suppose thatThenPutting the upper part in column echelon form by column operations on the whole matrix givesThe last three columns of B are zero columns. Therefore, the three last vectors of C,are a basis of the kernel of A.The problem of computing the kernel on a computer depends on the nature of the coefficients.If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic and Chinese remainder theorem, which reduces the problem to several similar ones over finite fields (this avoids the overhead induced by the non-linearity of the computational complexity of integer multiplication).[citation needed]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]
Kernel (algebra)
In the various branches of mathematics that fall under the heading of abstract algebra, the kernel of a homomorphism measures the degree to which the homomorphism fails to be injective.[1] An important special case is the kernel of a linear map. The kernel of a matrix, also called the null space, is the kernel of the linear map defined by the matrix.The definition of kernel takes various forms in various contexts. But in all of them, the kernel of a homomorphism is trivial (in a sense relevant to that context) if and only if the homomorphism is injective. The fundamental theorem on homomorphisms (or first isomorphism theorem) is a theorem, again taking various forms, that applies to the quotient algebra defined by the kernel.In this article, we first survey kernels for some important types of algebraic structures; then we give general definitions from universal algebra for generic algebraic structures.Let V and W be vector spaces over a field (or more generally, modules over a ring) and let T be a linear map from V to W. If 0W is the zero vector of W, then the kernel of T is the preimage of the zero subspace {0W}; that is, the subset of V consisting of all those elements of V that are mapped by T to the element 0W. The kernel is usually denoted as ker T, or some variation thereof:Since a linear map preserves zero vectors, the zero vector 0V of V must belong to the kernel. The transformation T is injective if and only if its kernel is reduced to the zero subspace.The kernel ker T is always a linear subspace of V. Thus, it makes sense to speak of the quotient space V/(ker T). The first isomorphism theorem for vector spaces states that this quotient space is naturally isomorphic to the image of T (which is a subspace of W). As a consequence, the dimension of V equals the dimension of the kernel plus the dimension of the image.If V and W are finite-dimensional and bases have been chosen, then T can be described by a matrix M, and the kernel can be computed by solving the homogeneous system of linear equations Mv = 0. In this case, the kernel of T may be identified to the kernel of the matrix M, also called "null space" of M. The dimension of the null space, called the nullity of M, is given by the number of columns of M minus the rank of M, as a consequence of the rank–nullity theorem.Solving homogeneous differential equations often amounts to computing the kernel of certain differential operators.For instance, in order to find all twice-differentiable functions f from the real line to itself such thatlet V be the space of all twice differentiable functions, let W be the space of all functions, and define a linear operator T from V to W byfor f in V and x an arbitrary real number.Then all solutions to the differential equation are in ker T.One can define kernels for homomorphisms between modules over a ring in an analogous manner. This includes kernels for homomorphisms between abelian groups as a special case. This example captures the essence of kernels in general abelian categories; see Kernel (category theory).Let G and H be groups and let f be a group homomorphism from G to H.If eH is the identity element of H, then the kernel of f is the preimage of the singleton set {eH}; that is, the subset of G consisting of all those elements of G that are mapped by f to the element eH.The kernel is usually denoted ker f (or a variation).In symbols:It turns out that ker f is not only a subgroup of G but in fact a normal subgroup.Thus, it makes sense to speak of the quotient group G/(ker f).The first isomorphism theorem for groups states that this quotient group is naturally isomorphic to the image of f (which is a subgroup of H).In the special case of abelian groups, this works in exactly the same way as in the previous section.Let R and S be rings (assumed unital) and let f be a ring homomorphism from R to S.If 0S is the zero element of S, then the kernel of f is its kernel as linear map over the integers, or, equivalently, as additive groups. It is the preimage of the zero ideal {0S}, which is, the subset of R consisting of all those elements of R that are mapped by f to the element 0S.The kernel is usually denoted ker f (or a variation).In symbols:Since a ring homomorphism preserves zero elements, the zero element 0R of R must belong to the kernel.The homomorphism f is injective if and only if its kernel is only the singleton set {0R}.This is always the case if R is a field, and S is not the zero ring.Since ker f contains the multiplicative identity only when S is the zero ring, it turns out that the kernel is generally not a subring of R. The kernel is a subrng, and, more precisely, a two-sided ideal of R.Thus, it makes sense to speak of the quotient ring R/(ker f).The first isomorphism theorem for rings states that this quotient ring is naturally isomorphic to the image of f (which is a subring of S). (note that rings need not be unital for the kernel definition).To some extent, this can be thought of as a special case of the situation for modules, since these are all bimodules over a ring R:However, the isomorphism theorem gives a stronger result, because ring isomorphisms preserve multiplication while module isomorphisms (even between rings) in general do not.This example captures the essence of kernels in general Mal'cev algebras.Let M and N be monoids and let f be a monoid homomorphism from M to N.Then the kernel of f is the subset of the direct product M × M consisting of all those ordered pairs of elements of M whose components are both mapped by f to the same element in N.The kernel is usually denoted ker f (or a variation).In symbols:Since f is a function, the elements of the form (m,m) must belong to the kernel.The homomorphism f is injective if and only if its kernel is only the diagonal set {(m,m) : m in M}.It turns out that ker f is an equivalence relation on M, and in fact a congruence relation.Thus, it makes sense to speak of the quotient monoid M/(ker f).The first isomorphism theorem for monoids states that this quotient monoid is naturally isomorphic to the image of f (which is a submonoid of N),(for the congruence relation).This is very different in flavour from the above examples.In particular, the preimage of the identity element of N is not enough to determine the kernel of f.All the above cases may be unified and generalized in universal algebra.Let A and B be algebraic structures of a given type and let f be a homomorphism of that type from A to B.Then the kernel of f is the subset of the direct product A × A consisting of all those ordered pairs of elements of A whose components are both mapped by f to the same element in B.The kernel is usually denoted ker f (or a variation).In symbols:Since f is a function, the elements of the form (a,a) must belong to the kernel.The homomorphism f is injective if and only if its kernel is exactly the diagonal set {(a,a) : a∈A}.It is easy to see that ker f is an equivalence relation on A, and in fact a congruence relation.Thus, it makes sense to speak of the quotient algebra A/(ker f).The first isomorphism theorem in general universal algebra states that this quotient algebra is naturally isomorphic to the image of f (which is a subalgebra of B).Note that the definition of kernel here (as in the monoid example) doesn't depend on the algebraic structure; it is a purely set-theoretic concept.For more on this general concept, outside of abstract algebra, see kernel of a function.In the case of Mal'cev algebras, this construction can be simplified. Every Mal'cev algebra has a special neutral element (the zero vector in the case of vector spaces, the identity element in the case of commutative groups, and the zero element in the case of rings or modules). The characteristic feature of a Mal'cev algebra is that we can recover the entire equivalence relation ker f from the equivalence class of the neutral element.To be specific, let A and B be Mal'cev algebraic structures of a given type and let f be a homomorphism of that type from A to B. If eB is the neutral element of B, then the kernel of f is the preimage of the singleton set {eB}; that is, the subset of A consisting of all those elements of A that are mapped by f to the element eB.The kernel is usually denoted ker f (or a variation). In symbols:Since a Mal'cev algebra homomorphism preserves neutral elements, the identity element eA of A must belong to the kernel. The homomorphism f is injective if and only if its kernel is only the singleton set {eA}.The notion of ideal generalises to any Mal'cev algebra (as linear subspace in the case of vector spaces, normal subgroup in the case of groups, two-sided ideals in the case of rings, and submodule in the case of modules). It turns out that ker f is not a subalgebra of A, but it is an ideal.Then it makes sense to speak of the quotient algebra G/(ker f).The first isomorphism theorem for Mal'cev algebras states that this quotient algebra is naturally isomorphic to the image of f (which is a subalgebra of B).The connection between this and the congruence relation for more general types of algebras is as follows.First, the kernel-as-an-ideal is the equivalence class of the neutral element eA under the kernel-as-a-congruence. For the converse direction, we need the notion of quotient in the Mal'cev algebra (which is division on either side for groups and subtraction for vector spaces, modules, and rings).Using this, elements a and b of A are equivalent under the kernel-as-a-congruence if and only if their quotient a/b is an element of the kernel-as-an-ideal.Sometimes algebras are equipped with a nonalgebraic structure in addition to their algebraic operations.For example, one may consider topological groups or topological vector spaces, with are equipped with a topology.In this case, we would expect the homomorphism f to preserve this additional structure; in the topological examples, we would want f to be a continuous map.The process may run into a snag with the quotient algebras, which may not be well-behaved.In the topological examples, we can avoid problems by requiring that topological algebraic structures be Hausdorff (as is usually done); then the kernel (however it is constructed) will be a closed set and the quotient space will work fine (and also be Hausdorff).The notion of kernel in category theory is a generalisation of the kernels of abelian algebras; see Kernel (category theory).The categorical generalisation of the kernel as a congruence relation is the kernel pair.(There is also the notion of difference kernel, or binary equaliser.)
Hilbert space
The mathematical concept of a Hilbert space, named after David Hilbert, generalizes the notion of Euclidean space. It extends the methods of vector algebra and calculus from the two-dimensional Euclidean plane and three-dimensional space to spaces with any finite or infinite number of dimensions. A Hilbert space is an abstract vector space possessing the structure of an inner product that allows length and angle to be measured. Furthermore, Hilbert spaces are complete: there are enough limits in the space to allow the techniques of calculus to be used.Hilbert spaces arise naturally and frequently in mathematics and physics, typically as infinite-dimensional function spaces. The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by David Hilbert, Erhard Schmidt, and Frigyes Riesz. They are indispensable tools in the theories of partial differential equations, quantum mechanics, Fourier analysis (which includes applications to signal processing and heat transfer), and ergodic theory (which forms the mathematical underpinning of thermodynamics). John von Neumann coined the term Hilbert space for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for functional analysis. Apart from the classical Euclidean spaces, examples of Hilbert spaces include spaces of square-integrable functions, spaces of sequences, Sobolev spaces consisting of generalized functions, and Hardy spaces of holomorphic functions.Geometric intuition plays an important role in many aspects of Hilbert space theory. Exact analogs of the Pythagorean theorem and parallelogram law hold in a Hilbert space. At a deeper level, perpendicular projection onto a subspace (the analog of "dropping the altitude" of a triangle) plays a significant role in optimization problems and other aspects of the theory. An element of a Hilbert space can be uniquely specified by its coordinates with respect to a set of coordinate axes (an orthonormal basis), in analogy with Cartesian coordinates in the plane. When that set of axes is countably infinite, the Hilbert space can also be usefully thought of in terms of the space of infinite sequences that are square-summable. The latter space is often in the older literature referred to as the Hilbert space. Linear operators on a Hilbert space are likewise fairly concrete objects: in good cases, they are simply transformations that stretch the space by different factors in mutually perpendicular directions in a sense that is made precise by the study of their spectrum.One of the most familiar examples of a Hilbert space is the Euclidean space consisting of three-dimensional vectors, denoted by ℝ3, and equipped with the dot product. The dot product takes two vectors x and y, and produces a real number x · y. If x and y are represented in Cartesian coordinates, then the dot product is defined byThe dot product satisfies the properties:An operation on pairs of vectors that, like the dot product, satisfies these three properties is known as a (real) inner product. A vector space equipped with such an inner product is known as a (real) inner product space. Every finite-dimensional inner product space is also a Hilbert space. The basic feature of the dot product that connects it with Euclidean geometry is that it is related to both the length (or norm) of a vector, denoted ||x||, and to the angle θ between two vectors x and y by means of the formulaMultivariable calculus in Euclidean space relies on the ability to compute limits, and to have useful criteria for concluding that limits exist. A mathematical seriesconsisting of vectors in ℝ3 is absolutely convergent provided that the sum of the lengths converges as an ordinary series of real numbers:[1]Just as with a series of scalars, a series of vectors that converges absolutely also converges to some limit vector L in the Euclidean space, in the sense thatThis property expresses the completeness of Euclidean space: that a series that converges absolutely also converges in the ordinary sense.Hilbert spaces are often taken over the complex numbers. The complex plane denoted by ℂ is equipped with a notion of magnitude, the complex modulus |z| which is defined as the square root of the product of z with its complex conjugate:If z = x + iy is a decomposition of z into its real and imaginary parts, then the modulus is the usual Euclidean two-dimensional length:The inner product of a pair of complex numbers z and w is the product of z with the complex conjugate of w:This is complex-valued. The real part of ⟨z,w⟩ gives the usual two-dimensional Euclidean dot product.A second example is the space ℂ2 whose elements are pairs of complex numbers z = (z1, z2). Then the inner product of z with another such vector w = (w1,w2) is given byThe real part of ⟨z,w⟩ is then the four-dimensional Euclidean dot product. This inner product is Hermitian symmetric, which means that the result of interchanging z and w is the complex conjugate:A Hilbert space H is a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product.[2]To say that H is a complex inner product space means that H is a complex vector space on which there is an inner product ⟨x,y⟩ associating a complex number to each pair of elements x, y of H that satisfies the following properties:It follows from properties 1 and 2 that a complex inner product is antilinear in its second argument, meaning thatA real inner product space is defined in the same way, except that H is a real vector space and the inner product takes real values. Such an inner product will be bilinear: that is, linear in each argument.The norm is the real-valued functionand the distance d between two points x, y in H is defined in terms of the norm byThat this function is a distance function means firstly that it is symmetric in x and y, secondly that the distance between x and itself is zero, and otherwise the distance between x and y must be positive, and lastly that the triangle inequality holds, meaning that the length of one leg of a triangle xyz cannot exceed the sum of the lengths of the other two legs:This last property is ultimately a consequence of the more fundamental Cauchy–Schwarz inequality, which assertswith equality if and only if x and y are linearly dependent.With a distance function defined in this way, any inner product space is a metric space, and sometimes is known as a pre-Hilbert space.[3] Any pre-Hilbert space that is additionally also a complete space is a Hilbert space.The Completeness of H is expressed using a form of the Cauchy criterion for sequences in H: a pre-Hilbert space H is complete if every Cauchy sequence converges with respect to this norm to an element in the space. Completeness can be characterized by the following equivalent condition: if a series of vectorsconverges absolutely in the sense thatthen the series converges in H, in the sense that the partial sums converge to an element of H.As a complete normed space, Hilbert spaces are by definition also Banach spaces. As such they are topological vector spaces, in which topological notions like the openness and closedness of subsets are well defined. Of special importance is the notion of a closed linear subspace of a Hilbert space that, with the inner product induced by restriction, is also complete (being a closed set in a complete metric space) and therefore a Hilbert space in its own right.The sequence space l2 consists of all infinite sequences z = (z1,z2,...) of complex numbers such that the seriesconverges. The inner product on l2 is defined bywith the latter series converging as a consequence of the Cauchy–Schwarz inequality.Completeness of the space holds provided that whenever a series of elements from l2 converges absolutely (in norm), then it converges to an element of l2. The proof is basic in mathematical analysis, and permits mathematical series of elements of the space to be manipulated with the same ease as series of complex numbers (or vectors in a finite-dimensional Euclidean space).[4]Prior to the development of Hilbert spaces, other generalizations of Euclidean spaces were known to mathematicians and physicists. In particular, the idea of an abstract linear space had gained some traction towards the end of the 19th century:[5] this is a space whose elements can be added together and multiplied by scalars (such as real or complex numbers) without necessarily identifying these elements with "geometric" vectors, such as position and momentum vectors in physical systems. Other objects studied by mathematicians at the turn of the 20th century, in particular spaces of sequences (including series) and spaces of functions,[6] can naturally be thought of as linear spaces. Functions, for instance, can be added together or multiplied by constant scalars, and these operations obey the algebraic laws satisfied by addition and scalar multiplication of spatial vectors.In the first decade of the 20th century, parallel developments led to the introduction of Hilbert spaces. The first of these was the observation, which arose during David Hilbert and Erhard Schmidt's study of integral equations,[7] that two square-integrable real-valued functions f and g on an interval [a,b] have an inner productwhich has many of the familiar properties of the Euclidean dot product. In particular, the idea of an orthogonal family of functions has meaning. Schmidt exploited the similarity of this inner product with the usual dot product to prove an analog of the spectral decomposition for an operator of the formwhere K is a continuous function symmetric in x and y. The resulting eigenfunction expansion expresses the function K as a series of the formwhere the functions φn are orthogonal in the sense that ⟨φn,φm⟩ = 0 for all n ≠ m. The individual terms in this series are sometimes referred to as elementary product solutions. However, there are eigenfunction expansions that fail to converge in a suitable sense to a square-integrable function: the missing ingredient, which ensures convergence, is completeness.[8]The second development was the Lebesgue integral, an alternative to the Riemann integral introduced by Henri Lebesgue in 1904.[9] The Lebesgue integral made it possible to integrate a much broader class of functions. In 1907, Frigyes Riesz and Ernst Sigismund Fischer independently proved that the space L2 of square Lebesgue-integrable functions is a complete metric space.[10] As a consequence of the interplay between geometry and completeness, the 19th century results of Joseph Fourier, Friedrich Bessel and Marc-Antoine Parseval on trigonometric series easily carried over to these more general spaces, resulting in a geometrical and analytical apparatus now usually known as the Riesz–Fischer theorem.[11]Further basic results were proved in the early 20th century. For example, the Riesz representation theorem was independently established by Maurice Fréchet and Frigyes Riesz in 1907.[12] John von Neumann coined the term abstract Hilbert space in his work on unbounded Hermitian operators.[13] Although other mathematicians such as Hermann Weyl and Norbert Wiener had already studied particular Hilbert spaces in great detail, often from a physically motivated point of view, von Neumann gave the first complete and axiomatic treatment of them.[14] Von Neumann later used them in his seminal work on the foundations of quantum mechanics,[15] and in his continued work with Eugene Wigner. The name "Hilbert space" was soon adopted by others, for example by Hermann Weyl in his book on quantum mechanics and the theory of groups.[16]The significance of the concept of a Hilbert space was underlined with the realization that it offers one of the best mathematical formulations of quantum mechanics.[17] In short, the states of a quantum mechanical system are vectors in a certain Hilbert space, the observables are hermitian operators on that space, the symmetries of the system are unitary operators, and measurements are orthogonal projections. The relation between quantum mechanical symmetries and unitary operators provided an impetus for the development of the unitary representation theory of groups, initiated in the 1928 work of Hermann Weyl.[16] On the other hand, in the early 1930s it became clear that classical mechanics can be described in terms of Hilbert space (Koopman–von Neumann classical mechanics) and that certain properties of classical dynamical systems can be analyzed using Hilbert space techniques in the framework of ergodic theory.[18]The algebra of observables in quantum mechanics is naturally an algebra of operators defined on a Hilbert space, according to Werner Heisenberg's matrix mechanics formulation of quantum theory. Von Neumann began investigating operator algebras in the 1930s, as rings of operators on a Hilbert space. The kind of algebras studied by von Neumann and his contemporaries are now known as von Neumann algebras. In the 1940s, Israel Gelfand, Mark Naimark and Irving Segal gave a definition of a kind of operator algebras called C*-algebras that on the one hand made no reference to an underlying Hilbert space, and on the other extrapolated many of the useful features of the operator algebras that had previously been studied. The spectral theorem for self-adjoint operators in particular that underlies much of the existing Hilbert space theory was generalized to C*-algebras. These techniques are now basic in abstract harmonic analysis and representation theory.Lebesgue spaces are function spaces associated to measure spaces (X, M, μ), where X is a set, M is a σ-algebra of subsets of X, and μ is a countably additive measure on M. Let L2(X, μ) be the space of those complex-valued measurable functions on X for which the Lebesgue integral of the square of the absolute value of the function is finite, i.e., for a function f in L2(X, μ),and where functions are identified if and only if they differ only on a set of measure zero.The inner product of functions f and g in L2(X, μ) is then defined asFor f and g in L2, this integral exists because of the Cauchy–Schwarz inequality, and defines an inner product on the space. Equipped with this inner product, L2 is in fact complete.[19] The Lebesgue integral is essential to ensure completeness: on domains of real numbers, for instance, not enough functions are Riemann integrable.[20]The Lebesgue spaces appear in many natural settings. The spaces L2(ℝ) and L2([0,1]) of square-integrable functions with respect to the Lebesgue measure on the real line and unit interval, respectively, are natural domains on which to define the Fourier transform and Fourier series. In other situations, the measure may be something other than the ordinary Lebesgue measure on the real line. For instance, if w is any positive measurable function, the space of all measurable functions f on the interval [0,1] satisfyingis called the weighted L2 space L2w([0,1]), and w is called the weight function. The inner product is defined byThe weighted space L2w([0,1]) is identical with the Hilbert space L2([0,1],μ) where the measure μ of a Lebesgue-measurable set A is defined byWeighted L2 spaces like this are frequently used to study orthogonal polynomials, because different families of orthogonal polynomials are orthogonal with respect to different weighting functions.Sobolev spaces, denoted by Hs or Ws,2, are Hilbert spaces. These are a special kind of function space in which differentiation may be performed, but that (unlike other Banach spaces such as the Hölder spaces) support the structure of an inner product. Because differentiation is permitted, Sobolev spaces are a convenient setting for the theory of partial differential equations.[21] They also form the basis of the theory of direct methods in the calculus of variations.[22]For s a non-negative integer and Ω ⊂ ℝn, the Sobolev space Hs(Ω) contains L2 functions whose weak derivatives of order up to s are also L2. The inner product in Hs(Ω) iswhere the dot indicates the dot product in the Euclidean space of partial derivatives of each order. Sobolev spaces can also be defined when s is not an integer.Sobolev spaces are also studied from the point of view of spectral theory, relying more specifically on the Hilbert space structure. If Ω is a suitable domain, then one can define the Sobolev space Hs(Ω) as the space of Bessel potentials;[23] roughly,Here Δ is the Laplacian and (1 − Δ)−s/2 is understood in terms of the spectral mapping theorem. Apart from providing a workable definition of Sobolev spaces for non-integer s, this definition also has particularly desirable properties under the Fourier transform that make it ideal for the study of pseudodifferential operators. Using these methods on a compact Riemannian manifold, one can obtain for instance the Hodge decomposition, which is the basis of Hodge theory.[24]The Hardy spaces are function spaces, arising in complex analysis and harmonic analysis, whose elements are certain holomorphic functions in a complex domain.[25] Let U denote the unit disc in the complex plane. Then the Hardy space H2(U) is defined as the space of holomorphic functions f on U such that the meansremain bounded for r < 1. The norm on this Hardy space is defined byHardy spaces in the disc are related to Fourier series. A function f is in H2(U) if and only ifwhereThus H2(U) consists of those functions that are L2 on the circle, and whose negative frequency Fourier coefficients vanish.The Bergman spaces are another family of Hilbert spaces of holomorphic functions.[26] Let D be a bounded open set in the complex plane (or a higher-dimensional complex space) and let L2,h(D) be the space of holomorphic functions f in D that are also in L2(D) in the sense thatwhere the integral is taken with respect to the Lebesgue measure in D. Clearly L2,h(D) is a subspace of L2(D); in fact, it is a closed subspace, and so a Hilbert space in its own right. This is a consequence of the estimate, valid on compact subsets K of D, thatwhich in turn follows from Cauchy's integral formula. Thus convergence of a sequence of holomorphic functions in L2(D) implies also compact convergence, and so the limit function is also holomorphic. Another consequence of this inequality is that the linear functional that evaluates a function f at a point of D is actually continuous on L2,h(D). The Riesz representation theorem implies that the evaluation functional can be represented as an element of L2,h(D). Thus, for every z ∈ D, there is a function ηz ∈ L2,h(D) such thatfor all f ∈ L2,h(D). The integrandis known as the Bergman kernel of D. This integral kernel satisfies a reproducing propertyA Bergman space is an example of a reproducing kernel Hilbert space, which is a Hilbert space of functions along with a kernel K(ζ,z) that verifies a reproducing property analogous to this one. The Hardy space H2(D) also admits a reproducing kernel, known as the Szegő kernel.[27] Reproducing kernels are common in other areas of mathematics as well. For instance, in harmonic analysis the Poisson kernel is a reproducing kernel for the Hilbert space of square-integrable harmonic functions in the unit ball. That the latter is a Hilbert space at all is a consequence of the mean value theorem for harmonic functions.Many of the applications of Hilbert spaces exploit the fact that Hilbert spaces support generalizations of simple geometric concepts like projection and change of basis from their usual finite dimensional setting. In particular, the spectral theory of continuous self-adjoint linear operators on a Hilbert space generalizes the usual spectral decomposition of a matrix, and this often plays a major role in applications of the theory to other areas of mathematics and physics.In the theory of ordinary differential equations, spectral methods on a suitable Hilbert space are used to study the behavior of eigenvalues and eigenfunctions of differential equations. For example, the Sturm–Liouville problem arises in the study of the harmonics of waves in a violin string or a drum, and is a central problem in ordinary differential equations.[28] The problem is a differential equation of the formfor an unknown function y on an interval [a,b], satisfying general homogeneous Robin boundary conditionsThe functions p, q, and w are given in advance, and the problem is to find the function y and constants λ for which the equation has a solution. The problem only has solutions for certain values of λ, called eigenvalues of the system, and this is a consequence of the spectral theorem for compact operators applied to the integral operator defined by the Green's function for the system. Furthermore, another consequence of this general result is that the eigenvalues λ of the system can be arranged in an increasing sequence tending to infinity.[nb 2]Hilbert spaces form a basic tool in the study of partial differential equations.[21] For many classes of partial differential equations, such as linear elliptic equations, it is possible to consider a generalized solution (known as a weak solution) by enlarging the class of functions. Many weak formulations involve the class of Sobolev functions, which is a Hilbert space. A suitable weak formulation reduces to a geometrical problem the analytic problem of finding a solution or, often what is more important, showing that a solution exists and is unique for given boundary data. For linear elliptic equations, one geometrical result that ensures unique solvability for a large class of problems is the Lax–Milgram theorem. This strategy forms the rudiment of the Galerkin method (a finite element method) for numerical solution of partial differential equations.[29]A typical example is the Poisson equation −Δu = g with Dirichlet boundary conditions in a bounded domain Ω in ℝ2. The weak formulation consists of finding a function u such that, for all continuously differentiable functions v in Ω vanishing on the boundary:This can be recast in terms of the Hilbert space H10(Ω) consisting of functions u such that u, along with its weak partial derivatives, are square integrable on Ω, and vanish on the boundary. The question then reduces to finding u in this space such that for all v in this spacewhere a is a continuous bilinear form, and b is a continuous linear functional, given respectively bySince the Poisson equation is elliptic, it follows from Poincaré's inequality that the bilinear form a is coercive. The Lax–Milgram theorem then ensures the existence and uniqueness of solutions of this equation.Hilbert spaces allow for many elliptic partial differential equations to be formulated in a similar way, and the Lax–Milgram theorem is then a basic tool in their analysis. With suitable modifications, similar techniques can be applied to parabolic partial differential equations and certain hyperbolic partial differential equations.The field of ergodic theory is the study of the long-term behavior of chaotic dynamical systems. The protypical case of a field that ergodic theory applies to is thermodynamics, in which—though the microscopic state of a system is extremely complicated (it is impossible to understand the ensemble of individual collisions between particles of matter)—the average behavior over sufficiently long time intervals is tractable. The laws of thermodynamics are assertions about such average behavior. In particular, one formulation of the zeroth law of thermodynamics asserts that over sufficiently long timescales, the only functionally independent measurement that one can make of a thermodynamic system in equilibrium is its total energy, in the form of temperature.An ergodic dynamical system is one for which, apart from the energy—measured by the Hamiltonian—there are no other functionally independent conserved quantities on the phase space. More explicitly, suppose that the energy E is fixed, and let ΩE be the subset of the phase space consisting of all states of energy E (an energy surface), and let Tt denote the evolution operator on the phase space. The dynamical system is ergodic if there are no continuous non-constant functions on ΩE such thatfor all w on ΩE and all time t. Liouville's theorem implies that there exists a measure μ on the energy surface that is invariant under the time translation. As a result, time translation is a unitary transformation of the Hilbert space L2(ΩE,μ) consisting of square-integrable functions on the energy surface ΩE with respect to the inner productThe von Neumann mean ergodic theorem[18] states the following:For an ergodic system, the fixed set of the time evolution consists only of the constant functions, so the ergodic theorem implies the following:[30] for any function f ∈ L2(ΩE,μ),That is, the long time average of an observable f is equal to its expectation value over an energy surface.One of the basic goals of Fourier analysis is to decompose a function into a (possibly infinite) linear combination of given basis functions: the associated Fourier series. The classical Fourier series associated to a function f defined on the interval [0,1] is a series of the formwhereThe example of adding up the first few terms in a Fourier series for a sawtooth function is shown in the figure. The basis functions are sine waves with wavelengths λ/n (for integer n) shorter than the wavelength λ of the sawtooth itself (except for n = 1, the fundamental wave). All basis functions have nodes at the nodes of the sawtooth, but all but the fundamental have additional nodes. The oscillation of the summed terms about the sawtooth is called the Gibbs phenomenon.A significant problem in classical Fourier series asks in what sense the Fourier series converges, if at all, to the function f. Hilbert space methods provide one possible answer to this question.[31] The functions en(θ) = e2πinθ form an orthogonal basis of the Hilbert space L2([0,1]). Consequently, any square-integrable function can be expressed as a seriesand, moreover, this series converges in the Hilbert space sense (that is, in the L2 mean).The problem can also be studied from the abstract point of view: every Hilbert space has an orthonormal basis, and every element of the Hilbert space can be written in a unique way as a sum of multiples of these basis elements. The coefficients appearing on these basis elements are sometimes known abstractly as the Fourier coefficients of the element of the space.[32] The abstraction is especially useful when it is more natural to use different basis functions for a space such as L2([0,1]). In many circumstances, it is desirable not to decompose a function into trigonometric functions, but rather into orthogonal polynomials or wavelets for instance,[33] and in higher dimensions into spherical harmonics.[34]For instance, if en are any orthonormal basis functions of L2[0,1], then a given function in L2[0,1] can be approximated as a finite linear combination[35]The coefficients {aj} are selected to make the magnitude of the difference ||f − fn||2 as small as possible. Geometrically, the best approximation is the orthogonal projection of f onto the subspace consisting of all linear combinations of the {ej}, and can be calculated by[36]That this formula minimizes the difference ||f − fn||2 is a consequence of Bessel's inequality and Parseval's formula.In various applications to physical problems, a function can be decomposed into physically meaningful eigenfunctions of a differential operator (typically the Laplace operator): this forms the foundation for the spectral study of functions, in reference to the spectrum of the differential operator.[37] A concrete physical application involves the problem of hearing the shape of a drum: given the fundamental modes of vibration that a drumhead is capable of producing, can one infer the shape of the drum itself?[38] The mathematical formulation of this question involves the Dirichlet eigenvalues of the Laplace equation in the plane, that represent the fundamental modes of vibration in direct analogy with the integers that represent the fundamental modes of vibration of the violin string.Spectral theory also underlies certain aspects of the Fourier transform of a function. Whereas Fourier analysis decomposes a function defined on a compact set into the discrete spectrum of the Laplacian (which corresponds to the vibrations of a violin string or drum), the Fourier transform of a function is the decomposition of a function defined on all of Euclidean space into its components in the continuous spectrum of the Laplacian. The Fourier transformation is also geometrical, in a sense made precise by the Plancherel theorem, that asserts that it is an isometry of one Hilbert space (the "time domain") with another (the "frequency domain"). This isometry property of the Fourier transformation is a recurring theme in abstract harmonic analysis, as evidenced for instance by the Plancherel theorem for spherical functions occurring in noncommutative harmonic analysis.In the mathematically rigorous formulation of quantum mechanics, developed by John von Neumann,[39] the possible states (more precisely, the pure states) of a quantum mechanical system are represented by unit vectors (called state vectors) residing in a complex separable Hilbert space, known as the state space, well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projectivization of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system; for example, the position and momentum states for a single non-relativistic spin zero particle is the space of all square-integrable functions, while the states for the spin of a single proton are unit elements of the two-dimensional complex Hilbert space of spinors. Each observable is represented by a self-adjoint linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate.The inner product between two state vectors is a complex number known as a probability amplitude. During an ideal measurement of a quantum mechanical system, the probability that a system collapses from a given initial state to a particular eigenstate is given by the square of the absolute value of the probability amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator—which explains the choice of self-adjoint operators, for all the eigenvalues must be real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.For a general system, states are typically not pure, but instead are represented as statistical mixtures of pure states, or mixed states, given by density matrices: self-adjoint operators of trace one on a Hilbert space. Moreover, for general quantum mechanical systems, the effects of a single measurement can influence other parts of a system in a manner that is described instead by a positive operator valued measure. Thus the structure both of the states and observables in the general theory is considerably more complicated than the idealization for pure states.Any true physical color can be represented by a combination of pure spectral colors. As physical colors can be composed of any number of physical colors, the space of physical colors may aptly be represented by a Hilbert space over spectral colors. Humans have three types of cone cells for color perception, so the perceivable colors can be represented by 3-dimensional Euclidean space. The many-to-one linear mapping from the Hilbert space of physical colors to the Euclidean space of human perceivable colors explains why many distinct physical colors may be perceived by humans to be identical (e.g., pure yellow light versus a mix of red and green light, see metamerism).Two vectors u and v in a Hilbert space H are orthogonal when ⟨u,v⟩ = 0. The notation for this is u ⊥ v. More generally, when S is a subset in H, the notation u ⊥ S means that u is orthogonal to every element from S.When u and v are orthogonal, one hasBy induction on n, this is extended to any family u1, ..., un of n orthogonal vectors,Whereas the Pythagorean identity as stated is valid in any inner product space, completeness is required for the extension of the Pythagorean identity to series. A series ∑uk of orthogonal vectors converges in H if and only if the series of squares of norms converges, andFurthermore, the sum of a series of orthogonal vectors is independent of the order in which it is taken.By definition, every Hilbert space is also a Banach space. Furthermore, in every Hilbert space the following parallelogram identity holds:Conversely, every Banach space in which the parallelogram identity holds is a Hilbert space, and the inner product is uniquely determined by the norm by the polarization identity.[40] For real Hilbert spaces, the polarization identity isFor complex Hilbert spaces, it isThe parallelogram law implies that any Hilbert space is a uniformly convex Banach space.[41]This subsection employs the Hilbert projection theorem. If C is a non-empty closed convex subset of a Hilbert space H and x a point in H, there exists a unique point y ∈ C that minimizes the distance between x and points in C,[42]This is equivalent to saying that there is a point with minimal norm in the translated convex set D = C − x. The proof consists in showing that every minimizing sequence (dn) ⊂ D is Cauchy (using the parallelogram identity) hence converges (using completeness) to a point in D that has minimal norm. More generally, this holds in any uniformly convex Banach space.[43]When this result is applied to a closed subspace F of H, it can be shown that the point y ∈ F closest to x is characterized by[44]This point y is the orthogonal projection of x onto F, and the mapping PF : x → y is linear (see Orthogonal complements and projections). This result is especially significant in applied mathematics, especially numerical analysis, where it forms the basis of least squares methods.[45]In particular, when F is not equal to H, one can find a nonzero vector v orthogonal to F (select x ∉ F and v = x − y). A very useful criterion is obtained by applying this observation to the closed subspace F generated by a subset S of H.The dual space H* is the space of all continuous linear functions from the space H into the base field. It carries a natural norm, defined byThis norm satisfies the parallelogram law, and so the dual space is also an inner product space. The dual space is also complete, and so it is a Hilbert space in its own right.The Riesz representation theorem affords a convenient description of the dual. To every element u of H, there is a unique element φu of H*, defined byThe mapping u ↦ φu is an antilinear mapping from H to H*. The Riesz representation theorem states that this mapping is an antilinear isomorphism.[46] Thus to every element φ of the dual H* there exists one and only one uφ in H such thatfor all x ∈ H. The inner product on the dual space H* satisfiesThe reversal of order on the right-hand side restores linearity in φ from the antilinearity of uφ. In the real case, the antilinear isomorphism from H to its dual is actually an isomorphism, and so real Hilbert spaces are naturally isomorphic to their own duals.The representing vector uφ is obtained in the following way. When φ ≠ 0, the kernel F = Ker(φ) is a closed vector subspace of H, not equal to H, hence there exists a nonzero vector v orthogonal to F. The vector u is a suitable scalar multiple λv of v. The requirement that φ(v) = ⟨v,u⟩ yieldsThis correspondence φ ↔ u is exploited by the bra–ket notation popular in physics. It is common in physics to assume that the inner product, denoted by ⟨x|y⟩, is linear on the right,The result ⟨x|y⟩ can be seen as the action of the linear functional ⟨x| (the bra) on the vector |y⟩ (the ket).The Riesz representation theorem relies fundamentally not just on the presence of an inner product, but also on the completeness of the space. In fact, the theorem implies that the topological dual of any inner product space can be identified with its completion. An immediate consequence of the Riesz representation theorem is also that a Hilbert space H is reflexive, meaning that the natural map from H into its double dual space is an isomorphism.In a Hilbert space H, a sequence {xn} is weakly convergent to a vector x ∈ H whenfor every v ∈ H.For example, any orthonormal sequence {fn} converges weakly to 0, as a consequence of Bessel's inequality. Every weakly convergent sequence {xn} is bounded, by the uniform boundedness principle.Conversely, every bounded sequence in a Hilbert space admits weakly convergent subsequences (Alaoglu's theorem).[47] This fact may be used to prove minimization results for continuous convex functionals, in the same way that the Bolzano–Weierstrass theorem is used for continuous functions on ℝd. Among several variants, one simple statement is as follows:[48]This fact (and its various generalizations) are fundamental for direct methods in the calculus of variations. Minimization results for convex functionals are also a direct consequence of the slightly more abstract fact that closed bounded convex subsets in a Hilbert space H are weakly compact, since H is reflexive. The existence of weakly convergent subsequences is a special case of the Eberlein–Šmulian theorem.Any general property of Banach spaces continues to hold for Hilbert spaces. The open mapping theorem states that a continuous surjective linear transformation from one Banach space to another is an open mapping meaning that it sends open sets to open sets. A corollary is the bounded inverse theorem, that a continuous and bijective linear function from one Banach space to another is an isomorphism (that is, a continuous linear map whose inverse is also continuous). This theorem is considerably simpler to prove in the case of Hilbert spaces than in general Banach spaces.[49] The open mapping theorem is equivalent to the closed graph theorem, which asserts that a function from one Banach space to another is continuous if and only if its graph is a closed set.[50] In the case of Hilbert spaces, this is basic in the study of unbounded operators (see closed operator).The (geometrical) Hahn–Banach theorem asserts that a closed convex set can be separated from any point outside it by means of a hyperplane of the Hilbert space. This is an immediate consequence of the best approximation property: if y is the element of a closed convex set F closest to x, then the separating hyperplane is the plane perpendicular to the segment xy passing through its midpoint.[51]The continuous linear operators A : H1 → H2 from a Hilbert space H1 to a second Hilbert space H2 are bounded in the sense that they map bounded sets to bounded sets. Conversely, if an operator is bounded, then it is continuous. The space of such bounded linear operators has a norm, the operator norm given byThe sum and the composite of two bounded linear operators is again bounded and linear. For y in H2, the map that sends x ∈ H1 to ⟨Ax, y⟩ is linear and continuous, and according to the Riesz representation theorem can therefore be represented in the formfor some vector A*y in H1. This defines another bounded linear operator A* : H2 → H1, the adjoint of A. One can see that A** = A.The set B(H) of all bounded linear operators on H (operators H → H), together with the addition and composition operations, the norm and the adjoint operation, is a C*-algebra, which is a type of operator algebra.An element A of B(H) is called 'self-adjoint' or 'Hermitian' if A* = A. If A is Hermitian and ⟨Ax,x⟩ ≥ 0 for every x, then A is called 'nonnegative', written A ≥ 0; if equality holds only when x = 0, then A is called 'positive'. The set of self adjoint operators admits a partial order, in which A ≥ B if A − B ≥ 0. If A has the form B*B for some B, then A is nonnegative; if B is invertible, then A is positive. A converse is also true in the sense that, for a non-negative operator A, there exists a unique non-negative square root B such thatIn a sense made precise by the spectral theorem, self-adjoint operators can usefully be thought of as operators that are "real". An element A of B(H) is called normal if A*A = AA*. Normal operators decompose into the sum of a self-adjoint operators and an imaginary multiple of a self adjoint operatorthat commute with each other. Normal operators can also usefully be thought of in terms of their real and imaginary parts.An element U of B(H) is called unitary if U is invertible and its inverse is given by U*. This can also be expressed by requiring that U be onto and ⟨Ux,Uy⟩ = ⟨x,y⟩ for all x, y ∈ H. The unitary operators form a group under composition, which is the isometry group of H.An element of B(H) is compact if it sends bounded sets to relatively compact sets. Equivalently, a bounded operator T is compact if, for any bounded sequence {xk}, the sequence {Txk} has a convergent subsequence. Many integral operators are compact, and in fact define a special class of operators known as Hilbert–Schmidt operators that are especially important in the study of integral equations. Fredholm operators differ from a compact operator by a multiple of the identity, and are equivalently characterized as operators with a finite dimensional kernel and cokernel. The index of a Fredholm operator T is defined byThe index is homotopy invariant, and plays a deep role in differential geometry via the Atiyah–Singer index theorem.Unbounded operators are also tractable in Hilbert spaces, and have important applications to quantum mechanics.[52] An unbounded operator T on a Hilbert space H is defined as a linear operator whose domain D(T) is a linear subspace of H. Often the domain D(T) is a dense subspace of H, in which case T is known as a densely defined operator.The adjoint of a densely defined unbounded operator is defined in essentially the same manner as for bounded operators. Self-adjoint unbounded operators play the role of the observables in the mathematical formulation of quantum mechanics. Examples of self-adjoint unbounded operators on the Hilbert space L2(ℝ) are:[53]These correspond to the momentum and position observables, respectively. Note that neither A nor B is defined on all of H, since in the case of A the derivative need not exist, and in the case of B the product function need not be square integrable. In both cases, the set of possible arguments form dense subspaces of L2(ℝ).Two Hilbert spaces H1 and H2 can be combined into another Hilbert space, called the (orthogonal) direct sum,[54] and denotedconsisting of the set of all ordered pairs (x1,x2) where xi ∈ Hi, i = 1,2, and inner product defined byMore generally, if Hi is a family of Hilbert spaces indexed by i ∈ I, then the direct sum of the Hi, denotedconsists of the set of all indexed familiesin the Cartesian product of the Hi such thatThe inner product is defined byEach of the Hi is included as a closed subspace in the direct sum of all of the Hi. Moreover, the Hi are pairwise orthogonal. Conversely, if there is a system of closed subspaces, Vi, i ∈ I, in a Hilbert space H, that are pairwise orthogonal and whose union is dense in H, then H is canonically isomorphic to the direct sum of Vi. In this case, H is called the internal direct sum of the Vi. A direct sum (internal or external) is also equipped with a family of orthogonal projections Ei onto the ith direct summand Hi. These projections are bounded, self-adjoint, idempotent operators that satisfy the orthogonality conditionThe spectral theorem for compact self-adjoint operators on a Hilbert space H states that H splits into an orthogonal direct sum of the eigenspaces of an operator, and also gives an explicit decomposition of the operator as a sum of projections onto the eigenspaces. The direct sum of Hilbert spaces also appears in quantum mechanics as the Fock space of a system containing a variable number of particles, where each Hilbert space in the direct sum corresponds to an additional degree of freedom for the quantum mechanical system. In representation theory, the Peter–Weyl theorem guarantees that any unitary representation of a compact group on a Hilbert space splits as the direct sum of finite-dimensional representations.If x1, y1 ∊ H1 and x2, y2 ∊ H2, then one defines an inner product on the (ordinary) tensor product as follows. On simple tensors, letAn example is provided by the Hilbert space L2([0,1]). The Hilbertian tensor product of two copies of L2([0,1]) is isometrically and linearly isomorphic to the space L2([0,1]2) of square-integrable functions on the square [0,1]2. This isomorphism sends a simple tensor f1 ⊗ f2 to the functionon the square.This example is typical in the following sense.[56] Associated to every simple tensor product x1 ⊗ x2 is the rank one operator from H∗1 to H2 that maps a given x* ∈ H∗1 asThe notion of an orthonormal basis from linear algebra generalizes over to the case of Hilbert spaces.[57] In a Hilbert space H, an orthonormal basis is a family {ek}k ∈ B of elements of H satisfying the conditions:A system of vectors satisfying the first two conditions basis is called an orthonormal system or an orthonormal set (or an orthonormal sequence if B is countable). Such a system is always linearly independent. Completeness of an orthonormal system of vectors of a Hilbert space can be equivalently restated as:This is related to the fact that the only vector orthogonal to a dense linear subspace is the zero vector, for if S is any orthonormal set and v is orthogonal to S, then v is orthogonal to the closure of the linear span of S, which is the whole space.Examples of orthonormal bases include:In the infinite-dimensional case, an orthonormal basis will not be a basis in the sense of linear algebra; to distinguish the two, the latter basis is also called a Hamel basis. That the span of the basis vectors is dense implies that every vector in the space can be written as the sum of an infinite series, and the orthogonality implies that this decomposition is unique.The space l2 of square-summable sequences of complex numbers is the set of infinite sequencesof complex numbers such thatThis space has an orthonormal basis:More generally, if B is any set, then one can form a Hilbert space of sequences with index set B, defined byThe summation over B is here defined bythe supremum being taken over all finite subsets of B. It follows that, for this sum to be finite, every element of l2(B) has only countably many nonzero terms. This space becomes a Hilbert space with the inner productfor all x, y ∈ l2(B). Here the sum also has only countably many nonzero terms, and is unconditionally convergent by the Cauchy–Schwarz inequality.An orthonormal basis of l2(B) is indexed by the set B, given byLet f1, ..., fn be a finite orthonormal system in H. For an arbitrary vector x ∈ H, letThen ⟨x, fk⟩ = ⟨y, fk⟩ for every k = 1, ..., n. It follows that x − y is orthogonal to each fk, hence x − y is orthogonal to y. Using the Pythagorean identity twice, it follows thatLet {fi}, i ∈ I, be an arbitrary orthonormal system in H. Applying the preceding inequality to every finite subset J of I gives the Bessel inequality[58](according to the definition of the sum of an arbitrary family of non-negative real numbers).Geometrically, Bessel's inequality implies that the orthogonal projection of x onto the linear subspace spanned by the fi has norm that does not exceed that of x. In two dimensions, this is the assertion that the length of the leg of a right triangle may not exceed the length of the hypotenuse.Bessel's inequality is a stepping stone to the more powerful Parseval identity, which governs the case when Bessel's inequality is actually an equality. If {ek}k ∈ B is an orthonormal basis of H, then every element x of H may be written asEven if B is uncountable, Bessel's inequality guarantees that the expression is well-defined and consists only of countably many nonzero terms. This sum is called the Fourier expansion of x, and the individual coefficients ⟨x,ek⟩ are the Fourier coefficients of x. Parseval's formula is thenConversely, if {ek} is an orthonormal set such that Parseval's identity holds for every x, then {ek} is an orthonormal basis.As a consequence of Zorn's lemma, every Hilbert space admits an orthonormal basis; furthermore, any two orthonormal bases of the same space have the same cardinality, called the Hilbert dimension of the space.[59] For instance, since l2(B) has an orthonormal basis indexed by B, its Hilbert dimension is the cardinality of B (which may be a finite integer, or a countable or uncountable cardinal number).As a consequence of Parseval's identity, if {ek}k ∈ B is an orthonormal basis of H, then the map Φ : H → l2(B) defined by Φ(x) = ⟨x,ek⟩k∈B is an isometric isomorphism of Hilbert spaces: it is a bijective linear mapping such thatfor all x, y ∈ H. The cardinal number of B is the Hilbert dimension of H. Thus every Hilbert space is isometrically isomorphic to a sequence space l2(B) for some set B.A Hilbert space is separable if and only if it admits a countable orthonormal basis. All infinite-dimensional separable Hilbert spaces are therefore isometrically isomorphic to l2.In the past, Hilbert spaces were often required to be separable as part of the definition.[60] Most spaces used in physics are separable, and since these are all isomorphic to each other, one often refers to any infinite-dimensional separable Hilbert space as "the Hilbert space" or just "Hilbert space".[61] Even in quantum field theory, most of the Hilbert spaces are in fact separable, as stipulated by the Wightman axioms. However, it is sometimes argued that non-separable Hilbert spaces are also important in quantum field theory, roughly because the systems in the theory possess an infinite number of degrees of freedom and any infinite Hilbert tensor product (of spaces of dimension greater than one) is non-separable.[62] For instance, a bosonic field can be naturally thought of as an element of a tensor product whose factors represent harmonic oscillators at each point of space. From this perspective, the natural state space of a boson might seem to be a non-separable space.[62] However, it is only a small separable subspace of the full tensor product that can contain physically meaningful fields (on which the observables can be defined). Another non-separable Hilbert space models the state of an infinite collection of particles in an unbounded region of space. An orthonormal basis of the space is indexed by the density of the particles, a continuous parameter, and since the set of possible densities is uncountable, the basis is not countable.[62]If S is a subset of a Hilbert space H, the set of vectors orthogonal to S is defined byS⊥ is a closed subspace of H (can be proved easily using the linearity and continuity of the inner product) and so forms itself a Hilbert space. If V is a closed subspace of H, then V⊥ is called the orthogonal complement of V. In fact, every x ∈ H can then be written uniquely as x = v + w, with v ∈ V and w ∈ V⊥. Therefore, H is the internal Hilbert direct sum of V and V⊥.The linear operator PV : H → H that maps x to v is called the orthogonal projection onto V. There is a natural one-to-one correspondence between the set of all closed subspaces of H and the set of all bounded self-adjoint operators P such that P2 = P. Specifically,This provides the geometrical interpretation of PV(x): it is the best approximation to x by elements of V.[63]Projections PU and PV are called mutually orthogonal if PUPV = 0. This is equivalent to U and V being orthogonal as subspaces of H. The sum of the two projections PU and PV is a projection only if U and V are orthogonal to each other, and in that case PU + PV = PU+V. The composite PUPV is generally not a projection; in fact, the composite is a projection if and only if the two projections commute, and in that case PUPV = PU∩V.By restricting the codomain to the Hilbert space V, the orthogonal projection PV gives rise to a projection mapping π : H → V; it is the adjoint of the inclusion mappingmeaning thatfor all x ∈ V and y ∈ H.The operator norm of the orthogonal projection PV onto a nonzero closed subspace V is equal to 1:Every closed subspace V of a Hilbert space is therefore the image of an operator P of norm one such that P2 = P. The property of possessing appropriate projection operators characterizes Hilbert spaces:[64]While this result characterizes the metric structure of a Hilbert space, the structure of a Hilbert space as a topological vector space can itself be characterized in terms of the presence of complementary subspaces:[65]The orthogonal complement satisfies some more elementary results. It is a monotone function in the sense that if U ⊂ V, then V⊥ ⊆ U⊥ with equality holding if and only if V is contained in the closure of U. This result is a special case of the Hahn–Banach theorem. The closure of a subspace can be completely characterized in terms of the orthogonal complement: if V is a subspace of H, then the closure of V is equal to V⊥⊥. The orthogonal complement is thus a Galois connection on the partial order of subspaces of a Hilbert space. In general, the orthogonal complement of a sum of subspaces is the intersection of the orthogonal complements:[66]If the Vi are in addition closed, thenThere is a well-developed spectral theory for self-adjoint operators in a Hilbert space, that is roughly analogous to the study of symmetric matrices over the reals or self-adjoint matrices over the complex numbers.[67] In the same sense, one can obtain a "diagonalization" of a self-adjoint operator as a suitable sum (actually an integral) of orthogonal projection operators.The spectrum of an operator T, denoted σ(T), is the set of complex numbers λ such that T − λ lacks a continuous inverse. If T is bounded, then the spectrum is always a compact set in the complex plane, and lies inside the disc |z| ≤ ||T||. If T is self-adjoint, then the spectrum is real. In fact, it is contained in the interval [m,M] whereMoreover, m and M are both actually contained within the spectrum.The eigenspaces of an operator T are given byUnlike with finite matrices, not every element of the spectrum of T must be an eigenvalue: the linear operator T − λ may only lack an inverse because it is not surjective. Elements of the spectrum of an operator in the general sense are known as spectral values. Since spectral values need not be eigenvalues, the spectral decomposition is often more subtle than in finite dimensions.However, the spectral theorem of a self-adjoint operator T takes a particularly simple form if, in addition, T is assumed to be a compact operator. The spectral theorem for compact self-adjoint operators states:[68]This theorem plays a fundamental role in the theory of integral equations, as many integral operators are compact, in particular those that arise from Hilbert–Schmidt operators.The general spectral theorem for self-adjoint operators involves a kind of operator-valued Riemann–Stieltjes integral, rather than an infinite summation.[69] The spectral family associated to T associates to each real number λ an operator Eλ, which is the projection onto the nullspace of the operator (T − λ)+, where the positive part of a self-adjoint operator is defined byThe operators Eλ are monotone increasing relative to the partial order defined on self-adjoint operators; the eigenvalues correspond precisely to the jump discontinuities. One has the spectral theorem, which assertsThe integral is understood as a Riemann–Stieltjes integral, convergent with respect to the norm on B(H). In particular, one has the ordinary scalar-valued integral representationA somewhat similar spectral decomposition holds for normal operators, although because the spectrum may now contain non-real complex numbers, the operator-valued Stieltjes measure dEλ must instead be replaced by a resolution of the identity.A major application of spectral methods is the spectral mapping theorem, which allows one to apply to a self-adjoint operator T any continuous complex function f defined on the spectrum of T by forming the integralThe resulting continuous functional calculus has applications in particular to pseudodifferential operators.[70]The spectral theory of unbounded self-adjoint operators is only marginally more difficult than for bounded operators. The spectrum of an unbounded operator is defined in precisely the same way as for bounded operators: λ is a spectral value if the resolvent operatorfails to be a well-defined continuous operator. The self-adjointness of T still guarantees that the spectrum is real. Thus the essential idea of working with unbounded operators is to look instead at the resolvent Rλ where λ is nonreal. This is a bounded normal operator, which admits a spectral representation that can then be transferred to a spectral representation of T itself. A similar strategy is used, for instance, to study the spectrum of the Laplace operator: rather than address the operator directly, one instead looks as an associated resolvent such as a Riesz potential or Bessel potential.A precise version of the spectral theorem in this case is:[71]There is also a version of the spectral theorem that applies to unbounded normal operators.Thomas Pynchon introduced the fictional character, Sammy Hilbert-Spaess (a pun on "Hilbert Space"), in his 1973 novel, Gravity's Rainbow.  Hilbert-Spaess is first described as a "a ubiquitous double agent" and later as "at least a double agent".  The novel had earlier referenced the work of fellow German mathematician Kurt Gödel's Incompleteness Theorems which showed that Hilbert's Program, Hilbert's formalized plan to unify mathematics into a single set of axioms, was not possible.[72][73][74]
Matrix analysis
In mathematics, particularly in linear algebra and applications, matrix analysis is the study of matrices and their algebraic properties.[1] Some particular topics out of many include; operations defined on matrices (such as matrix addition, matrix multiplication and operations derived from these), functions of matrices (such as matrix exponentiation and matrix logarithm, and even sines and cosines etc. of matrices),[2] and the eigenvalues of matrices (eigendecomposition of a matrix, eigenvalue perturbation theory).The set of all m×n matrices over a number field F denoted in this article Mmn(F) form a vector space. Examples of F include the set of integers ℤ, the real numbers ℝ, and set of complex numbers ℂ. The spaces Mmn(F) and Mpq(F) are different spaces if m and p are unequal, and if n and q are unequal; for instance M32(F) ≠ M23(F). Two m×n matrices A and B in Mmn(F) can be added together to form another matrix in the space Mmn(F):and multiplied by a α in F, to obtain another matrix in Mmn(F):Combining these two properties, a linear combination of matrices A and B are in Mmn(F) is another matrix in Mmn(F):where α and β are numbers in F.Any matrix can be expressed as a linear combination of basis matrices, which play the role of the basis vectors for the matrix space. For example, for the set of 2×2 matrices over the field of real numbers, M22(ℝ), one legitimate basis set of matrices is:because any 2×2 matrix can be expressed as:where a, b, c,d are all real numbers. This idea applies to other fields and matrices of higher dimensions.The determinant of a square matrix is an important property. The determinant indicates if a matrix is invertible (i.e. the inverse of a matrix exists when the determinant is nonzero). Determinants are used for finding eigenvalues of matrices (see below), and for solving a system of linear equations (see Cramer's rule).An n×n matrix A has eigenvectors x and eigenvalues λ defined by the relation:In words, the matrix multiplication of A followed by an eigenvector x (here an n-dimensional column matrix), is the same as multiplying the eigenvector by the eigenvalue. For an n×n matrix, there are n eigenvalues. The eigenvalues are the roots of the characteristic polynomial:where I is the n×n identity matrix.Roots of polynomials, in this context the eigenvalues, can all be different, or some may be equal (in which case eigenvalue has multiplicity, the number of times an eigenvalue occurs). After solving for the eigenvalues, the eigenvectors corresponding to the eigenvalues can be found by the defining equation.Two n×n matrices A and B are similar if they are related by a similarity transformation:The matrix P is called a similarity matrix, and is necessarily invertible.LU decomposition splits a matrix into a matrix product of an upper triangular matrix and a lower triangle matrix.Since matrices form vector spaces, one can form axioms (analogous to those of vectors) to define a "size" of a particular matrix. The norm of a matrix is a positive real number.For all matrices A and B in Mmn(F), and all numbers α in F, a matrix norm, delimited by double vertical bars || ... ||, fulfills:[note 1]The Frobenius norm is analogous to the dot product of Euclidean vectors; multiply matrix elements entry-wise, add up the results, then take the positive square root:It is defined for matrices of any dimension (i.e. no restriction to square matrices).Matrix elements are not restricted to constant numbers, they can be mathematical variables.A functions of a matrix takes in a matrix, and return something else (a number, vector, matrix, etc...).A matrix valued function takes in something (a number, vector, matrix, etc...) and returns a matrix.
Semi-simple operator
In mathematics, a linear operator T on a finite-dimensional vector space is semi-simple if every T-invariant subspace has a  complementary T-invariant subspace.[1]An important result regarding semi-simple operators is that, a linear operator on a finite dimensional vector space over an algebraically closed field is semi-simple if and only if it is diagonalizable.[1] This is because such an operator always has an eigenvector; if it is, in addition, semi-simple, then it has a complementary invariant hyperplane, which itself has an eigenvector, and thus by induction is diagonalizable. Conversely, diagonalizable operators are easily seen to be semi-simple, as invariant subspaces are direct sums of eigenspaces, and any basis for this space can be extended to an eigenbasis.
Synthetic geometry
Synthetic geometry (sometimes referred to as axiomatic or even pure geometry) is the study of geometry without the use of coordinates or formulas. It relies on the axiomatic method and the tools directly related to them, that is, compass and straightedge, to draw conclusions and solve problems. Only after the introduction of coordinate methods was there a reason to introduce the term "synthetic geometry" to distinguish this approach to geometry from other approaches.Other approaches to geometry are embodied in analytic and algebraic geometries, where one would use analysis and algebraic techniques to obtain geometric results.According to Felix Klein,[1]Synthetic geometry is that which studies figures as such, without recourse to formulas, whereas analytic geometry consistently makes use of such formulas as can be written down after the adoption of an appropriate system of coordinates.Geometry, as presented by Euclid in the Elements, is the quintessential example of the use of the synthetic method. It was the favoured method of Isaac Newton for the solution of geometric problems.[2] Synthetic methods were most prominent during the 19th century when geometers rejected coordinate methods in establishing the foundations of projective geometry and non-Euclidean geometries. For example the geometer Jakob Steiner (1796 – 1863) hated analytic geometry, and always gave preference to synthetic methods.[3]The process of logical synthesis begins with some arbitrary but definite starting point. This starting point is the introduction of primitive notions or primitives and axioms about these primitives:From a given set of axioms, synthesis proceeds as a carefully constructed logical argument. When a significant result is proved rigorously, it becomes a theorem.There is no fixed axiom set for geometry, as more than one consistent set can be chosen. Each such set may lead to a different geometry, while there are also examples of different sets giving the same geometry. With this plethora of possibilities, it is no longer appropriate to speak of "geometry" in the singular.Historically, Euclid's parallel postulate has turned out to be independent of the other axioms. Simply discarding it gives absolute geometry, while negating it yields hyperbolic geometry. Other consistent axiom sets can yield other geometries, such as projective, elliptic, spherical or affine geometry.Axioms of continuity and "betweeness" are also optional, for example, discrete geometries may be created by discarding or modifying them.Following the Erlangen program of Klein, the nature of any given geometry can be seen as the connection between symmetry and the content of the propositions, rather than the style of development.Euclid's original treatment remained unchallenged for over two thousand years, until the simultaneous discoveries of the non-Euclidean geometries by Gauss, Bolyai, Lobachevsky and Riemann in the 19th century led mathematicians to question Euclid's underlying assumptions.[5]One of the early French analysts summarized synthetic geometry this way:The heyday of synthetic geometry can be considered to have been the 19th century, when analytic methods based on coordinates and calculus were ignored by some geometers such as Jakob Steiner, in favor of a purely synthetic development of  projective geometry. For example, the treatment of the projective plane starting from axioms of incidence is actually a broader theory (with more models) than is found by starting with a vector space of dimension three. Projective geometry has in fact the simplest and most elegant synthetic expression of any geometry.In his Erlangen program, Felix Klein played down the tension between synthetic and analytic methods:The close axiomatic study of Euclidean geometry led to the construction of the Lambert quadrilateral and the Saccheri quadrilateral. These structures introduced the field of non-Euclidean geometry where Euclid's parallel axiom is denied. Gauss, Bolyai and Lobachevski independently constructed hyperbolic geometry, where parallel lines have an angle of parallelism that depends on their separation. This study became widely accessible through the Poincaré disc model where motions are given by Möbius transformations. Similarly, Riemann, a student of Gauss's, constructed Riemannian geometry, of which elliptic geometry is a particular case.Another example concerns inversive geometry as advanced by Ludwig Immanuel Magnus, which can be considered synthetic in spirit. The closely related operation of reciprocation expresses analysis of the plane.Karl von Staudt showed that algebraic axioms, such as commutativity and associativity of addition and multiplication, were in fact consequences of incidence of lines in geometric configurations. David Hilbert showed[8] that the Desargues configuration played a special role. Further work was done by Ruth Moufang and her students. The concepts have been one of the motivators of incidence geometry.When parallel lines are taken as primary, synthesis produces affine geometry. Though Euclidean geometry is both an affine and metric geometry, in general affine spaces may be missing a metric. The extra flexibility thus afforded makes affine geometry appropriate for the study of spacetime, as discussed in the history of affine geometry.In 1955 Herbert Busemann and Paul J. Kelley sounded a nostalgic note for synthetic geometry:For example, college studies now include linear algebra, topology, and graph theory where the subject is developed from first principles, and propositions are deduced by elementary proofs.Today's student of geometry has axioms other than Euclid's available: see Hilbert's axioms and Tarski's axioms.Ernst Kötter published a (German) report in 1901 on "The development of synthetic geometry from Monge to Staudt (1847)";[10]Synthetic proofs of geometric theorems make use of auxiliary constructs (such as helping lines) and concepts such as equality of sides or angles and similarity and congruence of triangles. Examples of such proofs can be found in the articles Butterfly theorem, Angle bisector theorem, Apollonius' theorem, British flag theorem, Ceva's theorem, Equal incircles theorem, Geometric mean theorem, Heron's formula, Isosceles triangle theorem, Law of cosines, and others that are linked to here.In conjunction with computational geometry, a computational synthetic geometry has been founded, having close connection, for example, with matroid theory. Synthetic differential geometry is an application of topos theory to the foundations of differentiable manifold theory.
Dual space
In mathematics, any vector space V has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants.The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the algebraic dual space.  When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the continuous dual space.Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.for all φ and ψ ∈ V∗, x ∈ V, and a ∈ F. Elements of the algebraic dual space V∗ are sometimes called covectors or one-forms.The pairing of a functional φ in the dual space V∗ and an element x of V is sometimes denoted by a bracket: φ(x) = [x,φ] [2]or φ(x) = ⟨φ,x⟩.[3] This pairing defines a nondegenerate bilinear mapping[4] ⟨·,·⟩ : V∗ × V → F called the natural pairing.If V is finite-dimensional, then V∗ has the same dimension as V. Given a basis {e1, ..., en}  in V, it is possible to construct a specific basis in V∗, called the dual basis. This dual basis is a set {e1, ..., en}  of linear functionals on V, defined by the relationfor any choice of coefficients ci ∈ F. In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equationsIn particular, if we interpret Rn as the space of columns of n real numbers, its dual space is typically written as the space of rows of n real numbers. Such a row acts on Rn as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a real number y. Then, seeing this functional as a matrix M, and x, y as a n × 1 matrix and a 1 × 1 matrix (trivially, a real number) respectively, if we have Mx = y, then, by dimension reasons, M must be a 1 × n matrix, i.e., M must be a row vector.If V consists of the space of geometrical vectors in the plane, then the level curves of an element of V∗ form a family of parallel lines in V, because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of V∗ can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if V is a vector space of any dimension, then the level sets of a linear functional in V∗ are parallel hyperplanes in V, and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.[5]If V is not finite-dimensional but has a basis[6] eα indexed by an infinite set A, then the same construction as in the finite-dimensional case yields linearly independent elements eα (α ∈ A) of the dual space, but they will not form a basis.Consider, for instance, the space R∞, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for i ∈ N, ei is the sequence consisting of all zeroes except in the ith position, which is 1. The dual space of R∞ is (isomorphic to) RN, the space of all sequences of real numbers: such a sequence (an) is applied to an element (xn) of R∞ to give the number ∑anxn, which is a finite sum because there are only finitely many nonzero xn. The dimension of R∞ is countably infinite, whereas RN does not have a countable basis.This observation generalizes to any[6] infinite-dimensional vector space V over any field F: a choice of basis {eα : α ∈ A}  identifies V with the space (FA)0 of functions f : A → F such that fα = f(α) is nonzero for only finitely many α ∈ A, where such a function f is identified with the vectorin V (the sum is finite by the assumption on f, and any v ∈ V may be written in this way by the definition of the basis).The dual space of V may then be identified with the space FA of all functions from A to F: a linear functional T on V is uniquely determined by the values θα = T(eα) it takes on the basis of V, and any function θ : A → F (with θ(α) = θα) defines a linear functional T on V byAgain the sum is finite because fα is nonzero for only finitely many α.Note that (FA)0 may be identified (essentially by definition) with the direct sumof infinitely many copies of F (viewed as a 1-dimensional vector space over itself) indexed by A, i.e., there are linear isomorphismsOn the other hand, FA is (again by definition), the direct product of infinitely many copies of F indexed by A, and so the identificationis a special case of a general result relating direct sums (of modules) to direct products.Thus if the basis is infinite, then the algebraic dual space is always of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.If V is finite-dimensional, then V is isomorphic to V∗. But there is in general no natural isomorphism between these two spaces.[7]  Any bilinear form ⟨·,·⟩ on V gives a mapping of V into its dual space viawhere the right hand side is defined as the functional on V taking each w ∈ V to ⟨v,w⟩.  In other words, the bilinear form determines a linear mappingdefined byThus there is a one-to-one correspondence between isomorphisms of V to subspaces of (resp., all of) V∗ and nondegenerate bilinear forms on V.If the vector space V is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms.  In that case, a given sesquilinear form ⟨·,·⟩ determines an isomorphism of V with the complex conjugate of the dual spaceThe conjugate space V∗ can be identified with the set of all additive complex-valued functionals f: V → C such thatIf f : V → W is a linear map, then the transpose (or dual) f∗ : W∗ → V∗ is defined byfor every φ ∈ W∗. The resulting functional f∗(φ) in V∗ is called the pullback of φ along f.The following identity holds for all φ ∈ W∗ and v ∈ V:where the bracket [·,·] on the left is the natural pairing of V with its dual space, and that on the right is the natural pairing of W with its dual.  This identity characterizes the transpose,[9] and is formally similar to the definition of the adjoint.The assignment f ↦ f∗ produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W∗ to V∗; this homomorphism is an isomorphism if and only if W is finite-dimensional. If V = W then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that (fg)∗ = g∗f∗. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify  (f∗)∗ with f using the natural injection into the double dual.If the linear map f is represented by the matrix A with respect to two bases of V and W, then f∗ is represented by the transpose matrix AT with respect to the dual bases of W∗ and V∗, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, f∗ is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.Let S be a subset of V. The annihilator of S in V∗, denoted here S0, is the collection of linear functionals f ∈ V∗ such that [f, s] = 0 for all s ∈ S. That is, S0 consists of all linear functionals f : V → F such that the restriction to S vanishes: f|S = 0.The annihilator of a subset is itself a vector space. In particular, ∅0 = V∗ is all of V∗ (vacuously), whereas V0 = 0 is the zero subspace. Furthermore, the assignment of an annihilator to a subset of V reverses inclusions, so that if S ⊂ T ⊂ V, thenMoreover, if A and B are two subsets of V, thenand equality holds provided V is finite-dimensional. If Ai is any family of subsets of V indexed by i belonging to some index set I, thenIn particular if A and B are subspaces of V, it follows thatIf V is finite-dimensional, and W is a vector subspace, thenafter identifying W with its image in the second dual space under the double duality isomorphism V ≈ V∗∗.  Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.If W is a subspace of V then the quotient space V/W is a vector space in its own right, and so has a dual.  By the first isomorphism theorem, a functional f : V → F factors through V/W if and only if W is in the kernel of f.  There is thus an isomorphismAs a particular consequence, if V is a direct sum of two subspaces A and B, then V∗ is a direct sum of A0 and B0.form its local base.Here are the three most important special cases.Let 1 < p < ∞ be a real number and consider the Banach space ℓ p of all sequences a = (an) for whichis finite. Define the number q by 1/p + 1/q = 1.  Then the continuous dual of ℓ p is naturally identified with ℓ q: given an element φ ∈ (ℓ p)′, the corresponding element of ℓ q is the sequence (φ(en)) where en denotes the sequence whose n-th term is 1 and all others are zero. Conversely, given an element a = (an) ∈ ℓ q, the corresponding continuous linear functional φ on ℓ p is defined by φ(b) = ∑n anbn for all b = (bn) ∈ ℓ p (see Hölder's inequality).In a similar manner, the continuous dual of ℓ 1 is naturally identified with ℓ ∞ (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces c (consisting of all convergent sequences, with the supremum norm) and c0 (the sequences converging to zero) are both naturally identified with ℓ 1.By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra–ket notation used by physicists in the mathematical formulation of quantum mechanics.By the Riesz–Markov–Kakutani representation theorem, the continuous dual of certain spaces of continuous functions can be described using measures.If T : V → W is a continuous linear map between two topological vector spaces, then the (continuous) transpose  T′ : W′ → V′ is defined by the same formula as before:The resulting functional T′(φ) is in V′. The assignment T → T′ produces a linear map between the space of continuous linear maps from V to W and the space of linear maps from W′ to V′.  When T and U are composable continuous linear maps, thenWhen V and W are normed spaces, the norm of the transpose in L(W′, V′) is equal to that of T in L(V, W). Several properties of transposition depend upon the Hahn–Banach theorem. For example, the bounded linear map T has dense range if and only if the transpose T′ is injective.When T is a compact linear map between two Banach spaces V and W, then the transpose T′ is compact. This can be proved using the Arzelà–Ascoli theorem.When V is a Hilbert space, there is an antilinear isomorphism iV from V onto its continuous dual V′. For every bounded linear map T on V, the transpose and the adjoint operators are linked byWhen T is a continuous linear map between two topological vector spaces V and W, then the transpose T′ is continuous when W′ and V′ are equipped with"compatible" topologies: for example when, for X = V and X = W, both duals X′ have the strong topology β(X′, X) of uniform convergence on bounded sets of X, or both have the weak-∗ topology σ(X′, X) of pointwise convergence on X.  The transpose T′ is continuous from β(W′, W) to β(V′, V), or from σ(W′, W) to σ(V′, V).Assume that W is a closed linear subspace of a normed space V, and consider the annihilator of W in V′,Then, the dual of the quotient V / W  can be identified with W⊥, and the dual of W can be identified with the quotient V′ / W⊥.[14] Indeed, let P denote the canonical surjection from V onto the quotient V / W ; then, the transpose P′ is an isometric isomorphism from (V / W )′ into V′, with range equal to W⊥. If j denotes the injection map from W into V, then the kernel of the transpose j′ is the annihilator of W:and it follows from the Hahn–Banach theorem that j′ induces an isometric isomorphismV′ / W⊥ → W′.If the dual of a normed space V is separable, then so is the space V itself. The converse is not true: for example the space ℓ 1 is separable, but its dual ℓ ∞ is not.The topology of V and the topology of real or complex numbers can be used to induce on V′ a dual space topology.In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator Ψ : V → V′′ from a normed space V into its continuous double dual V′′, defined byAs a consequence of the Hahn–Banach theorem, this map is in fact an isometry, meaning ‖ Ψ(x) ‖ = ‖ x ‖ for all x in V. Normed spaces for which the map Ψ is a bijection are called reflexive.When V is a topological vector space, one can still define Ψ(x) by the same formula, for every x ∈ V, however several difficulties arise. First, when V is not locally convex, the continuous dual may be equal to {0} and the map Ψ trivial.  However, if V is Hausdorff and locally convex, the map Ψ is injective from V to the algebraic dual V′∗ of the continuous dual, again as a consequence of the Hahn–Banach theorem.[15]Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual V′, so that the continuous double dual V′′ is not uniquely defined as a set. Saying that Ψ maps from V to V′′, or in other words, that Ψ(x) is continuous on V′ for every x ∈ V, is a reasonable minimal requirement on the topology of V′, namely that the evaluation mappingsbe continuous for the chosen topology on V′. Further, there is still a choice of a topology on V′′, and continuity of Ψ depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed  case.
Endomorphism
In mathematics, an endomorphism is a morphism (or homomorphism) from a mathematical object to itself.  For example, an endomorphism of a vector space V is a linear map f: V → V, and an endomorphism of a group G is a group homomorphism f: G → G. In general, we can talk about endomorphisms in any category. In the category of sets, endomorphisms are functions from a set S to itself.In any category, the composition of any two endomorphisms of X is again an endomorphism of X.  It follows that the set of all endomorphisms of X forms a monoid, denoted End(X) (or EndC(X) to emphasize the category C).An invertible endomorphism of X is called an automorphism.  The set of all automorphisms is a subset of End(X) with a group structure, called the automorphism group of X and denoted Aut(X).  In the following diagram, the arrows denote implication:Any two endomorphisms of an abelian group, A, can be added together by the rule (f + g)(a) = f(a) + g(a).  Under this addition, and with multiplication being defined as function composition, the endomorphisms of an abelian group form a ring (the endomorphism ring).  For example, the set of endomorphisms of ℤn is the ring of all n × n matrices with integer entries.  The endomorphisms of a vector space or module also form a ring, as do the endomorphisms of any object in a preadditive category.  The endomorphisms of a nonabelian group generate an algebraic structure known as a near-ring. Every ring with one is the endomorphism ring of its regular module, and so is a subring of an endomorphism ring of an abelian group;[1] however there are rings that are not the endomorphism ring of any abelian group.In any concrete category, especially for vector spaces, endomorphisms are maps from a set into itself, and may be interpreted as unary operators on that set, acting on the elements, and allowing to define the notion of orbits of elements, etc.Depending on the additional structure defined for the category at hand (topology, metric, ...), such operators can have properties like continuity, boundedness, and so on.  More details should be found in the article about operator theory.An endofunction is a function whose domain is equal to its codomain. A homomorphic endofunction is an endomorphism.Let S be an arbitrary set. Among endofunctions on S one finds permutations of S and constant functions associating to every x ∈ S the same c ∈ S.  Every permutation of S has the codomain equal to its domain and is bijective and invertible. A constant function on S, if S has more than 1 element, has an image that is a proper subset of its codomain, is not bijective (and non invertible). The function associating to each natural integer n the floor of n/2 has its image equal to its codomain and is not invertible.Finite endofunctions are equivalent to directed pseudoforests. For sets of size n there are nn endofunctions on the set.Particular examples of bijective endofunctions are the involutions; i.e., the functions coinciding with their inverses.
Levinson recursion
Levinson recursion or Levinson–Durbin recursion is a procedure in linear algebra to recursively calculate the solution to an equation involving a Toeplitz matrix. The algorithm runs in Θ(n2) time, which is a strong improvement over Gauss–Jordan elimination, which runs in Θ(n3).The Levinson–Durbin algorithm was proposed first by Norman Levinson in 1947, improved by James Durbin in 1960, and subsequently improved to 4n2 and then 3n2 multiplications by W. F. Trench and S. Zohar, respectively.Other methods to process data include Schur decomposition and Cholesky decomposition. In comparison to these, Levinson recursion (particularly split Levinson recursion) tends to be faster computationally, but more sensitive to computational inaccuracies like round-off errors.The Bareiss algorithm for Toeplitz matrices (not to be confused with the general Bareiss algorithm) runs about as fast as Levinson recursion, but it uses O(n2) space, whereas Levinson recursion uses only O(n) space.  The Bareiss algorithm, though, is numerically stable,[1][2] whereas Levinson recursion is at best only weakly stable (i.e. it exhibits numerical stability for well-conditioned linear systems).[3]Newer algorithms, called asymptotically fast or sometimes superfast Toeplitz algorithms, can solve in Θ(n logpn) for various p (e.g. p = 2,[4][5] p = 3 [6]). Levinson recursion remains popular for several reasons; for one, it is relatively easy to understand in comparison; for another, it can be faster than a superfast algorithm for small n (usually n < 256).[7]Matrix equations follow the form:For the sake of this article, êi is a vector made up entirely of zeroes, except for its ith place, which holds the value one. Its length will be implicitly determined by the surrounding context. The term N refers to the width of the matrix above – M is an N×N matrix. Finally, in this article, superscripts refer to an inductive index, whereas subscripts denote indices. For example (and definition), in this article, the matrix Tn is an n×n matrix which copies the upper left n×n block from M – that is, Tnij = Mij.Tn is also a Toeplitz matrix; meaning that it can be written as:The algorithm proceeds in two steps. In the first step, two sets of vectors, called the forward and backward vectors, are established. The forward vectors are used to help get the set of backward vectors; then they can be immediately discarded. The backwards vectors are necessary for the second step, where they are used to build the solution desired.An important simplification can occur when M is a symmetric matrix; then the two vectors are related by bni = fnn+1−i—that is, they are row-reversals of each other. This can save some extra computation in that special case.Even if the matrix is not symmetric, then the nth forward and backward vector may be found from the vectors of length n − 1 as follows. First, the forward vector may be extended with a zero to obtain:In going from Tn−1 to Tn, the extra column added to the matrix does not perturb the solution when a zero is used to extend the forward vector. However, the extra row added to the matrix has perturbed the solution; and it has created an unwanted error term εf which occurs in the last place. The above equation gives it the value of:This error will be returned to shortly and eliminated from the new forward vector; but first, the backwards vector must be extended in a similar (albeit reversed) fashion. For the backwards vector,As before, the extra column added to the matrix does not perturb this new backwards vector; but the extra row does. Here we have another unwanted error εb with value:If α and β are chosen so that the right hand side yields ê1 or ên, then the quantity in the parentheses will fulfill the definition of the nth forward or backward vector, respectively. With those alpha and beta chosen, the vector sum in the parentheses is simple and yields the desired result.Now, all the zeroes in the middle of the two vectors above being disregarded and collapsed, only the following equation is left:With these solved for (by using the Cramer 2×2 matrix inverse formula), the new forward and backward vectors are:Performing these vector summations, then, gives the nth forward and backward vectors from the prior ones. All that remains is to find the first of these vectors, and then some quick sums and multiplications give the remaining ones. The first forward and backward vectors are simply:The above steps give the N backward vectors for M. From there, a more arbitrary equation is:The solution is then built recursively by noticing that ifThen, extending with a zero again, and defining an error constant where necessary:We can then use the nth backward vector to eliminate the error term and replace it with the desired formula as follows:In practice, these steps are often done concurrently with the rest of the procedure, but they form a coherent unit and deserve to be treated as their own step.If M is not strictly Toeplitz, but block Toeplitz, the Levinson recursion can be derived in much the same way by regarding the block Toeplitz matrix as a Toeplitz matrix with matrix elements (Musicus 1988). Block Toeplitz matrices arise naturally in signal processing algorithms when dealing with multiple signal streams (e.g., in MIMO systems) or cyclo-stationary signals.Defining sourcesFurther workSummaries
Symmetric matrix
In linear algebra, a symmetric matrix is a square matrix that is equal to its transpose. Formally, matrix A is symmetric ifBecause equal matrices have equal dimensions, only square matrices can be symmetric.The entries of a symmetric matrix are symmetric with respect to the main diagonal. So if the entries are written as A = (aij), then aij = aji, for all indices i and j.The following 3 × 3 matrix is symmetric:Every square diagonal matrix is symmetric, since all off-diagonal elements are zero. Similarly in characteristic different from 2, each diagonal element of a skew-symmetric matrix must be zero, since each is its own negative.In linear algebra, a real symmetric matrix represents a self-adjoint operator[1] over a real inner product space. The corresponding object for a complex inner product space is a Hermitian matrix with complex-valued entries, which is equal to its conjugate transpose. Therefore, in linear algebra over the complex numbers, it is often assumed that a symmetric matrix refers to one which has real-valued entries.  Symmetric matrices appear naturally in a variety of applications, and typical numerical linear algebra software makes special accommodations for them.The sum and difference of two symmetric matrices is again symmetric, but this is not always true for the product: given symmetric matrices A and B, then AB is symmetric if and only if A and B commute, i.e., if AB = BA. So for integer n, An is symmetric if A is symmetric.  If A−1 exists, it is symmetric if and only if A is symmetric.Let Matn denote the space of n × n matrices. A symmetric n × n matrix is determined by n(n + 1)/2 scalars (the number of entries on or above the main diagonal). Similarly, a skew-symmetric matrix is determined by n(n − 1)/2 scalars (the number of entries above the main diagonal). If Symn denotes the space of n × n symmetric matrices and Skewn the space of n × n skew-symmetric matrices then Matn = Symn + Skewn and Symn ∩ Skewn = {0}, i.e.where ⊕ denotes the direct sum. Let X ∈ Matn thenNotice that 1/2(X + XT) ∈ Symn and 1/2(X − XT) ∈ Skewn. This is true for every square matrix X with entries from any field whose characteristic is different from 2.Any matrix congruent to a symmetric matrix is again symmetric: if X is a symmetric matrix then so is AXAT for any matrix A.  A symmetric matrix is necessarily a normal matrix.Since this definition is independent of the choice of basis, symmetry is a property that depends only on the linear operator A and a choice of inner product. This characterization of symmetry is useful, for example, in differential geometry, for each tangent space to a manifold may be endowed with an inner product, giving rise to what is called a Riemannian manifold. Another area where this formulation is used is in Hilbert spaces.The finite-dimensional spectral theorem says that any symmetric matrix whose entries are real can be diagonalized by an orthogonal matrix. More explicitly: For every symmetric real matrix A there exists a real orthogonal matrix Q such that D = QTAQ is a diagonal matrix. Every symmetric matrix is thus, up to choice of an orthonormal basis, a diagonal matrix.Every real symmetric matrix is Hermitian, and therefore all its eigenvalues are real. (In fact, the eigenvalues are the entries in the diagonal matrix D (above), and therefore D is uniquely determined by A up to the order of its entries.) Essentially, the property of being symmetric for real matrices corresponds to the property of being Hermitian for complex matrices.A complex symmetric matrix can be 'diagonalized' using a unitary matrix: thus if A is a complex symmetric matrix, there is a unitary matrix U such that  U A U T is a real diagonal matrix. This result is referred to as the Autonne–Takagi factorization. It was originally proved by Léon Autonne (1915) and Teiji Takagi (1925) and rediscovered with different proofs by several other mathematicians.[2][3] In fact, the matrix B = A†A is Hermitian and non-negative, so there is a unitary matrix V such that V†BV is diagonal with non-negative real entries. Thus C = VTAV is complex symmetric with C†C real. Writing C = X + iY with X and Y real symmetric matrices,  C†C = X2 + Y2 + i(XY − YX). Thus XY = YX. Since X and Y commute, there is a real orthogonal matrix W such that both WXWT and WYWT are diagonal. Setting U = WVT, the matrix UAUT is complex diagonal. Post-multiplying U by another diagonal matrix the diagonal entries can be made to be real and non-negative. Since their squares are the eigenvalues of A†A, they coincide with the singular values of A. (Note, about the eigen-decomposition of a complex symmetric matrix A, the Jordan normal form of A may not be diagonal, therefore A may not be diagonalized by any similarity transformation.)Using the Jordan normal form, one can prove that every square real matrix can be written as a product of two real symmetric matrices, and every square complex matrix can be written as a product of two complex symmetric matrices.[4]Every real non-singular matrix can be uniquely factored as the product of an orthogonal matrix and a symmetric positive definite matrix, which is called a polar decomposition. Singular matrices can also be factored, but not uniquely.A complex symmetric matrix may not be diagonalizable by similarity; every real symmetric matrix is diagonalizable by a real orthogonal similarity.Every complex symmetric matrix A can be diagonalized by unitary congruenceSymmetric n-by-n matrices of real functions appear as the Hessians of twice continuously differentiable functions of n real variables.Every quadratic form q on Rn can be uniquely written in the form q(x) = xTAx with a symmetric n-by-n matrix A. Because of the above spectral theorem, one can then say that every quadratic form, up to the choice of an orthonormal basis of Rn, "looks like"with real numbers λi. This considerably simplifies the study of quadratic forms, as well as the study of the level sets {x : q(x) = 1} which are generalizations of conic sections.This is important partly because the second-order behavior of every smooth multi-variable function is described by the quadratic form belonging to the function's Hessian; this is a consequence of Taylor's theorem.An n-by-n matrix A is said to be symmetrizable if there exists an invertible diagonal matrix D and symmetric matrix S such that A = DS.The transpose of a symmetrizable matrix is symmetrizable, since AT = (DS)T = SD = D−1 (DSD) and DSD is symmetric. A matrix A = (aij) is symmetrizable if and only if the following conditions are met:Other types of symmetry or pattern in square matrices have special names; see for example:See also symmetry in mathematics.
Multigrid method
Multigrid (MG) methods in numerical analysis are algorithms for solving differential equations using a hierarchy of discretizations. They are an example of a class of techniques called multiresolution methods, very useful in problems exhibiting multiple scales of behavior. For example, many basic relaxation methods exhibit different rates of convergence for short- and long-wavelength components, suggesting these different scales be treated differently, as in a Fourier analysis approach to multigrid.[1] MG methods can be used as solvers as well as preconditioners.The main idea of multigrid is to accelerate the convergence of a basic iterative method (known as relaxation, which generally reduces short-wavelength error) by a global correction of the fine grid solution approximation from time to time, accomplished by solving a coarse problem. The coarse problem, while cheaper to solve, is similar to the fine grid problem in that it also has short- and long-wavelength errors. It can also be solved by a combination of relaxation and appeal to still coarser grids. This recursive process is repeated until a grid is reached where the cost of direct solution there is negligible compared to the cost of one relaxation sweep on the fine grid. This multigrid cycle typically reduces all error components by a fixed amount bounded well below one, independent of the fine grid mesh size. The typical application for multigrid is in the numerical solution of elliptic partial differential equations in two or more dimensions.[2]Multigrid methods can be applied in combination with any of the common discretization techniques. For example, the finite element method may be recast as a multigrid method.[3] In these cases, multigrid methods are among the fastest solution techniques known today. In contrast to other methods, multigrid methods are general in that they can treat arbitrary regions and boundary conditions. They do not depend on the separability of the equations or other special properties of the equation.  They have also been widely used for more-complicated non-symmetric and nonlinear systems of equations, like the Lamé equations of elasticity or the Navier-Stokes equations.[4]There are many variations of multigrid algorithms, but the common features are that a hierarchy of discretizations (grids) is considered. The important steps are:[5][6]This approach has the advantage over other methods that it often scales linearly with the number of discrete nodes used. In other words, it can solve these problems to a given accuracy in a number of operations that is proportional to the number of unknowns.Multigrid methods can be generalized in many different ways.  They can be applied naturally in a time-stepping solution of parabolic partial differential equations, or they can be applied directly to time-dependent partial differential equations.[7]  Research on multilevel techniques for hyperbolic partial differential equations is underway.[8] Multigrid methods can also be applied to integral equations, or for problems in statistical physics.[9]Other extensions of multigrid methods include techniques where no partial differential equation nor geometrical problem background is used to construct the multilevel hierarchy.[10] Such algebraic multigrid methods (AMG) construct their hierarchy of operators directly from the system matrix. In classical AMG, the levels of the hierarchy are simply subsets of unknowns without any geometric interpretation. (More generally, coarse grid unknowns can be particular linear combinations of fine grid unknowns.) Thus, AMG methods become black-box solvers for certain classes of sparse matrices. AMG is regarded as advantageous mainly where geometric multigrid is too difficult to apply,[11] but is often used simply because it avoids the coding necessary for a true multigrid implementation. While classical AMG was developed first, a related algebraic method is known as smoothed aggregation (SA).Another set of multiresolution methods is based upon wavelets. These wavelet methods can be combined with multigrid methods.[12][13] For example, one use of wavelets is to reformulate the finite element approach in terms of a multilevel method.[14]Adaptive multigrid exhibits adaptive mesh refinement, that is, it adjusts the grid as the computation proceeds, in a manner dependent upon the computation itself.[15] The idea is to increase resolution of the grid only in regions of the solution where it is needed.Multigrid methods have also been adopted for the solution of initial value problems.[16]Of particular interest here are parallel-in-time multigrid methods:[17]in contrast to classical Runge-Kutta or linear multistep methods, they can offer concurrency in temporal direction.The well known Parareal parallel-in-time integration method can also be reformulated as a two-level multigrid in time.
Singular value decomposition
The singular-value decomposition can be computed using the following observations:Applications that employ the SVD include computing the pseudoinverse, least squares fitting of data, multivariable control, matrix approximation, and determining the rank, range and null space of a matrix.Suppose M is a m × n matrix whose entries come from the field K, which is either the field of real numbers  or the field of complex numbers. Then there exists a factorization, called a 'singular value decomposition' of M, of the formwhere The diagonal entries σi of Σ are known as the singular values of M. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, Σ, is uniquely determined by M (though not the matrices U and V if M is not square, see below).In the special, yet common case when M is an m × m real square matrix with positive determinant: U, V∗, and Σ are real m × m matrices as well. Σ can be regarded as a scaling matrix, and U, V∗ can be viewed as rotation matrices. Thus the expression UΣV∗ can be intuitively interpreted as a composition of three geometrical transformations: a rotation or reflection, a scaling, and another rotation or reflection. For instance, the figure explains how a shear matrix can be described as such a sequence.Using the polar decomposition theorem, we can also consider M = RP as the composition of a stretch (positive definite matrix P = VΣV∗) with eigenvalue scale factors σi along the orthogonal eigenvectors Vi of P, followed by a single rotation (unitary matrix R = UV∗).  If the rotation is done first, M = P'R, then R is the same and P' = UΣU∗ has the same eigenvalues, but is stretched along different (post-rotated) directions.  This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix P) to arbitrary matrices (M = RP) which both stretch and rotate.As shown in the figure, the singular values can be interpreted as the semiaxis of an ellipse in 2D. This concept can be generalized to n-dimensional Euclidean space, with the singular values of any n × n square matrix being viewed as the semiaxis of an n-dimensional ellipsoid. Similarly, the singular values of any m × n matrix can be viewed as the semiaxis of an n-dimensional ellipsoid in m-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See below for further details.Because U and V are unitary, we know that the columns U1, ..., Um of U yield an orthonormal basis of Km and the columns V1, ..., Vn of V yield an orthonormal basis of Kn (with respect to the standard scalar products on these spaces).The linear transformationhas a particularly simple description with respect to these orthonormal bases: we havewhere σi is the i-th diagonal entry of Σ, and T(Vi) = 0 for i > min(m,n).The geometric content of the SVD theorem can thus be summarized as follows: for every linear map T : Kn → Km one can find orthonormal bases of Kn and Km such that T maps the i-th basis vector of Kn to a non-negative multiple of the i-th basis vector of Km, and sends the left-over basis vectors to zero. With respect to these bases, the map T is therefore represented by a diagonal matrix with non-negative real diagonal entries.To get a more visual flavour of singular values and SVD factorization — at least when working on real vector spaces — consider the sphere S of radius one in Rn. The linear map T maps this sphere onto an ellipsoid in Rm. Non-zero singular values are simply the lengths of the semi-axes of this ellipsoid. Especially when n = m, and all the singular values are distinct and non-zero, the SVD of the linear map T can be easily analysed as a succession of three consecutive moves: consider the ellipsoid T(S) and specifically its axes; then consider the directions in Rn sent by T onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry V∗ sending these directions to the coordinate axes of Rn. On a second move, apply an endomorphism D diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of T(S) as stretching  coefficients. The composition D ∘ V∗ then sends the unit-sphere onto an ellipsoid isometric to T(S). To define the third and last move U, apply an isometry to this ellipsoid so as to carry it over T(S). As can be easily checked, the composition U ∘ D ∘ V∗ coincides with T.Consider the 4 × 5 matrixA singular-value decomposition of this matrix is given by UΣV∗Notice Σ is zero outside of the diagonal (grey italics) and one diagonal element is zero (red bold). Furthermore, because the matrices U and V∗ are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below.  In this case, because U and V∗ are real valued, each is an orthogonal matrix.is also a valid singular-value decomposition.In any singular-value decompositionthe diagonal entries of Σ are equal to the singular values of M. The first p = min(m, n) columns of U and V are, respectively, left- and right-singular vectors for the corresponding singular values.  Consequently, the above theorem implies that:As an exception, the left and right-singular vectors of singular value 0 comprise all unit vectors in the kernel and cokernel, respectively, of M, which by the rank–nullity theorem cannot be the same dimension if m ≠ n.  Even if all singular values are nonzero, if m > n then the cokernel is nontrivial, in which case U is padded with m − n orthogonal vectors from the cokernel.  Conversely, if m < n, then V is padded by n − m orthogonal vectors from the kernel.  However, if the singular value of 0 exists, the extra columns of U or V already appear as left or right-singular vectors.Non-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor eiφ (for the real case up to a sign).  Consequently, if all singular values of a square matrix M are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of U by a unit-phase factor and simultaneous multiplication of the corresponding column of V by the same unit-phase factor.In general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both U and V spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of U and V spanning the kernel and cokernel, respectively, of M.The singular-value decomposition is very general in the sense that it can be applied to any m × n matrix whereas eigenvalue decomposition can only be applied to diagonalizable matrices. Nevertheless, the two decompositions are related.Given an SVD of M, as described above, the following two relations hold:The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides.  Consequently:In the special case that M is a normal matrix, which by definition must be square, the spectral theorem says that it can be unitarily diagonalized using a basis of eigenvectors, so that it can be written M = UDU∗ for a unitary matrix U and a diagonal matrix D.  When M is also positive semi-definite, the decomposition M = UDU∗ is also a singular-value decomposition.  Otherwise, it can be recast as an SVD by moving the phase of each σi to either its corresponding Vi or Ui.  The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem:  M = SR, where S = UΣU* is positive semidefinite and normal, and  R = UV* is unitary.The singular-value decomposition can be used for computing the pseudoinverse of a matrix. Indeed, the pseudoinverse of the matrix M with singular-value decomposition M = UΣV∗ iswhere Σ+ is the pseudoinverse of Σ, which is formed by replacing every non-zero diagonal entry by its reciprocal and transposing the resulting matrix. The pseudoinverse is one way to solve linear least squares problems.A set of homogeneous linear equations can be written as Ax = 0 for a matrix A and vector x. A typical situation is that A is known and a non-zero x is to be determined which satisfies the equation. Such an x belongs to A's null space and is sometimes called a (right) null vector of A. The vector x can be characterized as a right-singular vector corresponding to a singular value of A that is zero. This observation means that if A is a square matrix and has no vanishing singular value, the equation has no non-zero x as a solution.  It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero x satisfying x∗A = 0, with x∗ denoting the conjugate transpose of x, is called a left null vector of A.A total least squares problem refers to determining the vector x which minimizes the 2-norm of a vector Ax under the constraint ||x|| = 1. The solution turns out to be the right-singular vector of A corresponding to the smallest singular value.Another application of the SVD is that it provides an explicit representation of the range and null space of a matrix M. The right-singular vectors corresponding to vanishing singular values of M span the null space of M and the left-singular vectors corresponding to the non-zero singular values of M span the range of M. E.g., in the above example the null space is spanned by the last two columns of V and the range is spanned by the first three columns of U.As a consequence, the rank of M equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in Σ. In numerical linear algebra the singular values can be used to determine the effective rank of a matrix, as rounding error may lead to small but non-zero singular values in a rank deficient matrix. Singular values beyond a significant gap are assumed to be numerically equivalent to zero.Here Ui and Vi are the i-th columns of the corresponding SVD matrices, σi are the ordered singular values, and each Ai is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero σi is exactly the rank of the matrix.Separable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described[1] by a Gabor filter in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, reverse correlation, one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of U in the SVD factorization is then a Gabor while the first column of V represents the time modulation (or vice versa). One may then define an index of separability,which is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.[2]It is possible to use the SVD of a square matrix A to determine the orthogonal matrix O closest to A. The closeness of fit is measured by the Frobenius norm of O − A. The solution is the product UV∗.[3] This intuitively makes sense because an orthogonal matrix would have the decomposition UIV∗ where I is the identity matrix, so that if A = UΣV∗ then the product A = UV∗ amounts to replacing the singular values with ones.A similar problem, with interesting applications in shape analysis, is the orthogonal Procrustes problem, which consists of finding an orthogonal matrix O which most closely maps A to B. Specifically,This problem is equivalent to finding the nearest orthogonal matrix to a given matrix M = ATB.The Kabsch algorithm (called Wahba's problem in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules.The SVD and pseudoinverse have been successfully applied to signal processing[4], image processing[5] and big data, e.g., in genomic signal processing.[6][7][8][9]The SVD is also applied extensively to the study of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.The SVD also plays a crucial role in the field of quantum information, in a form often referred to as the Schmidt decomposition. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be entangled: if the rank of the Σ matrix is larger than one.One application of SVD to rather large matrices is in numerical weather prediction, where Lanczos methods are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. The output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an ensemble forecast, giving a handle on some of the uncertainty that should be allowed for around the current central prediction.SVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with radial basis functions to interpolate solutions to three-dimensional unsteady flow problems.[10]Singular-value decomposition is used in recommender systems to predict people's item ratings.[11] Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.[12]Another code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository [13] implemented by Alexandros Ioannidis. The original SVD algorithm,[14] which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.Low-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease outbreak detection .[15] A combination of SVD and higher-order SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance.[16]An eigenvalue λ of a matrix M is characterized by the algebraic relation Mu = λu. When M is Hermitian, a variational characterization is also available. Let M be a real n × n symmetric matrix. DefineBy the extreme value theorem, this continuous function attains a maximum at some u when restricted to the closed unit sphere {||x|| ≤ 1}. By the Lagrange multipliers theorem, u necessarily satisfieswhere the nabla symbol, ∇, is the del operator.A short calculation shows the above leads to Mu = λu (symmetry of M is needed here). Therefore, λ is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.This section gives these two arguments for existence of singular-value decomposition.This implies thatwhere the subscripts on the identity matrices are used to remark that they are of different dimensions.Let us now defineThen,For V1 we already have V2 to make it unitary. Now, definewhich is the desired result:Notice the argument could begin with diagonalizing MM∗ rather than M∗M (This shows directly that MM∗ and M∗M have the same non-zero eigenvalues).The singular values can also be characterized as the maxima of uTMv, considered as a function of u and v, over particular subspaces. The singular vectors are the values of u and v where these maxima are attained.Consider the function σ restricted to Sm−1 × Sn−1. Since both Sm−1 and Sn−1 are compact sets, their product is also compact.  Furthermore, since σ is continuous, it attains a largest value for at least one pair of vectors u ∈ Sm−1 and v ∈ Sn−1. This largest value is denoted σ1 and the corresponding vectors are denoted u1 and v1. Since σ1 is the largest value of σ(u, v) it must be non-negative. If it were negative, changing the sign of either u1 or v1 would make it positive and therefore larger.Statement. u1, v1 are left and right-singular vectors of M with corresponding singular value σ1.Proof. Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:After some algebra, this becomesPlugging this into the pair of equations above, we haveThis proves the statement.More singular vectors and singular values can be found by maximizing σ(u, v) over normalized u, v which are orthogonal to u1 and v1, respectively.The passage from real to complex is similar to the eigenvalue case.The SVD of a matrix M is typically computed by a two-step procedure. In the first step, the matrix is reduced to a bidiagonal matrix. This takes O(mn2) floating-point operations (flop), assuming that m ≥ n. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an iterative method (as with eigenvalue algorithms). However, in practice it suffices to compute the SVD up to a certain precision, like the machine epsilon. If this precision is considered constant, then the second step takes O(n) iterations, each costing O(n) flops. Thus, the first step is more expensive, and the overall cost is O(mn2) flops (Trefethen & Bau III 1997, Lecture 31).The first step can be done using Householder reflections for a cost of 4mn2 − 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the QR decomposition and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops (Trefethen & Bau III 1997, Lecture 31).The second step can be done by a variant of the QR algorithm for the computation of eigenvalues, which was first described by Golub & Kahan (1965). The LAPACK subroutine DBDSQR[18] implements this iterative method, with some modifications to cover the case where the singular values are very small (Demmel & Kahan 1990). Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD[19] routine for the computation of the singular-value decomposition.The same algorithm is implemented in the GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi orthogonalization in step 2 (GSL Team 2007). This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 × 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 × 2 eigenvalue methods (Golub & Van Loan 1996, §8.6.3). Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms (Trefethen & Bau III 1997, Lecture 31).There is an alternative way which is not explicitly using the eigenvalue decomposition.[20] Usually the singular-value problem of  a matrix M is converted into an equivalent symmetric eigenvalue problem such as M M*, M*M, or The approaches using eigenvalue decompositions are based on QR algorithm which is well-developed to be stable and fast. Note that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives M ⇒ Q R and LQ decomposition of R gives R ⇒ L P*. Thus, at every iteration, we have M ⇒ Q L P*, update M ⇐ L and repeat the orthogonalizations.Eventually, QR decomposition and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively. This approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation.  But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the QR algorithm can calculate the eigenvalue decomposition.In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required.  Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD.  The following can be distinguished for an m×n matrix M of rank r:Only the n column vectors of U corresponding to the row vectors of V* are calculated. The remaining column vectors of U are not calculated. This is significantly quicker and more economical than the full SVD if n ≪ m. The matrix U'n is thus m×n, Σn is n×n diagonal, and V is n×n.The first stage in the calculation of a thin SVD will usually be a QR decomposition of M, which can make for a significantly quicker calculation if n ≪ m.Only the r column vectors of U and r row vectors of V* corresponding to the non-zero singular values Σr are calculated. The remaining vectors of U and V* are not calculated. This is quicker and more economical than the thin SVD if r ≪ n. The matrix Ur is thus m×r, Σr is r×r diagonal, and Vr* is r×n.Only the t column vectors of U and t row vectors of V* corresponding to the t largest singular values Σt are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if t≪r. The matrix Ut is thus m×t, Σt is t×t diagonal, and Vt* is t×n.The sum of the k largest singular values of M is a matrix norm, the Ky Fan k-norm of M. [21]The first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the operator norm of M as a linear operator with respect to the Euclidean norms of Km and Kn. In other words, the Ky Fan 1-norm is the operator norm induced by the standard ℓ2 Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator M on (possibly infinite-dimensional) Hilbert spacesBut, in the matrix case, (M* M)½ is a normal matrix, so ||M* M||½ is the largest eigenvalue of (M* M)½, i.e. the largest singular value of M.The last of the Ky Fan norms, the sum of all singular values, is the trace norm (also known as the 'nuclear norm'), defined by ||M|| = Tr[(M* M)½] (the eigenvalues of M* M are the squares of the singular values).The singular values are related to another norm on the space of operators. Consider the Hilbert–Schmidt inner product on the n × n matrices, defined bySo the induced norm isSince the trace is invariant under unitary equivalence, this showswhere σi are the singular values of M. This is called the Frobenius norm, Schatten 2-norm, or Hilbert–Schmidt norm of M. Direct calculation shows that the Frobenius norm of M = (mij) coincides with:In addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the Schatten norm.Two types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.The SVD singular values of a matrix A are unique and are invariant with respect to left and/or right unitary transformations of  A. In other words, the singular values of UAV, for unitary U and V, are equal to the singular values of A. This is an important property for applications in which it is necessary to preserve Euclidean distances, and invariance with respect to rotations.The Scale-Invariant SVD, or SI-SVD,[23] is analogous to the conventional SVD except that its uniquely-determined singular values are invariant withrespect to diagonal transformations of  A. In other words, the singular values of DAE, for nonsingular diagonal matrices D and E, are  equal to the singular values of A. This is an important property for applications for which invariance to the choice of units on variables (e.g., metric versus imperial units) is needed.TP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:The factorization M = UΣV∗ can be extended to a bounded operator M on a separable Hilbert space H. Namely, for any bounded operator M, there exist a partial isometry U, a unitary V, a measure space (X, μ), and a non-negative measurable f such thatThis can be shown by mimicking the linear algebraic argument for the matricial case above. VTf V* is the unique positive square root of M*M, as given by the Borel functional calculus for self adjoint operators. The reason why U need not be unitary is because, unlike the finite-dimensional case, given an isometry U1 with nontrivial kernel, a suitable U2 may not be found such thatis a unitary operator.As for matrices, the singular-value factorization is equivalent to the polar decomposition for operators: we can simply writeand notice that U V* is still a partial isometry while VTf V* is positive.The notion of singular values and left/right-singular vectors can be extended to compact operator on Hilbert space as they have a discrete spectrum. If T is compact, every non-zero λ in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If M is compact, so is M∗M. Applying the diagonalization result, the unitary image of its positive square root Tf  has a set of orthonormal eigenvectors {ei}  corresponding to strictly positive eigenvalues {σi}. For any ψ ∈ H,where the series converges in the norm topology on H. Notice how this resembles the expression from the finite-dimensional case. σi are called the singular values of M. {Uei}  (resp. {Vei} ) can be considered the left-singular (resp. right-singular) vectors of M.Compact operators on a Hilbert space are the closure of finite-rank operators in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:The singular-value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular-value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale Young in 1936;[24] they saw it as a generalization of the principal axis transformation for Hermitian matrices.Practical methods for computing the SVD date back to Kogbetliantz in 1954, 1955 and Hestenes in 1958.[25] resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of  Gene Golub and William Kahan published in 1965,[26] which uses Householder transformations or reflections.In 1970, Golub and Christian Reinsch[27] published a variant of the Golub/Kahan algorithm that is still the one most-used today.
Linear form
In linear algebra, a linear functional or linear form (also called a one-form or covector) is a linear map from a vector space to its field of scalars. In ℝn, if vectors are represented as column vectors, then linear functionals are represented as row vectors, and their action on vectors is given by the dot product, or the matrix product with the row vector on the left and the column vector on the right.  In general, if V is a vector space over a field k, then a linear functional f is a function from V to k that is linear:The set of all linear functionals from V to k, Homk(V,k), forms a vector space over k with the addition of the operations of addition and scalar multiplication (defined pointwise).  This space is called the dual space of V, or sometimes the algebraic dual space, to distinguish it from the continuous dual space.  It is often written V∗, V′, or Vᐯ when the field k is understood.If V is a topological vector space, the space of continuous linear functionals —  the continuous dual — is often simply called the dual space.  If V is a Banach space, then so is its (continuous) dual.  To distinguish the ordinary dual space from the continuous dual space, the former is sometimes called the algebraic dual space.  In finite dimensions, every linear functional is continuous, so the continuous dual is the same as the algebraic dual, but in infinite dimensions the continuous dual is a proper subspace of the algebraic dual.Suppose that vectors in the real coordinate space Rn are represented as column vectorsFor each row vector [a1 … an] there is a linear functional f defined byand each linear functional can be expressed in this form.Linear functionals first appeared in functional analysis, the study of vector spaces of functions.  A typical example of a linear functional is integration: the linear transformation defined by the Riemann integralis a linear functional from the vector space C[a, b] of continuous functions on the interval [a, b] to the real numbers. The linearity of I follows from the standard facts about the integral:Let Pn denote the vector space of real-valued polynomial functions of degree ≤n defined on an interval [a, b].  If c ∈ [a, b], then let evc : Pn → R be the evaluation functionalThe mapping f → f(c) is linear sinceIf x0, ..., xn are n + 1 distinct points in [a, b], then the evaluation functionals evxi, i = 0, 1, ..., n form a basis of the dual space of Pn.  (Lax (1996) proves this last fact using Lagrange interpolation.)The integration functional I defined above defines a linear functional on the subspace Pn of polynomials of degree ≤ n. If x0, ..., xn are n + 1 distinct points in [a, b], then there are coefficients a0, ..., an for whichfor all f ∈ Pn. This forms the foundation of the theory of numerical quadrature.This follows from the fact that the linear functionals evxi : f → f(xi) defined above form a basis of the dual space of Pn.[1]Linear functionals are particularly important in quantum mechanics.  Quantum mechanical systems are represented by Hilbert spaces, which are anti–isomorphic to their own dual spaces.  A state of a quantum mechanical system can be  identified with a linear functional.  For more information see bra–ket notation.In the theory of generalized functions, certain kinds of generalized functions called distributions can be realized as linear functionals on spaces of test functions.In finite dimensions, a linear functional can be visualized in terms of its level sets.  In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel hyperplanes.  This method of visualizing linear functionals is sometimes introduced in general relativity texts, such as Gravitation by Misner, Thorne & Wheeler (1973).Every non-degenerate bilinear form on a finite-dimensional vector space V induces an isomorphism V → V∗ : v ↦ v∗ such that where the bilinear form on V is denoted ⟨ , ⟩ (for instance, in Euclidean space ⟨v, w⟩ = v ⋅ w is the dot product of v and w).The inverse isomorphism is V∗ → V : v∗ ↦ v, where v is the unique element of V such thatThe above defined vector v∗ ∈ V∗ is said to be the dual vector of v ∈ V.In an infinite dimensional Hilbert space, analogous results hold by the Riesz representation theorem.  There is a mapping V → V∗ into the continuous dual space V∗.  However, this mapping is antilinear rather than linear.Or, more succinctly,where δ is the Kronecker delta.  Here the superscripts of the basis functionals are not exponents but are instead contravariant indices.due to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals.  ThenSo each component of a linear functional can be extracted by applying the functional to the corresponding basis vector.In higher dimensions, this generalizes as follows
Numerical analysis
Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. As an aspect of mathematics and computer science that generates, analyzes, and implements algorithms, the growth in power and the revolution in computing has raised the use of realistic mathematical models in science and engineering, and complex numerical analysis is required to provide solutions to these more involved models of the world. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.Before the advent of modern computers, numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.One of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.[2]Numerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of the square root of 2, modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.The overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following:The rest of this section outlines several important themes of numerical analysis.The field of numerical analysis predates the invention of modern computers by many centuries. Linear interpolation was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like Newton's method, Lagrange interpolation polynomial, Gaussian elimination, or Euler's method.To facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients.  Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions.  The canonical work in the field is the NIST publication edited by Abramowitz and Stegun, a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points.  The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.The mechanical calculator was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.Direct vs iterative methodsConsider the problem of solvingfor the unknown quantity x.For the iterative method, apply the bisection method to f(x) = 3x3 − 24. The initial values are a = 0, b = 3, f(a) = −24, f(b) = 57.We conclude from this table that the solution is between 1.875 and 2.0625. The algorithm might return any number in that range with an error less than 0.2.In a two-hour race, we have measured the speed of the car at three instants and recorded them in the following table.A discretization would be to say that the speed of the car was constant from 0:00 to 0:40, then from 0:40 to 1:20 and finally from 1:20 to 2:00. For instance, the total distance traveled in the first 40 minutes is approximately (7003720000000000000♠2/3 h × 7001388888888888888♠140 km/h) = 7004933000000000000♠93.3 km. This would allow us to estimate the total distance traveled as 7004933000000000000♠93.3 km + 7005100000000000000♠100 km + 7005120000000000000♠120 km = 7005313300000000000♠313.3 km, which is an example of numerical integration (see below) using a Riemann sum, because displacement is the integral of velocity.Ill-conditioned problem: Take the function f(x) = 1/(x − 1). Note that f(1.1) = 10 and f(1.001) = 1000: a change in x of less than 0.1 turns into a change in f(x) of nearly 1000. Evaluating f(x) near x = 1 is an ill-conditioned problem.Well-conditioned problem: By contrast, evaluating the same function f(x) = 1/(x − 1) near x = 10 is a well-conditioned problem. For instance, f(10) = 1/9 ≈ 0.111 and f(11) = 0.1: a modest change in x leads to a modest change in f(x).Direct methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the simplex method of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).In contrast to direct methods, iterative methods are not expected to terminate in a finite number of steps. Starting from an initial guess, iterative methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the bisection method, and Jacobi iteration. In computational matrix algebra, iterative methods are generally needed for large problems.Iterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.Furthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called discretization. For example, the solution of a differential equation is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.The study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.Round-off errors arise because it is impossible to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).What does it mean when we say that the truncation error is created when we approximate a mathematical procedure?  We know that to integrate a function exactly requires one to find the sum of infinite trapezoids.  But numerically one can find the sum of only finite trapezoids, and hence the approximation of the mathematical procedure.  Similarly, to differentiate a function, the differential element approaches zero but numerically we can only choose a finite value of the differential element.Numerical stability is an important notion in numerical analysis. An algorithm is called numerically stable if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is well-conditioned, meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is ill-conditioned, then any small error in the data will grow to be a large error.Both the original problem and the algorithm used to solve that problem can be well-conditioned and/or ill-conditioned, and any combination is possible.Observe that the Babylonian method converges quickly regardless of the initial guess, whereas Method X converges extremely slowly with initial guess x0 = 1.4 and diverges for initial guess x0 = 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable.The field of numerical analysis includes many sub-disciplines. Some of the major ones are:Interpolation: We have observed the temperature to vary from 20 degrees Celsius at 1:00 to 14 degrees at 3:00. A linear interpolation of this data would conclude that it was 17 degrees at 2:00 and 18.5 degrees at 1:30pm.Extrapolation: If the gross domestic product of a country has been growing an average of 5% per year and was 100 billion dollars last year, we might extrapolate that it will be 105 billion dollars this year.Regression: In linear regression, given n points, we compute a line that passes as close as possible to those n points.Optimization: Say you sell lemonade at a lemonade stand, and notice that at $1, you can sell 197 glasses of lemonade per day, and that for each increase of $0.01, you will sell one glass of lemonade less per day. If you could charge $1.485, you would maximize your profit, but due to the constraint of having to charge a whole cent amount, charging $1.48 or $1.49 per glass will both yield the maximum income of $220.52 per day.Differential equation: If you set up 100 fans to blow air from one end of the room to the other and then you drop a feather into the wind, what happens? The feather will follow the air currents, which may be very complex. One approximation is to measure the speed at which the air is blowing near the feather every second, and advance the simulated feather as if it were moving in a straight line at that same speed for one second, before measuring the wind speed again. This is called the Euler method for solving an ordinary differential equation.One of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control round-off errors arising from the use of floating point arithmetic.Interpolation solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?Extrapolation is very similar to interpolation, except that now we want to find the value of the unknown function at a point which is outside the given points.Regression is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), we want to determine the unknown function. The least squares-method is one popular way to achieve this.Much effort has been put in the development of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.Root-finding algorithms are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is differentiable and the derivative is known, then Newton's method is a popular choice. Linearization is another technique for solving nonlinear equations.Several important problems can be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm[4] is based on the singular value decomposition. The corresponding tool in statistics is called principal component analysis.Optimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some constraints.The field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.The method of Lagrange multipliers can be used to reduce optimization problems with constraints to unconstrained optimization problems.Numerical integration, in some instances also known as numerical quadrature, asks for the value of a definite integral. Popular methods use one of the Newton–Cotes formulas (like the midpoint rule or Simpson's rule) or Gaussian quadrature. These methods rely on a "divide and conquer" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use Monte Carlo or quasi-Monte Carlo methods (see Monte Carlo integration), or, in modestly large dimensions, the method of sparse grids.Numerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.Partial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a finite element method, a finite difference method, or (particularly in engineering) a finite volume method. The theoretical justification of these methods often involves theorems from functional analysis. This reduces the problem to the solution of an algebraic equation.Since the late twentieth century, most algorithms are implemented in a variety of programming languages. The Netlib repository contains various collections of software routines for numerical problems, mostly in Fortran and C. Commercial products implementing many different numerical algorithms include the IMSL and NAG libraries; a free-software alternative is the GNU Scientific Library.There are several popular numerical computing applications such as MATLAB, TK Solver, S-PLUS, and IDL as well as free and open source alternatives such as FreeMat, Scilab, GNU Octave (similar to Matlab), and IT++ (a C++ library). There are also programming languages such as R (similar to S-PLUS) and Python with libraries such as NumPy, SciPy and SymPy. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.[5][6]Many computer algebra systems such as Mathematica also benefit from the availability of arbitrary precision arithmetic which can provide more accurate results.Also, any spreadsheet software can be used to solve simple problems relating to numerical analysis.JournalsOnline textsOnline course material
Isomorphism
In mathematics, an isomorphism (from the Ancient Greek: ἴσος isos "equal", and μορφή morphe "form" or "shape") is a homomorphism or morphism (i.e. a mathematical mapping) that can be reversed by an inverse morphism. Two mathematical objects are isomorphic if an isomorphism exists between them.  An automorphism is an isomorphism whose source and target coincide.  The interest of isomorphisms lies in the fact that two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences.For most algebraic structures, including groups and rings, a homomorphism is an isomorphism if and only if it is bijective.In topology, where the morphisms are continuous functions, isomorphisms are also called homeomorphisms or bicontinuous functions. In mathematical analysis, where the morphisms are differentiable functions, isomorphisms are also called diffeomorphisms.A canonical isomorphism is a canonical map that is an isomorphism. Two objects are said to be canonically isomorphic if there is a canonical isomorphism between them. For example, the canonical map from a finite-dimensional vector space V to its second dual space is a canonical isomorphism; on the other hand, V is isomorphic to its dual space but not canonically in general.Isomorphisms are formalized using category theory.  A morphism f : X → Y in a category is an isomorphism if it admits a two-sided inverse, meaning that there is another morphism g : Y → X in that category such that gf = 1X and fg = 1Y, where 1X and 1Y are the identity morphisms of X and Y, respectively.[1]These structures are isomorphic under addition, under the following scheme:or in general (a,b) ↦ (3a + 4b) mod 6.For example, (1,1) + (1,0) = (0,1), which translates in the other system as 1 + 3 = 4.If one object consists of a set X with a binary relation R and the other object consists of a set Y with a binary relation S then an isomorphism from X to Y is a bijective function ƒ: X → Y such that:[2]S is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), an equivalence relation, or a relation with any other special properties, if and only if R is.Such an isomorphism is called an order isomorphism or (less commonly) an isotone isomorphism.If X = Y, then this is a relation-preserving automorphism.In a concrete category (that is, roughly speaking, a category whose objects are sets and morphisms are mappings between sets), such as the category of topological spaces or categories of algebraic objects like groups, rings, and modules, an isomorphism must be bijective on the underlying sets.  In algebraic categories (specifically, categories of varieties in the sense of universal algebra), an isomorphism is the same as a homomorphism which is bijective on underlying sets.  However, there are concrete categories in which bijective morphisms are not necessarily isomorphisms (such as the category of topological spaces), and there are categories in which each object admits an underlying set but in which isomorphisms need not be bijective (such as the homotopy category of CW-complexes).In abstract algebra, two basic isomorphisms are defined:Just as the automorphisms of an algebraic structure form a group, the isomorphisms between two algebras sharing a common structure form a heap. Letting a particular isomorphism identify the two structures turns this heap into a group.In mathematical analysis, the Laplace transform is an isomorphism mapping hard differential equations into easier algebraic equations.In category theory, let the category C consist of two classes, one of objects and the other of morphisms. Then a general definition of isomorphism that covers the previous and many other cases is: an isomorphism is a morphism ƒ: a → b that has an inverse, i.e. there exists a morphism g: b → a with ƒg = 1b and gƒ = 1a. For example, a bijective linear map is an isomorphism between vector spaces, and a bijective continuous function whose inverse is also continuous is an isomorphism between topological spaces, called a homeomorphism.In graph theory, an isomorphism between two graphs G and H is a bijective map f from the vertices of G to the vertices of H that preserves the "edge structure" in the sense that there is an edge from vertex u to vertex v in G if and only if there is an edge from ƒ(u) to ƒ(v) in H. See graph isomorphism.In mathematical analysis, an isomorphism between two Hilbert spaces is a bijection preserving addition, scalar multiplication, and inner product.In early theories of logical atomism, the formal relationship between facts and true propositions was theorized by Bertrand Russell and Ludwig Wittgenstein to be isomorphic. An example of this line of thinking can be found in Russell's Introduction to Mathematical Philosophy.In cybernetics, the good regulator or Conant–Ashby theorem is stated "Every good regulator of a system must be a model of that system". Whether regulated or self-regulating, an isomorphism is required between the regulator and processing parts of the system.In certain areas of mathematics, notably category theory, it is valuable to distinguish between equality on the one hand and isomorphism on the other.[3] Equality is when two objects are exactly the same, and everything that's true about one object is true about the other, while an isomorphism implies everything that's true about a designated part of one object's structure is true about the other's. For example, the setsare equal; they are merely different representations—the first an intensional one (in set builder notation), and the second extensional (by explicit enumeration)—of the same subset of the integers. By contrast, the sets {A,B,C} and {1,2,3} are not equal—the first has elements that are letters, while the second has elements that are numbers. These are isomorphic as sets, since finite sets are determined up to isomorphism by their cardinality (number of elements) and these both have three elements, but there are many choices of isomorphism—one isomorphism isand no one isomorphism is intrinsically better than any other.[note 1][note 2] On this view and in this sense, these two sets are not equal because one cannot consider them identical: one can choose an isomorphism between them, but that is a weaker claim than identity—and valid only in the context of the chosen isomorphism.Sometimes the isomorphisms can seem obvious and compelling, but are still not equalities. As a simple example, the genealogical relationships among Joe, John, and Bobby Kennedy are, in a real sense, the same as those among the American football quarterbacks in the Manning family: Archie, Peyton, and Eli. The father-son pairings and the elder-brother-younger-brother pairings correspond perfectly. That similarity between the two family structures illustrates the origin of the word isomorphism (Greek iso-, "same," and -morph, "form" or "shape"). But because the Kennedys are not the same people as the Mannings, the two genealogical structures are merely isomorphic and not equal.This corresponds to transforming a column vector (element of V) to a row vector (element of V*) by transpose, but a different choice of basis gives a different isomorphism: the isomorphism "depends on the choice of basis".More subtly, there is a map from a vector space V to its double dual V** = { x: V* → K} that does not depend on the choice of basis: For all v ∈ V and φ ∈ V*,However, there is a case where the distinction between natural isomorphism and equality is usually not made. That is for the objects that may be characterized by a universal property. In fact, there is a unique isomorphism, necessarily natural, between two objects sharing the same universal property. A typical example is the set of real numbers, which may be defined through infinite decimal expansion, infinite binary expansion, Cauchy sequences, Dedekind cuts and many other ways. Formally these constructions define different objects, which all are solutions of the same universal property. As these objects have exactly the same properties, one may forget the method of construction and considering them as equal. This is what everybody does when talking of "the set of the real numbers". The same occurs with quotient spaces: they are commonly constructed as sets of equivalence classes. However, talking of set of sets may be counterintuitive, and quotient spaces are commonly considered as a pair of a set of undetermined objects, often called "points", and a surjective map onto this set.If one wishes to draw a distinction between an arbitrary isomorphism (one that depends on a choice) and a natural isomorphism (one that can be done consistently), one may write ≈ for an unnatural isomorphism and ≅ for a natural isomorphism, as in V ≈  V* and V ≅ V**.This convention is not universally followed, and authors who wish to distinguish between unnatural isomorphisms and natural isomorphisms will generally explicitly state the distinction.Generally, saying that two objects are equal is reserved for when there is a notion of a larger (ambient) space that these objects live in. Most often, one speaks of equality of two subsets of a given set (as in the integer set example above), but not of two objects abstractly presented. For example, the 2-dimensional unit sphere in 3-dimensional spacewhich can be presented as the one-point compactification of the complex plane C ∪ {∞} or as the complex projective line (a quotient space)are three different descriptions for a mathematical object, all of which are isomorphic, but not equal because they are not all subsets of a single space: the first is a subset of R3, the second is C ≅ R2[note 3] plus an additional point, and the third is a subquotient of C2In the context of category theory, objects are usually at most isomorphic—indeed, a motivation for the development of category theory was showing that different constructions in homology theory yielded equivalent (isomorphic) groups. Given maps between two objects X and Y, however, one asks if they are equal or not (they are both elements of the set Hom(X, Y), hence equality is the proper relationship), particularly in commutative diagrams.
Relative change and difference
In any quantitative science, the terms relative change and relative difference are used to compare two quantities while taking into account the "sizes" of the things being compared. The comparison is expressed as a ratio and is a unitless number. By multiplying these ratios by 100 they can be expressed as percentages so the terms percentage change, percent(age) difference, or relative percentage difference are also commonly used. The distinction between "change" and "difference" depends on whether or not one of the quantities being compared is considered a standard or reference or starting value. When this occurs, the term relative change (with respect to the reference value) is used and otherwise the term relative difference is preferred. Relative difference is often used as a quantitative indicator of quality assurance and quality control for repeated measurements where the outcomes are expected to be the same. A special case of percent change (relative change expressed as a percentage) called percent error occurs in measuring situations where the reference value is the accepted or actual value (perhaps theoretically determined) and the value being compared to it is experimentally determined (by measurement).Given two numerical quantities, x and y, their difference, Δ = x − y, can be called their actual difference. When y is a reference value (a theoretical/actual/correct/accepted/optimal/starting, etc. value; the value that x is being compared to) then Δ is called their actual change. When there is no reference value, the sign of Δ has little meaning in the comparison of the two values since it doesn't matter which of the two values is written first, so one often works with |Δ| = |x − y|, the absolute difference instead of Δ, in these situations. Even when there is a reference value, if it doesn't matter whether the compared value is larger or smaller than the reference value, the absolute difference can be considered in place of the actual change.The absolute difference between two values is not always a good way to compare the numbers. For instance, the absolute difference of 1 between 6 and 5 is more significant than the same absolute difference between 100,000,001 and 100,000,000. We can adjust the comparison to take into account the "size" of the quantities involved, by defining, for positive values of xreference:The relative change is not defined if the reference value (xreference) is zero.For values greater than the reference value, the relative change should be a positive number and for values that are smaller, the relative change should be negative. The formula given above behaves in this way only if xreference is positive, and reverses this behavior if xreference is negative. For example, if we are calibrating a thermometer which reads −6 °C when it should read −10 °C, this formula for relative change (which would be called relative error in this application) gives ((−6) − (−10)) / (−10) = 4/−10 = −0.4, yet the reading is too high. To fix this problem we alter the definition of relative change so that it works correctly for all nonzero values of xreference:If the relationship of the value with respect to the reference value (that is, larger or smaller) does not matter in a particular application, the absolute difference may be used in place of the actual change in the above formula to produce a value for the relative change which is always non-negative.Defining relative difference is not as easy as defining relative change since there is no "correct" value to scale the absolute difference with. As a result, there are many options for how to define relative difference and which one is used depends on what the comparison is being used for. In general we can say that the absolute difference |Δ| is being scaled by some function of the values x and y, say f(x, y).[1]As with relative change, the relative difference is undefined if f(x, y) is zero.Several common choices for the function f(x, y) would be:Measures of relative difference are unitless numbers expressed as a fraction. Corresponding values of percent difference would be obtained by multiplying these values by 100 (and appending the % sign to indicate that the value is a percentage).One way to define the relative difference of two numbers is to take their absolute difference divided by the maximum absolute value of the two numbers.if at least one of the values does not equal zero. This approach is especially useful when comparing floating point values in programming languages for equality with a certain tolerance.[2] Another application is in the computation of approximation errors when the relative error of a measurement is required.Another way to define the relative difference of two numbers is to take their absolute difference divided by some functional value of the two numbers, for example, the absolute value of their arithmetic mean:This approach is often used when the two numbers reflect a change in some single underlying entity.[citation needed] A problem with the above approach arises when the functional value is zero. In this example, if x and y have the same magnitude but opposite sign, thenwhich causes division by 0.  So it may be better to replace the denominator with the average of the absolute values of x and y:[citation needed]Percent Error is a special case of the percentage form of relative change calculated from the absolute change between the experimental (measured) and theoretical (accepted) values, and dividing by the theoretical (accepted) value.The terms "Experimental" and "Theoretical" used in the equation above are commonly replaced with similar terms. Other terms used for experimental could be "measured," "calculated," or "actual" and another term used for theoretical could be "accepted."  Experimental value is what has been derived by use of calculation and/or measurement and is having its accuracy tested against the theoretical value, a value that is accepted by the scientific community or a value that could be seen as a goal for a successful result.Although it is common practice to use the absolute value version of relative change when discussing percent error, in some situations, it can be beneficial to remove the absolute values to provide more information about the result. Thus, if an experimental value is less than the theoretical value, the percent error will be negative. This negative result provides additional information about the experimental result. For example, experimentally calculating the speed of light and coming up with a negative percent error says that the experimental value is a velocity that is less than the speed of light. This is a big difference from getting a positive percent error, which means the experimental value is a velocity that is greater than the speed of light (violating the theory of relativity) and is a newsworthy result.The percent error equation, when rewritten by removing the absolute values, becomes:It is important to note that the two values in the numerator do not commute. Therefore, it is vital to preserve the order as above: subtract the theoretical value from the experimental value and not vice versa.A percentage change is a way to express a change in a variable. It represents the relative change between the old value and the new one.For example, if a house is worth $100,000 today and the year after its value goes up to $110,000, the percentage change of its value can be expressed asIt can then be said that the worth of the house went up by 10%.More generally, if V1 represents the old value and V2 the new one,Some calculators directly support this via a %CH or Δ% function.When the variable in question is a percentage itself, it is better to talk about its change by using percentage points, to avoid confusion between relative difference and absolute difference.If a bank were to raise the interest rate on a savings account from 3% to 4%, the statement that "the interest rate was increased by 1%" is ambiguous and should be avoided. The absolute change in this situation is 1 percentage point (4% − 3%), but the relative change in the interest rate is:In general, the term "percentage point(s)" indicates an absolute change or difference of percentages, while the percent sign or the word "percentage" refers to the relative change or difference.[3]Car M costs $50,000 and car L costs $40,000. We wish to compare these costs.[4] With respect to car L, the absolute difference is $10,000 = $50,000 − $40,000. That is, car M costs $10,000 more than car L. The relative difference is,and we say that car M costs 25% more than car L. It is also common to express the comparison as a ratio, which in this example is,and we say that car M costs 125% of the cost of car L.In this example the cost of car L was considered the reference value, but we could have made the choice the other way and considered the cost of car M as the reference value. The absolute difference is now −$10,000 = $40,000 − $50,000 since car L costs $10,000 less than car M. The relative difference,is also negative since car L costs 20% less than car M. The ratio form of the comparison,says that car L costs 80% of what car M costs.It is the use of the words "of" and "less/more than" that distinguish between ratios and relative differences.[5]  Change in a quantity can also be expressed logarithmically using the unit of logarithmic change: the decibel and the neper (Np).  Normalization with a factor of 100, as done for percent, yields the derived unit centineper (cNp), which aligns with the definition for percentage change for very small changes:The second advantage is that the total change after a series of cNp changes equals the sum of the changes.  With percent, summing the changes is only an approximation, with larger error for larger changes.  For example:
Triangle inequality
In mathematics, the triangle inequality states that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side.[1][2] This statement permits the inclusion of degenerate triangles, but some authors, especially those writing about elementary geometry, will exclude this possibility, thus leaving out the possibility of equality.[3] If x, y, and z are the lengths of the sides of the triangle, with no side being greater than z, then the triangle inequality states thatwith equality only in the degenerate case of a triangle with zero area.In Euclidean geometry and some other geometries, the triangle inequality is a theorem about distances, and it is written using vectors and vector lengths (norms):where the length z of the third side has been replaced by the vector sum x + y. When x and y are real numbers, they can be viewed as vectors in ℝ1, and the triangle inequality expresses a relationship between absolute values.In Euclidean geometry, for right triangles the triangle inequality is a consequence of the Pythagorean theorem, and for general triangles a consequence of the law of cosines, although it may be proven without these theorems. The inequality can be viewed intuitively in either ℝ2 or ℝ3. The figure at the right shows three examples beginning with clear inequality (top) and approaching equality (bottom). In the Euclidean case, equality occurs only if the triangle has a 180° angle and two 0° angles, making the three vertices collinear, as shown in the bottom example. Thus, in Euclidean geometry, the shortest distance between two points is a straight line.In spherical geometry, the shortest distance between two points is an arc of a great circle, but the triangle inequality holds provided the restriction is made that the distance between two points on a sphere is the length of a minor spherical line segment (that is, one with central angle in [0, π]) with those endpoints.[4][5]The triangle inequality is a defining property of norms and measures of distance. This property must be established as a theorem for any function proposed for such purposes for each particular space: for example, spaces such as the real numbers, Euclidean spaces, the Lp spaces (p ≥ 1), and inner product spaces.Euclid proved the triangle inequality for distances in plane geometry using the construction in the figure.[6] Beginning with triangle ABC, an isosceles triangle is constructed with one side taken as BC and the other equal leg BD along the extension of side AB. It then is argued that angle β > α, so side AD > AC. But AD = AB + BD = AB + BC so the sum of sides AB + BC > AC. This proof appears in Euclid's Elements, Book 1, Proposition 20.[7]For a proper triangle, the triangle inequality, as stated in words, literally translates into three inequalities (given that a proper triangle has side lengths a, b, c that are all positive and excludes the degenerate case of zero area):A more succinct form of this inequality system can be shown to beAnother way to state it isimplyingand thus that the longest side length is less than the semiperimeter.A mathematically equivalent formulation is that the area of a triangle with sides a, b, c must be a real number greater than zero. Heron's formula for the area isIn terms of either area expression, the triangle inequality imposed on all sides is equivalent to the condition that the expression under the square root sign be real and greater than zero (so the area expression is real and greater than zero).In the case of right triangles, the triangle inequality specializes to the statement that the hypotenuse is greater than either of the two sides, and less than their sum.[9]The second part of this theorem is already established above for any side of any triangle. The first part is established using the lower figure. In the figure, consider the right triangle ADC. An isosceles triangle ABC is constructed with equal sides AB = AC. From the triangle postulate, the angles in the right triangle ADC satisfy:Likewise, in the isosceles triangle ABC, the angles satisfy:Therefore,and so, in particular,That means side AD opposite angle α is shorter than side AB opposite the larger angle β. But AB = AC. Hence:A similar construction shows AC > DC, establishing the theorem.An alternative proof (also based upon the triangle postulate) proceeds by considering three positions for point B:[10] (i) as depicted (which is to be proven), or (ii) B coincident with D (which would mean the isosceles triangle had two right angles as base angles plus the vertex angle γ, which would violate the triangle postulate), or lastly, (iii) B interior to the right triangle between points A and D (in which case angle ABC is an exterior angle of a right triangle BDC and therefore larger than π/2, meaning the other base angle of the isosceles triangle also is greater than π/2 and their sum exceeds π in violation of the triangle postulate).This theorem establishing inequalities is sharpened by Pythagoras' theorem to the equality that the square of the length of the hypotenuse equals  the sum of the squares of the other two sides.Consider a triangle whose sides are in an arithmetic progression and let the sides be a, a + d, a + 2d. Then the triangle inequality requires thatTo satisfy all these inequalities requiresWhen d is chosen such that d = a/3, it generates a right triangle that is always similar to the Pythagorean triple with sides 3, 4, 5.Now consider a triangle whose sides are in a geometric progression and let the sides be a, ar, ar2. Then the triangle inequality requires thatThe first inequality requires a > 0; consequently it can be divided through and eliminated. With a > 0, the middle inequality only requires r > 0. This now leaves the first and third inequalities needing to satisfyThe first of these quadratic inequalities requires r to range in the region beyond the value of the positive root of the quadratic equation r2 + r − 1 = 0, i.e. r > φ − 1  where φ is the golden ratio. The second quadratic inequality requires r to range between 0 and the positive root of the quadratic equation r2 − r − 1 = 0, i.e. 0 < r < φ. The combined requirements result in r being confined to the rangeWhen r the common ratio is chosen such that r = √φ it generates a right triangle that is always similar to the Kepler triangle.The triangle inequality can be extended by mathematical induction to arbitrary polygonal paths, showing that the total length of such a path is no less than the length of the straight line between its endpoints. Consequently, the length of any polygon side is always less than the sum of the other polygon side lengths.Consider a quadrilateral whose sides are in a geometric progression and let the sides be a, ar, ar2, ar3. Then the generalized polygon inequality requires thatThese inequalities for a > 0 reduce to the followingThe left-hand side polynomials of these two inequalities have roots that are the tribonacci constant and its reciprocal. Consequently, r is limited to the range 1/t < r < t where t is the tribonacci constant.This generalization can be used to prove that the shortest curve between two points in Euclidean geometry is a straight line.No polygonal path between two points is shorter than the line between them. This implies that no curve can have an arc length less than the distance between its endpoints. By definition, the arc length of a curve is the least upper bound of the lengths of all polygonal approximations of the curve. The result for polygonal paths shows that the straight line between the endpoints is shortest of all the polygonal approximations. Because the arc length of the curve is greater than or equal to the length of every polygonal approximation, the curve itself cannot be shorter than the straight line path.[14]The converse of the triangle inequality theorem is also true: if three real numbers are such that each is less than the sum of the others, then there exists a triangle with these numbers as its side lengths and with positive area; and if one number equals the sum of the other two, there exists a degenerate triangle (i.e., with zero area) with these numbers as its side lengths.In either case, if the side lengths are a, b, c we can attempt to place a triangle in the Euclidean plane as shown in the diagram. We need to prove that there exists a real number h consistent with the values a, b, and c, in which case this triangle exists.By the Pythagorean theorem we have b2 = h2 + d2 and a2 = h2 + (c − d)2 according to the figure at the right. Subtracting these yields a2 − b2 = c2 − 2cd. This equation allows us to express d in terms of the sides of the triangle:For the height of the triangle we have that h2 = b2 − d2. By replacing d with the formula given above, we havewhich holds if the triangle inequality is satisfied for all sides. Therefore there does exist a real number h consistent with the sides a, b, c, and the triangle exists. If each triangle inequality holds strictly, h > 0 and the triangle is non-degenerate (has positive area); but if one of the inequalities holds with equality, so h = 0, the triangle is degenerate.In Euclidean space, the hypervolume of an (n − 1)-facet of an n-simplex is less than or equal to the sum of the hypervolumes of the other n facets.  In particular, the area of a triangular face of a tetrahedron is less than or equal to the sum of the areas of the other three sides.In a normed vector space V, one of the defining properties of the norm is the triangle inequality:that is, the norm of the sum of two vectors is at most as large as the sum of the norms of the two vectors.  This is also referred to as subadditivity. For any proposed function to behave as a norm, it must satisfy this requirement.[15]Proof:[16]After adding, The triangle inequality is useful in mathematical analysis for determining the best upper estimate on the size of the sum of two numbers, in terms of the sizes of the individual numbers.There is also a lower estimate, which can be found using the reverse triangle inequality which states that for any real numbers x and y:In a metric space M with metric d, the triangle inequality is a requirement upon distance:for all x, y, z in M. That is, the distance from x to z is at most as large as the sum of the distance from x to y and the distance from y to z.The triangle inequality is responsible for most of the interesting structure on a metric space, namely, convergence.  This is because the remaining requirements for a metric are rather simplistic in comparison.  For example, the fact that any convergent sequence in a metric space is a Cauchy sequence is a direct consequence of the triangle inequality, because if we choose any xn and xm such that d(xn, x) < ε/2 and d(xm, x) < ε/2, where ε > 0 is given and arbitrary (as in the definition of a limit in a metric space), then by the triangle inequality, d(xn, xm) ≤ d(xn, x) + d(xm, x) < ε/2 + ε/2 = ε, so that the sequence {xn} is a Cauchy sequence, by definition.This version of the triangle inequality reduces to the one stated above in case of normed vector spaces where a metric is induced via d(x, y) ≔ ‖x − y‖, with x − y being the vector pointing from point y to x.The reverse triangle inequality is an elementary consequence of the triangle inequality that gives lower bounds instead of upper bounds. For plane geometry the statement is:[19]In the case of a normed vector space, the statement is:Combining these two statements gives:In Minkowski space, if x and y are both timelike vectors lying in the future light cone, the triangle inequality is reversed:A physical example of this inequality is the twin paradox in special relativity. The same reversed form of the inequality holds if both vectors lie in the past light cone, and if one or both are null vectors. The result holds in n+1 dimensions for any n≥1.  If the plane defined by x and y is spacelike (and therefore a euclidean subspace) then the usual triangle inequality holds.
Matrix multiplication
In mathematics, matrix multiplication or matrix product is a binary operation that produces a matrix from two matrices with entries in a field, or, more generally, in a ring or even a semiring. The matrix product is designed for representing the composition of linear maps that are represented by matrices. Matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering.[1][2] In more detail, if A is an n × m matrix and B is an m × p matrix, their matrix product AB is an n × p matrix, in which the m entries across a row of A are multiplied with the m entries down a column of B and summed to produce an entry of AB.  When two linear maps are represented by matrices, then the matrix product represents the composition of the two maps.The definition of matrix product requires that the entries belong to a semiring, and does not require multiplication of elements of the semiring to be commutative. In many applications, the matrix elements belong to a field, although the tropical semiring is also a common choice for graph shortest path problems.[3] Even in the case of matrices over fields, the product is not commutative in general, although it is associative and is distributive over matrix addition. The identity matrices (which are the square matrices whose entries are zero outside of the main diagonal and 1 on the main diagonal) are identity elements of the matrix product. It follows that the n × n matrices over a ring form a ring, which is noncommutative except if n = 1 and the ground ring is commutative.A square matrix may have a multiplicative inverse, called an inverse matrix. In the common case where the entries belong to a commutative ring r, a matrix has an inverse if and only if its determinant has a multiplicative inverse in r. The determinant of a product of square matrices is the product of the determinants of the factors. The n × n matrices that have an inverse form a group under matrix multiplication, the subgroups of which are called matrix groups. Many classical groups (including all finite groups) are isomorphic to matrix groups; this is the starting point of the theory of group representations.This article will use the following notational conventions: matrices are represented by capital letters in bold, e.g. A, vectors in lowercase bold, e.g. a, and entries of vectors and matrices are italic (since they are numbers from a field), e.g. A and a. Index notation is often the clearest way to express definitions, and is used as standard in the literature. The i, j entry of matrix A is indicated by (A)ij, Aij or aij, whereas a numerical label (not matrix entries) on a collection of matrices is subscripted only, e.g. A1, A2, etc.If A is an n × m matrix and B is an m × p matrix,the matrix product C = AB  (denoted without multiplication signs or dots) is defined to be the n × p matrix[5][6][7][8]such that for i = 1, ..., n and j = 1, ..., p.Thus the product AB is defined if and only if the number of columns in A equals the number of rows in B, in this case m.Usually the entries are numbers, but they may be any kind mathematical objects for which an addition and a multiplication are defined, that are associative, and such that the addition is commutative, and the multiplication is distributive with respect to the addition. In particular, the entries may be matrices themselves (see block matrix).The figure to the right illustrates diagrammatically the product of two matrices A and B, showing how each intersection in the product matrix corresponds to a row of A and a column of B.The values at the intersections marked with circles are:Historically, matrix multiplication has been introduced for making easier and clarifying computations in linear algebra. This strong relationship between matrix multiplication and linear algebra remains fundamental in all mathematics, as well as in physics, engineering and computer science.If a vector space has a finite basis, its elements (vectors) are uniquely represented by a finite sequence, called coordinate vector, or scalars, which are the coordinates of the vector on the basis. These coordinates are commonly organized as a column matrix (also called column vector), that is a matrix with only one column.A linear map A from a vector space of dimension n into a vector space of dimension m maps a column vectoronto the column vectorThe linear map A is thus defined by the matrix The general form of a system of linear equations isUsing same notation as above, such a system is equivalent with the single matrix equationThe dot product of two column vectors is the matrix product More generally, any bilinear form over a vector space of finite dimension may be expressed as a matrix productand any inner product may be expressed as Matrix multiplication shares some properties with usual multiplication. However, matrix multiplication is not defined if the number of columns of the first factor differs from the number of rows of the second factor, and it is non-commutative, even when the product remains definite after changing the order of the factors.[9][10]For exampleandThe matrix product is distributive with respect of matrix addition. That is, if A, B, C, D are matrices of respective sizes m × n, n × p,  n × p, and p × q, one has (left distributivity)and (right distributivity)This results from the distributivity for coefficients by If the scalars have the commutative property, then all four matrices are equal. More generally, all four are equal if c belongs to the center of a ring containing the entries of the matrices, because in this case cX = Xc for all matrices X.These properties result from the bilinearity of the product of scalars:If the scalars have the commutative property, the transpose of a product of matrices is the product, in the reverse order, of the transposes of the factors. That is where T denotes the transpose, that is the interchange of rows and columns.This identity does not hold for noncommutative entries, since the order between the entries of A and B is reversed, when one expands the definition of the matrix product.If A and B have complex entries, thenwhere * denotes the entry-wise complex conjugate of a matrix.This results of applying to the definition of matrix product the fact that the conjugate of a sum is the sum of the conjugates of the summands and the conjugate of a product is the product of the conjugates of the factors.Transposition acts on the indices of the entries, while conjugation acts independently on the entries themselves. It results that, if A and B have complex entries, one haswhere † denotes the conjugate transpose (conjugate of the transpose, or equivalently transpose of the conjugate).Given three matrices A, B and C, the products (AB)C and A(BC) are defined if and only if the number of columns of A equals the number of rows of B  and the number of columns of B equals the number of rows of C (in particular, if one of the product is defined, the other is also defined). In this case, one has the associative propertyThis extends naturally to the product of any number of matrix provided that the dimension match. That is, if A1, A2, ..., An are matrices such that the number of columns of Ai equals the number of rows of Ai + 1 for i = 1, ..., n – 1, then the product is defined and does not depend on the order of the multiplications, if the order of the matrices is kept fixed.These properties may be proved by straightforward but complicate summation manipulations. This result also from the fact that matrices represent linear maps. Therefore, the associative property of matrices is simply a specific case of the associative property of function composition.Although the result of a sequence of matrix product does not depend on the order of operation (provided that the order of the matrices is not changed), the computational complexity may depend dramatically on this order.For example, if A, B and C are matrices of respective sizes 10×30, 30×5, 5×60, computing (AB)C needs 10×30×5 + 10×5×60 = 4,500 multiplications, while computing A(BC) needs 30×5×60 + 10×30×60 = 27,000 multiplications.Similarity transformations map product to products, that is In fact, one has If n > 1, many matrices do not have a multiplicative inverse. For example, a matrix such that all entries of a row (or a column) are 0 does not have an inverse. If it exists, the inverse of a matrix A is denoted A−1, and, thus verifiesA matrix that has an inverse is an invertible matrix. Otherwise, it is a singular matrix.A product of matrices is invertible if and only if each factor is invertible. In this case, one hasWhen R is commutative, and, in particular, when it is a field, the determinant of a product is the product of the determinants. As determinants are scalars, and scalars commute, one has thus One may raise a square matrix to any nonnegative integer power multiplying it by itself repeatedly in the same way as for ordinary numbers. That is,Computing the kth power of a matrix needs k – 1 times the time of a single matrix multiplication, if it is done with the trivial algorithm (repeated multiplication). As this may be very time consuming, one generally prefers using exponentiation by squaring, which requires less than 2 log2 k matrix multiplications, and is therefore much more efficient.An easy case for exponentiation is that of a diagonal matrix. Since the product of diagonal matrices amounts to simply multiplying corresponding diagonal elements together, the kth power of a diagonal matrix is obtained by raising the entries to the power k:The starting point of Strassen's proof is using block matrix multiplication. Specifically, a matrix of even dimension 2n×2n may be partitioned in four n×n blocksUnder this form, its inverse is Thus, the inverse of a 2n×2n matrix may be computed with two inversions, six multiplications and four additions or additive inverses of n×n matrices. It follows that, denoting respectively by I(n), M(n) and A(n) = n2 the number of operations needed for multiplying, inverting and adding n×n matrices, one has for some constant d.For matrices whose dimension is not a power of two, the same complexity is reached by increasing the dimension of the matrix to a power of two, by padding the matrix with rows and columns whose entries are 1 on the diagonal and 0 elsewhere.This proves the asserted complexity for matrices such that all submatrices that have to be inverted are indeed invertible. This complexity is thus proved for almost all matrices, as a matrix with randomly chosen entries is invertible with probability one.The same argument applies to LU decomposition, as, if the matrix A is invertible, the equalityThe argument applies also for the determinant, since it results from the block LU decomposition that The term "matrix multiplication" is most commonly reserved for the definition given in this article. It could be more loosely applied to other operations on matrices.
Dual norm
In functional analysis, the dual norm is a measure of the "size" of each continuous linear functional defined on a normed space.The dual norm is a special case of the operator norm defined for each (bounded) linear map between normed vector spaces.From the definition of dual norm we have the inequalityThe dual of the Euclidean norm is the Euclidean norm, sincewhich turns out to be the sum of the singular values,Proof: 
Matrix addition
In mathematics, matrix addition is the operation of adding two matrices by adding the corresponding entries together. However, there are other operations which could also be considered as a kind of addition for matrices, the direct sum and the Kronecker sum.Two matrices must have an equal number of rows and columns to be added.[1] The sum of two matrices A and B will be a matrix which has the same number of rows and columns as do A and B. The sum of A and B, denoted A + B, is computed by adding corresponding elements of A and B:[2][3]For example:We can also subtract one matrix from another, as long as they have the same dimensions. A − B is computed by subtracting elements of B from corresponding elements of A, and has the same dimensions as A and B. For example:Another operation, which is used less often, is the direct sum (denoted by ⊕). Note the Kronecker sum is also denoted ⊕; the context should make the usage clear.  The direct sum of any pair of matrices A of size m × n and B of size p × q is a matrix of size (m + p) × (n + q) defined as [4][2]For instance,The direct sum of matrices is a special type of block matrix, in particular the direct sum of square matrices is a block diagonal matrix.The adjacency matrix of the union of disjoint graphs or multigraphs is the direct sum of their adjacency matrices. Any element in the direct sum of two vector spaces of matrices can be represented as a direct sum of two matrices.In general, the direct sum of n matrices is:[2]where the zeros are actually blocks of zeros, i.e. zero matrices.
System of linear equations
In mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1]  For example,is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given bysince it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.The simplest kind of linear system involves two equations and two variables:Now substitute this expression for x into the bottom equation:A general system of m linear equations with n unknowns can be written asOften the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.The vector equation is equivalent to a matrix equation of the formwhere A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The number of vectors in a basis for the span is now expressed as the rank of the matrix.A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.A linear system may behave in any one of three possible ways:For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.The following pictures illustrate this trichotomy in the case of two variables:The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.For example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For a more complicated example, the equationsare not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.There are several algorithms for solving a system of linear equations.To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.For example, consider the following system:The solution set to this system can be described by the following equations:Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Here x is the free variable, and y and z are dependent.The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:For example, consider the following system:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsSolving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:The last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemis given byFor each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.)Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.There is also a quantum algorithm for linear systems of equations.[3]A system of linear equations is homogeneous if all of the constant terms are zero:A homogeneous system is equivalent to a matrix equation of the formwhere A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution.  If the system has a singular matrix then there is a solution set with an infinite number of solutions.  This solution set has the following additional properties:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A.Numerical solutions to a homogeneous system can be found with a singular value decomposition.There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asGeometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.
Padé table
In complex analysis, a Padé table is an array, possibly of infinite extent, of the rational Padé approximantsto a given complex formal power series. Certain sequences of approximants lying within a Padé table can often be shown to correspond with successive convergents of a continued fraction representation of a holomorphic or meromorphic function.Although earlier mathematicians had obtained sporadic results involving sequences of rational approximations to transcendental functions, Frobenius (in 1881) was apparently the first to organize the approximants in the form of a table. Henri Padé further expanded this notion in his doctoral thesis Sur la representation approchee d'une fonction par des fractions rationelles, in 1892. Over the ensuing 16 years Padé published 28 additional papers exploring the properties of his table, and relating the table to analytic continued fractions.[1]Modern interest in Padé tables was revived by H. S. Wall and Oskar Perron, who were primarily interested in the connections between the tables and certain classes of continued fractions. Daniel Shanks and Peter Wynn published influential papers about 1955, and W. B. Gragg obtained far-reaching convergence results during the '70s. More recently, the widespread use of electronic computers has stimulated a great deal of additional interest in the subject.[2]A function f(z) is represented by a formal power series:where c0 ≠ 0, by convention. The (m, n)th entry[3] Rm, n in the Padé table for f(z) is then given bywhere Pm(z) and Qn(z) are polynomials of degrees not more than m and n, respectively. The coefficients {ai} and {bi} can always be found by considering the expressionand equating coefficients of like powers of z up through m + n. For the coefficients of powers m + 1 to m + n, the right hand side is 0 and the resulting system of linear equations contains a homogeneous system of n equations in the n + 1 unknowns bi, and so admits of infinitely many solutions each of which determines a possible Qn. Pm is then easily found by equating the first m coefficients of the equation above. However, it can be shown that, due to cancellation, the generated rational functions Rm, n are all the same, so that the (m, n)th entry in the Padé table is unique.[2] Alternatively, we may require that b0 = 1, thus putting the table in a standard form.Although the entries in the Padé table can always be generated by solving this system of equations, that approach is computationally expensive. Usage of the Padé table has been extended to meromorphic functions by newer, timesaving methods such as the epsilon algorithm.[4]Because of the way the (m, n)th approximant is constructed, the differenceis a power series whose first term is of degree no less thanIf the first term of that difference is of degreethen the rational function Rm, n occupiescells in the Padé table, from position (m, n) through position (m+r, n+r), inclusive. In other words, if the same rational function appears more than once in the table, that rational function occupies a square block of cells within the table. This result is known as the block theorem.If a particular rational function occurs exactly once in the Padé table, it is called a normal approximant to f(z). If every entry in the complete Padé table is normal, the table itself is said to be normal. Normal Padé approximants can be characterized using determinants of the coefficients cn in the Taylor series expansion of f(z), as follows. Define the (m, n)th determinant bywith Dm,0 = 1, Dm,1 = cm, and ck = 0 for k < 0. ThenOne of the most important forms in which an analytic continued fraction can appear is as a regular C-fraction, which is a continued fraction of the formwhere the ai ≠ 0 are complex constants, and z is a complex variable.There is an intimate connection between regular C-fractions and Padé tables with normal approximants along the main diagonal: the "stairstep" sequence of Padé approximants R0,0, R1,0, R1,1, R2,1, R2,2, … is normal if and only if that sequence coincides with the successive convergents of a regular C-fraction. In other words, if the Padé table is normal along the main diagonal, it can be used to construct a regular C-fraction, and if a regular C-fraction representation for the function f(z) exists, then the main diagonal of the Padé table representing f(z) is normal.[2]Here is an example of a Padé table, for the exponential function.Several features are immediately apparent.The procedure used to derive Gauss's continued fraction can be applied to a certain confluent hypergeometric series to derive the following C-fraction expansion for the exponential function, valid throughout the entire complex plane:By applying the fundamental recurrence formulas one may easily verify that the successive convergents of this C-fraction are the stairstep sequence of Padé approximants R0,0, R1,0, R1,1, … In this particular case a closely related continued fraction can be obtained from the identitythat continued fraction looks like this:This fraction's successive convergents also appear in the Padé table, and form the sequence R0,0, R0,1, R1,1, R1,2, R2,2, …A formal Newton series L is of the formwhere the sequence {βk} of points in the complex plane is known as the set of interpolation points. A sequence of rational approximants Rm,n can be formed for such a series L in a manner entirely analogous to the procedure described above, and the approximants can be arranged in a Newton-Padé table. It has been shown[6] that some "staircase" sequences in the Newton-Padé table correspond with the successive convergents of a Thiele-type continued fraction, which is of the formMathematicians have also constructed two-point Padé tables by considering two series, one in powers of z, the other in powers of 1/z, which alternately represent the function f(z) in a neighborhood of zero and in a neighborhood of infinity.[2]
Gaussian elimination
In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 A.D. (see History section).To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the leftmost nonzero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (unreduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called forward elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.There are three types of elementary row operations which may be performed on the rows of a matrix:If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.For each row in a matrix, if the row does not consist of only zeros, then the leftmost nonzero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).Suppose the goal is to find and describe the set of solutions to the following system of linear equations:The table below is the row reduction process applied simultaneously to the system of equations, and its associated augmented matrix. In practice, one does not usually deal with the systems in terms of equations but instead makes use of the augmented matrix, which is more suitable for computer manipulations. The row reduction procedure may be summarized as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.The second column describes which row operations have just been performed. So for the first step, the x is eliminated from L2 by adding 3/2L1 to L2. Next x is eliminated from L3 by adding L1 to L3. These row operations are labelled in the table asOnce y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = −1, y = 3, and x = 2. So there is a unique solution to the original system of equations.Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss–Jordan elimination, to distinguish it from stopping after reaching echelon form.The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied.  Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life.  The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century.  Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5]  The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss–Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.[7]The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B:Computationally, for an n × n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.A variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists.  If A is an n × n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n × n identity matrix is augmented to the right of A, forming an n × 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n × 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A−1. If the algorithm is unable to reduce the left block to I, then A is not invertible.For example, consider the following matrixTo find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 × 6 matrix:By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A−1. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.The Gaussian elimination algorithm can be applied to any m × n matrix A. In this way, for example, some 6 × 9 matrices can be transformed to a matrix that has a row echelon form likewhere the stars are arbitrary entries and a, b, c, d, e are nonzero entries. This echelon matrix T contains a wealth of information about A: the rank of A is 5 since there are 5 nonzero rows in T; the vector space spanned by the columns of A has a basis consisting of the first, third, fourth, seventh and ninth column of A (the columns of a, b, c, d, e in T), and the stars show how the other columns of A can be written as linear combinations of the basis columns. This is a consequence of the distributivity of the dot product in the expression of a linear map as a matrix.All of this applies also to the reduced row echelon form, which is a particular row echelon format.The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n + 1)/2 divisions, 2n3 + 3n2 − 5n/6 multiplications, and 2n3 + 3n2 − 5n/6 subtractions,[8] for a total of approximately 2n3/3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating-point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9]However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5).This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).To put an n × n matrix into reduced echelon form by row operations, one needs n3 arithmetic operations, which is approximately 50% more computation steps.[10]One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row-reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means that any error existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]The Gaussian elimination can be performed over any field, not just the real numbers.Gaussian elimination does not generalize in any way to higher-order tensors (matrices are array representations of order-2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]As explained above, Gaussian elimination transforms a given m × n matrix A into a matrix in row-echelon form.In the following pseudocode, A[i, j] denotes the entry of the matrix A in row i and column j with the indices starting from 1. The transformation is performed in place, meaning that the original matrix is lost for being eventually replaced by its row-echelon form.This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.
Linear map
In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V → W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.  An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.In the language of abstract algebra, a linear map is a module homomorphism.  In the language of category theory it is a morphism in the category of modules over a given ring.Thus, a linear map is said to be operation preserving.  In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m × n matrix, then f(x) = Ax describes a linear map Rn → Rm (see Euclidean space).Let {v1, …, vn} be a basis for V.  Then every vector v in V is uniquely determined by the coefficients c1, …, cn in the field R:If f : V → W is a linear map,which implies that the function f is entirely determined by the vectors f(v1), …, f(vn). Now let {w1, …, wm}  be a basis for W.  Then we can represent each vector f(vj) asThus, the function f is entirely determined by the values of aij. If we put these values into an m × n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V.  To get M, every column j of M is a vectorcorresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),where M is the matrix of f. In other words, every column j = 1, …, n has a corresponding vector f(vj) whose coordinates a1j, …, amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.The matrices of a linear transformation can be represented visually:In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:The composition of linear maps is linear: if f : V → W and g : W → Z are linear, then so is their composition g ∘ f : V → Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.The inverse of a linear map, when defined, is again a linear map.If f1 : V → W and f2 : V → W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).If f : V → W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W).  Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps  is again a linear map, and the composition of maps is always associative.  This case is discussed in more detail below.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.A linear transformation f: V → V is an endomorphism of V; the set of all such endomorphisms End(V)  together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V → V.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).If V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n × n matrices with entries in K. The automorphism group of V is isomorphic to the  general linear group GL(n, K) of all n × n invertible matrices with entries in K.If f : V → W is linear, we define the kernel and the image or range of f byker(f) is a subspace of V and im(f) is a subspace of W.  The following dimension formula is known as the rank–nullity theorem:The number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, ρ(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or ν(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target.Formally, one has the exact sequenceThese can be interpreted thus: given a linear equation f(v) = w to solve,The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.For a linear operator with finite-dimensional kernel and co-kernel, one may define  index as:namely the degrees of freedom minus the number of constraints.For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) − dim(W), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → V → W → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.[8]No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.Let V and W denote vector spaces over a field, F. Let T: V → W be a linear map.Given a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Substituting this in the first expressionhenceTherefore, the matrix in the new basis is A′ = B−1AB, being B the matrix of the given basis.Therefore, linear maps are said to be 1-co- 1-contra-variant objects, or type (1, 1) tensors.A linear transformation between topological vector spaces, for example normed spaces, may be continuous.  If its domain and codomain are the same, it will then be a continuous linear operator.  A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9]  An infinite-dimensional domain may have discontinuous linear operators.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0).  For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.
Field extension
Field extensions are fundamental in algebraic number theory, and in the study of polynomial roots through Galois theory, and are widely used in algebraic geometry.A subfield of a field L is a subset K of L that is a field with respect to the field operations inherited from L. Equivalently, a subfield is a subset that contains 1, and is closed under the operations of addition, subtraction, multiplication, and taking the inverse of a nonzero element of L.As 1 – 1 = 0, the latter definition implies K and L have the same zero element.For example, the field of rational numbers is a subfield of the real numbers, which is itself a subfield of the complex numbers. More generally, the field of rational numbers is (or is isomorphic to) a subfield of any field of characteristic 0.The characteristic of a subfield is the same as the characteristic of the larger field.If K is a subfield of L, then L is an extension field or simply extension of K, and this pair of fields is a field extension. Such a field extension is denoted L / K (read as "L over K").If L is an extension of F which is in turn an extension of K, then F is said to be an intermediate field (or intermediate extension or subextension) of L / K.Given a field extension L / K, the larger field L is a K-vector space. The dimension of this vector space is called the degree of the extension and is denoted by [L : K]. The degree of an extension is 1 if and only if the two fields are equal. In this case, the extension is a trivial extension. Extensions of degree 2 and 3 are called quadratic extensions and cubic extensions, respectively. A finite extension is an extension that has a finite degree. The degree of a finite extension L / K is denoted [L : K]Given two extensions L / K and M / L, the extension M / K is finite if and only if both L / K and M / L are finite. In this case, one hasAn extension field of the form K(S) is often said to result from the adjunction of S to K.[7][8]In characteristic 0, every finite extension is a simple extension. This is the primitive element theorem, which does not hold true for fields of non-zero characteristic.If a simple extension K(s) / K is not finite, the field K(s) is isomorphic to the field of rational fractions in s over K.The notation L / K is purely formal and does not imply the formation of a quotient ring or quotient group or any other kind of division. Instead the slash expresses the word "over". In some literature the notation L:K is used.It is often desirable to talk about field extensions in situations where the small field is not actually contained in the larger one, but is naturally embedded. For this purpose, one abstractly defines a field extension as an injective ring homomorphism between two fields.Every non-zero ring homomorphism between fields is injective because fields do not possess nontrivial proper ideals, so field extensions are precisely the morphisms in the category of fields.Henceforth, we will suppress the injective homomorphism and assume that we are dealing with actual subfields.The field The fieldBy iterating the above construction, one can construct a splitting field of  any polynomial from K[X]. This is an extension field L of K in which the given polynomial splits into a product of linear factors.Given a field K, we can consider the field K(X) of all rational functions in the variable X with coefficients in K; the elements of K(X) are fractions of two polynomials over K, and indeed K(X) is the field of fractions of the polynomial ring K[X]. This field of rational functions is an extension field of K. This extension is infinite.The set of the elements of L that are algebraic over K form a subextension, which is called the algebraic closure of K in L. This results from the preceding characterization: if s and t are algebraic, the extensions K(s) /K and K(s)(t) /K(s) are finite. Thus K(s, t) /K is also finite, as well as the sub extensions K(s ± t) /K, K(st) /K and K(1/s) /K (if s ≠ 0. It follows that s ± t, st and 1/s are all algebraic.A simple extension is algebraic if and only if it is finite. This implies that an extension is algebraic if and only if it is the union of its finite subextensions, and that every finite extension is algebraic. An algebraic extension L/K is called normal if every irreducible polynomial in K[X] that has a root in L completely factors into linear factors over L. Every algebraic extension F/K admits a normal closure L, which is an extension field of F such that L/K is normal and which is minimal with this property.An algebraic extension L/K is called separable if the minimal polynomial of every element of L over K is separable, i.e., has no repeated roots in an algebraic closure over K. A Galois extension is a field extension that is both normal and separable.A consequence of the primitive element theorem states that every finite separable extension has a primitive element (i.e. is simple).Given any field extension L/K, we can consider its automorphism group Aut(L/K), consisting of all field automorphisms α: L → L with α(x) = x for all x in K. When the extension is Galois this automorphism group is called the Galois group of the extension. Extensions whose Galois group is abelian are called abelian extensions.For a given field extension L/K, one is often interested in the intermediate fields F (subfields of L that contain K). The significance of Galois extensions and Galois groups is that they allow a complete description of the intermediate fields: there is a bijection between the intermediate fields and the subgroups of the Galois group, described by the fundamental theorem of Galois theory.Field extensions can be generalized to ring extensions which consist of a ring and one of its subrings. A closer non-commutative analog are central simple algebras (CSAs) – ring extensions over a field, which are simple algebra (no non-trivial 2-sided ideals, just as for a field) and where the center of the ring is exactly the field. For example, the only finite field extension of the real numbers is the complex numbers, while the quaternions are a central simple algebra over the reals, and all CSAs over the reals are Brauer equivalent to the reals or the quaternions. CSAs can be further generalized to Azumaya algebras, where the base field is replaced by a commutative local ring.Given a field extension, one can "extend scalars" on associated algebraic objects. For example, given a real vector space, one can produce a complex vector space via complexification. In addition to vector spaces, one can perform extension of scalars for associative algebras defined over the field, such as polynomials or group algebras and the associated group representations. Extension of scalars of polynomials is often used implicitly, by just considering the coefficients as being elements of a larger field, but may also be considered more formally. Extension of scalars has numerous applications, as discussed in extension of scalars: applications.
Invertible matrix
In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such thatwhere In denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A−1.A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.Non-square matrices (m-by-n matrices for which m ≠ n) do not have an inverse.  However, in some cases such a matrix may have a left inverse or right inverse.  If A is m-by-n and the rank of A is equal to n, then A has a left inverse: an n-by-m matrix B such that BA = In.  If A has rank m, then it has a right inverse: an n-by-m matrix B such that AB = Im.Matrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.While the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. However, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. For a noncommutative ring, the usual determinant is not defined.  The conditions for existence of left-inverse or right-inverse are more complicated since a notion of rank does not exist over rings.The set of n × n invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree n.Let A be a square n by n matrix over a field K (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:Furthermore, the following properties hold for an invertible matrix A:A matrix that is its own inverse, i.e. such that A = A−1 and A2 = I, is called an involutory matrix.It follows from the associativity of matrix multiplication that iffor finite square matrices A and B, then alsoOver the field of real numbers, the set of singular n-by-n matrices, considered as a subset of Rn×n, is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all n-by-n matrices are invertible.Furthermore, the n-by-n invertible matrices are a dense open set in the topological space of all n-by-n matrices.  Equivalently, the set of singular matrices is closed and nowhere dense in the space of n-by-n matrices.In practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.Consider the following 2-by-2 matrix:As an example of a non-invertible, or singular, matrix, consider the matrixGauss-Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.A generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:Victor Pan and John Reif have done work that includes ways of generating a starting seed.[2][3] Byte magazine summarised one of their approaches.[4]Newton's method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton's method is also useful for "touch up" corrections to the Gauss–Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given byIf matrix A is positive definite, then its inverse can be obtained aswhere L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.Writing the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of small matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:so thatwhere |A| is the determinant of A, C is the matrix of cofactors, and CT represents the matrix transpose.The cofactor equation listed above yields the following result for 2 × 2 matrices. Inversion of these matrices can be done as follows:[6]This is possible because 1/(ad − bc) is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.The Cayley–Hamilton method givesA computationally efficient 3 × 3 matrix inversion is given by(where the scalar A is not to be confused with the matrix A).If the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given byThe determinant of A can be computed by applying the rule of Sarrus as follows:The Cayley–Hamilton decomposition givesWith increasing dimension, expressions for the inverse of A get complicated. For n = 4, the Cayley–Hamilton method leads to an expression that is still tractable:Matrices can also be inverted blockwise by using the following analytic inversion formula:    ( 1)where A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and D − CA−1B must be nonsingular.[7]) This strategy is particularly advantageous if A is diagonal and D − CA−1B (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion.This technique was reinvented several times and is due to Hans Boltz (1923),[citation needed] who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.The nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.The inversion procedure that led to Equation (1) performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and A − BD−1C are nonsingular,[8] the result is    ( 2)Equating Equations (1) and (2) leads to    ( 3)where Equation (3) is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.Since a blockwise inversion of an n × n matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.[9] There exist matrix multiplication algorithms with a complexity of O(n2.3727) operations, while the best proven lower bound is Ω(n2 log n).[10]If a matrix A has the property thatthen A is nonsingular and its inverse may be expressed by a Neumann series:[11]Truncating the sum results in an "approximate" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfies More generally, if A is "near" the invertible matrix X in the sense thatthen A is nonsingular and its inverse isIf it is also the case that A − X has rank 1 then this simplifies toSuppose that the invertible matrix A depends on a parameter t. Then the derivative of the inverse of A with respect to t is given byMore generally, ifthen,Therefore,Some of the properties of inverse matrices are shared by generalized inverses (e.g., the Moore–Penrose inverse), which can be defined for any m-by-n matrix.For most practical applications, it is not necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it is necessary that the matrix involved be invertible.Decomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.Although an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.Matrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.Matrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.
Rank (linear algebra)
In linear algebra, the rank of a matrix A is the dimension of the vector space generated (or spanned) by its columns.[1] This corresponds to the maximal number of linearly independent columns of A. This, in turn, is identical to the dimension of the space spanned by its rows.[2] Rank is thus a measure of the "nondegenerateness" of the system of linear equations and linear transformation encoded by A.  There are multiple equivalent definitions of rank. A matrix's rank is one of its most fundamental characteristics.The rank is commonly denoted rank(A) or rk(A); sometimes the parentheses are not written, as in rank A.In this section we give some definitions of the rank of a matrix.  Many definitions are possible; see Alternative definitions for several of these.The column rank of A is the dimension of the column space of A, while the row rank of A is the dimension of the row space of A.A fundamental result in linear algebra is that the column rank and the row rank are always equal. (Two proofs of this result are given in Proofs that column rank = row rank below.)  This number (i.e., the number of linearly independent rows or columns) is simply called the rank of A.A matrix is said to have full rank if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns.  A matrix is said to be rank deficient if it does not have full rank.The rank is also the dimension of the image of the linear transformation that is given by multiplication by A.  More generally, if a linear operator on a vector space (possibly infinite-dimensional) has finite-dimensional image (e.g., a finite-rank operator), then the rank of the operator is defined as the dimension of the image.The matrixhas rank 2: the first two rows are linearly independent, so the rank is at least 2, but since the third is a linear combination of the first two (the second subtracted from the first), the three rows are linearly dependent so the rank must be less than 3.The matrixhas rank 1: there are nonzero columns, so the rank is positive, but any pair of columns is linearly dependent.  Similarly, the transposeof A has rank 1.  Indeed, since the column vectors of A are the row vectors of the transpose of A, the statement that the column rank of a matrix equals its row rank is equivalent to the statement that the rank of a matrix is equal to the rank of its transpose, i.e., rk(A) = rk(AT).A common approach to finding the rank of a matrix is to reduce it to a simpler form, generally row echelon form, by elementary row operations. Row operations do not change the row space (hence do not change the row rank), and, being invertible, map the column space to an isomorphic space (hence do not change the column rank). Once in row echelon form, the rank is clearly the same for both row rank and column rank, and equals the number of pivots (or basic columns) and also the number of non-zero rows.For example, the matrix A given bycan be put in reduced row-echelon form by using the following elementary row operations:The final matrix (in row echelon form) has two non-zero rows and thus the rank of matrix A is 2.When applied to floating point computations on computers, basic Gaussian elimination (LU decomposition) can be unreliable, and a rank-revealing decomposition should be used instead. An effective alternative is the singular value decomposition (SVD), but there are other less expensive choices, such as QR decomposition with pivoting (so-called rank-revealing QR factorization), which are still more numerically robust than Gaussian elimination. Numerical determination of rank requires a criterion for deciding when a value, such as a singular value from the SVD, should be treated as zero, a practical choice which depends on both the matrix and the application.The fact that the column and row ranks of any matrix are equal forms an important part of the fundamental theorem of linear algebra. We present two proofs of this result. The first is short, uses only basic properties of linear combinations of vectors, and is valid over any field. The proof is based upon Wardlaw (2005).[3] The second is an elegant argument using orthogonality and is valid for matrices over the real numbers; it is based upon Mackiw (1995).[2] Both proofs can be found in the book by Banerjee and Roy (2014) [4]Let A be a matrix of size m × n (with m rows and n columns). Let the column rank of A be r and let c1,...,cr be any basis for the column space of A. Place these as the columns of an m × r matrix C. Every column of A can be expressed as a linear combination of the r columns in C. This means that there is an r × n matrix R such that A = CR. R is the matrix whose i-th column is formed from the coefficients giving the i-th column of A as a linear combination of the r columns of C. Now, each row of A is given by a linear combination of the r rows of R. Therefore, the rows of R form a spanning set of the row space of A and, by the Steinitz exchange lemma, the row rank of A cannot exceed r. This proves that the row rank of A is less than or equal to the column rank of A. This result can be applied to any matrix, so apply the result to the transpose of A. Since the row rank of the transpose of A is the column rank of A and the column rank of the transpose of A is the row rank of A, this establishes the reverse inequality and we obtain the equality of the row rank and the column rank of A. (Also see rank factorization.)In all the definitions in this section, the matrix A is taken to be an m × n matrix over an arbitrary field F.Given the matrix A, there is an associated linear mapping defined byThe rank of A is the dimension of the image of f.  This definition has the advantage that it can be applied to any linear map without need for a specific matrix.Given the same linear mapping f as above, the rank is n minus the dimension of the kernel of f.  The rank–nullity theorem states that this definition is equivalent to the preceding one.The rank of A is the maximal number of linearly independent rows of A; this is the dimension of the row space of A.As in the case of the "dimension of image" characterization, this can be generalized to a definition of the rank of any linear map: the rank of a linear map f : V → W is the minimal dimension k of an intermediate space X such that f can be written as the composition of a map V → X and a map X → W. Unfortunately, this definition does not suggest an efficient manner to compute the rank (for which it is better to use one of the alternative definitions). See rank factorization for details.The rank of A is the largest order of any non-zero minor in A.  (The order of a minor is the side-length of the square sub-matrix of which it is the determinant.) Like the decomposition rank characterization, this does not give an efficient way of computing the rank, but it is useful theoretically: a single non-zero minor witnesses a lower bound (namely its order) for the rank of the matrix, which can be useful (for example) to prove that certain operations do not lower the rank of a matrix.A non-vanishing p-minor (p × p submatrix with non-zero determinant) shows that the rows and columns of that submatrix are linearly independent, and thus those rows and columns of the full matrix are linearly independent (in the full matrix), so the row and column rank are at least as large as the determinantal rank; however, the converse is less straightforward.  The equivalence of determinantal rank and column rank is a strengthening of the statement that if the span of n vectors has dimension p, then p of those vectors span the space (equivalently, that one can choose a spanning set that is a subset of the vectors): the equivalence implies that a subset of the rows and a subset of the columns simultaneously define an invertible submatrix (equivalently, if the span of n vectors has dimension p, then p of these vectors span the space and there is a set of p coordinates on which they are linearly independent).We assume that A is an m × n matrix, and we define the linear map f by f(x) = Ax as above.One useful application of calculating the rank of a matrix is the computation of the number of solutions of a system of linear equations. According to the Rouché–Capelli theorem, the system is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, then the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank.  In this case (and assuming the system of equations is in the real or complex numbers) the system of equations has infinitely many solutions.In control theory, the rank of a matrix can be used to determine whether a linear system is controllable, or observable.In the field of communication complexity, the rank of the communication matrix of a function gives bounds on the amount of communication needed for two parties to compute the function.There are different generalizations of the concept of rank to matrices over arbitrary rings, where column rank, row rank, dimension of column space, and dimension of row space of a matrix may be different from the others or may not exist.Thinking of matrices as tensors, the tensor rank generalizes to arbitrary tensors; note that for tensors of order greater than 2 (matrices are order 2 tensors), rank is very hard to compute, unlike for matrices.There is a notion of rank for smooth maps between smooth manifolds. It is equal to the linear rank of the derivative.Matrix rank should not be confused with tensor order, which is called tensor rank. Tensor order is the number of indices required to write a tensor, and thus matrices all have tensor order 2. More precisely, matrices are tensors of type (1,1), having one row index and one column index, also called covariant order 1 and contravariant order 1; see Tensor (intrinsic definition) for details.Note that the tensor rank of a matrix can also mean the minimum number of simple tensors necessary to express the matrix as a linear combination, and that this definition does agree with matrix rank as here discussed.
Quadratic form
In mathematics, a quadratic form is a homogeneous polynomial of degree two in a number of variables.  For example,is a quadratic form in the variables x and y.Quadratic forms occupy a central place in various branches of mathematics, including number theory, linear algebra, group theory (orthogonal group), differential geometry (Riemannian metric, second fundamental form), differential topology (intersection forms of four-manifolds), and Lie theory (the Killing form).Quadratic forms are homogeneous quadratic polynomials in n variables. In the cases of one, two, and three variables they are called unary, binary, and ternary and have the following explicit form:where a, ..., f are the coefficients.[1] Note that quadratic functions, such as ax2 + bx + c in the one variable case, are not quadratic forms, as they are typically not homogeneous (unless b and c are both 0).The theory of quadratic forms and methods used in their study depend in a large measure on the nature of the coefficients, which may be real or complex numbers, rational numbers, or integers. In linear algebra, analytic geometry, and in the majority of applications of quadratic forms, the coefficients are real or complex numbers. In the algebraic theory of quadratic forms, the coefficients are elements of a certain field. In the arithmetic theory of quadratic forms, the coefficients belong to a fixed commutative ring, frequently the integers Z or the p-adic integers Zp.[2] Binary quadratic forms have been extensively studied in number theory, in particular, in the theory of quadratic fields, continued fractions, and modular forms. The theory of integral quadratic forms in n variables has important applications to algebraic topology.Using homogeneous coordinates, a non-zero quadratic form in n variables defines an (n−2)-dimensional quadric in the (n−1)-dimensional projective space. This is a basic construction in projective geometry. In this way one may visualize 3-dimensional real quadratic forms as conic sections.A closely related notion with geometric overtones is a quadratic space, which is a pair (V, q), with V a vector space over a field K, and q : V → K a quadratic form on V. An example is given by the three-dimensional Euclidean space and the square of the Euclidean norm expressing the distance between a point with coordinates (x, y, z) and the origin:The study of particular quadratic forms, in particular the question of whether a given integer can be the value of a quadratic form over the integers, dates back many centuries. One such case is Fermat's theorem on sums of two squares, which determines when an integer may be expressed in the form x2 + y2, where x, y are integers. This problem is related to the problem of finding Pythagorean triples, which appeared in the second millennium B.C.[3]In 628, the Indian mathematician Brahmagupta wrote Brāhmasphuṭasiddhānta which includes, among many other things, a study of equations of the form x2 − ny2 = c. In particular he considered what is now called Pell's equation, x2 − ny2 = 1, and found a method for its solution.[4] In Europe this problem was studied by Brouncker, Euler and Lagrange.In 1801 Gauss published Disquisitiones Arithmeticae, a major portion of which was devoted to a complete theory of binary quadratic forms over the integers. Since then, the concept has been generalized, and the connections with quadratic number fields, the modular group, and other areas of mathematics have been further elucidated.Any n×n real symmetric matrix A determines a quadratic form qA in n variables by the formulaConversely, given a quadratic form in n variables, its coefficients can be arranged into an n × n symmetric matrix. An important question in the theory of quadratic forms is how to simplify a quadratic form q by a homogeneous linear change of variables.  A fundamental theorem due to Jacobi asserts that a real quadratic form q has an orthogonal diagonalization.[5]so that the corresponding symmetric matrix is diagonal, and this is accomplished with a change of variables given by an orthogonal matrix – in this case the coefficients λ1, λ2, ..., λn are determined uniquely up to a permutation.There always exists a change of variables given by an invertible matrix, not necessarily orthogonal, such that the coefficients λi are 0, 1, and −1. Sylvester's law of inertia states that the numbers of each 1 and −1 are invariants of the quadratic form, in the sense that any other diagonalization will contain the same number of each. The signature of the quadratic form is the triple (n0, n+, n−), where n0 is the number of 0s and n± is the number of ±1s. Sylvester's law of inertia shows that this is a well-defined quantity attached to the quadratic form. The case when all λi have the same sign is especially important: in this case the quadratic form is called positive definite (all 1) or negative definite (all −1).  If none of the terms are 0, then the form is called nondegenerate; this includes positive definite, negative definite, and indefinite (a mix of 1 and −1); equivalently, a nondegenerate quadratic form is one whose associated symmetric form is a nondegenerate bilinear form. A real vector space with an indefinite nondegenerate quadratic form of index (p, q) (denoting p 1s and q −1s) is often denoted as Rp,q particularly in the physical theory of spacetime.These results are reformulated in a different way below.Let q be a quadratic form defined on an n-dimensional real vector space. Let A be the matrix of the quadratic form q in a given basis. This means that A is a symmetric n × n matrix such thatwhere x is the column vector of coordinates of v in the chosen basis. Under a change of basis, the column x is multiplied on the left by an n × n invertible matrix S, and the symmetric square matrix A is transformed into another symmetric square matrix B of the same size according to the formulaAny symmetric matrix A can be transformed into a diagonal matrixby a suitable choice of an orthogonal matrix S, and the diagonal entries of B are uniquely determined – this is Jacobi's theorem. If S is allowed to be any invertible matrix then B can be made to have only 0,1, and −1 on the diagonal, and the number of the entries of each type (n0 for 0, n+ for 1, and n− for −1) depends only on A. This is one of the formulations of Sylvester's law of inertia and the numbers n+ and n− are called the positive and negative indices of inertia. Although their definition involved a choice of basis and consideration of the corresponding real symmetric matrix A, Sylvester's law of inertia means that they are invariants of the quadratic form q.The quadratic form q is positive definite (resp., negative definite) if q(v) > 0 (resp., q(v) < 0) for every nonzero vector v.[6] When q(v) assumes both positive and negative values, q is an indefinite quadratic form. The theorems of Jacobi and Sylvester show that any positive definite quadratic form in n variables can be brought to the sum of n squares by a suitable invertible linear transformation: geometrically, there is only one positive definite real quadratic form of every dimension. Its isometry group is a compact orthogonal group O(n). This stands in contrast with the case of indefinite forms, when the corresponding group, the indefinite orthogonal group O(p, q), is non-compact. Further, the isometry groups of Q and −Q are the same (O(p, q) ≈ O(q, p)), but the associated Clifford algebras (and hence pin groups) are different.An n-ary quadratic form over a field K is a homogeneous polynomial of degree 2 in n variables with coefficients in K:This formula may be rewritten using matrices: let x be the column vector with components x1, ..., xn and A = (aij) be the n×n matrix over K whose entries are the coefficients of q. ThenTwo n-ary quadratic forms φ and ψ over K are equivalent if there exists a nonsingular linear transformation C ∈ GL(n, K) such thatLet the characteristic of K be different from 2.[7] The coefficient matrix A of q may be replaced by the symmetric matrix (A + AT)/2 with the same quadratic form, so it may be assumed from the outset that A is symmetric. Moreover, a symmetric matrix A is uniquely determined by the corresponding quadratic form. Under an equivalence C, the symmetric matrix A of φ and the symmetric matrix B of ψ are related as follows:The associated bilinear form of a quadratic form q is defined byThus, bq is a symmetric bilinear form over K with matrix A. Conversely, any symmetric bilinear form b defines a quadratic formand these two processes are the inverses of one another.  As a consequence, over a field of characteristic not equal to 2, the theories of symmetric bilinear forms and of quadratic forms in n variables are essentially the same.A quadratic form q in n variables over K induces a map from the n-dimensional coordinate space Kn into K:The map Q is a homogeneous function of degree 2, which means that it has the property that, for all a in K and v in V:When the characteristic of K is not 2, the map B : V × V → K defined below is bilinear over K:This bilinear form B is symmetric, i.e. B(x, y) = B(y, x) for all x, y in V, and it determines Q: Q(x) = B(x, x)  for all x in V.When the characteristic of K is 2, so that 2 is not a unit, it is still possible to use a quadratic form to define a symmetric bilinear form B′(x, y) = Q(x + y) − Q(x) − Q(y). However, Q(x) can no longer be recovered from this B′ in the same way, since B′(x, x) = 0 for all x (and is thus alternating[8]). Alternately, there always exists a bilinear form B″ (not in general either unique or symmetric) such that B″(x, x) = Q(x).The pair (V, Q) consisting of a finite-dimensional vector space V over K and a quadratic map Q from V to K is called a quadratic space, and B as defined here is the associated symmetric bilinear form of Q. The notion of a quadratic space is a coordinate-free version of the notion of quadratic form. Sometimes, Q is also called a quadratic form.Two n-dimensional quadratic spaces (V, Q) and (V′, Q′) are isometric if there exists an invertible linear transformation T : V → V′ (isometry) such thatThe isometry classes of n-dimensional quadratic spaces over K correspond to the equivalence classes of n-ary quadratic forms over K.Let R be a commutative ring, M be an R-module and b : M × M → R be an R-bilinear form.[9]  A mapping Q : M → R : v ↦ b(v, v) is the associated quadratic form of b, and B : M × M → R : (u, v) ↦ Q(u + v) − Q(u) − Q(v) is the polar form of Q.Alternatively, a quadratic form Q : M → R may be characterized as follows:Two elements v and w of V are called orthogonal if B(v, w) = 0. The kernel of a bilinear form B consists of the elements that are orthogonal to every element of V. Q is non-singular if the kernel of its associated bilinear form is {0}. If there exists a non-zero v in V such that Q(v) = 0, the quadratic form Q is isotropic, otherwise it is anisotropic. This terminology also applies to vectors and subspaces of a quadratic space. If the restriction of Q to a subspace U of V is identically zero, U is totally singular.The orthogonal group of a non-singular quadratic form Q is the group of the linear automorphisms of V that preserve Q, i.e. the group of isometries of (V, Q) into itself.If a quadratic space (A, Q) has a product so that A is an algebra over a field, and satisfiesEvery quadratic form q in n variables over a field of characteristic not equal to 2 is equivalent to a diagonal formSuch a diagonal form is often denoted byClassification of all quadratic forms up to equivalence can thus be reduced to the case of diagonal forms.Quadratic forms over the ring of integers are called integral quadratic forms, whereas the corresponding modules are quadratic lattices (sometimes, simply lattices). They play an important role in number theory and topology.An integral quadratic form has integer coefficients, such as x2 + xy + y2; equivalently, given a lattice Λ in a vector space V (over a field with characteristic 0, such as Q or R), a quadratic form Q is integral with respect to Λ if and only if it is integer-valued on Λ, meaning Q(x, y) ∈ Z if x, y ∈ Λ.This is the current use of the term; in the past it was sometimes used differently, as detailed below.Historically there was some confusion and controversy over whether the notion of integral quadratic form should mean:This debate was due to the confusion of quadratic forms (represented by polynomials) and symmetric bilinear forms (represented by matrices), and "twos out" is now the accepted convention; "twos in" is instead the theory of integral symmetric bilinear forms (integral symmetric matrices).this is the convention Gauss uses in Disquisitiones Arithmeticae.Several points of view mean that twos out has been adopted as the standard convention. Those include:There are also forms whose image consists of all but one of the positive integers.  For example, {1,2,5,5} has 15 as the exception.  Recently, the 15 and 290 theorems have completely characterized universal integral quadratic forms: if all coefficients are integers, then it represents all positive integers if and only if it represents all integers up through 290; if it has an integral matrix, it represents all positive integers if and only if it represents all integers up through 15.
Kernel (linear algebra)
In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L : V → W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W.  That is, in set-builder notation,The kernel of L is a linear subspace of the domain V.[1]In the linear map L : V → W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:It follows that the image of L is isomorphic to the quotient of V by the kernel:This implies the rank–nullity theorem:where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L).  This is the generalization to linear operators of the row space, or coimage, of a matrix.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring.The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L: V → W is continuous if and only if the kernel of L is a closed subspace of V.Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K.The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,The matrix equation is equivalent to a homogeneous system of linear equations:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The kernel of an m × n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:The product Ax can be written in terms of the dot product of vectors as follows:Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank–nullity theoremThe left null space, or cokernel, of a matrix A consists of all vectors x such that xTA = 0T, where T denotes the transpose of a column vector.  The left null space of A is the same as the kernel of AT.  The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation.  The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The kernel also plays a role in the solution to a nonhomogeneous system of linear equations:If u and v are two possible solutions to the above equation, thenThus, the difference of any two solutions to the equation Ax = b lies in the kernel of A.It follows that any solution to the equation Ax = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel.  That is, the solution set to the equation Ax = b isGeometrically, this says that the solution set to Ax = b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).We give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Consider the matrixThe kernel of this matrix consists of all vectors (x, y, z) ∈ R3 for whichwhich can be expressed as a homogeneous system of linear equations involving x, y, and z:which can be written in matrix form as:Gauss–Jordan elimination reduces this to:Rewriting yields:Now we can express an element of the kernel:for c a scalar.Since c is a free variable, this can be expressed equally well as,The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (−1,−26,16)T constitutes a basis of the kernel of A.Thus, the nullity of A is 1.Note also that the following dot products are zero:which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (−1,−26,16)T.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.A basis of the kernel of a matrix may be computed by Gaussian elimination.In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.For example, suppose thatThenPutting the upper part in column echelon form by column operations on the whole matrix givesThe last three columns of B are zero columns. Therefore, the three last vectors of C,are a basis of the kernel of A.The problem of computing the kernel on a computer depends on the nature of the coefficients.If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic and Chinese remainder theorem, which reduces the problem to several similar ones over finite fields (this avoids the overhead induced by the non-linearity of the computational complexity of integer multiplication).[citation needed]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]
Cauchy–Schwarz inequality
In mathematics, the Cauchy–Schwarz inequality, also known as the Cauchy–Bunyakovsky–Schwarz inequality, is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics.[1]The inequality for sums was published by Augustin-Louis Cauchy (1821), while the corresponding inequality for integrals was first proved byViktor Bunyakovsky (1859). The modern proof of the integral inequality was given by Hermann Amandus Schwarz (1888).[1]or LetThen, by linearity of the inner product in its first argument, one haswhich givesThis establishes the theorem.Titu's lemma (named after Titu Andreescu, also known as T2 Lemma, Engel's form, or Sedrakyan's inequality) states that for positive reals, we havewhich yields the Cauchy–Schwarz inequality.For the inner product space of square-integrable complex-valued functions, one hasA generalization of this is the Hölder inequality.The triangle inequality for the standard norm is often shown as a consequence of the Cauchy–Schwarz inequality, as follows: given vectors x and y:Taking square roots gives the triangle inequality.The Cauchy–Schwarz inequality is used to prove that the inner product is a continuous function with respect to the topology induced by the inner product itself.[8][9]The Cauchy–Schwarz inequality allows one to extend the notion of "angle between two vectors" to any real inner-product space by defining:[10][11]The Cauchy–Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [−1, 1] and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space. It can also be used to define an angle in complex inner-product spaces, by taking the absolute value or the real part of the right-hand side,[12][13] as is done when extracting a metric from quantum fidelity.Let X, Y be random variables, then the covariance inequality[14][15] is given byAfter defining an inner product on the set of random variables using the expectation of their product,then the Cauchy–Schwarz inequality becomesVarious generalizations of the Cauchy–Schwarz inequality exist in the context of operator theory, e.g. for operator-convex functions and operator algebras, where the domain and/or range are replaced by a C*-algebra or W*-algebra.which extends verbatim to positive functionals on C*-algebras:The next two theorems are further examples in operator algebra.Another generalization is a refinement obtained by interpolating between both sides the Cauchy-Schwarz inequality:  It can be easily proven by Hölder's inequality.[23] There are also non commutative versions for operators and tensor products of matrices.[24]
Pseudovector
In physics and mathematics, a pseudovector (or axial vector) is a quantity that transforms like a vector under a proper rotation, but in three dimensions gains an additional sign flip under an improper rotation such as a reflection. Geometrically it is the opposite, of equal magnitude but in the opposite direction, of its mirror image. This is as opposed to a true vector, also known, in this context, as a polar vector, which on reflection matches its mirror image.In three dimensions, the pseudovector p is associated  with the curl of a polar vector or with the cross product of two  polar vectors a and b:[2]The vector p calculated this way is a pseudovector. One example is the normal to an oriented plane. An oriented plane can be defined by two non-parallel vectors, a and b,[3] that span the plane. The vector a × b is a normal to the plane (there are two normals, one on each side – the right-hand rule will determine which), and is a pseudovector. This has consequences in computer graphics where it has to be considered when transforming surface normals.A number of quantities in physics behave as pseudovectors rather than polar vectors, including magnetic field and angular velocity. In mathematics pseudovectors are equivalent to three-dimensional bivectors, from which the transformation rules of pseudovectors can be derived. More generally in n-dimensional geometric algebra pseudovectors are the elements of the algebra with dimension n − 1, written ⋀n−1Rn. The label "pseudo" can be further generalized to pseudoscalars and pseudotensors, both of which gain an extra sign flip under improper rotations compared to a true scalar or tensor.Physical examples of pseudovectors include magnetic field, torque, angular velocity, and angular momentum.Consider the pseudovector angular momentum L = r × p. Driving in a car, and looking forward, each of the wheels has an angular momentum vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the "reflection" of this angular momentum "vector" (viewed as an ordinary vector) points to the right, but the actual angular momentum vector of the wheel (which is still turning forward in the reflection) still points to the left, corresponding to the extra minus sign in the reflection of a pseudovector.The distinction between vectors and pseudovectors becomes important in understanding the effect of symmetry on the solution to physical systems. Consider an electric current loop in the z = 0 plane that inside the loop generates a magnetic field oriented in the z direction. This system is symmetric (invariant) under mirror reflections through this plane, with the magnetic field unchanged by the reflection. But reflecting the magnetic field as a vector through that plane would be expected to reverse it; this expectation is corrected by realizing that the magnetic field is a pseudovector, with the extra sign flip leaving it unchanged.The definition of a "vector" in physics (including both polar vectors and pseudovectors) is more specific than the mathematical definition of "vector" (namely, any element of an abstract vector space). Under the physics definition, a "vector" is required to have components that "transform" in a certain way under a proper rotation: In particular, if everything in the universe were rotated, the vector would rotate in exactly the same way. (The coordinate system is fixed in this discussion; in other words this is the perspective of active transformations.) Mathematically, if everything in the universe undergoes a rotation described by a rotation matrix R, so that a displacement vector x is transformed to x′ = Rx, then any "vector" v must be similarly transformed to v′ = Rv. This important requirement is what distinguishes a vector (which might be composed of, for example, the x-, y-, and z-components of velocity) from any other triplet of physical quantities (For example, the length, width, and height of a rectangular box cannot be considered the three components of a vector, since rotating the box does not appropriately transform these three components.)(In the language of differential geometry, this requirement is equivalent to defining a vector to be a tensor of contravariant rank one.)The discussion so far only relates to proper rotations, i.e. rotations about an axis. However, one can also consider improper rotations, i.e. a mirror-reflection possibly followed by a proper rotation. (One example of an improper rotation is inversion through a point in 3-dimensional space.) Suppose everything in the universe undergoes an improper rotation described by the rotation matrix R, so that a position vector x is transformed to x′ = Rx. If the vector v is a polar vector, it will be transformed to v′ = Rv. If it is a pseudovector, it will be transformed to v′ = −Rv.The transformation rules for polar vectors and pseudovectors can be compactly stated aswhere the symbols are as described above, and the rotation matrix R can be either proper or improper. The symbol det denotes determinant; this formula works because the determinant of proper and improper rotation matrices are +1 and −1, respectively.Suppose v1 and v2 are known pseudovectors, and v3 is defined to be their sum, v3 = v1 + v2. If the universe is transformed by a rotation matrix R, then v3 is transformed toSo v3 is also a pseudovector. Similarly one can show that the difference between two pseudovectors is a pseudovector, that the sum or difference of two polar vectors is a polar vector, that multiplying a polar vector by any real number yields another polar vector, and that multiplying a pseudovector by any real number yields another pseudovector.On the other hand, suppose v1 is known to be a polar vector, v2 is known to be a pseudovector, and v3 is defined to be their sum, v3 = v1 + v2. If the universe is transformed by a rotation matrix R, then v3 is transformed toTherefore, v3 is neither a polar vector nor a pseudovector. For an improper rotation, v3 does not in general even keep the same magnitude:If the magnitude of v3 were to describe a measurable physical quantity, that would mean that the laws of physics would not appear the same if the universe was viewed in a mirror. In fact, this is exactly what happens in the weak interaction: Certain radioactive decays treat "left" and "right" differently, a phenomenon which can be traced to the summation of a polar vector with a pseudovector in the underlying theory. (See parity violation.)For a rotation matrix R, either proper or improper, the following mathematical equation is always true:where v1 and v2 are any three-dimensional vectors. (This equation can be proven either through a geometric argument or through an algebraic calculation.)Suppose v1 and v2 are known polar vectors, and v3 is defined to be their cross product, v3 = v1 × v2. If the universe is transformed by a rotation matrix R, then v3 is transformed toSo v3 is a pseudovector. Similarly, one can show:This is isomorphic to addition modulo 2, where "polar" corresponds to 1 and "pseudo" to 0.From the definition, it is clear that a displacement vector is a polar vector. The velocity vector is a displacement vector (a polar vector) divided by time (a scalar), so is also a polar vector. Likewise, the momentum vector is the velocity vector (a polar vector) times mass (a scalar), so is a polar vector. Angular momentum is the cross product of a displacement (a polar vector) and momentum (a polar vector), and is therefore a pseudovector. Continuing this way, it is straightforward to classify any vector as either a pseudovector or polar vector.Above, pseudovectors have been discussed using active transformations. An alternate approach, more along the lines of passive transformations, is to keep the universe fixed, but switch "right-hand rule" with "left-hand rule" everywhere in math and physics, including in the definition of the cross product. Any polar vector (e.g., a translation vector) would be unchanged, but pseudovectors (e.g., the magnetic field vector at a point) would switch signs. Nevertheless, there would be no physical consequences, apart from in the parity-violating phenomena such as certain radioactive decays.[4]One way to formalize pseudovectors is as follows: if V is an n-dimensional vector space, then a pseudovector of V is an element of the (n − 1)-th exterior power of V: Λn−1(V). The pseudovectors of V form a vector space with the same dimension as V.This definition is not equivalent to that requiring a sign flip under improper rotations, but it is general to all vector spaces.  In particular,  when n is even, such a pseudovector does not experience a sign flip, and when the characteristic of the underlying field of V is 2, a sign flip has no effect. Otherwise, the definitions are equivalent, though it should be borne in mind that without additional structure (specifically, a volume form), there is no natural identification of ⋀n−1(V) with V.In geometric algebra the basic elements are vectors, and these are used to build a hierarchy of elements using the definitions of products in this algebra. In particular, the algebra builds pseudovectors from vectors.The basic multiplication in the geometric algebra is the geometric product, denoted by simply juxtaposing two vectors as in ab. This product is expressed as:where the leading term is the customary vector dot product and the second term is called the wedge product. Using the postulates of the algebra, all combinations of dot and wedge products can be evaluated. A terminology to describe the various combinations is provided. For example, a multivector is a summation of k-fold wedge products of various k-values. A k-fold wedge product also is referred to as a k-blade.In the present context the pseudovector is one of these combinations. This term is attached to a different multivector depending upon the dimensions of the space (that is, the number of linearly independent vectors in the space). In three dimensions, the most general 2-blade or bivector can be expressed as the wedge product of two vectors and is a pseudovector.[5] In four dimensions, however, the pseudovectors are trivectors.[6] In general, it is a (n − 1)-blade, where n is the dimension of the space and algebra.[7] An n-dimensional space has n basis vectors and also n basis pseudovectors. Each basis pseudovector is formed from the outer (wedge) product of all but one of the n basis vectors. For instance, in four dimensions where the basis vectors are taken to be {e1, e2, e3, e4}, the pseudovectors can be written as: {e234, e134, e124, e123}.The transformation properties of the pseudovector in three dimensions has been compared to that of the vector cross product by Baylis.[8] He says: "The terms axial vector and pseudovector are often treated as synonymous, but it is quite useful to be able to distinguish a bivector from its dual." To paraphrase Baylis: Given two polar vectors (that is, true vectors) a and b in three dimensions, the cross product composed from a and b is the vector normal to their plane given by c = a × b. Given a set of right-handed orthonormal basis vectors { eℓ }, the cross product is expressed in terms of its components as:where superscripts label vector components. On the other hand, the plane of the two vectors is represented by the exterior product or wedge product, denoted by a ∧ b. In this context of geometric algebra, this bivector is called a pseudovector, and is the Hodge dual of the cross product.[9] The dual of e1 is introduced as e23 ≡ e2e3 = e2 ∧ e3, and so forth. That is, the dual of e1 is the subspace perpendicular to e1, namely the subspace spanned by e2 and e3. With this understanding,[10]For details, see Hodge star operator § Three dimensions. The cross product and wedge product are related by:where i = e1 ∧ e2 ∧ e3 is called the unit pseudoscalar.[11][12] It has the property:[13]Using the above relations, it is seen that if the vectors a and b are inverted by changing the signs of their components while leaving the basis vectors fixed, both the pseudovector and the cross product are invariant. On the other hand, if the components are fixed and the basis vectors eℓ  are inverted, then the pseudovector is invariant, but the cross product changes sign. This behavior of cross products is consistent with their definition as vector-like elements that change sign under transformation from a right-handed to a left-handed coordinate system, unlike polar vectors.As an aside, it may be noted that not all authors in the field of geometric algebra use the term pseudovector, and some authors follow the terminology that does not distinguish between the pseudovector and the cross product.[14] However, because the cross product does not generalize to other than three dimensions,[15]the notion of pseudovector based upon the cross product also cannot be extended to a space of any other number of dimensions. The pseudovector as a (n – 1)-blade in an n-dimensional space is not restricted in this way.Another important note is that pseudovectors, despite their name, are "vectors" in the sense of being elements of a vector space. The idea that "a pseudovector is different from a vector" is only true with a different and more specific definition of the term "vector" as discussed above.
Determinant
In linear algebra, the determinant is a value that can be computed from the elements of a square matrix.  The determinant of a matrix A is denoted det(A), det A, or |A|. Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix.In the case of a 2 × 2 matrix the determinant may be defined as:Similarly, for a 3 × 3 matrix A, its determinant is:Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix A.  This procedure can be extended to give a recursive definition for the determinant of an n × n matrix, the minor expansion formula.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although other methods of solution are much more computationally efficient. In linear algebra, a matrix (with entries in a field) is invertible if and only if its determinant is non-zero, and correspondingly the matrix is singular if and only if its determinant is zero. This leads to the use of determinants in defining the characteristic polynomial of a matrix, whose roots are the eigenvalues. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. This leads to the use of determinants in calculus, the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants appear frequently in algebraic identities such as the Vandermonde identity.Determinants possess many algebraic properties, including that the determinant of a product of matrices is equal to the product of determinants. Special types of matrices have special determinants; for example, the determinant of an orthogonal matrix is always plus or minus one, and the determinant of a complex Hermitian matrix is always real.There are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns.  Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted.  For example, here is the result for a 4 × 4 matrix:Another way to define the determinant is expressed in terms of the columns of the matrix.  If we write an n × n matrix A in terms of its column vectorswhere b and c are scalars, v is any vector of size n and I is the identity matrix of size n.  These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar.  These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n × n matrix contributes n! terms), so it will first be given explicitly for the case of 2 × 2 matrices and 3 × 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.Assume A is a square matrix with n rows and n columns, so that it can be written asThe entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.The determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The Leibniz formula for the determinant of a 2 × 2 matrix isIf the matrix entries are real numbers, the matrix A can be used to represent two linear maps:  one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A.  In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping.  The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The absolute value of ad − bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).To show that ad − bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sinθ for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a⊥ = (−b, a), such that |a⊥||b|cosθ' , which can be determined by the pattern of the scalar product to be equal to ad − bc:Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.The object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) ∧ (c, d)) is the signed area, which is also the determinant ad − bc.[3]The Laplace formula for the determinant of a 3 × 3 matrix isthis can be expanded out to givewhich is the Leibniz formula for the determinant of a 3 × 3 matrix.The rule of Sarrus is a mnemonic for the 3 × 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 × 3 matrix does not carry over into higher dimensions.The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The Leibniz formula for the determinant of an n × n matrix A isHere the sum is computed over all permutations σ of the set {1, 2, …, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ1 = 2, σ2 = 3, and σ3 = 1.  The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.is notation for the product of the entries at positions (i, σi), where i ranges from 1 to n:For example, the determinant of a 3 × 3 matrix A (n = 3) isor using two epsilon symbols aswhere now each ir and each jr should be summed over 1, …, n.The determinant has many properties. Some basic properties of determinants areThis can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:Properties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from n × n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n ≥ 2,[6] so there is no good definition of the determinant in this setting.Property 2 above implies that properties for columns have their counterparts in terms of rows:Property 5 says that the determinant on n × n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 7; this is essentially the method of Gaussian elimination.For example, the determinant ofcan be computed using the following matrices:Here, B is obtained from A by adding −1/2×the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = −det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (−2) · 2 · 4.5 = −18. Therefore, det(A) = −det(D) = +18.The following identity holds for a Schur complement of a square matrix:The Schur complement arises as the result of performing a block Gaussian elimination by multiplying the matrix M from the right with a block lower triangular matrixHere Ip denotes a p×p identity matrix. After multiplication with the matrix L the Schur complement appears in the upper p×p block. The product matrix isThat is, we have effected a Gaussian decompositionThe first and last matrices on the RHS have determinant unity,  so we haveThis is Schur's determinant identity.The determinant of a matrix product of square matrices equals the product of their determinants:Thus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) → K that maps M ↦ det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized  to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n−1) × (n−1)-matrix that results from A by removing the i-th row and the j-th column. The expression (−1)i + jMi,j is known as a cofactor. The determinant of A is given byCalculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 × 3 matrixalong the second column (j = 2 and the sum runs over i) is given by,However, Laplace expansion is efficient for small matrices only.The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,In terms of the adjugate matrix, Laplace's expansion can be written as[7]Sylvester's determinant theorem states that for A, an m × n matrix, and B, an n × m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):where Im and In are the m × m and n × n identity matrices, respectively.From this general result several consequences follow.The product of all non-zero eigenvalues is referred to as pseudo-determinant.Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationwhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatricesbeing positive, for all k between 1 and n.The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,or, for real matrices A,Here exp(A) denotes the matrix exponential of A, because every eigenvalue λ of A corresponds to the eigenvalue exp(λ) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingthe determinant of A is given byFor example, for n = 2, n = 3, and n = 4, respectively,cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic n, detA = (−)nc0 the signed constant term of the characteristic polynomial, determined recursively fromIn the general case, this may also be obtained from[9]where the sum is taken over the set of all integers kl ≥ 0 satisfying the equationThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = −(l – 1)! tr(Al) asThis formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1, i2, …, ir) and J = (j1, j2, …, jr). The product and trace of such matrices are defined in a natural way asAn important arbitrary  dimension n  identity can be obtained from the  Mercator series expansion of the logarithm when the expansion converges.  If every eigenvalue of A is less than 1 in absolute value,where I is the identity matrix.  More generally, if is expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I + sA).For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantwith equality if and only if  A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.Also,These inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.For a matrix equationthe solution is given by Cramer's rule:where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.It has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.Suppose A, B, C, and D are matrices of dimension n × n, n × m, m × n, and m × m, respectively. ThenThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)When A is invertible, one hasas can be seen by employing the decompositionWhen the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 × 2 matrix holds:[12]Generally, if all pairs of n × n matrices of the np × np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix  obtained by computing the determinant of the block matrix considering its entries as the entries of a p × p matrix.[13] As the previous formula shows, for p = 2, this criterion is sufficient, but not necessary.When A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)When D is a 1×1 matrix, B is a column vector, and C is a row vector thenIt can be seen, e.g. using the Leibniz formula, that the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn × n to R, and so it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]where adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveExpressed in terms of the entries of A, these areYet another equivalent formulation isThis identity is used in describing the tangent space of certain matrix Lie groups.The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X−1BX. Indeed, repeatedly applying the above identities yieldsThe determinant is therefore also called a similarity invariant. The determinant of a linear transformationfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant of a linear transformation A : V → V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear mapAs ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThis definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 ∧ v2 ∧ v3 ∧ … ∧ vn to v2 ∧ v1 ∧ v3 ∧ … ∧ vn, say, also changes its sign.For this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with k < n.The vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one.  To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define (T′w)(x1, …, xn) = w(Tx1, …, Txn).  As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple.  We call this scalar the determinant of T.The determinant can also be characterized as the unique functionfrom the set of all n × n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.This fact also implies that every other n-linear alternating function F: Mn(K) → K satisfiesThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.The determinant defines a mappingbetween the group of invertible n × n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R → S, there is a map GLn(f): GLn(R) → GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityholds. In other words, the following diagram commutes:For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaAnother infinite-dimensional notion of determinant is the functional determinant.For operators in a finite factor, one may define a positive real-valued determinant called the Fuglede−Kadison determinant using the canonical trace. In fact, corresponding to every tracial state on a von Neumann algebra there is a notion of Fuglede−Kadison determinant.For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Computational geometry, however, does frequently use calculations related to determinants.[18]Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n × n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)The LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies toIf the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[19] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If two matrices of order n can be multiplied in time M(n), where M(n) ≥ na for some a > 2, then the determinant can be computed in time O(M(n)).[20] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation.  Unfortunately this interesting method does not always work in its original form.Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[21] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[22]Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, 2 × 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[23][24][25][26]In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).It was Vandermonde (1771) who first recognized determinants as independent functions.[23] Laplace (1772)[27][28] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,[29][30] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[23][31] With him begins the theory in its generality.The next important figure was Jacobi[24] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[32][33]The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 × 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be n − 1 times differentiable), the Wronskian is defined to beIt is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 × 2 or 3 × 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn → Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn → Rm is represented by the m × n matrix A, then the n-dimensional volume of f(S) is given by:By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)·|det(a − b, b − c, c − d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.For a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. Forthe Jacobian matrix is the n × n matrix whose entries are given byIts determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function φ: Rn → Rm is given byThe Jacobian also occurs in the inverse function theorem.The third order Vandermonde determinant isIn general, the nth-order Vandermonde determinant is[34]where the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.Second orderThird orderwhere ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is[34]where ωj is an nth root of 1.
Levi-Civita symbol
In mathematics, particularly in linear algebra, tensor analysis, and differential geometry, the Levi-Civita symbol represents a collection of numbers; defined from the sign of a permutation of the natural numbers 1, 2, …, n, for some positive integer n. It is named after the Italian mathematician and physicist Tullio Levi-Civita. Other names include the permutation symbol, antisymmetric symbol, or alternating symbol, which refer to its antisymmetric property and definition in terms of permutations.The standard letters to denote the Levi-Civita symbol are the Greek lower case epsilon ε or ϵ, or less commonly the Latin lower case e. Index notation allows one to display permutations in a way compatible with tensor analysis:where each index i1, i2, ..., in takes values 1, 2, ..., n. There are nn indexed values of εi1i2…in, which can be arranged into an n-dimensional array. The key defining property of the symbol is total antisymmetry in all the indices. When any two indices are interchanged, equal or not, the symbol is negated:If any two indices are equal, the symbol is zero. When all indices are unequal, we have:where p (called the parity of the permutation) is the number of pairwise interchanges of indices necessary to unscramble i1, i2, ..., in into the order 1, 2, ..., n, and the factor (−1)p is called the sign or signature of the permutation. The value ε1 2 ... n must be defined, else the particular values of the symbol for all permutations are indeterminate. Most authors choose ε1 2 ... n = +1, which means the Levi-Civita symbol equals the sign of a permutation when the indices are all unequal. This choice is used throughout this article.The term "n-dimensional Levi-Civita symbol" refers to the fact that the number of indices on the symbol n matches the dimensionality of the vector space in question, which may be Euclidean or non-Euclidean, for example, ℝ3 or Minkowski space. The values of the Levi-Civita symbol are independent of any metric tensor and coordinate system. Also, the specific term "symbol" emphasizes that it is not a tensor because of how it transforms between coordinate systems; however it can be interpreted as a tensor density.The Levi-Civita symbol allows the determinant of a square matrix, and the cross product of two vectors in three-dimensional Euclidean space, to be expressed in index notation.The Levi-Civita symbol is most often used in three and four dimensions, and to some extent in two dimensions, so these are given here before defining the general case.In two dimensions, Levi-Civita symbol is defined by:The values can be arranged into a 2 × 2 antisymmetric matrix:Use of the two-dimensional symbol is relatively uncommon, although in certain specialized topics like supersymmetry[1] and twistor theory[2] it appears in the context of 2-spinors. The three- and higher-dimensional Levi-Civita symbols are used more commonly.In three dimensions, the Levi-Civita symbol is defined by:[3]That is, εijk is 1 if (i, j, k) is an even permutation of (1, 2, 3), −1 if it is an odd permutation, and 0 if any index is repeated. In three dimensions only, the cyclic permutations of (1, 2, 3) are all even permutations, similarly the anticyclic permutations are all odd permutations. This means in 3d it is sufficient to take cyclic or anticyclic permutations of (1, 2, 3) and easily obtain all the even or odd permutations.Analogous to 2-dimensional matrices, the values of the 3-dimensional Levi-Civita symbol can be arranged into a 3 × 3 × 3 array:where i is the depth (blue: i = 1; red: i = 2; green: i = 3), j is the row and k is the column.Some examples:In four dimensions, the Levi-Civita symbol is defined by:These values can be arranged into a 4 × 4 × 4 × 4 array, although in 4 dimensions and higher this is difficult to draw.Some examples:More generally, in n dimensions, the Levi-Civita symbol is defined by:[4]Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.Using the capital pi notation ∏ for ordinary multiplication of numbers, an explicit expression for the symbol is:where the signum function (denoted sgn) returns the sign of its argument while discarding the absolute value if nonzero. The formula is valid for all index values, and for any n (when n = 0 or n = 1, this is the empty product). However, computing the formula above naively has a time complexity of O(n2), whereas the sign can be computed from the parity of the permutation from its disjoint cycles in only O(n log(n)) cost.A tensor whose components in an orthonormal basis are given by the Levi-Civita symbol (a tensor of covariant rank n) is sometimes called a permutation tensor.Under the ordinary transformation rules for tensors the Levi-Civita symbol is unchanged under pure rotations, consistent with that it is (by definition) the same in all coordinate systems related by orthogonal transformations. However, the Levi-Civita symbol is a pseudotensor because under an orthogonal transformation of Jacobian determinant −1, for example, a reflection in an odd number of dimensions, it should acquire a minus sign if it were a tensor. As it does not change at all, the Levi-Civita symbol is, by definition, a pseudotensor. As the Levi-Civita symbol is a pseudotensor, the result of taking a cross product is a pseudovector, not a vector.[5]Under a general coordinate change, the components of the permutation tensor are multiplied by the Jacobian of the transformation matrix. This implies that in coordinate frames different from the one in which the tensor was defined, its components can differ from those of the Levi-Civita symbol by an overall factor. If the frame is orthonormal, the factor will be ±1 depending on whether the orientation of the frame is the same or not.[5]In index-free tensor notation, the Levi-Civita symbol is replaced by the concept of the Hodge dual.In a context where tensor index notation is used to manipulate tensor components, the Levi-Civita symbol may be written with its indices as either subscripts or superscripts with no change in meaning, as might be convenient. Thus, one could writeIn these examples, superscripts should be considered equivalent with subscripts.Summation symbols can be eliminated by using Einstein notation, where an index repeated between two or more terms indicates summation over that index.  For example,In the following examples, Einstein notation is used.In two dimensions, when all i, j, m, n each take the values 1 and 2,[3]    ( 1)    ( 2)    ( 3)In three dimensions, when all i, j, k, m, n each take values 1, 2, and 3:[3]    ( 4)    ( 5)    ( 6)The Levi-Civita symbol is related to the Kronecker delta. In three dimensions, the relationship is given by the following equations (vertical lines denote the determinant):[4]A special case of this result is (4):sometimes called the "contracted epsilon identity".In Einstein notation, the duplication of the i index implies the sum on i. The previous is then denoted εijkεimn = δjmδkn − δjnδkm.In n dimensions, when all i1, …,in, j1, ..., jn take values 1, 2, ..., n:    ( 7)    ( 8)    ( 9)where the exclamation mark (!) denotes the factorial, and δα…β… is the generalized Kronecker delta. For any n, the propertyfollows from the facts that In general, for n dimensions, one can write the product of two Levi-Civita symbols as:For (1), both sides are antisymmetric with respect of ij and mn. We therefore only need to consider the case i ≠ j and m ≠ n. By substitution, we see that the equation holds for ε12ε12, that is, for i = m = 1 and j = n = 2. (Both sides are then one). Since the equation is antisymmetric in ij and mn, any set of values for these can be reduced to the above case (which holds). The equation thus holds for all values of ij and mn.Using (1), we have for (2)Here we used the Einstein summation convention with i going from 1 to 2. Next, (3) follows similarly from (2).To establish (5), notice that both sides vanish when i ≠ j. Indeed, if i ≠ j, then one can not choose m and n such that both permutation symbols on the left are nonzero. Then, with i = j fixed, there are only two ways to choose m and n from the remaining two indices. For any such indices, we have(no summation), and the result follows.Then (6) follows since 3! = 6 and for any distinct indices i, j, k taking values 1, 2, 3, we have In linear algebra, the determinant of a 3 × 3 square matrix A = [aij] can be written[6]Similarly the determinant of an n × n matrix A = [aij] can be written as[5]where each ir should be summed over 1, …, n, or equivalently:where now each ir and each jr should be summed over 1, …, n. More generally, we have the identity[5]If a = (a1, a2, a3) and b = (b1, b2, b3) are vectors in ℝ3 (represented in some right-handed coordinate system using an orthonormal basis), their cross product can be written as a determinant:[5]hence also using the Levi-Civita symbol, and more simply:In Einstein notation, the summation symbols may be omitted, and the ith component of their cross product equals[4]The first component isthen by cyclic permutations of 1, 2, 3 the others can be derived immediately, without explicitly calculating them from the above formulae:From the above expression for the cross product, we have:If c = (c1, c2, c3) is a third vector, then the triple scalar product equalsFrom this expression, it can be seen that the triple scalar product is antisymmetric when exchanging any pair of arguments. For example,If F = (F1, F2, F3) is a vector field defined on some open set of ℝ3 as a function of position x = (x1, x2, x3) (using Cartesian coordinates). Then the ith component of the curl of F equals[4]which follows from the cross product expression above, substituting components of the gradient vector operator (nabla).In any arbitrary curvilinear coordinate system and even in the absence of a metric on the manifold, the Levi-Civita symbol as defined above may be considered to be a tensor density field in two different ways. It may be regarded as a contravariant tensor density of weight +1 or as a covariant tensor density of weight −1. In n dimensions using the generalized Kronecker delta,[7][8]Notice that these are numerically identical. In particular, the sign is the same.On a pseudo-Riemannian manifold, one may define a coordinate-invariant covariant tensor field whose coordinate representation agrees with the Levi-Civita symbol wherever the coordinate system is such that the basis of the tangent space is orthonormal with respect to the metric and matches a selected orientation. This tensor should not be confused with the tensor density field mentioned above. The presentation in this section closely follows Carroll 2004.The covariant Levi-Civita tensor (also known as the Riemannian volume form) in any coordinate system that matches the selected orientation iswhere gab is the representation of the metric in that coordinate system. We can similarly consider a contravariant Levi-Civita tensor by raising the indices with the metric as usual,but notice that if the metric signature contains an odd number of negatives q, then the sign of the components of this tensor differ from the standard Levi-Civita symbol:From this we can infer the identity,whereis the generalized Kronecker delta.In Minkowski space (the four-dimensional spacetime of special relativity), the covariant Levi-Civita tensor iswhere the sign depends on the orientation of the basis.  The contravariant Levi-Civita tensor isThe following are examples of the general identity above specialized to Minkowski space (with the negative sign arising from the odd number of negatives in the signature of the metric tensor in either sign convention):This article incorporates material from Levi-Civita permutation symbol on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
Line (geometry)
The notion of line or straight line was introduced by ancient mathematicians to represent straight objects (i.e., having no curvature) with negligible width and depth. Lines are an idealization of such objects. Until the 17th century, lines were defined in this manner: "The [straight or curved] line is the first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which […] will leave from its imaginary moving some vestige in length, exempt of any width. […] The straight line is that which is equally extended between its points."[1]Euclid described a line as "breadthless length" which "lies equally with respect to the points on itself"; he introduced several postulates as basic unprovable properties from which he constructed all of geometry, which is now called Euclidean geometry to avoid confusion with other geometries which have been introduced since the end of the 19th century (such as non-Euclidean, projective and affine geometry).In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.When a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry.  Thus in differential geometry a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.All definitions are ultimately circular in nature since they depend on concepts which must themselves have definitions, a dependence which cannot be continued indefinitely without returning to the starting point. To avoid this vicious circle certain concepts must be taken as primitive concepts; terms which are given no definition.[2] In geometry, it is frequently the case that the concept of line is taken as a primitive.[3] In those situations where a line is a defined concept, as in coordinate geometry, some other fundamental ideas are taken as primitives. When the line concept is a primitive, the behaviour and properties of lines are dictated by the axioms which they must satisfy.In a non-axiomatic or simplified axiomatic treatment of geometry, the concept of a primitive notion may be too abstract to be dealt with. In this circumstance it is possible that a description or mental image of a primitive notion is provided to give a foundation to build the notion on which would formally be based on the (unstated) axioms. Descriptions of this type may be referred to, by some authors, as definitions in this informal style of presentation. These are not true definitions and could not be used in formal proofs of statements. The "definition" of line in Euclid's Elements falls into this category.[4] Even in the case where a specific geometry is being considered (for example, Euclidean geometry), there is no generally accepted agreement among authors as to what an informal description of a line should be when the subject is not being treated formally.When geometry was first formalised by Euclid in the Elements, he defined a general line (straight or curved) to be "breadthless length" with a straight line being a line "which lies evenly with the points on itself".[5] These definitions serve little purpose since they use terms which are not, themselves, defined. In fact, Euclid did not use these definitions in this work and probably included them just to make it clear to the reader what was being discussed. In modern geometry, a line is simply taken as an undefined object with properties given by axioms,[6] but is sometimes defined as a set of points obeying a linear relationship when some other fundamental concept is left undefined.In an axiomatic formulation of Euclidean geometry, such as that of Hilbert (Euclid's original axioms contained various flaws which have been corrected by modern mathematicians),[7] a line is stated to have certain properties which relate it to other lines and points. For example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect in at most one point.[8] In two dimensions, i.e., the Euclidean plane, two lines which do not intersect are called parallel. In higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not.Any collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines.Lines in a Cartesian plane or, more generally, in affine coordinates, can be described algebraically by linear equations. In two dimensions, the equation for non-vertical lines is often given in the slope-intercept form:where:with fixed real coefficients a, b and c such that a and b are not both zero. Using this form, vertical lines correspond to the equations with b = 0.There are many variant ways to write the equation of a line which can all be converted from one to another by algebraic manipulation. These forms (see Linear equation for other forms) are generally named by the type of information (data) about the line that is needed to write down the form. Some of the important data of a line is its slope, x-intercept, known points on the line and y-intercept.If x0 ≠ x1, this equation may be rewritten asorIn three dimensions, lines can not be described by a single linear equation, so they are frequently described by parametric equations:where:They may also be described as the simultaneous solutions of two linear equationsThe normal form (also called the Hesse normal form,[9] after the German mathematician Ludwig Otto Hesse), is based on the normal segment for a given line, which is defined to be the line segment drawn from the origin perpendicular to the line. This segment joins the origin with the closest point on the line to the origin.  The normal form of the equation of a straight line on the plane is given by:Unlike the slope-intercept and intercept forms, this form can represent any line but also requires only two finite parameters, θ and p, to be specified. If p > 0, then θ is uniquely defined modulo 2π. On the other hand, if the line is through the origin (c = 0, p = 0), one drops the c/|c| term to compute sinθ and cosθ, and θ is only defined modulo π.In polar coordinates on the Euclidean plane the slope-intercept form of the equation of a line is expressed as:where m is the slope of the line and b is the y-intercept.  When θ = 0 the graph will be undefined. The equation can be rewritten to eliminate discontinuities in this manner:In polar coordinates on the Euclidean plane, the intercept form of the equation of a line that is non-horizontal, non-vertical, and does not pass through pole may be expressed as,Similarly, a horizontal line that doesn't pass through the pole is given by the equationThe equation of a line which passes through the pole is simply given as:where m is the slope of the line.A ray starting at point A is described by limiting λ. One ray is obtained if λ ≥ 0, and the opposite ray comes from λ ≤ 0.In three-dimensional space, a first degree equation in the variables x, y, and z defines a plane, so two such equations, provided the planes they give rise to are not parallel, define a line which is the intersection of the planes. More generally, in n-dimensional space n-1 first-degree equations in the n coordinate variables define a line under suitable conditions.In more general Euclidean space, Rn (and analogously in every other affine space), the line L passing through two different points a and b (considered as vectors) is the subsetThe direction of the line is from a (t = 0) to b (t = 1), or in other words, in the direction of the vector b − a. Different choices of a and b can yield the same line.Three points are said to be collinear if they lie on the same line. Three points usually determine a plane, but in the case of three collinear points this does not happen.In affine coordinates, in n-dimensional space the points X=(x1, x2, ..., xn), Y=(y1, y2, ..., yn), and Z=(z1, z2, ..., zn) are collinear if the matrixhas a rank less than 3. In particular, for three points in the plane (n = 2), the above matrix is square and the points are collinear if and only if its determinant is zero.Equivalently for three points in a plane, the points are collinear if and only if the slope between one pair of points equals the slope between any other pair of points (in which case the slope between the remaining pair of points will equal the other slopes). By extension, k points in a plane are collinear if and only if  any (k–1) pairs of points have the same pairwise slopes.In Euclidean geometry, the Euclidean distance d(a,b) between two points a and b may be used to express the collinearity between three points by:[10][11]However, there are other notions of distance (such as the Manhattan distance) for which this property is not true.In the geometries where the concept of a line is a primitive notion, as may be the case in some synthetic geometries, other methods of determining collinearity are needed.In a sense,[12] all lines in Euclidean geometry are equal, in that, without coordinates, one can not tell them apart from one another. However, lines may play special roles with respect to other objects in the geometry and be divided into types according to that relationship. For instance, with respect to a conic (a circle, ellipse, parabola, or hyperbola), lines can be:In the context of determining parallelism in Euclidean geometry, a transversal is a line that intersects two other lines that may or not be parallel to each other.For more general algebraic curves, lines could also be:With respect to triangles we have:For a convex quadrilateral with at most two parallel sides, the Newton line is the line that connects the midpoints of the two diagonals.For a hexagon with vertices lying on a conic we have the Pascal line and, in the special case where the conic is a pair of lines, we have the Pappus line.Parallel lines are lines in the same plane that never cross. Intersecting lines share a single point in common. Coincidental lines coincide with each other—every point that is on either one of them is also on the other.Perpendicular lines are lines that intersect at right angles.In three-dimensional space, skew lines are lines that are not in the same plane and thus do not intersect each other.In many models of projective geometry, the representation of a line rarely conforms to the notion of the "straight curve" as it is visualised in Euclidean geometry.  In elliptic geometry we see a typical example of this.[13] In the spherical representation of elliptic geometry, lines are represented by great circles of a sphere with diametrically opposite points identified. In a different model of elliptic geometry, lines are represented by Euclidean planes passing through the origin. Even though these representations are visually distinct, they satisfy all the properties (such as, two points determining a unique line) that make them suitable representations for lines in this geometry.Given a line and any point A on it, we may consider A as decomposing this line into two parts.Each such part is called a ray (or half-line) and the point A is called its initial point. The point A is considered to be a member of the ray.[14] Intuitively, a ray consists of those points on a line passing through A and proceeding indefinitely, starting at A, in one direction only along the line.  However, in order to use this concept of a ray in proofs a more precise definition is required.Given distinct points A and B, they determine a unique ray with initial point A.  As two points define a unique line, this ray consists of all the points between A and B (including A and B) and all the points C on the line through A and B such that B is between A and C.[15] This is, at times, also expressed as the set of all points C such that A is not between B and C.[16] A point D, on the line determined by A and B but not in the ray with initial point A determined by B, will determine another ray with initial point A. With respect to the AB ray, the AD ray is called the opposite ray.Thus, we would say that two different points, A and B, define a line and a decomposition of this line into the disjoint union of an open segment (A, B) and two rays, BC and AD (the point D is not drawn in the diagram, but is to the left of A on the line AB). These are not opposite rays since they have different initial points.In Euclidean geometry two rays with a common endpoint form an angle.The definition of a ray depends upon the notion of betweenness for points on a line. It follows that rays exist only for geometries for which this notion exists, typically Euclidean geometry or affine geometry over an ordered field. On the other hand, rays do not exist in projective geometry nor in a geometry over a non-ordered field, like the complex numbers or any finite field.In topology, a ray in a space X is a continuous embedding R+ → X. It is used to define the important concept of end of the space.A line segment is a part of a line that is bounded by two distinct end points and contains every point on the line between its end points.  Depending on how the line segment is defined, either of the two end points may or may not be part of the line segment.  Two or more line segments may have some of the same relationships as lines, such as being parallel, intersecting, or skew, but unlike lines they may be none of these, if they are coplanar and either do not intersect or are collinear.The "shortness" and "straightness" of a line, interpreted as the property that the distance along the line between any two of its points is minimized (see triangle inequality), can be generalized and leads to the concept of geodesics in metric spaces.
Permanent (mathematics)
In linear algebra, the permanent of a square matrix is a function of the matrix similar to the determinant. The permanent, as well as the determinant, is a polynomial in the entries of the matrix.[1] Both are special cases of a more general function of a matrix called the immanant.The permanent of an n-by-n matrix A = (ai,j) is defined asThe sum here extends over all elements σ of the symmetric group Sn; i.e. over all permutations of the numbers 1, 2, ..., n.For example,andThe definition of the permanent of A differs from that of the determinant of A in that the signatures of the permutations are not taken into account.The word, permanent, originated with Cauchy in 1812 as “fonctions symétriques permanentes” for a related type of function,[2] and was used by Muir (1882) in the modern, more specific, sense.[3]On the other hand, the basic multiplicative property of determinants is not valid for permanents.[6] A simple example shows that this is so.A formula similar to Laplace's for the development of a determinant along a row, column or diagonal is also valid for the permanent;[7] all signs have to be ignored for the permanent. For example, expanding along the first column,while expanding along the last row gives,Unlike the determinant, the permanent has no easy geometrical interpretation; it is mainly used in combinatorics, in treating boson Green's functions in quantum field theory, and in determining state probabilities of boson sampling systems[8]. However, it has two graph-theoretic interpretations: as the sum of weights of cycle covers of a directed graph, and as the sum of weights of perfect matchings in a bipartite graph.If the weight of a cycle-cover is defined to be the product of the weights of the arcs in each cycle, thenThus the permanent of A is equal to the sum of the weights of all perfect matchings of the graph.The answers to many counting questions can be computed as permanents of matrices that only have 0 and 1 as entries. Let Ω(n,k) be the class of all (0, 1)-matrices of order n with each row and column sum equal to k. Every matrix A in this class has perm(A) > 0.[10] The incidence matrices of projective planes are in the class Ω(n2 + n + 1, n + 1) for n an integer > 1. The permanents corresponding to the smallest projective planes have been calculated. For n = 2, 3, and 4 the values are 24, 3852 and 18,534,400 respectively.[10] Let Z be the incidence matrix of the projective plane with n = 2, the Fano plane. Remarkably, perm(Z) = 24 = |det (Z)|, the absolute value of the determinant of Z. This is a consequence of Z being a circulant matrix and the theorem:[11]where J is the n×n all 1's matrix and I is the identity matrix, and the ménage numbers are given bywhere I' is the (0, 1)-matrix with nonzero entries in positions (i, i + 1) and (n, 1).The Bregman–Minc inequality, conjectured by H. Minc in 1963[12] and proved by L. M. Brégman in 1973,[13] gives an upper bound for the permanent of an n × n (0, 1)-matrix.  If A has ri ones in row i for each 1 ≤ i ≤ n, the inequality states thatIn 1926 Van der Waerden conjectured that the minimum permanent among all n × n doubly stochastic matrices is n!/nn, achieved by the matrix for which all entries are equal to 1/n.[14] Proofs of this conjecture were published in 1980 by B. Gyires[15] and in 1981 by G. P. Egorychev[16] and D. I. Falikman;[17] Egorychev's proof is an application of the Alexandrov–Fenchel inequality.[18] For this work, Egorychev and Falikman won the Fulkerson Prize in 1982.[19]It may be rewritten in terms of the matrix entries as follows:MacMahon's Master Theorem relating permanents and determinants is:[24]where P(n,m) is the set of all m-permutations of the n-set {1,2,...,n}.[26]The generalization of the definition of a permanent to non-square matrices allows the concept to be used in a more natural way in some applications. For instance:Let S1, S2, ..., Sm be subsets (not necessarily distinct) of an n-set with m ≤ n. The incidence matrix of this collection of subsets is an m × n (0,1)-matrix A. The number of systems of distinct representatives (SDR's) of this collection is perm(A).[27]
Rank factorization
Consider the matrixIt is straightforward to check that
General linear group
In mathematics, the general linear group of degree n is the set of n×n invertible matrices, together with the operation of ordinary matrix multiplication. This forms a group, because the product of two invertible matrices is again invertible, and the inverse of an invertible matrix is invertible. The group is so named because the columns of an invertible matrix are linearly independent, hence the vectors/points they define are in general linear position, and matrices in the general linear group take points in general linear position to points in general linear position.To be more precise, it is necessary to specify what kind of objects may appear in the entries of the matrix. For example, the general linear group over R (the set of real numbers) is the group of n×n invertible matrices of real numbers, and is denoted by GLn(R) or GL(n, R).More generally, the general linear group of degree n over any field F (such as the complex numbers), or a ring R (such as the ring of integers), is the set of n×n invertible matrices with entries from F (or R), again with matrix multiplication as the group operation.[1] Typical notation is GLn(F) or GL(n, F), or simply GL(n) if the field is understood.More generally still, the general linear group of a vector space GL(V) is the abstract automorphism group, not necessarily written as matrices.The special linear group, written SL(n, F) or SLn(F), is the subgroup of GL(n, F) consisting of matrices with a determinant of 1.The group GL(n, F) and its subgroups are often called linear groups or matrix groups (the abstract group GL(V) is a linear group but not a matrix group). These groups are important in the theory of group representations, and also arise in the study of spatial symmetries and symmetries of vector spaces in general, as well as the study of polynomials. The modular group may be realised as a quotient of the special linear group SL(2, Z).If n ≥ 2, then the group GL(n, F) is not abelian.If V is a vector space over the field F, the general linear group of V, written GL(V) or Aut(V), is the group of all automorphisms of V, i.e. the set of all bijective linear transformations V → V, together with functional composition as group operation. If V has finite dimension n, then GL(V) and GL(n, F) are isomorphic. The isomorphism is not canonical; it depends on a choice of basis in V. Given a basis (e1, ..., en) of V and an automorphism T in GL(V), we have then for every basis vector ei thatfor some constants aij in F; the matrix corresponding to T is then just the matrix with entries given by the aij.In a similar way, for a commutative ring R the group GL(n, R) may be interpreted as the group of automorphisms of a free R-module M of rank n. One can also define GL(M) for any R-module, but in general this is not isomorphic to GL(n, R) (for any n).Over a field F, a matrix is invertible if and only if its determinant is nonzero. Therefore, an alternative definition of GL(n, F) is as the group of matrices with nonzero determinant.Over a commutative ring R, more care is needed: a matrix over R is invertible if and only if its determinant is a unit in R, that is, if its determinant is invertible in R. Therefore, GL(n, R) may be defined as the group of matrices whose determinants are units.Over a non-commutative ring R, determinants are not at all well behaved. In this case, GL(n, R) may be defined as the unit group of the matrix ring M(n, R).The general linear group GL(n, R) over the field of real numbers is a real Lie group of dimension n2. To see this, note that the set of all n×n real matrices, Mn(R), forms a real vector space of dimension n2. The subset GL(n, R) consists of those matrices whose determinant is non-zero. The determinant is a polynomial map, and hence GL(n, R) is an open affine subvariety of Mn(R) (a non-empty open subset of Mn(R) in the Zariski topology), and therefore[2]a smooth manifold of the same dimension.As a manifold, GL(n, R) is not connected but rather has two connected components: the matrices with positive determinant and the ones with negative determinant. The identity component, denoted by GL+(n, R), consists of the real n×n matrices with positive determinant. This is also a Lie group of dimension n2; it has the same Lie algebra as GL(n, R).The group GL(n, R) is also noncompact. "The"[3] maximal compact subgroup of GL(n, R) is the orthogonal group O(n), while "the" maximal compact subgroup of GL+(n, R) is the special orthogonal group SO(n). As for SO(n), the group GL+(n, R) is not simply connected (except when n = 1), but rather has a fundamental group isomorphic to Z for n = 2 or Z2 for n > 2.The general linear group over the field of complex numbers, GL(n, C), is a complex Lie group of complex dimension n2. As a real Lie group (through realification) it has dimension 2n2. The set of all real matrices forms a real Lie subgroup. These correspond to the inclusionswhich have real dimensions n2, 2n2, and 4n2 = (2n)2. Complex n-dimensional matrices can be characterized as real 2n-dimensional matrices that preserve a linear complex structure — concretely, that commute with a matrix J such that J2 = −I, where J corresponds to multiplying by the imaginary unit i.The Lie algebra corresponding to GL(n, C) consists of all n×n complex matrices with the commutator serving as the Lie bracket.Unlike the real case, GL(n, C) is connected. This follows, in part, since the multiplicative group of complex numbers C∗ is connected. The group manifold GL(n, C) is not compact; rather its maximal compact subgroup is the unitary group U(n). As for U(n), the group manifold GL(n, C) is not simply connected but has a fundamental group isomorphic to Z.If F is a finite field with q elements, then we sometimes write GL(n, q) instead of GL(n, F). When p is prime, GL(n, p) is the outer automorphism group  of the group Zpn, and also the automorphism group, because Zpn is abelian, so the inner automorphism group is trivial.The order of GL(n, q) is: For example, GL(3, 2) has order (8 − 1)(8 − 2)(8 − 4) = 168. It is the automorphism group of the Fano plane and of the group Z23, and is also known as PSL(2, 7).More generally, one can count points of Grassmannian over F: in other words the number of subspaces of a given dimension k. This requires only finding the order of the stabilizer subgroup of one such subspace and dividing into the formula just given, by the orbit-stabilizer theorem.These formulas are connected to the Schubert decomposition of the Grassmannian, and are q-analogs of the Betti numbers of complex Grassmannians. This was one of the clues leading to the Weil conjectures.Note that in the limit q ↦ 1 the order of GL(n, q) goes to 0! – but under the correct procedure (dividing by (q − 1)n) we see that it is the order of the symmetric group (See Lorscheid's article) – in the philosophy of the field with one element, one thus interprets the symmetric group as the general linear group over the field with one element: Sn ≅ GL(n, 1).The general linear group over a prime field, GL(ν, p), was constructed and its order computed by Évariste Galois in 1832, in his last letter (to Chevalier) and second (of three) attached manuscripts, which he used in the context of studying the Galois group of the general equation of order pν.[4]The special linear group, SL(n, F), is the group of all matrices with determinant 1. They are special in that they lie on a subvariety – they satisfy a polynomial equation (as the determinant is a polynomial in the entries). Matrices of this type form a group as the determinant of the product of two matrices is the product of the determinants of each matrix. SL(n, F) is a normal subgroup of GL(n, F).If we write F× for the multiplicative group of F (excluding 0), then the determinant is a group homomorphismthat is surjective and its kernel is the special linear group. Therefore, by the first isomorphism theorem, GL(n, F)/SL(n, F) is isomorphic to F×. In fact, GL(n, F) can be written as a semidirect product:When F is R or C, SL(n, F) is a Lie subgroup of GL(n, F) of dimension n2 − 1. The Lie algebra of SL(n, F) consists of all n×n matrices over F with vanishing trace. The Lie bracket is given by the commutator.The special linear group SL(n, R) can be characterized as the group of volume and orientation preserving linear transformations of Rn.The group SL(n, C) is simply connected, while SL(n, R) is not. SL(n, R) has the same fundamental group as GL+(n, R), that is, Z for n = 2 and Z2 for n > 2.The set of all invertible diagonal matrices forms a subgroup of GL(n, F) isomorphic to (F×)n. In fields like R and C, these correspond to rescaling the space; the so-called dilations and contractions.A scalar matrix is a diagonal matrix which is a constant times the identity matrix. The set of all nonzero scalar matrices forms a subgroup of GL(n, F) isomorphic to F× . This group is the center of GL(n, F). In particular, it is a normal, abelian subgroup.The center of SL(n, F) is simply the set of all scalar matrices with unit determinant, and is isomorphic to the group of nth roots of unity in the field F.The so-called classical groups are subgroups of GL(V) which preserve some sort of bilinear form on a vector space V. These include theThese groups provide important examples of Lie groups.The projective linear group PGL(n, F) and the projective special linear group PSL(n, F) are the quotients of GL(n, F) and SL(n, F) by their centers (which consist of the multiples of the identity matrix therein); they are the induced action on the associated projective space.The affine group Aff(n, F) is an extension of GL(n, F) by the group of translations in Fn. It can be written as a semidirect product:where GL(n, F) acts on Fn in the natural manner. The affine group can be viewed as the group of all affine transformations of the affine space underlying the vector space Fn.One has analogous constructions for other subgroups of the general linear group: for instance, the special affine group is the subgroup defined by the semidirect product, SL(n, F) ⋉ Fn, and the Poincaré group is the affine group associated to the Lorentz group, O(1, 3, F) ⋉ Fn.The general semilinear group ΓL(n, F) is the group of all invertible semilinear transformations, and contains GL. A semilinear transformation is a transformation which is linear "up to a twist", meaning "up to a field automorphism under scalar multiplication". It can be written as a semidirect product:where Gal(F) is the Galois group of F (over its prime field), which acts on GL(n, F) by the Galois action on the entries.The main interest of ΓL(n, F) is that the associated projective semilinear group PΓL(n, F) (which contains PGL(n, F)) is the collineation group of projective space, for n > 2, and thus semilinear maps are of interest in projective geometry.If one removes the restriction of the determinant being non-zero, the resulting algebraic structure is a monoid, usually called the full linear monoid,[6][7][8] but occasionally also full linear semigroup,[9] general linear monoid[10][11] etc. It is actually a regular semigroup.[7]The infinite general linear group or stable general linear group is the direct limit of the inclusions GL(n, F) → GL(n + 1, F) as the upper left block matrix. It is denoted by either GL(F) or GL(∞, F), and can also be interpreted as invertible infinite matrices which differ from the identity matrix in only finitely many places.[12]It is used in algebraic K-theory to define K1, and over the reals has a well-understood topology, thanks to Bott periodicity.It should not be confused with the space of (bounded) invertible operators on a Hilbert space, which is a larger group, and topologically much simpler, namely contractible – see Kuiper's theorem.
Pointwise
Examples includeSee pointwise product, scalar.Pointwise operations inherit such properties as associativity, commutativity and distributivity from corresponding operations on the codomain. An example of an operation on functions which is not pointwise is convolution.In order theory it is common to define a pointwise partial order on functions. With A, B posets, the set of functions A → B can be ordered by f ≤ g if and only if (∀x ∈ A) f(x) ≤ g(x). Pointwise orders also inherit some properties of the underlying posets. For instance if A and B are continuous lattices, then so is the set of functions A → B with pointwise order.[1] Using the pointwise order on functions one can concisely define other important notions, for instance:[2]An example of infinitary pointwise relation is pointwise convergence of functions — a sequence of functions withFor order theory examples:This article incorporates material from Pointwise on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
Bernstein polynomial
In the mathematical field of numerical analysis, a Bernstein polynomial, named after Sergei Natanovich Bernstein, is a polynomial in the Bernstein form, that is a linear combination of Bernstein basis polynomials.A numerically stable way to evaluate polynomials in Bernstein form is de Casteljau's algorithm.Polynomials in Bernstein form were first used by Bernstein in a constructive proof for the Stone–Weierstrass approximation theorem. With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bézier curves.The n + 1 Bernstein basis polynomials of degree n are defined asThe Bernstein basis polynomials of degree n form a basis for the vector space Πn of polynomials of degree at most n.A linear combination of Bernstein basis polynomialsis called a Bernstein polynomial or polynomial in Bernstein form of degree n.[1]The first few Bernstein basis polynomials are:The Bernstein basis polynomials have the following properties:and by the inverse binomial transformation the reverse transformation is[2]Let ƒ be a continuous function on the interval [0, 1]. Consider the Bernstein polynomialIt can be shown thatuniformly on the interval [0, 1].[3]  This is a stronger statement than the proposition that the limit holds for each value of x separately; that would be pointwise convergence rather than uniform convergence. Specifically, the word uniformly signifies thatBernstein polynomials thus afford one way to prove the Weierstrass approximation theorem that every real-valued continuous function on a real interval [a, b] can be uniformly approximated by polynomial functions over R.[4]A more general statement for a function with continuous kth derivative iswhere additionallyis an eigenvalue of Bn; the corresponding eigenfunction is a polynomial of degree k.Suppose K is a random variable distributed as the number of successes in n independent Bernoulli trials with probability x of success on each trial; in other words, K has a binomial distribution with parameters n and x.  Then we have the expected value E(K/n) = x.By the weak law of large numbers of probability theory,for every δ > 0. Moreover, this relation holds uniformly in x, which can be seen from its proof via Chebyshev's inequality, taking into account that the variance of K/n, equal to x(1-x)/n, is bounded from above by 1/(4n) irrespective of x.Because ƒ, being continuous on a closed bounded interval, must be uniformly continuous on that interval, one infers a statement of the formuniformly in x. Taking into account that ƒ is bounded (on the given interval) one gets for the expectationuniformly in x. To this end one splits the sum for the expectation in two parts. On one part the difference does not exceed ε; this part cannot contribute more than ε.On the other part the difference exceeds ε, but does not exceed 2M, where M is an upper bound for |ƒ(x)|; this part cannot contribute more than 2M times the small probability that the difference exceeds ε.Finally, one observes that the absolute value of the difference between expectations never exceeds the expectation of the absolute value of the difference, and that E(ƒ(K/n)) is just the Bernstein polynomial Bn(ƒ, x).See for instance.[5]
Jordan normal form
In linear algebra, a Jordan normal form (often called Jordan canonical form)[1]of a linear operator on a finite-dimensional vector space is an upper triangular matrix of a particular form called a Jordan matrix, representing the operator with respect to some basis. Such a matrix has each non-zero off-diagonal entry equal to 1, immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them.Let V be a vector space over a field K. Then a basis with respect to which the matrix has the required form exists if and only if all eigenvalues of the matrix lie in K, or equivalently if the characteristic polynomial of the operator splits into linear factors over K. This condition is always satisfied if K is algebraically closed (for instance, if it is the field of complex numbers). The diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue.[2][3][4]If the operator is originally given by a square matrix M, then its Jordan normal form is also called the Jordan normal form of M. Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given M is not entirely unique, as it is a block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.[2][3][4]The Jordan–Chevalley decomposition is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the Jordan normal form.[5][6][7]The Jordan normal form is named after Camille Jordan, who first stated the Jordan decomposition theorem in 1870.[8]Some textbooks have the ones on the subdiagonal, i.e., immediately below the main diagonal instead of on the superdiagonal.  The eigenvalues are still on the main diagonal.[9][10]An n × n matrix A is diagonalizable if and only if the sum of the dimensions of the eigenspaces is n. Or, equivalently, if and only if A has n linearly independent eigenvectors. Not all matrices are diagonalizable. Consider the following matrix:Including multiplicity, the eigenvalues of A are λ = 1, 2, 4, 4. The dimension of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so A is not diagonalizable. However, there is an invertible matrix P such that A = PJP−1, whereThe matrix J is almost diagonal. This is the Jordan normal form of A. The section Example below fills in the details of the computation.In general, a square complex matrix A is similar to a block diagonal matrixwhere each block Ji is a square matrix of the formSo there exists an invertible matrix P such that P−1AP = J is such that the only non-zero entries of J are on the diagonal and the superdiagonal. J is called the Jordan normal form of A. Each Ji is called a Jordan block of A. In a given Jordan block, every entry on the superdiagonal is 1.Assuming this result, we can deduce the following properties:Consider the matrix A from the example in the previous section. The Jordan normal form is obtained by some similarity transformation P−1AP = J, i.e.,Let P have column vectors pi, i = 1, ..., 4, thenWe see thatThus, given an eigenvalue λ, its corresponding Jordan block gives rise to a Jordan chain. The generator, or lead vector, say pr, of the chain is a generalized eigenvector such that (A − λ I)rpr = 0, where r is the size of the Jordan block. The vector p1 =  (A − λ I)r−1pr is an eigenvector corresponding to λ. In general, pi is a preimage of pi−1 under A − λ I. So the lead vector generates the chain via multiplication by (A − λ I).[12][13]Therefore, the statement that every square matrix A can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of A.We give a proof by induction that any complex-valued matrix A may be put in Jordan normal form. The 1 × 1 case is trivial. Let A be an n × n matrix. Take any eigenvalue λ of A. The range of A − λ I, denoted by Ran(A − λ I), is an invariant subspace of A. Also, since λ is an eigenvalue of A, the dimension of Ran(A − λ I), r, is strictly less than n. Let A'  denote the restriction of A to Ran(A − λ I), By inductive hypothesis, there exists a basis {p1, ..., pr} such that A' , expressed with respect to this basis, is in Jordan normal form.Next consider the subspace Ker(A − λ I). Ifthe desired result follows immediately from the rank–nullity theorem. This would be the case, for example, if A was Hermitian.Otherwise, iflet the dimension of Q be s ≤ r. Each vector in Q is an eigenvector of A'  corresponding to eigenvalue λ. So the Jordan form of A'  must contain s Jordan chains corresponding to s linearly independent eigenvectors. So the basis {p1, ..., pr} must contain s vectors, say {pr−s+1, ..., pr}, that are lead vectors in these Jordan chains from the Jordan normal form of A'. We can "extend the chains" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(A − λ I).) Let qi be such thatClearly no non-trivial linear combination of the qi can lie in Ker(A − λ I). Furthermore, no non-trivial linear combination of the qi can be in Ran(A − λ I), for that would contradict the assumption that each pi is a lead vector in a Jordan chain. The set {qi}, being preimages of the linearly independent set {pi} under A − λ I, is also linearly independent.Finally, we can pick any linearly independent set {z1, ..., zt} that spansBy construction, the union of the three sets {p1, ..., pr}, {qr−s +1, ..., qr}, and  {z1, ..., zt} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of A. Finally, by the rank–nullity theorem, the cardinality of the union is n. In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of A, and this shows A can be put in Jordan normal form.It can be shown that the Jordan normal form of a given matrix A is unique up to the order of the Jordan blocks.Knowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of A. Assuming the algebraic multiplicity m(λ) of an eigenvalue λ is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (A − λ I)m(λ). To see this, suppose an n × n matrix A has only one eigenvalue λ. So m(λ) = n. The smallest integer k1 such thatis the size of the largest Jordan block in the Jordan form of A. (This number k1 is also called the index of λ. See discussion in a following section.) The rank ofis the number of Jordan blocks of size k1. Similarly, the rank ofis twice the number of Jordan blocks of size k1 plus the number of Jordan blocks of size k1−1. The general case is similar.This can be used to show the uniqueness of the Jordan form. Let J1 and J2 be two Jordan normal forms of A. Then J1 and J2 are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of J1 and J2. This proves the uniqueness part of the statement.This real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form complex conjugate pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.One can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.Using the Jordan normal form, direct calculation gives a spectral mapping theorem for the polynomial functional calculus: Let A be an n × n matrix with eigenvalues λ1, ..., λn, then for any polynomial p, p(A) has eigenvalues p(λ1), ..., p(λn).The Cayley–Hamilton theorem asserts that every matrix A satisfies its characteristic equation: if p is the characteristic polynomial of A, then p(A) = 0. This can be shown via direct calculation in the Jordan form, since any Jordan block for λ is annihilated by (X − λ)m where m is the multiplicity of the root λ of p, the sum of the sizes of the Jordan blocks for λ, and therefore no less than the size of the block in question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the splitting field of p; this field extension does not change the matrix p(A) in any way.The minimal polynomial P of a square matrix A is the unique monic polynomial of least degree, m, such that P(A) = 0. Alternatively, the set of polynomials that annihilate a given A form an ideal I in C[x], the principal ideal domain of polynomials with complex coefficients. The monic element that generates I is precisely P.Let λ1, ..., λq be the distinct eigenvalues of A, and si be the size of the largest Jordan block corresponding to λi. It is clear from the Jordan normal form that the minimal polynomial of A has degree Σsi.While the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of elementary divisors. The elementary divisors of a square matrix A are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial m are the elementary divisors of the largest degree corresponding to distinct eigenvalues.The degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, A is diagonalizable.The Jordan form of a n × n matrix A is block diagonal, and therefore gives a decomposition of the n dimensional Euclidean space into invariant subspaces of A. Every Jordan block Ji corresponds to an invariant subspace Xi. Symbolically, we putwhere each Xi is the span of the corresponding Jordan chain, and k is the number of Jordan chains.One can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue λi, the size of its largest corresponding Jordan block si is called the index of  λi and denoted by ν(λi). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace Yi byThis gives the decompositionwhere l is the number of distinct eigenvalues of A. Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where A is a multiple of the identity matrix we have k = n and l = 1.The projection onto Yi and along all the other Yj ( j ≠ i ) is called the spectral projection of A at λi and is usually denoted by P(λi ; A). Spectral projections are mutually orthogonal in the sense that P(λi ; A) P(λj ; A) = 0 if i ≠ j. Also they commute with A and their sum is the identity matrix. Replacing every λi in the Jordan matrix J by one and zeroising all other entries gives P(λi ; J), moreover if U J U−1 is the similarity transformation such that A = U J U−1 then P(λi ; A) = U P(λi ; J) U−1. They are not confined to finite dimensions. See below for their application to compact operators, and in holomorphic functional calculus for a more general discussion.Comparing the two decompositions, notice that, in general, l ≤ k. When A is normal, the subspaces Xi's in the first decomposition are one-dimensional and mutually orthogonal. This is the spectral theorem for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.It might be of interest here to note some properties of the index, ν(λ). More generally, for a complex number λ, its index can be defined as the least non-negative integer ν(λ) such thatSo ν(λ) > 0 if and only if λ is an eigenvalue of A. In the finite-dimensional case, ν(λ) ≤ the algebraic multiplicity of λ.Jordan reduction can be extended to any square matrix M whose entries lie in a field K.  The result states that any M can be written as a sum D + N where D is semisimple, N is nilpotent, and DN = ND. This is called the Jordan–Chevalley decomposition. Whenever K contains the eigenvalues of M, in particular when K is algebraically closed, the normal form can be expressed explicitly as the direct sum of Jordan blocks. Similar to the case when K is the complex numbers, knowing the dimensions of the kernels of (M − λI)k for 1 ≤ k ≤ m, where m is the algebraic multiplicity of the eigenvalue λ, allows one to determine the Jordan form of M. We may view the underlying vector space V as a K[x]-module by regarding the action of x on V as application of M and extending by K-linearity. Then the polynomials (x − λ)k are the elementary divisors of M, and the Jordan normal form is concerned with representing M in terms of blocks associated to the elementary divisors.The proof of the Jordan normal form is usually carried out as an application to the ring K[x] of the structure theorem for finitely generated modules over a principal ideal domain, of which it is a corollary.In a different direction, for compact operators on a Banach space, a result analogous to the Jordan normal form holds. One restricts to compact operators because every point x in the spectrum of a compact operator T, the only exception being when x is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.Let X be a Banach space, L(X) be the bounded operators on X, and σ(T) denote the spectrum of T ∈ L(X). The holomorphic functional calculus is defined as follows:Fix a bounded operator T. Consider the family Hol(T) of complex functions that is holomorphic on some open set G containing σ(T). Let Γ = {γi} be a finite collection of Jordan curves such that σ(T) lies in the inside of Γ, we define f(T) byThe open set G could vary with f and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar case. Although the integral makes sense for continuous f, we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that σ(T) lie in the inside of Γ ensures f(T) is well defined; it does not depend on the choice of Γ. The functional calculus is the mapping Φ from Hol(T) to L(X) given byWe will require the following properties of this functional calculus:In the finite-dimensional case, σ(T) = {λi} is a finite discrete set in the complex plane. Let ei be the function that is 1 in some open neighborhood of λi and 0 elsewhere. By property 3 of the functional calculus, the operatoris a projection. Moreoever, let νi be the index of λi andThe spectral mapping theorem tells ushas spectrum {0}. By property 1, f(T) can be directly computed in the Jordan form, and by inspection, we see that the operator f(T)ei(T) is the zero matrix.By property 3, f(T) ei(T) = ei(T) f(T). So ei(T) is precisely the projection ontothe subspaceThe relationimplieswhere the index i runs through the distinct eigenvalues of T. This is exactly the invariant subspace decompositiongiven in a previous section. Each ei(T) is the projection onto the subspace spanned by the Jordan chains corresponding to λi and along the subspaces spanned by the Jordan chains corresponding to λj for j ≠ i. In other words, ei(T) = P(λi;T). This explicit identification of the operators ei(T) in turn gives an explicit form of holomorphic functional calculus for matrices:Notice that the expression of f(T) is a finite sum because, on each neighborhood of λi, we have chosen the Taylor series expansion of f centered at λi.Let T be a bounded operator λ be an isolated point of σ(T). (As stated above, when T is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)The point λ is called a pole of operator T with order ν if the resolvent function RT defined byhas a pole of order ν at λ.We will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators.Consider the annular region A centered at the eigenvalue λ with sufficiently small radius ε such that the intersection of the open disc Bε(λ) and σ(T) is {λ}. The resolvent function RT is holomorphic on A.Extending a result from classical function theory, RT has a Laurent series representation on A:whereBy the previous discussion on the functional calculus,But we have shown that the smallest positive integer m such thatis precisely the index of λ, ν(λ). In other words, the function RT has a pole of order ν(λ) at λ.This example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.Consider the matrixwhich is mentioned in the beginning of the article.The characteristic polynomial of A isThis shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation Av = λ v. It is spanned by the column vector v = (−1, 1, 0, 0)T. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by w = (1, −1, 0, 1)T. Finally, the eigenspace corresponding to the eigenvalue 4 is also one-dimensional (even though this is a double eigenvalue) and is spanned by x = (1, 0, −1, 1)T. So, the geometric multiplicity (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix A is the direct sumThere are three chains. Two have length one: {v} and {w}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculatewhere I is the 4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of A − 4I, e.g., y = (1,0,0,0)T. Now, (A − 4I)y = x and (A − 4I)x = 0, so {y, x} is a chain of length two corresponding to the eigenvalue 4.The transition matrix P such that P−1AP = J is formed by putting these vectors next to each other as followsA computation shows that the equation P−1AP = J indeed holds.If we had interchanged the order of which the chain vectors appeared, that is, changing the order of v, w and {x, y} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.If the matrix A has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrixIf ε = 0, then the Jordan normal form is simplyHowever, for ε ≠ 0, the Jordan normal form isThis ill conditioning makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in numerical analysis; the stable Schur decomposition[15] or pseudospectra[16] are better alternatives.The Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let f(z) be an analytical function of a complex argument. Applying the function on a n×n Jordan block J with eigenvalue λ results in an upper triangular matrix:The following example shows the application to the power function f(z)=zn:
Seven-dimensional cross product
In mathematics, the seven-dimensional cross product is a bilinear operation on vectors in seven-dimensional Euclidean space. It assigns to any two vectors a, b in R7 a vector a × b also in R7.[1] Like the cross product in three dimensions, the seven-dimensional product is anticommutative and a × b is orthogonal both to a and to b. Unlike in three dimensions, it does not satisfy the Jacobi identity, and while the three-dimensional cross product is unique up to a sign, there are many seven-dimensional cross products. The seven-dimensional cross product has the same relationship to the octonions as the three-dimensional product does to the quaternions.The seven-dimensional cross product is one way of generalising the cross product to other than three dimensions, and it is the only other non-trivial bilinear product of two vectors that is vector-valued, anticommutative and orthogonal.[2] In other dimensions there are vector-valued products of three or more vectors that satisfy these conditions, and binary products with bivector results.The product can be given by a multiplication table, such as the one here. This table, due to Cayley,[3][4] gives the product of basis vectors ei and ej for each i, j from 1 to 7. For example, from the tableThe table can be used to calculate the product of any two vectors. For example, to calculate the e1 component of x × y the basis vectors that multiply to produce e1 can be picked out to giveThis can be repeated for the other six components.There are 480 such tables, one for each of the products satisfying the definition.[5] This table can be summarized by the relation[4]The top left 3 × 3 corner of this table gives the cross product in three dimensions.The cross product on a Euclidean space V is a bilinear map  from V × V to V, mapping vectors x and y in V to another vector x × y also in V, where x × y has the properties[1][6]where (x·y) is the Euclidean dot product and |x| is the Euclidean norm. The first property states that the product is perpendicular to its arguments, while the second property gives the magnitude of the product. An equivalent expression in terms of the angle θ between the vectors[7] is[8]which is the area of the parallelogram in the plane of x and y with the two vectors as sides.[9] A third statement of the magnitude condition isif x × x = 0 is assumed as a separate axiom.[10]Given the properties of bilinearity, orthogonality and magnitude, a nonzero cross product exists only in three and seven dimensions.[2][8][10]  This can be shown by postulating the properties required for the cross product, then deducing an equation which is only satisfied when the dimension is 0, 1, 3 or 7. In zero dimensions there is only the zero vector, while in one dimension all vectors are parallel, so in both these cases the product must be identically zero.The restriction to 0, 1, 3 and 7 dimensions is related to Hurwitz's theorem, that normed division algebras are only possible in 1, 2, 4 and 8 dimensions. The cross product is formed from the product of the normed division algebra by restricting it to the 0, 1, 3, or 7 imaginary dimensions of the algebra, giving nonzero products in only three and seven dimensions.[11]In contrast the three-dimensional cross product, which is unique (apart from sign), there are many possible binary cross products in seven dimensions. One way to see this is to note that given any pair of vectors x and y ∈ ℝ7 and any vector v of magnitude |v| = |x||y| sin θ in the five-dimensional space perpendicular to the plane spanned by x and y, it is possible to find a cross product with a multiplication table (and an associated set of basis vectors) such that x × y = v. Unlike in three dimensions, x × y = a × b does not imply that a and b lie in the same plane as x and y.[8]Further properties follow from the definition, including the following identities:Other properties follow only in the three-dimensional case, and are not satisfied by the seven-dimensional cross product, notably,To define a particular cross product, an orthonormal basis {ej} may be selected and a multiplication table provided that determines all the products {ei × ej}. One possible multiplication table is described in the Example section, but it is not unique.[5] Unlike three dimensions, there are many tables because every pair of unit vectors is perpendicular to five other unit vectors, allowing many choices for each cross product.Once we have established a multiplication table, it is then applied to general vectors x and y by expressing x and y in terms of the basis and expanding x × y through bilinearity.  Using e1  to e7 for the basis vectors a different multiplication table from the one in the Introduction, leading to a different cross product, is given with anticommutativity by[8]More compactly this rule can be written aswith i = 1...7 modulo 7 and the indices i, i + 1 and i + 3 allowed to permute evenly. Together with anticommutativity this generates the product. This rule directly produces the two diagonals immediately adjacent to the diagonal of zeros in the table. Also, from an identity in the subsection on consequences,which produces diagonals further out, and so on.The ej component of cross product x × y is given by selecting all occurrences of ej in the table and collecting the corresponding components of x from the left column and of y from the top row. The result is:As the cross product is bilinear the operator x×– can be written as a matrix, which takes the form[citation needed]The cross product is then given byTwo different multiplication tables have been used in this article, and there are more.[5][12] These multiplication tables are characterized by the Fano plane,[13][14] and these are shown in the figure for the two tables used here: at top, the one described by Sabinin, Sbitneva, and Shestakov, and at bottom that described by Lounesto. The numbers under the Fano diagrams (the set of lines in the diagram) indicate a set of indices for seven independent products in each case, interpreted as ijk → ei × ej = ek. The multiplication table is recovered from the Fano diagram by following either the straight line connecting any three points, or the circle in the center, with a sign as given by the arrows. For example, the first row of multiplications resulting in e1 in the above listing is obtained by following the three paths connected to e1 in the lower Fano diagram: the circular path e2 × e4, the diagonal path e3 × e7, and the edge path e6 × e1 = e5 rearranged using one of the above identities as:oralso obtained directly from the diagram with the rule that any two unit vectors on a straight line are connected by multiplication to the third unit vector on that straight line with signs according to the arrows (sign of the permutation that orders the unit vectors).It can be seen that both multiplication rules follow from the same Fano diagram by simply renaming the unit vectors, and changing the sense of the center unit vector. Considering all possible permutations of the basis there are 480 multiplication tables and so 480 cross products like this.[14]The product can also be calculated using geometric algebra. The product starts with the exterior product, a bivector valued product of two vectors:This is bilinear, alternate, has the desired magnitude, but is not vector valued. The vector, and so the cross product, comes from the product of this bivector with a trivector. In three dimensions up to a scale factor there is only one trivector, the pseudoscalar of the space, and a product of the above bivector and one of the two unit trivectors gives the vector result, the dual of the bivector.A similar calculation is done is seven dimensions, except as trivectors form a 35-dimensional space there are many trivectors that could be used, though not just any trivector will do. The trivector that gives the same product as the above coordinate transform isThis is combined with the exterior product to give the cross productJust as the 3-dimensional cross product can be expressed in terms of the quaternions, the 7-dimensional cross product can be expressed in terms of the octonions. After identifying ℝ7 with the imaginary octonions (the orthogonal complement of the real line in O), the cross product is given in terms of octonion multiplication byConversely, suppose V is a 7-dimensional Euclidean space with a given cross product. Then one can define a bilinear multiplication on ℝ⊕V as follows:The space ℝ⊕V with this multiplication is then isomorphic to the octonions.[16]The cross product only exists in three and seven dimensions as one can always define a multiplication on a space of one higher dimension as above, and this space can be shown to be a normed division algebra. By Hurwitz's theorem such algebras only exist in one, two, four, and eight dimensions, so the cross product must be in zero, one, three or seven dimensions. The products in zero and one dimensions are trivial, so non-trivial cross products only exist in three and seven dimensions.[17][18]The failure of the 7-dimension cross product to satisfy the Jacobi identity is due to the nonassociativity of the octonions. In fact,where [x, y, z] is the associator.In three dimensions the cross product is invariant under the action of the rotation group, SO(3), so the cross product of x and y after they are rotated is the image of x × y under the rotation. But this invariance is not true in seven dimensions; that is, the cross product is not invariant under the group of rotations in seven dimensions, SO(7). Instead it is invariant under the exceptional Lie group G2, a subgroup of SO(7).[8][16]Nonzero binary cross products exist only in three and seven dimensions. Further products are possible when lifting the restriction that it must be a binary product.[19][20] We require the product to be multi-linear, alternating, vector-valued, and orthogonal to each of the input vectors ai. The orthogonality requirement implies that in n dimensions, no more than n − 1 vectors can be used. The magnitude of the product should equal the volume of the parallelotope with the vectors as edges, which can be calculated using the Gram determinant. The conditions areThe Gram determinant is the squared volume of the parallelotope with a1, ..., ak as edges.With these conditions a non-trivial cross product only exists:One version of the product of three vectors in eight dimensions is given byThere are also trivial products. As noted already, a binary product only exists in 7, 3, 1 and 0 dimensions, the last two being identically zero. A further trivial 'product' arises in even dimensions, which takes a single vector and produces a vector of the same magnitude orthogonal to it through the left contraction with a suitable bivector. In two dimensions this is a rotation through a right angle.
Row and column vectors
In linear algebra, a column vector or column matrix is an m × 1 matrix, that is, a matrix consisting of a single column of m elements,Similarly, a row vector or row matrix is a 1 × m matrix, that is, a matrix consisting of a single row of m elements[1]Throughout, boldface is used for the row and column vectors. The transpose (indicated by T) of a row vector is a column vectorand the transpose of a column vector is a row vectorThe set of all row vectors forms a vector space called row space, similarly the set of all column vectors forms a vector space called column space. The dimensions of the row and column spaces equals the number of entries in the row or column vector.The column space can be viewed as the dual space to the row space, since any linear functional on the space of column vectors can be represented uniquely as an inner product with a specific row vector.To simplify writing column vectors in-line with other text, sometimes they are written as row vectors with the transpose operation applied to them.orSome authors also use the convention of writing both column vectors and row vectors as rows, but separating row vector elements with commas and column vector elements with semicolons (see alternative notation 2 in the table below). Matrix multiplication involves the action of multiplying each row vector of one matrix by each column vector of another matrix. The dot product of two vectors a and b is equivalent to the matrix product of the row vector representation of a and the column vector representation of b,which is also equivalent to the matrix product of the row vector representation of b and the column vector representation of a,The matrix product of a column and a row vector gives the outer product of two vectors a and b, an example of the more general tensor product. The matrix product of the column vector representation of a and the row vector representation of b gives the components of their dyadic product,which is the transpose of the matrix product of the column vector representation of b and the row vector representation of a,Frequently a row vector presents itself for an operation within n-space expressed by an n × n matrix M,Then p is also a row vector and may present to another n × n matrix Q,Conveniently, one can write t = p Q = v MQ telling us that the matrix product transformation MQ can take v directly to t.  Continuing with row vectors, matrix transformations further reconfiguring n-space can be applied to the right of previous outputs.In contrast, when a column vector is transformed to become another column under an n × n matrix action, the operation occurs to the left,leading to the algebraic expression QM v  for the composed output from v input. The matrix transformations mount up to the left in this use of a column vector for input to matrix transformation.Nevertheless, using the transpose operation these differences between inputs of a row or column nature are resolved by an antihomomorphism between the groups arising on the two sides. The technical construction uses the dual space associated with a vector space to develop the transpose of a linear map.For an instance where this row vector input convention has been used to good effect see Raiz Usmani,[2] where on page 106 the convention allows the statement "The product mapping ST of U into W [is given] by:(The Greek letters represent row vectors).Ludwik Silberstein used row vectors for spacetime events; he applied Lorentz transformation matrices on the right in his Theory of Relativity in 1914 (see page 143).In 1963 when McGraw-Hill published Differential Geometry by Heinrich Guggenheimer of the University of Minnesota, he uses the row vector convention in chapter 5, "Introduction to transformation groups" (eqs. 7a,9b and 12 to 15). When H. S. M. Coxeter reviewed[3] Linear Geometry by Rafael Artzy, he wrote, "[Artzy] is to be congratulated on his choice of the 'left-to-right' convention, which enables him to regard a point as a row matrix instead of the clumsy column that many authors prefer." J. W. P. Hirschfeld used right multiplication of row vectors by matrices in his description of projectivities on the Galois geometry PG(1,q).[4]In the study of stochastic processes with a stochastic matrix, it is conventional to use a row vector as the stochastic vector.[5]
Mathematical analysis
Mathematical analysis is the branch of mathematics dealing with limitsand related theories, such as differentiation, integration, measure, infinite series, and analytic functions.[1][2]These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).Mathematical analysis formally developed in the 17th century during the Scientific Revolution,[3] but many of its ideas can be traced back to earlier mathematicians. Early results in analysis were implicitly present in the early days of ancient Greek mathematics. For instance, an infinite geometric sum is implicit in Zeno's paradox of the dichotomy.[4] Later, Greek mathematicians such as Eudoxus and Archimedes made more explicit, but informal, use of the concepts of limits and convergence when they used the method of exhaustion to compute the area and volume of regions and solids.[5] The explicit use of infinitesimals appears in Archimedes' The Method of Mechanical Theorems, a work rediscovered in the 20th century.[6] In Asia, the Chinese mathematician Liu Hui used the method of exhaustion in the 3rd century AD to find the area of a circle.[7] Zu Chongzhi established a method that would later be called Cavalieri's principle to find the volume of a sphere in the 5th century.[8]  The Indian mathematician Bhāskara II gave examples of the derivative and used what is now known as Rolle's theorem in the 12th century.[9]In the 14th century, Madhava of Sangamagrama developed infinite series expansions, like the power series and the Taylor series, of functions such as sine, cosine, tangent and arctangent.[10] Alongside his development of the Taylor series of the trigonometric functions, he also estimated the magnitude of the error terms created by truncating these series and gave a rational approximation of an infinite series.  His followers at the Kerala School of Astronomy and Mathematics further expanded his works, up to the 16th century.The modern foundations of mathematical analysis were established in 17th century Europe.[3] Descartes and Fermat independently developed analytic geometry, and a few decades later Newton and Leibniz independently developed infinitesimal calculus, which grew, with the stimulus of applied work that continued through the 18th century, into analysis topics such as the calculus of variations, ordinary and partial differential equations, Fourier analysis, and generating functions. During this period, calculus techniques were applied to approximate discrete problems by continuous ones.In the 18th century, Euler introduced the notion of mathematical function.[11] Real analysis began to emerge as an independent subject when Bernard Bolzano introduced the modern definition of continuity in 1816,[12] but Bolzano's work did not become widely known until the 1870s. In 1821, Cauchy began to put calculus on a firm logical foundation by rejecting the principle of the generality of algebra widely used in earlier work, particularly by Euler.  Instead, Cauchy formulated calculus in terms of geometric ideas and infinitesimals.  Thus, his definition of continuity required an infinitesimal change in x to correspond to an infinitesimal change in y.  He also introduced the concept of the Cauchy sequence, and started the formal theory of complex analysis. Poisson, Liouville, Fourier and others studied partial differential equations and harmonic analysis.  The contributions of these mathematicians and others, such as Weierstrass, developed the (ε, δ)-definition of limit approach, thus founding the modern field of mathematical analysis.In the middle of the 19th century Riemann introduced his theory of integration. The last third of the century saw the arithmetization of analysis by Weierstrass, who thought that geometric reasoning was inherently misleading, and introduced the "epsilon-delta" definition of limit.Then, mathematicians started worrying that they were assuming the existence of a continuum of real numbers without proof. Dedekind then constructed the real numbers by Dedekind cuts, in which irrational numbers are formally defined, which serve to fill the "gaps" between rational numbers, thereby creating a complete set: the continuum of real numbers, which had already been developed by Simon Stevin in terms of decimal expansions. Around that time, the attempts to refine the theorems of Riemann integration led to the study of the "size" of the set of discontinuities of real functions.Also, "monsters" (nowhere continuous functions, continuous but nowhere differentiable functions, space-filling curves) began to be investigated. In this context, Jordan developed his theory of measure, Cantor developed what is now called naive set theory, and Baire proved the Baire category theorem. In the early 20th century, calculus was formalized using an axiomatic set theory. Lebesgue solved the problem of measure, and Hilbert introduced Hilbert spaces to solve integral equations. The idea of normed vector space was in the air, and in the 1920s Banach created functional analysis.In mathematics, a metric space is a set where a notion of distance (called a metric) between elements of the set is defined.Much of analysis happens in some metric space; the most commonly used are the real line, the complex plane, Euclidean space, other vector spaces, and the integers. Examples of analysis without a metric include measure theory (which describes size rather than distance) and functional analysis (which studies topological vector spaces that need not have any sense of distance).A sequence is an ordered list. Like a set, it contains members (also called elements, or terms). Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Most precisely, a sequence can be defined as a function whose domain is a countable totally ordered set, such as the natural numbers.One of the most important properties of a sequence is convergence. Informally, a sequence converges if it has a limit. Continuing informally, a (singly-infinite) sequence has a limit if it approaches some point x, called the limit, as n becomes very large. That is, for an abstract sequence (an) (with n running from 1 to infinity understood) the distance between an and x approaches 0 as n → ∞, denotedReal analysis (traditionally, the theory of functions of a real variable) is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable.[13][14] In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers.[15] It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, mechanical engineering, electrical engineering, and particularly, quantum field theory.Complex analysis is particularly concerned with the analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear operators acting upon these spaces and respecting these structures in a suitable sense.[16][17] The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces.  This point of view turned out to be particularly useful for the study of differential and integral equations.A differential equation is a mathematical equation for an unknown function of one or several variables that relates the values of the function itself and its derivatives of various orders.[18][19][20] Differential equations play a prominent role in engineering, physics, economics, biology, and other disciplines.Differential equations arise in many areas of science and technology, specifically whenever a deterministic relation involving some continuously varying quantities (modeled by functions) and their rates of change in space or time (expressed as derivatives) is known or postulated. This is illustrated in classical mechanics, where the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow one (given the position, velocity, acceleration and various forces acting on the body) to express these variables dynamically as a differential equation for the unknown position of the body as a function of time. In some cases, this differential equation (called an equation of motion) may be solved explicitly.Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).[22]Modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.Techniques from analysis are also found in other areas such as:The vast majority of classical mechanics, relativity, and quantum mechanics is based on applied analysis, and differential equations in particular. Examples of important differential equations include Newton's second law, the Schrödinger equation, and the Einstein field equations.Functional analysis is also a major factor in quantum mechanics.When processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate individual components of a compound waveform, concentrating them for easier detection or removal.  A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.[23]Techniques from analysis are used in many areas of mathematics, including:
Material point method
The material point method (MPM) is a numerical technique used to simulate the behavior of solids, liquids, gases, and any other continuum material. Especially, it is a robust spatial discretization method for simulating multi-phase (solid-fluid-gas) interactions. In the MPM, a continuum body is described by a number of small Lagrangian elements referred to as 'material points'. These material points are surrounded by a background mesh/grid that is used only to calculate gradient terms such as the deformation gradient. Unlike other mesh-based methods like the finite element method, finite volume method or finite difference method, the MPM is not a mesh based method and is instead categorized as a meshless/meshfree or continuum-based particle method, examples of which are smoothed particle hydrodynamics and peridynamics. Despite the presence of a background mesh, the MPM does not encounter the drawbacks of mesh-based methods (high deformation tangling, advection errors etc.) which makes it a promising and powerful tool in computational mechanics.The MPM was originally proposed, as an extension of a similar method known as FLIP (a further extension of a method called PIC) to computational solid dynamics, in the early 1990 by Professors Deborah L. Sulsky, Zhen Chen and Howard L. Schreyer at University of New Mexico. After this initial development, the MPM has been further developed both in the national labs as well as the University of New Mexico, Oregon State University, University of Utah and more across the US and the world. Recently the number of institutions researching the MPM has been growing with added popularity and awareness coming from various sources such as the MPM's use in the Disney film Frozen.An MPM simulation consists of the following stages:(Prior to the time integration phase)(During the time integration phase - explicit formulation)2. Material point quantities are extrapolated to grid nodes.3. Equations of motion are solved on the grid.5.Resetting of grid.The PIC was originally conceived to solve problems in fluid dynamics, and developed by Harlow at Los Alamos National Laboratory in 1957.[1] One of the first PIC codes was the Fluid-Implicit Particle (FLIP) program, which was created by Brackbill in 1986[2] and has been constantly in development ever since. Until the 1990s, the PIC method was used principally in fluid dynamics.Motivated by the need for better simulating penetration problems in solid dynamics, Sulsky, Chen and Schreyer started in 1993 to reformulate the PIC and develop the MPM, with funding from Sandia National Laboratories.[3] The original MPM was then further extended by Bardenhagen et al.. to include frictional contact,[4] which enabled the simulation of granular flow,[5] and by Nairn to include explicit cracks[6] and crack propagation (known as CRAMP).Recently, an MPM implementation based on a micro-polar Cosserat continuum [7] has been used to simulate high-shear granular flow, such as silo discharge. MPM's uses were further extended into Geotechnical engineering with the recent development of a quasi-static, implicit MPM solver which provides numerically stable analyses of large-deformation problems in Soil mechanics.[8]Annual workshops on the use of MPM are held at various locations in the United States. The Fifth MPM Workshop was held at Oregon State University, in Corvallis, OR, on April 2 and 3, 2009.The uses of the PIC or MPM method can be divided into two broad categories: firstly, there are many applications involving fluid dynamics, plasma physics, magnetohydrodynamics, and multiphase applications. The second category of applications comprises problems in solid mechanics.The PIC method has been used to simulate a wide range of fluid-solid interactions, including sea ice dynamics,[9] penetration of biological soft tissues,[10] fragmentation of gas-filled canisters,[11] dispersion of atmospheric pollutants,[12] multiscale simulations coupling molecular dynamics with MPM,[13][14] and fluid-membrane interactions.[15] In addition, the PIC-based FLIP code has been applied in magnetohydrodynamics and plasma processing tools, and simulations in astrophysics and free-surface flow.[16]As a result of a joint effort between UCLA's mathematics department and Walt Disney Animation Studios, MPM was successfully used to simulate snow in the 2013 computer-animated film Frozen.[17][18][19]MPM has also been used extensively in solid mechanics, to simulate impact, penetration, collision and rebound, as well as crack propagation.[20][21] MPM has also become a widely used method within the field of soil mechanics: it has been used to simulate granular flow, silo discharge, pile driving, bucket filling, and material failure; and to model soil stress distribution, compaction, and hardening. It is now being used in wood mechanics problems such as simulations of transverse compression on the cellular level including cell wall contact [22] (this work received the George Marra Award for paper of the year from the Society of Wood Science and Technology [1])One subset of numerical methods are Meshfree methods, which are defined as methods for which "a predefined mesh is not necessary, at least in field variable interpolation". Ideally, a meshfree method does not make use of a mesh "throughout the process of solving the problem governed by partial differential equations, on a given arbitrary domain, subject to all kinds of boundary conditions," although existing methods are not ideal and fail in at least one of these respects. Meshless methods, which are also sometimes called particle methods, share a "common feature that the history of state variables is traced at points (particles) which are not connected with any element mesh, the distortion of which is a source of numerical difficulties." As can be seen by these varying interpretations, some scientists consider MPM to be a meshless method, while others do not. All agree, however, that MPM is a particle method.The Arbitrary Lagrangian Eulerian (ALE) methods form another subset of numerical methods which includes MPM. Purely Lagrangian methods employ a framework in which a space is discretised into initial subvolumes, whose flowpaths are then charted over time. Purely Eulerian methods, on the other hand, employ a framework in which the motion of material is described relative to a mesh that remains fixed in space throughout the calculation. As the name indicates, ALE methods combine Lagrangian and Eulerian frames of reference.PIC methods may be based on either the strong form collocation or a weak form discretisation of the underlying partial differential equation (PDE). Those based on the strong form are properly referred to as finite-volume PIC methods. Those based on the weak form discretisation of PDEs may be called either PIC or MPM.MPM solvers can model problems in one, two, or three spatial dimensions, and can also model axisymmetric problems. MPM can be implemented to solve either quasi-static or dynamic equations of motion, depending on the type of problem that is to be modeled.The time-integration used for MPM may be either explicit or implicit. The advantage to implicit integration is guaranteed stability, even for large timesteps. On the other hand, explicit integration runs much faster and is easier to implement.Unlike FEM, MPM does not require periodical remeshing steps and remapping of state variables, and is therefore better suited to the modeling of large material deformations. In MPM, particles and not the mesh points store all the information on the state of the calculation. Therefore, no numerical error results from the mesh returning to its original position after each calculation cycle, and no remeshing algorithm is required.The particle basis of MPM allows it to treat crack propagation and other discontinuities better than FEM, which is known to impose the mesh orientation on crack propagation in a material. Also, particle methods are better at handling history-dependent constitutive models.Because in MPM nodes remain fixed on a regular grid, the calculation of gradients is trivial.In simulations with two or more phases it is rather easy to detect contact between entities, as particles can interact via the grid with other particles in the same body, with other solid bodies, and with fluids.MPM is more expensive in terms of storage than other methods, as MPM makes use of mesh as well as particle data. MPM is more computationally expensive than FEM, as the grid must be reset at the end of each MPM calculation step and reinitialised at the beginning of the following step. Spurious oscillation may occur as particles cross the boundaries of the mesh in MPM, although this effect can be minimized by using generalized interpolation methods (GIMP). In MPM as in FEM, the size and orientation of the mesh can impact the results of a calculation: for example, in MPM, strain localisation is known to be particularly sensitive to mesh refinement.A commercial package based on a meshless method is MPMsim.
Lorentz transformation
In physics, the Lorentz transformations are a one-parameter family of linear transformations from a coordinate frame in space time to another frame that moves at a constant velocity, the parameter, within the former. The transformations are named after the Dutch physicist Hendrik Lorentz. The respective inverse transformation is then parametrized by the negative of this velocity.Frames of reference can be divided into two groups: inertial (relative motion with constant velocity) and non-inertial (accelerating, moving in curved paths, rotational motion with constant angular velocity, etc.). The term "Lorentz transformations" only refers to transformations between inertial frames, usually in the context of special relativity. In each reference frame, an observer can use a local coordinate system (most exclusively Cartesian coordinates in this context) to measure lengths, and a clock to measure time intervals. An observer is a real or imaginary entity that can take measurements, say humans, or any other living organism—or even robots and computers. An event is something that happens at a point in space at an instant of time, or more formally a point in spacetime. The transformations connect the space and time coordinates of an event as measured by an observer in each frame.[nb 1]They supersede the Galilean transformation of Newtonian physics, which assumes an absolute space and time (see Galilean relativity). The Galilean transformation is a good approximation only at relative speeds much smaller than the speed of light. Lorentz transformations have a number of unintuitive features that do not appear in Galilean transformations. For example, they reflect the fact that observers moving at different velocities may measure different distances, elapsed times, and even different orderings of events, but always such that the speed of light is the same in all inertial reference frames. The invariance of light speed is one of the postulates of special relativity.Historically, the transformations were the result of attempts by Lorentz and others to explain how the speed of light was observed to be independent of the reference frame, and to understand the symmetries of the laws of electromagnetism. The Lorentz transformation is in accordance with special relativity, but was derived before special relativity.The Lorentz transformation is a linear transformation. It may include a rotation of space; a rotation-free Lorentz transformation is called a Lorentz boost. In Minkowski space, the mathematical model of spacetime in special relativity, the Lorentz transformations preserve the spacetime interval between any two events. This property is the defining property of a Lorentz transformation. They describe only the transformations in which the spacetime event at the origin is left fixed. They can be considered as a hyperbolic rotation of Minkowski space. The more general set of transformations that also includes translations is known as the Poincaré group.Many physicists—including Woldemar Voigt, George FitzGerald, Joseph Larmor, and Hendrik Lorentz[3] himself—had been discussing the physics implied by these equations since 1887.[4] Early in 1889, Oliver Heaviside had shown from Maxwell's equations that the electric field surrounding a spherical distribution of charge should cease to have spherical symmetry once the charge is in motion relative to the aether. FitzGerald then conjectured that Heaviside’s distortion result might be applied to a theory of intermolecular forces. Some months later, FitzGerald published the conjecture that bodies in motion are being contracted, in order to explain the baffling outcome of the 1887 aether-wind experiment of Michelson and Morley. In 1892, Lorentz independently presented the same idea in a more detailed manner, which was subsequently called FitzGerald–Lorentz contraction hypothesis.[5] Their explanation was widely known before 1905.[6]Lorentz (1892–1904) and Larmor (1897–1900), who believed the luminiferous aether hypothesis, also looked for the transformation under which Maxwell's equations are invariant when transformed from the aether to a moving frame. They extended the FitzGerald–Lorentz contraction hypothesis and found out that the time coordinate has to be modified as well ("local time"). Henri Poincaré gave a physical interpretation to local time (to first order in v/c, the relative velocity of the two reference frames normalized to the speed of light) as the consequence of clock synchronization, under the assumption that the speed of light is constant in moving frames.[7] Larmor is credited to have been the first to understand the crucial time dilation property inherent in his equations.[8]In 1905, Poincaré was the first to recognize that the transformation has the properties of a mathematical group,and named it after Lorentz.[9]Later in the same year Albert Einstein published what is now called special relativity, by deriving the Lorentz transformation under the assumptions of the principle of relativity and the constancy of the speed of light in any inertial reference frame, and by abandoning the mechanistic aether as unnecessary.[10]An event is something that happens at a certain point in spacetime, or more generally, the point in spacetime itself. In any inertial frame an event is specified by a time coordinate ct and a set of Cartesian coordinates x, y, z to specify position in space in that frame. Subscripts label individual events.From Einstein's second postulate of relativity follows    (D1)in all inertial frames for events connected by light signals. The quantity on the left is called the spacetime interval between events a1 = (t1, x1, y1, z1) and a2 = (t2, x2, y2, z2). The interval between any two events, not necessarily separated by light signals, is in fact invariant, i.e., independent of the state of relative motion of observers in different inertial frames, as is shown using homogeneity and isotropy of space. The transformation sought after thus must possess the property that    (D2)where (ct, x, y, z) are the spacetime coordinates used to define events in one frame, and (ct′, x′, y′, z′) are the coordinates in another frame. First one observes that (D2) is satisfied if an arbitrary 4-tuple b of numbers are added to events a1 and a2. Such transformations are called spacetime translations and are not dealt with further here. Then one observes that a linear solution preserving the origin of the simpler problem    (D3)solves the general problem too. (A solution satisfying the left formula automatically satisfies the right formula, see polarization identity.) Finding the solution to the simpler problem is just a matter of look-up in the theory of classical groups that preserve bilinear forms of various signature.[nb 2] Equation (D3) can be written more compactly as    (D4)where (·, ·) refers to the bilinear form of signature (1, 3) on ℝ4 exposed by the right hand side formula in (D3). The alternative notation defined on the right is referred to as the relativistic dot product. Spacetime mathematically viewed as ℝ4 endowed with this bilinear form is known as Minkowski space M. The Lorentz transformation is thus an element of the group Lorentz group O(1, 3), the Lorentz group or, for those that prefer the other metric signature, O(3, 1) (also called the Lorentz group).[nb 3] One has    (D5)which is precisely preservation of the bilinear form (D3) which implies (by linearity of Λ and bilinearity of the form) that (D2) is satisfied. The elements of the Lorentz group are rotations and boosts and mixes thereof. If the spacetime translations are included, then one obtains the inhomogeneous Lorentz group or the Poincaré group.The relations between the primed and unprimed spacetime coordinates are the Lorentz transformations, each coordinate in one frame is a linear function of all the coordinates in the other frame, and the inverse functions are the inverse transformation. Depending on how the frames move relative to each other, and how they are oriented in space relative to each other, other parameters that describe direction, speed, and orientation enter the transformation equations.Transformations describing relative motion with constant (uniform) velocity and without rotation of the space coordinate axes are called boosts, and the relative velocity between the frames is the parameter of the transformation. The other basic type of Lorentz transformations is rotations in the spatial coordinates only, these are also inertial frames since there is no relative motion, the frames are simply tilted (and not continuously rotating), and in this case quantities defining the rotation are the parameters of the transformation (e.g., axis–angle representation, or Euler angles, etc.). A combination of a rotation and boost is a homogeneous transformation, which transforms the origin back to the origin.The full Lorentz group O(3, 1) also contains special transformations that are neither rotations nor boosts, but rather reflections in a plane through the origin. Two of these can be singled out; spatial inversion in which the spatial coordinates of all events are reversed in sign and temporal inversion in which the time coordinate for each event gets its sign reversed.Boosts should not be conflated with mere displacements in spacetime; in this case, the coordinate systems are simply shifted and there is no relative motion. However, these also count as symmetries forced by special relativity since they leave the spacetime interval invariant. A combination of a rotation with a boost, followed by a shift in spacetime, is an inhomogeneous Lorentz transformation, an element of the Poincaré group, which is also called the inhomogeneous Lorentz group. A "stationary" observer in frame F defines events with coordinates t, x, y, z. Another frame F′ moves with velocity v relative to F, and an observer in this "moving" frame F′ defines events using the coordinates t′, x′, y′, z′.The coordinate axes in each frame are parallel (the x and x′ axes are parallel, the y and y′ axes are parallel, and the z and z′ axes are parallel), remain mutually perpendicular, and relative motion is along the coincident xx′ axes. At t = t′ = 0, the origins of both coordinate systems are the same, (x, y, z) = (x′, y′, z′) = (0, 0, 0). In other words, the times and positions are coincident at this event. If all these hold, then the coordinate systems are said to be in standard configuration, or synchronized.If an observer in F records an event t, x, y, z, then an observer in F′ records the same event with coordinates[12]where v is the relative velocity between frames in the x-direction, c is the speed of light, and(lowercase gamma) is the Lorentz factor.Here, v is the parameter of the transformation, for a given boost it is a constant number, but can take a continuous range of values. In the setup used here, positive relative velocity v > 0 is motion along the positive directions of the xx′ axes, zero relative velocity v = 0 is no relative motion, while negative relative velocity v < 0 is relative motion along the negative directions of the xx′ axes. The magnitude of relative velocity v cannot equal or exceed c, so only subluminal speeds −c < v < c are allowed. The corresponding range of γ is 1 ≤ γ < ∞.The transformations are not defined if v is outside these limits. At the speed of light (v = c) γ is infinite, and faster than light (v > c) γ is a complex number, each of which make the transformations unphysical. The space and time coordinates are measurable quantities and numerically must be real numbers.As an active transformation, an observer in F′ notices the coordinates of the event to be "boosted" in the negative directions of the xx′ axes, because of the −v in the transformations. This has the equivalent effect of the coordinate system F′ boosted in the positive directions of the xx′ axes, while the event does not change and is simply represented in another coordinate system, a passive transformation.The inverse relations (t, x, y, z in terms of t′, x′, y′, z′) can be found by algebraically solving the original set of equations. A more efficient way is to use physical principles. Here F′ is the "stationary" frame while F is the "moving" frame. According to the principle of relativity, there is no privileged frame of reference, so the transformations from F′ to F must take exactly the same form as the transformations from F to F′. The only difference is F moves with velocity −v relative to F′ (i.e., the relative velocity has the same magnitude but is oppositely directed). Thus if an observer in F′ notes an event t′, x′, y′, z′, then an observer in F notes the same event with coordinatesand the value of γ remains unchanged. This "trick" of simply reversing the direction of relative velocity while preserving its magnitude, and exchanging primed and unprimed variables, always applies to finding the inverse transformation of every boost in any direction.Sometimes it is more convenient to use β = v/c (lowercase beta) instead of v, so thatwhich shows much more clearly the symmetry in the transformation. From the allowed ranges of v and the definition of β, it follows −1 < β < 1. The use of β and γ is standard throughout the literature.The Lorentz transformations can also be derived in a way that resembles circular rotations in 3d space using the hyperbolic functions. For the boost in the x direction, the results arewhere ζ (lowercase zeta) is a parameter called rapidity (many other symbols are used, including θ, ϕ, φ, η, ψ, ξ). Given the strong resemblance to rotations of spatial coordinates in 3d space in the Cartesian xy, yz, and zx planes, a Lorentz boost can be thought of as a hyperbolic rotation of spacetime coordinates in the xt, yt, and zt Cartesian-time planes of 4d Minkowski space. The parameter ζ is the hyperbolic angle of rotation, analogous to the ordinary angle for circular rotations. This transformation can be illustrated with a Minkowski diagram.The hyperbolic functions arise from the difference between the squares of the time and spatial coordinates in the spacetime interval, rather than a sum. The geometric significance of the hyperbolic functions can be visualized by taking x = 0 or ct = 0 in the transformations. Squaring and subtracting the results, one can derive hyperbolic curves of constant coordinate values but varying ζ, which parametrizes the curves according to the identityConversely the ct and x axes can be constructed for varying coordinates but constant ζ. The definitionprovides the link between a constant value of rapidity, and the slope of the ct axis in spacetime. A consequence these two hyperbolic formulae is an identity that matches the Lorentz factorComparing the Lorentz transformations in terms of the relative velocity and rapidity, or using the above formulae, the connections between β, γ, and ζ areTaking the inverse hyperbolic tangent gives the rapiditySince −1 < β < 1, it follows −∞ < ζ < ∞. From the relation between ζ and β, positive rapidity ζ > 0 is motion along the positive directions of the xx′ axes, zero rapidity ζ = 0 is no relative motion, while negative rapidity ζ < 0 is relative motion along the negative directions of the xx′ axes.The inverse transformations are obtained by exchanging primed and unprimed quantities to switch the coordinate frames, and negating rapidity ζ → −ζ since this is equivalent to negating the relative velocity. Therefore,The inverse transformations can be similarly visualized by considering the cases when x′ = 0 and ct′ = 0.So far the Lorentz transformations have been applied to one event. If there are two events, there is a spatial separation and time interval between them. It follows from the linearity of the Lorentz transformations that two values of space and time coordinates can be chosen, the Lorentz transformations can be applied to each, then subtracted to get the Lorentz transformations of the differences;with inverse relationswhere Δ (uppercase delta) indicates a difference of quantities; e.g., Δx = x2 − x1 for two values of x coordinates, and so on.These transformations on differences rather than spatial points or instants of time are useful for a number of reasons:A critical requirement of the Lorentz transformations is the invariance of the speed of light, a fact used in their derivation, and contained in the transformations themselves. If in F the equation for a pulse of light along the x direction is x = ct, then in F′ the Lorentz transformations give x′ = ct′, and vice versa, for any −c < v < c.For relative speeds much less than the speed of light, the Lorentz transformations reduce to the Galilean transformationin accordance with the correspondence principle. It is sometimes said that nonrelativistic physics is a physics of "instantaneous action at a distance".[13]Three unintuitive, but correct, predictions of the transformations are:The use of vectors allows positions and velocities to be expressed in arbitrary directions compactly. A single boost in any direction depends on the full relative velocity vector v with a magnitude |v| = v that cannot equal or exceed c, so that 0 ≤ v < c.Only time and the coordinates parallel to the direction of relative motion change, while those coordinates perpendicular do not. With this in mind, split the spatial position vector r as measured in F, and r′ as measured in F′, each into components perpendicular (⊥) and parallel ( ‖ ) to v,then the transformations arewhere · is the dot product. The Lorentz factor γ retains its definition for a boost in any direction, since it depends only on the magnitude of the relative velocity. The definition β = v/c with magnitude 0 ≤ β < 1 is also used by some authors.Introducing a unit vector n = v/v = β/β in the direction of relative motion, the relative velocity is v = vn with magnitude v and direction n, and vector projection and rejection give respectivelyAccumulating the results gives the full transformations,The projection and rejection also applies to r′. For the inverse transformations, exchange r and r′ to switch observed coordinates, and negate the relative velocity v → −v (or simply the unit vector n → −n since the magnitude v is always positive) to obtainThe unit vector has the advantage of simplifying equations for a single boost, allows either v or β to be reinstated when convenient, and the rapidity parametrization is immediately obtained by replacing β and βγ. It is not convenient for multiple boosts.The vectorial relation between relative velocity and rapidity is[14]and the "rapidity vector" can be defined aseach of which serves as a useful abbreviation in some contexts. The magnitude of ζ is the absolute value of the rapidity scalar confined to 0 ≤ ζ < ∞, which agrees with the range 0 ≤ β < 1.Defining the coordinate velocities and Lorentz factor bytaking the differentials in the coordinates and time of the vector transformations, then dividing equations, leads toThe velocities u and u′ are the velocity of some massive object. They can also be for a third inertial frame (say F′′), in which case they must be constant. Denote either entity by X. Then X moves with velocity u relative to F, or equivalently with velocity u′ relative to F′, in turn F′ moves with velocity v relative to F. The inverse transformations can be obtained in a similar way, or as with position coordinates exchange u and u′, and change v to −v.The transformation of velocity is useful in stellar aberration, the Fizeau experiment, and the relativistic Doppler effect.The Lorentz transformations of acceleration can be similarly obtained by taking differentials in the velocity vectors, and dividing these by the time differential.In general, given four quantities A and Z = (Zx, Zy, Zz) and their Lorentz-boosted counterparts A′ and Z′ = (Z′x, Z′y, Z′z), a relation of the formimplies the quantities transform under Lorentz transformations similar to the transformation of spacetime coordinates;The decomposition of Z (and Z′) into components perpendicular and parallel to v is exactly the same as for the position vector, as is the process of obtaining the inverse transformations (exchange (A, Z) and (A′, Z′) to switch observed quantities, and reverse the direction of relative motion by the substitution n ↦ −n).The quantities (A, Z) collectively make up a four vector, where A is the "timelike component", and Z the "spacelike component". Examples of A and Z are the following:For a given object (e.g., particle, fluid, field, material), if A or Z correspond to properties specific to the object like its charge density, mass density, spin, etc., its properties can be fixed in the rest frame of that object. Then the Lorentz transformations give the corresponding properties in a frame moving relative to the object with constant velocity. This breaks some notions taken for granted in non-relativistic physics. For example, the energy E of an object is a scalar in non-relativistic mechanics, but not in relativistic mechanics because energy changes under Lorentz transformations; its value is different for various inertial frames. In the rest frame of an object, it has a rest energy and zero momentum. In a boosted frame its energy is different and it appears to have a momentum. Similarly, in non-relativistic quantum mechanics the spin of a particle is a constant vector, but in relativistic quantum mechanics spin s depends on relative motion. In the rest frame of the particle, the spin pseudovector can be fixed to be its ordinary non-relativistic spin with a zero timelike quantity st, however a boosted observer will perceive a nonzero timelike component and an altered spin.[15]Not all quantities are invariant in the form as shown above, for example orbital angular momentum L does not have a timelike quantity, and neither does the electric field E nor the magnetic field B. The definition of angular momentum is L = r × p, and in a boosted frame the altered angular momentum is L′ = r′ × p′. Applying this definition using the transformations of coordinates and momentum leads to the transformation of angular momentum. It turns out L transforms with another vector quantity N = (E/c2)r − tp related to boosts, see relativistic angular momentum for details. For the case of the E and B fields, the transformations cannot be obtained as directly using vector algebra. The Lorentz force is the definition of these fields, and in F it is F = q(E + v × B) while in F′ it is F′ = q(E′ + v′ × B′). A method of deriving the EM field transformations in an efficient way which also illustrates the unit of the electromagnetic field uses tensor algebra, given below.Throughout, italic non-bold capital letters are 4×4 matrices, while non-italic bold letters are 3×3 matrices.Writing the coordinates in column vectors and the Minkowski metric η as a square matrixthe spacetime interval takes the form (T denotes transpose)and is invariant under a Lorentz transformationwhere Λ is a square matrix which can depend on parameters.From the invariance of the spacetime interval it followsand this matrix equation contains the general conditions on the Lorentz transformation to ensure invariance of the spacetime interval. Taking the determinant of the equation using the product rule[nb 4] gives immediatelyWriting the Minkowski metric as a block matrix, and the Lorentz transformation in the most general form,carrying out the block matrix multiplications obtains general conditions on Γ, a, b, M to ensure relativistic invariance. Not much information can be directly extracted from all the conditions, however one of the resultsis useful; bTb ≥ 0 always so it follows thatThe negative inequality may be unexpected, because Γ multiplies the time coordinate and this has an effect on time symmetry. If the positive equality holds, then Γ is the Lorentz factor.The determinant and inequality provide four ways to classify Lorentz transformations (herein LTs for brevity). Any particular LT has only one determinant sign and only one inequality. There are four sets which include every possible pair given by the intersections ("n"-shaped symbol meaning "and") of these classifying sets.where "+" and "−" indicate the determinant sign, while "↑" for ≥ and "↓" for ≤ denote the inequalities.The full Lorentz group splits into the union ("u"-shaped symbol meaning "or") of four disjoint setsThe Lorentz boost iswhere the boost matrix isThe boosts along the Cartesian directions can be readily obtained, for example the unit vector in the x direction has components nx = 1 and ny = nz = 0.The matrices make one or more successive transformations easier to handle, rather than rotely iterating the transformations to obtain the result of more than one transformation. If a frame F′ is boosted with velocity u relative to frame F, and another frame F′′ is boosted with velocity v relative to F′, the separate boosts areand the composition of the two boosts connects the coordinates in F′′ and F,Successive transformations act on the left. If u and v are collinear (parallel or antiparallel along the same line of relative motion), the boost matrices commute: B(v)B(u) = B(u)B(v) and this composite transformation happens to be another boost.If u and v are not collinear but in different directions, the situation is considerably more complicated. Lorentz boosts along different directions do not commute: B(v)B(u) and B(u)B(v) are not equal. Also, each of these compositions is not a single boost, but still a Lorentz transformation as each boost still preserves invariance of the spacetime interval. It turns out the composition of any two Lorentz boosts is equivalent to a boost followed or preceded by a rotation on the spatial coordinates, in the form of R(ρ)B(w) or B(w)R(ρ). The w and w are composite velocities, while ρ and ρ are rotation parameters (e.g. axis-angle variables, Euler angles, etc.). The rotation in block matrix form is simplywhere R(ρ) is a 3d rotation matrix, which rotates any 3d vector in one sense (active transformation), or equivalently the coordinate frame in the opposite sense (passive transformation). It is not simple to connect w and ρ (or w and ρ) to the original boost parameters u and v. In a composition of boosts, the R matrix is named the Wigner rotation, and gives rise to the Thomas precession. These articles give the explicit formulae for the composite transformation matrices, including expressions for w, ρ, w, ρ.In this article the axis-angle representation is used for ρ. The rotation is about an axis in the direction of a unit vector e, through angle θ (positive anticlockwise, negative clockwise, according to the right-hand rule). The "axis-angle vector"will serve as a useful abbreviation.Spatial rotations alone are also Lorentz transformations they leave the spacetime interval invariant. Like boosts, successive rotations about different axes do not commute. Unlike boosts, the composition of any two rotations is equivalent to a single rotation. Some other similarities and differences between the boost and rotation matrices include:The most general proper Lorentz transformation Λ(v, θ) includes a boost and rotation together, and is a nonsymmetric matrix. As special cases, Λ(0, θ) = R(θ) and Λ(v, 0) = B(v). An explicit form of the general Lorentz transformation is cumbersome to write down and will not be given here. Nevertheless, closed form expressions for the transformation matrices will be given below using group theoretical arguments. It will be easier to use the rapidity parametrization for boosts, in which case one writes Λ(ζ, θ) and B(ζ).The set of transformationswith matrix multiplication as the operation of composition forms a group, called the "restricted Lorentz group", and is the special indefinite orthogonal group SO+(3,1). (The plus sign indicates that it preserves the orientation of the temporal dimension).For simplicity, look at the infinitesimal Lorentz boost in the x direction (examining a boost in any other direction, or rotation about any axis, follows an identical procedure). The infinitesimal boost is a small boost away from the identity, obtained by the Taylor expansion of the boost matrix to first order about ζ = 0,where the higher order terms not shown are negligible because ζ is small, and Bx is simply the boost matrix in the x direction. The derivative of the matrix is the matrix of derivatives (of the entries, with respect to the same variable), and it is understood the derivatives are found first then evaluated at ζ = 0,For now, Kx is defined by this result (its significance will be explained shortly). In the limit of an infinite number of infinitely small steps, the finite boost transformation in the form of a matrix exponential is obtainedwhere the limit definition of the exponential has been used (see also characterizations of the exponential function). More generally[nb 5]The axis-angle vector θ and rapidity vector ζ are altogether six continuous variables which make up the group parameters (in this particular representation), and the generators of the group are K = (Kx, Ky, Kz) and J = (Jx, Jy, Jz), each vectors of matrices with the explicit forms[nb 6]These are all defined in an analogous way to Kx above, although the minus signs in the boost generators are conventional. Physically, the generators of the Lorentz group correspond to important symmetries in spacetime: J are the rotation generators which correspond to angular momentum, and K are the boost generators which correspond to the motion of the system in spacetime. The derivative of any smooth curve C(t) with C(0) = I in the group depending on some group parameter t with respect to that group parameter, evaluated at t = 0, serves as a definition of a corresponding group generator G, and this reflects an infinitesimal transformation away from the identity. The smooth curve can always be taken as an exponential as the exponential will always map G smoothly back into the group via t → exp(tG) for all t; this curve will yield G again when differentiated at t = 0.Expanding the exponentials in their Taylor series obtainswhich compactly reproduce the boost and rotation matrices as given in the previous section.It has been stated that the general proper Lorentz transformation is a product of a boost and rotation. At the infinitesimal level the productis commutative because only linear terms are required (products like (θ·J)(ζ·K) and (ζ·K)(θ·J) count as higher order terms and are negligible). Taking the limit as before leads to the finite transformation in the form of an exponentialThe converse is also true, but the decomposition of a finite general Lorentz transformation into such factors is nontrivial. In particular,because the generators do not commute. For a description of how to find the factors of a general Lorentz transformation in terms of a boost and a rotation in principle (this usually does not yield an intelligible expression in terms of generators J and K), see Wigner rotation. If, on the other hand, the decomposition is given in terms of the generators, and one wants to find the product in terms of the generators, then the Baker–Campbell–Hausdorff formula applies.Lorentz generators can be added together, or multiplied by real numbers, to obtain more Lorentz generators. In other words, the set of all Lorentz generatorstogether with the operations of ordinary matrix addition and multiplication of a matrix by a number, forms a vector space over the real numbers.[nb 7] The generators Jx, Jy, Jz, Kx, Ky, Kz form a basis set of V, and the components of the axis-angle and rapidity vectors, θx, θy, θz, ζx, ζy, ζz, are the coordinates of a Lorentz generator with respect to this basis.[nb 8]Three of the commutation relations of the Lorentz generators arewhere the bracket [A, B] = AB − BA is known as the commutator, and the other relations can be found by taking cyclic permutations of x, y, z components (i.e. change x to y, y to z, and z to x, repeat).Linking terminology used in mathematics and physics: A group generator is any element of the Lie algebra. A group parameter is a component of a coordinate vector representing an arbitrary element of the Lie algebra with respect to some basis. A basis, then, is a set of generators being a basis of the Lie algebra in the usual vector space sense.The exponential map from the Lie algebra to the Lie group,provides a one-to-one correspondence between small enough neighborhoods of the origin of the Lie algebra and neighborhoods of the identity element of the Lie group. It the case of the Lorentz group, the exponential map is just the matrix exponential. Globally, the exponential map is not one-to-one, but in the case of the Lorentz group, it is surjective (onto). Hence any group element can be expressed as an exponential of an element of the Lie algebra.Lorentz transformations also include parity inversionwhich negates all the spatial coordinates only, and time reversalwhich negates the time coordinate only, because these transformations leave the spacetime interval invariant. Here I is the 3d identity matrix. These are both symmetric, they are their own inverses (see involution (mathematics)), and each have determinant −1. This latter property makes them improper transformations.If Λ is a proper orthochronous Lorentz transformation, then TΛ is improper antichronous, PΛ is improper orthochronous, and TPΛ = PTΛ is proper antichronous.Two other spacetime symmetries have not been accounted for. For the spacetime interval to be invariant, it can be shown[16] that it is necessary and sufficient for the coordinate transformation to be of the formwhere C is a constant column containing translations in time and space. If C ≠ 0, this is an inhomogeneous Lorentz transformation or Poincaré transformation.[17][18] If C = 0, this is a homogeneous Lorentz transformation. Poincaré transformations are not dealt further in this article.Writing the general matrix transformation of coordinates as the matrix equationallows the transformation of other physical quantities that cannot be expressed as four-vectors; e.g., tensors or spinors of any order in 4d spacetime, to be defined. In the corresponding tensor index notation, the above matrix expression iswhere lower and upper indices label covariant and contravariant components respectively,[19] and the summation convention is applied. It is a standard convention to use Greek indices that take the value 0 for time components, and 1, 2, 3 for space components, while Latin indices simply take the values 1, 2, 3, for spatial components. Note that the first index (reading left to right) corresponds in the matrix notation to a row index. The second index corresponds to the column index.The transformation matrix is universal for all four-vectors, not just 4-dimensional spacetime coordinates. If A is any four-vector, then in tensor index notationAlternatively, one writesin which the primed indices denote the indices of A in the primed frame. This notation cuts risk of exhausting the Greek alphabet roughly in half.For a general n-component object one may writewhere Π is the appropriate representation of the Lorentz group, an n×n matrix for every Λ. In this case, the indices should not be thought of as spacetime indices (sometimes called Lorentz indices), and they run from 1 to n. E.g., if X is a bispinor, then the indices are called Dirac indices.There are also vector quantities with covariant indices. They are generally obtained from their corresponding objects with contravariant indices by the operation of lowering an index; e.g.,where η is the metric tensor. (The linked article also provides more information about what the operation of raising and lowering indices really is mathematically.) The inverse of this transformation is given bywhere, when viewed as matrices, ημν is the inverse of ημν. As it happens, ημν = ημν. This is referred to as raising an index. To transform a covariant vector Aμ, first raise its index, then transform it according to the same rule as for contravariant 4-vectors, then finally lower the index;ButI. e., it is the (μ, ν)-component of the inverse Lorentz transformation. One defines (as a matter of notation),and may in this notation writeNow for a subtlety. The implied summation on the right hand side ofis running over a row index of the matrix representing Λ−1. Thus, in terms of matrices, this transformation should be thought of as the inverse transpose of Λ acting on the column vector Aμ. That is, in pure matrix notation,This means exactly that covariant vectors (thought of as column matrices) transform according to the dual representation of the standard representation of the Lorentz group. This notion generalizes to general representations, simply replace Λ with Π(Λ).If A and B are linear operators on vector spaces U and V, then a linear operator A ⊗ B may be defined on the tensor product of U and V, denoted U ⊗ V according to[20]From this it is immediately clear that if u and v are a four-vectors in V, then u ⊗ v ∈ T2V ≡ V ⊗ V transforms asThe second step uses the bilinearity of the tensor product and the last step defines a 2-tensor on component form, or rather, it just renames the tensor u ⊗ v.These observations generalize in an obvious way to more factors, and using the fact that a general tensor on a vector space V can be written as a sum of a coefficient (component!) times tensor products of basis vectors and basis covectors, one arrives at the transformation law for any tensor quantity T. It is given by[21]where Λχ′ψ is defined above. This form can generally be reduced to the form for general n-component objects given above with a single matrix (Π(Λ)) operating on column vectors. This latter form is sometimes preferred; e.g., for the electromagnetic field tensor.Lorentz transformations can also be used to illustrate that the magnetic field B and electric field E are simply different aspects of the same force — the electromagnetic force, as a consequence of relative motion between electric charges and observers.[22] The fact that the electromagnetic field shows relativistic effects becomes clear by carrying out a simple thought experiment.[23]The electric and magnetic fields transform differently from space and time, but exactly the same way as relativistic angular momentum and the boost vector.The electromagnetic field strength tensor is given byin SI units. In relativity, the Gaussian system of units is often preferred over SI units, even in texts whose main choice of units is SI units, because in it the electric field E and the magnetic induction B have the same units making the appearance of the electromagnetic field tensor more natural.[24] Consider a Lorentz boost in the x-direction. It is given by[25]where the field tensor is displayed side by side for easiest possible reference in the manipulations below.The general transformation law (T3) becomesFor the magnetic field one obtainsFor the electric field resultsHere, β = (β, 0, 0) is used. These results can be summarized byand are independent of the metric signature. For SI units, substitute E → ​E⁄c. Misner, Thorne & Wheeler (1973) refer to this last form as the 3 + 1 view as opposed to the geometric view represented by the tensor expressionand make a strong point of the ease with which results that are difficult to achieve using the 3 + 1 view can be obtained and understood. Only objects that have well defined Lorentz transformation properties (in fact under any smooth coordinate transformation) are geometric objects. In the geometric view, the electromagnetic field is a six-dimensional geometric object in spacetime as opposed to two interdependent, but separate, 3-vector fields in space and time. The fields E (alone) and B (alone) do not have well defined Lorentz transformation properties. The mathematical underpinnings are equations (T1) and (T2) that immediately yield (T3). One should note that the primed and unprimed tensors refer to the same event in spacetime. Thus the complete equation with spacetime dependence isLength contraction has an effect on charge density ρ and current density J, and time dilation has an effect on the rate of flow of charge (current), so charge and current distributions must transform in a related way under a boost. It turns out they transform exactly like the space-time and energy-momentum four-vectors,or, in the simpler geometric view,One says that charge density transforms as the time component of a four-vector. It is a rotational scalar. The current density is a 3-vector.The Maxwell equations are invariant under Lorentz transformations.Equation (T1) hold unmodified for any representation of the Lorentz group, including the bispinor representation. In (T2) one simply replaces all occurrences of Λ by the bispinor representation Π(Λ),The above equation could, for instance, be the transformation of a state in Fock space describing two free electrons.A general noninteracting multi-particle state (Fock space state) in quantum field theory transforms according to the rule[26]    ( 1)where W(Λ, p) is the Wigner rotation and D(j) is the (2j + 1)-dimensional representation of SO(3).
The Nine Chapters on the Mathematical Art
The Nine Chapters on the Mathematical Art (simplified Chinese: 九章算术; traditional Chinese: 九章算術; pinyin: Jiǔzhāng Suànshù; Wade–Giles: chiu3 chang1 suan4 shu1) is a Chinese mathematics book, composed by several generations of scholars from the 10th–2nd century BCE, its latest stage being from the 2nd century CE. This book is one of the earliest surviving mathematical texts from China, the first being Suan shu shu (202 BCE – 186 BCE) and Zhoubi Suanjing (compiled throughout the Han until the late 2nd century CE). It lays out an approach to mathematics that centres on finding the most general methods of solving problems, which may be contrasted with the approach common to ancient Greek mathematicians, who tended to deduce propositions from an initial set of axioms.Entries in the book usually take the form of a statement of a problem, followed by the statement of the solution, and an explanation of the procedure that led to the solution. These were commented on by Liu Hui in the 3rd century.The full title of The Nine Chapters on the Mathematical Art appears on two bronze standard measures which are dated to 179 CE, but there is speculation that the same book existed beforehand under different titles.[1]Most scholars believe that Chinese mathematics and the mathematics of the ancient Mediterranean world had developed more or less independently up to the time when the Nine Chapters reached its final form. The method of chapter 7 was not found in Europe until the 13th century, and the method of chapter 8 uses Gaussian elimination before Carl Friedrich Gauss (1777–1855).[2] There is also the mathematical proof given in the treatise for the Pythagorean theorem.[3] The influence of The Nine Chapters greatly assisted the development of ancient mathematics in the regions of Korea and Japan. Its influence on mathematical thought in China persisted until the Qing Dynasty era.Liu Hui wrote a very detailed commentary on this book in 263.  He analyses the procedures of the Nine Chapters step by step, in a manner which is clearly designed to give the reader confidence that they are reliable, although he is not concerned to provide formal proofs in the Euclidean manner. Liu's commentary is of great mathematical interest in its own right. Liu credits the earlier mathematicians Zhang Cang (fl. 165 BCE - d. 142 BCE) and Geng Shouchang (fl. 75 BCE-49 BCE) (see armillary sphere) with the initial arrangement and commentary on the book, yet Han Dynasty records do not indicate the names of any authors of commentary, as they are not mentioned until the 3rd century.[4]The Nine Chapters is an anonymous work, and its origins are not clear. Until recent years, there was no substantial evidence of related mathematical writing that might have preceded it, with the exception of mathematical work by those such as Jing Fang (78–37 BCE), Liu Xin (d. 23), and Zhang Heng (78–139) and the geometry clauses of the Mozi of the 4th century BCE.  This is no longer the case. The Suàn shù shū (算數書) or writings on reckoning is an ancient Chinese text on mathematics approximately seven thousand characters in length, written on 190 bamboo strips. It was discovered together with other writings in 1983 when archaeologists opened a tomb in Hubei province. It is among the corpus of texts known as the Zhangjiashan Han bamboo texts. From documentary evidence this tomb is known to have been closed in 186 BCE, early in the Western Han dynasty. While its relationship to the Nine Chapters is still under discussion by scholars, some of its contents are clearly paralleled there. The text of the Suàn shù shū is however much less systematic than the Nine Chapters; and appears to consist of a number of more or less independent short sections of text drawn from a number of sources. The Zhoubi Suanjing, a mathematics and astronomy text, was also compiled during the Han, and was even mentioned as a school of mathematics in and around 180 CE by Cai Yong.Contents of The Nine Chapters are as follows:
Linear map
In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V → W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication.  An important special case is when V = W, in which case the map is called a linear operator,[1] or an endomorphism of V. Sometimes the term linear function has the same meaning as linear map, while in analytic geometry it does not.A linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension);[2] for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.In the language of abstract algebra, a linear map is a module homomorphism.  In the language of category theory it is a morphism in the category of modules over a given ring.Thus, a linear map is said to be operation preserving.  In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.If V and W are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from V to W can be represented by a matrix.[6] This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if A is a real m × n matrix, then f(x) = Ax describes a linear map Rn → Rm (see Euclidean space).Let {v1, …, vn} be a basis for V.  Then every vector v in V is uniquely determined by the coefficients c1, …, cn in the field R:If f : V → W is a linear map,which implies that the function f is entirely determined by the vectors f(v1), …, f(vn). Now let {w1, …, wm}  be a basis for W.  Then we can represent each vector f(vj) asThus, the function f is entirely determined by the values of aij. If we put these values into an m × n matrix M, then we can conveniently use it to compute the vector output of f for any vector in V.  To get M, every column j of M is a vectorcorresponding to f(vj) as defined above. To define it more clearly, for some column j that corresponds to the mapping f(vj),where M is the matrix of f. In other words, every column j = 1, …, n has a corresponding vector f(vj) whose coordinates a1j, …, amj are the elements of column j. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.The matrices of a linear transformation can be represented visually:In two-dimensional space R2 linear maps are described by 2 × 2 real matrices. These are some examples:The composition of linear maps is linear: if f : V → W and g : W → Z are linear, then so is their composition g ∘ f : V → Z. It follows from this that the class of all vector spaces over a given field K, together with K-linear maps as morphisms, forms a category.The inverse of a linear map, when defined, is again a linear map.If f1 : V → W and f2 : V → W are linear, then so is their pointwise sum f1 + f2 (which is defined by (f1 + f2)(x) = f1(x) + f2(x)).If f : V → W is linear and a is an element of the ground field K, then the map af, defined by (af)(x) = a(f(x)), is also linear.Thus the set L(V, W) of linear maps from V to W itself forms a vector space over K, sometimes denoted Hom(V, W).  Furthermore, in the case that V = W, this vector space (denoted End(V)) is an associative algebra under composition of maps, since the composition of two linear maps  is again a linear map, and the composition of maps is always associative.  This case is discussed in more detail below.Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.A linear transformation f: V → V is an endomorphism of V; the set of all such endomorphisms End(V)  together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field K (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: V → V.An endomorphism of V that is also an isomorphism is called an automorphism of V. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of V forms a group, the automorphism group of V which is denoted by Aut(V) or GL(V). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(V) is the group of units in the ring End(V).If V has finite dimension n, then End(V) is isomorphic to the associative algebra of all n × n matrices with entries in K. The automorphism group of V is isomorphic to the  general linear group GL(n, K) of all n × n invertible matrices with entries in K.If f : V → W is linear, we define the kernel and the image or range of f byker(f) is a subspace of V and im(f) is a subspace of W.  The following dimension formula is known as the rank–nullity theorem:The number dim(im(f)) is also called the rank of f and written as rank(f), or sometimes, ρ(f); the number dim(ker(f)) is called the nullity of f and written as null(f) or ν(f). If V and W are finite-dimensional, bases have been chosen and f is represented by the matrix A, then the rank and nullity of f are equal to the rank and nullity of the matrix A, respectively.This is the dual notion to the kernel: just as the kernel is a subspace of the domain, the co-kernel is a quotient space of the target.Formally, one has the exact sequenceThese can be interpreted thus: given a linear equation f(v) = w to solve,The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space W/f(V) is the dimension of the target space minus the dimension of the image.For a linear operator with finite-dimensional kernel and co-kernel, one may define  index as:namely the degrees of freedom minus the number of constraints.For a transformation between finite-dimensional vector spaces, this is just the difference dim(V) − dim(W), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.The index of an operator is precisely the Euler characteristic of the 2-term complex 0 → V → W → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.[8]No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.Let V and W denote vector spaces over a field, F. Let T: V → W be a linear map.Given a linear map which is an endomorphism whose matrix is A, in the basis B of the space it transforms vector coordinates [u] as [v] = A[u]. As vectors change with the inverse of B (vectors are contravariant) its inverse transformation is [v] = B[v'].Substituting this in the first expressionhenceTherefore, the matrix in the new basis is A′ = B−1AB, being B the matrix of the given basis.Therefore, linear maps are said to be 1-co- 1-contra-variant objects, or type (1, 1) tensors.A linear transformation between topological vector spaces, for example normed spaces, may be continuous.  If its domain and codomain are the same, it will then be a continuous linear operator.  A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional.[9]  An infinite-dimensional domain may have discontinuous linear operators.An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0).  For a specific example, sin(nx)/n converges to 0, but its derivative cos(nx) does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).A specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.Another application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.
Range (mathematics)
In mathematics, and more specifically in naive set theory, the range of a function refers to either the codomain or the image of the function, depending upon usage.  Modern usage almost always uses range to mean image.The codomain of a function is some arbitrary super-set of image.  In real analysis, it is the real numbers.  In complex analysis, it is the complex numbers.The image of a function is the set of all outputs of the function.  The image is always a subset of the codomain.As the term "range" can have different meanings, it is considered a good practice to define it the first time it is used in a textbook or article.Older books, when they use the word "range", tend to use it to mean what is now called the codomain.[1][2]  More modern books, if they use the word "range" at all, generally use it to mean what is now called the image.[3]  To avoid any confusion, a number of modern books don't use the word "range" at all.[4]When "range" is used to mean "codomain", the image of a function f is already implicitly defined. It is (by definition of image) the (maybe trivial) subset of the "range" which equals {y | there exists an x in the domain of f such that y = f(x)}.When "range" is used to mean "image", the range of a function f is by definition {y | there exists an x in the domain of f such that y = f(x)}.  In this case, the codomain of f must not be specified, because any codomain which contains this image as a (maybe trivial) subset will work.In both cases, image f ⊆ range f ⊆ codomain f, with at least one of the containments being equality.
Quantum mechanics
Quantum mechanics (QM; also known as quantum physics, quantum theory, the wave mechanical model, or matrix mechanics), including quantum field theory, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles.[2]Classical physics, the physics existing before quantum mechanics, describes nature at ordinary (macroscopic) scale. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3] Quantum mechanics differs from classical physics in that energy, momentum, angular momentum and other quantities of a bound system are restricted to discrete values (quantization); objects have characteristics of both particles and waves (wave-particle duality); and there are limits to the precision with which quantities can be measured (uncertainty principle).[note 1]Quantum mechanics gradually arose from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and from the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schrödinger, Werner Heisenberg, Max Born and others.  The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.Important applications of quantum theory[5] include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy.  Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.[6]Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[7] In 1803, Thomas Young, an English polymath, performed the famous double-slit experiment that he later described in a paper titled On the nature of light and colours. This experiment played a major role in the general acceptance of the wave theory of light.In 1838, Michael Faraday discovered cathode rays. These studies were followed by the 1859 statement of the black-body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system can be discrete, and the 1900 quantum hypothesis of Max Planck.[8] Planck's hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets) precisely matched the observed patterns of black-body radiation.In 1896, Wilhelm Wien empirically determined a distribution law of black-body radiation,[9] known as Wien's law in his honor. Ludwig Boltzmann independently arrived at this result by considerations of Maxwell's equations. However, it was valid only at high frequencies and underestimated the radiance at low frequencies. Later, Planck corrected this model using Boltzmann's statistical interpretation of thermodynamics and proposed what is now called Planck's law, which led to the development of quantum mechanics.Following Max Planck's solution in 1900 to the black-body radiation problem (reported 1859), Albert Einstein offered a quantum-based theory to explain the photoelectric effect (1905, reported 1887).  Around 1900–1910, the atomic theory and the corpuscular theory of light[10] first came to be widely accepted as scientific fact; these latter theories can be viewed as quantum theories of matter and electromagnetic radiation, respectively.Among the first to study quantum phenomena in nature were Arthur Compton, C. V. Raman, and Pieter Zeeman, each of whom has a quantum effect named after him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Ernest Rutherford experimentally discovered the nuclear model of the atom, for which Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld.[11] This phase is known as old quantum theory.According to Planck, each energy element (E) is proportional to its frequency (ν):where h is Planck's constant.Planck cautiously insisted that this was simply an aspect of the processes of absorption and emission of radiation and had nothing to do with the physical reality of the radiation itself.[12] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[13] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. He won the 1921 Nobel Prize in Physics for this work.Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete quantum of energy that was dependent on its frequency.[14]The foundations of quantum mechanics were established during the first half of the 20th century by Max Planck, Niels Bohr, Werner Heisenberg, Louis de Broglie, Arthur Compton, Albert Einstein, Erwin Schrödinger, Max Born, John von Neumann, Paul Dirac, Enrico Fermi, Wolfgang Pauli, Max von Laue, Freeman Dyson, David Hilbert, Wilhelm Wien, Satyendra Nath Bose, Arnold Sommerfeld, and others. The Copenhagen interpretation of Niels Bohr became widely accepted.In the mid-1920s, developments in quantum mechanics led to its becoming the standard formulation for atomic physics. In the summer of 1925, Bohr and Heisenberg published results that closed the old quantum theory. Out of deference to their particle-like behavior in certain processes and measurements, light quanta came to be called photons (1926). In 1926 Erwin Schrödinger suggested a partial differential equation for the wave functions of particles like electrons. And when effectively restricted to a finite region, this equation allowed only certain modes, corresponding to discrete quantum states—whose properties turned out to be exactly the same as implied by matrix mechanics.[15] From Einstein's simple postulation was born a flurry of debating, theorizing, and testing. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[citation needed]It was found that subatomic particles and electromagnetic waves are neither simply particle nor wave but have certain properties of each. This originated the concept of wave–particle duality.[citation needed]By 1930, quantum mechanics had been further unified and formalized by the work of David Hilbert, Paul Dirac and John von Neumann[16] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry, quantum electronics, quantum optics, and quantum information science. Its speculative modern developments include string theory and quantum gravity theories. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies.[citation needed]While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors,[17]  and superfluids.[18]The word quantum derives from the Latin, meaning "how great" or "how much".[19] In quantum mechanics, it refers to a discrete unit assigned to certain physical quantities such as the energy of an atom at rest (see Figure 1). The discovery that particles are discrete packets of energy with wave-like properties led to the branch of physics dealing with atomic and subatomic systems which is today called quantum mechanics. It underlies the mathematical framework of many fields of physics and chemistry, including condensed matter physics, solid-state physics, atomic physics, molecular physics, computational physics, computational chemistry, quantum chemistry, particle physics, nuclear chemistry, and nuclear physics.[20][better source needed] Some fundamental aspects of the theory are still actively studied.[21]Quantum mechanics is essential to understanding the behavior of systems at atomic length scales and smaller. If the physical nature of an atom were solely described by classical mechanics, electrons would not orbit the nucleus, since orbiting electrons emit radiation (due to circular motion) and would quickly collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, non-deterministic, smeared, probabilistic wave–particle orbital about the nucleus, defying the traditional assumptions of classical mechanics and electromagnetism.[22]Quantum mechanics was initially developed to provide a better explanation and description of the atom, especially the differences in the spectra of light emitted by different isotopes of the same chemical element, as well as subatomic particles. In short, the quantum-mechanical atomic model has succeeded spectacularly in the realm where classical mechanics and electromagnetism falter.Broadly speaking, quantum mechanics incorporates four classes of phenomena for which classical physics cannot account:However, later, in October 2018, physicists reported that quantum behavior can be explained with classical physics for a single particle, but not for multiple particles as in quantum entanglement and related nonlocality phenomena.[23][24]In the mathematically rigorous formulation of quantum mechanics developed by Paul Dirac,[25] David Hilbert,[26] John von Neumann,[27] and Hermann Weyl,[28] the possible states of a quantum mechanical system are symbolized[29] as unit vectors (called state vectors). Formally, these reside in a complex separable Hilbert space—variously called the state space or the associated Hilbert space of the system—that is well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system—for example, the state space for position and momentum states is the space of square-integrable functions, while the state space for the spin of a single proton is just the product of two complex planes. Each observable is represented by a maximally Hermitian (precisely: by a self-adjoint) linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. If the operator's spectrum is discrete, the observable can attain only those discrete eigenvalues.In the formalism of quantum mechanics, the state of a system at a given time is described by a complex wave function, also referred to as state vector in a complex vector space.[30] This abstract mathematical object allows for the calculation of probabilities of outcomes of concrete experiments. For example, it allows one to compute the probability of finding an electron in a particular region around the nucleus at a particular time. Contrary to classical mechanics, one can never make simultaneous predictions of conjugate variables, such as position and momentum, to arbitrary precision. For instance, electrons may be considered (to a certain probability) to be located somewhere within a given region of space, but with their exact positions unknown. Contours of constant probability density, often referred to as "clouds", may be drawn around the nucleus of an atom to conceptualize where the electron might be located with the most probability. Heisenberg's uncertainty principle quantifies the inability to precisely locate the particle given its conjugate momentum.[31]According to one interpretation, as the result of a measurement, the wave function containing the probability information for a system collapses from a given initial state to a particular eigenstate. The possible results of a measurement are the eigenvalues of the operator representing the observable—which explains the choice of Hermitian operators, for which all the eigenvalues are real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator. Heisenberg's uncertainty principle is represented by the statement that the operators corresponding to certain observables do not commute.The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of "wave function collapse" (see, for example, the relative state interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled, so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.[32]Generally, quantum mechanics does not assign definite values. Instead, it makes a prediction using a probability distribution; that is, it describes the probability of obtaining the possible outcomes from measuring an observable. Often these results are skewed by many causes, such as dense probability clouds. Probability clouds are approximate (but better than the Bohr model) whereby electron location is given by a probability function, the wave function eigenvalue, such that the probability is the squared modulus of the complex amplitude, or quantum state nuclear attraction.[33][34] Naturally, these probabilities will depend on the quantum state at the "instant" of the measurement. Hence, uncertainty is involved in the value. There are, however, certain states that are associated with a definite value of a particular observable. These are known as eigenstates of the observable ("eigen" can be translated from German as meaning "inherent" or "characteristic").[35]In the everyday world, it is natural and intuitive to think of everything (every observable) as being in an eigenstate. Everything appears to have a definite position, a definite momentum, a definite energy, and a definite time of occurrence. However, quantum mechanics does not pinpoint the exact values of a particle's position and momentum (since they are conjugate pairs) or its energy and time (since they too are conjugate pairs).  Rather, it provides only a range of probabilities in which that particle might be given its momentum and momentum probability. Therefore, it is helpful to use different words to describe states having uncertain values and states having definite values (eigenstates).Usually, a system will not be in an eigenstate of the observable (particle) we are interested in. However, if one measures the observable, the wave function will instantaneously be an eigenstate (or "generalized" eigenstate) of that observable. This process is known as wave function collapse, a controversial and much-debated process[36] that involves expanding the system under study to include the measurement device. If one knows the corresponding wave function at the instant before the measurement, one will be able to compute the probability of the wave function collapsing into each of the possible eigenstates.For example, the free particle in the previous example will usually have a wave function that is a wave packet centered around some mean position x0 (neither an eigenstate of position nor of momentum). When one measures the position of the particle, it is impossible to predict with certainty the result.[32] It is probable, but not certain, that it will be near x0, where the amplitude of the wave function is large. After the measurement is performed, having obtained some result x, the wave function collapses into a position eigenstate centered at x.[37]The time evolution of a quantum state is described by the Schrödinger equation, in which the Hamiltonian (the operator corresponding to the total energy of the system) generates the time evolution. The time evolution of wave functions is deterministic in the sense that—given a wave function at an initial time—it makes a definite prediction of what the wave function will be at any later time.[38]During a measurement, on the other hand, the change of the initial wave function into another, later wave function is not deterministic, it is unpredictable (i.e., random). A time-evolution simulation can be seen here.[39][40]Wave functions change as time progresses. The Schrödinger equation describes how wave functions change in time, playing a role similar to Newton's second law in classical mechanics. The Schrödinger equation, applied to the aforementioned example of the free particle, predicts that the center of a wave packet will move through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more uncertain with time. This also has the effect of turning a position eigenstate (which can be thought of as an infinitely sharp wave packet) into a broadened wave packet that no longer represents a (definite, certain) position eigenstate.[41]Some wave functions produce probability distributions that are constant, or independent of time—such as when in a stationary state of constant energy, time vanishes in the absolute square of the wave function. Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics it is described by a static, spherically symmetric wave function surrounding the nucleus (Fig. 1) (note, however, that only the lowest angular momentum states, labeled s, are spherically symmetric).[42]The Schrödinger equation acts on the entire probability amplitude, not merely its absolute value. Whereas the absolute value of the probability amplitude encodes information about probabilities, its phase encodes information about the interference between quantum states. This gives rise to the "wave-like" behavior of quantum states. As it turns out, analytic solutions of the Schrödinger equation are available for only a very small number of relatively simple model Hamiltonians, of which the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom are the most important representatives. Even the helium atom—which contains just one more electron than does the hydrogen atom—has defied all attempts at a fully analytic treatment.There exist several techniques for generating approximate solutions, however. In the important method known as perturbation theory, one uses the analytic result for a simple quantum mechanical model to generate a result for a more complicated model that is related to the simpler model by (for one example) the addition of a weak potential energy. Another method is the "semi-classical equation of motion" approach, which applies to systems for which quantum mechanics produces only weak (small) deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.There are numerous mathematically equivalent formulations of quantum mechanics. One of the oldest and most commonly used formulations is the "transformation theory" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics—matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrödinger).[43]Especially since Werner Heisenberg was awarded the Nobel Prize in Physics in 1932 for the creation of quantum mechanics, the role of Max Born in the development of QM was overlooked until the 1954 Nobel award. The role is noted in a 2005 biography of Born, which recounts his role in the matrix formulation of quantum mechanics, and the use of probability amplitudes. Heisenberg himself acknowledges having learned matrices from Born, as published in a 1940 festschrift honoring Max Planck.[44]  In the matrix formulation, the instantaneous state of a quantum system encodes the probabilities of its measurable properties, or "observables". Examples of observables include energy, position, momentum, and angular momentum. Observables can be either continuous (e.g., the position of a particle) or discrete (e.g., the energy of an electron bound to a hydrogen atom).[45] An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.The rules of quantum mechanics are fundamental. They assert that the state space of a system is a Hilbert space (crucially, that the space has an inner product) and that observables of that system are Hermitian operators acting on vectors in that space—although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system. An important guide for making these choices is the correspondence principle, which states that the predictions of quantum mechanics reduce to those of classical mechanics when a system moves to higher energies or, equivalently, larger quantum numbers, i.e. whereas a single particle exhibits a degree of randomness, in systems incorporating millions of particles averaging takes over and, at the high energy limit, the statistical probability of random behaviour approaches zero. In other words, classical mechanics is simply a quantum mechanics of large systems. This "high energy" limit is known as the classical or correspondence limit. One can even start from an established classical model of a particular system, then attempt to guess the underlying quantum model that would give rise to the classical model in the correspondence limit.When quantum mechanics was originally formulated, it was applied to models whosecorrespondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg. These three men shared the Nobel Prize in Physics in 1979 for this work.[46]It has proven difficult to construct quantum models of gravity, the remaining fundamental force. Semi-classical approximations are workable, and have led to predictions such as Hawking radiation. However, the formulation of a complete theory of quantum gravity is hindered by apparent incompatibilities between general relativity (the most accurate theory of gravity currently known) and some of the fundamental assumptions of quantum theory. The resolution of these incompatibilities is an area of active research, and theories such as string theory are among the possible candidates for a future theory of quantum gravity.Classical mechanics has also been extended into the complex domain, with complex classical mechanics exhibiting behaviors similar to quantum mechanics.[47]Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy.[48] According to the correspondence principle between classical and quantum mechanics, all objects obey the laws of quantum mechanics, and classical mechanics is just an approximation for large systems of objects (or a statistical quantum mechanics of a large collection of particles).[49] The laws of classical mechanics thus follow from the laws of quantum mechanics as a statistical average at the limit of large systems or large quantum numbers.[50] However, chaotic systems do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.Quantum coherence is an essential difference between classical and quantum theories as illustrated by the Einstein–Podolsky–Rosen (EPR) paradox — an attack on a certain philosophical interpretation of quantum mechanics by an appeal to local realism.[51] Quantum interference involves adding together probability amplitudes, whereas classical "waves" infer that there is an adding together of intensities. For microscopic bodies, the extension of the system is much smaller than the coherence length, which gives rise to long-range entanglement and other nonlocal phenomena characteristic of quantum systems.[52] Quantum coherence is not typically evident at macroscopic scales, though an exception to this rule may occur at extremely low temperatures (i.e. approaching absolute zero) at which quantum behavior may manifest itself macroscopically.[53] This is in accordance with the following observations:A big difference between classical and quantum mechanics is that they use very different kinematic descriptions.[56]In Niels Bohr's mature view, quantum mechanical phenomena are required to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics.[57][58][59][60] The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, in terms of both position and momentum, of an initial condition or "state" (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition.[61][62] In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous "state" in the classical sense of that word.[63][64] Thus there are two kinds of processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined.[61] Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schrödinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.[65][66]For many experiments, it is possible to think of the initial and final conditions of the system as being a particle. In some cases it appears that there are potentially several spatially distinct pathways or trajectories by which a particle might pass from initial to final condition. It is an important feature of the quantum kinematic description that it does not permit a unique definite statement of which of those pathways is actually followed. Only the initial and final conditions are definite, and, as stated in the foregoing paragraph, they are defined only as precisely as allowed by the configuration space description or its equivalent. In every case for which a quantum kinematic description is needed, there is always a compelling reason for this restriction of kinematic precision. An example of such a reason is that for a particle to be experimentally found in a definite position, it must be held motionless; for it to be experimentally found to have a definite momentum, it must have free motion; these two are logically incompatible.[67][68]Classical kinematics does not primarily demand experimental description of its phenomena. It allows completely precise description of an instantaneous state by a value in phase space, the Cartesian product of configuration and momentum spaces. This description simply assumes or imagines a state as a physically existing entity without concern about its experimental measurability. Such a description of an initial condition, together with Newton's laws of motion, allows a precise deterministic and causal prediction of a final condition, with a definite trajectory of passage. Hamiltonian dynamics can be used for this. Classical kinematics also allows the description of a process analogous to the initial and final condition description used by quantum mechanics. Lagrangian mechanics applies to this.[69] For processes that need account to be taken of actions of a small number of Planck constants, classical kinematics is not adequate; quantum mechanics is needed.Even with the defining postulates of both Einstein's theory of general relativity and quantum theory being indisputably supported by rigorous and repeated empirical evidence, and while they do not directly contradict each other theoretically (at least with regard to their primary claims), they have proven extremely difficult to incorporate into one consistent, cohesive model.[70]Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. Many prominent physicists, including Stephen Hawking, have labored for many years in the attempt to discover a theory underlying everything. This TOE would combine not only the different models of subatomic physics, but also derive the four fundamental forces of nature - the strong force, electromagnetism, the weak force, and gravity - from a single force or phenomenon. While Stephen Hawking was initially a believer in the Theory of Everything, after considering Gödel's Incompleteness Theorem, he has concluded that one is not obtainable, and has stated so publicly in his lecture "Gödel and the End of Physics" (2002).[71]The quest to unify the fundamental forces through quantum mechanics is still ongoing. Quantum electrodynamics (or "quantum electromagnetism"), which is currently (in the perturbative regime at least) the most accurately tested physical theory in competition with general relativity,[72][73] has been successfully merged with the weak nuclear force into the electroweak force and work is currently being done to merge the electroweak and strong force into the electrostrong force. Current predictions state that at around 1014 GeV the three aforementioned forces are fused into a single unified field.[74] Beyond this "grand unification", it is speculated that it may be possible to merge gravity with the other three gauge symmetries, expected to occur at roughly 1019 GeV. However — and while special relativity is parsimoniously incorporated into quantum electrodynamics — the expanded general relativity, currently the best theory describing the gravitation force, has not been fully incorporated into quantum theory. One of those searching for a coherent TOE is Edward Witten, a theoretical physicist who formulated the M-theory, which is an attempt at describing the supersymmetrical based string theory. M-theory posits that our apparent 4-dimensional spacetime is, in reality, actually an 11-dimensional spacetime containing 10 spatial dimensions and 1 time dimension, although 7 of the spatial dimensions are - at lower energies - completely "compactified" (or infinitely curved) and not readily amenable to measurement or probing.Another popular theory is Loop quantum gravity (LQG), a theory first proposed by Carlo Rovelli that describes the quantum properties of gravity. It is also a theory of quantum space and quantum time, because in general relativity the geometry of spacetime is a manifestation of gravity. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature of the granularity of the photons in the quantum theory of electromagnetism or the discrete levels of the energy of the atoms. But here it is space itself which is discrete.More precisely, space can be viewed as an extremely fine fabric or network "woven" of finite loops. These networks of loops are called spin networks. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the Planck length, which is approximately 1.616×10−35 m. According to theory, there is no meaning to length shorter than this (cf. Planck scale energy). Therefore, LQG predicts that not just matter, but also space itself, has an atomic structure.Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. Even fundamental issues, such as Max Born's basic rules concerning probability amplitudes and probability distributions, took decades to be appreciated by society and many leading scientists. Richard Feynman once said, "I think I can safely say that nobody understands quantum mechanics."[75]  According to Steven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."[76]The Copenhagen interpretation—due largely to Niels Bohr and Werner Heisenberg—remains most widely accepted amongst physicists, some 75 years after its enunciation. According to this interpretation, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but instead must be considered a final renunciation of the classical idea of "causality." It is also believed therein that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the conjugate nature of evidence obtained under different experimental situations.Albert Einstein, himself one of the founders of quantum theory, did not accept some of the more philosophical or metaphysical interpretations of quantum mechanics, such as rejection of determinism and of causality. He is famously quoted as saying, in response to this aspect, "God does not play with dice".[77] He rejected the concept that the state of a physical system depends on the experimental arrangement for its measurement. He held that a state of nature occurs in its own right, regardless of whether or how it might be observed. In that view, he is supported by the currently accepted definition of a quantum state, which remains invariant under arbitrary choice of configuration space for its representation, that is to say, manner of observation. He also held that underlying quantum mechanics there should be a theory that thoroughly and directly expresses the rule against action at a distance; in other words, he insisted on the principle of locality. He considered, but rejected on theoretical grounds, a particular proposal for hidden variables to obviate the indeterminism or acausality of quantum mechanical measurement. He considered that quantum mechanics was a currently valid but not a permanently definitive theory for quantum phenomena. He thought its future replacement would require profound conceptual advances, and would not come quickly or easily. The Bohr-Einstein debates provide a vibrant critique of the Copenhagen Interpretation from an epistemological point of view. In arguing for his views, he produced a series of objections, the most famous of which has become known as the Einstein–Podolsky–Rosen paradox.John Bell showed that this EPR paradox led to experimentally testable differences between quantum mechanics and theories that rely on added hidden variables. Experiments have been performed confirming the accuracy of quantum mechanics, thereby demonstrating that quantum mechanics cannot be improved upon by addition of hidden variables.[78] Alain Aspect's initial experiments in 1982, and many subsequent experiments since, have definitively verified quantum entanglement. By the early 1980s, experiments had shown that such inequalities were indeed violated in practice—so that there were in fact correlations of the kind suggested by quantum mechanics. At first these just seemed like isolated esoteric effects, but by the mid-1990s, they were being codified in the field of quantum information theory, and led to constructions with names like quantum cryptography and quantum teleportation.[79] Entanglement, as demonstrated in Bell-type experiments, does not, however, violate causality, since no transfer of information happens. Quantum entanglement forms the basis of quantum cryptography, which is proposed for use in high-security commercial applications in banking and government.The Everett many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[80] This is not accomplished by introducing some "new axiom" to quantum mechanics, but on the contrary, by removing the axiom of the collapse of the wave packet. All of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a real physical—not just formally mathematical, as in other interpretations—quantum superposition. Such a superposition of consistent state combinations of different systems is called an entangled state. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett's interpretation is perfectly consistent with John Bell's experiments and makes them intuitively understandable. However, according to the theory of quantum decoherence, these "parallel universes" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes entangled with both the physicist who measured it and a huge number of other particles, some of which are photons flying away at the speed of light towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring all these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one could theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist's memory). In light of these Bell tests, Cramer (1986) formulated his transactional interpretation[81] which is unique in providing a physical explanation for the Born rule.[82] Relational quantum mechanics appeared in the late 1990s as the modern derivative of the Copenhagen Interpretation.Quantum mechanics has had enormous[83] success in explaining many of the features of our universe. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Quantum mechanics has strongly influenced string theories, candidates for a Theory of Everything (see reductionism).Quantum mechanics is also critically important for understanding how individual atoms are joined by covalent bond to form molecules. The application of quantum mechanics to chemistry is known as quantum chemistry.  Quantum mechanics can also provide quantitative insight into ionic and covalent bonding processes by explicitly showing which molecules are energetically favorable to which others and the magnitudes of the energies involved.[84] Furthermore, most of the calculations performed in modern computational chemistry rely on quantum mechanics.In many aspects modern technology operates at a scale where quantum effects are significant.Many modern electronic devices are designed using quantum mechanics. Examples include the laser, the transistor (and thus the microchip), the electron microscope, and magnetic resonance imaging (MRI). The study of semiconductors led to the invention of the diode and the transistor, which are indispensable parts of modern electronics systems, computer and telecommunication devices. Another application is for making laser diode and light emitting diode which are a high-efficiency source of light.Many electronic devices operate under effect of quantum tunneling. It even exists in the simple light switch. The switch would not work if electrons could not quantum tunnel through the layer of oxidation on the metal contact surfaces. Flash memory chips found in USB drives use quantum tunneling to erase their memory cells. Some negative differential resistance devices also utilize quantum tunneling effect, such as resonant tunneling diode. Unlike classical diodes, its current is carried by resonant tunneling through two or more potential barriers (see right figure). Its negative resistance behavior can only be understood with quantum mechanics: As the confined state moves close to Fermi level, tunnel current increases. As it moves away, current decreases. Quantum  mechanics is necessary to understanding and designing such electronic devices.Researchers are currently seeking robust methods of directly manipulating quantum states. Efforts are being made to more fully develop quantum cryptography, which will theoretically allow guaranteed secure transmission of information.An inherent advantage yielded by quantum cryptography when compared to classical cryptography is the detection of passive eavesdropping. This is a natural result of the behavior of quantum bits; due to the observer effect, if a bit in a superposition state were to be observed, the superposition state would collapse into an eigenstate. Because the intended recipient was expecting to receive the bit in a superposition state, the intended recipient would know there was an attack, because the bit's state would no longer be in a superposition.[85]Another goal is the development of quantum computers, which are expected to perform certain computational tasks exponentially faster than classical computers. Instead of using classical bits, quantum computers use qubits, which can be in superpositions of states. Quantum programmers are able to  manipulate the superposition of qubits in order to solve problems that classical computing cannot do effectively, such as searching unsorted databases or integer factorization. IBM claims that the advent of quantum computing may progress the fields of medicine, logistics, financial services, artificial intelligence and cloud security.[86]Another active research topic is quantum teleportation, which deals with techniques to transmit quantum information over arbitrary distances.While quantum mechanics primarily applies to the smaller atomic regimes of matter and energy, some systems exhibit quantum mechanical effects on a large scale. Superfluidity, the frictionless flow of a liquid at temperatures near absolute zero, is one well-known example. So is the closely related phenomenon of superconductivity, the frictionless flow of an electron gas in a conducting material (an electric current) at sufficiently low temperatures. The fractional quantum Hall effect is a topological ordered state which corresponds to patterns of long-range quantum entanglement.[87] States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.Quantum theory also provides accurate descriptions for many previously unexplained phenomena, such as black-body radiation and the stability of the orbitals of electrons in atoms. It has also given insight into the workings of many different biological systems, including smell receptors and protein structures.[88] Recent work on photosynthesis has provided evidence that quantum correlations play an essential role in this fundamental process of plants and many other organisms.[89] Even so, classical physics can often provide good approximations to results otherwise obtained by quantum physics, typically in circumstances with large numbers of particles or large quantum numbers. Since classical formulas are much simpler and easier to compute than quantum formulas, classical approximations are used and preferred when the system is large enough to render the effects of quantum mechanics insignificant.For example, consider a free particle. In quantum mechanics, a free matter is described by a wave function.  The particle properties of the matter become apparent when we measure its position and velocity.  The wave properties of the matter become apparent when we measure its wave properties like interference.  The wave–particle duality feature is incorporated in the relations of coordinates and operators in the formulation of quantum mechanics.  Since the matter is free (not subject to any interactions), its quantum state can be represented as a wave of arbitrary shape and extending over space as a wave function. The position and momentum of the particle are observables. The Uncertainty Principle states that both the position and the momentum cannot simultaneously be measured with complete precision. However, one can measure the position (alone) of a moving free particle, creating an eigenstate of position with a wave function that is very large (a Dirac delta) at a particular position x, and zero everywhere else. If one performs a position measurement on such a wave function, the resultant x will be obtained with 100% probability (i.e., with full certainty, or complete precision). This is called an eigenstate of position—or, stated in mathematical terms, a generalized position eigenstate (eigendistribution). If the particle is in an eigenstate of position, then its momentum is completely unknown. On the other hand, if the particle is in an eigenstate of momentum, then its position is completely unknown.[90]In an eigenstate of momentum having a plane wave form, it can be shown that the wavelength is equal to h/p, where h is Planck's constant and p is the momentum of the eigenstate.[91]With the differential operator defined bythe previous equation is evocative of the classic kinetic energy analogue,The general solutions of the Schrödinger equation for the particle in a box areor, from Euler's formula,The infinite potential walls of the box determine the values of C, D, and k at x = 0 and x = L where ψ must be zero. Thus, at x = 0,and D = 0. At x = L,in which C cannot be zero as this would conflict with the Born interpretation. Therefore, since sin(kL) = 0, kL must be an integer multiple of π,The quantization of energy levels follows from this constraint on k, sinceA finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth.The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well.This is a model for the quantum tunneling effect which plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy. Quantum tunneling is central to physical phenomena involved in superlattices.As in the classical case, the potential for the quantum harmonic oscillator is given byThis problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. The eigenstates are given bywhere Hn are the Hermite polynomialsand the corresponding energy levels areThis is another example illustrating the quantification of energy for bound states.The potential in this case is given by:The solutions are superpositions of left- and right-moving waves:andwith coefficients A and B determined from the boundary conditions and by imposing a continuous derivative on the solution, and where the wave vectors are related to the energy viaandEach term of the solution can be interpreted as an incident, reflected, or transmitted component of the wave, allowing the calculation of transmission and reflection coefficients. Notably, in contrast to classical mechanics, incident particles with energies greater than the potential step are partially reflected.The following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus.More technical:On Wikibooks
Normal matrix
In mathematics, a complex square matrix A is normal if it commutes with its conjugate transpose A*:The concept of normal matrices can be extended to normal operators on infinite dimensional normed spaces and to normal elements in C*-algebras. As in the matrix case, normality means commutativity is preserved, to the extent possible, in the noncommutative setting. This makes normal operators, and normal elements of C*-algebras, more amenable to analysis.Spectral theorem states that a matrix is normal if and only if it is unitarily similar to a diagonal matrix, and therefore any matrix A satisfying the equation A∗A = AA∗ is diagonalizable.Among complex matrices, all unitary, Hermitian, and skew-Hermitian matrices are normal. Likewise, among real matrices, all orthogonal, symmetric, and skew-symmetric matrices are normal. However, it is not the case that all normal matrices are either unitary or (skew-)Hermitian. For example,is neither unitary, Hermitian, nor skew-Hermitian, yet it is normal because Let A be a normal upper triangular matrix. Since (A∗A)ii = (AA∗)ii, one has ⟨ei, A*Aei⟩ = ⟨ei, AA*ei⟩; i.e., the first row must have the same norm as the first column:The first entry of row 1 and column 1 are the same, and the rest of column 1 is zero. This implies the first row must be zero for entries 2 through n. Continuing this argument for row–column pairs 2 through n shows A is diagonal.The concept of normality is important because normal matrices are precisely those to which the spectral theorem applies: The diagonal entries of Λ are the eigenvalues of A, and the columns of U are the eigenvectors of A. The matching eigenvalues in Λ come in the same order as the eigenvectors are ordered as columns of U.Another way of stating the spectral theorem is to say that normal matrices are precisely those matrices that can be represented by a diagonal matrix with respect to a properly chosen orthonormal basis of Cn. Phrased differently: a matrix is normal if and only if its eigenspaces span Cn and are pairwise orthogonal with respect to the standard inner product of Cn.The spectral theorem for normal matrices is a special case of the more general Schur decomposition which holds for all square matrices. Let A be a square matrix. Then by Schur decomposition it is unitary similar to an upper-triangular matrix, say, B. If A is normal, so is B. But then B must be diagonal, for, as noted above, a normal upper-triangular matrix is diagonal.The spectral theorem permits the classification of normal matrices in terms of their spectra, for example: In general, the sum or product of two normal matrices need not be normal. However, the following holds: In this special case, the columns of U∗ are eigenvectors of both A and B and form an orthonormal basis in Cn. This follows by combining the theorems that, over an algebraically closed field, commuting matrices are simultaneously triangularizable and a normal matrix is diagonalizable – the added result is that these can both be done simultaneously.It is possible to give a fairly long list of equivalent definitions of a normal matrix. Let A be a n × n complex matrix. Then the following are equivalent:Some but not all of the above generalize to normal operators on infinite-dimensional Hilbert spaces. For example, a bounded operator satisfying (9) is only quasinormal.It is occasionally useful (but sometimes misleading) to think of the relationships of different kinds of normal matrices as analogous to the relationships between different kinds of complex numbers:As a special case, the complex numbers may be embedded in the normal 2 × 2 real matrices by the mapping which preserves addition and multiplication. It is easy to check that this embedding respects all of the above analogies.
Linear form
In linear algebra, a linear functional or linear form (also called a one-form or covector) is a linear map from a vector space to its field of scalars. In ℝn, if vectors are represented as column vectors, then linear functionals are represented as row vectors, and their action on vectors is given by the dot product, or the matrix product with the row vector on the left and the column vector on the right.  In general, if V is a vector space over a field k, then a linear functional f is a function from V to k that is linear:The set of all linear functionals from V to k, Homk(V,k), forms a vector space over k with the addition of the operations of addition and scalar multiplication (defined pointwise).  This space is called the dual space of V, or sometimes the algebraic dual space, to distinguish it from the continuous dual space.  It is often written V∗, V′, or Vᐯ when the field k is understood.If V is a topological vector space, the space of continuous linear functionals —  the continuous dual — is often simply called the dual space.  If V is a Banach space, then so is its (continuous) dual.  To distinguish the ordinary dual space from the continuous dual space, the former is sometimes called the algebraic dual space.  In finite dimensions, every linear functional is continuous, so the continuous dual is the same as the algebraic dual, but in infinite dimensions the continuous dual is a proper subspace of the algebraic dual.Suppose that vectors in the real coordinate space Rn are represented as column vectorsFor each row vector [a1 … an] there is a linear functional f defined byand each linear functional can be expressed in this form.Linear functionals first appeared in functional analysis, the study of vector spaces of functions.  A typical example of a linear functional is integration: the linear transformation defined by the Riemann integralis a linear functional from the vector space C[a, b] of continuous functions on the interval [a, b] to the real numbers. The linearity of I follows from the standard facts about the integral:Let Pn denote the vector space of real-valued polynomial functions of degree ≤n defined on an interval [a, b].  If c ∈ [a, b], then let evc : Pn → R be the evaluation functionalThe mapping f → f(c) is linear sinceIf x0, ..., xn are n + 1 distinct points in [a, b], then the evaluation functionals evxi, i = 0, 1, ..., n form a basis of the dual space of Pn.  (Lax (1996) proves this last fact using Lagrange interpolation.)The integration functional I defined above defines a linear functional on the subspace Pn of polynomials of degree ≤ n. If x0, ..., xn are n + 1 distinct points in [a, b], then there are coefficients a0, ..., an for whichfor all f ∈ Pn. This forms the foundation of the theory of numerical quadrature.This follows from the fact that the linear functionals evxi : f → f(xi) defined above form a basis of the dual space of Pn.[1]Linear functionals are particularly important in quantum mechanics.  Quantum mechanical systems are represented by Hilbert spaces, which are anti–isomorphic to their own dual spaces.  A state of a quantum mechanical system can be  identified with a linear functional.  For more information see bra–ket notation.In the theory of generalized functions, certain kinds of generalized functions called distributions can be realized as linear functionals on spaces of test functions.In finite dimensions, a linear functional can be visualized in terms of its level sets.  In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel hyperplanes.  This method of visualizing linear functionals is sometimes introduced in general relativity texts, such as Gravitation by Misner, Thorne & Wheeler (1973).Every non-degenerate bilinear form on a finite-dimensional vector space V induces an isomorphism V → V∗ : v ↦ v∗ such that where the bilinear form on V is denoted ⟨ , ⟩ (for instance, in Euclidean space ⟨v, w⟩ = v ⋅ w is the dot product of v and w).The inverse isomorphism is V∗ → V : v∗ ↦ v, where v is the unique element of V such thatThe above defined vector v∗ ∈ V∗ is said to be the dual vector of v ∈ V.In an infinite dimensional Hilbert space, analogous results hold by the Riesz representation theorem.  There is a mapping V → V∗ into the continuous dual space V∗.  However, this mapping is antilinear rather than linear.Or, more succinctly,where δ is the Kronecker delta.  Here the superscripts of the basis functionals are not exponents but are instead contravariant indices.due to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals.  ThenSo each component of a linear functional can be extracted by applying the functional to the corresponding basis vector.In higher dimensions, this generalizes as follows
Matrix calculus
In mathematics, matrix calculus is a specialized notation for doing multivariable calculus, especially over spaces of matrices.  It collects the various partial derivatives of a single function with respect to many variables, and/or of a multivariate function with respect to a single variable, into vectors and matrices that can be treated as single entities.  This greatly simplifies operations such as finding the maximum or minimum of a multivariate function and solving systems of differential equations. The notation used here is commonly used in statistics and engineering, while the tensor index notation is preferred in physics.Two competing notational conventions split the field of matrix calculus into two separate groups. The two groups can be distinguished by whether they write the derivative of a scalar with respect to a vector as a column vector or a row vector. Both of these conventions are possible even when the common assumption is made that vectors should be treated as column vectors when combined with matrices (rather than row vectors). A single convention can be somewhat standard throughout a single field that commonly uses matrix calculus (e.g. econometrics, statistics, estimation theory and machine learning). However, even within a given field different authors can be found using competing conventions. Authors of both groups often write as though their specific convention were standard. Serious mistakes can result when combining results from different authors without carefully verifying that compatible notations have been used. Definitions of these two conventions and comparisons between them are collected in the layout conventions section.Matrix calculus refers to a number of different notations that use matrices and vectors to collect the derivative of each component of the dependent variable with respect to each component of the independent variable. In general, the independent variable can be a scalar, a vector, or a matrix while the dependent variable can be any of these as well. Each different situation will lead to a different set of rules, or a separate calculus, using the broader sense of the term. Matrix notation serves as a convenient way to collect the many derivatives in an organized way.More complicated examples include the derivative of a scalar function with respect to a matrix, known as the gradient matrix, which collects the derivative with respect to each matrix element in the corresponding position in the resulting matrix. In that case the scalar must be a function of each of the independent variables in the matrix. As another example, if we have an n-vector of dependent variables, or functions, of m independent variables we might consider the derivative of the dependent vector with respect to the independent vector. The result could be collected in an m×n matrix consisting of all of the possible derivative combinations. There are, of course, a total of nine possibilities using scalars, vectors, and matrices. Notice that as we consider higher numbers of components in each of the independent and dependent variables we can be left with a very large number of possibilities.The six kinds of derivatives that can be most neatly organized in matrix form are collected in the following table.[1]Here, we have used the term "matrix" in its most general sense, recognizing that vectors and scalars are simply matrices with one column and then one row respectively. Moreover, we have used bold letters to indicate vectors and bold capital letters for matrices. This notation is used throughout.Notice that we could also talk about the derivative of a vector with respect to a matrix, or any of the other unfilled cells in our table. However, these derivatives are most naturally organized in a tensor of rank higher than 2, so that they do not fit neatly into a matrix. In the following three sections we will define each one of these derivatives and relate them to other branches of mathematics. See the layout conventions section for a more detailed table.The matrix derivative is a convenient notation for keeping track of partial derivatives for doing calculations. The Fréchet derivative is the standard way in the setting of functional analysis to take derivatives with respect to vectors. In the case that a matrix function of a matrix is Fréchet differentiable, the two derivatives will agree up to translation of notations. As is the case in general for partial derivatives, some formulae may extend under weaker analytic conditions than the existence of the derivative as approximating linear mapping.Matrix calculus is used for deriving optimal stochastic estimators, often involving the use of Lagrange multipliers. This includes the derivation of:The vector and matrix derivatives presented in the sections to follow take full advantage of matrix notation, using a single variable to represent a large number of variables. In what follows we will distinguish scalars, vectors and matrices by their typeface. We will let M(n,m) denote the space of real n×m matrices with n rows and m columns.  Such matrices will be denoted using bold capital letters: A, X, Y, etc.  An element of M(n,1), that is, a column vector, is denoted with a boldface lowercase letter: a, x, y, etc. An element of M(1,1) is a scalar, denoted with lowercase italic typeface: a, t, x, etc. XT denotes matrix transpose, tr(X) is the trace, and det(X) or |X| is the determinant. All functions are assumed to be of differentiability class C1 unless otherwise noted. Generally letters from the first half of the alphabet (a, b, c, …) will be used to denote constants, and from the second half (t, x, y, …) to denote variables.NOTE: As mentioned above, there are competing notations for laying out systems of partial derivatives in vectors and matrices, and no standard appears to be emerging yet.  The next two introductory sections use the numerator layout convention simply for the purposes of convenience, to avoid overly complicating the discussion.  The section after them discusses layout conventions in more detail.  It is important to realize the following:The tensor index notation with its Einstein summation convention is very similar to the matrix calculus, except one writes only a single component at a time.  It has the advantage that one can easily manipulate arbitrarily high rank tensors, whereas tensors of rank higher than two are quite unwieldy with matrix notation.  All of the work here can be done in this notation without use of the single-variable matrix notation. However, many problems in estimation theory and other areas of applied mathematics would result in too many indices to properly keep track of, pointing in favor of matrix calculus in those areas.  Also, Einstein notation can be very useful in proving the identities presented here (see section on differentiation) as an alternative to typical element notation, which can become cumbersome when the explicit sums are carried around. Note that a matrix can be considered a tensor of rank two.Because vectors are matrices with only one column, the simplest matrix derivatives are vector derivatives.The notations developed here can accommodate the usual operations of vector calculus by identifying the space M(n,1) of n-vectors with the Euclidean space Rn, and the scalar M(1,1) is identified with R. The corresponding concept from vector calculus is indicated at the end of each subsection.NOTE: The discussion in this section assumes the numerator layout convention for pedagogical purposes.  Some authors use different conventions.  The section on layout conventions discusses this issue in greater detail.  The identities given further down are presented in forms that can be used in conjunction with all common layout conventions.Example Simple examples of this include the velocity vector in Euclidean space, which is the tangent vector of the position vector (considered as a function of time). Also, the acceleration is the tangent vector of the velocity.In vector calculus, the gradient of a scalar field y in the space Rn (whose independent coordinates are the components of x) is the transpose of the derivative of a scalar by a vector. In physics, the electric field is the vector gradient of the electric potential.The directional derivative of a scalar function f(x) of the space vector x in the direction of the unit vector u is defined using the gradient as follows.Each of the previous two cases can be considered as an application of the derivative of a vector with respect to a vector, using a vector of size one appropriately. Similarly we will find that the derivatives involving matrices will reduce to derivatives involving vectors in a corresponding way.In vector calculus, the derivative of a vector function y with respect to a vector x whose components represent a space is known as the pushforward (or differential), or the Jacobian matrix.There are two types of derivatives with matrices that can be organized into a matrix of the same size. These are the derivative of a matrix by a scalar and the derivative of a scalar by a matrix. These can be useful in minimization problems found in many areas of applied mathematics and have adopted the names tangent matrix and gradient matrix respectively after their analogs for vectors.NOTE: The discussion in this section assumes the numerator layout convention for pedagogical purposes.  Some authors use different conventions.  The section on layout conventions discusses this issue in greater detail.  The identities given further down are presented in forms that can be used in conjunction with all common layout conventions.The derivative of a matrix function Y by a scalar x is known as the tangent matrix and is given (in numerator layout notation) byThe derivative of a scalar y function of a p×q matrix X of independent variables, with respect to the matrix X, is given (in numerator layout notation) byImportant examples of scalar functions of matrices include the trace of a matrix and the determinant.In analog with vector calculus this derivative is often written as the following.Also in analog with vector calculus, the directional derivative of a scalar f(X) of a matrix X in the direction of matrix Y is given byIt is the gradient matrix, in particular, that finds many uses in minimization problems in estimation theory, particularly in the derivation of the Kalman filter algorithm, which is of great importance in the field.The three types of derivatives that have not been considered are those involving vectors-by-matrices, matrices-by-vectors, and matrices-by-matrices. These are not as widely considered and a notation is not widely agreed upon. As for vectors, the other two types of higher matrix derivatives can be seen as applications of the derivative of a matrix by a matrix by using a matrix with one column in the correct place. For this reason, in this subsection we consider only how one can write the derivative of a matrix by another matrix.Note that this definition encompasses all of the preceding definitions as special cases.According to Jan R. Magnus and Heinz Neudecker, the following notations are both unsuitable, as the determinant of the second resulting matrix would have "no interpretation" and "a useful chain rule does not exist" if these notations are being used:[2]The Jacobian matrix, according to Magnus and Neudecker,[2] isThis section discusses the similarities and differences between notational conventions that are used in the various fields that take advantage of matrix calculus. Although there are largely two consistent conventions, some authors find it convenient to mix the two conventions in forms that are discussed below. After this section equations will be listed in both competing forms separately.Keep in mind that various authors use different combinations of numerator and denominator layouts for different types of derivatives, and there is no guarantee that an author will consistently use either numerator or denominator layout for all types.  Match up the formulas below with those quoted in the source to determine the layout used for that particular type of derivative, but be careful not to assume that derivatives of other types necessarily follow the same kind of layout.When taking derivatives with an aggregate (vector or matrix) denominator in order to find a maximum or minimum of the aggregate, it should be kept in mind that using numerator layout will produce results that are transposed with respect to the aggregate.  For example, in attempting to find the maximum likelihood estimate of a multivariate normal distribution using matrix calculus, if the domain is a kx1 column vector, then the result using the numerator layout will be in the form of a 1xk row vector. Thus, either the results should be transposed at the end or the denominator layout (or mixed layout) should be used.(denominator layout) size-m row vector (denominator layout) size-n column vector (denominator layout) n×m matrix (denominator layout) p×q matrix The results of operations will be transposed when switching between numerator-layout and denominator-layout notation.Using numerator-layout notation, we have:[1]The following definitions are only provided in numerator-layout notation:Using denominator-layout notation, we have:[3]As noted above, in general, the results of operations will be transposed when switching between numerator-layout and denominator-layout notation.To help make sense of all the identities below, keep in mind the most important rules: the chain rule, product rule and sum rule.  The sum rule applies universally, and the product rule applies in most of the cases below, provided that the order of matrix products is maintained, since matrix products are not commutative.  The chain rule applies in some of the cases, but unfortunately does not apply in matrix-by-scalar derivatives or scalar-by-matrix derivatives (in the latter case, mostly involving the trace operator applied to matrices).  In the latter case, the product rule can't quite be applied directly, either, but the equivalent can be done with a bit more work using the differential identities.This is presented first because all of the operations that apply to vector-by-vector differentiation apply directly to vector-by-scalar or scalar-by-vector differentiation simply by reducing the appropriate vector in the numerator or denominator to a scalar.The fundamental identities are placed above the thick black line.Note that exact equivalents of the scalar product rule and chain rule do not exist when applied to matrix-valued functions of matrices.  However, the product rule of this sort does apply to the differential form (see below), and this is the way to derive many of the identities below involving the trace function, combined with the fact that the trace function allows transposing and cyclic permutation, i.e.:Therefore,(For the last step, see the 'Conversion from differential to derivative form' section.)i.e. mixed layout if denominator layout for X is being used.Further see Derivative of the exponential map.It is often easier to work in differential form and then convert back to normal derivatives.  This only works well using the numerator layout. In these rules, "a" is a scalar.To convert to normal derivative form, first convert it to one of the following canonical forms, and then use these identities:Matrix differential calculus is used in statistics, particularly for the statistical analysis of multivariate distributions, especially the multivariate normal distribution and other elliptical distributions.[10][11][12]It is used in regression analysis to compute, for example, the ordinary least squares regression formula for the case of multiple explanatory variables.
Field (mathematics)
In mathematics, a field  is a set on which addition, subtraction, multiplication, and division are defined, and behave as the corresponding operations on  rational and real numbers do. A field is thus a fundamental algebraic structure, which is widely used in algebra, number theory and many other areas of mathematics.The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle can not be done with a compass and straightedge. Moreover, it shows that quintic equations are algebraically unsolvable.Fields serve as foundational notions in several mathematical domains. This includes different branches of analysis, which are based on fields with additional structure. Basic theorems in analysis hinge on the structural properties of the field of real numbers. Most importantly for algebraic purposes, any field may be used as the scalars for a vector space, which is the standard general context for linear algebra. Number fields, the siblings of the field of rational numbers, are studied in depth in number theory. Function fields can help describe properties of geometric objects.Informally, a field is a set, along with two operations defined on that set: an addition operation written as a + b, and a multiplication operation written as a ⋅ b, both of which behave similarly as they behave for rational numbers and real numbers, including the existence of an additive inverse −a for all elements a, and of a multiplicative inverse b−1 for every nonzero element b. This allows us to consider also the so-called inverse operations of subtraction a − b, and division a / b, via defining:Formally, a field is a set F together with two operations called addition and multiplication.[1] An operation is a mapping that associates an element of the set to every pair of its elements. The result of the addition of a and b is called the sum of a and b and denoted a + b. Similarly, the result of the multiplication of a and b is called the product of a and b, and denoted ab or a⋅b. These operations are required to satisfy the following properties, referred to as field axioms. In these axioms,  a, b and c are arbitrary elements of the field F.This may be summarized by saying: a field has two operations, called addition and multiplication; it is an abelian group under the addition, with 0 as additive identity; the nonzero elements are an abelian group under the multiplication, with 1 as multiplicative identity; the multiplication is distributive over the addition.Fields can also be defined in different, but equivalent ways. One can alternatively define a field by four binary operations (add, subtract, multiply, divide), and their required properties. Division by zero is, by definition, excluded.[2] In order to avoid existential quantifiers, fields can be defined by two binary operations (addition and multiplication), two unary operations (yielding the additive and multiplicative inverses, respectively), and two nullary operations (the constants 0 and 1). These operations are then subject to the conditions above. Avoiding existential quantifiers is important in constructive mathematics and computing.[3] One may equivalently define a field by the same two binary operations, one unary operation, (the multiplicative inverse} and two constants 1 and –1, since 0 = 1 + (–1) and –a = (–1) a.Rational numbers have been widely used a long time before the elaboration of the concept of field.They are numbers that can be written as fractionsa/b, where a and b are integers, and b ≠ 0. The additive inverse of such a fraction is −a/b, and the multiplicative inverse (provided that a ≠ 0) is b/a, which can be seen as follows:The abstractly required field axioms reduce to standard properties of rational numbers. For example, the law of distributivity can be proven as follows:[4]The real numbers R, with the usual operations of addition and multiplication, also form a field. The complex numbers C consist of expressionswhere i is the imaginary unit, i.e., a (non-real) number satisfying i2 = −1.Addition and multiplication of real numbers are defined in such a way that expressions of this type satisfy all field axioms and thus hold for C. For example, the distributive law enforcesIt is immediate that this is again an expression of the above type, and so the complex numbers form a field. Complex numbers can be geometrically represented as points in the plane, with Cartesian coordinates given by the real numbers of their describing expression, or as the arrows from the origin to these points, specified by their length and an angle enclosed with some distinct direction. Addition then corresponds to combining the arrows to the intuitive parallelogram (adding the Cartesian coordinates), and the multiplication is –less intuitively– combining rotating and scaling of the arrows (adding the angles and multiplying the lengths). The fields of real and complex numbers are used throughout mathematics, physics, engineering, statistics, and many other scientific disciplines.In addition to familiar number systems such as the rationals, there are other, less immediate examples of fields. The following example is a field consisting of four elements called O, I, A, and B. The notation is chosen such that O plays the role of the additive identity element (denoted 0 in the axioms above), and I is the multiplicative identity (denoted 1 in the axioms above). The field axioms can be verified by using some more field theory, or by direct computation. For example,This field is called a finite field with four elements, and is denoted F4 or GF(4).[6] The subset consisting of O and I (highlighted in red in the tables at the right) is also a field, known as the binary field F2 or GF(2). In the context of computer science and Boolean algebra, O and I are often denoted respectively by false and true, the addition is then denoted XOR (exclusive or), and the multiplication is denoted AND. In other words, the structure of the binary field is the basic structure that allows computing with bits.In this section, F denotes an arbitrary field and a and b are arbitrary elements of F.One has a · 0 = 0 and −a = (−1) · a.[7] In particular, one may deduce the additive inverse of every element as soon as one knows –1.If ab = 0 then a or b must be 0. Indeed, if a ≠ 0, then 0 = a–1⋅0 = a–1(ab) = (a–1a)b = b. This means that every field is an integral domain.The axioms of a field F imply that it is an abelian group under addition. This group is called the additive group of the field, and is sometimes denoted by (F, +) when denoting it simply as F could be confusing.Similarly, the nonzero elements of F form an abelian group under multiplication, called the multiplicative group, and denoted by (F \ {0}, ·) or just F \ {0} or F*. A field may thus be defined as set F equipped with two operations denoted as an addition and a multiplication such that F is an abelian group under addition, F \ {0} is an abelian group under multiplication (where 0 is the identity element of the addition), and multiplication is distributive over addition.[nb 1] Some elementary statements about fields can therefore be obtained by applying general facts of groups. For example, the additive and multiplicative inverses −a and a−1 are uniquely determined by a.The requirement 1 ≠ 0 follows, because 1 is the identity element of a group that does not contain 0.[8] Thus, the trivial ring, consisting of a single element, is not a field.Every finite subgroup of the multiplicative group of a field is cyclic (see Root of unity § Cyclic groups).In addition to the multiplication of two elements of F, it is possible to define the product n ⋅ a of an arbitrary element a of F by a positive integer n to be the n-fold sumIf there is no positive integer such thatthen F is said to have characteristic 0.[9] For example, Q has characteristic 0 since no positive integer n is zero. Otherwise, if there is a positive integer n satisfying this equation, the smallest such positive integer can be shown to be a prime number. It is usually denoted by p and the field is said to have characteristic p then.For example, the field F4 has characteristic 2 since (in the notation of the above addition table) I + I = O.If F has characteristic p, then p ⋅ a = 0 for all a in F. This implies that since all other binomial coefficients appearing in the binomial formula are divisible by p. Here, ap := a ⋅ a ⋅ ... ⋅ a (p factors) is the p-th power, i.e., the p-fold product of the element a. Therefore, the Frobenius mapis compatible with the addition in F (and also with the multiplication), and is therefore a field homomorphism.[10] The existence of this homomorphism makes fields in characteristic p quite different from fields of characteristic 0.A subfield E of a field F is a subset of F that is a field with respect to the field operations of F. Equivalently E is a subset of F that contains 1, and is closed under addition, multiplication, additive inverse and multiplicative inverse of a nonzero element. This means that 1 ∊ E, that for all a, b ∊ E both a + b and a · b are in E, and that for all a ≠ 0 in E, both –a and 1/a are in E.Field homomorphisms are maps f: E → F between two fields such that f(e1 + e2) = f(e1) + f(e2), f(e1e2) = f(e1)f(e2), and f(1E) = 1F, where e1 and e2 are arbitrary elements of E. All field homomorphisms are injective.[11] If f is also surjective, it is called an isomorphism (or the fields E and F are called isomorphic).A field is called a prime field if it has no proper (i.e., strictly smaller) subfields. Any field F contains a prime field. If the characteristic of F is p (a prime number), the prime field is isomorphic to the finite field Fp introduced below. Otherwise the prime field is isomorphic to Q.[12]Finite fields (also called Galois fields) are fields with finitely many elements, whose number is also referred to as the order of the field. The above introductory example F4 is a field with four elements. Its subfield F2 is the smallest field, because by definition a field has at least two distinct elements 1 ≠ 0.The simplest finite fields, with prime order, are most directly accessible using modular arithmetic. For a fixed positive integer n, arithmetic "modulo n" means to work with the numbersThe addition and multiplication on this set are done by performing the operation in question in the set Z of integers, dividing by n and taking the remainder as result. This construction yields a field precisely if n is a prime number. For example, taking the prime n = 2 results in the above-mentioned field F2. For n = 4 and more generally, for any composite number (i.e., any number n which can be expressed as a product n = r⋅s of two strictly smaller natural numbers), Z/nZ is not a field: the product of two non-zero elements is zero since r⋅s = 0 in Z/nZ, which, as was explained above, prevents Z/nZ from being a field. The field Z/pZ with p elements (p being prime) constructed in this way is usually denoted by  Fp.Every finite field F has q = pn elements, where p is prime and n ≥ 1. This statement holds since F may be viewed as a vector space over its prime field. The dimension of this vector space is necessarily finite, say n, which implies the asserted statement.[13]A field with q = pn elements can be constructed as the splitting field of the polynomial Such a splitting field is an extension of Fp in which the polynomial f has q zeros. This means f has as many zeros as possible since the degree of f is q. For q = 22 = 4, it can be checked case by case using the above multiplication table that all four elements of F4 satisfy the equation x4 = x, so they are zeros of f. By contrast, in F2, f has only two zeros (namely 0 and 1), so f does not split into linear factors in this smaller field. Elaborating further on basic field-theoretic notions, it can be shown that two finite fields with the same order are isomorphic.[14] It is thus customary to speak of the finite field with q elements, denoted by Fq or GF(q).Historically, three algebraic disciplines led to the concept of a field: the question of solving polynomial equations, algebraic number theory, and algebraic geometry.[15] A first step towards the notion of a field was made in 1770 by Joseph-Louis Lagrange, who observed that permuting the zeros x1, x2, x3 of a cubic polynomial in the expression(with ω being a third root of unity) only yields two values. This way, Lagrange conceptually explained the classical solution method of Scipione del Ferro and François Viète, which proceeds by reducing a cubic equation for an unknown x to an quadratic equation for x3.[16] Together with a similar observation for equations of degree 4, Lagrange thus linked what eventually became the concept of fields and the concept of groups.[17] Vandermonde, also in 1770, and to a fuller extent, Carl Friedrich Gauss, in his Disquisitiones Arithmeticae (1801), studied the equationfor a prime p and, again using modern language, the resulting cyclic Galois group. Gauss deduced that a regular p-gon can be constructed if p = 22k + 1. Building on Lagrange's work, Paolo Ruffini claimed (1799) that quintic equations (polynomial equations of degree 5) cannot be solved algebraically, however his arguments were flawed. These gaps were filled by Niels Henrik Abel in 1824.[18] Évariste Galois, in 1832, devised necessary and sufficient criteria for a polynomial equation to be algebraically solvable, thus establishing in effect what is known as Galois theory today. Both Abel and Galois worked with what is today called an algebraic number field, but conceived neither an explicit notion of a field, nor of a group.In 1871 Richard Dedekind introduced, for a set of real or complex numbers that is closed under the four arithmetic operations, the German word Körper, which means "body" or "corpus" (to suggest an organically closed entity). The English term "field" was introduced by Moore (1893).[19]By a field we will mean every infinite system of real or complex numbers so closed in itself and perfect that addition, subtraction, multiplication, and division of any two of these numbers again yields a number of the system.In 1881 Leopold Kronecker defined what he called a domain of rationality, which is a field of rational fractions in modern terms. Kronecker's notion did not cover the field of all algebraic numbers (which is a field in Dedekind's sense), but on the other hand was more abstract than Dedekind's in that it made no specific assumption on the nature of the elements of a field. Kronecker interpreted a field such as Q(π) abstractly as the rational function field Q(X). Prior to this, examples of transcendental numbers were known since Joseph Liouville's work in 1844, until Charles Hermite (1873) and Ferdinand von Lindemann (1882) proved the transcendence of e and π, respectively.[21]The first clear definition of an abstract field is due to Weber (1893).[22] In particular, Heinrich Martin Weber's notion included the field Fp. Giuseppe Veronese (1891) studied the field of formal power series, which led Hensel (1904) to introduce the field of p-adic numbers. Steinitz (1910) synthesized the knowledge of abstract field theory accumulated so far. He axiomatically studied the properties of fields and defined many important field-theoretic concepts. The majority of the theorems mentioned in the sections Galois theory, Constructing fields and Elementary notions can be found in Steinitz's work. Artin & Schreier (1927) linked the notion of orderings in a field, and thus the area of analysis, to purely algebraic properties.[23] Emil Artin redeveloped Galois theory from 1928 through 1942, eliminating the dependency on the primitive element theorem.A commutative ring is a set, equipped with an addition and multiplication operation, satisfying all the axioms of a field, except for the existence of multiplicative inverses a−1.[24] For example, the integers Z form a commutative ring, but not a field: the reciprocal of an integer n is not itself an integer, unless n = ±1.In the hierarchy of algebraic structures fields can be characterized as the commutative rings R in which every nonzero element is a unit (which means every element is invertible). Similarly, fields are the commutative rings with precisely two distinct ideals, (0) and R. Fields are also precisely the commutative rings in which (0) is the only prime ideal.Given a commutative ring R, there are two ways to construct a field related to R, i.e., two ways of modifying R such that all nonzero elements become invertible: forming the field of fractions, and forming residue fields. The field of fractions of Z is Q, the rationals, while the residue fields of Z are the finite fields Fp.Given an integral domain R, its field of fractions Q(R) is built with the fractions of two elements of R exactly as Q is constructed from the integers. More precisely, the elements of Q(R) are the fractions a/b where a and b  are in R, and  b ≠ 0. Two fractions a/b and  c/d are equal if and only if ad = bc. The operation on the fractions work exactly as for rational numbers. For example, It is straightforward to show that, if the ring is an integral domain, the set of the fractions form a field.[25]The field F(x) of the rational fractions over a field (or an integral domain) F is the field of fractions of the polynomial ring F[x]. The field F((x)) of Laurent seriesover a field F is the field of fractions of the ring F[[x]] of formal power series (in which k ≥ 0). Since any Laurent series is a fraction of a power series divided by a power of x (as opposed to an arbitrary power series), the representation of fractions is less important in this situation, though.In addition the field of fractions, which embeds R injectively into a field, a field can be obtained from a commutative ring R by means of a surjective map onto a field F. Any field obtained in this way is a quotient R / m, where m is a maximal ideal of R. If R has only one maximal ideal m, this field is called the residue field of R.[26]The ideal generated by a single polynomial f in the polynomial ring R = E[X] (over a field E) is maximal if and only if f is irreducible in E, i.e., if f can not be expressed as the product of two polynomials in E[X] of smaller degree. This yields a fieldThis field F contains an element x (namely the residue class of X) which satisfies the equationFor example, C is obtained from R by adjoining the imaginary unit symbol i, which satisfies f(i) = 0, where f(X) = X2 + 1. Moreover, f is irreducible over R, which implies that the map that sends a polynomial f(X) ∊ R[X] to f(i) yields an isomorphismFields can be constructed inside a given bigger container field. Suppose given a field E, and a field F containing E as a subfield. For any element x of F, there is a smallest subfield of F containing E and x, called the subfield of F generated by x and denoted E(x).[27] The passage from E to E(x) is referred to by adjoining an element to E. More generally, for a subset S ⊂ F, there is a minimal subfield of F containing E and S, denoted by E(S).The compositum of two subfields E and E'  of some field F is the smallest subfield of F containing both E and E'. The compositum can be used to construct the biggest subfield of F satisfying a certain property, for example the biggest subfield of F, which is, in the language introduced below, algebraic over E.[nb 2]The notion of a subfield E ⊂ F can also be regarded from the opposite point of view, by referring to F being a field extension (or just extension) of E, denoted byand read "F over E".A basic datum of a field extension is its degree [F : E], i.e., the dimension of F as an E-vector space. It satisfies the formula[28]Extensions whose degree is finite are referred to as finite extensions. The extensions C / R and F4 / F2 are of degree 2, whereas R / Q is an infinite extension.with en, ..., e0 in E, and en ≠ 0. For example, the imaginary unit i in  C is algebraic over R, and even over Q, since it satisfies the equationA field extension in which every element of F is algebraic over E is called an algebraic extension. Any finite extension is necessarily algebraic, as can be deduced from the above multiplicativity formula.[29]The subfield E(x) generated by an element x, as above, is an algebraic extension of E if and only if x is an algebraic element. That is to say, if x is algebraic, all other elements of E(x) are necessarily algebraic as well. Moreover, the degree of the extension E(x) / E, i.e., the dimension of E(x) as an E-vector space, equals the minimal degree n such that there is a polynomial equation involving x, as above. If this degree is n, then the elements of E(x) have the form For example, the field Q(i) of Gaussian rationals is the subfield of C consisting of all numbers of the form a + bi where both a and b are rational numbers: summands of the form i2 (and similarly for higher exponents) don't have to be considered here, since a + bi + ci2 can be simplified to a − c + bi.The above-mentioned field of rational fractions E(X), where X is an indeterminate, is not an algebraic extension of E since there is no polynomial equation with coefficients in E whose zero is X. Elements, such as X, which are not algebraic are called transcendental. Informally speaking, the indeterminate X and its powers do not interact with elements of E. A similar construction can be carried out with a set of indeterminates, instead of just one.Once again, the field extension E(x) / E discussed above is a key example: if x is not algebraic (i.e., x is not a root of a polynomial with coefficients in E), then E(x) is isomorphic to E(X). This isomorphism is obtained by substituting x to X in rational fractions.A subset S of a field F is a transcendence basis if it is algebraically independent (don't satisfy any polynomial relations) over E and if F is an algebraic extension of E(S). Any field extension F / E has a transcendence basis.[30] Thus, field extensions can be split into ones of the form E(S) / E (purely transcendental extensions) and algebraic extensions.A field is algebraically closed if it does not have any strictly bigger algebraic extensions or, equivalently, if any polynomial equationhas a solution x ∊ F.[31] By the fundamental theorem of algebra, C is algebraically closed, i.e., any polynomial equation with complex coefficients has a complex solution. The rational and the real numbers are not algebraically closed since the equationdoes not have any rational or real solution. A field containing F is called an algebraic closure of F if it is algebraic over F (roughly speaking, not too big compared to F) and is algebraically closed (big enough to contain solutions of all polynomial equations).By the above, C is an algebraic closure of R. The situation that the algebraic closure is a finite extension of the field F is quite special: by the Artin-Schreier theorem, the degree of this extension is necessarily 2, and F is elementarily equivalent to R. Such fields are also known as real closed fields.Any field F has an algebraic closure, which is moreover unique up to (non-unique) isomorphism. It is commonly referred to as the algebraic closure and denoted F. For example, the algebraic closure Q of Q is called the field of algebraic numbers. The field F is usually rather implicit since its construction requires the ultrafilter lemma, a set-theoretic axiom that is weaker than the axiom of choice.[32] In this regard, the algebraic closure of Fq, is exceptionally simple. It is the union of the finite fields containing Fq (the ones of order qn). For any algebraically closed field F of characteristic 0, the algebraic closure of the field F((t)) of Laurent series is the field of Puiseux series, obtained by adjoining roots of t.[33]Since fields are ubiquitous in mathematics and beyond, several refinements of the concept have been adapted to the needs of particular mathematical areas.A field F is called an ordered field if any two elements can be compared, so that x + y ≥ 0 and xy ≥ 0 whenever x ≥ 0 and y ≥ 0. For example, the reals form an ordered field, with the usual ordering ≥. The Artin-Schreier theorem states that a field can be ordered if and only if it is a formally real field, which means that any quadratic equationonly has the solution x1 = x2 = ... = xn = 0.[34] The set of all possible orders on a fixed field F is isomorphic to the set of ring homomorphisms from the Witt ring W(F) of quadratic forms over F, to Z.[35]An Archimedean field is an ordered field such that for each element there exists a finite expressionwhose value is greater than that element, that is, there are no infinite elements.  Equivalently, the field contains no infinitesimals (elements smaller than all rational numbers); or, yet equivalent, the field is isomorphic to a subfield of R.An ordered field is Dedekind-complete if all upper bounds, lower bounds (see Dedekind cut) and limits, which should exist, do exist. More formally, each bounded subset of F is required to have a least upper bound. Any complete field is necessarily Archimedean,[36] since in any non-Archimedean field there is neither a greatest infinitesimal nor a least positive rational, whence the sequence 1/2, 1/3, 1/4, …, every element of which is greater than every infinitesimal, has no limit.Since every proper subfield of the reals also contains such gaps, R is the unique complete ordered field, up to isomorphism.[37] Several foundational results in calculus follow directly from this characterization of the reals.The hyperreals R* form an ordered field that is not Archimedean. It is an extension of the reals obtained by including infinite and infinitesimal numbers. These are larger, respectively smaller than any real number. The hyperreals form the foundational basis of non-standard analysis.Another refinement of the notion of a field is a topological field, in which the set F is a topological space, such that all operations of the field (addition, multiplication, the maps a ↦ −a and a ↦ a−1) are continuous maps with respect to the topology of the space.[38]The topology of all the fields discussed below is induced from a metric, i.e., a functionthat measures a distance between any two elements of F.The completion of F is another field in which, informally speaking, the "gaps" in the original field F are filled, if there are any. For example, any irrational number x, such as x = √2, is a "gap" in the rationals Q in the sense that it is a real number that can be approximated arbitrarily closely by rational numbers p/q, in the sense that distance of x and p/q given by the absolute value |x − p/q| is as small as desired.The following table lists some examples of this construction. The fourth column shows an example of a zero sequence, i.e., a sequence whose limit (for n → ∞) is zero.The field Qp is used in number theory and p-adic analysis. The algebraic closure Qp carries a unique norm extending the one on Qp, but is not complete. The completion of this algebraic closure, however, is algebraically closed. Because of its rough analogy to the complex numbers, it is called the field of complex p-adic numbers and is denoted by Cp.[39]The following topological fields are called local fields:[40][nb 3]These two types of local fields share some fundamental similarities. In this relation, the elements p ∈ Qp and t ∈ Fp((t)) (referred to as uniformizer) correspond to each other. The first manifestation of this is at an elementary level: the elements of both fields can be expressed as power series in the uniformizer, with coefficients in Fp. (However, since the addition in Qp is done using carrying, which is not the case in Fp((t)), these fields are not isomorphic.) The following facts show that this superficial similarity goes much deeper:Differential fields are fields equipped with a derivation, i.e., allow to take derivatives of elements in the field.[42] For example, the field R(X), together with the standard derivative of polynomials forms a differential field. These fields are central to differential Galois theory, a variant of Galois theory dealing with linear differential equations.Galois theory studies algebraic extensions of a field by studying the symmetry in the arithmetic operations of addition and multiplication. An important notion in this area are finite Galois extensions F / E, which are, by definition, those that are separable and normal. The primitive element theorem shows that finite separable extensions are necessarily simple, i.e., of the formwhere f is an irreducible polynomial (as above).[43] For such an extension, being normal and separable means that all zeros of f are contained in F and that f has only simple zeros. The latter condition is always satisfied if E has characteristic 0.The tensor product of fields is not usually a field. For example, a finite extension F / E of degree n is a Galois extension if and only if there is an isomorphism of F-algebrasThis fact is the beginning of Grothendieck's Galois theory, a far-reaching extension of Galois theory applicable to algebro-geometric objects.[46]Basic invariants of a field F include the characteristic and the transcendence degree of F over its prime field. The latter is defined as the maximal number of elements in F that are algebraically independent over the prime field. Two algebraically closed fields E and F are isomorphic precisely if these two data agree.[47] This implies that any two uncountable algebraically closed fields of the same cardinality and the same characteristic are isomorphic. For example, Qp, Cp and C are isomorphic (but not isomorphic as topological fields).In model theory, a branch of mathematical logic, two fields E and F are called elementarily equivalent if every mathematical statement that is true for E is also true for F and conversely. The mathematical statements in question are required to be first-order sentences (involving 0, 1, the addition and multiplication). A typical example isThe Lefschetz principle states that C is elementarily equivalent to any algebraically closed field F of characteristic zero. Moreover, any fixed statement φ holds in C if and only if it holds in any algebraically closed field of sufficiently high characteristic.[48]If U is an ultrafilter on a set I, and Fi is a field for every i in I, the ultraproduct of the Fi with respect to U is a field.[49] It is denoted bysince it behaves in several ways as a limit of the fields Fi: Łoś's theorem states that any first order statement that holds for all but finitely many Fi, also holds for the ultraproduct. Applied to the above sentence φ, this shows that there is an isomorphism[nb 4]The Ax–Kochen theorem mentioned above also follows from this and an isomorphism of the ultraproducts (in both cases over all primes p)In addition, model theory also studies the logical properties of various other types of fields, such as real closed fields or exponential fields (which are equipped with an exponential function  exp : F → Fx).[50]For fields that are not algebraically closed (or not separably closed), the absolute Galois group Gal(F) is fundamentally important: extending the case of finite Galois extensions outlined above, this group governs all finite separable extensions of F. By elementary means, the group Gal(Fq) can be shown to be the Prüfer group, the profinite completion of Z. This statement subsumes the fact that the only algebraic extensions of Gal(Fq) are the fields Gal(Fqn) for n > 0, and that the Galois groups of these finite extensions are given byA description in terms of generators and relations is also known for the Galois groups of p-adic number fields (finite extensions of Qp).[51]Representations of Galois groups and of related groups such as the Weil group are fundamental in many branches of arithmetic, such as the Langlands program. The cohomological study of such representations is done using Galois cohomology.[52] For example, the Brauer group, which is classically defined as the group of central simple F-algebras, can be reinterpreted as a Galois cohomology group, namelyMilnor K-theory is defined asThe norm residue isomorphism theorem, proved around 2000 by Vladimir Voevodsky, relates this to Galois cohomology by means of an isomorphismAlgebraic K-theory is related to the group of invertible matrices with coefficients the given field. For example, the process of taking the determinant of an invertible matrix leads to an isomorphism K1(F) = F×. Matsumoto's theorem shows that K2(F) agrees with K2M(F). In higher degrees, K-theory diverges from Milnor K-theory and remains hard to compute in general.If a ≠ 0, then the equation has a unique solution x in F, namely x = b/a. This observation, which is an immediate consequence of the definition of a field, is the essential ingredient used to show that any vector space has a basis.[53] Roughly speaking, this allows choosing a coordinate system in any vector space, which is of central importance in linear algebra both from a theoretical point of view, and also for practical applications.Modules (the analogue of vector spaces) over most rings, including the ring Z of integers, have a more complicated structure. A particular situation arises when a ring R is a vector space over a field F in its own right. Such rings are called F-algebras and are studied in depth in the area of commutative algebra. For example, Noether normalization asserts that any finitely generated F-algebra is closely related to (more precisely, finitely generated as a module over) a polynomial ring F[x1, ..., xn].[54]A widely applied cryptographic routine uses the fact that discrete exponentiation, i.e., computingin a (large) finite field Fq can be performed much more efficiently than the discrete logarithm, which is the inverse operation, i.e., determining the solution n to an equationIn elliptic curve cryptography, the multiplication in a finite field is replaced by the operation of adding points on an elliptic curve, i.e., the solutions of an equation of the formFinite fields are also used in coding theory and combinatorics.Functions on a suitable topological space X into a field k can be added and multiplied pointwise, e.g., the product of two functions is defined by the product of their values within the domain:This makes these functions a k-commutative algebra.For having a field of functions, one must consider algebras of functions that are integral domains. In this case the ratios of two functions, i.e., expressions of the formform a field, called field of functions.This occurs in two main cases. When X is a complex manifold X. In this case, one considers the algebra of holomorphic functions, i.e., complex differentiable functions. Their ratios form the field of  meromorphic functions on X.The function field of an algebraic variety X (a geometric object defined as the common zeros of polynomial equations) consists of ratios of regular functions, i.e., ratios of polynomial functions on the variety. The function field of the n-dimensional space over a field k is k(x1, ..., xn), i.e., the field consisting of ratios of polynomials in n indeterminates. The function field of X is the same as the one of any open dense subvariety. In other words, the function field is insensitive to replacing X by a (slightly) smaller subvariety.The function field is invariant under isomorphism and birational equivalence of varieties. It is therefore an important tool for the study of abstract algebraic varieties and for the classification of algebraic varieties. For example, the dimension, which equals the transcendence degree of k(X), is invariant under birational equivalence.[55] For curves (i.e., the dimension is one), the function field k(X) is very close to X: if X is smooth and proper (the analogue of being compact), X can be reconstructed, up to isomorphism, from its field of functions.[nb 5] In higher dimension the function field remembers less, but still decisive information about X. The study of function fields and their geometric meaning in higher dimensions is referred to as birational geometry. The minimal model program attempts to identify the simplest (in a certain precise sense) algebraic varieties with a prescribed function field.Global fields are in the limelight in algebraic number theory and arithmetic geometry.They are, by definition, number fields (finite extensions of Q) or function fields over Fq (finite extensions of Fq(t)). As for local fields, these two types of fields share several similar features, even though they are of characteristic 0 and positive characteristic, respectively. This function field analogy can help to shape mathematical expectations, often first by understanding questions about function fields, and later treating the number field case. The latter is often more difficult. For example, the Riemann hypothesis concerning the zeros of the Riemann zeta function (open as of 2017) can be regarded as being parallel to the Weil conjectures (proven in 1974 by Pierre Deligne).Cyclotomic fields are among the most intensely studied number fields. They are of the form Q(ζn), where ζn is a primitive n-th root of unity, i.e., a complex number satisfying ζn = 1 and ζm ≠ 1 for all m < n.[56] For n being a regular prime, Kummer used cyclotomic fields to prove Fermat's last theorem, which asserts the non-existence of rational nonzero solutions to the equationLocal fields are completions of global fields. Ostrowski's theorem asserts that the only completions of Q, a global field, are the local fields Qp and R. Studying arithmetic questions in global fields may sometimes be done by looking at the corresponding questions locally. This technique is called the local-global principle. For example, the Hasse–Minkowski theorem reduces the problem of finding rational solutions of quadratic equations to solving these equations in R and Qp, whose solutions can easily be described.[57]Unlike for local fields, the Galois groups of global fields are not known. Inverse Galois theory studies the (unsolved) problem whether any finite group is the Galois group Gal(F/Q) for some number field F.[58] Class field theory describes the abelian extensions, i.e., ones with abelian Galois group, or equivalently the abelianized Galois groups of global fields. A classical statement, the Kronecker–Weber theorem, describes the maximal abelian Qab extension of Q: it is the fieldIn addition to the additional structure that fields may enjoy, fields admit various other related notions. Since in any field 0 ≠ 1, any field has at least two elements. Nonetheless, there is a concept of field with one element, which is suggested to be a limit of the finite fields Fp, as p tends to 1.[59] In addition to division rings, there are various other weaker algebraic structures related to fields such as quasifields, near-fields and semifields.There are also proper classes with field structure, which are sometimes called Fields, with a capital F. The surreal numbers form a Field containing the reals, and would be a field except for the fact that they are a proper class, not a set. The nimbers, a concept from game theory form a Field.[60]Dropping one or several axioms in the definition of a field leads to other algebraic structures. As was mentioned above, commutative rings satisfy all axioms of fields, except for multiplicative inverses. Dropping instead the condition that multiplication is commutative leads to the concept of a division ring or skew field.[nb 6] The only division rings that are finite-dimensional R-vector spaces are R itself, C (which is a field), the quaternions H (in which multiplication is non-commutative), and the octonions O (in which multiplication is neither commutative nor associative). This fact was proved using methods of algebraic topology in 1958 by Michel Kervaire, Raoul Bott, and John Milnor.[61] The non-existence of an odd-dimensional division algebra is more classical. It can be deduced from the hairy ball theorem illustrated at the right.[citation needed]
Jordan normal form
In linear algebra, a Jordan normal form (often called Jordan canonical form)[1]of a linear operator on a finite-dimensional vector space is an upper triangular matrix of a particular form called a Jordan matrix, representing the operator with respect to some basis. Such a matrix has each non-zero off-diagonal entry equal to 1, immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them.Let V be a vector space over a field K. Then a basis with respect to which the matrix has the required form exists if and only if all eigenvalues of the matrix lie in K, or equivalently if the characteristic polynomial of the operator splits into linear factors over K. This condition is always satisfied if K is algebraically closed (for instance, if it is the field of complex numbers). The diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue.[2][3][4]If the operator is originally given by a square matrix M, then its Jordan normal form is also called the Jordan normal form of M. Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given M is not entirely unique, as it is a block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.[2][3][4]The Jordan–Chevalley decomposition is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the Jordan normal form.[5][6][7]The Jordan normal form is named after Camille Jordan, who first stated the Jordan decomposition theorem in 1870.[8]Some textbooks have the ones on the subdiagonal, i.e., immediately below the main diagonal instead of on the superdiagonal.  The eigenvalues are still on the main diagonal.[9][10]An n × n matrix A is diagonalizable if and only if the sum of the dimensions of the eigenspaces is n. Or, equivalently, if and only if A has n linearly independent eigenvectors. Not all matrices are diagonalizable. Consider the following matrix:Including multiplicity, the eigenvalues of A are λ = 1, 2, 4, 4. The dimension of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so A is not diagonalizable. However, there is an invertible matrix P such that A = PJP−1, whereThe matrix J is almost diagonal. This is the Jordan normal form of A. The section Example below fills in the details of the computation.In general, a square complex matrix A is similar to a block diagonal matrixwhere each block Ji is a square matrix of the formSo there exists an invertible matrix P such that P−1AP = J is such that the only non-zero entries of J are on the diagonal and the superdiagonal. J is called the Jordan normal form of A. Each Ji is called a Jordan block of A. In a given Jordan block, every entry on the superdiagonal is 1.Assuming this result, we can deduce the following properties:Consider the matrix A from the example in the previous section. The Jordan normal form is obtained by some similarity transformation P−1AP = J, i.e.,Let P have column vectors pi, i = 1, ..., 4, thenWe see thatThus, given an eigenvalue λ, its corresponding Jordan block gives rise to a Jordan chain. The generator, or lead vector, say pr, of the chain is a generalized eigenvector such that (A − λ I)rpr = 0, where r is the size of the Jordan block. The vector p1 =  (A − λ I)r−1pr is an eigenvector corresponding to λ. In general, pi is a preimage of pi−1 under A − λ I. So the lead vector generates the chain via multiplication by (A − λ I).[12][13]Therefore, the statement that every square matrix A can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of A.We give a proof by induction that any complex-valued matrix A may be put in Jordan normal form. The 1 × 1 case is trivial. Let A be an n × n matrix. Take any eigenvalue λ of A. The range of A − λ I, denoted by Ran(A − λ I), is an invariant subspace of A. Also, since λ is an eigenvalue of A, the dimension of Ran(A − λ I), r, is strictly less than n. Let A'  denote the restriction of A to Ran(A − λ I), By inductive hypothesis, there exists a basis {p1, ..., pr} such that A' , expressed with respect to this basis, is in Jordan normal form.Next consider the subspace Ker(A − λ I). Ifthe desired result follows immediately from the rank–nullity theorem. This would be the case, for example, if A was Hermitian.Otherwise, iflet the dimension of Q be s ≤ r. Each vector in Q is an eigenvector of A'  corresponding to eigenvalue λ. So the Jordan form of A'  must contain s Jordan chains corresponding to s linearly independent eigenvectors. So the basis {p1, ..., pr} must contain s vectors, say {pr−s+1, ..., pr}, that are lead vectors in these Jordan chains from the Jordan normal form of A'. We can "extend the chains" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(A − λ I).) Let qi be such thatClearly no non-trivial linear combination of the qi can lie in Ker(A − λ I). Furthermore, no non-trivial linear combination of the qi can be in Ran(A − λ I), for that would contradict the assumption that each pi is a lead vector in a Jordan chain. The set {qi}, being preimages of the linearly independent set {pi} under A − λ I, is also linearly independent.Finally, we can pick any linearly independent set {z1, ..., zt} that spansBy construction, the union of the three sets {p1, ..., pr}, {qr−s +1, ..., qr}, and  {z1, ..., zt} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of A. Finally, by the rank–nullity theorem, the cardinality of the union is n. In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of A, and this shows A can be put in Jordan normal form.It can be shown that the Jordan normal form of a given matrix A is unique up to the order of the Jordan blocks.Knowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of A. Assuming the algebraic multiplicity m(λ) of an eigenvalue λ is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (A − λ I)m(λ). To see this, suppose an n × n matrix A has only one eigenvalue λ. So m(λ) = n. The smallest integer k1 such thatis the size of the largest Jordan block in the Jordan form of A. (This number k1 is also called the index of λ. See discussion in a following section.) The rank ofis the number of Jordan blocks of size k1. Similarly, the rank ofis twice the number of Jordan blocks of size k1 plus the number of Jordan blocks of size k1−1. The general case is similar.This can be used to show the uniqueness of the Jordan form. Let J1 and J2 be two Jordan normal forms of A. Then J1 and J2 are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of J1 and J2. This proves the uniqueness part of the statement.This real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form complex conjugate pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.One can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.Using the Jordan normal form, direct calculation gives a spectral mapping theorem for the polynomial functional calculus: Let A be an n × n matrix with eigenvalues λ1, ..., λn, then for any polynomial p, p(A) has eigenvalues p(λ1), ..., p(λn).The Cayley–Hamilton theorem asserts that every matrix A satisfies its characteristic equation: if p is the characteristic polynomial of A, then p(A) = 0. This can be shown via direct calculation in the Jordan form, since any Jordan block for λ is annihilated by (X − λ)m where m is the multiplicity of the root λ of p, the sum of the sizes of the Jordan blocks for λ, and therefore no less than the size of the block in question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the splitting field of p; this field extension does not change the matrix p(A) in any way.The minimal polynomial P of a square matrix A is the unique monic polynomial of least degree, m, such that P(A) = 0. Alternatively, the set of polynomials that annihilate a given A form an ideal I in C[x], the principal ideal domain of polynomials with complex coefficients. The monic element that generates I is precisely P.Let λ1, ..., λq be the distinct eigenvalues of A, and si be the size of the largest Jordan block corresponding to λi. It is clear from the Jordan normal form that the minimal polynomial of A has degree Σsi.While the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of elementary divisors. The elementary divisors of a square matrix A are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial m are the elementary divisors of the largest degree corresponding to distinct eigenvalues.The degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, A is diagonalizable.The Jordan form of a n × n matrix A is block diagonal, and therefore gives a decomposition of the n dimensional Euclidean space into invariant subspaces of A. Every Jordan block Ji corresponds to an invariant subspace Xi. Symbolically, we putwhere each Xi is the span of the corresponding Jordan chain, and k is the number of Jordan chains.One can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue λi, the size of its largest corresponding Jordan block si is called the index of  λi and denoted by ν(λi). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace Yi byThis gives the decompositionwhere l is the number of distinct eigenvalues of A. Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where A is a multiple of the identity matrix we have k = n and l = 1.The projection onto Yi and along all the other Yj ( j ≠ i ) is called the spectral projection of A at λi and is usually denoted by P(λi ; A). Spectral projections are mutually orthogonal in the sense that P(λi ; A) P(λj ; A) = 0 if i ≠ j. Also they commute with A and their sum is the identity matrix. Replacing every λi in the Jordan matrix J by one and zeroising all other entries gives P(λi ; J), moreover if U J U−1 is the similarity transformation such that A = U J U−1 then P(λi ; A) = U P(λi ; J) U−1. They are not confined to finite dimensions. See below for their application to compact operators, and in holomorphic functional calculus for a more general discussion.Comparing the two decompositions, notice that, in general, l ≤ k. When A is normal, the subspaces Xi's in the first decomposition are one-dimensional and mutually orthogonal. This is the spectral theorem for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.It might be of interest here to note some properties of the index, ν(λ). More generally, for a complex number λ, its index can be defined as the least non-negative integer ν(λ) such thatSo ν(λ) > 0 if and only if λ is an eigenvalue of A. In the finite-dimensional case, ν(λ) ≤ the algebraic multiplicity of λ.Jordan reduction can be extended to any square matrix M whose entries lie in a field K.  The result states that any M can be written as a sum D + N where D is semisimple, N is nilpotent, and DN = ND. This is called the Jordan–Chevalley decomposition. Whenever K contains the eigenvalues of M, in particular when K is algebraically closed, the normal form can be expressed explicitly as the direct sum of Jordan blocks. Similar to the case when K is the complex numbers, knowing the dimensions of the kernels of (M − λI)k for 1 ≤ k ≤ m, where m is the algebraic multiplicity of the eigenvalue λ, allows one to determine the Jordan form of M. We may view the underlying vector space V as a K[x]-module by regarding the action of x on V as application of M and extending by K-linearity. Then the polynomials (x − λ)k are the elementary divisors of M, and the Jordan normal form is concerned with representing M in terms of blocks associated to the elementary divisors.The proof of the Jordan normal form is usually carried out as an application to the ring K[x] of the structure theorem for finitely generated modules over a principal ideal domain, of which it is a corollary.In a different direction, for compact operators on a Banach space, a result analogous to the Jordan normal form holds. One restricts to compact operators because every point x in the spectrum of a compact operator T, the only exception being when x is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.Let X be a Banach space, L(X) be the bounded operators on X, and σ(T) denote the spectrum of T ∈ L(X). The holomorphic functional calculus is defined as follows:Fix a bounded operator T. Consider the family Hol(T) of complex functions that is holomorphic on some open set G containing σ(T). Let Γ = {γi} be a finite collection of Jordan curves such that σ(T) lies in the inside of Γ, we define f(T) byThe open set G could vary with f and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar case. Although the integral makes sense for continuous f, we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that σ(T) lie in the inside of Γ ensures f(T) is well defined; it does not depend on the choice of Γ. The functional calculus is the mapping Φ from Hol(T) to L(X) given byWe will require the following properties of this functional calculus:In the finite-dimensional case, σ(T) = {λi} is a finite discrete set in the complex plane. Let ei be the function that is 1 in some open neighborhood of λi and 0 elsewhere. By property 3 of the functional calculus, the operatoris a projection. Moreoever, let νi be the index of λi andThe spectral mapping theorem tells ushas spectrum {0}. By property 1, f(T) can be directly computed in the Jordan form, and by inspection, we see that the operator f(T)ei(T) is the zero matrix.By property 3, f(T) ei(T) = ei(T) f(T). So ei(T) is precisely the projection ontothe subspaceThe relationimplieswhere the index i runs through the distinct eigenvalues of T. This is exactly the invariant subspace decompositiongiven in a previous section. Each ei(T) is the projection onto the subspace spanned by the Jordan chains corresponding to λi and along the subspaces spanned by the Jordan chains corresponding to λj for j ≠ i. In other words, ei(T) = P(λi;T). This explicit identification of the operators ei(T) in turn gives an explicit form of holomorphic functional calculus for matrices:Notice that the expression of f(T) is a finite sum because, on each neighborhood of λi, we have chosen the Taylor series expansion of f centered at λi.Let T be a bounded operator λ be an isolated point of σ(T). (As stated above, when T is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)The point λ is called a pole of operator T with order ν if the resolvent function RT defined byhas a pole of order ν at λ.We will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators.Consider the annular region A centered at the eigenvalue λ with sufficiently small radius ε such that the intersection of the open disc Bε(λ) and σ(T) is {λ}. The resolvent function RT is holomorphic on A.Extending a result from classical function theory, RT has a Laurent series representation on A:whereBy the previous discussion on the functional calculus,But we have shown that the smallest positive integer m such thatis precisely the index of λ, ν(λ). In other words, the function RT has a pole of order ν(λ) at λ.This example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.Consider the matrixwhich is mentioned in the beginning of the article.The characteristic polynomial of A isThis shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation Av = λ v. It is spanned by the column vector v = (−1, 1, 0, 0)T. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by w = (1, −1, 0, 1)T. Finally, the eigenspace corresponding to the eigenvalue 4 is also one-dimensional (even though this is a double eigenvalue) and is spanned by x = (1, 0, −1, 1)T. So, the geometric multiplicity (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix A is the direct sumThere are three chains. Two have length one: {v} and {w}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculatewhere I is the 4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of A − 4I, e.g., y = (1,0,0,0)T. Now, (A − 4I)y = x and (A − 4I)x = 0, so {y, x} is a chain of length two corresponding to the eigenvalue 4.The transition matrix P such that P−1AP = J is formed by putting these vectors next to each other as followsA computation shows that the equation P−1AP = J indeed holds.If we had interchanged the order of which the chain vectors appeared, that is, changing the order of v, w and {x, y} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.If the matrix A has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrixIf ε = 0, then the Jordan normal form is simplyHowever, for ε ≠ 0, the Jordan normal form isThis ill conditioning makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in numerical analysis; the stable Schur decomposition[15] or pseudospectra[16] are better alternatives.The Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let f(z) be an analytical function of a complex argument. Applying the function on a n×n Jordan block J with eigenvalue λ results in an upper triangular matrix:The following example shows the application to the power function f(z)=zn:
Set (mathematics)
In mathematics, a set is a collection of distinct objects, considered as an object in its own right. For example, the numbers 2, 4, and 6 are distinct objects when considered separately, but when they are considered collectively they form a single set of size three, written {2,4,6}. The concept of a set is one of the most fundamental in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived. In mathematics education, elementary topics from set theory such as Venn diagrams are taught at a young age, while more advanced concepts are taught as part of a university degree.The German word Menge, rendered as "set" in English, was coined by Bernard Bolzano in his work The Paradoxes of the Infinite.A set is a well-defined collection of distinct objects. The objects that make up a set (also known as the set's elements or members) can be anything: numbers, people, letters of the alphabet, other sets, and so on. Georg Cantor, one of the founders of set theory, gave the following definition of a set at the beginning of his Beiträge zur Begründung der transfiniten Mengenlehre:[1]A set is a gathering together into a whole of definite, distinct objects of our perception [Anschauung] or of our thought—which are called elements of the set.Sets are conventionally denoted with capital letters. Sets A and B are equal if and only if they have precisely the same elements.[2]For technical reasons, Cantor's definition turned out to be inadequate; today, in contexts where more rigor is required, one can use axiomatic set theory, in which the notion of a "set" is taken as a primitive notion and the properties of sets are defined by a collection of axioms. The most basic properties are that a set can have elements, and that two sets are equal (one and the same) if and only if every element of each set is an element of the other; this property is called the extensionality of sets.There are two ways of describing, or specifying the members of, a set. One way is by intensional definition, using a rule or semantic description:The second way is by extension – that is, listing each member of the set.  An extensional definition is denoted by enclosing the list of members in curly brackets:One often has the choice of specifying a set either intensionally or extensionally.  In the examples above, for instance, A = C and B = D.In an extensional definition, a set member can be listed two or more times, for example, {11, 6, 6}.  However, per extensionality, two definitions of sets which differ only in that one of the definitions lists members multiple times define the same set.  Hence, the set {11, 6, 6} is identical to the set {11, 6}. Moreover, the order in which the elements of a set are listed is irrelevant (unlike for a sequence or tuple).  We can illustrate these two important points with an example:For sets with many elements, the enumeration of members can be abbreviated. For instance, the set of the first thousand positive integers may be specified extensionally aswhere the ellipsis ("...") indicates that the list continues in the obvious way.The notation with braces may also be used in an intensional specification of a set.  In this usage, the braces have the meaning "the set of all ...". So, E = {playing card suits} is the set whose four members are ♠, ♦, ♥, and ♣.  A more general form of this is set-builder notation, through which, for instance, the set F of the twenty smallest integers that are four less than perfect square can be denotedIn this notation, the colon (":") means "such that", and the description can be interpreted as "F is the set of all numbers of the form n2 − 4, such that n is a whole number in the range from 0 to 19 inclusive." Sometimes the vertical bar ("|") is used instead of the colon.If B is a set and x is one of the objects of B, this is denoted x ∈ B, and is read as "x belongs to B", or "x is an element of B". If y is not a member of B then this is written as y ∉ B, and is read as "y does not belong to B".For example, with respect to the sets A = {1,2,3,4}, B = {blue, white, red}, and F = {n2 − 4 : n is an integer; and 0 ≤ n ≤ 19} defined above,If every member of set A is also a member of set B, then A is said to be a subset of B, written A ⊆ B (also pronounced A is contained in B). Equivalently, we can write B ⊇ A, read as B is a superset of A, B includes A, or B contains A. The relationship between sets established by ⊆ is called inclusion or containment.If A is a subset of, but not equal to, B, then A is called a proper subset of B, written A ⊊ B (A is a proper subset of B) or B ⊋ A (B is a proper superset of A).The expressions A ⊂ B and B ⊃ A are used differently by different authors; some authors use them to mean the same as A ⊆ B (respectively B ⊇ A), whereas others use them to mean the same as A ⊊ B (respectively B ⊋ A).Examples:The empty set is a subset of every set and every set is a subset of itself:Every set is a subset of the universal set:An obvious but useful identity, which can often be used to show that two seemingly different sets are equal:A partition of a set S is a set of nonempty subsets of S such that every element x in S is in exactly one of these subsets.The power set of a set S is the set of all subsets of S. The power set contains S itself and the empty set because these are both subsets of S. For example, the power set of the set {1, 2, 3} is {{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}, {2}, {3}, ∅}. The power set of a set S is usually written as P(S).The power set of a finite set with n elements has 2n elements. For example, the set {1, 2, 3} contains three elements, and the power set shown above contains 23 = 8 elements.The power set of an infinite (either countable or uncountable) set is always uncountable. Moreover, the power set of a set is always strictly "bigger" than the original set in the sense that there is no way to pair every element of S with exactly one element of P(S). (There is never an onto map or surjection from S onto P(S).)Every partition of a set S is a subset of the powerset of S.The cardinality | S | of a set S is "the number of members of S." For example, if  B = {blue, white, red}, then | B | = 3.There is a unique set with no members, called the empty set (or the null set), which is denoted by the symbol ∅ (other notations are used; see empty set).  The cardinality of the empty set is zero. For example, the set of all three-sided squares has zero members and thus is the empty set. Though it may seem trivial, the empty set, like the number zero, is important in mathematics. Indeed, the existence of this set is one of the fundamental concepts of axiomatic set theory.Some sets have infinite cardinality.  The set N of natural numbers, for instance, is infinite. Some infinite cardinalities are greater than others.  For instance, the set of real numbers has greater cardinality than the set of natural numbers. However, it can be shown that the cardinality of (which is to say, the number of points on) a straight line is the same as the cardinality of any segment of that line, of the entire plane, and indeed of any finite-dimensional Euclidean space.There are some sets or kinds of sets that hold great mathematical importance and are referred to with such regularity that they have acquired special names and notational conventions to identify them. One of these is the empty set, denoted {} or ∅. A set with exactly one element, x, is a unit set, or singleton, {x}.[2]Many of these sets are represented using blackboard bold or bold typeface. Special sets of numbers includePositive and negative sets are denoted by a superscript - or +. For example, ℚ+ represents the set of positive rational numbers.Each of the above sets of numbers has an infinite number of elements, and each can be considered to be a proper subset of the sets listed below it. The primes are used less frequently than the others outside of number theory and related fields.There are several fundamental operations for constructing new sets from given sets.Two sets can be "added" together. The union of A and B, denoted by A ∪ B, is the set of all things that are members of either A or B.Examples:Some basic properties of unions:A new set can also be constructed by determining which members two sets have "in common". The intersection of A and B, denoted by A ∩ B, is the set of all things that are members of both A and B. If A ∩ B = ∅, then A and B are said to be disjoint.Examples:Some basic properties of intersections:Two sets can also be "subtracted". The relative complement of B in A (also called the set-theoretic difference of A and B), denoted by A \ B (or A − B), is the set of all elements that are members of A but not members of B. Note that it is valid to "subtract" members of a set that are not in the set, such as removing the element green from the set {1, 2, 3}; doing so has no effect.In certain settings all sets under discussion are considered to be subsets of a given universal set U. In such cases, U \ A is called the absolute complement or simply complement of A, and is denoted by A′.Examples:Some basic properties of complements:An extension of the complement is the symmetric difference, defined for sets A, B asFor example, the symmetric difference of {7,8,9,10} and {9,10,11,12} is the set {7,8,11,12}. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring (with the empty set as neutral element) and intersection as the multiplication of the ring.A new set can be constructed by associating every element of one set with every element of another set.  The Cartesian product of two sets A and B, denoted by A × B is the set of all ordered pairs (a, b) such that a is a member of A and b is a member of B.Examples:Some basic properties of Cartesian products:Let A and B be finite sets; then the cardinality of the Cartesian product is the product of the cardinalities:Set theory is seen as the foundation from which virtually all of mathematics can be derived. For example, structures in abstract algebra, such as groups, fields and rings, are sets closed under one or more operations.One of the main applications of naive set theory is constructing relations.  A relation from a domain A to a codomain B is a subset of the Cartesian product A × B.  Given this concept, we are quick to see that the set F of all ordered pairs (x, x2), where x is real, is quite familiar.  It has a domain set R and a codomain set that is also R, because the set of all squares is subset of the set of all real numbers.  If placed in functional notation, this relation becomes f(x) = x2.  The reason these two are equivalent is for any given value, y that the function is defined for, its corresponding ordered pair, (y, y2) is a member of the set F.Although initially naive set theory, which defines a set merely as any well-defined collection, was well accepted, it soon ran into several obstacles. It was found that this definition spawned several paradoxes, most notably:The reason is that the phrase well-defined is not very well-defined. It was important to free set theory of these paradoxes because nearly all of mathematics was being redefined in terms of set theory. In an attempt to avoid these paradoxes, set theory was axiomatized based on first-order logic, and thus axiomatic set theory was born.For most purposes, however, naive set theory is still useful.The inclusion–exclusion principle is a counting technique that can be used to count the number of elements in a union of two sets, if the size of each set and the size of their intersection are known. It can be expressed symbolically asA more general form of the principle can be used to find the cardinality of any finite union of sets:Augustus De Morgan stated two laws about sets.If A and B are any two sets then,The complement of A union B equals the complement of A intersected with the complement of B.The complement of A intersected with B is equal to the complement of A union to the complement of B.
René Descartes
René Descartes (/deɪˈkɑːrt/, UK also /ˈdeɪkɑːrt/;[15] French: [ʁəne dekaʁt]; Latinized: Renatus Cartesius; adjectival form: "Cartesian";[16] 31 March 1596 – 11 February 1650) was a French philosopher, mathematician, and scientist. A native of the Kingdom of France, he spent about 20 years (1629–49) of his life in the Dutch Republic after serving for a while in the Dutch States Army of Maurice of Nassau, Prince of Orange and the Stadtholder of the United Provinces. He is generally considered one of the most notable intellectual figures of the Dutch Golden Age.[17] Descartes' Meditations on First Philosophy (1641) continues to be a standard text at most university philosophy departments. Descartes' influence in mathematics is equally apparent; the Cartesian coordinate system (see below) was named after him. He is credited as the father of analytical geometry, the bridge between algebra and geometry, used in the discovery of infinitesimal calculus and analysis. Descartes was also one of the key figures in the Scientific Revolution.Descartes refused to accept the authority of previous philosophers. He frequently set his views apart from those of his predecessors. In the opening section of the Passions of the Soul, an early modern treatise on emotions, Descartes goes so far as to assert that he will write on this topic "as if no one had written on these matters before". His best known philosophical statement is "I think, therefore I am" (French: Je pense, donc je suis; Latin: Ego cogito, ergo sum), found in Discourse on the Method (1637; written in French and Latin) and Principles of Philosophy (1644; written in Latin).[18]Many elements of his philosophy have precedents in late Aristotelianism, the revived Stoicism of the 16th century, or in earlier philosophers like Augustine. In his natural philosophy, he differed from the schools on two major points: first, he rejected the splitting of corporeal substance into matter and form; second, he rejected any appeal to final ends, divine or natural, in explaining natural phenomena.[19] In his theology, he insists on the absolute freedom of God's act of creation.Descartes laid the foundation for 17th-century continental rationalism, later advocated by Spinoza and Leibniz, and opposed by the empiricist school of thought consisting of Hobbes, Locke, Berkeley, and Hume. Leibniz, Spinoza[20], and Descartes were all well versed in mathematics as well as philosophy, and Descartes and Leibniz contributed greatly to science as well.René Descartes was born in La Haye en Touraine (now Descartes, Indre-et-Loire), France, on 31 March 1596.[21] His mother, Jeanne Brochard, died soon after giving birth to him, and so he was not expected to survive.[21] Descartes' father, Joachim, was a member of the Parlement of Brittany at Rennes.[22] René lived with his grandmother and with his great-uncle. Although the Descartes family was Roman Catholic, the Poitou region was controlled by the Protestant Huguenots.[23] In 1607, late because of his fragile health, he entered the Jesuit Collège Royal Henry-Le-Grand at La Flèche,[24][25] where he was introduced to mathematics and physics, including Galileo's work.[24][26] After graduation in 1614, he studied for two years (1615–16) at the University of Poitiers, earning a Baccalauréat and Licence in canon and civil law in 1616,[24] in accordance with his father's wishes that he should become a lawyer.[27] From there he moved to Paris.In Discourse on the Method, Descartes recalls,I entirely abandoned the study of letters. Resolving to seek no knowledge other than that of which could be found in myself or else in the great book of the world, I spent the rest of my youth traveling, visiting courts and armies, mixing with people of diverse temperaments and ranks, gathering various experiences, testing myself in the situations which fortune offered me, and at all times reflecting upon whatever came my way so as to derive some profit from it.Given his ambition to become a professional military officer, in 1618, Descartes joined, as a mercenary, the Protestant Dutch States Army in Breda under the command of Maurice of Nassau,[24] and undertook a formal study of military engineering, as established by Simon Stevin. Descartes, therefore, received much encouragement in Breda to advance his knowledge of mathematics.[24] In this way, he became acquainted with Isaac Beeckman,[24] the principal of a Dordrecht school, for whom he wrote the Compendium of Music (written 1618, published 1650). Together they worked on free fall, catenary, conic section, and fluid statics. Both believed that it was necessary to create a method that thoroughly linked mathematics and physics.[28]While in the service of the Catholic Duke Maximilian of Bavaria since 1619,[29] Descartes was present at the Battle of the White Mountain outside Prague, in November 1620.[30]According to Adrien Baillet, on the night of 10–11 November 1619 (St. Martin's Day), while stationed in Neuburg an der Donau, Descartes shut himself in a room with an "oven" (probably a Kachelofen or masonry heater) to escape the cold. While within, he had three dreams[31] and believed that a divine spirit revealed to him a new philosophy. However, it is likely that what Descartes considered to be his second dream was actually an episode of exploding head syndrome.[32] Upon exiting, he had formulated analytical geometry and the idea of applying the mathematical method to philosophy. He concluded from these visions that the pursuit of science would prove to be, for him, the pursuit of true wisdom and a central part of his life's work.[33][34] Descartes also saw very clearly that all truths were linked with one another so that finding a fundamental truth and proceeding with logic would open the way to all science. Descartes discovered this basic truth quite soon: his famous "I think, therefore I am".[28]In 1620 Descartes left the army. He visited Basilica della Santa Casa in Loreto, then visited various countries before returning to France, and during the next few years spent time in Paris. It was there that he composed his first essay on method: Regulae ad Directionem Ingenii (Rules for the Direction of the Mind).[28] He arrived in La Haye in 1623, selling all of his property to invest in bonds, which provided a comfortable income for the rest of his life.[35] Descartes was present at the siege of La Rochelle by Cardinal Richelieu in 1627.[36] In the fall of the same year, in the residence of the papal nuncio Guidi di Bagno, where he came with Mersenne and many other scholars to listen to a lecture given by the alchemist Nicolas de Villiers, Sieur de Chandoux on the principles of a supposed new philosophy,[37] Cardinal Bérulle urged him to write an exposition of his own new philosophy in some location beyond the reach of the inquisition.[38]Descartes returned to the Dutch Republic in 1628.[31] In April 1629 he joined the University of Franeker, studying under Adriaan Metius, living either with a Catholic family, or renting the Sjaerdemaslot, where he invited in vain a French cook and an optician.[citation needed] The next year, under the name "Poitevin", he enrolled at the Leiden University to study mathematics with Jacobus Golius, who confronted him with Pappus's hexagon theorem, and astronomy with Martin Hortensius.[39] In October 1630 he had a falling-out with Beeckman, whom he accused of plagiarizing some of his ideas. In Amsterdam, he had a relationship with a servant girl, Helena Jans van der Strom, with whom he had a daughter, Francine, who was born in 1635 in Deventer. She died of scarlet fever at the age of 5.Unlike many moralists of the time, Descartes was not devoid of passions but rather defended them; he wept upon Francine's death in 1640.[40] "Descartes said that he did not believe that one must refrain from tears to prove oneself a man." Russell Shorto postulated that the experience of fatherhood and losing a child formed a turning point in Descartes' work, changing its focus from medicine to a quest for universal answers.[41]Despite frequent moves,[42] he wrote all his major work during his 20-plus years in the Netherlands, where he managed to revolutionize mathematics and philosophy.[43] In 1633, Galileo was condemned by the Catholic Church, and Descartes abandoned plans to publish Treatise on the World, his work of the previous four years. Nevertheless, in 1637 he published part of this work[44] in three essays: "Les Météores" (The Meteors), "La Dioptrique" (Dioptrics) and "La Géométrie" (Geometry), preceded by an introduction, his famous Discours de la méthode (Discourse on the Method).[44] In it, Descartes lays out four rules of thought, meant to ensure that our knowledge rests upon a firm foundation.The first was never to accept anything for true which I did not clearly know to be such; that is to say, carefully to avoid precipitancy and prejudice, and to comprise nothing more in my judgment than what was presented to my mind so clearly and distinctly as to exclude all ground of doubt.In La Géométrie, Descartes exploited the discoveries he made with Pierre de Fermat, having been able to do so because his paper, Introduction to Loci, was published posthumously in 1679.[45] This later became known as Cartesian Geometry.[45]Descartes continued to publish works concerning both mathematics and philosophy for the rest of his life. In 1641 he published a metaphysics work, Meditationes de Prima Philosophia (Meditations on First Philosophy), written in Latin and thus addressed to the learned. It was followed, in 1644, by Principia Philosophiæ (Principles of Philosophy), a kind of synthesis of the Discourse on the Method and Meditations on First Philosophy. In 1643, Cartesian philosophy was condemned at the University of Utrecht, and Descartes was obliged to flee to the Hague, and settled in Egmond-Binnen.Descartes began (through Alfonso Polloti, an Italian general in Dutch service) a long correspondence with Princess Elisabeth of Bohemia, devoted mainly to moral and psychological subjects. Connected with this correspondence, in 1649 he published Les Passions de l'âme (Passions of the Soul), that he dedicated to the Princess. In 1647, he was awarded a pension by King Louis XIV of France, though it was never paid.[46] A French translation of Principia Philosophiæ, prepared by Abbot Claude Picot, was published in 1647. This edition Descartes also dedicated to Princess Elisabeth. In the preface to the French edition, Descartes praised true philosophy as a means to attain wisdom. He identifies four ordinary sources to reach wisdom and finally says that there is a fifth, better and more secure, consisting in the search for first causes.[47]By 1649, Descartes had become famous throughout Europe for being one of the continent's greatest philosophers and scientists.[44] That year, Queen Christina of Sweden invited Descartes to her court in to organize a new scientific academy and tutor her in his ideas about love. She was interested in and stimulated Descartes to publish the "Passions of the Soul", a work based on his correspondence with Princess Elisabeth.[48] Descartes accepted, and moved to Sweden in the middle of winter.[49]He was a guest at the house of Pierre Chanut, living on Västerlånggatan, less than 500 meters from Tre Kronor in Stockholm. There, Chanut and Descartes made observations with a Torricellian barometer, a tube with mercury. Challenging Blaise Pascal, Descartes took the first set of barometric readings in Stockholm to see if atmospheric pressure could be used in forecasting the weather.[50][51]Descartes apparently started giving lessons to Queen Christina after her birthday, three times a week, at 5 a.m, in her cold and draughty castle. Soon it became clear they did not like each other; she did not like his mechanical philosophy, nor did he appreciate her interest in Ancient Greek.  By 15 January 1650, Descartes had seen Christina only four or five times. On 1 February he contracted pneumonia and died on 11 February.[52] The cause of death was pneumonia according to Chanut, but peripneumonia according to the doctor Van Wullen who was not allowed to bleed him.[53] (The winter seems to have been mild,[54] except for the second half of January which was harsh as described by Descartes himself; however, "this remark was probably intended to be as much Descartes' take on the intellectual climate as it was about the weather."[48])In 1996 E. Pies, a German scholar, published a book questioning this account, based on a letter by Johann van Wullen, who had been sent by Christina to treat him, something Descartes refused, and more arguments against its veracity have been raised since.[55] Descartes might have been assassinated[56][57] as he asked for an emetic: wine mixed with tobacco.[58][dubious  – discuss]As a Catholic[59][60][61] in a Protestant nation, he was interred in a graveyard used mainly for orphans in Adolf Fredriks kyrka in Stockholm. His manuscripts came into the possession of Claude Clerselier, Chanut's brother-in-law, and "a devout Catholic who has begun the process of turning Descartes into a saint by cutting, adding and publishing his letters selectively."[62] In 1663, the Pope placed his works on the Index of Prohibited Books. In 1666 his remains were taken to France and buried in the Saint-Étienne-du-Mont. In 1671 Louis XIV prohibited all the lectures in Cartesianism. Although the National Convention in 1792 had planned to transfer his remains to the Panthéon, he was reburied in the Abbey of Saint-Germain-des-Prés in 1819, missing a finger and skull.[63] His skull is on display in the Musée de l'Homme in Paris.[64]Initially, Descartes arrives at only a single first principle: I think. Thought cannot be separated from me, therefore, I exist (Discourse on the Method and Principles of Philosophy). Most famously, this is known as cogito ergo sum (English: "I think, therefore I am"). Therefore, Descartes concluded, if he doubted, then something or someone must be doing the doubting, therefore the very fact that he doubted proved his existence.  "The simple meaning of the phrase is that if one is skeptical of existence, that is in and of itself proof that he does exist."[65] These two first principles—I think and I exist—were later confirmed by Descartes's clear and distinct perception (delineated in his Third Meditation): that I clearly and distinctly perceive these two principles, Descartes reasoned, ensures their indubitability.Descartes concludes that he can be certain that he exists because he thinks. But in what form? He perceives his body through the use of the senses; however, these have previously been unreliable. So Descartes determines that the only indubitable knowledge is that he is a thinking thing. Thinking is what he does, and his power must come from his essence. Descartes defines "thought" (cogitatio) as "what happens in me such that I am immediately conscious of it, insofar as I am conscious of it". Thinking is thus every activity of a person of which the person is immediately conscious.[66] He gave reasons for thinking that waking thoughts are distinguishable from dreams, and that one's mind cannot have been "hijacked" by an evil demon placing an illusory external world before one's senses.[67]And so something that I thought I was seeing with my eyes is in fact grasped solely by the faculty of judgment which is in my mind.In this manner, Descartes proceeds to construct a system of knowledge, discarding perception as unreliable and, instead, admitting only deduction as a method.[68]Descartes, influenced by the automatons on display throughout the city of Paris, began to investigate the connection between the mind and body, and how the two interact.[69] His main influences for dualism were theology and physics.[70] The theory on the dualism of mind and body is Descartes' signature doctrine and permeates other theories he advanced. Known as Cartesian dualism (or Mind-Body Dualism), his theory on the separation between the mind and the body went on to influence subsequent Western philosophies. In Meditations on First Philosophy, Descartes attempted to demonstrate the existence of God and the distinction between the human soul and the body. Humans are a union of mind and body;[71] thus Descartes' dualism embraced the idea that mind and body are distinct but closely joined. While many contemporary readers of Descartes found the distinction between mind and body difficult to grasp, he thought it was entirely straightforward. Descartes employed the concept of modes, which are the ways in which substances exist. In Principles of Philosophy, Descartes explained, "we can clearly perceive a substance apart from the mode which we say differs from it, whereas we cannot, conversely, understand the mode apart from the substance". To perceive a mode apart from its substance requires an intellectual abstraction,[72] which Descartes explained as follows:The intellectual abstraction consists in my turning my thought away from one part of the contents of this richer idea the better to apply it to the other part with greater attention. Thus, when I consider a shape without thinking of the substance or the extension whose shape it is, I make a mental abstraction.[72]According to Descartes two substances are really distinct when each of them can exist apart from the other. Thus Descartes reasoned that God is distinct from humans, and the body and mind of a human are also distinct from one another.[73] He argued that the great differences between body (an extended thing) and mind (an un-extended, immaterial thing) make the two ontologically distinct. But that the mind was utterly indivisible: because "when I consider the mind, or myself in so far as I am merely a thinking thing, I am unable to distinguish any part within myself; I understand myself to be something quite single and complete."[74]In Meditations Descartes discussed a piece of wax and exposed the single most characteristic doctrine of Cartesian dualism: that the universe contained two radically different kinds of substances—the mind or soul defined as thinking, and the body defined as matter and unthinking.[75] The Aristotelian philosophy of Descartes' days held that the universe was inherently purposeful or theological. Everything that happened, be it the motion of the stars or the growth of a tree, was supposedly explainable by a certain purpose, goal or end that worked its way out within nature. Aristotle called this the "final cause", and these final causes were indispensable for explaining the ways nature operated. With his theory on dualism Descartes fired the opening shot for the battle between the traditional Aristotelian science and the new science of Kepler and Galileo which denied the final cause for explaining nature. Descartes' dualism provided the philosophical rationale for the latter and he expelled the final cause from the physical universe (or res extensa). For Descartes the only place left for the final cause was the mind (or res cogitans). Therefore, while Cartesian dualism paved the way for modern physics, it also held the door open for religious beliefs about the immortality of the soul.[76]Descartes' dualism of mind and matter implied a concept of human beings. A human was according to Descartes a composite entity of mind and body. Descartes gave priority to the mind and argued that the mind could exist without the body, but the body could not exist without the mind. In Meditations Descartes even argues that while the mind is a substance, the body is composed only of "accidents".[77] But he did argue that mind and body are closely joined:[78] Nature also teaches me, by the sensations of pain, hunger, thirst and so on, that I am not merely present in my body as a pilot in his ship, but that I am very closely joined and, as it were, intermingled with it, so that I and the body form a unit. If this were not so, I, who am nothing but a thinking thing, would not feel pain when the body was hurt, but would perceive the damage purely by the intellect, just as a sailor perceives by sight if anything in his ship is broken.[78]Descartes' discussion on embodiment raised one of the most perplexing problems of his dualism philosophy: What exactly is the relationship of union between the mind and the body of a person?[78] Therefore, Cartesian dualism set the agenda for philosophical discussion of the mind–body problem for many years after Descartes' death.[79] Descartes was also a rationalist and believed in the power of innate ideas.[80] Descartes argued the theory of innate knowledge and that all humans were born with knowledge through the higher power of God. It was this theory of innate knowledge that later led philosopher John Locke (1632–1704) to combat the theory of empiricism, which held that all knowledge is acquired through experience.[81]In The Passions of the Soul written between 1645 and 1646 Descartes discussed the common contemporary belief that the human body contained animal spirits. These animal spirits were believed to be light and roaming fluids circulating rapidly around the nervous system between the brain and the muscles, and served as a metaphor for feelings, like being in high or bad spirit. These animal spirits were believed to affect the human soul, or passions of the soul. Descartes distinguished six basic passions: wonder, love, hatred, desire, joy and sadness. All of these passions, he argued, represented different combinations of the original spirit, and influenced the soul to will or want certain actions. He argued, for example, that fear is a passion that moves the soul to generate a response in the body. In line with his dualist teachings on the separation between the soul and the body, he hypothesized that some part of the brain served as a connector between the soul and the body and singled out the pineal gland as connector.[82] Descartes argued that signals passed from the ear and the eye to the pineal gland, through animal spirits. Thus different motions in the gland cause various animal spirits. He argued that these motions in the pineal gland are based on God's will and that humans are supposed to want and like things that are useful to them. But he also argued that the animal spirits that moved around the body could distort the commands from the pineal gland, thus humans had to learn how to control their passions.[83]Descartes advanced a theory on automatic bodily reactions to external events which influenced 19th-century reflex theory. He argued that external motions such as touch and sound reach the endings of the nerves and affect the animal spirits. Heat from fire affects a spot on the skin and sets in motion a chain of reactions, with the animal spirits reaching the brain through the central nervous system, and in turn animal spirits are sent back to the muscles to move the hand away from the fire.[83] Through this chain of reactions the automatic reactions of the body do not require a thought process.[80]Above all he was among the first scientists who believed that the soul should be subject to scientific investigation. He challenged the views of his contemporaries that the soul was divine, thus religious authorities regarded his books as dangerous. Descartes' writings went on to form the basis for theories on emotions and how cognitive evaluations were translated into affective processes. Descartes believed that the brain resembled a working machine and unlike many of his contemporaries believed that mathematics and mechanics could explain the most complicated processes of the mind. In the 20th century Alan Turing advanced computer science based on mathematical biology as inspired by Descartes. His theories on reflexes also served as the foundation for advanced physiological theories more than 200 years after his death. The physiologist Ivan Pavlov was a great admirer of Descartes.[84]For Descartes, ethics was a science, the highest and most perfect of them. Like the rest of the sciences, ethics had its roots in metaphysics.[68] In this way, he argues for the existence of God, investigates the place of man in nature, formulates the theory of mind-body dualism, and defends free will. However, as he was a convinced rationalist, Descartes clearly states that reason is sufficient in the search for the goods that we should seek, and virtue consists in the correct reasoning that should guide our actions. Nevertheless, the quality of this reasoning depends on knowledge, because a well-informed mind will be more capable of making good choices, and it also depends on mental condition. For this reason, he said that a complete moral philosophy should include the study of the body. He discussed this subject in the correspondence with Princess Elisabeth of Bohemia, and as a result wrote his work The Passions of the Soul, that contains a study of the psychosomatic processes and reactions in man, with an emphasis on emotions or passions.[85] His works about human passion and emotion would be the basis for the philosophy of his followers (see Cartesianism), and would have a lasting impact on ideas concerning what literature and art should be, specifically how it should invoke emotion.[86]Humans should seek the sovereign good that Descartes, following Zeno, identifies with virtue, as this produces a solid blessedness or pleasure. For Epicurus the sovereign good was pleasure, and Descartes says that, in fact, this is not in contradiction with Zeno's teaching, because virtue produces a spiritual pleasure, that is better than bodily pleasure. Regarding Aristotle's opinion that happiness depends on the goods of fortune, Descartes does not deny that this good contributes to happiness but remarks that they are in great proportion outside one's own control, whereas one's mind is under one's complete control.[85] The moral writings of Descartes came at the last part of his life, but earlier, in his Discourse on the Method he adopted three maxims to be able to act while he put all his ideas into doubt. This is known as his "Provisional Morals".In the third and fifth Meditation, he offers an ontological proof of a benevolent God (through both the ontological argument and trademark argument). Because God is benevolent, he can have some faith in the account of reality his senses provide him, for God has provided him with a working mind and sensory system and does not desire to deceive him. From this supposition, however, he finally establishes the possibility of acquiring knowledge about the world based on deduction and perception. Regarding epistemology, therefore, he can be said to have contributed such ideas as a rigorous conception of foundationalism and the possibility that reason is the only reliable method of attaining knowledge. He, nevertheless, was very much aware that experimentation was necessary to verify and validate theories.[68]In his Meditations on First Philosophy Descartes sets forth two proofs for God's existence. One of these is founded upon the possibility of thinking the "idea of a being that is supremely perfect and infinite," and suggests that "of all the ideas that are in me, the idea that I have of God is the most true, the most clear and distinct."[87] Descartes considered himself to be a devout Catholic[59][60][61] and one of the purposes of the Meditations was to defend the Catholic faith. His attempt to ground theological beliefs on reason encountered intense opposition in his time, however: Pascal regarded Descartes' views as rationalist and mechanist, and accused him of deism: "I cannot forgive Descartes; in all his philosophy, Descartes did his best to dispense with God. But Descartes could not avoid prodding God to set the world in motion with a snap of his lordly fingers; after that, he had no more use for God," while a powerful contemporary, Martin Schoock, accused him of atheist beliefs, though Descartes had provided an explicit critique of atheism in his Meditations. The Catholic Church prohibited his books in 1663.[46][88][check quotation syntax]Descartes also wrote a response to external world scepticism. Through this method of scepticism, he does not doubt for the sake of doubting but to achieve concrete and reliable information. In other words, certainty. He argues that sensory perceptions come to him involuntarily, and are not willed by him. They are external to his senses, and according to Descartes, this is evidence of the existence of something outside of his mind, and thus, an external world. Descartes goes on to show that the things in the external world are material by arguing that God would not deceive him as to the ideas that are being transmitted, and that God has given him the "propensity" to believe that such ideas are caused by material things. Descartes also believes a substance is something that does not need any assistance to function or exist. Descartes further explains how only God can be a true “substance”. But minds are substances, meaning they need only God for it to function. The mind is a thinking substance. The means for a thinking substance stem from ideas.[89]Descartes is often regarded as the first thinker to emphasize the use of reason to develop the natural sciences.[90] For him the philosophy was a thinking system that embodied all knowledge, and expressed it in this way:[68]Thus, all Philosophy is like a tree, of which Metaphysics is the root, Physics the trunk, and all the other sciences the branches that grow out of this trunk, which are reduced to three principals, namely, Medicine, Mechanics, and Ethics. By the science of Morals, I understand the highest and most perfect which, presupposing an entire knowledge of the other sciences, is the last degree of wisdom.In his Discourse on the Method, he attempts to arrive at a fundamental set of principles that one can know as true without any doubt. To achieve this, he employs a method called hyperbolical/metaphysical doubt, also sometimes referred to as methodological scepticism: he rejects any ideas that can be doubted and then re-establishes them in order to acquire a firm foundation for genuine knowledge.[91] Descartes built his ideas from scratch. He relates this to architecture: the top soil is taken away to create a new building or structure. Descartes calls his doubt the soil and new knowledge the buildings. To Descartes, Aristotle's foundationalism is incomplete and his method of doubt enhances foundationalism.[67]Descartes denied that animals had reason or intelligence. He argued that animals did not lack sensations or perceptions, but these could be explained mechanistically.[92] Whereas humans had a soul, or mind, and were able to feel pain and anxiety, animals by virtue of not having a soul could not feel pain or anxiety. If animals showed signs of distress then this was to protect the body from damage, but the innate state needed for them to suffer was absent. Although Descartes' views were not universally accepted they became prominent in Europe and North America, allowing humans to treat animals with impunity. The view that animals were quite separate from humanity and merely machines allowed for the maltreatment of animals, and was sanctioned in law and societal norms until the middle of the 19th century. The publications of Charles Darwin would eventually erode the Cartesian view of animals. Darwin argued that the continuity between humans and other species opened the possibilities that animals did not have dissimilar properties to suffer.[93]Descartes has often been dubbed the father of modern Western philosophy, the thinker whose approach has profoundly changed the course of Western philosophy and set the basis for modernity.[94][95] The first two of his Meditations on First Philosophy, those that formulate the famous methodic doubt, represent the portion of Descartes' writings that most influenced modern thinking.[96] It has been argued that Descartes himself didn't realize the extent of this revolutionary move.[97] In shifting the debate from "what is true" to "of what can I be certain?," Descartes arguably shifted the authoritative guarantor of truth from God to humanity (even though Descartes himself claimed he received his visions from God)—while the traditional concept of "truth" implies an external authority, "certainty" instead relies on the judgment of the individual.In an anthropocentric revolution, the human being is now raised to the level of a subject, an agent, an emancipated being equipped with autonomous reason. This was a revolutionary step that established the basis of modernity, the repercussions of which are still being felt: the emancipation of humanity from Christian revelational truth and Church doctrine; humanity making its own law and taking its own stand.[98][99][100] In modernity, the guarantor of truth is not God anymore but human beings, each of whom is a "self-conscious shaper and guarantor" of their own reality.[101][102] In that way, each person is turned into a reasoning adult, a subject and agent,[101] as opposed to a child obedient to God. This change in perspective was characteristic of the shift from the Christian medieval period to the modern period, a shift that had been anticipated in other fields, and which was now being formulated in the field of philosophy by Descartes.[101][103]This anthropocentric perspective of Descartes' work, establishing human reason as autonomous, provided the basis for the Enlightenment's emancipation from God and the Church. According to Martin Heidegger, the perspective of Descartes' work also provided the basis for all subsequent anthropology.[104] Descartes' philosophical revolution is sometimes said to have sparked modern anthropocentrism and subjectivism.[94][105][106][107]One of Descartes' most enduring legacies was his development of Cartesian or analytic geometry, which uses algebra to describe geometry. He "invented the convention of representing unknowns in equations by x, y, and z, and knowns by a, b, and c". He also "pioneered the standard notation" that uses superscripts to show the powers or exponents; for example, the 2 used in x2 to indicate x squared.[108][109] He was first to assign a fundamental place for algebra in our system of knowledge, using it as a method to automate or mechanize reasoning, particularly about abstract, unknown quantities. European mathematicians had previously viewed geometry as a more fundamental form of mathematics, serving as the foundation of algebra. Algebraic rules were given geometric proofs by mathematicians such as Pacioli, Cardan, Tartaglia and Ferrari. Equations of degree higher than the third were regarded as unreal, because a three-dimensional form, such as a cube, occupied the largest dimension of reality. Descartes professed that the abstract quantity a2 could represent length as well as an area. This was in opposition to the teachings of mathematicians, such as Vieta, who argued that it could represent only area. Although Descartes did not pursue the subject, he preceded Gottfried Wilhelm Leibniz in envisioning a more general science of algebra or "universal mathematics," as a precursor to symbolic logic, that could encompass logical principles and methods symbolically, and mechanize general reasoning.[110]Descartes' work provided the basis for the calculus developed by Newton and Leibniz, who applied infinitesimal calculus to the tangent line problem, thus permitting the evolution of that branch of modern mathematics.[111] His rule of signs is also a commonly used method to determine the number of positive and negative roots of a polynomial.The beginning to Descartes' interest in physics is accredited to the amateur scientist and mathematician Isaac Beeckman, who was at the forefront of a new school of thought known as mechanical philosophy. With this foundation of reasoning, Descartes formulated many of his theories on mechanical and geometrical physics.[112] Descartes discovered an early form of the law of conservation of mechanical momentum (a measure of the motion of an object), and envisioned it as pertaining to motion in a straight line, as opposed to perfect circular motion, as Galileo had envisioned it. He outlined his views on the universe in his Principles of Philosophy.Descartes also made contributions to the field of optics. He showed by using geometric construction and the law of refraction (also known as Descartes' law or more commonly Snell's law) that the angular radius of a rainbow is 42 degrees (i.e., the angle subtended at the eye by the edge of the rainbow and the ray passing from the sun through the rainbow's centre is 42°).[113] He also independently discovered the law of reflection, and his essay on optics was the first published mention of this law.[114]Current opinion is that Descartes had the most influence of anyone on the young Newton, and this is arguably one of Descartes' most important contributions. Newton continued Descartes' work on cubic equations, which freed the subject from the fetters of the Greek perspectives. The most important concept was his very modern treatment of independent variables.[115]Although Descartes was well known in academic circles towards the end of his life, the teaching of his works in schools was controversial. Henri de Roy (Henricus Regius, 1598–1679), Professor of Medicine at the University of Utrecht, was condemned by the Rector of the University, Gijsbert Voet (Voetius), for teaching Descartes' physics.[116]In January 2010, a previously unknown letter from Descartes, dated 27 May 1641, was found by the Dutch philosopher Erik-Jan Bos when browsing through Google. Bos found the letter mentioned in a summary of autographs kept by Haverford College in Haverford, Pennsylvania. The college was unaware that the letter had never been published. This was the third letter by Descartes found in the last 25 years.[117][118]Descartes... that which he himself founded... modern (and that means, at the same time, Western) metaphysics.The Descartes most familiar to twentieth-century philosophers is the Descartes of the first two Meditations, someone proccupied with hyperbolic doubt of the material world and the certainty of knowledge of the self that emerges from the famous cogito argument.Husserl has taken Descartes very seriously in a historical as well as in a systematic sense [...] [in The Crisis of the European Sciences and Transcendental Phenomenology, Husserl] finds in the first two Meditations of Descartes a depth which it is difficult to fathom, and which Descartes himself was so little able to appreciate that he let go "the great discovery" he had in his hands.For up to Descartes...a particular sub-iectum...lies at the foundation of its own fixed qualities and changing circumstances. The superiority of a sub-iectum...arises out of the claim of man to a...self-supported, unshakeable foundation of truth, in the sense of certainty. Why and how does this claim acquire its decisive authority? The claim originates in that emancipation of man in which he frees himself from obligation to Christian revelational truth and Church doctrine to a legislating for himself that takes its stand upon itself.With the interpretation of man as subiectum, Descartes creates the metaphysical presupposition for future anthropology of every kind and tendency.... the kind of anthropocentric subjectivism which has emerged from the Cartesian revolution.When, with the beginning of modern times, religious belief was becoming more and more externalized as a lifeless convention, men of intellect were lifted by a new belief: their great belief in an autonomous philosophy and science. [...] in philosophy, the Meditations were epoch-making in a quite unique sense, and precisely because of their going back to the pure ego cogito. Descartes work has been used, in fact to inaugurates an entirely new kind of philosophy. Changing its total style, philosophy takes a radical turn: from naïve objectivism to transcendental subjectivism.GeneralBibliographiesStanford Encyclopedia of PhilosophyInternet Encyclopedia of PhilosophyOther
Algebra over a field
In mathematics, an algebra over a field (often simply called an algebra) is a vector space equipped with a bilinear product. Thus, an algebra is an algebraic structure, which consists of a set, together with operations of multiplication, addition, and scalar multiplication by elements of the underlying field, and satisfies the axioms implied by "vector space" and "bilinear".[1]The multiplication operation in an algebra may or may not be associative, leading to the notions of associative algebras and nonassociative algebras. Given an integer n, the ring of real square matrices of order n is an example of an associative algebra over the field of real numbers under matrix addition and matrix multiplication since matrix multiplication is associative. Three-dimensional Euclidean space with multiplication given by the vector cross product is an example of a nonassociative algebra over the field of real numbers since the vector cross product is nonassociative, satisfying the Jacobi identity instead.An algebra is unital or unitary if it has an identity element with respect to the multiplication. The ring of real square matrices of order n forms a unital algebra since the identity matrix of order n is the identity element with respect to matrix multiplication. It is an example of a unital associative algebra, a (unital) ring that is also a vector space.Many authors use the term algebra to mean associative algebra, or unital associative algebra, or in  some subjects such as algebraic geometry, unital associative commutative algebra.Replacing the field of scalars by a commutative ring leads to the more general notion of an algebra over a ring. Algebras are not to be confused with vector spaces equipped with a bilinear form, like inner product spaces, as, for such a space, the result of a product is not in the space, but rather in the field of coefficients.Any complex number may be written a + bi, where a and b are real numbers and i is the imaginary unit. In other words, a complex number is represented by the vector (a, b) over the field of real numbers. So the complex numbers form a two-dimensional real vector space, where addition is given by (a, b) + (c, d) = (a + c, b + d) and scalar multiplication is given by c(a, b) = (ca, cb), where all of a, b, c and d are real numbers. We use the symbol · to multiply two vectors together, which we use complex multiplication to define: (a, b) · (c, d) = (ac − bd, ad + bc).The following statements are basic properties of the complex numbers. If x, y, z are complex numbers and a, b are real numbers, thenThis example fits into the following definition by taking the field K to be the real numbers, and the vector space A to be the complex numbers.Let K be a field, and let A be a vector space over K equipped with an additional binary operation from A × A to A, denoted here by · (i.e. if x and y are any two elements of A, x · y is the product of x and y).  Then A is an algebra over K if the following identities hold for all elements x, y, and z of A, and all elements (often called scalars) a and b of K:These three axioms are another way of saying that the binary operation is bilinear. An algebra over K is sometimes also called a K-algebra, and K is called the base field of A. The binary operation is often referred to as multiplication in A. The convention adopted in this article is that multiplication of elements of an algebra is not necessarily associative, although some authors use the term algebra to refer to an associative algebra.Notice that when a binary operation on a vector space is commutative, as in the above example of the complex numbers, it is left distributive exactly when it is right distributive. But in general, for non-commutative operations (such as the next example of the quaternions), they are not equivalent, and therefore require separate axioms.The real numbers may be viewed as a one-dimensional vector space with a compatible multiplication, and hence a one-dimensional algebra over itself. Likewise, as we saw above, the complex numbers form a two-dimensional vector space over the field of real numbers, and hence form a two dimensional algebra over the reals. In both these examples, every non-zero vector has an inverse, making them both division algebras. Although there are no division algebras in 3 dimensions, in 1843, the quaternions were defined and provided the now famous 4-dimensional example of an algebra over the real numbers, where one can not only multiply vectors, but also divide. Any quaternion may be written as (a, b, c, d) = a + bi + cj + dk. Unlike the complex numbers, the quaternions are an example of a non-commutative algebra: for instance, (0,1,0,0) · (0,0,1,0) = (0,0,0,1) but (0,0,1,0) · (0,1,0,0) = (0,0,0,−1).The quaternions were soon followed by several other hypercomplex number systems, which were the early examples of algebras over a field.Previous examples are associative algebras. An example of a non-associative algebra is a three dimensional vector space equipped with the cross product. This is a simple example of a class of nonassociative algebras, which is widely used in mathematics and physics, the Lie algebras.Given K-algebras A and B, a K-algebra homomorphism is a K-linear map f: A → B such that f(xy) = f(x) f(y) for all x,y in A. The space of all K-algebra homomorphisms between A and B is frequently written asA K-algebra isomorphism is a bijective K-algebra homomorphism. For all practical purposes, isomorphic algebras differ only by notation.A subalgebra of an algebra over a field K is a linear subspace that has the property that the product of any two of its elements is again in the subspace. In other words, a subalgebra of an algebra is a subset of elements that is closed under addition, multiplication, and scalar multiplication. In symbols, we say that a subset L of a K-algebra A is a subalgebra if for every x, y in L and c in K, we have that x · y, x + y, and cx are all in L.In the above example of the complex numbers viewed as a two-dimensional algebra over the real numbers, the one-dimensional real line is a subalgebra.A left ideal of a K-algebra is a linear subspace that has the property that any element of the subspace multiplied on the left by any element of the algebra produces an element of the subspace. In symbols, we say that a subset L of a K-algebra A is a left ideal if for every x and y in L, z in A and c in K, we have the following three statements.If (3) were replaced with x · z is in L, then this would define a right ideal. A two-sided ideal is a subset that is both a left and a right ideal. The term ideal on its own is usually taken to mean a two-sided ideal. Of course when the algebra is commutative, then all of these notions of ideal are equivalent. Notice that conditions (1) and (2) together are equivalent to L being a linear subspace of A. It follows from condition (3) that every left or right ideal is a subalgebra.It is important to notice that this definition is different from the definition of an ideal of a ring, in that here we require the condition (2). Of course if the algebra is unital, then condition (3) implies condition (2).Algebras over fields come in many different types. These types are specified by insisting on some further axioms, such as commutativity or associativity of the multiplication operation, which are not required in the broad definition of an algebra. The theories corresponding to the different types of algebras are often very different.An algebra is unital or unitary if it has a unit or identity element I with Ix = x = xI for all x in the algebra.An algebra is called zero algebra if uv = 0 for all u, v in the algebra,[2] not to be confused with the algebra with one element.  It is inherently non-unital (except in the case of only one element), associative and commutative.One may define a unital zero algebra by taking the direct sum of modules of a field (or more generally a ring) K and a K-vector space (or module) V, and defining the product of every pair of elements of V to be zero. That is, if λ, μ ∈ K and u, v ∈ V, then (λ + u) (μ + v) = λμ + (λv + μu). If e1, ... ed is a basis of V, the unital zero algebra is the quotient of the polynomial ring K[E1, ..., En] by the ideal generated by the EiEj for every pair (i, j).An example of unital zero algebra is the algebra of dual numbers, the unital zero R-algebra built from a one dimensional real vector space.These unital zero algebras may be more generally useful, as they allow to translate any general property of the algebras to properties of vector spaces or modules. For example, the theory of Gröbner bases was introduced by Bruno Buchberger for ideals in a polynomial ring R = K[x1, ..., xn] over a field. The construction of the unital zero algebra over a free R-module allows extending this theory as a Gröbner basis theory for sub modules of a free module. This extension allows, for computing a Gröbner basis of a submodule, to use, without any modification, any algorithm and any software for computing Gröbner bases of ideals.Examples detailed in the main article include:The definition of an associative K-algebra with unit is also frequently given in an alternative way. In this case, an algebra over a field K is a ring A together with a ring homomorphismwhere Z(A) is the center of A. Since η is a ring morphism, then one must have either that A is the zero ring, or that η is injective.  This definition is equivalent to that above, with scalar multiplicationgiven byGiven two such associative unital K-algebras A and B, a unital K-algebra morphism f: A → B is a ring morphism that commutes with the scalar multiplication defined by η, which one may write asFor algebras over a field, the bilinear multiplication from A × A to A is completely determined by the multiplication of basis elements of A.Conversely, once a basis for A has been chosen, the products of basis elements can be set arbitrarily, and then extended in a unique way to a bilinear operator on A, i.e., so the resulting multiplication satisfies the algebra laws.Thus, given the field K, any finite-dimensional algebra can be specified up to isomorphism by giving its dimension (say n), and specifying n3 structure coefficients ci,j,k, which are scalars.These structure coefficients determine the multiplication in A via the following rule:where e1,...,en form a basis of A.Note however that several different sets of structure coefficients can give rise to isomorphic algebras.When the algebra can be endowed with a metric, then the structure coefficients are generally written with upper and lower indices, so as to distinguish their transformation properties under coordinate transformations. Specifically, lower indices are covariant indices, and transform via pullbacks, while upper indices are contravariant, transforming under pushforwards. Thus, in mathematical physics, the structure coefficients are often written ci,jk, and their defining rule is written using the Einstein notation asIf you apply this to vectors written in index notation, then this becomesIf K is only a commutative ring and not a field, then the same process works if A is a free module over K. If it isn't, then the multiplication is still completely determined by its action on a set that spans A; however, the structure constants can't be specified arbitrarily in this case, and knowing only the structure constants does not specify the algebra up to isomorphism.Two-dimensional, three-dimensional and four-dimensional unital associative algebras over the field of complex numbers were completely classified up to isomorphism by Eduard Study.[4]There exist two two-dimensional algebras. Each algebra consists of linear combinations (with complex coefficients) of two basis elements, 1 (the identity element) and a. According to the definition of an identity element,It remains to specifyThere exist five three-dimensional algebras. Each algebra consists of linear combinations of three basis elements, 1 (the identity element), a and b. Taking into account the definition of an identity element, it is sufficient to specifyThe fourth algebra is non-commutative, others are commutative.In some areas of mathematics, such as commutative algebra, it is common to consider the more general concept of an algebra over a ring, where a commutative unital ring R replaces the field K. The only part of the definition that changes is that A is assumed to be an R-module (instead of a vector space over K). 
Frame (linear algebra)
In linear algebra, a frame of an inner product space is a generalization of a basis of a vector space to sets that may be linearly dependent. In the terminology of signal processing, a frame provides a redundant, stable way of representing a signal.[1] Frames are used in error detection and correction and the design and analysis of filter banks and more generally in applied mathematics, computer science, and engineering.[2].A set of vectors that satisfies the frame condition is a frame for the vector space.[3]The numbers A and B are called the lower and upper frame bounds, respectively.[3] The frame bounds are not unique because numbers less than A and greater than B are also valid frame bounds. The optimal lower bound is the supremum of all lower bounds and the optimal upper bound is the infimum of all upper bounds.A frame is called overcomplete (or redundant) if it is not a basis for the vector space.By using this definition we may rewrite the frame condition asBecause of the various mathematical components surrounding frames, frame theory has roots in harmonic and functional analysis, operator theory, linear algebra, and matrix theory.[6]The Fourier transform has been used for over a century as a way of decomposing and expanding signals. However, the Fourier transform masks key information regarding the moment of emission and the duration of a signal.  In 1946, Dennis Gabor was able to solve this using a technique that simultaneously reduced noise, provided resiliency, and created quantization while encapsulating important signal characteristics.[1] This discovery marked the first concerted effort towards frame theory.The frame condition was first described by Richard Duffin and Albert Charles Schaeffer in a 1952 article on nonharmonic Fourier series as a way of computing the coefficients in a linear combination of the vectors of a linearly dependent spanning set (in their terminology, a "Hilbert space frame").[7] In the 1980s, Stéphane Mallat, Ingrid Daubechies, and Yves Meyer used frames to analyze wavelets. Today frames are associated with wavelets, signal and image processing, and data compression.A frame satisfies a generalization of Parseval's identity, namely the frame condition, while still maintaining norm equivalence between a signal and its sequence of coefficients.In signal processing, each vector is interpreted as a signal. In this interpretation, a vector expressed as a linear combination of the frame vectors is a redundant signal. Using a frame, it is possible to create a simpler, more sparse representation of a signal as compared with a family of elementary signals (that is, representing a signal strictly with a set of linearly independent vectors may not always be the most compact form).[8] Frames, therefore, provide robustness. Because they provide a way of producing the same vector within a space, signals can be encoded in various ways. This facilitates fault tolerance and resilience to a loss of signal.  Finally, redundancy can be used to mitigate noise, which is relevant to the restoration, enhancement, and reconstruction of signals.In signal processing, it is common to assume the vector space is a Hilbert space.A frame is a tight frame if A = B; in other words, the frame satisfies a generalized version of Parseval's identity. For example, the union of k disjoint orthonormal bases of a vector space is a tight frame with A = B = k. A tight frame is a Parseval frame (sometimes called a normalized frame) if A = B = 1. Each orthonormal basis is a Parseval frame, but the converse is not always true.A frame is an exact frame if no proper subset of the frame spans the inner product space. Each basis for an inner product space is an exact frame for the space (so a basis is a special case of a frame).A Bessel Sequence is a set of vectors that satisfies only the upper bound of the frame condition.and we see that Continuous Frames are indeed the natural generalization of the frames mentioned above.Just like in the discrete case we can define the Analysis, Synthesis, and Frame operators when dealing with continuous frames.It is defined as follows:The adjoint operator of the Continuous Analysis Operator is the Continuous Synthesis Operator which is the map:which, when substituted in the frame condition inequality, yieldsThuswhich proves thatAlternatively, we can letwhich shows that
Commutative ring
In ring theory, a branch of abstract algebra, a commutative ring is a ring in which the multiplication operation is commutative. The study of commutative rings is called commutative algebra. Complementarily, noncommutative algebra is the study of noncommutative rings where multiplication is not required to be commutative.A ring is a set R equipped with two binary operations, i.e. operations combining any two elements of the ring to a third. They are called addition and multiplication and commonly denoted by "+" and "⋅"; e.g. a + b and a ⋅ b. To form a ring these two operations have to satisfy a number of properties: the ring has to be an abelian group under addition as well as a monoid under multiplication, where multiplication distributes over addition; i.e., a ⋅ (b + c) = (a ⋅ b) + (a ⋅ c). The identity elements for addition and multiplication are denoted 0 and 1, respectively.If the multiplication is commutative, i.e.then the ring R is called commutative. In the remainder of this article, all rings will be commutative, unless explicitly stated otherwise.An important example, and in some sense crucial, is the ring of integers Z with the two operations of addition and multiplication. As the multiplication of integers is a commutative operation, this is a commutative ring. It is usually denoted Z as an abbreviation of the German word Zahlen (numbers).If R is a given commutative ring, then the set of all polynomials in the variable X whose coefficients are in R forms the polynomial ring, denoted R[X]. The same holds true for several variables.If V is some topological space, for example a subset of some Rn, real- or complex-valued continuous functions on V form a commutative ring. The same is true for differentiable or holomorphic functions, when the two concepts are defined, such as for V a complex manifold.In contrast to fields, where every nonzero element is multiplicatively invertible, the concept of divisibility for rings is richer. An element a of ring R is called a unit if it possesses a multiplicative inverse. Another particular type of element is the zero divisors, i.e. a non-zero element a such that there exists a non-zero element b of the ring such that ab = 0. If R possesses no zero divisors, it is called an integral domain (or domain). An element a satisfying an = 0 for some positive integer n is called nilpotent.The localization of a ring is a process in which some elements are rendered invertible, i.e. multiplicative inverses are added to the ring. Concretely, if S is a multiplicatively closed subset of R (i.e. whenever s, t ∈ S then so is st) then the localization of R at S, or ring of fractions with denominators in S, usually denoted S−1R consists of symbolssubject to certain rules that mimic the cancellation familiar from rational numbers. Indeed, in this language Q is the localization of Z at all nonzero integers. This construction works for any integral domain R instead of Z. The localization (R \ {0})−1R is a field, called the quotient field of R.Many of the following notions also exist for not necessarily commutative rings, but the definitions and properties are usually more complicated. For example, all ideals in a commutative ring are automatically two-sided, which simplifies the situation considerably.For a ring R, an R-module M is like what a vector space is to a field. That is, elements in a module can be added; they can be multiplied by elements of R subject to the same axioms as for a vector space. The study of modules is significantly more involved than the one of vector spaces in linear algebra, since several features of vector spaces fail for modules in general: modules need not be free, i.e., of the formEven for free modules, the rank of a free module (i.e. the analog of the dimension of vector spaces) may not be well-defined. Finally, submodules of finitely generated modules need not be finitely generated (unless R is Noetherian, see below).Ideals of a ring R are the submodules of R, i.e., the modules contained in R. In more detail, an ideal I is a non-empty subset of R such that for all r in R, i and j in I, both ri and i + j are in I. For various applications, understanding the ideals of a ring is of particular importance, but often one proceeds by studying modules in general.Any ring has two ideals, namely the zero ideal {0} and R, the whole ring. These two ideals are the only ones precisely if R is a field. Given any subset F = {fj}j ∈ J of R (where J is some index set), the ideal generated by F is the smallest ideal that contains F. Equivalently, it is given by finite linear combinationsIf F consists of a single element r, the ideal generated by F consists of the multiples of r, i.e., the elements of the form rs for arbitrary elements s. Such an ideal is called a principal ideal. If every ideal is a principal ideal, R is called a principal ideal ring; two important cases are Z and k[X], the polynomial ring over a field k. These two are in addition domains, so they are called principal ideal domains.Unlike for general rings, for a principal ideal domain, the properties of individual elements are strongly tied to the properties of the ring as a whole. For example, any principal ideal domain R is a unique factorization domain (UFD) which means that any element is a product of irreducible elements, in a (up to reordering of factors) unique way. Here, an element a in a domain is called irreducible if the only way of expressing it as a productis by either b or c being a unit. An example, important in field theory, are irreducible polynomials, i.e., irreducible elements in k[X], for a field k. The fact that Z is a UFD can be stated more elementarily by saying that any natural number can be uniquely decomposed as product of powers of prime numbers. It is also known as the fundamental theorem of arithmetic.An element a is a prime element if whenever a divides a product bc, a divides b or c. In a domain, being prime implies being irreducible. The converse is true in a unique factorization domain, but false in general.The definition of ideals is such that "dividing" I "out" gives another ring, the factor ring R / I: it is the set of cosets of I together with the operationsFor example, the ring Z/nZ (also denoted Zn), where n is an integer, is the ring of integers modulo n. It is the basis of modular arithmetic.An ideal is proper if it is strictly smaller than the whole ring. An ideal that is not strictly contained in any proper ideal is called maximal.  An ideal m is maximal if and only if R / m is a field.  Except for the zero ring, any ring (with identity) possesses at least one maximal ideal; this follows from Zorn's lemma.A ring is called Noetherian (in honor of Emmy Noether, who developed this concept) if every ascending chain of idealsbecomes stationary, i.e. becomes constant beyond some index n. Equivalently, any ideal is generated by finitely many elements, or, yet equivalent, submodules of finitely generated modules are finitely generated.Being Noetherian is a highly important finiteness condition, and the condition is preserved under many operations that occur frequently in geometry. For example, if R is Noetherian, then so is the polynomial ring R[X1, X2, ..., Xn] (by Hilbert's basis theorem), any localization S−1R, and also any factor ring R / I.Any non-noetherian ring R is the union of its Noetherian subrings. This fact, known as Noetherian approximation, allows the extension of certain theorems to non-Noetherian rings.A ring is called Artinian (after Emil Artin), if every descending chain of idealsbecomes stationary eventually. Despite the two conditions appearing symmetric, Noetherian rings are much more general than Artinian rings. For example, Z is Noetherian, since every ideal can be generated by one element, but is not Artinian, as the chainshows. In fact, by the Hopkins–Levitzki theorem, every Artinian ring is Noetherian. More precisely, Artinian rings can be characterized as the Noetherian rings whose Krull dimension is zero.As was mentioned above, Z is a unique factorization domain. This is not true for more general rings, as algebraists realized in the 19th century. For example, inthere are two genuinely distinct ways of writing 6 as a product:Any maximal ideal is a prime ideal or, more briefly, is prime. Moreover, an ideal I is prime if and only if the factor ring R / I is an integral domain. Proving that an ideal is prime, or equivalently that a ring has no zero-divisors can be very difficult. Yet another way of expressing the same is to say that the complement R \ p is multiplicatively closed. The localisation (R \ p)−1R is important enough to have its own notation: Rp. This ring has only one maximal ideal, namely pRp. Such rings are called local.The spectrum of a ring R,[nb 1]  denoted by Spec R, is the set of all prime ideals of R. It is equipped with a topology, the Zariski topology, which reflects the algebraic properties of R: a basis of open subsets is given byInterpreting f as a function that takes the value f mod p (i.e., the image of f in the residue field R/p), this subset is the locus where f is non-zero. The spectrum also makes precise the intuition that localisation and factor rings are complementary: the natural maps R → Rf and R → R / fR correspond, after endowing the spectra of the rings in question with their Zariski topology, to complementary open and closed immersions respectively. Even for basic rings, such as illustrated for R = Z at the right, the Zariski topology is quite different from the one on the set of real numbers.The spectrum contains the set of maximal ideals, which is occasionally denoted mSpec (R). For an algebraically closed field k, mSpec (k[T1, ..., Tn] / (f1, ..., fm)) is in bijection with the setThus, maximal ideals reflect the geometric properties of solution sets of polynomials, which is an initial motivation for the study of commutative rings. However, the consideration of non-maximal ideals as part of the geometric properties of a ring is useful for several reasons. For example, the minimal prime ideals (i.e., the ones not strictly containing smaller ones) correspond to the irreducible components of Spec R. For a Noetherian ring R, Spec R has only finitely many irreducible components. This is a geometric restatement of primary decomposition, according to which any ideal can be decomposed as a product of finitely many primary ideals. This fact is the ultimate generalization of the decomposition into prime ideals in Dedekind rings.The resulting equivalence of the two said categories aptly reflects algebraic properties of rings in a geometrical manner.Similar to the fact that manifolds are locally given by open subsets of Rn, affine schemes are local models for schemes, which are the object of study in algebraic geometry. Therefore, several notions concerning commutative rings stem from geometric intuition.The Krull dimension (or dimension) dim R of a ring R measures the "size" of a ring by, roughly speaking, counting independent elements in R. The dimension of algebras over a field k can be axiomatized by four properties:The dimension is defined, for any ring R, as the supremum of lengths n of chains of prime idealsFor example, a field is zero-dimensional, since the only prime ideal is the zero ideal. The integers are one-dimensional, since chains are of the form (0) ⊊ (p), where p is a prime number. For non-Noetherian rings, and also non-local rings, the dimension may be infinite, but Noetherian local rings have finite dimension. Among the four axioms above, the first two are elementary consequences of the definition, whereas the remaining two hinge on important facts in commutative algebra, the going-up theorem and Krull's principal ideal theorem.A ring homomorphism or, more colloquially, simply a map, is a map f : R → S such thatThese conditions ensure f(0) = 0. Similarly as for other algebraic structures, a ring homomorphism is thus a map that is compatible with the structure of the algebraic objects in question. In such a situation S is also called an R-algebra, by understanding that s in S may be multiplied by some r of R, by settingThe kernel and image of f are defined by ker (f) = {r ∈ R, f(r) = 0} and im (f) = f(R) = {f(r), r ∈ R}. The kernel is an ideal of R, and the image is a subring of S.A ring homomorphism is called an isomorphism if it is bijective. An example of a ring isomorphism, known as the Chinese remainder theorem, iswhere n = p1p2...pk is a product of pairwise distinct prime numbers.Commutative rings, together with ring homomorphisms, form a category. The ring Z is the initial object in this category, which means that for any commutative ring R, there is a unique ring homomorphism Z → R. By means of this map, an integer n can be regarded as an element of R. For example, the binomial formulawhich is valid for any two elements a and b in any commutative ring R is understood in this sense by interpreting the binomial coefficients as elements of R using this map.Given two R-algebras S and T, their tensor productis again a commutative R-algebra. In some cases, the tensor product can serve to find a T-algebra which relates to Z as S relates to R. For example,An R-algebra S is called finitely generated (as an algebra) if there are finitely many elements s1, ..., sn such that any element of s is expressible as a polynomial in the si. Equivalently, S is isomorphic toA much stronger condition is that S is finitely generated as an R-module, which means that any s can be expressed as a R-linear combination of some finite set s1, ..., sn.A ring is called local if it has only a single maximal ideal, denoted by m. For any (not necessarily local) ring R, the localizationat a prime ideal p is local. This localization reflects the geometric properties of Spec R "around p". Several notions and problems in commutative algebra can be reduced to the case when R is local, making local rings a particularly deeply studied class of rings. The residue field of R is defined asAny R-module M yields a k-vector space given by M / mM. Nakayama's lemma shows this passage is preserving important information: a finitely generated module M is zero if and only if M / mM is zero.The k-vector space m/m2 is an algebraic incarnation of the cotangent space. Informally, the elements of m can be thought of as functions which vanish at the point p, whereas m2 contains the ones which vanish with order at least 2. For any Noetherian local ring R, the inequalityholds true, reflecting the idea that the cotangent (or equivalently the tangent) space has at least the dimension of the space Spec R. If equality holds true in this estimate, R is called a regular local ring. A Noetherian local ring is regular if and only if the ring (which is the ring of functions on the tangent cone)is isomorphic to a polynomial ring over k. Broadly speaking, regular local rings are somewhat similar to polynomial rings.[1] Regular local rings are UFD's.[2]Discrete valuation rings are equipped with a function which assign an integer to any element r. This number, called the valuation of r can be informally thought of as a zero or pole order of r. Discrete valuation rings are precisely the one-dimensional regular local rings. For example, the ring of germs of holomorphic functions on a Riemann surface is a discrete valuation ring.By Krull's principal ideal theorem, a foundational result in the dimension theory of rings, the dimension ofis at least r − n. A ring R is called a complete intersection ring if it can be presented in a way that attains this minimal bound. This notion is also mostly studied for local rings. Any regular local ring is a complete intersection ring, but not conversely.A ring R is a set-theoretic complete intersection if the reduced ring associated to R, i.e., the one obtained by dividing out all nilpotent elements, is a complete intersection. As of 2017, it is in general unknown, whether curves in three-dimensional space are set-theoretic complete intersections.[3]The depth of a local ring R is the number of elements in some (or, as can be shown, any) maximal regular sequence, i.e., a sequence a1, ..., an ∈ m such that all ai are non-zero divisors inFor any local Noetherian ring, the inequalityholds. A local ring in which equality takes place is called a Cohen–Macaulay ring. Local complete intersection rings, and a fortiori, regular local rings are Cohen–Macaulay, but not conversely. Cohen–Macaulay combine desirable properties of regular rings (such as the property of being universally catenary rings, which means that the (co)dimension of primes is well-behaved), but are also more robust under taking quotients than regular local rings.[4]There are several ways to construct new rings out of given ones. The aim of such constructions is often to improve certain properties of the ring so as to make it more readily understandable. For example, an integral domain that is integrally closed in its field of fractions is called normal. This is a desirable property, for example any normal one-dimensional ring is necessarily regular. Rendering[clarification needed] a ring normal is known as normalization.If I is an ideal in a commutative ring R, the powers of I form topological neighborhoods of 0 which allow R to be viewed as a topological ring. This topology is called the I-adic topology. R can then be completed with respect to this topology. Formally, the I-adic completion is the inverse limit of the rings R/In. For example, if k is a field, k[[X]], the formal power series ring in one variable over k, is the I-adic completion of k[X] where I is the principal ideal generated by X. This ring serves as an algebraic analogue of the disk. Analogously, the ring of p-adic integers is the completion of Z  with respect to the principal ideal (p). Any ring that is isomorphic to its own completion, is called complete.Complete local rings satisfy Hensel's lemma, which roughly speaking allows extending solutions (of various problems) over the residue field k to R.Several deeper aspects of commutative rings have been studied using methods from homological algebra. Hochster (2007) lists some open questions in this area of active research.Projective modules can be defined to be the direct summands of free modules. If R is local, any finitely generated projective module is actually free, which gives content to an analogy between projective modules and vector bundles.[5] The Quillen–Suslin theorem asserts that any finitely generated projective module over k[T1, ..., Tn] (k a field) is free, but in general these two concepts differ. A local Noetherian ring is regular if and only if its global dimension is finite, say n, which means that any finitely generated R-module has a resolution by projective modules of length at most n.The proof of this and other related statements relies on the usage of homological methods, such as theExt functor. This functor is the derived functor of the functorThe latter functor is exact if M is projective, but not otherwise: for a surjective map E → F of R-modules, a map M → F need not extend to a map M → E. The higher Ext functors measure the non-exactness of the Hom-functor. The importance of this standard construction in homological algebra stems can be seen from the fact that a local Noetherian ring R with residue field k is regular if and only ifvanishes for all large enough n. Moreover, the dimensions of these Ext-groups, known as Betti numbers, grow polynomially in n if and only if R is a local complete intersection ring.[6] A key argument in such considerations is the Koszul complex, which provides an explicit free resolution of the residue field k of a local ring R in terms of a regular sequence.The tensor product is another non-exact functor relevant in the context of commutative rings: for a general R-module M, the functoris only right exact. If it is exact, M is called flat. Despite being defined in terms of homological algebra, flatness has profound geometric implications. For example, if an R-algebra S is flat, the dimensions of the fibers(for prime ideals p in R) have the "expected" dimension, namely dim S − dim R + dim (R / p).By Wedderburn's theorem, every finite division ring is commutative, and therefore a finite field. Another condition ensuring commutativity of a ring, due to Jacobson, is the following: for every element r of R there exists an integer n > 1 such that rn = r.[7] If, r2 = r for every r, the ring is called Boolean ring. More general conditions which guarantee commutativity of a ring are also known.[8]A graded ring R = ⨁i∊Z Ri is called graded-commutative ifIf the Ri are connected by differentials ∂ such that an abstract form of the product rule holds, i.e.,R is called a commutative differential graded algebra (cdga). An example is the complex of differential forms on a manifold, with the multiplication given by the exterior product, is a cdga. The cohomology of a cdga is a graded-commutative ring, sometimes referred to as the cohomology ring. A broad range examples of graded rings arises in this way. For example, the Lazard ring is the ring of cobordism classes of complex manifolds.A graded-commutative ring with respect to a grading  by Z/2 (as opposed to Z) is called a superalgebra.A related notion is an almost commutative ring, which means that R is filtered in such a way that the associated graded ringis commutative. An example is the Weyl algebra and more general rings of differential operators.A simplicial commutative ring is a simplicial object in the category of commutative rings. They are building blocks for (connective) derived algebraic geometry. A closely related but more general notion is that of E∞-ring.The ring of matrices is not commutative, since matrix multiplication fails to be commutative.[9]However, any two matrices A and B that do commute can be simultaneously diagonalized, i.e., there is an invertible matrix P such that both PAP−1 and PBP−1 are diagonal matrices. This fact makes representations of commutative Lie groups particularly simpler to understand than in general.An example is the set of matrices of divided differences with respect to a fixed set of nodes.
Manifold
In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point.  More precisely, each point of an n-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension n. In this more precise terminology, a manifold is referred to as an n-manifold.One-dimensional manifolds include lines and circles, but not figure eights (because they have crossing points that are not locally homeomorphic to Euclidean 1-space).  Two-dimensional manifolds are also called surfaces.  Examples include the plane, the sphere, and the torus, which can all be embedded (formed without self-intersections) in three dimensional real space, but also the Klein bottle and real projective plane, which will always self-intersect when immersed in three-dimensional real space.Although a manifold locally resembles Euclidean space, meaning that every point has a neighborhood homeomorphic to an open subset of Euclidean space, globally it may not: manifolds in general are not homeomorphic to Euclidean space.  For example, the surface of the sphere is not homeomorphic to the Euclidean plane, because (among other properties) it has the global topological property of compactness that Euclidean space lacks, but in a region it can be charted by means of map projections of the region into the Euclidean plane (in the context of manifolds they are called charts).  When a region appears in two neighbouring charts, the two representations do not coincide exactly and a transformation is needed to pass from one to the other, called a transition map.The concept of a manifold is central to many parts of geometry and modern mathematical physics because it allows complicated structures to be described and understood in terms of the simpler local topological properties of Euclidean space.  Manifolds naturally arise as solution sets of systems of equations and as graphs of functions.Manifolds can be equipped with additional structure.  One important class of manifolds is the class of differentiable manifolds; this differentiable structure allows calculus to be done on manifolds.  A Riemannian metric on a manifold allows distances and angles to be measured.  Symplectic manifolds serve as the phase spaces in the Hamiltonian formalism of classical mechanics, while four-dimensional Lorentzian manifolds model spacetime in general relativity.A surface is a two dimensional manifold, meaning that it locally resembles the Euclidean plane near each point.  For example, the surface of a globe can be described by a collection of maps (called charts), which together form an atlas of the globe.  Although no individual map is sufficient to cover the entire surface of the globe, any place in the globe will be in at least one of the charts.  Many places will appear in more than one chart.  For example, a map of North America will likely include parts of South America and the Arctic circle.  These regions of the globe will be described in full in separate charts, which in turn will contain parts of North America.  There is a relation between adjacent charts, called a transition map that allows them to be consistently patched together to cover the whole of the globe.Describing the coordinate charts on surfaces explicitly requires knowledge of functions of two variables, because these patching functions must map a region in the plane to another region of the plane.  However, one-dimensional examples of manifolds (or curves) can be described with functions of a single variable only.Manifolds have applications in computer-graphics and augmented-reality given the need to associate pictures(texture) to coordinates (i.e CT scans ).In an augmented reality setting, a picture (tangent plane) can be seen as something associated with a coordinate  and by using sensors for detecting movements and rotation one can have knowledge of how the picture is oriented and placed in space. After a line, the circle is the simplest example of a topological manifold. Topology ignores bending, so a small piece of a circle is treated exactly the same as a small piece of a line. Consider, for instance, the top part of the unit circle, x2 + y2 = 1, where the y-coordinate is positive (indicated by the yellow circular arc in Figure 1). Any point of this arc can be uniquely described by its x-coordinate. So, projection onto the first coordinate is a continuous, and invertible, mapping from the upper arc to the open interval (−1, 1):Such functions along with the open regions they map are called charts. Similarly, there are charts for the bottom (red), left (blue), and right (green) parts of the circle:Together, these parts cover the whole circle and the four charts form an atlas for the circle.Such a function is called a transition map.The top, bottom, left, and right charts show that the circle is a manifold, but they do not form the only possible atlas. Charts need not be geometric projections, and the number of charts is a matter of choice. Consider the chartsandHere s is the slope of the line through the point at coordinates (x,y) and the fixed pivot point (−1, 0); t follows similarly, but with pivot point (+1, 0). The inverse mapping from s to (x, y) is given byIt can easily be confirmed that x2 + y2 = 1 for all values of the slope s. These two charts provide a second atlas for the circle, withEach chart omits a single point, either (−1, 0) for s or (+1, 0) for t, so neither chart alone is sufficient to cover the whole circle. It can be proved that it is not possible to cover the full circle with a single chart. For example, although it is possible to construct a circle from a single line interval by overlapping and "gluing" the ends, this does not produce a chart; a portion of the circle will be mapped to both ends at once, losing invertibility.The sphere is an example of a surface. The unit sphere of implicit equationmay be covered by an atlas of six charts: the plane z = 0 divides the sphere into two half spheres (z > 0 and z < 0), which may both be mapped on the disc x2 + y2 < 1 by the projection on the xy plane of coordinates. This provides two charts; the four other charts are provided by a similar construction with the two other coordinate planes.As for the circle, one may define one chart that covers the whole sphere excluding one point. Thus two charts are sufficient, but the sphere cannot be covered by a single chart.This example is historically significant, as it has motivated the terminology; it became apparent that the whole surface of the Earth cannot have a plane representation consisting of a single map (also called "chart", see nautical chart), and therefore one needs atlases for covering the whole Earth surface.Viewed using calculus, the circle transition function T is simply a function between open intervals, which gives a meaning to the statement that T is differentiable. The transition map T, and all the others, are differentiable on (0, 1); therefore, with this atlas the circle is a differentiable manifold. It is also smooth and analytic because the transition functions have these properties as well.Other circle properties allow it to meet the requirements of more specialized types of manifold. For example, the circle has a notion of distance between two points, the arc-length between the points; hence it is a Riemannian manifold.Manifolds need not be connected (all in "one piece"); an example is a pair of separate circles.Manifolds need not be closed; thus a line segment without its end points is a manifold. And they are never countable, unless the dimension of the manifold is 0. Putting these freedoms together, other examples of manifolds are a parabola, a hyperbola (two open, infinite pieces), and the locus of points on a cubic curve y2 = x3 − x (a closed loop piece and an open, infinite piece).However, excluded are examples like two touching circles that share a point to form a figure-8; at the shared point a satisfactory chart cannot be created. Even with the bending allowed by topology, the vicinity of the shared point looks like a "+", not a line. A "+" is not homeomorphic to a closed interval (line segment), since deleting the center point from the "+" gives a space with four components (i.e. pieces), whereas deleting a point from a closed interval gives a space with at most two pieces; topological operations always preserve the number of pieces.Informally, a manifold is a space that is "modeled on" Euclidean space.There are many different kinds of manifolds, depending on the context.  In geometry and topology, all manifolds are topological manifolds, possibly with additional structure, such as a differentiable structure. A manifold can be constructed by giving a collection of coordinate charts, that is a covering by open sets with homeomorphisms to a Euclidean space, and patching functions: homeomorphisms from one region of Euclidean space to another region if they correspond to the same part of the manifold in two different coordinate charts.  A manifold can be given additional structure if the patching functions satisfy axioms beyond continuity. For instance, differentiable manifolds have homeomorphisms on overlapping neighborhoods diffeomorphic with each other, so that the manifold has a well-defined set of functions which are differentiable in each neighborhood, and so differentiable on the manifold as a whole.Formally, a (topological) manifold is a second countable Hausdorff space that is locally homeomorphic to Euclidean space.Second countable and Hausdorff are point-set conditions; second countable excludes spaces which are in some sense 'too large' such as the long line, while Hausdorff excludes spaces such as "the line with two origins" (these generalizations of manifolds are discussed in non-Hausdorff manifolds).Locally homeomorphic to Euclidean space means that every point has a neighborhood homeomorphic to an open Euclidean n-ball,Generally manifolds are taken to have a fixed dimension (the space must be locally homeomorphic to a fixed n-ball), and such a space is called an n-manifold; however, some authors admit manifolds where different points can have different dimensions.[1] If a manifold has a fixed dimension, it is called a pure manifold. For example, the sphere has a constant dimension of 2 and is therefore a pure manifold whereas the disjoint union of a sphere and a line in three-dimensional space is not a pure manifold. Since dimension is a local invariant (i.e. the map sending each point to the dimension of its neighbourhood over which a chart is defined, is locally constant), each connected component has a fixed dimension.Scheme-theoretically, a manifold is a locally ringed space, whose structure sheaf is locally isomorphic to the sheaf of continuous (or differentiable, or complex-analytic, etc.) functions on Euclidean space. This definition is mostly used when discussing analytic manifolds in algebraic geometry.The spherical Earth is navigated using flat maps or charts, collected in an atlas. Similarly, a differentiable manifold can be described using mathematical maps, called coordinate charts, collected in a mathematical atlas. It is not generally possible to describe a manifold with just one chart, because the global structure of the manifold is different from the simple structure of the charts.  For example, no single flat map can represent the entire Earth without separation of adjacent features across the map's boundaries or duplication of coverage. When a manifold is constructed from multiple overlapping charts, the regions where they overlap carry information essential to understanding the global structure.A coordinate map, a coordinate chart, or simply a chart, of a manifold is an invertible map between a subset of the manifold and a simple space such that both the map and its inverse preserve the desired structure.[2] For a topological manifold, the simple space is a subset of some Euclidean space Rn and interest focuses on the topological structure. This structure is preserved by homeomorphisms, invertible maps that are continuous in both directions.In the case of a differentiable manifold, a set of charts called an atlas allows us to do calculus on manifolds. Polar coordinates, for example, form a chart for the plane R2 minus the positive x-axis and the origin.  Another example of a chart is the map χtop mentioned in the section above, a chart for the circle.The description of most manifolds requires more than one chart (a single chart is adequate for only the simplest manifolds). A specific collection of charts which covers a manifold is called an atlas. An atlas is not unique as all manifolds can be covered multiple ways using different combinations of charts. Two atlases are said to be equivalent if their union is also an atlas.The atlas containing all possible charts consistent with a given atlas is called the maximal atlas (i.e. an equivalence class containing that given atlas (under the already defined equivalence relation given in the previous paragraph)). Unlike an ordinary atlas, the maximal atlas of a given manifold is unique. Though it is useful for definitions, it is an abstract object and not used directly (e.g. in calculations).Charts in an atlas may overlap and a single point of a manifold may be represented in several charts. If two charts overlap, parts of them represent the same region of the manifold, just as a map of Europe and a map of Asia may both contain Moscow.  Given two overlapping charts, a transition function can be defined which goes from an open ball in Rn to the manifold and then back to another (or perhaps the same) open ball in Rn.  The resultant map, like the map T in the circle example above, is called a change of coordinates, a coordinate transformation, a transition function, or a transition map.An atlas can also be used to define additional structure on the manifold. The structure is first defined on each chart separately. If all the transition maps are compatible with this structure, the structure transfers to the manifold.This is the standard way differentiable manifolds are defined. If the transition functions of an atlas for a topological manifold preserve the natural differential structure of Rn (that is, if they are diffeomorphisms), the differential structure transfers to the manifold and turns it into a differentiable manifold.  Complex manifolds are introduced in an analogous way by requiring that the transition functions of an atlas are holomorphic functions.  For symplectic manifolds, the transition functions must be symplectomorphisms.The structure on the manifold depends on the atlas, but sometimes different atlases can be said to give rise to the same structure. Such atlases are called compatible.These notions are made precise in general through the use of pseudogroups.A manifold with boundary is a manifold with an edge. For example, a sheet of paper is a 2-manifold with a 1-dimensional boundary. The boundary of an n-manifold with boundary is an (n − 1)-manifold.  A disk (circle plus interior) is a 2-manifold with boundary.  Its boundary is a circle, a 1-manifold. A square with interior is also a 2-manifold with boundary. A ball (sphere plus interior) is a 3-manifold with boundary.  Its boundary is a sphere, a 2-manifold.  (See also Boundary (topology)).In technical language, a manifold with boundary is a space containing both interior points and boundary points.  Every interior point has a neighborhood homeomorphic to the open n-ball {(x1, x2, …, xn) | Σxi2 < 1} .  Every boundary point has a neighborhood homeomorphic to the "half" n-ball  {(x1, x2, …, xn) | Σxi2 < 1 and x1 ≥ 0} .  The homeomorphism must send each boundary point to a point with x1 = 0.Let M be a manifold with boundary. The interior of M, denoted Int M, is the set of points in M which have neighborhoods homeomorphic to an open subset of Rn. The boundary of M, denoted ∂M, is the complement of Int M in M. The boundary points can be characterized as those points which land on the boundary hyperplane (xn = 0) of Rn+ under some coordinate chart.If M is a manifold with boundary of dimension n, then Int M is a manifold (without boundary) of dimension n and ∂M is a manifold (without boundary) of dimension n − 1.A single manifold can be constructed in different ways, each stressing a different aspect of the manifold, thereby  leading to a slightly different viewpoint.Perhaps the simplest way to construct a manifold is the one used in the example above of the circle. First, a subset of R2 is identified, and then an atlas covering this subset is constructed. The concept of manifold grew historically from constructions like this. Here is another example, applying this method to the construction of a sphere:A sphere can be treated in almost the same way as the circle. In mathematics a sphere is just the surface (not the solid interior), which can be defined as a subset of R3:The sphere is two-dimensional, so each chart will map part of the sphere to an open subset of R2. Consider the northern hemisphere, which is the part with positive z coordinate (coloured red in the picture on the right). The function χ defined bymaps the northern hemisphere to the open unit disc by projecting it on the (x, y) plane. A similar chart exists for the southern hemisphere. Together with two charts projecting on the (x, z) plane and two charts projecting on the (y, z) plane, an atlas of six charts is obtained which covers the entire sphere.This can be easily generalized to higher-dimensional spheres.A manifold can be constructed by gluing together pieces in a consistent manner, making them into overlapping charts. This construction is possible for any manifold and hence it is often used as a characterisation, especially for differentiable and Riemannian manifolds. It focuses on an atlas, as the patches naturally provide charts, and since there is no exterior space involved it leads to an intrinsic view of the manifold.The manifold is constructed by specifying an atlas, which is itself defined by transition maps. A point of the manifold is therefore an equivalence class of points which are mapped to each other by transition maps. Charts map equivalence classes to points of a single patch. There are usually strong demands on the consistency of the transition maps. For topological manifolds they are required to be homeomorphisms; if they are also diffeomorphisms, the resulting manifold is a differentiable manifold.This can be illustrated with the transition map t = 1⁄s from the second half of the circle example. Start with two copies of the line. Use the coordinate s for the first copy, and t for the second copy. Now, glue both copies together by identifying the point t on the second copy with the point s = 1⁄t on the first copy (the points t = 0 and s = 0 are not identified with any point on the first and second copy, respectively). This gives a circle.The first construction and this construction are very similar, but they represent rather different points of view. In the first construction, the manifold is seen as embedded in some Euclidean space. This is the extrinsic view. When a manifold is viewed in this way, it is easy to use intuition from Euclidean spaces to define additional structure. For example, in a Euclidean space it is always clear whether a vector at some point is tangential or normal to some surface through that point.The patchwork construction does not use any embedding, but simply views the manifold as a topological space by itself. This abstract point of view is called the intrinsic view. It can make it harder to imagine what a tangent vector might be, and there is no intrinsic notion of a normal bundle, but instead there is an intrinsic stable normal bundle.The n-sphere Sn is a generalisation of the idea of a circle (1-sphere) and sphere (2-sphere) to higher dimensions. An n-sphere Sn can be constructed by gluing together two copies of Rn. The transition map between them is defined asThis function is its own inverse and thus can be used in both directions. As the transition map is a smooth function, this atlas defines a smooth manifold.In the case n = 1, the example simplifies to the circle example given earlier.It is possible to define different points of a manifold to be same. This can be visualized as gluing these points together in a single point, forming a quotient space. There is, however, no reason to expect such quotient spaces to be manifolds. Among the possible quotient spaces that are not necessarily manifolds, orbifolds and CW complexes are considered to be relatively well-behaved. An example of a quotient space of a manifold that is also a manifold is the real projective space identified as a quotient space of the corresponding sphere.One method of identifying points (gluing them together) is through a right (or left) action of a group, which acts on the manifold. Two points are identified if one is moved onto the other by some group element. If M is the manifold and G is the group, the resulting quotient space is denoted by M / G (or G \ M).Manifolds which can be constructed by identifying points include tori and real projective spaces (starting with a plane and a sphere, respectively).Two manifolds with boundaries can be glued together along a boundary. If this is done the right way, the result is also a manifold. Similarly, two boundaries of a single manifold can be glued together.Formally, the gluing is defined by a bijection between the two boundaries[dubious  – discuss]. Two points are identified when they are mapped onto each other. For a topological manifold this bijection should be a homeomorphism, otherwise the result will not be a topological manifold. Similarly for a differentiable manifold it has to be a diffeomorphism. For other manifolds other structures should be preserved.A finite cylinder may be constructed as a manifold by starting with a strip [0, 1] × [0, 1] and gluing a pair of opposite edges on the boundary by a suitable diffeomorphism. A projective plane may be obtained by gluing a sphere with a hole in it to a Möbius strip along their respective circular boundaries.The Cartesian product of manifolds is also a manifold.The dimension of the product manifold is the sum of the dimensions of its factors. Its topology is the product topology, and a Cartesian product of charts is a chart for the product manifold. Thus, an atlas for the product manifold can be constructed using atlases for its factors. If these atlases define a differential structure on the factors, the corresponding atlas defines a differential structure on the product manifold. The same is true for any other structure defined on the factors. If one of the factors has a boundary, the product manifold also has a boundary. Cartesian products may be used to construct tori and finite cylinders, for example, as S1 × S1 and S1 × [0, 1], respectively.The study of manifolds combines many important areas of mathematics: it generalizes concepts such as curves and surfaces as well as ideas from linear algebra and topology.Before the modern concept of a manifold there were several important results.Non-Euclidean geometry considers spaces where Euclid's parallel postulate fails. Saccheri first studied such geometries in 1733 but sought only to disprove them. Gauss, Bolyai and Lobachevsky independently discovered them 100 years later. Their research uncovered two types of spaces whose geometric structures differ from that of classical Euclidean space; these gave rise to hyperbolic geometry and elliptic geometry. In the modern theory of manifolds, these notions correspond to Riemannian manifolds with constant negative and positive curvature, respectively.Carl Friedrich Gauss may have been the first to consider abstract spaces as mathematical objects in their own right.  His theorema egregium gives a method for computing the curvature of a surface without considering the ambient space in which the surface lies.  Such a surface would, in modern terminology, be called a manifold; and in modern terms, the theorem proved that the curvature of the surface is an intrinsic property. Manifold theory has come to focus exclusively on these intrinsic properties (or invariants), while largely ignoring the extrinsic properties of the ambient space.Another, more topological example of an intrinsic property of a manifold is its Euler characteristic. Leonhard Euler showed that for a convex polytope in the three-dimensional Euclidean space with V vertices (or corners), E edges, and F faces,The same formula will hold if we project the vertices and edges of the polytope onto a sphere, creating a topological map with V vertices, E edges, and F faces, and in fact, will remain true for any spherical map, even if it does not arise from any convex polytope.[3] Thus 2 is a topological invariant of the sphere, called its Euler characteristic.  On the other hand, a torus can be sliced open by its 'parallel' and 'meridian' circles, creating a map with V = 1 vertex, E = 2 edges, and F = 1 face. Thus the Euler characteristic of the torus is 1 − 2 + 1 = 0. The Euler characteristic of other surfaces is a useful topological invariant, which can be extended to higher dimensions using Betti numbers. In the mid nineteenth century, the Gauss–Bonnet theorem linked the Euler characteristic to the Gaussian curvature.Investigations of Niels Henrik Abel and Carl Gustav Jacobi on inversion of elliptic integrals in the first half of 19th century led them to consider special types of complex manifolds, now known as Jacobians. Bernhard Riemann further contributed to their theory, clarifying the geometric meaning of the process of analytic continuation of functions of complex variables.Another important source of manifolds in 19th century mathematics was analytical mechanics, as developed by Siméon Poisson, Jacobi, and William Rowan Hamilton. The possible states of a mechanical system are thought to be points of an abstract space, phase space in Lagrangian and Hamiltonian formalisms of classical mechanics. This space is, in fact, a high-dimensional manifold, whose dimension corresponds to the degrees of freedom of the system and where the points are specified by their generalized coordinates. For an unconstrained movement of free particles the manifold is equivalent to the Euclidean space, but various conservation laws constrain it to more complicated formations, e.g. Liouville tori. The theory of a rotating solid body, developed in the 18th century by Leonhard Euler and Joseph-Louis Lagrange, gives another example where the manifold is nontrivial. Geometrical and topological aspects of classical mechanics were emphasized by Henri Poincaré, one of the founders of topology.Riemann was the first one to do extensive work generalizing the idea of a surface to higher dimensions. The name manifold comes from Riemann's original German term, Mannigfaltigkeit, which William Kingdon Clifford translated as "manifoldness". In his Göttingen inaugural lecture, Riemann described the set of all possible values of a variable with certain constraints as a Mannigfaltigkeit, because the variable can have many values. He distinguishes between stetige Mannigfaltigkeit and diskrete Mannigfaltigkeit (continuous manifoldness and discontinuous manifoldness), depending on whether the value changes continuously or not. As continuous examples, Riemann refers to not only colors and the locations of objects in space, but also the possible shapes of a spatial figure. Using induction, Riemann constructs an n-fach ausgedehnte Mannigfaltigkeit (n times extended manifoldness or n-dimensional manifoldness) as a continuous stack of (n−1) dimensional manifoldnesses. Riemann's intuitive notion of a Mannigfaltigkeit evolved into what is today formalized as a manifold. Riemannian manifolds and Riemann surfaces are named after Riemann.In his very influential paper, Analysis Situs,[4] Henri Poincaré gave a definition of a (differentiable) manifold (variété) which served as a precursor to the modern concept of a manifold.[5]In the first section of Analysis Situs, Poincaré defines a manifold as the level set of a continuously differentiable function between Euclidean spaces that satisfies the nondegeneracy hypothesis of the implicit function theorem. In the third section, he begins by remarking that the graph of a continuously differentiable function is a manifold in the latter sense. He then proposes a new, more general, definition of manifold based on a 'chain of manifolds' (une chaîne des variétés).Hermann Weyl gave an intrinsic definition for differentiable manifolds in his lecture course on Riemann surfaces in 1911–1912, opening the road to the general concept of a topological space that followed shortly. During the 1930s Hassler Whitney and others clarified the foundational aspects of the subject, and thus intuitions dating back to the latter half of the 19th century became precise, and developed through differential geometry and Lie group theory. Notably, the Whitney embedding theorem[6] showed that the intrinsic definition in terms of charts was equivalent to Poincaré's definition in terms of subsets of Euclidean space.Two-dimensional manifolds, also known as a 2D surfaces embedded in our common 3D space, were considered by Riemann under the guise of Riemann surfaces, and rigorously classified in the beginning of the 20th century by Poul Heegaard and Max Dehn. Henri Poincaré pioneered the study of three-dimensional manifolds and raised a fundamental question about them, today known as the Poincaré conjecture.  After nearly a century of effort by many mathematicians, starting with Poincaré himself, Grigori Perelman proved the Poincaré conjecture (see the Solution of the Poincaré conjecture). William Thurston's geometrization program, formulated in the 1970s, provided a far-reaching extension of the Poincaré conjecture to the general three-dimensional manifolds. Four-dimensional manifolds were brought to the forefront of mathematical research in the 1980s by Michael Freedman and in a different setting, by Simon Donaldson, who was motivated by the then recent progress in theoretical physics (Yang–Mills theory), where they serve as a substitute for ordinary 'flat' spacetime.  Andrey Markov Jr. showed in 1960 that no algorithm exists for classifying four-dimensional manifolds. Important work on higher-dimensional manifolds, including analogues of the Poincaré conjecture, had been done earlier by René Thom, John Milnor, Stephen Smale and Sergei Novikov. One of the most pervasive and flexible techniques underlying much work on the topology of manifolds is Morse theory.The simplest kind of manifold to define is the topological manifold, which looks locally like some "ordinary" Euclidean space Rn. By definition, all manifolds are topological manifolds, so the phrase "topological manifold" is usually used to emphasize that a manifold lacks additional structure, or that only its topological properties are being considered. Formally, a topological manifold is a topological space locally homeomorphic to  a  Euclidean space. This means that every point has a neighbourhood for which there exists a homeomorphism (a bijective continuous function whose inverse is also continuous) mapping that neighbourhood to Rn. These homeomorphisms are the charts of the manifold.It is to be noted that a topological manifold looks locally like a Euclidean space in a rather weak manner: while for each individual chart it is possible to distinguish differentiable functions or measure distances and angles, merely by virtue of being a topological manifold a space does not have any particular and consistent choice of such concepts. In order to discuss such properties for a manifold, one needs to specify further structure and consider differentiable manifolds and Riemannian manifolds discussed below. In particular, the same underlying topological manifold can have several mutually incompatible classes of differentiable functions and an infinite number of ways to specify distances and angles.Usually additional technical assumptions on the topological space are made to exclude pathological cases. It is customary to require that the space be Hausdorff and second countable.The dimension of the manifold at a certain point is the dimension of the Euclidean space that the charts at that point map to (number n in the definition). All points in a connected manifold have the same dimension. Some authors require that all charts of a topological manifold map to Euclidean spaces of same dimension. In that case every topological manifold has a topological invariant, its dimension. Other authors allow disjoint unions of topological manifolds with differing dimensions to be called manifolds.For most applications a special kind of topological manifold, namely a differentiable manifold, is used. If the local charts on a manifold are compatible in a certain sense, one can define directions, tangent spaces, and differentiable functions on that manifold. In particular it is possible to use calculus on a differentiable manifold. Each point of an n-dimensional differentiable manifold has a tangent space. This is an n-dimensional Euclidean space consisting of the tangent vectors of the curves through the point.Two important classes of differentiable manifolds are smooth and analytic manifolds. For smooth manifolds the transition maps are smooth, that is infinitely differentiable. Analytic manifolds are smooth manifolds with the additional condition that the transition maps are analytic (they can be expressed as power series). The sphere can be given analytic structure, as can most familiar curves and surfaces.There are also topological manifolds, i.e., locally Euclidean spaces, which possess no differentiable structures at all.[7]A rectifiable set generalizes the idea of a piecewise smooth or rectifiable curve to higher dimensions; however, rectifiable sets are not in general manifolds.To measure distances and angles on manifolds, the manifold must be Riemannian. A 'Riemannian manifold' is a differentiable manifold in which each tangent space is equipped with an inner product ⟨⋅,⋅⟩ in a manner which varies smoothly from point to point. Given two tangent vectors u and v, the inner product ⟨u,v⟩ gives a real number. The dot (or scalar) product is a typical example of an inner product. This allows one to define various notions such as length, angles, areas (or volumes), curvature and divergence of vector fields.All differentiable manifolds (of constant dimension) can be given the structure of a Riemannian manifold. The Euclidean space itself carries a natural structure of Riemannian manifold (the tangent spaces are naturally identified with the Euclidean space itself and carry the standard scalar product of the space). Many familiar curves and surfaces, including for example all n-spheres, are specified as subspaces of a Euclidean space and inherit a metric from their embedding in it.A Finsler manifold allows the definition of distance but does not require the concept of angle; it is an analytic manifold in which each tangent space is equipped with a norm, ||·||, in a manner which varies smoothly from point to point. This norm can be extended to a metric, defining the length of a curve; but it cannot in general be used to define an inner product.Any Riemannian manifold is a Finsler manifold.Lie groups, named after Sophus Lie, are differentiable manifolds that carry also the structure of a group which is such that the group operations are defined by smooth maps.A Euclidean vector space with the group operation of vector addition is an example of a non-compact Lie group. A simple example of a compact Lie group is the circle: the group operation is simply rotation. This group, known as U(1), can be also characterised as the group of complex numbers of modulus 1 with multiplication as the group operation.Other examples of Lie groups include special groups of matrices, which are all subgroups of the general linear group, the group of n by n matrices with non-zero determinant. If the matrix entries are real numbers, this will be an n2-dimensional disconnected manifold. The orthogonal groups, the symmetry groups of the sphere and hyperspheres, are n(n−1)/2 dimensional manifolds, where n−1 is the dimension of the sphere. Further examples can be found in the table of Lie groups.Different notions of manifolds have different notions of classification and invariant; in this section we focus on smooth closed manifolds.The classification of smooth closed manifolds is well understood in principle, except in dimension 4: in low dimensions (2 and 3) it is geometric, via the uniformization theorem and the solution of the Poincaré conjecture, and in high dimension (5 and above) it is algebraic, via surgery theory. This is a classification in principle: the general question of whether two smooth manifolds are diffeomorphic is not computable in general. Further, specific computations remain difficult, and there are many open questions.Orientable surfaces can be visualized, and their diffeomorphism classes enumerated, by genus. Given two orientable surfaces, one can determine if they are diffeomorphic by computing their respective genera and comparing: they are diffeomorphic if and only if the genera are equal, so the genus forms a complete set of invariants.This is much harder in higher dimensions: higher-dimensional manifolds cannot be directly visualized (though visual intuition is useful in understanding them), nor can their diffeomorphism classes be enumerated, nor can one in general determine if two different descriptions of a higher-dimensional manifold refer to the same object.However, one can determine if two manifolds are different if there is some intrinsic characteristic that differentiates them. Such criteria are commonly referred to as invariants, because, while they may be defined in terms of some presentation (such as the genus in terms of a triangulation), they are the same relative to all possible descriptions of a particular manifold: they are invariant under different descriptions.Naively, one could hope to develop an arsenal of invariant criteria that would definitively classify all manifolds up to isomorphism.Unfortunately, it is known that for manifolds of dimension 4 and higher, no program exists that can decide whether two manifolds are diffeomorphic.Smooth manifolds have a rich set of invariants, coming from point-set topology,classic algebraic topology, and geometric topology. The most familiar invariants, which are visible for surfaces, are orientability (a normal invariant, also detected by homology) and genus (a homological invariant).Smooth closed manifolds have no local invariants (other than dimension), though geometric manifolds have local invariants, notably the curvature of a Riemannian manifold and the torsion of a manifold equipped with an affine connection.This distinction between local invariants and no local invariants is a common way to distinguish between geometry and topology. All invariants of a smooth closed manifold are thus global.Algebraic topology is a source of a number of important global invariant properties.  Some key criteria include the simply connected property and orientability (see below).  Indeed, several branches of mathematics, such as homology and homotopy theory, and the theory of characteristic classes were founded in order to study invariant properties of manifolds.In dimensions two and higher, a simple but important invariant criterion is the question of whether a manifold admits a meaningful orientation. Consider a topological manifold with charts mapping to Rn. Given an ordered basis for Rn, a chart causes its piece of the manifold to itself acquire a sense of ordering, which in 3-dimensions can be viewed as either right-handed or left-handed. Overlapping charts are not required to agree in their sense of ordering, which gives manifolds an important freedom. For some manifolds, like the sphere, charts can be chosen so that overlapping regions agree on their "handedness"; these are orientable manifolds. For others, this is impossible. The latter possibility is easy to overlook, because any closed surface embedded (without self-intersection) in three-dimensional space is orientable.Some illustrative examples of non-orientable manifolds include: (1) the Möbius strip, which is a manifold with boundary, (2) the Klein bottle, which must intersect itself in its 3-space representation, and (3) the real projective plane, which arises naturally in geometry. Begin with an infinite circular cylinder standing vertically, a manifold without boundary. Slice across it high and low to produce two circular boundaries, and the cylindrical strip between them. This is an orientable manifold with boundary, upon which "surgery" will be performed. Slice the strip open, so that it could unroll to become a rectangle, but keep a grasp on the cut ends. Twist one end 180°, making the inner surface face out, and glue the ends back together seamlessly. This results in a strip with a permanent half-twist: the Möbius strip. Its boundary is no longer a pair of circles, but (topologically) a single circle; and what was once its "inside" has merged with its "outside", so that it now has only a single side.Take two Möbius strips; each has a single loop as a boundary. Straighten out those loops into circles, and let the strips distort into cross-caps.  Gluing the circles together will produce a new, closed manifold without boundary, the Klein bottle. Closing the surface does nothing to improve the lack of orientability, it merely removes the boundary. Thus, the Klein bottle is a closed surface with no distinction between inside and outside. Note that in three-dimensional space, a Klein bottle's surface must pass through itself. Building a Klein bottle which is not self-intersecting requires four or more dimensions of space.Begin with a sphere centered on the origin. Every line through the origin pierces the sphere in two opposite points called antipodes. Although there is no way to do so physically, it is possible (by considering a quotient space) to mathematically merge each antipode pair into a single point. The closed surface so produced is the real projective plane, yet another non-orientable surface. It has a number of equivalent descriptions and constructions, but this route explains its name: all the points on any given line through the origin project to the same "point" on this "plane".For two dimensional manifolds a key invariant property is the genus, or the "number of handles" present in a surface. A torus is a sphere with one handle, a double torus is a sphere with two handles, and so on.  Indeed, it is possible to fully characterize compact, two-dimensional manifolds on the basis of genus and orientability.  In higher-dimensional manifolds genus is replaced by the notion of Euler characteristic, and more generally Betti numbers and homology and cohomology.Just as there are various types of manifolds, there are various types of maps of manifolds. In addition to continuous functions and smooth functions generally, there are maps with special properties. In geometric topology a basic type are embeddings, of which knot theory is a central example, and generalizations such as immersions, submersions, covering spaces, and ramified covering spaces.Basic results include the Whitney embedding theorem and Whitney immersion theorem.In Riemannian geometry, one may ask for maps to preserve the Riemannian metric, leading to notions of isometric embeddings, isometric immersions, and Riemannian submersions; a basic result is the Nash embedding theorem.A basic example of maps between manifolds are scalar-valued functions on a manifold,sometimes called regular functions or functionals, by analogy with algebraic geometry or linear algebra. These are of interest both in their own right, and to study the underlying manifold.In geometric topology, most commonly studied are Morse functions, which yield handlebody decompositions, while in mathematical analysis, one often studies solution to partial differential equations, an important example of which is harmonic analysis, where one studies harmonic functions: the kernel of the Laplace operator. This leads to such functions as the spherical harmonics, and to heat kernel methods of studying manifolds, such as hearing the shape of a drum and some proofs of the Atiyah–Singer index theorem.
De Casteljau's algorithm
In the mathematical field of numerical analysis, De Casteljau's algorithm is a recursive method to evaluate polynomials in Bernstein form or Bézier curves, named after its inventor Paul de Casteljau. De Casteljau's algorithm can also be used to split a single Bézier curve into two Bézier curves at an arbitrary parameter value.Although the algorithm is slower for most architectures when compared with the direct approach, it is more numerically stable.where b is a Bernstein basis polynomialThe curve at point t0 can be evaluated with the recurrence relationHere is an example implementation of De Casteljau's algorithm in Haskell:When doing the calculation by hand it is useful to write down the coefficients in a triangle scheme asWhen choosing a point t0 to evaluate a Bernstein polynomial we can use the two diagonals of the triangle scheme to construct a division of the polynomialintoandWe want to evaluate the Bernstein polynomial of degree 2 with the Bernstein coefficientsat the point t0.We start the recursion withand with the second iteration the recursion stops withwhich is the expected Bernstein polynomial of degree 2.When evaluating a Bézier curve of degree n in 3-dimensional space with n+1 control points Piwithwe split the Bézier curve into three separate equationswhich we evaluate individually using De Casteljau's algorithm.The geometric interpretation of De Casteljau's algorithm is straightforward. The following picture shows this process for a cubic Bézier curve:In general, operations on a rational curve (or surface) are equivalent to operations on a nonrational curve in a projective space. This representation as the "weighted control points" and weights is often convenient when evaluating rational curves.
Wave function
A wave function in quantum physics is a mathematical description of the quantum state of an isolated quantum system.  The wave function is a  complex-valued probability amplitude, and the probabilities for the possible results of measurements made on the system can be derived from it.  The most common symbols for a wave function are the Greek letters ψ or Ψ (lower-case and capital psi, respectively).The wave function is a function of the degrees of freedom corresponding to some maximal set of commuting observables.  Once such a representation is chosen, the wave function can be derived from the quantum state.For a given system, the choice of which commuting degrees of freedom to use is not unique, and correspondingly the domain of the wave function is also not unique.  For instance, it may be taken to be a function of all the position coordinates of the particles over position space, or the momenta of all the particles over momentum space; the two are related by a Fourier transform.  Some particles, like electrons and photons, have nonzero spin, and the wave function for such particles includes spin as an intrinsic, discrete degree of freedom; other discrete variables can also be included, such as isospin.  When a system has internal degrees of freedom, the wave function at each point in the continuous degrees of freedom (e.g., a point in space) assigns a complex number for each possible value of the discrete degrees of freedom (e.g., z-component of spin) – these values are often displayed in a column matrix (e.g., a 2 × 1 column vector for a non-relativistic electron with spin ​1⁄2).According to the superposition principle of quantum mechanics, wave functions can be added together and multiplied by complex numbers to form new wave functions and form a Hilbert space.  The inner product between two wave functions is a measure of the overlap between the corresponding physical states, and is used in the foundational probabilistic interpretation of quantum mechanics, the Born rule, relating transition probabilities to inner products.  The Schrödinger equation determines how wave functions evolve over time, and a wave function behaves qualitatively like other waves, such as water waves or waves on a string, because the Schrödinger equation is mathematically a type of wave equation.  This explains the name "wave function," and gives rise to wave–particle duality.  However, the wave function in quantum mechanics describes a kind of physical phenomenon, still open to different interpretations, which fundamentally differs from that of classic mechanical waves.[1][2][3][4][5][6][7]In Born's statistical interpretation in non-relativistic quantum mechanics,[8][9][10] the squared modulus of the wave function, |ψ|2, is a real number interpreted as the probability density of measuring a particle's being detected at a given place – or having a given momentum – at a given time, and possibly having definite values for discrete degrees of freedom.  The integral of this quantity, over all the system's degrees of freedom, must be 1 in accordance with the probability interpretation.  This general requirement that a wave function must satisfy is called the normalization condition.  Since the wave function is complex valued, only its relative phase and relative magnitude can be measured—its value does not, in isolation, tell anything about the magnitudes or directions of measurable observables; one has to apply quantum operators, whose eigenvalues correspond to sets of possible results of measurements, to the wave function ψ and calculate the statistical distributions for measurable quantities.In the 1920s and 1930s, quantum mechanics was developed using calculus and linear algebra. Those who used the techniques of calculus included Louis de Broglie, Erwin Schrödinger, and others, developing "wave mechanics". Those who applied the methods of linear algebra included Werner Heisenberg, Max Born, and others, developing "matrix mechanics". Schrödinger subsequently showed that the two approaches were equivalent.[14]In 1926, Schrödinger published the famous wave equation now named after him, indeed the Schrödinger equation, based on classical conservation of energy using quantum operators and the de Broglie relations such that the solutions of the equation are the wave functions for the quantum system.[15] However, no one was clear on how to interpret it.[16] At first, Schrödinger and others thought that wave functions represent particles that are spread out with most of the particle being where the wave function is large.[17] This was shown to be incompatible with the elastic scattering of a wave packet (representing a particle) off a target; it spreads out in all directions.[8] While a scattered particle may scatter in any direction, it does not break up and take off in all directions. In 1926, Born provided the perspective of probability amplitude.[8][9][18] This relates calculations of quantum mechanics directly to probabilistic experimental observations.It is accepted as part of the Copenhagen interpretation of quantum mechanics. There are many other interpretations of quantum mechanics. In 1927, Hartree and Fock made the first step in an attempt to solve the N-body wave function, and developed the self-consistency cycle: an iterative algorithm to approximate the solution. Now it is also known as the Hartree–Fock method.[19] The Slater determinant and permanent (of a matrix) was part of the method, provided by John C. Slater.Schrödinger did encounter an equation for the wave function that satisfied relativistic energy conservation before he published the non-relativistic one, but discarded it as it predicted negative probabilities and negative energies. In 1927, Klein, Gordon and Fock also found it, but incorporated the electromagnetic interaction and proved that it was Lorentz invariant. De Broglie also arrived at the same equation in 1928. This relativistic wave equation is now most commonly known as the Klein–Gordon equation.[20]In 1927, Pauli phenomenologically found a non-relativistic equation to describe spin-1/2 particles in electromagnetic fields, now called the Pauli equation.[21] Pauli found the wave function was not described by a single complex function of space and time, but needed two complex numbers, which respectively correspond to the spin +1/2 and −1/2 states of the fermion. Soon after in 1928, Dirac found an equation from the first successful unification of special relativity and quantum mechanics applied to the electron, now called the Dirac equation. In this, the wave function is a spinor represented by four complex-valued components:[19] two for the electron and two for the electron's antiparticle, the positron. In the non-relativistic limit, the Dirac wave function resembles the Pauli wave function for the electron. Later, other relativistic wave equations were found.All these wave equations are of enduring importance. The Schrödinger equation and the Pauli equation are under many circumstances excellent approximations of the relativistic variants. They are considerably easier to solve in practical problems than the relativistic counterparts.The Klein–Gordon equation and the Dirac equation, while being relativistic, do not represent full reconciliation of quantum mechanics and special relativity. The branch of quantum mechanics where these equations are studied the same way as the Schrödinger equation, often called relativistic quantum mechanics, while very successful, has its limitations (see e.g. Lamb shift) and conceptual problems (see e.g. Dirac sea).Relativity makes it inevitable that the number of particles in a system is not constant. For full reconciliation, quantum field theory is needed.[22] In this theory, the wave equations and the wave functions have their place, but in a somewhat different guise. The main objects of interest are not the wave functions, but rather operators, so called field operators (or just fields where "operator" is understood) on the Hilbert space of states (to be described next section). It turns out that the original relativistic wave equations and their solutions are still needed to build the Hilbert space. Moreover, the free fields operators, i.e. when interactions are assumed not to exist, turn out to (formally) satisfy the same equation as do the fields (wave functions) in many cases.Thus the Klein–Gordon equation (spin 0) and the Dirac equation (spin ​1⁄2) in this guise remain in the theory. Higher spin analogues include the Proca equation (spin 1), Rarita–Schwinger equation (spin ​3⁄2), and, more generally, the Bargmann–Wigner equations. For massless free fields two examples are the free field Maxwell equation (spin 1) and the free field Einstein equation (spin 2) for the field operators.[23] All of them are essentially a direct consequence of the requirement of Lorentz invariance. Their solutions must transform under Lorentz transformation in a prescribed way, i.e. under a particular representation of the Lorentz group and that together with few other reasonable demands, e.g. the cluster decomposition principle,[24] with implications for causality is enough to fix the equations.It should be emphasized that this applies to free field equations; interactions are not included. If a Lagrangian density (including interactions) is available, then the Lagrangian formalism will yield an equation of motion at the classical level. This equation may be very complex and not amenable to solution. Any solution would refer to a fixed number of particles and would not account for the term "interaction" as referred to in these theories, which involves the creation and annihilation of particles and not external potentials as in ordinary "first quantized" quantum theory.In string theory, the situation remains analogous. For instance, a wave function in momentum space has the role of Fourier expansion coefficient in a general state of a particle (string) with momentum that is not sharply defined.[25]For now, consider the simple case of a non-relativistic single particle, without spin, in one spatial dimension. More general cases are discussed below.The state of such a particle is completely described by its wave function,where x is position and t is time. This is a complex-valued function of two real variables x and t.For one spinless particle in 1d, if the wave function is interpreted as a probability amplitude, the square modulus of the wave function, the positive real numberis interpreted as the probability density that the particle is at x. The asterisk indicates the complex conjugate. If the particle's position is measured, its location cannot be determined from the wave function, but is described by a probability distribution. The probability that its position x will be in the interval a ≤ x ≤ b is the integral of the density over this interval:where t is the time at which the particle was measured. This leads to the normalization condition:because if the particle is measured, there is 100% probability that it will be somewhere.For a given system, the set of all possible normalizable wave functions (at any given time) forms an abstract mathematical vector space, meaning that it is possible to add together different wave functions, and multiply wave functions by complex numbers (see vector space for details). Technically, because of the normalization condition, wave functions form a projective space rather than an ordinary vector space. This vector space is infinite-dimensional, because there is no finite set of functions which can be added together in various combinations to create every possible function. Also, it is a Hilbert space, because the inner product of two wave functions Ψ1 and Ψ2 can be defined as the complex number (at time t)[nb 1]More details are given below. Although the inner product of two wave functions is a complex number, the inner product of a wave function Ψ with itself,is always a positive real number. The number ||Ψ|| (not ||Ψ||2) is called the norm of the wave function Ψ.If (Ψ, Ψ) = 1, then Ψ is normalized. If Ψ is not normalized, then dividing by its norm gives the normalized function Ψ/||Ψ||. Two wave functions Ψ1 and Ψ2 are orthogonal if (Ψ1, Ψ2) = 0. If they are normalized and orthogonal, they are orthonormal. Orthogonality (hence also orthonormality) of wave functions is not a necessary condition wave functions must satisfy, but is instructive to consider since this guarantees linear independence of the functions. In a linear combination of orthogonal wave functions Ψn we have,If the wave functions Ψn were nonorthogonal, the coefficients would be less simple to obtain.In the Copenhagen interpretation, the modulus squared of the inner product (a complex number) gives a real numberwhich, assuming both wave functions are normalized, is interpreted as the probability of the wave function Ψ2 "collapsing" to the new wave function Ψ1 upon measurement of an observable, whose eigenvalues are the possible results of the measurement, with Ψ1 being an eigenvector of the resulting eigenvalue. This is the Born rule,[8] and is one of the fundamental postulates of quantum mechanics.At a particular instant of time, all values of the wave function Ψ(x, t) are components of a vector. There are uncountably infinitely many of them and integration is used in place of summation. In Bra–ket notation, this vector is writtenand is referred to as a "quantum state vector", or simply "quantum state". There are several advantages to understanding wave functions as representing elements of an abstract vector space:The time parameter is often suppressed, and will be in the following. The x coordinate is a continuous index. The |x⟩ are the basis vectors, which are orthonormal so their inner product is a delta function;thusandwhich illuminates the identity operatorFinding the identity operator in a basis allows the abstract state to be expressed explicitly in a basis, and more (the inner product between two state vectors, and other operators for observables, can be expressed in the basis).The particle also has a wave function in momentum space:where p is the momentum in one dimension, which can be any value from −∞ to +∞, and t is time.Analogous to the position case, the inner product of two wave functions Φ1(p, t) and Φ2(p, t) can be defined as:One particular solution to the time-independent Schrödinger equation isa plane wave, which can be used in the description of a particle with momentum exactly p, since it is an eigenfunction of the momentum operator. These functions are not normalizable to unity (they aren't square-integrable), so they are not really elements of physical Hilbert space. The setforms what is called the momentum basis. This "basis" is not a basis in the usual mathematical sense. For one thing, since the functions aren't normalizable, they are instead normalized to a delta function,For another thing, though they are linearly independent, there are too many of them (they form an uncountable set) for a basis for physical Hilbert space. They can still be used to express all functions in it using Fourier transforms as described next.The x and p representations areNow take the projection of the state Ψ onto eigenfunctions of momentum using the last expression in the two equations,[26]Then utilizing the known expression for suitably normalized eigenstates of momentum in the position representation solutions of the free Schrödinger equationone obtainsLikewise, using eigenfunctions of position,The position-space and momentum-space wave functions are thus found to be Fourier transforms of each other.[27] The two wave functions contain the same information, and either one alone is sufficient to calculate any property of the particle. As representatives of elements of abstract physical Hilbert space, whose elements are the possible states of the system under consideration, they represent the same state vector, hence identical physical states, but they are not generally equal when viewed as square-integrable functions.In practice, the position-space wave function is used much more often than the momentum-space wave function. The potential entering the relevant equation (Schrödinger, Dirac, etc.) determines in which basis the description is easiest. For the harmonic oscillator, x and p enter symmetrically, so there it doesn't matter which description one uses. The same equation (modulo constants) results. From this follows, with a little bit of afterthought, a factoid: The solutions to the wave equation of the harmonic oscillator are eigenfunctions of the Fourier transform in L2.[nb 2]Following are the general forms of the wave function for systems in higher dimensions and more particles, as well as including other degrees of freedom than position coordinates or momentum components.The position-space wave function of a single particle without spin in three spatial dimensions is similar to the case of one spatial dimension above:where r is the position vector in three-dimensional space, and t is time. As always Ψ(r, t) is a complex-valued function of real variables. As a single vector in Dirac notationAll the previous remarks on inner products, momentum space wave functions, Fourier transforms, and so on extend to higher dimensions.For a particle with spin, ignoring the position degrees of freedom, the wave function is a function of spin only (time is a parameter);where sz is the spin projection quantum number along the z axis. (The z axis is an arbitrary choice; other axes can be used instead if the wave function is transformed appropriately, see below.) The sz parameter, unlike r and t, is a discrete variable. For example, for a spin-1/2 particle, sz can only be +1/2 or −1/2, and not any other value. (In general, for spin s, sz can be s, s − 1, ... , −s + 1, −s). Inserting each quantum number gives a complex valued function of space and time, there are 2s + 1 of them. These can be arranged into a column vector[nb 3]In bra–ket notation, these easily arrange into the components of a vector[nb 4]The entire vector ξ is a solution of the Schrödinger equation (with a suitable Hamiltonian), which unfolds to a coupled system of 2s + 1 ordinary differential equations with solutions ξ(s, t), ξ(s − 1, t), ..., ξ(−s, t). The term "spin function" instead of "wave function" is used by some authors. This contrasts the solutions to position space wave functions, the position coordinates being continuous degrees of freedom, because then the Schrödinger equation does take the form of a wave equation.More generally, for a particle in 3d with any spin, the wave function can be written in "position–spin space" as:and these can also be arranged into a column vectorin which the spin dependence is placed in indexing the entries, and the wave function is a complex vector-valued function of space and time only.All values of the wave function, not only for discrete but continuous variables also, collect into a single vectorFor a single particle, the tensor product ⊗ of its position state vector |ψ⟩ and spin state vector |ξ⟩ gives the composite position-spin state vectorwith the identificationsThe tensor product factorization is only possible if the orbital and spin angular momenta of the particle are separable in the Hamiltonian operator underlying the system's dynamics (in other words, the Hamiltonian can be split into the sum of orbital and spin terms[28]). The time dependence can be placed in either factor, and time evolution of each can be studied separately. The factorization is not possible for those interactions where an external field or any space-dependent quantity couples to the spin; examples include a particle in a magnetic field, and spin–orbit coupling.The preceding discussion is not limited to spin as a discrete variable, the total angular momentum J may also be used.[29] Other discrete degrees of freedom, like isospin, can expressed similarly to the case of spin above.If there are many particles, in general there is only one wave function, not a separate wave function for each particle. The fact that one wave function describes many particles is what makes quantum entanglement and the EPR paradox possible. The position-space wave function for N particles is written:[19]where ri is the position of the ith particle in three-dimensional space, and t is time. Altogether, this is a complex-valued function of 3N + 1 real variables.In quantum mechanics there is a fundamental distinction between identical particles and distinguishable particles. For example, any two electrons are identical and fundamentally indistinguishable from each other; the laws of physics make it impossible to "stamp an identification number" on a certain electron to keep track of it.[27] This translates to a requirement on the wave function for a system of identical particles:where the + sign occurs if the particles are all bosons and − sign if they are all fermions. In other words, the wave function is either totally symmetric in the positions of bosons, or totally antisymmetric in the positions of fermions.[30] The physical interchange of particles corresponds to mathematically switching arguments in the wave function. The antisymmetry feature of fermionic wave functions leads to the Pauli principle. Generally, bosonic and fermionic symmetry requirements are the manifestation of particle statistics and are present in other quantum state formalisms.For N distinguishable particles (no two being identical, i.e. no two having the same set of quantum numbers), there is no requirement for the wave function to be either symmetric or antisymmetric.For a collection of particles, some identical with coordinates r1, r2, ... and others distinguishable x1, x2, ... (not identical with each other, and not identical to the aforementioned identical particles), the wave function is symmetric or antisymmetric in the identical particle coordinates ri only:Again, there is no symmetry requirement for the distinguishable particle coordinates xi.The wave function for N particles each with spin is the complex-valued functionAccumulating all these components into a single vector,For identical particles, symmetry requirements apply to both position and spin arguments of the wave function so it has the overall correct symmetry.The formulae for the inner products are integrals over all coordinates or momenta and sums over all spin quantum numbers. For the general case of N particles with spin in 3d,this is altogether N three-dimensional volume integrals and N sums over the spins. The differential volume elements d3ri are also written "dVi" or "dxi dyi dzi".The multidimensional Fourier transforms of the position or position–spin space wave functions yields momentum or momentum–spin space wave functions.For the general case of N particles with spin in 3d, if Ψ is interpreted as a probability amplitude, the probability density isand the probability that particle 1 is in region R1 with spin sz1 = m1 and particle 2 is in region R2 with spin sz2 = m2 etc. at time t is the integral of the probability density over these regions and evaluated at these spin numbers:For systems in time-independent potentials, the wave function can always be written as a function of the degrees of freedom multiplied by a time-dependent phase factor, the form of which is given by the Schrödinger equation. For N particles, considering their positions only and suppressing other degrees of freedom,where E is the energy eigenvalue of the system corresponding to the eigenstate Ψ. Wave functions of this form are called stationary states.The time dependence of the quantum state and the operators can be placed according to unitary transformations on the operators and states. For any quantum state |Ψ⟩ and operator O, in the Schrödinger picture |Ψ(t)⟩ changes with time according to the Schrödinger equation while O is constant. In the Heisenberg picture it is the other way round, |Ψ⟩ is constant while O(t) evolves with time according to the Heisenberg equation of motion. The Dirac (or interaction) picture is intermediate, time dependence is places in both operators and states which evolve according to equations of motion. It is useful primarily in computing S-matrix elements.[31]The following are solutions to the Schrödinger equation for one nonrelativistic spinless particle.One of most prominent features of the wave mechanics is a possibility for a particle to reach a location with a prohibitive (in classical mechanics) force potential. A common model is the "potential barrier", the one-dimensional case has the potentialand the steady-state solutions to the wave equation have the form (for some constants k, κ)Note that these wave functions are not normalized; see scattering theory for discussion.The standard interpretation of this is as a stream of particles being fired at the step from the left (the direction of negative x): setting Ar = 1 corresponds to firing particles singly; the terms containing Ar and Cr signify motion to the right, while Al and Cl – to the left. Under this beam interpretation, put Cl = 0 since no particles are coming from the right. By applying the continuity of wave functions and their derivatives at the boundaries, it is hence possible to determine the constants above.In a semiconductor crystallite whose radius is smaller than the size of its exciton Bohr radius, the excitons are squeezed, leading to quantum confinement. The energy levels can then be modeled using the particle in a box model in which the energy of different states is dependent on the length of the box.The wave functions for the quantum harmonic oscillator can be expressed in terms of Hermite polynomials Hn, they arewhere n = 0,1,2,....The wave functions of an electron in a Hydrogen atom are expressed in terms of spherical harmonics and generalized Laguerre polynomials (these are defined differently by different authors—see main article on them and the hydrogen atom).It is convenient to use spherical coordinates, and the wave function can be separated into functions of each coordinate,[32]where R are radial functions and Ymℓ(θ, φ) are spherical harmonics of degree ℓ and order m. This is the only atom for which the Schrödinger equation has been solved exactly. Multi-electron atoms require approximative methods. The family of solutions is:[33]where a0 = 4πε0ħ2/mee2 is the Bohr radius,L2ℓ + 1n − ℓ − 1 are the generalized Laguerre polynomials of degree n − ℓ − 1, n = 1, 2, ...  is the principal quantum number, ℓ = 0, 1, ... n − 1 the azimuthal quantum number, m = −ℓ, −ℓ + 1, ...,  ℓ − 1, ℓ the magnetic quantum number. Hydrogen-like atoms have very similar solutions.This solution does not take into account the spin of the electron.In the figure of the hydrogen orbitals, the 19 sub-images are images of wave functions in position space (their norm squared). The wave functions represent the abstract state characterized by the triple of quantum numbers (n, l, m), in the lower right of each image. These are the principal quantum number, the orbital angular momentum quantum number, and the magnetic quantum number. Together with one spin-projection quantum number of the electron, this is a complete set of observables.The figure can serve to illustrate some further properties of the function spaces of wave functions. The concept of function spaces enters naturally in the discussion about wave functions. A function space is a set of functions, usually with some defining requirements on the functions (in the present case that they are square integrable), sometimes with an algebraic structure on the set (in the present case a vector space structure with an inner product), together with a topology on the set. The latter will sparsely be used here, it is only needed to obtain a precise definition of what it means for a subset of a function space to be closed. It will be concluded below that the function space of wave functions is a Hilbert space. This observation is the foundation of the predominant mathematical formulation of quantum mechanics.A wave function is an element of a function space partly characterized by the following concrete and abstract descriptions.This similarity is of course not accidental. There are also a distinctions between the spaces to keep in mind.Basic states are characterized by a set of quantum numbers. This is a set of eigenvalues of a maximal set of commuting observables. Physical observables are represented by linear operators, also called observables, on the vectors space. Maximality means that there can be added to the set no further algebraically independent observables that commute with the ones already present. A choice of such a set may be called a choice of representation.The abstract states are "abstract" only in that an arbitrary choice necessary for a particular explicit description of it is not given. This is the same as saying that no choice of maximal set of commuting observables has been given. This is analogous to a vector space without a specified basis. Wave functions corresponding to a state are accordingly not unique. This non-uniqueness reflects the non-uniqueness in the choice of a maximal set of commuting observables. For one spin particle in one dimension, to a particular state there corresponds two wave functions, Ψ(x, Sz) and Ψ(p, Sy), both describing the same state. Each choice of representation should be thought of as specifying a unique function space in which wave functions corresponding to that choice of representation lives. This distinction is best kept, even if one could argue that two such function spaces are mathematically equal, e.g. being the set of square integrable functions. One can then think of the function spaces as two distinct copies of that set.There is an additional algebraic structure on the vector spaces of wave functions and the abstract state space.This motivates the introduction of an inner product on the vector space of abstract quantum states, compatible with the mathematical observations above when passing to a representation. It is denoted (Ψ, Φ), or in the Bra–ket notation ⟨Ψ|Φ⟩. It yields a complex number. With the inner product, the function space is an inner product space. The explicit appearance of the inner product (usually an integral or a sum of integrals) depends on the choice of representation, but the complex number (Ψ, Φ) does not. Much of the physical interpretation of quantum mechanics stems from the Born rule. It states that the probability p of finding upon measurement the state Φ given the system is in the state Ψ iswhere Φ and Ψ are assumed normalized. Consider a scattering experiment. In quantum field theory, if Φout describes a state in the "distant future" (an "out state") after interactions between scattering particles have ceased, and Ψin an "in state" in the "distant past", then the quantities (Φout, Ψin), with Φout and Ψin varying over a complete set of in states and out states respectively, is called the S-matrix or scattering matrix. Knowledge of it is, effectively, having solved the theory at hand, at least as far as predictions go. Measurable quantities such as decay rates and scattering cross sections are calculable from the S-matrix.[35]The above observations encapsulate the essence of the function spaces of which wave functions are elements. However, the description is not yet complete. There is a further technical requirement on the function space, that of completeness, that allows one to take limits of sequences in the function space, and be ensured that, if the limit exists, it is an element of the function space. A complete inner product space is called a Hilbert space. The property of completeness is crucial in advanced treatments and applications of quantum mechanics. For instance, the existence of projection operators or orthogonal projections relies on the completeness of the space.[36] These projection operators, in turn, are essential for the statement and proof of many useful theorems, e.g. the spectral theorem. It is not very important in introductory quantum mechanics, and technical details and links may be found in footnotes like the one that follows.[nb 7] The space L2 is a Hilbert space, with inner product presented later. The function space of the example of the figure is a subspace of L2. A subspace of a Hilbert space is a Hilbert space if it is closed.In summary, the set of all possible normalizable wave functions for a system with a particular choice of basis, together with the null vector, constitute a Hilbert space.Not all functions of interest are elements of some Hilbert space, say L2. The most glaring example is the set of functions e​2πip · x⁄h. These are plane wave solutions of the Schrödinger equation for a free particle, but are not normalizable, hence not in L2. But they are nonetheless fundamental for the description. One can, using them, express functions that are normalizable using wave packets. They are, in a sense, a basis (but not a Hilbert space basis, nor a Hamel basis) in which wave functions of interest can be expressed. There is also the artifact "normalization to a delta function" that is frequently employed for notational convenience, see further down. The delta functions themselves aren't square integrable either.The above description of the function space containing the wave functions is mostly mathematically motivated. The function spaces are, due to completeness, very large in a certain sense. Not all functions are realistic descriptions of any physical system. For instance, in the function space L2 one can find the function that takes on the value 0 for all rational numbers and -i for the irrationals in the interval [0, 1]. This is square integrable,[nb 8] but can hardly represent a physical state.While the space of solutions as a whole is a Hilbert space there are many other Hilbert spaces that commonly occur as ingredients.More generally, one may consider a unified treatment of all second order polynomial solutions to the Sturm–Liouville equations in the setting of Hilbert space. These include the Legendre and Laguerre polynomials as well as Chebyshev polynomials, Jacobi polynomials and Hermite polynomials. All of these actually appear in physical problems, the latter ones in the harmonic oscillator, and what is otherwise a bewildering maze of properties of special functions becomes an organized body of facts. For this, see Byron & Fuller (1992, Chapter 5).There occurs also finite-dimensional Hilbert spaces. The space ℂn is a Hilbert space of dimension n. The inner product is the standard inner product on these spaces. In it, the "spin part" of a single particle wave function resides.With more particles, the situations is more complicated. One has to employ tensor products and use representation theory of the symmetry groups involved (the rotation group and the Lorentz group respectively) to extract from the tensor product the spaces in which the (total) spin wave functions reside. (Further problems arise in the relativistic case unless the particles are free.[37] See the Bethe–Salpeter equation.) Corresponding remarks apply to the concept of isospin, for which the symmetry group is SU(2). The models of the nuclear forces of the sixties (still useful today, see nuclear force) used the symmetry group SU(3). In this case, as well, the part of the wave functions corresponding to the inner symmetries reside in some ℂn or subspaces of tensor products of such spaces.Due to the infinite-dimensional nature of the system, the appropriate mathematical tools are objects of study in functional analysis.Not all introductory textbooks take the long route and introduce the full Hilbert space machinery, but the focus is on the non-relativistic Schrödinger equation in position representation for certain standard potentials. The following constraints on the wave function are sometimes explicitly formulated for the calculations and physical interpretation to make sense:[38][39]It is possible to relax these conditions somewhat for special purposes.[nb 10] If these requirements are not met, it is not possible to interpret the wave function as a probability amplitude.[40]This does not alter the structure of the Hilbert space that these particular wave functions inhabit, but it should be pointed out that the subspace of the square-integrable functions L2, which is a Hilbert space, satisfying the second requirement is not closed in L2, hence not a Hilbert space in itself.[nb 11] The functions that does not meet the requirements are still needed for both technical and practical reasons.[nb 12][nb 13]As has been demonstrated, the set of all possible wave functions in some representation for a system constitute an in general infinite-dimensional Hilbert space. Due to the multiple possible choices of representation basis, these Hilbert spaces are not unique. One therefore talks about an abstract Hilbert space, state space, where the choice of representation and basis is left undetermined. Specifically, each state is represented as an abstract vector in state space.[41] A quantum state |Ψ⟩ in any representation is generally expressed as a vectorwhere These quantum numbers index the components of the state vector. More, all α are in an n-dimensional set A = A1 × A2 × ... An where each Ai is the set of allowed values for αi; all ω are in an m-dimensional "volume" Ω ⊆ ℝm where Ω = Ω1 × Ω2 × ... Ωm and each Ωi ⊆ ℝ is the set of allowed values for ωi, a subset of the real numbers ℝ. For generality n and m are not necessarily equal.Example: (a) For a single particle in 3d with spin s, neglecting other degrees of freedom, using Cartesian coordinates, we could take α = (sz) for the spin quantum number of the particle along the z direction, and ω = (x, y, z) for the particle's position coordinates. Here A = {−s, −s + 1, ..., s − 1, s}  is the set of allowed spin quantum numbers and Ω = ℝ3 is the set of all possible particle positions throughout 3d position space. (b) An alternative choice is α = (sy) for the spin quantum number along the y direction and ω = (px, py, pz) for the particle's momentum components. In this case A and Ω are the same as before.The probability of finding system with α in some or all possible discrete-variable configurations, D ⊆ A, and ω in some or all possible continuous-variable configurations, C ⊆ Ω, is the sum and integral over the density,[nb 14]Since the sum of all probabilities must be 1, the normalization conditionmust hold at all times during the evolution of the system.The normalization condition requires ρ dmω to be dimensionless, by dimensional analysis Ψ must have the same units as (ω1ω2...ωm)−1/2.Whether the wave function really exists, and what it represents, are major questions in the interpretation of quantum mechanics. Many famous physicists of a previous generation puzzled over this problem, such as Schrödinger, Einstein and Bohr. Some advocate formulations or variants of the Copenhagen interpretation (e.g. Bohr, Wigner and von Neumann) while others, such as Wheeler or Jaynes, take the more classical approach[42] and regard the wave function as representing information in the mind of the observer, i.e. a measure of our knowledge of reality.  Some, including Schrödinger, Bohm and Everett and others, argued that the wave function must have an objective, physical existence. Einstein thought that a complete description of physical reality should refer directly to physical space and time, as distinct from the wave function, which refers to an abstract mathematical space.[43]
Perspectivity
In geometry and in its applications to drawing, a perspectivity is the formation of an image in a picture plane of a scene viewed from a fixed point.The science of graphical perspective uses perspectivities to make realistic images in proper proportion. According to Kirsti Andersen, the first author to describe perspectivity was Leon Alberti in his De Pictura (1435).[1] In English, Brook Taylor presented his Linear Perspective in 1715, where he explained "Perspective is the Art of drawing on a Plane the Appearances of any Figures, by the Rules of Geometry".[2] In a second book, New Principles of Linear Perspective (1719), Taylor wroteIn projective geometry the points of a line are called a projective range, and the set of lines in a plane on a point is called a pencil.The existence of a perspectivity means that corresponding points are in perspective. The dual concept, axial perspectivity, is the correspondence between the lines of two pencils determined by a projective range.The composition of two perspectivities is, in general, not a perspectivity. A perspectivity or a composition of two or more perspectivities is called a projectivity (projective transformation, projective collineation and homography are synonyms).There are several results concerning projectivities and perspectivities which hold in any pappian projective plane:[6]Theorem: Any projectivity between two distinct projective ranges can be written as the composition of no more than two perspectivities.Theorem: Any projectivity from a projective range to itself can be written as the composition of three perspectivities.Theorem: A projectivity between two distinct projective ranges which fixes a point is a perspectivity.The bijective correspondence between points on two lines in a plane determined by a point of that plane not on either line has higher-dimensional analogues which will also be called perspectivities.Let Sm and Tm be two distinct m-dimensional projective spaces contained in an n-dimensional projective space Rn. Let Pn−m−1 be an (n − m − 1)-dimensional subspace of Rn with no points in common with either Sm or Tm. For each point X of Sm, the space L spanned by X and Pn-m-1 meets Tm in a point Y = fP(X). This correspondence fP is also called a perspectivity.[7] The central perspectivity described above is the case with n = 2 and m = 1.Let S2 and T2 be two distinct projective planes in a projective 3-space R3. With O and O* being points of R3 in neither plane, use the construction of the last section to project S2 onto T2 by the perspectivity with center O followed by the projection of T2 back onto S2 with the perspectivity with center O*. This composition is a bijective map of the points of S2 onto itself which preserves collinear points and is called a perspective collineation (central collineation in more modern terminology).[8] Let φ be a perspective collineation of S2. Each point of the line of intersection of S2 and T2 will be fixed by φ and this line is called the axis of φ. Let point P be the intersection of line OO* with the plane S2. P is also fixed by φ and every line of S2 that passes through P is stabilized by φ (fixed, but not necessarily pointwise fixed). P is called the center of φ. The restriction of φ to any line of S2 not passing through P is the central perspectivity in S2 with center P between that line and the line which is its image under φ.
Projection (linear algebra)
In linear algebra and functional analysis, a projection is a linear transformation P from a vector space to itself such that P 2 = P. That is, whenever P is applied twice to any value, it gives the same result as if it were applied once (idempotent). It leaves its image unchanged.[1] Though abstract, this definition of "projection" formalizes and generalizes the idea of graphical projection.  One can also consider the effect of a projection on a geometrical object by examining the effect of the projection on points in the object.For example, the function which maps the point (x, y, z) in three-dimensional space R3 to the point (x, y, 0) is an orthogonal projection onto the x–y plane. This function is represented by the matrixThe action of this matrix on an arbitrary vector isTo see that P is indeed a projection, i.e., P = P2, we computeA simple example of a non-orthogonal (oblique) projection (for definition see below) isVia matrix multiplication, one sees thatproving that P is indeed a projection.The projection P is orthogonal if and only if α = 0.Let W be a finite dimensional vector space and P be a projection on W. Suppose the subspaces U and V are the range and kernel of P respectively.Then P has the following properties:In infinite dimensional vector spaces, thespectrum of a projection is contained in {0, 1} asThe product of projections is not, in general, a projection, even if they are orthogonal. If projections commute, then their product is a projection.A projection is orthogonal if and only if it is self-adjoint. Using the self-adjoint and idempotent properties of P, for any x and y in W we have Px ∈ U, y − Py ∈ V, andfor every x and y in W; thus P = P*.Let H be a complete metric space with an inner product, and let U be a closed linear subspace of H (and hence complete as well).Obviously Px is in U. It remains to show that Px satisfies <x − Px, Px> = 0 and that it is linear.By taking the difference between the equations we have But since we may choose v = Px + Py − P(x + y) (as it is itself in U) it follows that Px + Py = P(x + y). Similarly we have λPx = P(λx) for every scalar λ.An orthogonal projection is a bounded operator. This is because for every v in the vector space we have, by Cauchy–Schwarz inequality:A simple case occurs when the orthogonal projection is onto a line. If u is a unit vector on the line, then the projection is given by the outer productby the properties of the dot product of parallel and perpendicular vectors.This formula can be generalized to orthogonal projections on a subspace of arbitrary dimension. Let u1, ..., uk be an orthonormal basis of the subspace U, and let A denote the n-by-k matrix whose columns are u1, ..., uk. Then the projection is given by:[4]which can be rewritten asThe matrix AT is the partial isometry that vanishes on the orthogonal complement of U and A is the isometry that embeds U into the underlying vector space. The range of PA is therefore the final space of A. It is also clear that A·AT is the identity operator on U.The orthonormality condition can also be dropped. If u1, ..., uk is a (not necessarily orthonormal) basis, and A is the matrix with these vectors as columns, then the projection is:[5][6]If the orthogonal condition is enhanced to AT W B = AT WT B = 0 with W non-singular, the following holds:All these formulas also hold for complex inner product spaces, provided that the conjugate transpose is used instead of the transpose. Further details on sums of projectors can be found in Banerjee and Roy (2014).[8] Also see Banerjee (2004)[9] for application of sums of projectors in basic spherical trigonometry.The term oblique projections is sometimes used to refer to non-orthogonal projections. These projections are also used to represent spatial figures in two-dimensional drawings (see oblique projection), though not as frequently as orthogonal projections. Whereas calculating the fitted value of an ordinary least squares regression requires an orthogonal projection, calculating the fitted value of an instrumental variables regression requires an oblique projection.Projections are defined by their null space and the basis vectors used to characterize their range (which is the complement of the null space). When these basis vectors are orthogonal to the null space, then the projection is an orthogonal projection. When these basis vectors are not orthogonal to the null space, the projection is an oblique projection. Let the vectors u1, ..., uk form a basis for the range of the projection, and assemble these vectors in the n-by-k matrix A. The range and the null space are complementary spaces, so the null space has dimension n − k. It follows that the orthogonal complement of the null space has dimension k. Let v1, ..., vk form a basis for the orthogonal complement of the null space of the projection, and assemble these vectors in the matrix B. Then the projection is defined byThis expression generalizes the formula for orthogonal projections given above.[10][11]Any projection P = P2 on a vector space of dimension d over a field is a diagonalizable matrix, since its minimal polynomial divides x2 − x, which splits into distinct linear factors.  Thus there exists a basis in which P has the formwhere r is the rank of P.  Here Ir is the identity matrix of size r, and 0d−r is the zero matrix of size d − r.  If the vector space is complex and equipped with an inner product, then there is an orthonormal basis in which the matrix of P is[12]The converse holds also, with an additional assumption. Suppose U is a closed subspace of X. If there exists a closed subspace V such that X = U ⊕ V, then the projection P with range U and kernel V is continuous. This follows from the closed graph theorem. Suppose xn → x and Pxn → y. One needs to show that Px = y. Since U is closed and {Pxn} ⊂ U, y lies in U, i.e. Py = y. Also, xn − Pxn = (I − P)xn → x − y. Because V is closed and {(I − P)xn} ⊂ V, we have x − y ∈ V, i.e. P(x − y) = Px − Py = Px − y = 0, which proves the claim.The above argument makes use of the assumption that both U and V are closed. In general, given a closed subspace U, there need not exist a complementary closed subspace V, although for Hilbert spaces this can always be done by taking the orthogonal complement. For Banach spaces, a one-dimensional subspace always has a closed complementary subspace. This is an immediate consequence of Hahn–Banach theorem. Let U be the linear span of u. By Hahn–Banach, there exists a bounded linear functional φ such that φ(u) = 1. The operator P(x) = φ(x)u satisfies P2 = P, i.e. it is a projection. Boundedness of φ implies continuity of P and therefore ker(P) = ran(I − P) is a closed complementary subspace of U.Projections (orthogonal and otherwise) play a major role in algorithms for certain linear algebra problems:As stated above, projections are a special case of idempotents. Analytically, orthogonal projections are non-commutative generalizations of characteristic functions. Idempotents are used in classifying, for instance, semisimple algebras, while measure theory begins with considering characteristic functions of measurable sets. Therefore, as one can imagine, projections are very often encountered in the context operator algebras. In particular, a von Neumann algebra is generated by its complete lattice of projections.
Spectral theorem
In mathematics, particularly linear algebra and functional analysis, a spectral theorem is a result about when a linear operator or matrix can be diagonalized (that is, represented as a diagonal matrix in some basis). This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix. The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces. In general, the spectral theorem identifies a class of linear operators that can be modeled by multiplication operators, which are as simple as one can hope to find. In more abstract language, the spectral theorem is a statement about commutative C*-algebras. See also spectral theory for a historical perspective.Examples of operators to which the spectral theorem applies are self-adjoint operators or more generally normal operators on Hilbert spaces.The spectral theorem also provides a canonical decomposition, called the spectral decomposition, eigenvalue decomposition, or eigendecomposition, of the underlying vector space on which the operator acts.Augustin-Louis Cauchy proved the spectral theorem for self-adjoint matrices, i.e., that every real, symmetric matrix is diagonalizable. In addition, Cauchy was the first to be systematic about determinants.[1][2] The spectral theorem as generalized by John von Neumann is today perhaps the most important result of operator theory.This article mainly focuses on the simplest kind of spectral theorem, that for a self-adjoint operator on a Hilbert space. However, as noted above, the spectral theorem also holds for normal operators on a Hilbert space.(An equivalent condition is that A∗ = A, where A∗ is the hermitian conjugate of A.) In the case that A is identified with a Hermitian matrix, the matrix of A∗ can be identified with its conjugate transpose. (If A is a real matrix, this is equivalent to AT = A, that is, A is a symmetric matrix.)This condition implies that all eigenvalues of a Hermitian map are real: it is enough to apply it to the case when x = y is an eigenvector. (Recall that an eigenvector of a linear map A is a (non-zero) vector x such that Ax = λx for some scalar λ. The value λ is the corresponding eigenvalue. Moreover, the eigenvalues are solutions to the characteristic polynomial.)Theorem. If A is Hermitian, there exists an orthonormal basis of V consisting of eigenvectors of A. Each eigenvalue is real.We provide a sketch of a proof for the case where the underlying field of scalars is the complex numbers.By the fundamental theorem of algebra, applied to the characteristic polynomial of A, there is at least one eigenvalue λ1 and eigenvector e1. Then since we find that λ1 is real. Now consider the space K = span{e1}⊥, the orthogonal complement of e1. By Hermiticity, K is an invariant subspace of A. Applying the same argument to K shows that A has an eigenvector e2 ∈ K. Finite induction then finishes the proof.The spectral theorem holds also for symmetric maps on finite-dimensional real inner product spaces, but the existence of an eigenvector does not follow immediately from the  fundamental theorem of algebra. To prove this, consider A as a Hermitian matrix and use the fact that all eigenvalues of a Hermitian matrix are real.If one chooses the eigenvectors of A as an orthonormal basis, the matrix representation of A in this basis is diagonal. Equivalently, A can be written as a linear combination of pairwise orthogonal projections, called its spectral decomposition. Letbe the eigenspace corresponding to an eigenvalue λ. Note that the definition does not depend on any choice of specific eigenvectors. V is the orthogonal direct sum of the spaces Vλ where the index ranges over eigenvalues. In other words, if Pλ denotes the orthogonal projection onto Vλ, and λ1, ..., λm are the eigenvalues of A, then the spectral decomposition may be written asThe spectral decomposition is a special case of both the Schur decomposition and the singular value decomposition.The spectral theorem extends to a more general class of matrices. Let A be an operator on a finite-dimensional inner product space. A is said to be normal  if A∗A = AA∗. One can show that A is normal if and only if it is unitarily diagonalizable. Proof: By the Schur decomposition, we can write any matrix as A = UTU∗, where U is unitary and T is upper-triangular.If A is normal, one sees that TT∗ = T*T. Therefore, T must be diagonal since a normal upper triangular matrix is diagonal (see normal matrix). The converse is obvious.In other words, A is normal if and only if there exists a unitary matrix U such thatwhere D is a diagonal matrix. Then, the entries of the diagonal of D are the eigenvalues of A. The column vectors of U are the eigenvectors of A and they are orthonormal. Unlike the Hermitian case, the entries of D need not be real.In the more general setting of Hilbert spaces, which may have an infinite dimension, the statement of the spectral theorem for compact self-adjoint operators is virtually the same as in the finite-dimensional case.Theorem. Suppose A is a compact self-adjoint operator on a (real or complex) Hilbert space V. Then there is an orthonormal basis of V consisting of eigenvectors of A. Each eigenvalue is real.As for Hermitian matrices, the key point is to prove the existence of at least one nonzero eigenvector. One cannot rely on determinants to show existence of eigenvalues, but one can use a maximization argument analogous to the variational characterization of eigenvalues. If the compactness assumption is removed, it is not true that every self-adjoint operator has eigenvectors.The next generalization we consider is that of bounded self-adjoint operators on a Hilbert space. Such operators may have no eigenvalues: for instance let A be the operator of multiplication by t on L2[0, 1], that is,[3]One formulation of the spectral theorem expresses the operator A as an integral of the coordinate function over the operator's spectrum with respect to a projection-valued measure.[5]When the self-adjoint operator in question is compact, this version of the spectral theorem reduces to something similar to the finite-dimensional spectral theorem above, except that the operator is expressed as a finite or countably infinite linear combination of projections, that is, the measure consists only of atoms.An alternative formulation of the spectral theorem says that every bounded self-adjoint operator is unitarily equivalent to a multiplication operator. The significance of this result is that multiplication operators are in many ways easy to understand.Theorem.[6] Let A  be a bounded self-adjoint operator on a Hilbert space H.  Then there is a measure space (X, Σ, μ) and a real-valued essentially bounded measurable function f on X and a unitary operator U:H → L2μ(X) such thatThe spectral theorem is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.There is also an analogous spectral theorem for bounded normal operators on Hilbert spaces.  The only difference in the conclusion is that now f may be complex-valued.There is also a formulation of the spectral theorem in terms of direct integrals. It is similar to the multiplication-operator formulation, but more canonical.Many important linear operators which occur in analysis, such as differential operators, are unbounded. There is also a spectral theorem for self-adjoint operators that applies in these cases.  To give an example, every constant-coefficient differential operator is unitarily equivalent to a multiplication operator. Indeed, the unitary operator that implements this equivalence is the Fourier transform; the multiplication operator is a type of Fourier multiplier.In general, spectral theorem for self-adjoint operators may take several equivalent forms.[10] Notably, all of the formulations given in the previous section for bounded self-adjoint operators—the projection-valued measure version, the multiplication-operator version, and the direct-integral version—continue to hold for unbounded self-adjoint operators, with small technical modifications to deal with domain issues.
Quaternion
In mathematics, the quaternions are a number system that extends the complex numbers. They were first described by Irish mathematician William Rowan Hamilton in 1843[1][2]  and applied to mechanics in three-dimensional space. A feature of quaternions is that multiplication of two quaternions is noncommutative. Hamilton defined a quaternion as the quotient of two directed lines in a three-dimensional space[3] or equivalently as the quotient of two vectors.[4]Quaternions are generally represented in the form:where a, b, c, and d are real numbers, and  i, j, and k are the fundamental quaternion units.Quaternions find uses in both pure and applied mathematics, in particular for calculations involving three-dimensional rotations such as in three-dimensional computer graphics, computer vision, and crystallographic texture analysis.[5] In practical applications, they can be used alongside other methods, such as Euler angles and rotation matrices, or as an alternative to them, depending on the application.The unit quaternions can be thought of as a choice of a group structure on the 3-sphere S3 that gives the group Spin(3), which is isomorphic to SU(2) and also to the universal cover of SO(3).Quaternions were introduced by Hamilton in 1843.[7] Important precursors to this work included Euler's four-square identity (1748) and Olinde Rodrigues' parameterization of general rotations by four parameters (1840), but neither of these writers treated the four-parameter rotations as an algebra.[8][9] Carl Friedrich Gauss had also discovered quaternions in 1819, but this work was not published until 1900.[10][11]Hamilton knew that the complex numbers could be interpreted as points in a plane, and he was looking for a way to do the same for points in three-dimensional space. Points in space can be represented by their coordinates, which are triples of numbers, and for many years he had known how to add and subtract triples of numbers. However, Hamilton had been stuck on the problem of multiplication and division for a long time. He could not figure out how to calculate the quotient of the coordinates of two points in space.The great breakthrough in quaternions finally came on Monday 16 October 1843 in Dublin, when Hamilton was on his way to the Royal Irish Academy where he was going to preside at a council meeting. As he walked along the towpath of the Royal Canal with his wife, the concepts behind quaternions were taking shape in his mind. When the answer dawned on him, Hamilton could not resist the urge to carve the formula for the quaternions,into the stone of Brougham Bridge as he paused on it. Although the carving has since faded away, there has been an annual pilgrimage since 1989 called the Hamilton Walk for scientists and mathematicians who walk from Dunsink Observatory to the Royal Canal bridge in remembrance of Hamilton's discovery.On the following day, Hamilton wrote a letter to his friend and fellow mathematician, John T. Graves, describing the train of thought that led to his discovery. This letter was later published in a letter to a science magazine;[12] Hamilton states:And here there dawned on me the notion that we must admit, in some sense, a fourth dimension of space for the purpose of calculating with triples ... An electric circuit seemed to close, and a spark flashed forth.[12]Hamilton called a quadruple with these rules of multiplication a quaternion, and he devoted most of the remainder of his life to studying and teaching them. Hamilton's treatment is more geometric than the modern approach, which emphasizes quaternions' algebraic properties. He founded a school of "quaternionists", and he tried to popularize quaternions in several books. The last and longest of his books, Elements of Quaternions,[13] was 800 pages long; it was edited by his son and published shortly after his death.After Hamilton's death, his student Peter Tait continued promoting quaternions. At this time, quaternions were a mandatory examination topic in Dublin. Topics in physics and geometry that would now be described using vectors, such as kinematics in space and Maxwell's equations, were described entirely in terms of quaternions. There was even a professional research association, the Quaternion Society, devoted to the study of quaternions and other hypercomplex number systems.From the mid-1880s, quaternions began to be displaced by vector analysis, which had been developed by Josiah Willard Gibbs, Oliver Heaviside, and Hermann von Helmholtz. Vector analysis described the same phenomena as quaternions, so it borrowed some ideas and terminology liberally from the literature of quaternions. However, vector analysis was conceptually simpler and notationally cleaner, and eventually quaternions were relegated to a minor role in mathematics and physics. A side-effect of this transition is that Hamilton's work is difficult to comprehend for many modern readers. Hamilton's original definitions are unfamiliar and his writing style was wordy and difficult to understand.However, quaternions have had a revival since the late 20th century, primarily due to their utility in describing spatial rotations. The representations of rotations by quaternions are more compact and quicker to compute than the representations by matrices.  In addition, unlike Euler angles, they are not susceptible to “gimbal lock”. For this reason, quaternions are used in computer graphics,[14][15] computer vision, robotics,[16] control theory, signal processing, attitude control, physics, bioinformatics,[17][18] molecular dynamics, computer simulations, and orbital mechanics. For example, it is common for the attitude control systems of spacecraft to be commanded in terms of quaternions.  Quaternions have received another boost from number theory because of their relationships with the quadratic forms.[19]P.R. Girard's essay The quaternion group and modern physics[20] discusses some roles of quaternions in physics. It "shows how various physical covariance groups: SO(3), the Lorentz group, the general relativity group, the Clifford algebra SU(2), and the conformal group can be readily related to the quaternion group" in modern algebra. Girard began by discussing group representations and by representing some space groups of crystallography. He proceeded to kinematics of rigid body motion. Next he used complex quaternions (biquaternions) to represent the Lorentz group of special relativity, including the Thomas precession. He cited five authors, beginning with Ludwik Silberstein, who used a potential function of one quaternion variable to express Maxwell's equations in a single differential equation. Concerning general relativity, he expressed the Runge–Lenz vector. He mentioned the Clifford biquaternions (split-biquaternions) as an instance of Clifford algebra. Finally, invoking the reciprocal of a biquaternion, Girard described conformal maps on spacetime. Among the fifty references, Girard included Alexander Macfarlane and his Bulletin of the Quaternion Society. In 1999 he showed how Einstein's equations of general relativity could be formulated within a Clifford algebra that is directly linked to quaternions.[21]The finding of 1924 that in quantum mechanics the spin of an electron and other matter particles (known as spinors) can be described using quaternions furthered their interest; quaternions helped to understand how rotations of electrons by 360° can be discerned from those by 720° (the “Plate trick”).[22][23] As of  2018[update], their use has not overtaken rotation groups.[a]A quaternion is an expression of the form where a, b, c, d, are real numbers, and i, j, k, are symbols that can be interpreted as unit-vectors pointing along the three spatial axes. In practice, if one of a, b, c, d is 0, the corresponding term is omitted; if a, b, c, d are all zero, the quaternion is the zero quaternion, denoted 0; if one of b, c, d  equals 1, the corresponding term is written simply i, j, or k.and the componentwise scalar multiplicationA multiplicative group structure, called the Hamilton product, denoted by juxtaposition, can be defined on the quaternions in the following way:Thus the quaternions form a division algebra.The basis elements i, j, and k commute with the real quaternion 1, that isThe other products of basis elements are defined byandThese multiplication formulas are equivalent toIn fact, the equality ijk = –1 results fromThe converse implication results from manipulations similar to the following. By right-multiplying both sides of −1 = ijk by –k, one getsAll other products can be determined by similar methods.The center of a noncommutative ring is the subring of elements c such that cx = xc for every x. The center of the quaternion algebra is the subfield of real quaternions. In fact, it is a part of the definition that the real quaternions belong to the center. Conversely, if q = a + bi + cj + dk belongs to the center, thenand c = d = 0. A similar computation with j instead of i shows that one has also b = 0. Thus q = a is a real quaternion.The noncommutativity of multiplication has some unexpected consequences, among them that polynomial equations over the quaternions can have more distinct solutions than the degree of the polynomial. For example, the equation z2 + 1 = 0, has infinitely many quaternion solutions, which are the quaternions z = bi + cj + dk such that b2 + c2 + d2 = 1. Thus these "roots of –1" form a unit sphere in the three-dimensional space of vector quaternions.For two elements a1 + b1i + c1j + d1k and a2 + b2i + c2j + d2k, their product, called the  Hamilton product (a1 + b1i + c1j + d1k) (a2 + b2i + c2j + d2k), is determined by the products of the basis elements and the distributive law. The distributive law makes it possible to expand the product so that it is a sum of products of basis elements. This gives the following expression:Now the basis elements can be multiplied using the rules given above to get:[7]The product of two rotation quaternions[24] will be equivalent to the rotation a2 + b2i + c2j + d2k followed by the rotation  a1 + b1i + c1j + d1k.A quaternion of the form a + 0i + 0j + 0k, where a is a real number, is called scalar, and a quaternion of the form 0 + bi + cj + dk, where b, c, and d are real numbers, and at least one of b, c or d is nonzero, is called a vector quaternion.  If a + bi + cj + dk is any quaternion, then a is called its scalar part and bi + cj + dk is called its vector part. Even though every quaternion can be viewed as a vector in a four-dimensional vector space, it is common to refer to the vector part as vectors in three-dimensional space.  With this convention, a vector is the same as an element of the vector space R3.[b]Hamilton also called vector quaternions right quaternions[25][26] and real numbers (considered as quaternions with zero vector part) scalar quaternions.If a quaternion is divided up into a scalar part and a vector part, i.e.then the formulas for addition and multiplication are:where "·" is the dot product and "×" is the cross product.The conjugation of a quaternion, in stark contrast to the complex setting, can be expressed with multiplication and addition of quaternions:Conjugation can be used to extract the scalar and vector parts of a quaternion. The scalar part of p is (p + p∗) / 2, and the vector part of p is (p − p∗) / 2.The square root of the product of a quaternion with its conjugate is called its norm and is denoted ||q|| (Hamilton called this quantity the tensor of q, but this conflicts with modern meaning of "tensor"). In formula, this is expressed as follows:This is always a non-negative real number, and it is the same as the Euclidean norm on H considered as the vector space R4. Multiplying a quaternion by a real number scales its norm by the absolute value of the number. That is, if α is real, thenThis is a special case of the fact that the norm is multiplicative, meaning thatfor any two quaternions p and q. Multiplicativity is a consequence of the formula for the conjugate of a product.Alternatively it follows from the identity(where i denotes the usual imaginary unit) and hence from the multiplicative property of determinants of square matrices.This norm makes it possible to define the distance d(p, q) between p and q as the norm of their difference:This makes H into a metric space. Addition and multiplication are continuous in the metric topology. Indeed, for any scalar, positive a it holdsContinuity follows from taking a to zero in the limit. Continuity for multiplication holds similarly.A unit quaternion is a quaternion of norm one. Dividing a non-zero quaternion q by its norm produces a unit quaternion Uq called the versor of q:This makes it possible to divide two quaternions p and q in two different ways (when q is non-zero). That is, their quotient can be either p q−1 or q−1p. The notation p/q is ambiguous because it does not specify whether q divides on the left or the right.The set H of all quaternions is a vector space over the real numbers with dimension 4. (In comparison, the real numbers have dimension 1, the complex numbers have dimension 2, and the octonions have dimension 8.) Multiplication of quaternions is associative and distributes over vector addition, but it is not commutative. Therefore, the quaternions H are a non-commutative associative algebra over the real numbers. Even though H contains copies of the complex numbers, it is not an associative algebra over the complex numbers.Because it is possible to divide quaternions, they form a division algebra. This is a structure similar to a field except for the non-commutativity of multiplication. Finite-dimensional associative division algebras over the real numbers are very rare. The Frobenius theorem states that there are exactly three: R, C, and H. The norm makes the quaternions into a normed algebra, and normed division algebras over the reals are also very rare: Hurwitz's theorem says that there are only four: R, C, H, and O (the octonions). The quaternions are also an example of a composition algebra and of a unital Banach algebra.Because the product of any two basis vectors is plus or minus another basis vector, the set {±1, ±i, ±j, ±k}  forms a group under multiplication. This non-Abelian Group is called the quaternion group and is denoted Q8.[27] The real group ring of Q8 is a ring R[Q8] which is also an eight-dimensional vector space over R. It has one basis vector for each element of Q8.  The quaternions are the quotient ring of R[Q8] by the ideal generated by the elements 1 + (−1), i + (−i), j + (−j), and k + (−k). Here the first term in each of the differences is one of the basis elements 1, i, j, and k, and the second term is one of basis elements −1, −i, −j, and −k, not the additive inverses of 1, i, j, and k.The vector part of a quaternion can be interpreted as a coordinate vector in R3, therefore the algebraic operations of the quaternions reflect the geometry of R3.  Operations such as the vector dot and cross products can be defined in terms of quaternions, and this makes it possible to apply quaternion techniques wherever spatial vectors arise. A useful application of quaternions has been to interpolate the orientations of key-frames in computer graphics.[14]For the remainder of this section, i, j, and k will denote both the three imaginary[28] basis vectors of H and a basis for R3. Notice that replacing i by −i, j by −j, and k by −k sends a vector to its additive inverse, so the additive inverse of a vector is the same as its conjugate as a quaternion. For this reason, conjugation is sometimes called the spatial inverse.For two vector quaternions p = b1i + c1j + d1k and q = b2i + c2j + d2k their dot product, by analogy to vectors in R3, isIt can also be expressed in a component-free manner asThis is equal to the scalar parts of the products pq∗, qp∗, p∗q, and q∗p. Note that their vector parts are different.The cross product of p and q relative to the orientation determined by the ordered basis i, j, and k is(Recall that the orientation is necessary to determine the sign.) This is equal to the vector part of the product pq (as quaternions), as well as the vector part of −q∗p∗. It also has the formulaFor the commutator, [p, q] = pq − qp, of two vector quaternions one obtainsIn general, let p and q be quaternions and writeThis shows that the noncommutativity of quaternion multiplication comes from the multiplication of vector quaternions. It also shows that two quaternions commute if and only if their vector parts are collinear.  Hamilton[29] showed that this product computes the third vertex of a spherical triangle from two given vertices and their associated arc-lengths, which is also an algebra of points in Elliptic geometry.Unit quaternions can be identified with rotations in R3 and were called versors by Hamilton[29].  Also see Quaternions and spatial rotation for more information about modeling three-dimensional rotations using quaternions.See Andrew J. Hanson.[30] for visualization of quaternions,Just as complex numbers can be represented as matrices, so can quaternions. There are at least two ways of representing quaternions as matrices in such a way that quaternion addition and multiplication correspond to matrix addition and matrix multiplication. One is to use 2 × 2 complex matrices, and the other is to use 4 × 4 real matrices. In each case, the representation given is one of a family of linearly related representations. In the terminology of abstract algebra, these are injective homomorphisms from H to the matrix rings M(2, C) and M(4, R), respectively.Using 2 × 2 complex matrices, the quaternion a + bi + cj + dk can be represented asThis representation has the following properties:Using 4 × 4 real matrices, that same quaternion can be written asHowever, the representation of quaternions in M(4,ℝ) is not unique.  For example, the same quaternion can also be represented asIn fact, there exist 48 distinct representations of this form.  More precisely, there are 48 sets of quadruples of matrices such that a function sending 1, i, j, and k to the matrices in the quadruple is a homomorphism, that is, it sends sums and products of quaternions to sums and products of matrices.[32] In this representation, the conjugate of a quaternion corresponds to the transpose of the matrix. The fourth power of the norm of a quaternion is the determinant of the corresponding matrix. As with the 2 × 2 complex representation above, complex numbers can again be produced by constraining the coefficients suitably; for example, as block diagonal matrices with two 2 × 2 blocks by setting c = d = 0.Each 4x4 matrix representation of quaternions corresponds to a multiplication table of unit quaternions. For example, the last matrix representation given above corresponds to the multiplication tableConstraining any such multiplication table to have the identity in the first row and column and for the signs of the row headers to be opposite to those of the column headers, then there are 3 possible choices for the second column (ignoring sign), 2 possible choices for the third column (ignoring sign), and 1 possible choice for the fourth column (ignoring sign); that makes 6 possibilities. Then, the second column can be chosen to be either positive or negative, the third column can be chosen to be positive or negative, and the fourth column can be chosen to be positive or negative, giving 8 possibilities for the sign. Multiplying the possibilities for the letter positions and for their signs yields 48. Then replacing 1 with a, i with b, j with c, and k with d and removing the row and column headers yields a matrix representation of a + bi + cj + dk.Quaternions are also used in one of the proofs of Lagrange's four-square theorem in number theory, which states that every nonnegative integer is the sum of four integer squares. As well as being an elegant theorem in its own right, Lagrange's four square theorem has useful applications in areas of mathematics outside number theory, such as combinatorial design theory. The quaternion-based proof uses Hurwitz quaternions, a subring of the ring of all quaternions for which there is an analog of the Euclidean algorithm.Quaternions can be represented as pairs of complex numbers. From this perspective, quaternions are the result of applying the Cayley–Dickson construction to the complex numbers. This is a generalization of the construction of the complex numbers as pairs of real numbers.Let C2 be a two-dimensional vector space over the complex numbers. Choose a basis consisting of two elements 1 and j.  A vector in C2 can be written in terms of the basis elements 1 and j asIf we define j2 = −1 and ij = −ji, then we can multiply two vectors using the distributive law. Writing k in place of the product ij leads to the same rules for multiplication as the usual quaternions. Therefore, the above vector of complex numbers corresponds to the quaternion a + bi + cj + dk. If we write the elements of C2 as ordered pairs and quaternions as quadruples, then the correspondence isIn the complex numbers, C, there are just two numbers, i and −i, whose square is −1 . In H there are infinitely many square roots of minus one: the quaternion solution for the square root of −1 is the unit sphere in R3.  To see this, let q = a + bi + cj + dk be a quaternion, and assume that its square is −1.  In terms of a, b, c, and d, this meansTo satisfy the last three equations, either a = 0 or b, c, and d are all 0.  The latter is impossible because a is a real number and the first equation would imply that  a2 = −1.  Therefore,  a = 0 and  b2 + c2 + d2 = 1. In other words: a quaternion squares to −1 if and only if it is a vector quaternion with norm 1. By definition, the set of all such vectors forms the unit sphere.Only negative real quaternions have infinitely many square roots. All others have just two (or one in the case of 0).[citation needed]The identification of the square roots of minus one in H was given by Hamilton[33] but was frequently omitted in other texts. By 1971 the sphere was included by Sam Perlis in his three-page exposition included in Historical Topics in Algebra (page 39) published by the National Council of Teachers of Mathematics. More recently, the sphere of square roots of minus one is described in Ian R. Porteous's book Clifford Algebras and the Classical Groups (Cambridge, 1995) in proposition 8.13 on page 60.Each pair of square roots of −1 creates a distinct copy of the complex numbers inside the quaternions.  If  q2 = −1, then the copy is determined by the functionIn the language of abstract algebra, each is an injective ring homomorphism from C to H.  The images of the embeddings corresponding to q and −q are identical.Every non-real quaternion lies in a subspace of H isomorphic to C.  Write q as the sum of its scalar part and its vector part:Decompose the vector part further as the product of its norm and its versor:The relationship of quaternions to each other within the complex subplanes of H can also be identified and expressed in terms of commutative subrings. Specifically, since two quaternions p and q commute (i.e., pq = qp) only if they lie in the same complex subplane of H, the profile of H as a union of complex planes arises when one seeks to find all commutative subrings of the quaternion ring. This method of commutative subrings is also used to profile the split-quaternions, which as an algebra over the reals are isomorphic to 2 × 2 real matrices.Like functions of a complex variable, functions of a quaternion variable suggest useful physical models. For example, the original electric and magnetic fields described by Maxwell were functions of a quaternion variable. Examples of other functions include the extension of the Mandelbrot set and Julia sets into 4 dimensional space.[34] Given a quaternion,the exponential is computed asIt follows that the polar decomposition of a quaternion may be writtenandThe geodesic distance dg (p, q) between unit quaternions p and q is defined as:and amounts to the absolute value of half the angle subtended by p and q along a great arc of the S3 sphere.This angle can also be computed from the quaternion dot product without the logarithm as:The term "conjugation", besides the meaning given above, can also mean taking an element a to rar−1 where r is some non-zero quaternion. All elements that are conjugate to a given element (in this sense of the word conjugate) have the same real part and the same norm of the vector part. (Thus the conjugate in the other sense is one of the conjugates in this sense.)Thus the multiplicative group of non-zero quaternions acts by conjugation on the copy of R3 consisting of quaternions with real part equal to zero. Conjugation by a unit quaternion (a quaternion of absolute value 1) with real part cos(θ) is a rotation by an angle 2θ, the axis of the rotation being the direction of the vector part. The advantages of quaternions are:The set of all unit quaternions (versors) forms a 3-sphere S3 and a group (a Lie group) under multiplication, double covering the group SO(3, R) of real orthogonal 3×3 matrices of determinant 1 since two unit quaternions correspond to every rotation under the above correspondence. See the plate trick.The image of a subgroup of versors  is a point group, and conversely, the preimage of a point group is a subgroup of versors. The preimage of a finite point group is called by the same name, with the prefix binary. For instance, the preimage of the icosahedral group is the binary icosahedral group.The versors' group is isomorphic to SU(2), the group of complex unitary 2×2 matrices of determinant 1.Let A be the set of quaternions of the form a + bi + cj + dk where a, b, c, and d are either all integers or all rational numbers with odd numerator and denominator 2. The set A is a ring (in fact a domain) and a lattice and is called the ring of Hurwitz quaternions. There are 24 unit quaternions in this ring, and they are the vertices of a regular 24-cell with Schläfli symbol {3,4,3}. They correspond to the double cover of the rotational symmetry group of the regular tetrahedron. Similarly, the vertices of a regular 600-cell with Schläfli symbol {3,3,5} can be taken as the unit icosians, corresponding to the double cover of the rotational symmetry group of the regular icosahedron. The double cover of the rotational symmetry group of the regular octahedron corresponds to the quaternions that represent the vertices of the disphenoidal 288-cell.The Quaternions can be generalized into further algebras called quaternion algebras. Take F to be any field with characteristic different from 2, and a and b to be elements of F; a four-dimensional unitary associative algebra can be defined over F with basis 1, i, j, and ij, where i2 = a, j2 = b and ij = −ji (so (ij)2 = −ab).Quaternion algebras are isomorphic to the algebra of 2×2 matrices over F or form division algebras over F, depending on the choice of a and b.The usefulness of quaternions for geometrical computations can be generalised to other dimensions by identifying the quaternions as the even part Cℓ+3,0(R) of the Clifford algebra Cℓ3,0(R).  This is an associative multivector algebra built up from fundamental basis elements σ1, σ2, σ3 using the product rulesIf these fundamental basis elements are taken to represent vectors in 3D space, then it turns out that the reflection of a vector r in a plane perpendicular to a unit vector w can be written:Two reflections make a rotation by an angle twice the angle between the two reflection planes, socorresponds to a rotation of 180° in the plane containing σ1 and σ2.  This is very similar to the corresponding quaternion formula,In fact, the two are identical, if we make the identificationand it is straightforward to confirm that this preserves the Hamilton relationsIn this picture, quaternions correspond not to vectors but to bivectors – quantities with magnitude and orientations associated with particular 2D planes rather than 1D directions.   The relation to complex numbers becomes clearer, too: in 2D, with two vector directions σ1 and σ2, there is only one bivector basis element σ1σ2, so only one imaginary.  But in 3D, with three vector directions, there are three bivector basis elements σ1σ2, σ2σ3, σ3σ1, so three imaginaries.This reasoning extends further.  In the Clifford algebra Cℓ4,0(R), there are six bivector basis elements, since with four different basic vector directions, six different pairs and therefore six different linearly independent planes can be defined. Rotations in such spaces using these generalisations of quaternions, called rotors, can be very useful for applications involving homogeneous coordinates.  But it is only in 3D that the number of basis bivectors equals the number of basis vectors, and each bivector can be identified as a pseudovector.There are several advantages for placing quaternions in this wider setting:[37]For further detail about the geometrical uses of Clifford algebras, see Geometric algebra.The quaternions are "essentially" the only (non-trivial) central simple algebra (CSA) over the real numbers, in the sense that every CSA over the reals is Brauer equivalent to either the reals or the quaternions. Explicitly, the Brauer group of the reals consists of two classes, represented by the reals and the quaternions, where the Brauer group is the set of all CSAs, up to equivalence relation of one CSA being a matrix ring over another. By the Artin–Wedderburn theorem (specifically, Wedderburn's part), CSAs are all matrix algebras over a division algebra, and thus the quaternions are the only non-trivial division algebra over the reals.CSAs – rings over a field, which are simple algebras (have no non-trivial 2-sided ideals, just as with fields) whose center is exactly the field – are a noncommutative analog of extension fields, and are more restrictive than general ring extensions. The fact that the quaternions are the only non-trivial CSA over the reals (up to equivalence) may be compared with the fact that the complex numbers are the only non-trivial field extension of the reals.
Translation
Translation is the communication of the meaning of a source-language text by means of an equivalent target-language text.[1]  The English language draws a terminological distinction (not all languages do) between translating (a written text) and interpreting (oral or sign-language communication between users of different languages); under this distinction, translation can begin only after the appearance of writing within a language community.A translator always risks inadvertently introducing source-language words, grammar, or syntax into the target-language rendering. On the other hand, such "spill-overs" have sometimes imported useful source-language calques and loanwords that have enriched target languages.  Translators, including early translators of sacred texts, have helped shape the very languages into which they have translated.[2]Because of the laboriousness of the translation process, since the 1940s efforts have been made, with varying degrees of success, to automate translation or to mechanically aid the human translator.[3] More recently, the rise of the Internet has fostered a world-wide market for translation services and has facilitated "language localization".[4]The English word "translation" derives from the Latin word translatio,[6] which comes from trans, "across" + ferre, "to carry" or "to bring" (-latio in turn coming from latus, the past participle of ferre).  Thus translatio is "a carrying across" or "a bringing across":  in this case, of a text from one language to another.[7]The Germanic languages (except for modern Dutch) and some Slavic languages have calqued their words for the concept of "translation" on translatio.[7]The Romance languages and the remaining Slavic languages have derived their words for the concept of "translation" from an alternative Latin word, traductio, itself derived from traducere ("to lead across" or "to bring across", from trans, "across" + ducere, "to lead" or "to bring").[7]The Ancient Greek term for "translation", μετάφρασις (metaphrasis, "a speaking across"), has supplied English with "metaphrase" (a "literal", or "word-for-word", translation)—as contrasted with "paraphrase" ("a saying in other words", from  παράφρασις, paraphrasis).[7] "Metaphrase" corresponds, in one of the more recent terminologies, to "formal equivalence"; and "paraphrase", to "dynamic equivalence".[8]Strictly speaking, the concept of metaphrase—of "word-for-word translation"—is an imperfect concept, because a given word in a given language often carries more than one meaning; and because a similar given meaning may often be represented in a given language by more than one word.  Nevertheless, "metaphrase" and "paraphrase" may be useful as ideal concepts that mark the extremes in the spectrum of possible approaches to translation.[a]Discussions of the theory and practice of translation reach back into antiquity and show remarkable continuities. The ancient Greeks distinguished between metaphrase (literal translation) and paraphrase. This distinction was adopted by English poet and translator John Dryden (1631–1700), who described translation as the judicious blending of these two modes of phrasing when selecting, in the target language, "counterparts," or equivalents, for the expressions used in the source language:When [words] appear... literally graceful, it were an injury to the author that they should be changed. But since... what is beautiful in one [language] is often barbarous, nay sometimes nonsense, in another, it would be unreasonable to limit a translator to the narrow compass of his author's words: 'tis enough if he choose out some expression which does not vitiate the sense.[7]Dryden cautioned, however, against the license of "imitation", i.e., of adapted translation: "When a painter copies from the life... he has no privilege to alter features and lineaments..."[8]This general formulation of the central concept of translation—equivalence—is as adequate as any that has been proposed since Cicero and Horace, who, in 1st-century-BCE Rome, famously and literally cautioned against translating "word for word" (verbum pro verbo).[8]Despite occasional theoretical diversity, the actual practice of translation has hardly changed since antiquity. Except for some extreme metaphrasers in the early Christian period and the Middle Ages, and adapters in various periods (especially pre-Classical Rome, and the 18th century), translators have generally shown prudent flexibility in seeking equivalents—"literal" where possible, paraphrastic where necessary—for the original meaning and other crucial "values" (e.g., style, verse form, concordance with musical accompaniment or, in films, with speech articulatory movements) as determined from context.[8]In general, translators have sought to preserve the context itself by reproducing the original order of sememes, and hence word order—when necessary, reinterpreting the actual grammatical structure, for example, by shifting from active to passive voice, or vice versa. The grammatical differences between "fixed-word-order" languages[10] (e.g. English, French, German) and "free-word-order" languages[11] (e.g., Greek, Latin, Polish, Russian) have been no impediment in this regard.[8]  The particular syntax (sentence-structure) characteristics of a text's source language are adjusted to the syntactic requirements of the target language. When a target language has lacked terms that are found in a source language, translators have borrowed those terms, thereby enriching the target language. Thanks in great measure to the exchange of calques and loanwords between languages, and to their importation from other languages, there are few concepts that are "untranslatable" among the modern European languages.[8]  A greater problem, however, is translating terms relating to cultural concepts that have no equivalent in the target language.[12]  For full comprehension, such situations require the provision of a gloss.Generally, the greater the contact and exchange that have existed between two languages, or between those languages and a third one, the greater is the ratio of metaphrase to paraphrase that may be used in translating among them. However, due to shifts in ecological niches of words, a common etymology is sometimes misleading as a guide to current meaning in one or the other language. For example, the English actual should not be confused with the cognate French actuel ("present", "current"), the Polish aktualny ("present", "current," "topical", "timely", "feasible"),[13] the Swedish aktuell ("topical", "presently of importance"), the Russian актуальный ("urgent", "topical") or the Dutch actueel ("current").The translator's role as a bridge for "carrying across" values between cultures has been discussed at least since Terence, the 2nd-century-BCE Roman adapter of Greek comedies. The translator's role is, however, by no means a passive, mechanical one, and so has also been compared to that of an artist.  The main ground seems to be the concept of parallel creation found in critics such as Cicero. Dryden observed that "Translation is a type of drawing after life..." Comparison of the translator with a musician or actor goes back at least to Samuel Johnson's remark about Alexander Pope playing Homer on a flageolet, while Homer himself used a bassoon.[13]If translation be an art, it is no easy one.  In the 13th century, Roger Bacon wrote that if a translation is to be true, the translator must know both languages, as well as the science that he is to translate; and finding that few translators did, he wanted to do away with translation and translators altogether.[14]The translator of the Bible into German, Martin Luther (1483–1546), is credited with being the first European to posit that one translates satisfactorily only toward his own language. L.G. Kelly states that since Johann Gottfried Herder in the 18th century, "it has been axiomatic" that one translates only toward his own language.[15]Compounding the demands on the translator is the fact that no dictionary or thesaurus can ever be a fully adequate guide in translating. The Scottish historian Alexander Tytler, in his Essay on the Principles of Translation (1790), emphasized that assiduous reading is a more comprehensive guide to a language than are dictionaries. The same point, but also including listening to the spoken language, had earlier, in 1783, been made by the Polish poet and grammarian Onufry Kopczyński.[16]The translator's special role in society is described in a posthumous 1803 essay by "Poland's La Fontaine", the Roman Catholic Primate of Poland, poet, encyclopedist, author of the first Polish novel, and translator from French and Greek, Ignacy Krasicki:[T]ranslation... is in fact an art both estimable and very difficult, and therefore is not the labor and portion of common minds; [it] should be [practiced] by those who are themselves capable of being actors, when they see greater use in translating the works of others than in their own works, and hold higher than their own glory the service that they render their country.[17]Due to Western colonialism and cultural dominance in recent centuries, Western translation traditions have largely replaced other traditions. The Western traditions draw on both ancient and medieval traditions, and on more recent European innovations.Though earlier approaches to translation are less commonly used today, they retain importance when dealing with their products, as when historians view ancient or medieval records to piece together events which took place in non-Western or pre-Western environments. Also, though heavily influenced by Western traditions and practiced by translators taught in Western-style educational systems, Chinese and related translation traditions retain some theories and philosophies unique to the Chinese tradition.Traditions of translating material among the languages of ancient Egypt, Mesopotamia, Assyria (Syriac language), Anatolia, and Israel (Hebrew language) go back several millennia.  There exist partial translations of the Sumerian Epic of Gilgamesh (c. 2000 BCE) into Southwest Asian languages of the second millennium BCE.[18]An early example of a bilingual document is the 1274 BCE Treaty of Kadesh between the ancient Egyptian and Hittie empires.There is a separate tradition of translation in South, Southeast and East Asia (primarily of texts from the Indian and Chinese civilizations), connected especially with the rendering of religious, particularly Buddhist, texts and with the governance of the Chinese empire. Classical Indian translation is characterized by loose adaptation, rather than the closer translation more commonly found in Europe; and Chinese translation theory identifies various criteria and limitations in translation.In the East Asian sphere of Chinese cultural influence, more important than translation per se has been the use and reading of Chinese texts, which also had substantial influence on the Japanese, Korean and Vietnamese languages, with substantial borrowings of Chinese vocabulary and writing system. Notable is the Japanese kanbun, a system for glossing Chinese texts for Japanese speakers.Though Indianized states in Southeast Asia often translated Sanskrit material into the local languages, the literate elites and scribes more commonly used Sanskrit as their primary language of culture and government.Some special aspects of translating from Chinese are illustrated in Perry Link's discussion of translating the work of the Tang Dynasty poet Wang Wei (699–759 CE).[19]Some of the art of classical Chinese poetry [writes Link] must simply be set aside as untranslatable.  The internal structure of Chinese characters has a beauty of its own, and the calligraphy in which classical poems were written is another important but untranslatable dimension.  Since Chinese characters do not vary in length, and because there are exactly five characters per line in a poem like [the one that Eliot Weinberger discusses in 19 Ways of Looking at Wang Wei (with More Ways)], another untranslatable feature is that the written result, hung on a wall, presents a rectangle.  Translators into languages whose word lengths vary can reproduce such an effect only at the risk of fatal awkwardness....Another imponderable is how to imitate the 1-2, 1-2-3 rhythm in which five-syllable lines in classical Chinese poems normally are read.  Chinese characters are pronounced in one syllable apiece, so producing such rhythms in Chinese is not hard and the results are unobtrusive; but any imitation in a Western language is almost inevitably stilted and distracting.  Even less translatable are the patterns of tone arrangement in classical Chinese poetry.  Each syllable (character) belongs to one of two categories determined by the pitch contour in which it is read; in a classical Chinese poem the patterns of alternation of the two categories exhibit parallelism and mirroring.[20]Once the untranslatables have been set aside, the problems for a translator, especially of Chinese poetry, are two:  What does the translator think the poetic line says?  And once he thinks he understands it, how can he render it into the target language?  Most of the difficulties, according to Link, arise in addressing the second problem, "where the impossibility of perfect answers spawns endless debate."  Almost always at the center is the letter-versus-spirit dilemma.  At the literalist extreme, efforts are made to dissect every conceivable detail about the language of the original Chinese poem.  "The dissection, though," writes Link, "normally does to the art of a poem approximately what the scalpel of an anatomy instructor does to the life of a frog."[20]Chinese characters, in avoiding grammatical specificity, offer advantages to poets (and, simultaneously, challenges to poetry translators) that are associated primarily with absences of subject, number, and tense.[21]It is the norm in classical Chinese poetry, and common even in modern Chinese prose, to omit subjects; the reader or listener infers a subject.  Some Western languages, however, ask by grammatical rule that subjects always be stated.  Most of the translators cited in Eliot Weinberger's 19 Ways of Looking at Wang Wei supply a subject.  Weinberger points out, however, that when an "I" as a subject is inserted, a "controlling individual mind of the poet" enters and destroys the effect of the Chinese line.  Without a subject, he writes, "the experience becomes both universal and immediate to the reader."  Another approach to the subjectlessness is to use the target language's passive voice; but this again particularizes the experience too much.[21]Nouns have no number in Chinese.  "If," writes Link, "you want to talk in Chinese about one rose, you may, but then you use a "measure word" to say "one blossom-of roseness."[21]Chinese verbs are tense-less:  there are several ways to specify when something happened or will happen, but verb tense is not one of them.  For poets, this creates the great advantage of ambiguity.  According to Link, Weinberger's insight about subjectlessness—that it produces an effect "both universal and immediate"—applies to timelessness as well.[21]Link proposes a kind of uncertainty principle that may be applicable not only to translation from the Chinese language, but to all translation:Dilemmas about translation do not have definitive right answers (although there can be unambiguously wrong ones if misreadings of the original are involved).  Any translation (except machine translation, a different case) must pass through the mind of a translator, and that mind inevitably contains its own store of perceptions, memories, and values.Weinberger [...] pushes this insight further when he writes that "every reading of every poem, regardless of language, is an act of translation:  translation into the reader's intellectual and emotional life."  Then he goes still further:  because a reader's mental life shifts over time, there is a sense in which "the same poem cannot be read twice."[21]Translation of material into Arabic expanded after the creation of Arabic script in the 5th century, and gained great importance with the rise of Islam and Islamic empires. Arab translation initially focused primarily on politics, rendering Persian, Greek, even Chinese and Indic diplomatic materials into Arabic. It later focused on translating classical Greek and Persian works, as well as some Chinese and Indian texts, into Arabic for scholarly study at major Islamic learning centers, such as the Al-Karaouine (Fes, Morocco), Al-Azhar (Cairo, Egypt), and the Al-Nizamiyya of Baghdad. In terms of theory, Arabic translation drew heavily on earlier Near Eastern traditions as well as more contemporary Greek and Persian traditions.Arabic translation efforts and techniques are important to Western translation traditions due to centuries of close contacts and exchanges.  Especially after the Renaissance, Europeans began more intensive study of Arabic and Persian translations of classical works as well as scientific and philosophical works of Arab and oriental origins.  Arabic and, to a lesser degree, Persian became important sources of material and perhaps of techniques for revitalized Western traditions, which in time would overtake the Islamic and oriental traditions.In the 19th century, after the Middle East's Islamic clerics and copyistshad conceded defeat in their centuries-old battle to contain the corrupting effects of the printing press, [an] explosion in publishing... ensued.  Along with expanding secular education, printing transformed an overwhelmingly illiterate society into a partly literate one.In the past, the sheikhs and the government had exercised a monopoly over knowledge.  Now an expanding elite benefitted from a stream of information on virtually anything that interested them.  Between 1880 and 1908... more than six hundred newspapers and periodicals were founded in Egypt alone.The most prominent among them was al-Muqtataf...  [It] was the popular expression of a translation movement that had begun earlier in the century with military and medical manuals and highlights from the Enlightenment canon.  (Montesquieu's Considerations on the Romans and Fénelon's Telemachus had been favorites.)[22]A translator who contributed mightily to the advance of the Islamic Enlightenment was the Egyptian cleric Rifaa al-Tahtawi (1801–73), who had spent five years in Paris in the late 1820s, teaching religion to Muslim students.  After returning to Cairo with the encouragement of Muhammad Ali (1769–1849), the Ottoman viceroy of Egypt, al–Tahtawi became head of the new school of languages and embarked on an intellectual revolution by initiating a program to translate some two thousand European and Turkish volumes, ranging from ancient texts on geography and geometry to Voltaire's biography of Peter the Great, along with the Marseillaise and the entire Code Napoléon.  This was the biggest, most meaningful importation of foreign thought into Arabic since Abbasid times (750–1258).[23]In France al-Tahtawi had been struck by the way the French language... was constantly renewing itself to fit modern ways of living.  Yet Arabic has its own sources of reinvention.  The root system that Arabic shares with other Semitic tongues such as Hebrew is capable of expanding the meanings of words using structured consonantal variations:  the word for airplane, for example, has the same root as the word for bird.[24]The movement to translate English and European texts transformed the Arabic and Ottoman Turkish languages, and new words, simplified syntax, and directness came to be valued over the previous convolutions.  Educated Arabs and Turks in the new professions and the modernized civil service expressed skepticism, writes Christopher de Bellaigue, "with a freedom that is rarely witnessed today....  No longer was legitimate knowledge defined by texts in the religious schools, interpreted for the most part with stultifying literalness.  It had come to include virtually any intellectual production anywhere in the world."  One of the neologisms that, in a way, came to characterize the infusion of new ideas via translation was "darwiniya", or "Darwinism".[22]One of the most influential liberal Islamic thinkers of the time was Muhammad Abduh (1849–1905), Egypt's senior judicial authority—its chief mufti—at the turn of the 20th century and an admirer of Darwin who in 1903 visited Darwin's exponent Herbert Spencer at his home in Brighton.  Spencer's view of society as an organism with its own laws of evolution paralleled Abduh's ideas.[25]After World War I, when Britain and France divided up the Middle East's countries, apart from Turkey, between them, pursuant to the Sykes-Picot agreement—in violation of solemn wartime promises of postwar Arab autonomy—there came an immediate reaction:  the Muslim Brotherhood emerged in Egypt, the House of Saud took over the Hijaz, and regimes led by army officers came to power in Iran and Turkey.  "[B]oth illiberal currents of the modern Middle East," writes de Bellaigue, "Islamism and militarism, received a major impetus from Western empire-builders."  As often happens in countries undergoing social crisis, the aspirations of the Muslim world's translators and modernizers, such as Muhammad Abduh, largely had to yield to retrograde currents.[26]Fidelity (or "faithfulness") and felicity[27] (or transparency), dual ideals in translation, are often (though not always) at odds. A 17th-century French critic coined the phrase "les belles infidèles" to suggest that translations, like women, can be either faithful or beautiful, but not both.[b]  Fidelity is the extent to which a translation accurately renders the meaning of the source text, without distortion.Transparency is the extent to which a translation appears to a native speaker of the target language to have originally been written in that language, and conforms to its grammar, syntax and idiom. John Dryden (1631–1700) wrote in his preface to the translation anthology Sylvae:Where I have taken away some of [the original authors'] Expressions, and cut them shorter, it may possibly be on this consideration, that what was beautiful in the Greek or Latin, would not appear so shining in the English; and where I have enlarg'd them, I desire the false Criticks would not always think that those thoughts are wholly mine, but that either they are secretly in the Poet, or may be fairly deduc'd from him; or at least, if both those considerations should fail, that my own is of a piece with his, and that if he were living, and an Englishman, they are such as he wou'd probably have written.[29]A translation that meets the criterion of fidelity (faithfulness) is said to be "faithful"; a translation that meets the criterion of transparency, "idiomatic".  Depending on the given translation, the two qualities may not be mutually exclusive.  The criteria for judging the fidelity of a translation vary according to the subject, type and use of the text, its literary qualities, its social or historical context, etc.  The criteria for judging the transparency of a translation appear more straightforward: an unidiomatic translation "sounds wrong"; and, in the extreme case of word-for-word translations generated by many machine-translation systems, often results in patent nonsense.Nevertheless, in certain contexts a translator may consciously seek to produce a literal translation. Translators of literary, religious, or historic texts often adhere as closely as possible to the source text, stretching the limits of the target language to produce an unidiomatic text.  Also, a translator may adopt expressions from the source language in order to provide "local color".While current Western translation practice is dominated by the dual concepts of "fidelity" and "transparency", this has not always been the case.  There have been periods, especially in pre-Classical Rome and in the 18th century, when many translators stepped beyond the bounds of translation proper into the realm of adaptation.  Adapted translation retains currency in some non-Western traditions. The Indian epic, the Ramayana, appears in many versions in the various Indian languages, and the stories are different in each.  Similar examples are to be found in medieval Christian literature, which adjusted the text to local customs and mores.Many non-transparent-translation theories draw on concepts from German Romanticism, the most obvious influence being the German theologian and philosopher Friedrich Schleiermacher. In his seminal lecture "On the Different Methods of Translation" (1813) he distinguished between translation methods that move "the writer toward [the reader]", i.e., transparency, and those that move the "reader toward [the author]", i.e., an extreme fidelity to the foreignness of the source text. Schleiermacher favored the latter approach; he was motivated, however, not so much by a desire to embrace the foreign, as by a nationalist desire to oppose France's cultural domination and to promote German literature.In recent decades, prominent advocates of such "non-transparent" translation have included the French scholar Antoine Berman, who identified twelve deforming tendencies inherent in most prose translations,[30] and the American theorist Lawrence Venuti, who has called on translators to apply "foreignizing" rather than domesticating translation strategies.[31]The question of fidelity vs. transparency has also been formulated in terms of, respectively, "formal equivalence" and "dynamic [or functional] equivalence" – expressions associated with the translator Eugene Nida and originally coined to describe ways of translating the Bible; but the two approaches are applicable to any translation.  "Formal equivalence" corresponds to "metaphrase", and "dynamic equivalence" to "paraphrase".  "Formal equivalence" (sought via "literal" translation) attempts to render the text literally, or "word for word" (the latter expression being itself a word-for-word rendering of the classical Latin verbum pro verbo) – if necessary, at the expense of features natural to the target language.  By contrast, "dynamic equivalence" (or "functional equivalence") conveys the essential thoughts expressed in a source text—if necessary, at the expense of literality, original sememe and word order, the source text's active vs. passive voice, etc. There is, however, no sharp boundary between formal and functional equivalence. On the contrary, they represent a spectrum of translation approaches. Each is used at various times and in various contexts by the same translator, and at various points within the same text – sometimes simultaneously. Competent translation entails the judicious blending of formal and functional equivalents.[32]Common pitfalls in translation, especially when practiced by inexperienced translators, involve false equivalents such as "false friends"[33] and false cognates.A "back-translation" is a translation of a translated text back into the language of the original text, made without reference to the original text. Comparison of a back-translation with the original text is sometimes used as a check on the accuracy of the original translation, much as the accuracy of a mathematical operation is sometimes checked by reversing the operation.  But the results of such reverse-translation operations, while useful as approximate checks, are not always precisely reliable.[34]  Back-translation must in general be less accurate than back-calculation because linguistic symbols (words) are often ambiguous, whereas mathematical symbols are intentionally unequivocal.  In the context of machine translation, a back-translation is also called a "round-trip translation."  When translations are produced of material used in medical clinical trials, such as informed-consent forms, a back-translation is often required by the ethics committee or institutional review board.[35]Mark Twain provided humorously telling evidence for the frequent unreliability of back-translation when he issued his own back-translation of a French translation of his short story, "The Celebrated Jumping Frog of Calaveras County".  He published his back-translation in a 1903 volume together with his English-language original, the French translation, and a "Private History of the 'Jumping Frog' Story". The latter included a synopsized adaptation of his story that Twain stated had appeared, unattributed to Twain, in a Professor Sidgwick's Greek Prose Composition (p. 116) under the title, "The Athenian and the Frog"; the adaptation had for a time been taken for an independent ancient Greek precursor to Twain's "Jumping Frog" story.[36]When a document survives only in translation, the original having been lost, researchers sometimes undertake back-translation in an effort to reconstruct the original text. An example involves the novel The Saragossa Manuscript by the Polish aristocrat Jan Potocki (1761–1815), who wrote the novel in French and anonymously published fragments in 1804 and 1813–14. Portions of the original French-language manuscript were subsequently lost; however, the missing fragments survived in a Polish translation that was made by Edmund Chojecki in 1847 from a complete French copy, now lost. French-language versions of the complete Saragossa Manuscript have since been produced, based on extant French-language fragments and on French-language versions that have been back-translated from Chojecki's Polish version.[37]Many works by the influential Classical physician Galen survive only in medieval Arabic translation. Some survive only in Renaissance Latin translations from the Arabic, thus at a second remove from the original. To better understand Galen, scholars have attempted back-translation of such works in order to reconstruct the original Greek.[citation needed]When historians suspect that a document is actually a translation from another language, back-translation into that hypothetical original language can provide supporting evidence by showing that such characteristics as idioms, puns, peculiar grammatical structures, etc., are in fact derived from the original language.  For example, the known text of the Till Eulenspiegel folk tales is in High German but contains puns that work only when back-translated to Low German.  This seems clear evidence that these tales (or at least large portions of them) were originally written in Low German and translated into High German by an over-metaphrastic translator.Supporters of Aramaic primacy—of the view that the Christian New Testament or its sources were originally written in the Aramaic language—seek to prove their case by showing that difficult passages in the existing Greek text of the New Testament make much better sense when back-translated to Aramaic: that, for example, some incomprehensible references are in fact Aramaic puns that do not work in Greek.  Due to similar indications, it is believed that the 2nd century Gnostic Gospel of Judas, which survives only in Coptic, was originally written in Greek.John Dryden (1631–1700), the dominant English-language literary figure of his age, illustrates, in his use of back-translation, translators' influence on the evolution of languages and literary styles.  Dryden is believed to be the first person to posit that English sentences should not end in prepositions because Latin sentences cannot end in prepositions.[38][39]  Dryden created the proscription against "preposition stranding" in 1672 when he objected to Ben Jonson's 1611 phrase, "the bodies that those souls were frighted from", though he did not provide the rationale for his preference.[40]  Dryden often translated his writing into Latin, to check whether his writing was concise and elegant, Latin being considered an elegant and long-lived language with which to compare; then he back-translated his writing back to English according to Latin-grammar usage. As Latin does not have sentences ending in prepositions, Dryden may have applied Latin grammar to English, thus forming the controversial rule of no sentence-ending prepositions, subsequently adopted by other writers.[41][c]Competent translators show the following attributes:A competent translator is not only bilingual but bicultural.  A language is not merely a collection of words and of rules of grammar and syntax for generating sentences, but also a vast interconnecting system of connotations and cultural references whose mastery, writes linguist Mario Pei, "comes close to being a lifetime job."[43]  The complexity of the translator's task cannot be overstated; one author suggests that becoming an accomplished translator—after having already acquired a good basic knowledge of both languages and cultures—may require a minimum of ten years' experience.  Viewed in this light, it is a serious misconception to assume that a person who has fair fluency in two languages will, by virtue of that fact alone, be consistently competent to translate between them.[16]The translator's role in relation to a text has been compared to that of an artist, e.g., a musician or actor, who interprets a work of art.  Translation, like other human activities,[44] entails making choices, and choice implies interpretation.[13][d]  Mark Polizzotti writes:  "A good translation offers not a reproduction of the work but an interpretation, a re-representation, just as the performance of a play or a sonata is a representation of the script or the score, one among many possible representations."[45] The English-language novelist Joseph Conrad, whose writings Zdzisław Najder has described as verging on "auto-translation" from Conrad's Polish and French linguistic personae,[46] advised his niece and Polish translator Aniela Zagórska:  "[D]on't trouble to be too scrupulous...  I may tell you (in French) that in my opinion il vaut mieux interpréter que traduire [it is better to interpret than to translate]....  Il s'agit donc de trouver les équivalents.  Et là, ma chère, je vous prie laissez vous guider plutôt par votre tempérament que par une conscience sévère.... [It is, then, a question of finding the equivalent expressions.  And there, my dear, I beg you to let yourself be guided more by your temperament than by a strict conscience....]"[47]  Conrad advised another translator that the prime requisite for a good translation is that it be "idiomatic".  "For in the idiom is the clearness of a language and the language's force and its picturesqueness—by which last I mean the picture-producing power of arranged words."[48]  Conrad thought C.K. Scott Moncrieff's English translation of Marcel Proust's À la recherche du temps perdu (In Search of Lost Time—or, in Scott Moncrieff's rendering, Remembrance of Things Past) to be preferable to the French original.[49][e]The necessity of making choices, and therefore of interpretation, in translating[f] (and in other fields of human endeavor) stems from the ambiguity that subjectively pervades the universe.  Part of the ambiguity, for a translator, involves the structure of human language. Psychologist and neural scientist Gary Marcus notes that "virtually every sentence [that people generate] is ambiguous, often in multiple ways.  Our brain is so good at comprehending language that we do not usually notice."[51]  An example of linguistic ambiguity is the "pronoun disambiguation problem" ("PDP"):  a machine has no way of determining to whom or what a pronoun in a sentence—such as "he", "she" or "it"—refers.[52]  Such disambiguation is not infallible by a human, either.Ambiguity is a concern to both translators and, as the writings of poet and literary critic William Empson have demonstrated, to literary critics.  Ambiguity may be desirable, indeed essential, in poetry and diplomacy; it can be more problematic in ordinary prose.[53]A translator is faced with two contradictory tasks:  when translating, he must strive for omniscience; when reviewing his translation, he must assume (the naive reader's) ignorance. A translator may render only parts of the original text, provided he indicates that this is what he is doing.  But a translator should not assume the role of censor and surreptitiously delete or bowdlerize passages merely to please a political or moral interest.[54]Translating has served as a school of writing for many an author, much as the copying of masterworks of painting has schooled many a novice painter.[55]  A translator who can competently render an author's thoughts into the translator's own language, should certainly be able to adequately render, in his own language, any thoughts of his own.  Translating (like analytic philosophy) compels precise analysis of language elements and of their usage.  In 1946 the poet Ezra Pound, then at St. Elizabeth's Hospital, in Washington, D.C., advised a visitor, the 18-year-old beginning poet W.S. Merwin:  "The work of translation is the best teacher you'll ever have."[56][g]  Merwin, translator-poet who took Pound's advice to heart, writes of translation as an "impossible, unfinishable" art.[58]Translators, including monks who spread Buddhist texts in East Asia, and the early modern European translators of the Bible, in the course of their work have shaped the very languages into which they have translated. They have acted as bridges for conveying knowledge between cultures; and along with ideas, they have imported from the source languages, into their own languages, loanwords and calques of grammatical structures, idioms, and vocabulary.Interpreting, or "interpretation," is the facilitation of oral or sign-language communication, either simultaneously or consecutively, between two, or among three or more, speakers who are not speaking, or signing, the same language.  The term "interpreting," rather than "interpretation," is preferentially used for this activity by Anglophone translators, to avoid confusion with other meanings of the word "interpretation."  Unlike English, many languages do not employ two separate words to denote the activities of written and live-communication (oral or sign-language) translators.[h] Even English does not always make the distinction, frequently using "translating" as a synonym for "interpreting."Interpreters have sometimes played crucial roles in history.  A prime example is La Malinche, also known as Malintzin, Malinalli and Doña Marina, an early-16th-century Nahua woman from the Mexican Gulf Coast.  As a child she had been sold or given to Maya slave-traders from Xicalango, and thus had become bilingual. Subsequently, given along with other women to the invading Spaniards, she became instrumental in the Spanish conquest of Mexico, acting as interpreter, adviser, intermediary and lover to Hernán Cortés.[60]Nearly three centuries later, in the United States, a comparable role as interpreter was played for the Lewis and Clark Expedition of 1804–6 by Sacagawea. As a child, the Lemhi Shoshone woman had been kidnapped by Hidatsa Indians and thus had become bilingual.  Sacagawea facilitated the expedition's traverse of the North American continent to the Pacific Ocean.[61]Sworn translation, also called "certified translation," aims at legal equivalence between two documents written in different languages. It is performed by someone authorized to do so by local regulations. Some countries recognize declared competence. Others require the translator to be an official state appointee. In some countries, such as the United Kingdom, translators must be accredited by certain translation institutes or associations in order to be able to carry out certified translations.Many commercial services exist that will interpret spoken language via telephone.  There is also at least one custom-built mobile device that does the same thing. The device connects users to human interpreters who can translate between English and 180 other languages.[62]Web-based human translation is generally favored by companies and individuals that wish to secure more accurate translations. In view of the frequent inaccuracy of machine translations, human translation remains the most reliable, most accurate form of translation available.[63] With the recent emergence of translation crowdsourcing,[64][65] translation-memory techniques, and internet applications,[66] translation agencies have been able to provide on-demand human-translation services to businesses, individuals, and enterprises.While not instantaneous like its machine counterparts such as Google Translate and Yahoo! Babel Fish, web-based human translation has been gaining popularity by providing relatively fast, accurate translation of business communications, legal documents, medical records, and software localization. [67] Web-based human translation also appeals to private website users and bloggers.[68] Contents of websites are translatable but urls of websites are not translatable into other languages. Language tools on the internet provide help in understanding text. (see reference link).[69]Computer-assisted translation (CAT), also called "computer-aided translation," "machine-aided human translation" (MAHT) and "interactive translation," is a form of translation wherein a human translator creates a target text with the assistance of a computer program. The machine supports a human translator.Computer-assisted translation can include standard dictionary and grammar software. The term, however, normally refers to a range of specialized programs available to the translator, including translation-memory, terminology-management, concordance, and alignment programs.These tools speed up and facilitate human translation, but they do not provide translation.  The latter is a function of tools known broadly as machine translation.Machine translation (MT) is a process whereby a computer program analyzes a source text and, in principle, produces a target text without human intervention. In reality, however, machine translation typically does involve human intervention, in the form of pre-editing and post-editing.[70]  With proper terminology work, with preparation of the source text for machine translation (pre-editing), and with reworking of the machine translation by a human translator (post-editing), commercial machine-translation tools can produce useful results, especially if the machine-translation system is integrated with a translation-memory or globalization-management system.[71]Unedited machine translation is publicly available through tools on the Internet such as Google Translate, Babel Fish, Babylon, and StarDict. These produce rough translations that, under favorable circumstances, "give the gist" of the source text.[72]  With the Internet, translation software can help non-native-speaking individuals understand web pages published in other languages. Whole-page-translation tools are of limited utility, however, since they offer only a limited potential understanding of the original author's intent and context; translated pages tend to be more erroneously humorous and confusing than enlightening.Interactive translations with pop-up windows are becoming more popular. These tools show one or more possible equivalents for each word or phrase. Human operators merely need to select the likeliest equivalent as the mouse glides over the foreign-language text. Possible equivalents can be grouped by pronunciation.  Also, companies such as Ectaco produce pocket devices that provide machine translations.Relying exclusively on unedited machine translation, however, ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error; therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human.[i]  Claude Piron writes that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved.[74] Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software, such that the output will not be meaningless.[70]The weaknesses of pure machine translation, unaided by human expertise, are those of artificial intelligence itself.[75]  Translator Mark Polizzotti holds that machine translation, by Google Translate and the like, is unlikely to threaten human translators anytime soon, because machines will never grasp nuance and connotation.[76]Translation of literary works (novels, short stories, plays, poems, etc.) is considered a literary pursuit in its own right. Notable in Canadian literature specifically as translators are figures such as Sheila Fischman, Robert Dickson, and Linda Gaboriau; and the Canadian Governor General's Awards annually present prizes for the best English-to-French and French-to-English literary translations.Other writers, among many who have made a name for themselves as literary translators, include Vasily Zhukovsky, Tadeusz Boy-Żeleński, Vladimir Nabokov, Jorge Luis Borges, Robert Stiller, Lydia Davis, Haruki Murakami, Achy Obejas, and Jhumpa Lahiri.In the 2010s a substantial gender imbalance was noted in literary translation into English,[77] with far more male writers being translated than women writers. In 2014 Meytal Radzinski launched the Women in Translation campaign to address this.[78][79][80]The first important translation in the West was that of the Septuagint, a collection of Jewish Scriptures translated into early Koine Greek in Alexandria between the 3rd and 1st centuries BCE. The dispersed Jews had forgotten their ancestral language and needed Greek versions (translations) of their Scriptures.[81]Throughout the Middle Ages, Latin was the lingua franca of the western learned world. The 9th-century Alfred the Great, king of Wessex in England, was far ahead of his time in commissioning vernacular Anglo-Saxon translations of Bede's Ecclesiastical History and Boethius' Consolation of Philosophy. Meanwhile, the Christian Church frowned on even partial adaptations of St. Jerome's Vulgate of c. 384 CE,[82] the standard Latin Bible.In Asia, the spread of Buddhism led to large-scale ongoing translation efforts spanning well over a thousand years. The Tangut Empire was especially efficient in such efforts; exploiting the then newly invented block printing, and with the full support of the government (contemporary sources describe the Emperor and his mother personally contributing to the translation effort, alongside sages of various nationalities), the Tanguts took mere decades to translate volumes that had taken the Chinese centuries to render.[citation needed]The Arabs undertook large-scale efforts at translation.  Having conquered the Greek world, they made Arabic versions of its philosophical and scientific works.  During the Middle Ages, translations of some of these Arabic versions were made into Latin, chiefly at Córdoba in Spain.[83]  King Alfonso X el Sabio (Alphonse the Wise) of Castille in the 13th century promoted this effort by founding a Schola Traductorum (School of Translation) in Toledo.  There Arabic texts, Hebrew texts, and Latin texts were translated into the other tongues by Muslim, Jewish and Christian scholars, who also argued the merits of their respective religions.  Latin translations of Greek and original Arab works of scholarship and science helped advance European Scholasticism, and thus European science and culture.The broad historic trends in Western translation practice may be illustrated on the example of translation into the English language.The first fine translations into English were made in the 14th century by Geoffrey Chaucer, who adapted from the Italian of Giovanni Boccaccio in his own Knight's Tale and Troilus and Criseyde; began a translation of the French-language Roman de la Rose; and completed a translation of Boethius from the Latin. Chaucer founded an English poetic tradition on adaptations and translations from those earlier-established literary languages.[83]The first great English translation was the Wycliffe Bible (c. 1382), which showed the weaknesses of an underdeveloped English prose. Only at the end of the 15th century did the great age of English prose translation begin with Thomas Malory's Le Morte Darthur—an adaptation of Arthurian romances so free that it can, in fact, hardly be called a true translation. The first great Tudor translations are, accordingly, the Tyndale New Testament (1525), which influenced the Authorized Version (1611), and Lord Berners' version of Jean Froissart's Chronicles (1523–25).[83]Meanwhile, in Renaissance Italy, a new period in the history of translation had opened in Florence with the arrival, at the court of Cosimo de' Medici, of the Byzantine scholar Georgius Gemistus Pletho shortly before the fall of Constantinople to the Turks (1453). A Latin translation of Plato's works was undertaken by Marsilio Ficino. This and Erasmus' Latin edition of the New Testament led to a new attitude to translation. For the first time, readers demanded rigor of rendering, as philosophical and religious beliefs depended on the exact words of Plato, Aristotle and Jesus.[83]Non-scholarly literature, however, continued to rely on adaptation. France's Pléiade, England's Tudor poets, and the Elizabethan translators adapted themes by Horace, Ovid, Petrarch and modern Latin writers, forming a new poetic style on those models. The English poets and translators sought to supply a new public, created by the rise of a middle class and the development of printing, with works such as the original authors would have written, had they been writing in England in that day.[83]The Elizabethan period of translation saw considerable progress beyond mere paraphrase toward an ideal of stylistic equivalence, but even to the end of this period, which actually reached to the middle of the 17th century, there was no concern for verbal accuracy.[84]In the second half of the 17th century, the poet John Dryden sought to make Virgil speak "in words such as he would probably have written if he were living and an Englishman". As great as Dryden's poem is, however, one is reading Dryden, and not experiencing the Roman poet's concision. Similarly, Homer arguably suffers from Alexander Pope's endeavor to reduce the Greek poet's "wild paradise" to order. Both works live on as worthy English epics, more than as a point of access to the Latin or Greek.[84]Throughout the 18th century, the watchword of translators was ease of reading. Whatever they did not understand in a text, or thought might bore readers, they omitted. They cheerfully assumed that their own style of expression was the best, and that texts should be made to conform to it in translation. For scholarship they cared no more than had their predecessors, and they did not shrink from making translations from translations in third languages, or from languages that they hardly knew, or—as in the case of James Macpherson's "translations" of Ossian—from texts that were actually of the "translator's" own composition.[84]The 19th century brought new standards of accuracy and style. In regard to accuracy, observes J.M. Cohen, the policy became "the text, the whole text, and nothing but the text", except for any bawdy passages and the addition of copious explanatory footnotes.[j] In regard to style, the Victorians' aim, achieved through far-reaching metaphrase (literality) or pseudo-metaphrase, was to constantly remind readers that they were reading a foreign classic. An exception was the outstanding translation in this period, Edward FitzGerald's Rubaiyat of Omar Khayyam (1859), which achieved its Oriental flavor largely by using Persian names and discreet Biblical echoes and actually drew little of its material from the Persian original.[84]In advance of the 20th century, a new pattern was set in 1871 by Benjamin Jowett, who translated Plato into simple, straightforward language. Jowett's example was not followed, however, until well into the new century, when accuracy rather than style became the principal criterion.[84]As a language evolves, texts in an earlier version of the language—original texts, or old translations—may become difficult for modern readers to understand. Such a text may therefore be translated into more modern language, producing a "modern translation" (e.g., a "modern English translation" or "modernized translation").Such modern rendering is applied either to literature from classical languages such as Latin or Greek, notably to the Bible (see "Modern English Bible translations"), or to literature from an earlier stage of the same language, as with the works of William Shakespeare (which are largely understandable by a modern audience, though with some difficulty) or with Geoffrey Chaucer's Middle English Canterbury Tales (which is understandable to most modern readers only through heavy dependence on footnotes).Modern translation is applicable to any language with a long literary history.  For example, in Japanese the 11th-century Tale of Genji is generally read in modern translation  (see "Genji: modern readership").Modern translation often involves literary scholarship and textual revision, as there is frequently not one single canonical text. This is particularly noteworthy in the case of the Bible and Shakespeare, where modern scholarship can result in substantive textual changes.Modern translation meets with opposition from some traditionalists.  In English, some readers prefer the Authorized King James Version of the Bible to modern translations, and Shakespeare in the original of c. 1600 to modern translations.An opposite process involves translating modern literature into classical languages, for the purpose of extensive reading (for examples, see "List of Latin translations of modern literature").Views on the possibility of satisfactorily translating poetry show a broad spectrum, depending largely on the degree of latitude to be granted the translator in regard to a poem's formal features (rhythm, rhyme, verse form, etc.).  Douglas Hofstadter, in his 1997 book, Le Ton beau de Marot, argued that a good translation of a poem must convey as much as possible not only of its literal meaning but also of its form and structure (meter, rhyme or alliteration scheme, etc.).[85]The Russian-born linguist and semiotician Roman Jakobson, however, had in his 1959 paper "On Linguistic Aspects of Translation", declared that "poetry by definition [is] untranslatable".  Vladimir Nabokov, another Russian-born author, took a view similar to Jakobson's.  He considered rhymed, metrical, versed poetry to be in principle untranslatable and therefore rendered his 1964 English translation of Alexander Pushkin's Eugene Onegin in prose.Hofstadter, in Le Ton beau de Marot, criticized Nabokov's attitude toward verse translation.  In 1999 Hofstadter published his own translation of Eugene Onegin, in verse form.Gregory Hays, in the course of discussing Roman adapted translations of ancient Greek literature, makes approving reference to some views on the translating of poetry expressed by David Bellos, an accomplished French-to-English translator.  Hays writes:Among the idées reçues [received ideas] skewered by David Bellos is the old saw that "poetry is what gets lost in translation."  The saying is often attributed to Robert Frost, but as Bellos notes, the attribution is as dubious as the idea itself.  A translation is an assemblage of words, and as such it can contain as much or as little poetry as any other such assemblage.  The Japanese even have a word (chōyaku, roughly "hypertranslation") to designate a version that deliberately improves on the original.[86]Book-title translations can be either descriptive or symbolic. Descriptive book titles, for example Antoine de Saint-Exupéry's Le Petit Prince  (The Little Prince), are meant to be informative, and can name the protagonist, and indicate the theme of the book.  An example of a symbolic book title is Stieg Larsson's  The Girl with the Dragon Tattoo, whose original Swedish title is Män som hatar kvinnor  (Men Who Hate Women).  Such symbolic book titles usually indicate the theme, issues, or atmosphere of the work.When translators are working with long book titles, the translated titles are often shorter and indicate the theme of the book.[87]The translation of plays poses many problems such as the added element of actors, speech duration, translation literalness, and the relationship between the arts of drama and acting. Successful play translators are able to create language that allows the actor and the playwright to work together effectively.[88] Play translators must also take into account several other aspects: the final performance, varying theatrical and acting traditions, characters' speaking styles, modern theatrical discourse, and even the acoustics of the auditorium, i.e., whether certain words will have the same effect on the new audience as they had on the original audience.[89]Audiences in Shakespeare's time were more accustomed than modern playgoers to actors having longer stage time.[90]  Modern translators tend to simplify the sentence structures of earlier dramas, which included compound sentences with intricate hierarchies of subordinate clauses.[91][92]In translating Chinese literature, translators struggle to find true fidelity in translating into the target language. In The Poem Behind the Poem, Barnstone argues that poetry "can't be made to sing through a mathematics that doesn't factor in the creativity of the translator".[93]A notable piece of work translated into English is the Wen Xuan, an anthology representative of major works of Chinese literature. Translating this work requires a high knowledge of the genres presented in the book, such as poetic forms, various prose types including memorials, letters, proclamations, praise poems, edicts, and historical, philosophical and political disquisitions, threnodies and laments for the dead, and examination essays. Thus the literary translator must be familiar with the writings, lives, and thought of a large number of its 130 authors, making the Wen Xuan one of the most difficult literary works to translate.[94]Translation generally, much as with Kurt Gödel's conception of mathematics, requires, to varying extents, more information than appears in the page of text being translated.Translation of a text that is sung in vocal music for the purpose of singing in another language—sometimes called "singing translation"—is closely linked to translation of poetry because most vocal music, at least in the Western tradition, is set to verse, especially verse in regular patterns with rhyme. (Since the late 19th century, musical setting of prose and free verse has also been practiced in some art music, though popular music tends to remain conservative in its retention of stanzaic forms with or without refrains.) A rudimentary example of translating poetry for singing is church hymns, such as the German chorales translated into English by Catherine Winkworth.[k]Translation of sung texts is generally much more restrictive than translation of poetry, because in the former there is little or no freedom to choose between a versified translation and a translation that dispenses with verse structure. One might modify or omit rhyme in a singing translation, but the assignment of syllables to specific notes in the original musical setting places great challenges on the translator.  There is the option in prose sung texts, less so in verse, of adding or deleting a syllable here and there by subdividing or combining notes, respectively, but even with prose the process is almost like strict verse translation because of the need to stick as closely as possible to the original prosody of the sung melodic line.Other considerations in writing a singing translation include repetition of words and phrases, the placement of rests and/or punctuation, the quality of vowels sung on high notes, and rhythmic features of the vocal line that may be more natural to the original language than to the target language. A sung translation may be considerably or completely different from the original, thus resulting in a contrafactum.Translations of sung texts—whether of the above type meant to be sung or of a more or less literal type meant to be read—are also used as aids to audiences, singers and conductors, when a work is being sung in a language not known to them. The most familiar types are translations presented as subtitles or surtitles projected during opera performances, those inserted into concert programs, and those that accompany commercial audio CDs of vocal music. In addition, professional and amateur singers often sing works in languages they do not know (or do not know well), and translations are then used to enable them to understand the meaning of the words they are singing.An important role in history has been played by translation of religious texts. Such translations may be influenced by tension between the text and the religious values the translators wish to convey. For example, Buddhist monks who translated the Indian sutras into Chinese occasionally adjusted their translations to better reflect China's distinct culture, emphasizing notions such as filial piety.One of the first recorded instances of translation in the West was the rendering of the Old Testament into Greek in the 3rd century BCE. The translation is known as the "Septuagint", a name that refers to the supposedly seventy translators (seventy-two, in some versions) who were commissioned to translate the Bible at Alexandria, Egypt. According to legend, each translator worked in solitary confinement in his own cell, and,  according to legend, all seventy versions proved identical. The Septuagint became the source text for later translations into many languages, including Latin, Coptic, Armenian and Georgian.Still considered one of the greatest translators in history, for having rendered the Bible into Latin, is Jerome (347–420 C.E.), the patron saint of translators. For centuries the Roman Catholic Church used his translation (known as the Vulgate), though even this translation stirred controversy.  By contrast with Jerome's contemporary, Augustine of Hippo (354–430 C.E.), who endorsed precise translation, Jerome believed in adaptation, and sometimes invention, in order to more effectively bring across the meaning.  Jerome's colorful Vulgate translation of the Bible includes some crucial instances of "overdetermination".  For example, Isaiah's prophecy announcing that the Savior will be born of a virgin, uses the word 'almah, which is also used to describe the dancing girls at Solomon's court, and simply means young and nubile.  Jerome, writes Marina Warner, translates it as virgo, "adding divine authority to the virulent cult of sexual disgust that shaped Christian moral theology (the [Moslem] Quran, free from this linguistic trap, does not connect Mariam/Mary's miraculous nature with moral horror of sex)."  The apple that Eve offered to Adam, according to Mark Polizzotti, could equally well have been an apricot, orange, or banana; but Jerome liked the pun malus/malum (apple/evil}.[95] Pope Francis has suggested that the phrase "lead us not into temptation", in the Lord's Prayer found in the Gospels of Matthew (the first Gospel, written c. 80–90 C.E.) and Luke (the third Gospel, written c. 80–110 C.E.), should more properly be translated, "do not let us fall into temptation", commenting that God does not lead people into temptation—Satan does.[l]  Some important early Christian authors interpreted the Bible's Greek text and Jerome's Latin Vulgate similarly to Pope Francis.  A.J.B. Higgins[97] in 1943 showed that among the earliest Christian authors, the understanding and even the text of this devotional verse underwent considerable changes.  These ancient writers suggest that, even if the Greek and Latin texts are left unmodified, something like "do not let us fall" could be an acceptable English rendering.  Higgins cited Tertullian, the earliest of the Latin Church Fathers (c. 155–c. 240 C.E., "do not allow us to be led") and Cyprian (c. 200–258 C.E., "do not allow us to be led into temptation").  A later author, Ambrose (C. 340–397 C.E.), followed Cyprian's interpretation.  Augustine of Hippo (354–430), familiar with Jerome's Latin Vulgate rendering, observed that "many people... say it this way:  'and do not allow us to be led into temptation.'"[98]  In 863 C.E. the brothers Saints Cyril and Methodius, the Byzantine Empire's "Apostles to the Slavs", began translating parts of the Bible into the Old Church Slavonic language, using the Glagolitic script that they had devised, based on the Greek alphabet.The periods preceding and contemporary with the Protestant Reformation saw translations of the Bible into vernacular (local) European languages—a development that contributed to Western Christianity's split into Roman Catholicism and Protestantism over disparities between Catholic and Protestant renderings of crucial words and passages (and due to a Protestant-perceived need to reform the Roman Catholic Church).  Lasting effects on the religions, cultures, and languages of their respective countries were exerted by such Bible translations as Martin Luther's into German (the New Testament, 1522), Jakub Wujek's into Polish (1599, as revised by the Jesuits), and William Tyndale's (New Testament, 1526 and revisions) and the King James Version into English (1611). Efforts to translate the Bible into English had their martyrs. William Tyndale (c. 1494–1536) was convicted of heresy at Antwerp, was strangled to death while tied at the stake, and then his dead body was burned.[99]  Earlier, John Wycliffe (c. mid-1320s – 1384) had managed to die a natural death, but 30 years later the Council of Constance in 1415 declared him a heretic and decreed that his works and earthly remains should be burned; the order, confirmed by Pope Martin V, was carried out in 1428, and Wycliffe's corpse was exhumed and burned and the ashes cast into the River Swift.  Debate and religious schism over different translations of religious texts continue, as demonstrated by, for example, the King James Only movement.A famous mistranslation of a Biblical text is the rendering of the Hebrew word קֶרֶן‎ (keren), which has several meanings, as "horn" in a context where it more plausibly means "beam of light":  as a result, for centuries artists, including sculptor Michelangelo, have rendered Moses the Lawgiver with horns growing from his forehead.Such fallibility of the translation process has contributed to the Islamic world's ambivalence about translating the Quran (also spelled Koran) from the original Arabic, as received by the prophet Muhammad from Allah (God) through the angel Gabriel incrementally between 609 and 632 C.E., the year of Muhammad's death.  During prayers, the Quran, as the miraculous and inimitable word of Allah, is recited only in Arabic.  However, as of 1936, it had been translated into at least 102 languages.[100]A fundamental difficulty in translating the Quran accurately stems from the fact that an Arabic word, like a Hebrew or Aramaic word, may have a range of meanings, depending on context.  This is said to be a linguistic feature, particularly of all Semitic languages, that adds to the usual similar difficulties encountered in translating between any two languages.[100]  There is always an element of human judgment—of interpretation—involved in understanding and translating a text.  Muslims regard any translation of the Quran as but one possible interpretation of the Quranic (Classical) Arabic text, and not as a full equivalent of that divinely communicated original.  Hence such a translation is often called an "interpretation" rather than a translation.[101]To complicate matters further, as with other languages, the meanings and usages of some expressions have changed over time, between the Classical Arabic of the Quran, and modern Arabic.  Thus a modern Arabic speaker may misinterpret the meaning of a word or passage in the Quran.  Moreover, the interpretation of a Quranic passage will also depend on the historic context of Muhammad's life and of his early community.  Properly researching that context requires a detailed knowledge of hadith and sirah, which are themselves vast and complex texts.  Hence, analogously to the translating of Chinese literature, an attempt at an accurate translation of the Quran requires a knowledge not only of the Arabic language and of the target language, including their respective evolutions, but also a deep understanding of the two cultures involved.Technical translation renders documents such as manuals, instruction sheets, internal memos, minutes, financial reports, and other documents for a limited audience (who are directly affected by the document) and whose useful life is often limited.  Thus, a user guide for a particular model of refrigerator is useful only for the owner of the refrigerator, and will remain useful only as long as that refrigerator model is in use.  Similarly, software documentation generally pertains to a particular software, whose applications are used only by a certain class of users.[102]
Tensor product
In mathematics, the tensor product V ⊗ W of two vector spaces V and W (over the same field) is itself a vector space, endowed with the operation of bilinear composition, denoted by ⊗, from ordered pairs in the Cartesian product V × W onto V ⊗ W in a way that generalizes the outer product.  The tensor product of V and W is the vector space generated by the symbols v ⊗ w, with v ∈ V and w ∈ W, in which the relations of bilinearity are imposed for the product operation ⊗, and no other relations are assumed to hold.  The tensor product space is thus the "freest" (or most general) such vector space, in the sense of having the fewest constraints.The tensor product of (finite dimensional) vector spaces has dimension equal to the product of the dimensions of the two factors:In particular, this distinguishes the tensor product from the direct sum vector space, whose dimension is the sum of the dimensions of the two summands:More generally, the tensor product can be extended to other categories of mathematical objects in addition to vector spaces, such as to  matrices, tensors, algebras, topological vector spaces, and modules.  In each such case the tensor product is characterized by a similar universal property: it is the freest bilinear operation.  The general concept of a "tensor product" is captured by monoidal categories; that is, the class of all things that have a tensor product is a monoidal category.The intuitive motivation for the tensor product relies on the concept of tensors more generally. In particular, a tensor is an object which can be considered a special type of multilinear map, which takes in a certain number of vectors (its order) and outputs a scalar. Such objects are useful in a number of areas of application, such as Riemannian geometry, famous for its use in Albert Einstein's general theory of relativity in modern physics, where the metric tensor is a fundamental concept: in particular, the metric tensor takes in two vectors, conceived of roughly as small arrows emanating from a specific point within a curved space, or manifold, and returns a local dot product of them relative to that particular point—an operation which encodes roughly the vectors' lengths as well as the angle between them. As the dot product is a scalar, the metric tensor is thus seen to deserve its name. There is one metric tensor at each point of the manifold, and variation in the metric tensor thus encodes how that distance and angle concepts, and so the laws of analytic geometry, vary throughout the manifold.If we have a basis for the vector spaces, and the vector space is finite-dimensional, we can represent the vectors in terms of components under those basis vectors:The purpose of the succeeding sections is to find a definition that is equivalent to this where it is applicable but which does not require a specific choice of basis and which can also more easily be applied to infinite-dimensional settings where the usual basis concepts (Hamel basis) may be ill-behaved. Not requiring a specific basis is useful from a theoretical point of view since while every vector space has a basis, not all bases are necessarily constructible, and moreover that result itself depends on the acceptance of the axiom of choice which may be rejected in some systems of mathematics. Also, it is useful to find an abstract construction for analysis from the point of view of category theory, the theory of the very zoomed-out "big picture of maths" and how all mathematical objects relate to each other in a very general sense. A very important real-life use for having such a definition can be found in another field of modern physics called quantum mechanics: the tensor product in this form allows us to talk of the wave function of a system of two particles as an abstract Hilbert space vector without having to specify a specific basis of observables.The above definition will actually work for any vector space in which we can specify a basis, since we can just rebuild it as the free vector space over that basis: the above construction exactly mirrors how you represent vectors via the Hamel basis construction by design. In effect, we haven't gained anything ... until we do this.in the first case andThis is useful to us because the outer product satisfies the following linearity properties, which can be proven by simple algebra on the corresponding matrix expressions (the vectors below are generic, not the example ones above):Equality between two concrete tensors is then obtained if using the above rules will permit us to rearrange one sum of outer products into the other by suitably decomposing vectors—regardless of if we have a set of actual basis vectors. Applying that to our example above, we see that of course we havefor which substitution ingives usElements of V ⊗ W are often referred to as tensors, although this term refers to many other related concepts as well.[1] If v belongs to V and w belongs to W, then the equivalence class of (v, w) is denoted by v ⊗ w, which is called the tensor product of v with w. In physics and engineering, this use of the  "⊗" symbol refers specifically to the outer product operation; the result of the outer product v ⊗ w is one of the standard ways of representing the equivalence class v ⊗ w.[2] An element of V ⊗ W that can be written in the form v ⊗ w is called a pure or simple tensor.  In general, an element of the tensor product space is not a pure tensor, but rather a finite linear combination of pure tensors.  For example, if v1 and v2 are linearly independent, and w1 and w2 are also linearly independent, then v1 ⊗ w1 + v2 ⊗ w2 cannot be written as a pure tensor. The number of simple tensors required to express an element of a tensor product is called the tensor rank (not to be confused with tensor order, which is the number of spaces one has taken the product of, in this case 2; in notation, the number of indices), and for linear operators or matrices, thought of as (1, 1) tensors (elements of the space V ⊗ V∗), it agrees with matrix rank.Given bases {vi}  and {wj}  for V and W respectively, the tensors {vi ⊗ wj} form a basis for V ⊗ W. Therefore, if V and W are finite-dimensional, the dimension of the tensor product is the product of dimensions of the original spaces; for instance Rm ⊗ Rn is isomorphic to Rmn.The tensor product also operates on linear maps between vector spaces. Specifically, given two linear maps S : V → X and T : W → Y between vector spaces, the tensor product of the two linear maps S and T is a linear mapdefined byIn this way, the tensor product becomes a bifunctor from the category of vector spaces to itself, covariant in both arguments.[3]If S and T are both injective, surjective, or continuous then S ⊗ T is, respectively, injective, surjective, continuous.By choosing bases of all vector spaces involved, the linear maps S and T can be represented by matrices. Then, the matrix describing the tensor product S ⊗ T is the Kronecker product of the two matrices. For example, if V, X, W, and Y above are all two-dimensional and bases have been fixed for all of them, and S and T are given by the matricesrespectively, then the tensor product of these two matrices isThe resultant rank is at most 4, and thus the resultant dimension is 4. Here rank denotes the tensor rank (number of requisite indices), while the matrix rank counts the number of degrees of freedom in the resulting array.A dyadic product is the special case of the tensor product between two vectors of the same dimension.This characterization can simplify proofs about the tensor product. For example, the tensor product is symmetric, meaning there is a canonical isomorphism:Similar reasoning can be used to show that the tensor product is associative, that is, there are natural isomorphismsThe category of vector spaces with tensor product is an example of a symmetric monoidal category.The universal-property definition of a tensor product is valid in more categories than just the category of vector spaces. Instead of using multilinear (bilinear) maps, the general tensor product definition uses multimorphisms.[4]Let n be a non-negative integer.  The nth tensor power of the vector space V is the n-fold tensor product of V with itself.  That isA permutation σ of the set {1, 2, ..., n}  determines a mapping of the nth Cartesian power of V as follows:Letbe the natural multilinear embedding of the Cartesian power of V into the tensor power of V.  Then, by the universal property, there is a unique isomorphismsuch thatThe isomorphism τσ is called the braiding map associated to the permutation σ.For non-negative integers r and s a type (r,s) tensor on a vector space V is an element ofHere V∗ is the dual vector space (which consists of all linear maps f from V to the ground field K).There is a product map, called the (tensor) product of tensors[5]It is defined by grouping all occurring "factors" V together: writing vi for an element of V and fi for elements of the dual space,Picking a basis of V and the corresponding dual basis of V∗ naturally induces a basis for Trs(V) (this basis is described in the article on Kronecker products). In terms of these bases, the components of a (tensor) product of two (or more) tensors can be computed. For example, if F and G are two covariant tensors of rank m and n respectively (i.e. F ∈ T 0m, and G ∈ T 0n), then the components of their tensor product are given by[6]Thus, the components of the tensor product of two tensors are the ordinary product of the components of each tensor. Another example: let U be a tensor of type (1, 1) with components Uαβ, and let V be a tensor of type (1, 0) with components V γ.  ThenandProducts of tensors form an algebra, called the tensor algebra.A particular example is the tensor product of some vector space V with its dual vector space V∗ (which consists of all linear maps f from V to the ground field K). In this case, there is a canonical evaluation mapwhich on elementary tensors is defined byThe resulting mapis called tensor contraction (for r, s > 0).On the other hand, if V is finite-dimensional, there is a canonical map in the other direction (called the coevaluation map)where v1, ..., vn is any basis of V, and vi∗ is its dual basis. Surprisingly, this map does not depend on our choice of basis.[7]The interplay of evaluation and coevaluation map can be used to characterize finite-dimensional vector spaces without referring to bases.[8]Given two finite dimensional vector spaces U, V, denote the dual space of U as U*, we have the following relation: This result implies Furthermore, given three vector spaces U, V, W the tensor product is linked to the vector space of all linear maps, as follows:Here Hom(-,-) denotes the K-vector space of all linear maps. This is an example of adjoint functors: the tensor product is "left adjoint" to Hom.where u∗ in End(V∗) is the transpose of u, that is, in terms of the obvious pairing on V ⊗ V∗,The tensor product of two modules A and B over a commutative ring R is defined in exactly the same way as the tensor product of vector spaces over a field:where now F(A × B) is the free R-module generated by the cartesian product and G is the R-module generated by the same relations as above.More generally, the tensor product can be defined even if the ring is non-commutative (ab ≠ ba). In this case A has to be a right-R-module and B is a left-R-module, and instead of the last two relations above, the relationis imposed. If R is non-commutative, this is no longer an R-module, but just an abelian group.The universal property also carries over, slightly modified: the map φ : A × B → A ⊗R B defined by (a, b) ↦ a ⊗ b is a middle linear map (referred to as "the canonical middle linear map".[9]); that is,[10] it satisfies:Let A be a right R-module and B be a left R-module B. Then the tensor product of A and B is an abelian group defined byFor vector spaces, the tensor product V ⊗ W is quickly computed since bases of V of W immediately determine a basis of V ⊗ W, as was mentioned above. For modules over a general (commutative) ring, not every module is free. For example, Z/nZ is not a free abelian group (= Z-module). The tensor product with Z/nZ is given byHere NJ := ⨁j ∈ J N and the map is determined by sending some n ∈ N in the jth copy of NJ to ajin (in NI). Colloquially, this may be rephrased by saying that a presentation of M gives rise to a presentation of M ⊗R N. This is referred to by saying that the tensor product is a right exact functor. It is not in general left exact, that is, given an injective map of R-modules M1 → M2, the tensor productis not usually injective. For example, tensoring the (injective) map given by multiplication with n, n : Z → Z with Z/nZ yields the zero map 0 : Z/nZ → Z/nZ, which is not injective. Higher Tor functors measure the defect of the tensor product being not left exact. All higher Tor functors are assembled in the derived tensor product.Let R be a commutative ring. The tensor product of R-modules applies, in particular, if A and B are R-algebras. In this case, the tensor product A ⊗R B is an R-algebra itself by puttingFor example,A particular example is when A and B are fields containing a common subfield R. The tensor product of fields is closely related to Galois theory: if, say, A = R[x] / f(x), where f is some irreducible polynomial with coefficients in R, the tensor product can be calculated aswhere now f is interpreted as the same polynomial, but with its coefficients regarded as elements of B. In the larger field B, the polynomial may become reducible, which brings in Galois theory. For example, if A = B is a Galois extension of R, thenis isomorphic (as an A-algebra) to the Adeg(f).Hilbert spaces generalize finite-dimensional vector spaces to countably-infinite dimensions. The tensor product is still defined; it is the tensor product of Hilbert spaces.When the basis for a vector space is no longer countable, then the appropriate axiomatic formalization for the vector space is that of a topological vector space. The tensor product is still defined, it is the topological tensor product.Some vector spaces can be decomposed into direct sums of subspaces. In such cases, the tensor product of two spaces can be decomposed into sums of products of the subspaces (in analogy to the way that multiplication distributes over addition).Vector spaces endowed with an additional multiplicative structure are called algebras. The tensor product of such algebras is described by the Littlewood–Richardson rule.This is a special case of the product of tensors if they are seen as multilinear maps (see also tensors as multilinear maps). Thus the components of the tensor product of multilinear forms can be computed by the Kronecker product.It should be mentioned that, though called "tensor product", this is not a tensor product of graphs in the above sense; actually it is the category-theoretic product in the category of graphs and graph homomorphisms. However it is actually the Kronecker tensor product of the adjacency matrices of the graphs. Compare also the section Tensor product of linear maps above.The most general setting for the tensor product is the monoidal category. It captures the algebraic essence of tensoring, without making any specific reference to what is being tensored. Thus, all tensor products can be expressed as an application of the monoidal category to some particular setting, acting on some particular objects.A number of important subspaces of the tensor algebra can be constructed as quotients: these include the exterior algebra, the symmetric algebra, the Clifford algebra, the Weyl algebra, and the universal enveloping algebra in general. Note that when the underlying field of V does not have characteristic 2, then this definition is equivalent toThe symmetric algebra is constructed in a similar manner, from the symmetric productMore generallyThat is, in the symmetric algebra two adjacent vectors (and therefore all of them) can be interchanged. The resulting objects are called symmetric tensors.Additional algebras result from quotienting by other polynomials; the general case is given by the universal enveloping algebras.Array programming languages may have this pattern built in.  For example, in APL the tensor product is expressed as ○.× (for example A ○.× B or A ○.× B ○.× C).  In J the tensor product is the dyadic form of */ (for example a */ b or a */ b */ c).Note that J's treatment also allows the representation of some tensor fields, as a and b may be functions instead of constants. This product of two functions is a derived function, and if a and b are differentiable, then a */ b is differentiable.However, these kinds of notation are not universally present in array languages.  Other array languages may require explicit treatment of indices (for example, MATLAB), and/or may not support higher-order functions such as the Jacobian derivative (for example, Fortran/APL).
Convex cone
In linear algebra, a convex cone is a subset of a vector space over an ordered field that is closed under linear combinations with positive coefficients.A subset C of a vector space V is a cone (or sometimes called a linear cone) if for each x in C and positive scalars α, the product αx is in C.[1]A cone C is a convex cone if αx + βy belongs to C, for any positive scalars α, β, and any x, y in C.[2][3]This concept is meaningful for any vector space that allows the concept of "positive" scalar, such as spaces over the rational, algebraic, or (more commonly) the real numbers. Also note that the scalars in the definition are positive meaning that the origin does not have to belong to C. Some authors use a definition that ensures the origin belongs to C.[4] Because of the scaling parameters α and β, cones are infinite in extent and not bounded.It follows from the above property that a convex cone can also be defined as a linear cone that is closed under convex combinations, or just under additions. More succinctly, a set C is a convex cone if and only if αC = C and C + C = C, for any positive scalar α.An affine convex cone is the set resulting from applying an affine transformation to a convex cone.[5] A common example is translating a convex cone by a point p: p+C. Technically, such transformations can produce non-cones. For example, unless p=0, p+C is not a linear cone. However, it is still called an affine convex cone.Half-spaces (open or closed) are affine convex cones. Moreover (in finite dimensions), any convex cone C that is not the whole space V must be contained in some closed half-space H of V; this is a special case of Farkas' lemma.Polyhedral cones play a central role in the representation theory of polyhedra. For instance, the decomposition theorem for polyhedra states that every polyhedron can be written as the Minkowski sum of a convex polytope and a polyhedral cone.[11][12] Polyhedral cones also play an important part in proving the related Finite Basis Theorem for polytopes which shows that every polytope is a polyhedron and every bounded polyhedron is a polytope.[11][13][14]According to the above definition, if C is a convex cone, then C ∪ {0} is a convex cone, too. A convex cone is said to be pointed if 0 is in C, and blunt if 0 is not in C.[1][15] Blunt cones can be excluded from the definition of convex cone by substituting "non-negative" for "positive" in the condition of α, β.A cone is called flat if it contains some nonzero vector x and its opposite -x, meaning C contains a linear subspace of dimension at least one, and salient otherwise.[16][17] A blunt convex cone is necessarily salient, but the converse is not necessarily true. A convex cone C is salient if and only if C ∩ −C ⊆ {0}.Some authors require salient cones to be pointed.[18] The term "pointed" is also often used to refer to a closed cone that contains no complete line (i.e., no nontrivial subspace of the ambient vector space V, or what is called a salient cone).[19][20][21] The term proper (convex) cone is variously defined, depending on the context and author. It often means a cone that satisfies other properties like being convex, closed, pointed, salient, and full-dimensional.[22][23][24] Because of these varying definitions, the context or source should be consulted for the definition of these terms.Let C ⊂ V be a set, not necessary a convex set, in a real vector space V equipped with an inner product. The (continuous or topological) dual cone to C is the setwhich is always a convex cone.More generally, the (algebraic) dual cone to C ⊂ V in a linear space V is a subset of the dual space V* defined by:In other words, if V* is the algebraic dual space of V, it is the set of linear functionals that are nonnegative on the primal cone C. If we take V* to be the continuous dual space then it is the set of continuous linear functionals nonnegative on C.[26] This notion does not require the specification of an inner product on V.In finite dimensions, the two notions of dual cone are essentially the same because every finite dimensional linear functional is continuous,[27] and every continuous linear functional in a inner product space induces a linear isomorphism (nonsingular linear map) from V* to V, and this isomorphism will take the dual cone given by the second definition, in V*, onto the one given by the first definition; see the Riesz representation theorem.[26]If C is equal to its dual cone, then C is called self-dual. A cone can be said to be self-dual without reference to any given inner product, if there exists an inner product with respect to which it is equal to its dual by the first definition.
Overlap–save method
    (Eq.1)where h[m]=0 for m outside the region [1, M].The concept is to compute short segments of y[n] of an arbitrary length L, and concatenate the segments together.  Consider a segment that begins at n = kL + M, for any integer k, and define:Then, for kL + M  ≤  n  ≤  kL + L + M − 1, and equivalently M  ≤  n − kL  ≤  L + M − 1, we can write:The task is thereby reduced to computing yk[n], for M  ≤  n  ≤  L + M − 1.  The process described above is illustrated in the accompanying figure.Now note that if we periodically extend xk[n] with period N  ≥  L + M − 1, according to:The advantage is that the circular convolution can be computed very efficiently as follows, according to the circular convolution theorem:where:When the DFT and its inverse is implemented by the FFT algorithm, the pseudocode above requires about N log2(N) + N complex multiplications for the FFT, product of arrays, and IFFT.[note 1]  Each iteration produces N-M+1 output samples, so the number of complex multiplications per output sample is about:    (Eq.2)For example, when M=201 and N=1024, Eq.2 equals 13.67, whereas direct evaluation of Eq.1 would require up to 201 complex multiplications per output sample, the worst case being when both x and h are complex-valued.  Also note that for any given M, Eq.2 has a minimum with respect to N.  It diverges for both small and large block sizes.Overlap–discard[1] and Overlap–scrap[2] are less commonly used labels for the same method described here.  However, these labels are actually better (than overlap–save) to distinguish from overlap–add, because both methods "save", but only one discards.  "Save" merely refers to the fact that M − 1 input (or output) samples from segment k are needed to process segment k + 1.The overlap–save algorithm may be extended to include other common operations of a system:[note 2][3]
System of linear equations
In mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1]  For example,is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given bysince it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.The simplest kind of linear system involves two equations and two variables:Now substitute this expression for x into the bottom equation:A general system of m linear equations with n unknowns can be written asOften the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.The vector equation is equivalent to a matrix equation of the formwhere A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The number of vectors in a basis for the span is now expressed as the rank of the matrix.A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.A linear system may behave in any one of three possible ways:For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.The following pictures illustrate this trichotomy in the case of two variables:The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.For example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For a more complicated example, the equationsare not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.There are several algorithms for solving a system of linear equations.To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.For example, consider the following system:The solution set to this system can be described by the following equations:Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Here x is the free variable, and y and z are dependent.The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:For example, consider the following system:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsSolving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:The last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemis given byFor each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.)Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.There is also a quantum algorithm for linear systems of equations.[3]A system of linear equations is homogeneous if all of the constant terms are zero:A homogeneous system is equivalent to a matrix equation of the formwhere A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution.  If the system has a singular matrix then there is a solution set with an infinite number of solutions.  This solution set has the following additional properties:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A.Numerical solutions to a homogeneous system can be found with a singular value decomposition.There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asGeometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.
Abelian group
In abstract algebra, an abelian group, also called a commutative group, is a group in which the result of applying the group operation to two group elements does not depend on the order in which they are written. That is, these are the groups that obey the axiom of commutativity. Abelian groups generalize the arithmetic of addition of integers. They are named after early 19th century mathematician Niels Henrik Abel.[1]The concept of an abelian group is one of the first concepts encountered in undergraduate abstract algebra, from which many other basic concepts, such as modules and vector spaces, are developed. The theory of abelian groups is generally simpler than that of their non-abelian counterparts, and finite abelian groups are very well understood. On the other hand, the theory of infinite abelian groups is an area of current research.An abelian group is a set, A, together with an operation • that combines any two elements a and b to form another element denoted a • b. The symbol • is a general placeholder for a concretely given operation. To qualify as an abelian group, the set and operation, (A, •), must satisfy five requirements known as the abelian group axioms:A group in which the group operation is not commutative is called a "non-abelian group" or "non-commutative group".There are two main notational conventions for abelian groups – additive and multiplicative.Generally, the multiplicative notation is the usual notation for groups, while the additive notation is the usual notation for modules and rings. The additive notation may also be used to emphasize that a particular group is abelian, whenever both abelian and non-abelian groups are considered, some notable exceptions being near-rings and partially ordered groups, where an operation is written additively even when non-abelian.To verify that a finite group is abelian, a table (matrix) – known as a Cayley table – can be constructed in a similar fashion to a multiplication table. If the group is G = {g1 = e, g2, ..., gn} under the operation ⋅, the (i, j)th entry of this table contains the product gi ⋅ gj. The group is abelian if and only if this table is symmetric about the main diagonal.This is true since if the group is abelian, then gi ⋅ gj = gj ⋅ gi. This implies that the (i, j)th entry of the table equals the (j, i)th entry, thus the table is symmetric about the main diagonal.In general, matrices, even invertible matrices, do not form an abelian group under multiplication because matrix multiplication is generally not commutative. However, some groups of matrices are abelian groups under matrix multiplication – one example is the group of 2×2 rotation matrices.Camille Jordan named abelian groups after Norwegian mathematician Niels Henrik Abel, because Abel found that the commutativity of the group of a polynomial implies that the roots of the polynomial can be calculated by using radicals. See Section 6.5 of Cox (2004) for more information on the historical background.If n is a natural number and x is an element of an abelian group G written additively, then nx can be defined as x + x + ... + x (n summands) and (−n)x = −(nx). In this way, G becomes a module over the ring Z of integers. In fact, the modules over Z can be identified with the abelian groups.Theorems about abelian groups (i.e. modules over the principal ideal domain Z) can often be generalized to theorems about modules over an arbitrary principal ideal domain. A typical example is the classification of finitely generated abelian groups which is a specialization of the structure theorem for finitely generated modules over a principal ideal domain. In the case of finitely generated abelian groups, this theorem guarantees that an abelian group splits as a direct sum of a torsion group and a free abelian group. The former may be written as a direct sum of finitely many groups of the form Z/pkZ for p prime, and the latter is a direct sum of finitely many copies of Z.If f, g : G → H are two group homomorphisms between abelian groups, then their sum f + g, defined by (f + g) (x) = f(x) + g(x), is again a homomorphism. (This is not true if H is a non-abelian group.) The set Hom(G, H) of all group homomorphisms from G to H thus turns into an abelian group in its own right.Somewhat akin to the dimension of vector spaces, every abelian group has a rank. It is defined as the maximal cardinality of a set of linearly independent elements of the group. The integers and the rational numbers have rank one, as well as every subgroup of the rationals.The center Z(G) of a group G is the set of elements that commute with every element of G. A group G is abelian if and only if it is equal to its center Z(G). The center of a group G is always a characteristic abelian subgroup of G. If the quotient group G/Z(G) of a group by its center is cyclic then G is abelian.[3]Cyclic groups of integers modulo n, Z/nZ, were among the first examples of groups. It turns out that an arbitrary finite abelian group is isomorphic to a direct sum of finite cyclic groups of prime power order, and these orders are uniquely determined, forming a complete system of invariants. The automorphism group of a finite abelian group can be described directly in terms of these invariants. The theory had been first developed in the 1879 paper of Georg Frobenius and Ludwig Stickelberger and later was both simplified and generalized to finitely generated modules over a principal ideal domain, forming an important chapter of linear algebra.Any group of prime order is isomorphic to a cyclic group and therefore abelian. Any group whose order is a square of a prime number is abelian.[4] In fact, for every prime number p there are (up to isomorphism) exactly two groups of order p2, namely Zp2 and Zp×Zp.The fundamental theorem of finite abelian groups states that every finite abelian group G can be expressed as the direct sum of cyclic subgroups of prime-power order; it is also known as the basis theorem for finite abelian groups.[5] This is generalized by the fundamental theorem of finitely generated abelian groups, with finite groups being the special case when G has zero rank; this in turn admits numerous further generalizations.The classification was proven by Leopold Kronecker in 1870, though it was not stated in modern group-theoretic terms until later, and was preceded by a similar classification of quadratic forms by Gauss in 1801; see history for details.The cyclic group Zmn of order mn is isomorphic to the direct sum of Zm and Zn if and only if m and n are coprime. It follows that any finite abelian group G is isomorphic to a direct sum of the formin either of the following canonical ways:For example, Z15 can be expressed as the direct sum of two cyclic subgroups of order 3 and 5: Z15 ≅ {0, 5, 10} ⊕ {0, 3, 6, 9, 12}. The same can be said for any abelian group of order 15, leading to the remarkable conclusion that all abelian groups of order 15 are isomorphic.For another example, every abelian group of order 8 is isomorphic to either Z8 (the integers 0 to 7 under addition modulo 8), Z4 ⊕ Z2 (the odd integers 1 to 15 under multiplication modulo 16), or Z2 ⊕ Z2 ⊕ Z2.See also list of small groups for finite abelian groups of order 30 or less.One can apply the fundamental theorem to count (and sometimes determine) the automorphisms of a given finite abelian group G. To do this, one uses the fact that if G splits as a direct sum H ⊕ K of subgroups of coprime order, then Aut(H ⊕ K) ≅ Aut(H) ⊕ Aut(K).Given this, the fundamental theorem shows that to compute the automorphism group of G it suffices to compute the automorphism groups of the Sylow p-subgroups separately (that is, all direct sums of cyclic subgroups, each with order a power of p). Fix a prime p and suppose the exponents ei of the cyclic factors of the Sylow p-subgroup are arranged in increasing order:for some n > 0. One needs to find the automorphisms ofOne special case is when n = 1, so that there is only one cyclic prime-power factor in the Sylow p-subgroup P. In this case the theory of automorphisms of a finite cyclic group can be used. Another special case is when n is arbitrary but ei = 1 for 1 ≤ i ≤ n. Here, one is considering P to be of the formso elements of this subgroup can be viewed as comprising a vector space of dimension n over the finite field of p elements Fp. The automorphisms of this subgroup are therefore given by the invertible linear transformations, sowhere GL is the appropriate general linear group. This is easily shown to have orderIn the most general case, where the ei and n are arbitrary, the automorphism group is more difficult to determine. It is known, however, that if one definesandthen one has in particular dk ≥ k, ck ≤ k, andOne can check that this yields the orders in the previous examples as special cases (see Hillar, C., & Rhea, D.).This homomorphism is surjective, and its kernel is finitely generated (since integers form a Noetherian ring). Let us consider the matrix M with integer entries, such that the entries of its jth column are the coefficients of the jth generator of the kernel. Then, the abelian group is isomorphic to the cokernel of linear map defined by M. Conversely every integer matrix defines a finitely generated abelian group.It follows that the study of finitely generated abelian groups is totally equivalent with the study of integer matrices. In particular, changing the generating set of A is equivalent with multiplying M on the left by a unimodular matrix (that is an invertible integer matrix whose inverse is also an integer matrix). Changing the generating set of the kernel of M is equivalent with multiplying M on the right by an unimodular matrix.The Smith normal form of M is a matrixwhere r is the number of zero rows at the bottom of r (and also the rank of the group). This is the fundamental theorem of finitely generated abelian groups.The existence of algorithms for Smith normal form shows that the fundamental theorem of finitely generated abelian groups is not only a theorem of abstract existence, but provides a way for computing expression of finitely generated abelian groups as direct sums.The simplest infinite abelian group is the infinite cyclic group Z. Any finitely generated abelian group A is isomorphic to the direct sum of r copies of Z and a finite abelian group, which in turn is decomposable into a direct sum of finitely many cyclic groups of primary orders. Even though the decomposition is not unique, the number r, called the rank of A, and the prime powers giving the orders of finite cyclic summands are uniquely determined.By contrast, classification of general infinitely generated abelian groups is far from complete. Divisible groups, i.e. abelian groups A in which the equation nx = a admits a solution x ∈ A for any natural number n and element a of A, constitute one important class of infinite abelian groups that can be completely characterized. Every divisible group is isomorphic to a direct sum, with summands isomorphic to Q and Prüfer groups Qp/Zp for various prime numbers p, and the cardinality of the set of summands of each type is uniquely determined.[6] Moreover, if a divisible group A is a subgroup of an abelian group G then A admits a direct complement: a subgroup C of G such that G = A ⊕ C. Thus divisible groups are injective modules in the category of abelian groups, and conversely, every injective abelian group is divisible (Baer's criterion). An abelian group without non-zero divisible subgroups is called reduced.Two important special classes of infinite abelian groups with diametrically opposite properties are torsion groups and torsion-free groups, exemplified by the groups Q/Z (periodic) and Q (torsion-free).An abelian group is called periodic or torsion, if every element has finite order. A direct sum of finite cyclic groups is periodic. Although the converse statement is not true in general, some special cases are known. The first and second Prüfer theorems state that if A is a periodic group, and it either has a bounded exponent, i.e., nA = 0 for some natural number n, or is countable and the p-heights of the elements of A are finite for each p, then A is isomorphic to a direct sum of finite cyclic groups.[7] The cardinality of the set of direct summands isomorphic to Z/pmZ in such a decomposition is an invariant of A. These theorems were later subsumed in the Kulikov criterion. In a different direction, Helmut Ulm found an extension of the second Prüfer theorem to countable abelian p-groups with elements of infinite height: those groups are completely classified by means of their Ulm invariants.An abelian group is called torsion-free if every non-zero element has infinite order. Several classes of torsion-free abelian groups have been studied extensively:An abelian group that is neither periodic nor torsion-free is called mixed. If A is an abelian group and T(A) is its torsion subgroup, then the factor group A/T(A) is torsion-free. However, in general the torsion subgroup is not a direct summand of A, so  A is not isomorphic to T(A) ⊕ A/T(A). Thus the theory of mixed groups involves more than simply combining the results about periodic and torsion-free groups.One of the most basic invariants of an infinite abelian group A is its rank: the cardinality of the maximal linearly independent subset of A. Abelian groups of rank 0 are precisely the periodic groups, while torsion-free abelian groups of rank 1 are necessarily subgroups of Q and can be completely described. More generally, a torsion-free abelian group of finite rank r is a subgroup of Qr. On the other hand, the group of p-adic integers Zp is a torsion-free abelian group of infinite Z-rank and the groups Znp with different n are non-isomorphic, so this invariant does not even fully capture properties of some familiar groups.The classification theorems for finitely generated, divisible, countable periodic, and rank 1 torsion-free abelian groups explained above were all obtained before 1950 and form a foundation of the classification of more general infinite abelian groups. Important technical tools used in classification of infinite abelian groups are pure and basic subgroups. Introduction of various invariants of torsion-free abelian groups has been one avenue of further progress. See the books by Irving Kaplansky, László Fuchs, Phillip Griffith, and David Arnold, as well as the proceedings of the conferences on Abelian Group Theory published in Lecture Notes in Mathematics for more recent findings.The additive group of a ring is an abelian group, but not all abelian groups are additive groups of rings (with nontrivial multiplication). Some important topics in this area of study are:Many large abelian groups possess a natural topology, which turns them into topological groups.The collection of all abelian groups, together with the homomorphisms between them, forms the category Ab, the prototype of an abelian category.Nearly all well-known algebraic structures other than Boolean algebras are undecidable. Hence it is surprising that Tarski's student Wanda Szmielew (1955) proved that the first order theory of abelian groups, unlike its nonabelian counterpart, is decidable. This decidability, plus the fundamental theorem of finite abelian groups described above, highlight some of the successes in abelian group theory, but there are still many areas of current research:Moreover, abelian groups of infinite order lead, quite surprisingly, to deep questions about the set theory commonly assumed to underlie all of mathematics. Take the Whitehead problem: are all Whitehead groups of infinite order also free abelian groups? In the 1970s, Saharon Shelah proved that the Whitehead problem is:Among mathematical adjectives derived from the proper name of a mathematician, the word "abelian" is rare in that it is often spelled with a lowercase a, rather than an uppercase A, indicating how ubiquitous the concept is in modern mathematics.[8]
Multiplicative inverse
In mathematics, a multiplicative inverse or reciprocal for a number x, denoted by 1/x or x−1, is a number which when multiplied by x yields the multiplicative identity, 1. The multiplicative inverse of a fraction a/b is b/a. For the multiplicative inverse of a real number, divide 1 by the number. For example, the reciprocal of 5 is one fifth (1/5 or 0.2), and the reciprocal of 0.25 is 1 divided by 0.25, or 4. The reciprocal function, the function f(x) that maps x to 1/x, is one of the simplest examples of a function which is its own inverse (an involution).The term reciprocal was in common use at least as far back as the third edition of Encyclopædia Britannica (1797) to describe two numbers whose product is 1; geometrical quantities in inverse proportion are described as reciprocall in a 1570 translation of Euclid's Elements.[1]In the phrase multiplicative inverse, the qualifier multiplicative is often omitted and then tacitly understood (in contrast to the additive inverse). Multiplicative inverses can be defined over many mathematical domains as well as numbers. In these cases it can happen that ab ≠ ba; then "inverse" typically implies that an element is both a left and right inverse.The notation f −1 is sometimes also used for the inverse function of the function f, which is not in general equal to the multiplicative inverse. For example, the multiplicative inverse 1/(sin x) = (sin x)−1 is the cosecant of x, and not the inverse sine of x denoted by sin−1 x or arcsin x. Only for linear maps are they strongly related (see below). The terminology difference reciprocal versus inverse is not sufficient to make this distinction, since many authors prefer the opposite naming convention, probably for historical reasons (for example in French, the inverse function is preferably called bijection réciproque).In the real numbers, zero does not have a reciprocal because no real number multiplied by 0 produces 1 (the product of any number with zero is zero). With the exception of zero, reciprocals of every real number are real, reciprocals of every rational number are rational, and reciprocals of every complex number are complex. The property that every element other than zero has a multiplicative inverse is part of the definition of a field, of which these are all examples. On the other hand, no integer other than 1 and −1 has an integer reciprocal, and so the integers are not a field.In modular arithmetic, the modular multiplicative inverse of a is also defined: it is the number x such that ax ≡ 1 (mod n).  This multiplicative inverse exists if and only if a and n are coprime.  For example, the inverse of 3 modulo 11 is 4 because 4 · 3 ≡ 1 (mod 11).  The extended Euclidean algorithm may be used to compute it.The sedenions are an algebra in which every nonzero element has a multiplicative inverse, but which nonetheless has divisors of zero, i.e. nonzero elements x, y such that xy = 0.A square matrix has an inverse if and only if its determinant has an inverse in the coefficient ring. The linear map that has the matrix A−1 with respect to some base is then the reciprocal function of the map having A as matrix in the same base. Thus, the two distinct notions of the inverse of a function are strongly related in this case, while they must be carefully distinguished in the general case (as noted above).The trigonometric functions are related by the reciprocal identity: the cotangent is the reciprocal of the tangent; the secant is the reciprocal of the cosine; the cosecant is the reciprocal of the sine.A ring in which every nonzero element has a multiplicative inverse is a division ring; likewise an algebra in which this holds is a division algebra.For a complex number in polar form z = r(cos φ + i sin φ), the reciprocal simply takes the reciprocal of the magnitude and the negative of the angle:In real calculus, the derivative of 1/x = x−1 is given by the power rule with the power −1:The power rule for integrals (Cavalieri's quadrature formula) cannot be used to compute the integral of 1/x, because doing so would result in division by 0:Instead the integral is given by:The reciprocal may be computed by hand with the use of long division.This continues until the desired precision is reached. For example, suppose we wish to compute 1/17 ≈ 0.0588 with 3 digits of precision. Taking x0 = 0.1, the following sequence is produced:A typical initial guess can be found by rounding b to a nearby power of 2, then using bit shifts to compute its reciprocal.In constructive mathematics, for a real number x to have a reciprocal, it is not sufficient that x ≠ 0. There must instead be given a rational number r such that 0 < r < |x|. In terms of the approximation algorithm described above, this is needed to prove that the change in y will eventually become arbitrarily small.This iteration can also be generalised to a wider sort of inverses, e.g. matrix inverses.If the multiplication is associative, an element x with a multiplicative inverse cannot be a zero divisor (x is a zero divisor if some nonzero y, xy = 0). To see this, it is sufficient to multiply the equation xy = 0 by the inverse of x (on the left), and then simplify using associativity. In the absence of associativity, the sedenions provide a counterexample.The converse does not hold: an element which is not a zero divisor is not guaranteed to have a multiplicative inverse.Within Z, all integers except −1, 0, 1 provide examples; they are not zero divisors nor do they have inverses in Z.If the ring or algebra is finite, however, then all elements a which are not zero divisors do have a (left and right) inverse. For, first observe that the map f(x) = ax must be injective: f(x) = f(y) implies x = y:Distinct elements map to distinct elements, so the image consists of the same finite number of elements, and the map is necessarily surjective. Specifically, ƒ (namely multiplication by a) must map some element x to 1, ax = 1, so that x is an inverse for a.The expansion of the reciprocal 1/q in any base can also act [3] as a source of pseudo-random numbers, if q is a "suitable" safe prime, a prime of the form 2p + 1 where p is also a prime. A sequence of pseudo-random numbers of length q − 1 will be produced by the expansion.
Diagonalizable matrix
In linear algebra, a square matrix A is called diagonalizable if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix P such that P−1AP is a diagonal matrix. If V is a finite-dimensional vector space, then a linear map T : V → V is called diagonalizable if there exists an ordered basis of V with respect to which T is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map.[1] A square matrix that is not diagonalizable is called defective.Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle; once their eigenvalues and eigenvectors are known, one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power, and the determinant of a diagonal matrix is simply the product of all diagonal entries. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or anisotropic scaling) — it scales the space, as does a homogeneous dilation, but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).The fundamental fact about diagonalizable maps and matrices is expressed by the following:Another characterization: A matrix or linear map is diagonalizable over the field F if and only if its minimal polynomial is a product of distinct linear factors over F. (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)The following sufficient (but not necessary) condition is often useful.which has eigenvalues 1, 2, 2 (not all distinct) and is diagonalizable with diagonal form (similar to A)and change of basis matrix PAs a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex n×n matrices that are not diagonalizable over C, considered as a subset of Cn×n, has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology: the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface. From that follows also density in the usual (strong) topology given by a norm. The same is not true over R.The Jordan–Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each "block" is a one-by-one matrix.If a matrix A can be diagonalized, that is,then:the above equation can be rewritten asSo the column vectors of P are right eigenvectors of A, and the corresponding diagonal entry is the corresponding eigenvalue. The invertibility of P also suggests that the eigenvectors are linearly independent and form a basis of Fn. This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of P−1 are the left eigenvectors of A.When a complex matrix A[a] is a Hermitian matrix (or a real matrix[b], a symmetric matrix), eigenvectors of A can be chosen to form an orthonormal basis of ℂn (or ℝn for a real matrix). Under such circumstance P will be a unitary matrix (resp. orthogonal matrix) and P−1 equals the conjugate transpose of P (if real, then the transpose) of P).For most practical work matrices are diagonalized numerically using computer software. Many algorithms exist to accomplish this.A set of matrices is said to be simultaneously diagonalizable if there exists a single invertible matrix P such that P−1AP is a diagonal matrix for every A in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalizable.[2]The set of all n×n diagonalizable matrices (over C) with n > 1 is not simultaneously diagonalizable. For instance, the matricesare diagonalizable but not simultaneously diagonalizable because they do not commute.A set consists of commuting normal matrices if and only if it is simultaneously diagonalizable by a unitary matrix; that is, there exists a unitary matrix U such that U*AU is diagonal for every A in the set.In the language of Lie theory, a set of simultaneously diagonalizable matrices generate a toral Lie algebra.In general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to "do the best one can", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal – known as Jordan normal form.Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices. This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, considerThis matrix is not diagonalizable: there is no matrix U such that U−1CU is a diagonal matrix. Indeed, C has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.Some real matrices are not diagonalizable over the reals. Consider for instance the matrixThe matrix B does not have any real eigenvalues, so there is no real matrix Q such that Q−1BQ is a diagonal matrix. However, we can diagonalize B if we allow complex numbers. Indeed, if we takethen Q−1BQ is diagonal. It is easy to find that B is the rotation matrix which rotates counterclockwise by angle θ = 3π/2Note that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.Consider a matrixThis matrix has eigenvaluesA is a 3×3 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly n distinct eigenvalues in an n×n matrix then this matrix is diagonalizable.These eigenvalues are the values that will appear in the diagonalized form of matrix A, so by finding the eigenvalues of A we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize A.The eigenvectors of A areNow, let P be the matrix with these eigenvectors as its columns:Note that there is no preferred order of the eigenvectors in P; changing the order of the eigenvectors in P just changes the order of the eigenvalues in the diagonalized form of A.[3]Then P diagonalizes A, as a simple computation confirms, having calculated P −1 using any suitable method:Diagonalization can be used to compute the powers of a matrix A efficiently, provided the matrix is diagonalizable. Suppose we have found thatand the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions that can be defined as power series.This is particularly useful in finding closed form expressions for terms of linear recursive sequences, such as the Fibonacci numbers.For example, consider the following matrix:Calculating the various powers of M reveals a  surprising pattern:The above phenomenon can be explained by diagonalizing M.  To accomplish this, we need a basis of R2 consisting of eigenvectors of M.  One such eigenvector basis is given bywhere ei denotes the standard basis of Rn. The reverse change of basis is given byStraightforward calculations show thatThus, a and b are the eigenvalues corresponding to u and v, respectively. By linearity of matrix multiplication, we have thatSwitching back to the standard basis, we haveThe preceding relations, expressed in matrix form, arethereby explaining the above phenomenon.In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schrödinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space).A very common approximation is to truncate Hilbert space to finite dimension, after which the  Schrödinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle, valid for Hamiltonians that are bounded from below.First-order perturbation theory also leads to matrix eigenvalue problem for degenerate states.
Dual basis
In linear algebra, given a vector space V with a basis B of vectors indexed by an index set I (the cardinality of I is the dimensionality of V), its dual set is a set B∗ of vectors in the dual space V∗ with the same index set I such that B and B∗ form a biorthogonal system. The dual set is always linearly independent but does not necessarily span V∗. If it does span V∗, then B∗ is called the dual basis or reciprocal basis for the basis B.The dual set always exists and gives an injection from V into V∗, namely the mapping that sends vi to vi. This says, in particular, that the dual space has dimension greater or equal to that of V.The dual of an infinite-dimensional space has greater dimensionality (this being a greater infinite cardinality) than the original space has, and thus these cannot have a basis with the same indexing set. However, a dual set of vectors exists, which defines a subspace of the dual isomorphic to the original space. Further, for topological vector spaces, a continuous dual space can be defined, in which case a dual basis may exist.In the case of finite-dimensional vector spaces, the dual set is always a dual basis and it is unique. These bases are denoted by B = { e1, …, en } and B∗ = { e1, …, en }. If one denotes the evaluation of a covector on a vector as a pairing, the biorthogonality condition becomes:The association of a dual basis with a basis gives a map from the space of bases of V to the space of bases of V∗, and this is also an isomorphism. For topological fields such as the real numbers, the space of duals is a topological space, and this gives a homeomorphism between the Stiefel manifolds of bases of these spaces.To perform operations with a vector, we must have a straightforward method of calculating its components. In a Cartesian frame the necessary operation is the dot product of the vector and the base vector.[1] E.g.,In a non-Cartesian frame, we do not necessarily have ei · ej = 0 for all i ≠ j. However, it is always possible to find a vector ei such thatthe equality holds when ei is the dual base of eiFor example, the standard basis vectors of R2 (the Cartesian plane) areand the standard basis vectors of its dual space R2* areIn 3-dimensional Euclidean space, for a given basis {e1, e2, e3}, you can find the biorthogonal (dual) basis {e1, e2, e3} by formulas below:where T denotes the transpose and
Homography
In projective geometry, a homography is an isomorphism of projective spaces, induced by an isomorphism of the vector spaces from which the projective spaces derive.[1] It is a bijection that maps lines to lines, and thus a collineation. In general, some collineations are not homographies, but the fundamental theorem of projective geometry asserts that is not so in the case of real projective spaces of dimension at least two. Synonyms include projectivity, projective transformation, and projective collineation.Historically, homographies (and projective spaces) have been introduced to study perspective and projections in Euclidean geometry, and the term homography, which, etymologically, roughly means "similar drawing" date from this time. At the end of the 19th century, formal definitions of projective spaces were introduced, which differed from extending Euclidean or affine spaces by adding points at infinity. The term "projective transformation" originated in these abstract constructions. These constructions divide into two classes that have been shown to be equivalent. A projective space may be constructed as the set of the lines of a vector space over a given field (the above definition is based on this version); this construction facilitates the definition of projective coordinates and allows using the tools of linear algebra for the study of homographies. The alternative approach consists in defining the projective space through a set of axioms, which do not involve explicitly any field (incidence geometry, see also synthetic geometry); in this context, collineations are easier to define than homographies, and homographies are defined as specific collineations, thus called "projective collineations".For sake of simplicity, unless otherwise stated, the projective spaces considered in this article are supposed to be defined over a (commutative) field. Equivalently Pappus's hexagon theorem and Desargues's theorem are supposed to be true. A large part of the results remain true, or may be generalized to projective geometries for which these theorems do not hold.Historically, the concept of homography had been introduced to understand, explain and study visual perspective, and, specifically, the difference in appearance of two plane objects viewed from different points of view.In the Euclidean space of dimension 3, a central projection from a point O (the center) onto a plane P that does not contain O is the mapping that sends a point A to the intersection (if it exists) of the line OA and the plane P. The projection is not defined if the point A belongs to the plane passing through O and parallel to P. The notion of projective space was originally introduced by extending the Euclidean space, that is, by adding points at infinity to it, in order to define the projection for every point except O.Given another plane Q, which does not contain O, the restriction to Q of the above projection is called a perspectivity.With these definitions, a perspectivity is only a partial function, but it becomes a bijection if extended to projective spaces. Therefore, this notion is normally defined for projective spaces. The notion is also easily generalized to projective spaces of any dimension, over any field, in the following way: Given two projective spaces P and Q of dimension n, a perspectivity is a bijection from P to Q that may be obtained by embedding P and Q in a projective space R of dimension n + 1 and restricting to P a central projection onto Q.If f is a perspectivity from P to Q, and g a perspectivity from Q to P, with a different center, then g ⋅ f is a homography from P to itself, which is called a central collineation, when the dimension of P is at least two. (see § Central collineation below and Perspectivity § Perspective collineations).Originally, a homography was defined as the composition of a finite number of perspectivities.[2] It is a part of the fundamental theorem of projective geometry (see below) that this definition coincides with the more algebraic definition sketched in the introduction and detailed below.When the projective spaces are defined by adding points at infinity to affine spaces (projective completion) the preceding formulas become, in affine coordinates,which generalizes the expression of the homographic function of the next section. This defines only a partial function between affine spaces, which is defined only outside the hyperplane where the denominator is zero.The projective line over a field K may be identified with the union of K and a point, called the "point at infinity" and denoted by ∞ (see projective line). With this representation of the projective line, the homographies are the mappingswhich are called homographic functions or linear fractional transformations.In the case of the complex projective line, which can be identified with the Riemann sphere, the homographies are called Möbius transformations.These correspond precisely with those bijections of the Riemann sphere that preserve orientation and are conformal.[3]In the study of collineations, the case of projective lines is special due to the small dimension. When the line is viewed as a projective space in isolation, any permutation of the points of a projective line is a collineation,[4] since every set of points are collinear. However, if the projective line is embedded in a higher-dimensional projective space, the geometric structure of that space can be used to impose a geometric structure on the line. Thus, in synthetic geometry, the homographies and the collineations of the projective line that are considered are those obtained by restrictions to the line of collineations and homographies of spaces of higher dimension. This means that the fundamental theorem of projective geometry (see below) remains valid in the one-dimensional setting. A homography of a projective line may also be properly defined by insisting that the mapping preserves cross-ratios.[5]A projective frame or projective basis of a projective space of dimension n is an ordered set of n + 2 points such no hyperplane contains n + 1 of them. A projective frame is sometimes called a simplex,[6] although a simplex in a space of dimension n has at most n + 1 vertices.It follows that, given two frames, there is exactly one homography mapping the first one onto the second one. In particular, the only homography fixing the points of a frame is the identity map. This result is much more difficult in synthetic geometry (where projective spaces are defined through axioms). It is sometimes called the first fundamental theorem of projective geometry.[7]One may also consider the projective space P(Kn+1). It has a canonical frame consisting of the image by p of the canonical basis of Kn+1 (consisting of the elements having only one nonzero entry, which is equal to 1), and (1, 1, ..., 1). On this basis, the homogeneous coordinates of p(v) are simply the entries (coefficients) of v. Given another projective space P(V) of the same dimension, and a frame F of it, there is one homography h mapping F onto the canonical frame of P(Kn+1). The projective coordinates of a point a on the frame F are the homogeneous coordinates of h(a) on the canonical frame of P(Kn+1).In above sections, homographies have been defined through linear algebra. In synthetic geometry, they are traditionally defined as the composition of one or several special homographies called central collineations. It is a part of the fundamental theorem of projective geometry that the two definitions are equivalent.In a projective space, P, of dimension n ≥ 2, a collineation of P is a bijection from P onto P that maps lines onto lines. A central collineation (traditionally these were called perspectivities,[8] but this term may be confusing, having another meaning; see Perspectivity) is a bijection α from P to P, such that there exists a hyperplane H (called the axis of α), which is fixed pointwise by α (that is, α(X) = X for all points X in H) and a point O (called the center of α), which is fixed linewise by α (any line through O is mapped to itself by α, but not necessarily pointwise).[9] There are two types of central collineations. Elations are the central collineations in which the center is incident with the axis and homologies are those in which the center is not incident with the axis. A central collineation is uniquely defined by its center, its axis, and the image α(P) of any given point P that differs from the center O and does not belong to the axis. (The image α(Q) of any other point Q is the intersection of the line defined by O and Q and the line passing through α(P) and the intersection with the axis of the line defined by P and Q.)A central collineation is a homography defined by a (n+1) × (n+1) matrix that has an eigenspace of dimension n. It is a homology, if the matrix has another eigenvalue and is therefore diagonalizable. It is an elation, if all the eigenvalues are equal and the matrix is not diagonalizable.The composition of two central collineations, while still a homography in general, is not a central collineation. In fact, every homography is the composition of a finite number of central collineations. In synthetic geometry, this property, which is a part of the fundamental theory of projective geometry is taken as the definition of homographies.[10]There are collineations besides the homographies. In particular, any field automorphism σ of a field F induces a collineation of every projective space over F by applying σ to all homogeneous coordinates (over a projective frame) of a point. These collineations are called automorphic collineations.The fundamental theorem of projective geometry consists of the three following theorems.If projective spaces are defined by means of axioms (synthetic geometry), the third part is simply a definition. On the other hand, if projective spaces are defined by means of linear algebra, the first part is an easy corollary of the definitions. Therefore, the proof of the first part in synthetic geometry, and the proof of the third part in terms of linear algebra both are fundamental steps of the proof of the equivalence of the two ways of defining projective spaces.As every homography has an inverse mapping and the composition of two homographies is another, the homographies of a given projective space form a group. For example, the Möbius group is the homography group of any complex projective line.As all the projective spaces of the same dimension over the same field are isomorphic, the same is true for their homography groups. They are therefore considered as a single group acting on several spaces, and only the dimension and the field appear in the notation, not the specific projective space.Homography groups also called projective linear groups are denoted PGL(n + 1, F) when acting on a projective space of dimension n over a field F. Above definition of homographies shows that PGL(n + 1, F) may be identified to the quotient group GL(n + 1, F) / F×I, where GL(n + 1, F) is the general linear group of the invertible matrices, and F×I is the group of the products by a nonzero element of F of the identity matrix of size (n + 1) × (n + 1).When F is a Galois field GF(q) then the homography group is written PGL(n, q). For example, PGL(2, 7) acts on the eight points in the projective line over the finite field GF(7), while PGL(2, 4), which is isomorphic to the alternating group A5, is the homography group of the projective line with five points.[12]The homography group PGL(n + 1, F) is a subgroup of the collineation group PΓL(n + 1, F) of the collineations of a projective space of dimension n. When the points and lines of the projective space are viewed as a block design, whose blocks are the sets of points contained in a line, it is common to call the collineation group the automorphism group of the design.The cross-ratio of four collinear points is an invariant under the homography that is fundamental for the study of the homographies of the lines.Three distinct points a, b and c on a projective line over a field F form a projective frame of this line. There is therefore a unique homography h of this line onto F ∪ ∞ that maps a to ∞, b to 0, and c to 1. Given a fourth point on the same line, the cross-ratio of the four points a, b, c and d, denoted [a, b; c, d], is the element h(d) of F ∪ ∞. In other words, if d has homogeneous coordinates [k : 1] over the projective frame (a, b, c), then [a, b; c, d] = k.[13]Suppose A is a ring and U is its group of units. Homographies act on a projective line over A, written P(A), consisting of points U(a, b) with homogeneous coordinates. The homographies on P(A) are described by matrix mappingsWhen A is a commutative ring, the homography may be writtenbut otherwise the linear fractional transformation is seen as an equivalence:Ring homographies have been used in quaternion analysis, and with dual quaternions to facilitate screw theory. When A is taken to be biquaternions the homographies exhibit conformal symmetry of an electromagnetic field. The homography group of the ring of integers Z is modular group PSL(2, Z).
Square-free polynomial
A square-free decomposition or square-free factorization of a polynomial is a factorization into powers of square-free factorswhere those of the ak that are not equal to 1 are pairwise coprime square-free polynomials.[1] Every non-zero polynomial with coefficients in a field admits a square-free factorization, which is unique up to the multiplication of the factors by non-zero constants. The square-free factorization is much easier to compute than the complete factorization into irreducible factors, and is thus often preferred when the complete factorization is not really needed, as for the partial fraction decomposition and the symbolic integration of rational fractions. Square-free factorization is the first step of the polynomial factorization algorithms which are implemented in computer algebra systems. Therefore, the algorithm of square-free factorization is basic in computer algebra.In the case of univariate polynomials over a field, any multiple factor of a polynomial introduces a nontrivial common factor of f and its formal derivative f ′, so a sufficient condition for f to be square-free is that the greatest common divisor of f and f ′ is 1. This condition is also necessary over a field of characteristic 0 or, more generally, over a perfect field, because over such a field, every irreducible polynomial is separable, and thus coprime with its derivative. There are also known algorithms for the computation of the square-free decomposition of multivariate polynomials.[2]This section describes Yun's algorithm for the square-free decomposition of univariate polynomials over a field of characteristic 0.[1] It proceeds by a succession of GCD computations and exact divisions.The input is thus a non-zero polynomial f, and the first step of the algorithm consists of computing the GCD a0 of f and its formal derivative f'. Ifis the desired factorization, we have thus  and andThis is formalized into an algorithm as follows:In general, a polynomial has no square root. More precisely, most polynomials cannot be written as the square of another polynomial.A polynomial has a square root if and only if all exponents of the square-free decomposition are even. In this case, the square root is obtained by dividing by 2 these exponents.Thus the problem of deciding if a polynomial has a square root, and of computing it if it exists, is a special case of square-free factorization. 
Function composition
In mathematics, function composition is the pointwise application of one function to the result of another to produce a third function. For instance, the functions f : X → Y and g : Y → Z can be composed to yield a function which maps x in X to g(f(x)) in Z.  Intuitively, if z is a function of y, and y is a function of x, then z is a function of x. The resulting composite function is denoted g ∘ f : X → Z, defined by (g ∘ f )(x) = g(f(x)) for all x in X.[note 1]The notation g ∘ f is read as "g circle f ", "g round f ", "g about f ", "g composed with f ", "g after f ", "g following f ", "g of f", or "g on f ". Intuitively, composing two functions is a chaining process in which the output of the inner function becomes the input of the outer function.The composition of functions is a special case of the composition of relations, so all properties of the latter are true of composition of functions.[1] The composition of functions has some additional properties.The composition of functions is always associative—a property inherited from the composition of relations.[1] That is, if f, g, and h are three functions with suitably chosen domains and codomains, then f ∘ (g ∘ h) = (f ∘ g) ∘ h, where the parentheses serve to indicate that composition is to be performed first for the parenthesized functions. Since there is no distinction between the choices of placement of parentheses, they may be left off without causing any ambiguity.In a strict sense, the composition g ∘ f can be built only if f's codomain equals g's domain; in a wider sense it is sufficient that the former is a subset of the latter.[note 2]Moreover, it is often convenient to tacitly restrict f's domain such that f produces only values in g's domain; for example, the composition g ∘ f of the functions f : ℝ → (−∞,+9]  defined by f(x) = 9 − x2 and g : [0,+∞) → ℝ defined by g(x) = √x can be defined on the interval [−3,+3].The functions g and f are said to commute with each other if g ∘ f = f ∘ g. Commutativity is a special property, attained only by particular functions, and often in special circumstances. For example, |x| + 3 = |x + 3| only when x ≥ 0. The picture shows another example.The composition of one-to-one functions is always one-to-one. Similarly, the composition of two onto functions is always onto. It follows that composition of two bijections is also a bijection. The inverse function of a composition (assumed invertible) has the property that (f ∘ g)−1 = ( g−1 ∘ f −1).[2]Derivatives of compositions involving differentiable functions can be found using the chain rule. Higher derivatives of such functions are given by Faà di Bruno's formula.Suppose one has two (or more) functions f: X → X, g: X → X having the same domain and codomain; these are often called transformations. Then one can form chains of transformations composed together, such as f ∘ f ∘ g ∘ f. Such chains have the algebraic structure of a monoid, called a transformation monoid or (much more seldom) composition monoid.  In general, transformation monoids can have remarkably complicated structure. One particular notable example is the de Rham curve. The set of all functions f: X → X is called the full transformation semigroup[3] or symmetric semigroup[4] on X. (One can actually define two semigroups depending how one defines the semigroup operation as the left or right composition of functions.[5])If the transformation are bijective (and thus invertible), then the set of all  possible combinations of these functions forms a transformation group; and one says that the group is generated by these functions.  A fundamental result in group theory, Cayley's theorem, essentially says that any group is in fact just a subgroup of a permutation group (up to isomorphism).[6]The set of all bijective functions f: X → X (called permutations) forms a group with respect to the composition operator. This is the symmetric group, also sometimes called the composition group.In the symmetric semigroup (of all transformations) one also finds a weaker, non-unique notion of inverse (called a pseudoinverse) because the symmetric semigroup is a regular semigroup.[7]If Y ⊆ X, then f: X→Y may compose with itself; this is sometimes denoted as f 2. That is:More generally, for any natural number n ≥ 2, the nth functional power can be defined inductively by f n = f ∘ f n−1 = f n−1 ∘ f. Repeated composition of such a function with itself is called iterated function.Note: If f takes its values in a ring (in particular for real or complex-valued f ), there is a risk of confusion, as f n could also stand for the n-fold product of f, e.g. f 2(x) = f(x) · f(x). For trigonometric functions, usually the latter is meant, at least for positive exponents. For example, in trigonometry, this superscript notation represents standard exponentiation when used with trigonometric functions:sin2(x) = sin(x) · sin(x).However, for negative exponents (especially −1), it nevertheless usually refers to the inverse function, e.g., tan−1 = arctan ≠ 1/tan.In some cases, when, for a given function f, the equation g ∘ g = f has a unique solution g, that function can be defined as the functional square root of f, then written as g = f 1/2.More generally, when gn = f has a unique solution for some natural number n > 0, then f m/n can be defined as gm.Under additional restrictions, this idea can be generalized so that the iteration count  becomes a continuous parameter; in this case, such a system is called a flow, specified through solutions of Schröder's equation. Iterated functions and flows occur naturally in the study of fractals and dynamical systems.To avoid ambiguity, some mathematicians choose to write  f °n for the n-th iterate  of the function f.Many mathematicians, particularly in group theory, omit the composition symbol, writing gf for g ∘ f.[8]In the mid-20th century, some mathematicians decided that writing "g ∘ f " to mean "first apply f, then apply g" was too confusing and decided to change notations. They write "xf " for "f(x)" and "(xf)g" for "g(f(x))".[9] This can be more natural and seem simpler than writing functions on the left in some areas – in linear algebra, for instance, when x is a row vector and f and g denote matrices and the composition is by matrix multiplication. This alternative notation is called postfix notation. The order is important because function composition is not necessarily commutative (e.g matrix multiplication). Successive transformations applying and composing to the right agrees with the left-to-right reading sequence.Mathematicians who use postfix notation may write "fg", meaning first apply f and then apply g, in keeping with the order the symbols occur in postfix notation, thus making the notation "fg" ambiguous.  Computer scientists may write "f ; g" for this,[10] thereby disambiguating the order of composition. To distinguish the left composition operator from a text semicolon, in the Z notation the ⨾  character is used for left relation composition.[11] Since all functions are  binary relations, it is correct to use the [fat] semicolon for function composition as well (see the article on composition of relations for further details on this notation).Given a function g, the composition operator Cg is defined as that operator which maps functions to functions asComposition operators are studied in the field of operator theory.Function composition appears in one form or another in numerous programming languages.Partial composition is possible for multivariate functions. The function resulting when some argument xi of the function f is replaced by the function g is called a composition of f and g in some computer engineering contexts, and is denoted f |xi = gWhen g is a simple constant b, composition degenerates into a (partial) valuation, whose result is also known as restriction or co-factor.[12]In general, the composition of multivariate functions may involve several other functions as arguments, as in the definition of primitive recursive function. Given f, a n-ary function, and n m-ary functions g1, ..., gn, the composition of f with g1, ..., gn, is the m-ary functionThis is sometimes called the generalized composite of f with g1, ..., gn.[13] The partial composition in only one argument mentioned previously can be instantiated from this more general scheme by setting all argument functions except one to be suitably chosen projection functions. Note also that g1, ..., gn can be seen as a single vector/tuple-valued function in this generalized scheme, in which case this is precisely the standard definition of function composition.[14]A set of finitary operations on some base set X is called a clone if it contains all projections and is closed under generalized composition. Note that a clone generally contains operations of various arities.[13] The notion of commutation also finds an interesting generalization in the multivariate case; a function f of arity n is said to commute with a function g of arity m if f is a homomorphism preserving g, and vice versa i.e.:[15]A unary operation always commutes with itself, but this is not necessarily the case for a binary (or higher arity) operation. A binary (or higher arity) operation that commutes with itself is called medial or entropic.[15]Composition can be generalized to arbitrary binary relations.If R ⊆ X × Y and S ⊆ Y × Z are two binary relations, then their composition S∘R is the relation defined as {(x, z) ∈ X × Z : ∃y ∈ Y. (x, y) ∈ R ∧ (y, z)  ∈ S}.Considering a function as a special case of a binary relation (namely functional relations), function composition satisfies the definition for relation composition.The composition is defined in the same way for partial functions and Cayley's theorem has its analogue called Wagner-Preston theorem.[16]The category of sets with functions as morphisms is the prototypical category. The axioms of a category are in fact inspired from the properties (and also the definition) of function composition.[17] The structures given by composition are axiomatized and generalized in category theory with the concept of morphism as the category-theoretical replacement of functions. The reversed order of composition in the formula (f ∘ g)−1 = (g−1 ∘ f −1) applies for composition of relations using converse relations, and thus in group theory. These structures form dagger categories.The composition symbol ∘  is encoded as .mw-parser-output .monospaced{font-family:monospace,monospace}U+2218 ∘ .mw-parser-output .smallcaps{font-variant:small-caps}RING OPERATOR (HTML &#8728;); see the Degree symbol article for similar-appearing Unicode characters. In TeX, it is written \circ.
James Joseph Sylvester
James Joseph Sylvester FRS HFRSE LLD (3 September 1814 – 15 March 1897) was an English mathematician. He made fundamental contributions to matrix theory, invariant theory, number theory, partition theory, and combinatorics.  He played a leadership role in American mathematics in the later half of the 19th century as a professor at the Johns Hopkins University and as founder of the American Journal of Mathematics.  At his death, he was professor at Oxford.James Joseph was born in London on 3 September 1814, the son of Abraham Joseph, a merchant.[1]  James later adopted the surname Sylvester when his older brother did so upon emigration to the United States—a country which at that time required all immigrants to have a given name, a middle name, and a surname. At the age of 14, Sylvester was a student of Augustus De Morgan at the University of London. His family withdrew him from the University after he was accused of stabbing a fellow student with a knife. Subsequently, he attended the Liverpool Royal Institution.Sylvester began his study of mathematics at St John's College, Cambridge in 1831,[2] where his tutor was John Hymers. Although his studies were interrupted for almost two years due to a prolonged illness, he nevertheless ranked second in Cambridge's famous mathematical examination, the tripos, for which he sat in 1837. However, Sylvester was not issued a degree, because graduates at that time were required to state their acceptance of the Thirty-Nine Articles of the Church of England, and Sylvester could not do so because he was Jewish. For the same reason, he was unable to compete for a Fellowship or obtain a Smith's prize.[3] In 1838, Sylvester became professor of natural philosophy at University College London and in 1839 a Fellow of the Royal Society of London. In 1841, he was awarded a BA and an MA by Trinity College, Dublin. In the same year he moved to the United States to become a professor of mathematics at the University of Virginia, but left after less than four months following a violent encounter with two students he had disciplined. He moved to New York City and began friendships with the Harvard mathematician Benjamin Peirce (father of Charles Sanders Peirce) and the Princeton physicist Joseph Henry.  However, he left in November 1843 after being denied appointment as Professor of Mathematics at Columbia College (now University), again for his Judaism, and returned to England.On his return to England, he was hired in 1844 by the Equity and Law Life Assurance Society for which he developed successful actuarial models and served as de facto CEO, a position that required a law degree. As a result, he studied for the Bar, meeting a fellow British mathematician studying law, Arthur Cayley, with whom he made significant contributions to invariant theory and also matrix theory during a long collaboration.[4][incomplete short citation]  He did not obtain a position teaching university mathematics until 1855, when he was appointed professor of mathematics at the Royal Military Academy, Woolwich, from which he retired in 1869, because the compulsory retirement age was 55. The Woolwich academy initially refused to pay Sylvester his full pension, and only relented after a prolonged public controversy, during which Sylvester took his case to the letters page of The Times.One of Sylvester's lifelong passions was for poetry; he read and translated works from the original French, German, Italian, Latin and Greek, and many of his mathematical papers contain illustrative quotes from classical poetry. Following his early retirement, Sylvester (1870) published a book entitled The Laws of Verse in which he attempted to codify a set of laws for prosody in poetry.In 1872, he finally received his B.A. and M.A. from Cambridge, having been denied the degrees due to his being a Jew.[2]In 1876[5] Sylvester again crossed the Atlantic Ocean to become the inaugural professor of mathematics at the new Johns Hopkins University in Baltimore, Maryland. His salary was $5,000 (quite generous for the time), which he demanded be paid in gold.  After negotiation, agreement was reached on a salary that was not paid in gold.[6]  In 1878 he founded the American Journal of Mathematics.  The only other mathematical journal in the US at that time was the Analyst, which eventually became the Annals of Mathematics.In 1883, he returned to England to take up the Savilian Professor of Geometry at Oxford University. He held this chair until his death, although in 1892 the University appointed a deputy professor to the same chair.Sylvester died in London on 15 March 1897. He is buried in Balls Pond Road Jewish Cemetery on Kingsbury Road in London.[7]Sylvester invented a great number of mathematical terms such as "matrix" (in 1850),[8] "graph" (combinatorics)[9] and "discriminant".[10]  He coined the term "totient" for Euler's totient function φ(n).[11]  His collected scientific work fills four volumes. In 1880, the Royal Society of London awarded Sylvester the Copley Medal, its highest award for scientific achievement; in 1901, it instituted the Sylvester Medal in his memory, to encourage mathematical research after his death in Oxford. In Discrete geometry he is remembered for Sylvester's Problem and a result on the orchard problem.Sylvester House, a portion of an undergraduate dormitory at Johns Hopkins University, is named in his honor. Several professorships there are named in his honor also.
Complex conjugate
In mathematics, the complex conjugate of a complex number is the number with an equal real part and an imaginary part equal in magnitude but opposite in sign.[1][2] For example, the complex conjugate of 3 + 4i is 3 − 4i.Complex conjugates are important for finding roots of polynomials. According to the complex conjugate root theorem, if a complex number is a root to a polynomial in one variable with real coefficients (such as the quadratic equation or the cubic equation), so is its conjugate.The following properties apply for all complex numbers z and w, unless stated otherwise, and can be proved by writing z and w in the form a + ib.A significant property of the complex conjugate is that a complex number is equal to its complex conjugate if its imaginary part is zero, that is, if the complex number is real.For any two complex numbers w,z:The penultimate relation is involution; i.e., the conjugate of the conjugate of a complex number z is z. The ultimate relation is the method of choice to compute the inverse of a complex number if it is given in rectangular coordinates.These uses of the conjugate of z as a variable are illustrated in Frank Morley's book Inversive Geometry (1933), written with his son Frank Vigor Morley.The other planar real algebras, dual numbers, and split-complex numbers are also analyzed using complex conjugation.Taking the conjugate transpose (or adjoint) of complex matrices generalizes complex conjugation. Even more general is the concept of adjoint operator for operators on (possibly infinite-dimensional) complex Hilbert spaces. All this is subsumed by the *-operations of C*-algebras.Note that all these generalizations are multiplicative only if the factors are reversed:Since the multiplication of planar real algebras is commutative, this reversal is not needed there.One example of this notion is the conjugate transpose operation of complex matrices defined above. It should be remarked that on generic complex vector spaces there is no canonical notion of complex conjugation.
Hilbert space
The mathematical concept of a Hilbert space, named after David Hilbert, generalizes the notion of Euclidean space. It extends the methods of vector algebra and calculus from the two-dimensional Euclidean plane and three-dimensional space to spaces with any finite or infinite number of dimensions. A Hilbert space is an abstract vector space possessing the structure of an inner product that allows length and angle to be measured. Furthermore, Hilbert spaces are complete: there are enough limits in the space to allow the techniques of calculus to be used.Hilbert spaces arise naturally and frequently in mathematics and physics, typically as infinite-dimensional function spaces. The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by David Hilbert, Erhard Schmidt, and Frigyes Riesz. They are indispensable tools in the theories of partial differential equations, quantum mechanics, Fourier analysis (which includes applications to signal processing and heat transfer), and ergodic theory (which forms the mathematical underpinning of thermodynamics). John von Neumann coined the term Hilbert space for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for functional analysis. Apart from the classical Euclidean spaces, examples of Hilbert spaces include spaces of square-integrable functions, spaces of sequences, Sobolev spaces consisting of generalized functions, and Hardy spaces of holomorphic functions.Geometric intuition plays an important role in many aspects of Hilbert space theory. Exact analogs of the Pythagorean theorem and parallelogram law hold in a Hilbert space. At a deeper level, perpendicular projection onto a subspace (the analog of "dropping the altitude" of a triangle) plays a significant role in optimization problems and other aspects of the theory. An element of a Hilbert space can be uniquely specified by its coordinates with respect to a set of coordinate axes (an orthonormal basis), in analogy with Cartesian coordinates in the plane. When that set of axes is countably infinite, the Hilbert space can also be usefully thought of in terms of the space of infinite sequences that are square-summable. The latter space is often in the older literature referred to as the Hilbert space. Linear operators on a Hilbert space are likewise fairly concrete objects: in good cases, they are simply transformations that stretch the space by different factors in mutually perpendicular directions in a sense that is made precise by the study of their spectrum.One of the most familiar examples of a Hilbert space is the Euclidean space consisting of three-dimensional vectors, denoted by ℝ3, and equipped with the dot product. The dot product takes two vectors x and y, and produces a real number x · y. If x and y are represented in Cartesian coordinates, then the dot product is defined byThe dot product satisfies the properties:An operation on pairs of vectors that, like the dot product, satisfies these three properties is known as a (real) inner product. A vector space equipped with such an inner product is known as a (real) inner product space. Every finite-dimensional inner product space is also a Hilbert space. The basic feature of the dot product that connects it with Euclidean geometry is that it is related to both the length (or norm) of a vector, denoted ||x||, and to the angle θ between two vectors x and y by means of the formulaMultivariable calculus in Euclidean space relies on the ability to compute limits, and to have useful criteria for concluding that limits exist. A mathematical seriesconsisting of vectors in ℝ3 is absolutely convergent provided that the sum of the lengths converges as an ordinary series of real numbers:[1]Just as with a series of scalars, a series of vectors that converges absolutely also converges to some limit vector L in the Euclidean space, in the sense thatThis property expresses the completeness of Euclidean space: that a series that converges absolutely also converges in the ordinary sense.Hilbert spaces are often taken over the complex numbers. The complex plane denoted by ℂ is equipped with a notion of magnitude, the complex modulus |z| which is defined as the square root of the product of z with its complex conjugate:If z = x + iy is a decomposition of z into its real and imaginary parts, then the modulus is the usual Euclidean two-dimensional length:The inner product of a pair of complex numbers z and w is the product of z with the complex conjugate of w:This is complex-valued. The real part of ⟨z,w⟩ gives the usual two-dimensional Euclidean dot product.A second example is the space ℂ2 whose elements are pairs of complex numbers z = (z1, z2). Then the inner product of z with another such vector w = (w1,w2) is given byThe real part of ⟨z,w⟩ is then the four-dimensional Euclidean dot product. This inner product is Hermitian symmetric, which means that the result of interchanging z and w is the complex conjugate:A Hilbert space H is a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product.[2]To say that H is a complex inner product space means that H is a complex vector space on which there is an inner product ⟨x,y⟩ associating a complex number to each pair of elements x, y of H that satisfies the following properties:It follows from properties 1 and 2 that a complex inner product is antilinear in its second argument, meaning thatA real inner product space is defined in the same way, except that H is a real vector space and the inner product takes real values. Such an inner product will be bilinear: that is, linear in each argument.The norm is the real-valued functionand the distance d between two points x, y in H is defined in terms of the norm byThat this function is a distance function means firstly that it is symmetric in x and y, secondly that the distance between x and itself is zero, and otherwise the distance between x and y must be positive, and lastly that the triangle inequality holds, meaning that the length of one leg of a triangle xyz cannot exceed the sum of the lengths of the other two legs:This last property is ultimately a consequence of the more fundamental Cauchy–Schwarz inequality, which assertswith equality if and only if x and y are linearly dependent.With a distance function defined in this way, any inner product space is a metric space, and sometimes is known as a pre-Hilbert space.[3] Any pre-Hilbert space that is additionally also a complete space is a Hilbert space.The Completeness of H is expressed using a form of the Cauchy criterion for sequences in H: a pre-Hilbert space H is complete if every Cauchy sequence converges with respect to this norm to an element in the space. Completeness can be characterized by the following equivalent condition: if a series of vectorsconverges absolutely in the sense thatthen the series converges in H, in the sense that the partial sums converge to an element of H.As a complete normed space, Hilbert spaces are by definition also Banach spaces. As such they are topological vector spaces, in which topological notions like the openness and closedness of subsets are well defined. Of special importance is the notion of a closed linear subspace of a Hilbert space that, with the inner product induced by restriction, is also complete (being a closed set in a complete metric space) and therefore a Hilbert space in its own right.The sequence space l2 consists of all infinite sequences z = (z1,z2,...) of complex numbers such that the seriesconverges. The inner product on l2 is defined bywith the latter series converging as a consequence of the Cauchy–Schwarz inequality.Completeness of the space holds provided that whenever a series of elements from l2 converges absolutely (in norm), then it converges to an element of l2. The proof is basic in mathematical analysis, and permits mathematical series of elements of the space to be manipulated with the same ease as series of complex numbers (or vectors in a finite-dimensional Euclidean space).[4]Prior to the development of Hilbert spaces, other generalizations of Euclidean spaces were known to mathematicians and physicists. In particular, the idea of an abstract linear space had gained some traction towards the end of the 19th century:[5] this is a space whose elements can be added together and multiplied by scalars (such as real or complex numbers) without necessarily identifying these elements with "geometric" vectors, such as position and momentum vectors in physical systems. Other objects studied by mathematicians at the turn of the 20th century, in particular spaces of sequences (including series) and spaces of functions,[6] can naturally be thought of as linear spaces. Functions, for instance, can be added together or multiplied by constant scalars, and these operations obey the algebraic laws satisfied by addition and scalar multiplication of spatial vectors.In the first decade of the 20th century, parallel developments led to the introduction of Hilbert spaces. The first of these was the observation, which arose during David Hilbert and Erhard Schmidt's study of integral equations,[7] that two square-integrable real-valued functions f and g on an interval [a,b] have an inner productwhich has many of the familiar properties of the Euclidean dot product. In particular, the idea of an orthogonal family of functions has meaning. Schmidt exploited the similarity of this inner product with the usual dot product to prove an analog of the spectral decomposition for an operator of the formwhere K is a continuous function symmetric in x and y. The resulting eigenfunction expansion expresses the function K as a series of the formwhere the functions φn are orthogonal in the sense that ⟨φn,φm⟩ = 0 for all n ≠ m. The individual terms in this series are sometimes referred to as elementary product solutions. However, there are eigenfunction expansions that fail to converge in a suitable sense to a square-integrable function: the missing ingredient, which ensures convergence, is completeness.[8]The second development was the Lebesgue integral, an alternative to the Riemann integral introduced by Henri Lebesgue in 1904.[9] The Lebesgue integral made it possible to integrate a much broader class of functions. In 1907, Frigyes Riesz and Ernst Sigismund Fischer independently proved that the space L2 of square Lebesgue-integrable functions is a complete metric space.[10] As a consequence of the interplay between geometry and completeness, the 19th century results of Joseph Fourier, Friedrich Bessel and Marc-Antoine Parseval on trigonometric series easily carried over to these more general spaces, resulting in a geometrical and analytical apparatus now usually known as the Riesz–Fischer theorem.[11]Further basic results were proved in the early 20th century. For example, the Riesz representation theorem was independently established by Maurice Fréchet and Frigyes Riesz in 1907.[12] John von Neumann coined the term abstract Hilbert space in his work on unbounded Hermitian operators.[13] Although other mathematicians such as Hermann Weyl and Norbert Wiener had already studied particular Hilbert spaces in great detail, often from a physically motivated point of view, von Neumann gave the first complete and axiomatic treatment of them.[14] Von Neumann later used them in his seminal work on the foundations of quantum mechanics,[15] and in his continued work with Eugene Wigner. The name "Hilbert space" was soon adopted by others, for example by Hermann Weyl in his book on quantum mechanics and the theory of groups.[16]The significance of the concept of a Hilbert space was underlined with the realization that it offers one of the best mathematical formulations of quantum mechanics.[17] In short, the states of a quantum mechanical system are vectors in a certain Hilbert space, the observables are hermitian operators on that space, the symmetries of the system are unitary operators, and measurements are orthogonal projections. The relation between quantum mechanical symmetries and unitary operators provided an impetus for the development of the unitary representation theory of groups, initiated in the 1928 work of Hermann Weyl.[16] On the other hand, in the early 1930s it became clear that classical mechanics can be described in terms of Hilbert space (Koopman–von Neumann classical mechanics) and that certain properties of classical dynamical systems can be analyzed using Hilbert space techniques in the framework of ergodic theory.[18]The algebra of observables in quantum mechanics is naturally an algebra of operators defined on a Hilbert space, according to Werner Heisenberg's matrix mechanics formulation of quantum theory. Von Neumann began investigating operator algebras in the 1930s, as rings of operators on a Hilbert space. The kind of algebras studied by von Neumann and his contemporaries are now known as von Neumann algebras. In the 1940s, Israel Gelfand, Mark Naimark and Irving Segal gave a definition of a kind of operator algebras called C*-algebras that on the one hand made no reference to an underlying Hilbert space, and on the other extrapolated many of the useful features of the operator algebras that had previously been studied. The spectral theorem for self-adjoint operators in particular that underlies much of the existing Hilbert space theory was generalized to C*-algebras. These techniques are now basic in abstract harmonic analysis and representation theory.Lebesgue spaces are function spaces associated to measure spaces (X, M, μ), where X is a set, M is a σ-algebra of subsets of X, and μ is a countably additive measure on M. Let L2(X, μ) be the space of those complex-valued measurable functions on X for which the Lebesgue integral of the square of the absolute value of the function is finite, i.e., for a function f in L2(X, μ),and where functions are identified if and only if they differ only on a set of measure zero.The inner product of functions f and g in L2(X, μ) is then defined asFor f and g in L2, this integral exists because of the Cauchy–Schwarz inequality, and defines an inner product on the space. Equipped with this inner product, L2 is in fact complete.[19] The Lebesgue integral is essential to ensure completeness: on domains of real numbers, for instance, not enough functions are Riemann integrable.[20]The Lebesgue spaces appear in many natural settings. The spaces L2(ℝ) and L2([0,1]) of square-integrable functions with respect to the Lebesgue measure on the real line and unit interval, respectively, are natural domains on which to define the Fourier transform and Fourier series. In other situations, the measure may be something other than the ordinary Lebesgue measure on the real line. For instance, if w is any positive measurable function, the space of all measurable functions f on the interval [0,1] satisfyingis called the weighted L2 space L2w([0,1]), and w is called the weight function. The inner product is defined byThe weighted space L2w([0,1]) is identical with the Hilbert space L2([0,1],μ) where the measure μ of a Lebesgue-measurable set A is defined byWeighted L2 spaces like this are frequently used to study orthogonal polynomials, because different families of orthogonal polynomials are orthogonal with respect to different weighting functions.Sobolev spaces, denoted by Hs or Ws,2, are Hilbert spaces. These are a special kind of function space in which differentiation may be performed, but that (unlike other Banach spaces such as the Hölder spaces) support the structure of an inner product. Because differentiation is permitted, Sobolev spaces are a convenient setting for the theory of partial differential equations.[21] They also form the basis of the theory of direct methods in the calculus of variations.[22]For s a non-negative integer and Ω ⊂ ℝn, the Sobolev space Hs(Ω) contains L2 functions whose weak derivatives of order up to s are also L2. The inner product in Hs(Ω) iswhere the dot indicates the dot product in the Euclidean space of partial derivatives of each order. Sobolev spaces can also be defined when s is not an integer.Sobolev spaces are also studied from the point of view of spectral theory, relying more specifically on the Hilbert space structure. If Ω is a suitable domain, then one can define the Sobolev space Hs(Ω) as the space of Bessel potentials;[23] roughly,Here Δ is the Laplacian and (1 − Δ)−s/2 is understood in terms of the spectral mapping theorem. Apart from providing a workable definition of Sobolev spaces for non-integer s, this definition also has particularly desirable properties under the Fourier transform that make it ideal for the study of pseudodifferential operators. Using these methods on a compact Riemannian manifold, one can obtain for instance the Hodge decomposition, which is the basis of Hodge theory.[24]The Hardy spaces are function spaces, arising in complex analysis and harmonic analysis, whose elements are certain holomorphic functions in a complex domain.[25] Let U denote the unit disc in the complex plane. Then the Hardy space H2(U) is defined as the space of holomorphic functions f on U such that the meansremain bounded for r < 1. The norm on this Hardy space is defined byHardy spaces in the disc are related to Fourier series. A function f is in H2(U) if and only ifwhereThus H2(U) consists of those functions that are L2 on the circle, and whose negative frequency Fourier coefficients vanish.The Bergman spaces are another family of Hilbert spaces of holomorphic functions.[26] Let D be a bounded open set in the complex plane (or a higher-dimensional complex space) and let L2,h(D) be the space of holomorphic functions f in D that are also in L2(D) in the sense thatwhere the integral is taken with respect to the Lebesgue measure in D. Clearly L2,h(D) is a subspace of L2(D); in fact, it is a closed subspace, and so a Hilbert space in its own right. This is a consequence of the estimate, valid on compact subsets K of D, thatwhich in turn follows from Cauchy's integral formula. Thus convergence of a sequence of holomorphic functions in L2(D) implies also compact convergence, and so the limit function is also holomorphic. Another consequence of this inequality is that the linear functional that evaluates a function f at a point of D is actually continuous on L2,h(D). The Riesz representation theorem implies that the evaluation functional can be represented as an element of L2,h(D). Thus, for every z ∈ D, there is a function ηz ∈ L2,h(D) such thatfor all f ∈ L2,h(D). The integrandis known as the Bergman kernel of D. This integral kernel satisfies a reproducing propertyA Bergman space is an example of a reproducing kernel Hilbert space, which is a Hilbert space of functions along with a kernel K(ζ,z) that verifies a reproducing property analogous to this one. The Hardy space H2(D) also admits a reproducing kernel, known as the Szegő kernel.[27] Reproducing kernels are common in other areas of mathematics as well. For instance, in harmonic analysis the Poisson kernel is a reproducing kernel for the Hilbert space of square-integrable harmonic functions in the unit ball. That the latter is a Hilbert space at all is a consequence of the mean value theorem for harmonic functions.Many of the applications of Hilbert spaces exploit the fact that Hilbert spaces support generalizations of simple geometric concepts like projection and change of basis from their usual finite dimensional setting. In particular, the spectral theory of continuous self-adjoint linear operators on a Hilbert space generalizes the usual spectral decomposition of a matrix, and this often plays a major role in applications of the theory to other areas of mathematics and physics.In the theory of ordinary differential equations, spectral methods on a suitable Hilbert space are used to study the behavior of eigenvalues and eigenfunctions of differential equations. For example, the Sturm–Liouville problem arises in the study of the harmonics of waves in a violin string or a drum, and is a central problem in ordinary differential equations.[28] The problem is a differential equation of the formfor an unknown function y on an interval [a,b], satisfying general homogeneous Robin boundary conditionsThe functions p, q, and w are given in advance, and the problem is to find the function y and constants λ for which the equation has a solution. The problem only has solutions for certain values of λ, called eigenvalues of the system, and this is a consequence of the spectral theorem for compact operators applied to the integral operator defined by the Green's function for the system. Furthermore, another consequence of this general result is that the eigenvalues λ of the system can be arranged in an increasing sequence tending to infinity.[nb 2]Hilbert spaces form a basic tool in the study of partial differential equations.[21] For many classes of partial differential equations, such as linear elliptic equations, it is possible to consider a generalized solution (known as a weak solution) by enlarging the class of functions. Many weak formulations involve the class of Sobolev functions, which is a Hilbert space. A suitable weak formulation reduces to a geometrical problem the analytic problem of finding a solution or, often what is more important, showing that a solution exists and is unique for given boundary data. For linear elliptic equations, one geometrical result that ensures unique solvability for a large class of problems is the Lax–Milgram theorem. This strategy forms the rudiment of the Galerkin method (a finite element method) for numerical solution of partial differential equations.[29]A typical example is the Poisson equation −Δu = g with Dirichlet boundary conditions in a bounded domain Ω in ℝ2. The weak formulation consists of finding a function u such that, for all continuously differentiable functions v in Ω vanishing on the boundary:This can be recast in terms of the Hilbert space H10(Ω) consisting of functions u such that u, along with its weak partial derivatives, are square integrable on Ω, and vanish on the boundary. The question then reduces to finding u in this space such that for all v in this spacewhere a is a continuous bilinear form, and b is a continuous linear functional, given respectively bySince the Poisson equation is elliptic, it follows from Poincaré's inequality that the bilinear form a is coercive. The Lax–Milgram theorem then ensures the existence and uniqueness of solutions of this equation.Hilbert spaces allow for many elliptic partial differential equations to be formulated in a similar way, and the Lax–Milgram theorem is then a basic tool in their analysis. With suitable modifications, similar techniques can be applied to parabolic partial differential equations and certain hyperbolic partial differential equations.The field of ergodic theory is the study of the long-term behavior of chaotic dynamical systems. The protypical case of a field that ergodic theory applies to is thermodynamics, in which—though the microscopic state of a system is extremely complicated (it is impossible to understand the ensemble of individual collisions between particles of matter)—the average behavior over sufficiently long time intervals is tractable. The laws of thermodynamics are assertions about such average behavior. In particular, one formulation of the zeroth law of thermodynamics asserts that over sufficiently long timescales, the only functionally independent measurement that one can make of a thermodynamic system in equilibrium is its total energy, in the form of temperature.An ergodic dynamical system is one for which, apart from the energy—measured by the Hamiltonian—there are no other functionally independent conserved quantities on the phase space. More explicitly, suppose that the energy E is fixed, and let ΩE be the subset of the phase space consisting of all states of energy E (an energy surface), and let Tt denote the evolution operator on the phase space. The dynamical system is ergodic if there are no continuous non-constant functions on ΩE such thatfor all w on ΩE and all time t. Liouville's theorem implies that there exists a measure μ on the energy surface that is invariant under the time translation. As a result, time translation is a unitary transformation of the Hilbert space L2(ΩE,μ) consisting of square-integrable functions on the energy surface ΩE with respect to the inner productThe von Neumann mean ergodic theorem[18] states the following:For an ergodic system, the fixed set of the time evolution consists only of the constant functions, so the ergodic theorem implies the following:[30] for any function f ∈ L2(ΩE,μ),That is, the long time average of an observable f is equal to its expectation value over an energy surface.One of the basic goals of Fourier analysis is to decompose a function into a (possibly infinite) linear combination of given basis functions: the associated Fourier series. The classical Fourier series associated to a function f defined on the interval [0,1] is a series of the formwhereThe example of adding up the first few terms in a Fourier series for a sawtooth function is shown in the figure. The basis functions are sine waves with wavelengths λ/n (for integer n) shorter than the wavelength λ of the sawtooth itself (except for n = 1, the fundamental wave). All basis functions have nodes at the nodes of the sawtooth, but all but the fundamental have additional nodes. The oscillation of the summed terms about the sawtooth is called the Gibbs phenomenon.A significant problem in classical Fourier series asks in what sense the Fourier series converges, if at all, to the function f. Hilbert space methods provide one possible answer to this question.[31] The functions en(θ) = e2πinθ form an orthogonal basis of the Hilbert space L2([0,1]). Consequently, any square-integrable function can be expressed as a seriesand, moreover, this series converges in the Hilbert space sense (that is, in the L2 mean).The problem can also be studied from the abstract point of view: every Hilbert space has an orthonormal basis, and every element of the Hilbert space can be written in a unique way as a sum of multiples of these basis elements. The coefficients appearing on these basis elements are sometimes known abstractly as the Fourier coefficients of the element of the space.[32] The abstraction is especially useful when it is more natural to use different basis functions for a space such as L2([0,1]). In many circumstances, it is desirable not to decompose a function into trigonometric functions, but rather into orthogonal polynomials or wavelets for instance,[33] and in higher dimensions into spherical harmonics.[34]For instance, if en are any orthonormal basis functions of L2[0,1], then a given function in L2[0,1] can be approximated as a finite linear combination[35]The coefficients {aj} are selected to make the magnitude of the difference ||f − fn||2 as small as possible. Geometrically, the best approximation is the orthogonal projection of f onto the subspace consisting of all linear combinations of the {ej}, and can be calculated by[36]That this formula minimizes the difference ||f − fn||2 is a consequence of Bessel's inequality and Parseval's formula.In various applications to physical problems, a function can be decomposed into physically meaningful eigenfunctions of a differential operator (typically the Laplace operator): this forms the foundation for the spectral study of functions, in reference to the spectrum of the differential operator.[37] A concrete physical application involves the problem of hearing the shape of a drum: given the fundamental modes of vibration that a drumhead is capable of producing, can one infer the shape of the drum itself?[38] The mathematical formulation of this question involves the Dirichlet eigenvalues of the Laplace equation in the plane, that represent the fundamental modes of vibration in direct analogy with the integers that represent the fundamental modes of vibration of the violin string.Spectral theory also underlies certain aspects of the Fourier transform of a function. Whereas Fourier analysis decomposes a function defined on a compact set into the discrete spectrum of the Laplacian (which corresponds to the vibrations of a violin string or drum), the Fourier transform of a function is the decomposition of a function defined on all of Euclidean space into its components in the continuous spectrum of the Laplacian. The Fourier transformation is also geometrical, in a sense made precise by the Plancherel theorem, that asserts that it is an isometry of one Hilbert space (the "time domain") with another (the "frequency domain"). This isometry property of the Fourier transformation is a recurring theme in abstract harmonic analysis, as evidenced for instance by the Plancherel theorem for spherical functions occurring in noncommutative harmonic analysis.In the mathematically rigorous formulation of quantum mechanics, developed by John von Neumann,[39] the possible states (more precisely, the pure states) of a quantum mechanical system are represented by unit vectors (called state vectors) residing in a complex separable Hilbert space, known as the state space, well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projectivization of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system; for example, the position and momentum states for a single non-relativistic spin zero particle is the space of all square-integrable functions, while the states for the spin of a single proton are unit elements of the two-dimensional complex Hilbert space of spinors. Each observable is represented by a self-adjoint linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate.The inner product between two state vectors is a complex number known as a probability amplitude. During an ideal measurement of a quantum mechanical system, the probability that a system collapses from a given initial state to a particular eigenstate is given by the square of the absolute value of the probability amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator—which explains the choice of self-adjoint operators, for all the eigenvalues must be real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.For a general system, states are typically not pure, but instead are represented as statistical mixtures of pure states, or mixed states, given by density matrices: self-adjoint operators of trace one on a Hilbert space. Moreover, for general quantum mechanical systems, the effects of a single measurement can influence other parts of a system in a manner that is described instead by a positive operator valued measure. Thus the structure both of the states and observables in the general theory is considerably more complicated than the idealization for pure states.Any true physical color can be represented by a combination of pure spectral colors. As physical colors can be composed of any number of physical colors, the space of physical colors may aptly be represented by a Hilbert space over spectral colors. Humans have three types of cone cells for color perception, so the perceivable colors can be represented by 3-dimensional Euclidean space. The many-to-one linear mapping from the Hilbert space of physical colors to the Euclidean space of human perceivable colors explains why many distinct physical colors may be perceived by humans to be identical (e.g., pure yellow light versus a mix of red and green light, see metamerism).Two vectors u and v in a Hilbert space H are orthogonal when ⟨u,v⟩ = 0. The notation for this is u ⊥ v. More generally, when S is a subset in H, the notation u ⊥ S means that u is orthogonal to every element from S.When u and v are orthogonal, one hasBy induction on n, this is extended to any family u1, ..., un of n orthogonal vectors,Whereas the Pythagorean identity as stated is valid in any inner product space, completeness is required for the extension of the Pythagorean identity to series. A series ∑uk of orthogonal vectors converges in H if and only if the series of squares of norms converges, andFurthermore, the sum of a series of orthogonal vectors is independent of the order in which it is taken.By definition, every Hilbert space is also a Banach space. Furthermore, in every Hilbert space the following parallelogram identity holds:Conversely, every Banach space in which the parallelogram identity holds is a Hilbert space, and the inner product is uniquely determined by the norm by the polarization identity.[40] For real Hilbert spaces, the polarization identity isFor complex Hilbert spaces, it isThe parallelogram law implies that any Hilbert space is a uniformly convex Banach space.[41]This subsection employs the Hilbert projection theorem. If C is a non-empty closed convex subset of a Hilbert space H and x a point in H, there exists a unique point y ∈ C that minimizes the distance between x and points in C,[42]This is equivalent to saying that there is a point with minimal norm in the translated convex set D = C − x. The proof consists in showing that every minimizing sequence (dn) ⊂ D is Cauchy (using the parallelogram identity) hence converges (using completeness) to a point in D that has minimal norm. More generally, this holds in any uniformly convex Banach space.[43]When this result is applied to a closed subspace F of H, it can be shown that the point y ∈ F closest to x is characterized by[44]This point y is the orthogonal projection of x onto F, and the mapping PF : x → y is linear (see Orthogonal complements and projections). This result is especially significant in applied mathematics, especially numerical analysis, where it forms the basis of least squares methods.[45]In particular, when F is not equal to H, one can find a nonzero vector v orthogonal to F (select x ∉ F and v = x − y). A very useful criterion is obtained by applying this observation to the closed subspace F generated by a subset S of H.The dual space H* is the space of all continuous linear functions from the space H into the base field. It carries a natural norm, defined byThis norm satisfies the parallelogram law, and so the dual space is also an inner product space. The dual space is also complete, and so it is a Hilbert space in its own right.The Riesz representation theorem affords a convenient description of the dual. To every element u of H, there is a unique element φu of H*, defined byThe mapping u ↦ φu is an antilinear mapping from H to H*. The Riesz representation theorem states that this mapping is an antilinear isomorphism.[46] Thus to every element φ of the dual H* there exists one and only one uφ in H such thatfor all x ∈ H. The inner product on the dual space H* satisfiesThe reversal of order on the right-hand side restores linearity in φ from the antilinearity of uφ. In the real case, the antilinear isomorphism from H to its dual is actually an isomorphism, and so real Hilbert spaces are naturally isomorphic to their own duals.The representing vector uφ is obtained in the following way. When φ ≠ 0, the kernel F = Ker(φ) is a closed vector subspace of H, not equal to H, hence there exists a nonzero vector v orthogonal to F. The vector u is a suitable scalar multiple λv of v. The requirement that φ(v) = ⟨v,u⟩ yieldsThis correspondence φ ↔ u is exploited by the bra–ket notation popular in physics. It is common in physics to assume that the inner product, denoted by ⟨x|y⟩, is linear on the right,The result ⟨x|y⟩ can be seen as the action of the linear functional ⟨x| (the bra) on the vector |y⟩ (the ket).The Riesz representation theorem relies fundamentally not just on the presence of an inner product, but also on the completeness of the space. In fact, the theorem implies that the topological dual of any inner product space can be identified with its completion. An immediate consequence of the Riesz representation theorem is also that a Hilbert space H is reflexive, meaning that the natural map from H into its double dual space is an isomorphism.In a Hilbert space H, a sequence {xn} is weakly convergent to a vector x ∈ H whenfor every v ∈ H.For example, any orthonormal sequence {fn} converges weakly to 0, as a consequence of Bessel's inequality. Every weakly convergent sequence {xn} is bounded, by the uniform boundedness principle.Conversely, every bounded sequence in a Hilbert space admits weakly convergent subsequences (Alaoglu's theorem).[47] This fact may be used to prove minimization results for continuous convex functionals, in the same way that the Bolzano–Weierstrass theorem is used for continuous functions on ℝd. Among several variants, one simple statement is as follows:[48]This fact (and its various generalizations) are fundamental for direct methods in the calculus of variations. Minimization results for convex functionals are also a direct consequence of the slightly more abstract fact that closed bounded convex subsets in a Hilbert space H are weakly compact, since H is reflexive. The existence of weakly convergent subsequences is a special case of the Eberlein–Šmulian theorem.Any general property of Banach spaces continues to hold for Hilbert spaces. The open mapping theorem states that a continuous surjective linear transformation from one Banach space to another is an open mapping meaning that it sends open sets to open sets. A corollary is the bounded inverse theorem, that a continuous and bijective linear function from one Banach space to another is an isomorphism (that is, a continuous linear map whose inverse is also continuous). This theorem is considerably simpler to prove in the case of Hilbert spaces than in general Banach spaces.[49] The open mapping theorem is equivalent to the closed graph theorem, which asserts that a function from one Banach space to another is continuous if and only if its graph is a closed set.[50] In the case of Hilbert spaces, this is basic in the study of unbounded operators (see closed operator).The (geometrical) Hahn–Banach theorem asserts that a closed convex set can be separated from any point outside it by means of a hyperplane of the Hilbert space. This is an immediate consequence of the best approximation property: if y is the element of a closed convex set F closest to x, then the separating hyperplane is the plane perpendicular to the segment xy passing through its midpoint.[51]The continuous linear operators A : H1 → H2 from a Hilbert space H1 to a second Hilbert space H2 are bounded in the sense that they map bounded sets to bounded sets. Conversely, if an operator is bounded, then it is continuous. The space of such bounded linear operators has a norm, the operator norm given byThe sum and the composite of two bounded linear operators is again bounded and linear. For y in H2, the map that sends x ∈ H1 to ⟨Ax, y⟩ is linear and continuous, and according to the Riesz representation theorem can therefore be represented in the formfor some vector A*y in H1. This defines another bounded linear operator A* : H2 → H1, the adjoint of A. One can see that A** = A.The set B(H) of all bounded linear operators on H (operators H → H), together with the addition and composition operations, the norm and the adjoint operation, is a C*-algebra, which is a type of operator algebra.An element A of B(H) is called 'self-adjoint' or 'Hermitian' if A* = A. If A is Hermitian and ⟨Ax,x⟩ ≥ 0 for every x, then A is called 'nonnegative', written A ≥ 0; if equality holds only when x = 0, then A is called 'positive'. The set of self adjoint operators admits a partial order, in which A ≥ B if A − B ≥ 0. If A has the form B*B for some B, then A is nonnegative; if B is invertible, then A is positive. A converse is also true in the sense that, for a non-negative operator A, there exists a unique non-negative square root B such thatIn a sense made precise by the spectral theorem, self-adjoint operators can usefully be thought of as operators that are "real". An element A of B(H) is called normal if A*A = AA*. Normal operators decompose into the sum of a self-adjoint operators and an imaginary multiple of a self adjoint operatorthat commute with each other. Normal operators can also usefully be thought of in terms of their real and imaginary parts.An element U of B(H) is called unitary if U is invertible and its inverse is given by U*. This can also be expressed by requiring that U be onto and ⟨Ux,Uy⟩ = ⟨x,y⟩ for all x, y ∈ H. The unitary operators form a group under composition, which is the isometry group of H.An element of B(H) is compact if it sends bounded sets to relatively compact sets. Equivalently, a bounded operator T is compact if, for any bounded sequence {xk}, the sequence {Txk} has a convergent subsequence. Many integral operators are compact, and in fact define a special class of operators known as Hilbert–Schmidt operators that are especially important in the study of integral equations. Fredholm operators differ from a compact operator by a multiple of the identity, and are equivalently characterized as operators with a finite dimensional kernel and cokernel. The index of a Fredholm operator T is defined byThe index is homotopy invariant, and plays a deep role in differential geometry via the Atiyah–Singer index theorem.Unbounded operators are also tractable in Hilbert spaces, and have important applications to quantum mechanics.[52] An unbounded operator T on a Hilbert space H is defined as a linear operator whose domain D(T) is a linear subspace of H. Often the domain D(T) is a dense subspace of H, in which case T is known as a densely defined operator.The adjoint of a densely defined unbounded operator is defined in essentially the same manner as for bounded operators. Self-adjoint unbounded operators play the role of the observables in the mathematical formulation of quantum mechanics. Examples of self-adjoint unbounded operators on the Hilbert space L2(ℝ) are:[53]These correspond to the momentum and position observables, respectively. Note that neither A nor B is defined on all of H, since in the case of A the derivative need not exist, and in the case of B the product function need not be square integrable. In both cases, the set of possible arguments form dense subspaces of L2(ℝ).Two Hilbert spaces H1 and H2 can be combined into another Hilbert space, called the (orthogonal) direct sum,[54] and denotedconsisting of the set of all ordered pairs (x1,x2) where xi ∈ Hi, i = 1,2, and inner product defined byMore generally, if Hi is a family of Hilbert spaces indexed by i ∈ I, then the direct sum of the Hi, denotedconsists of the set of all indexed familiesin the Cartesian product of the Hi such thatThe inner product is defined byEach of the Hi is included as a closed subspace in the direct sum of all of the Hi. Moreover, the Hi are pairwise orthogonal. Conversely, if there is a system of closed subspaces, Vi, i ∈ I, in a Hilbert space H, that are pairwise orthogonal and whose union is dense in H, then H is canonically isomorphic to the direct sum of Vi. In this case, H is called the internal direct sum of the Vi. A direct sum (internal or external) is also equipped with a family of orthogonal projections Ei onto the ith direct summand Hi. These projections are bounded, self-adjoint, idempotent operators that satisfy the orthogonality conditionThe spectral theorem for compact self-adjoint operators on a Hilbert space H states that H splits into an orthogonal direct sum of the eigenspaces of an operator, and also gives an explicit decomposition of the operator as a sum of projections onto the eigenspaces. The direct sum of Hilbert spaces also appears in quantum mechanics as the Fock space of a system containing a variable number of particles, where each Hilbert space in the direct sum corresponds to an additional degree of freedom for the quantum mechanical system. In representation theory, the Peter–Weyl theorem guarantees that any unitary representation of a compact group on a Hilbert space splits as the direct sum of finite-dimensional representations.If x1, y1 ∊ H1 and x2, y2 ∊ H2, then one defines an inner product on the (ordinary) tensor product as follows. On simple tensors, letAn example is provided by the Hilbert space L2([0,1]). The Hilbertian tensor product of two copies of L2([0,1]) is isometrically and linearly isomorphic to the space L2([0,1]2) of square-integrable functions on the square [0,1]2. This isomorphism sends a simple tensor f1 ⊗ f2 to the functionon the square.This example is typical in the following sense.[56] Associated to every simple tensor product x1 ⊗ x2 is the rank one operator from H∗1 to H2 that maps a given x* ∈ H∗1 asThe notion of an orthonormal basis from linear algebra generalizes over to the case of Hilbert spaces.[57] In a Hilbert space H, an orthonormal basis is a family {ek}k ∈ B of elements of H satisfying the conditions:A system of vectors satisfying the first two conditions basis is called an orthonormal system or an orthonormal set (or an orthonormal sequence if B is countable). Such a system is always linearly independent. Completeness of an orthonormal system of vectors of a Hilbert space can be equivalently restated as:This is related to the fact that the only vector orthogonal to a dense linear subspace is the zero vector, for if S is any orthonormal set and v is orthogonal to S, then v is orthogonal to the closure of the linear span of S, which is the whole space.Examples of orthonormal bases include:In the infinite-dimensional case, an orthonormal basis will not be a basis in the sense of linear algebra; to distinguish the two, the latter basis is also called a Hamel basis. That the span of the basis vectors is dense implies that every vector in the space can be written as the sum of an infinite series, and the orthogonality implies that this decomposition is unique.The space l2 of square-summable sequences of complex numbers is the set of infinite sequencesof complex numbers such thatThis space has an orthonormal basis:More generally, if B is any set, then one can form a Hilbert space of sequences with index set B, defined byThe summation over B is here defined bythe supremum being taken over all finite subsets of B. It follows that, for this sum to be finite, every element of l2(B) has only countably many nonzero terms. This space becomes a Hilbert space with the inner productfor all x, y ∈ l2(B). Here the sum also has only countably many nonzero terms, and is unconditionally convergent by the Cauchy–Schwarz inequality.An orthonormal basis of l2(B) is indexed by the set B, given byLet f1, ..., fn be a finite orthonormal system in H. For an arbitrary vector x ∈ H, letThen ⟨x, fk⟩ = ⟨y, fk⟩ for every k = 1, ..., n. It follows that x − y is orthogonal to each fk, hence x − y is orthogonal to y. Using the Pythagorean identity twice, it follows thatLet {fi}, i ∈ I, be an arbitrary orthonormal system in H. Applying the preceding inequality to every finite subset J of I gives the Bessel inequality[58](according to the definition of the sum of an arbitrary family of non-negative real numbers).Geometrically, Bessel's inequality implies that the orthogonal projection of x onto the linear subspace spanned by the fi has norm that does not exceed that of x. In two dimensions, this is the assertion that the length of the leg of a right triangle may not exceed the length of the hypotenuse.Bessel's inequality is a stepping stone to the more powerful Parseval identity, which governs the case when Bessel's inequality is actually an equality. If {ek}k ∈ B is an orthonormal basis of H, then every element x of H may be written asEven if B is uncountable, Bessel's inequality guarantees that the expression is well-defined and consists only of countably many nonzero terms. This sum is called the Fourier expansion of x, and the individual coefficients ⟨x,ek⟩ are the Fourier coefficients of x. Parseval's formula is thenConversely, if {ek} is an orthonormal set such that Parseval's identity holds for every x, then {ek} is an orthonormal basis.As a consequence of Zorn's lemma, every Hilbert space admits an orthonormal basis; furthermore, any two orthonormal bases of the same space have the same cardinality, called the Hilbert dimension of the space.[59] For instance, since l2(B) has an orthonormal basis indexed by B, its Hilbert dimension is the cardinality of B (which may be a finite integer, or a countable or uncountable cardinal number).As a consequence of Parseval's identity, if {ek}k ∈ B is an orthonormal basis of H, then the map Φ : H → l2(B) defined by Φ(x) = ⟨x,ek⟩k∈B is an isometric isomorphism of Hilbert spaces: it is a bijective linear mapping such thatfor all x, y ∈ H. The cardinal number of B is the Hilbert dimension of H. Thus every Hilbert space is isometrically isomorphic to a sequence space l2(B) for some set B.A Hilbert space is separable if and only if it admits a countable orthonormal basis. All infinite-dimensional separable Hilbert spaces are therefore isometrically isomorphic to l2.In the past, Hilbert spaces were often required to be separable as part of the definition.[60] Most spaces used in physics are separable, and since these are all isomorphic to each other, one often refers to any infinite-dimensional separable Hilbert space as "the Hilbert space" or just "Hilbert space".[61] Even in quantum field theory, most of the Hilbert spaces are in fact separable, as stipulated by the Wightman axioms. However, it is sometimes argued that non-separable Hilbert spaces are also important in quantum field theory, roughly because the systems in the theory possess an infinite number of degrees of freedom and any infinite Hilbert tensor product (of spaces of dimension greater than one) is non-separable.[62] For instance, a bosonic field can be naturally thought of as an element of a tensor product whose factors represent harmonic oscillators at each point of space. From this perspective, the natural state space of a boson might seem to be a non-separable space.[62] However, it is only a small separable subspace of the full tensor product that can contain physically meaningful fields (on which the observables can be defined). Another non-separable Hilbert space models the state of an infinite collection of particles in an unbounded region of space. An orthonormal basis of the space is indexed by the density of the particles, a continuous parameter, and since the set of possible densities is uncountable, the basis is not countable.[62]If S is a subset of a Hilbert space H, the set of vectors orthogonal to S is defined byS⊥ is a closed subspace of H (can be proved easily using the linearity and continuity of the inner product) and so forms itself a Hilbert space. If V is a closed subspace of H, then V⊥ is called the orthogonal complement of V. In fact, every x ∈ H can then be written uniquely as x = v + w, with v ∈ V and w ∈ V⊥. Therefore, H is the internal Hilbert direct sum of V and V⊥.The linear operator PV : H → H that maps x to v is called the orthogonal projection onto V. There is a natural one-to-one correspondence between the set of all closed subspaces of H and the set of all bounded self-adjoint operators P such that P2 = P. Specifically,This provides the geometrical interpretation of PV(x): it is the best approximation to x by elements of V.[63]Projections PU and PV are called mutually orthogonal if PUPV = 0. This is equivalent to U and V being orthogonal as subspaces of H. The sum of the two projections PU and PV is a projection only if U and V are orthogonal to each other, and in that case PU + PV = PU+V. The composite PUPV is generally not a projection; in fact, the composite is a projection if and only if the two projections commute, and in that case PUPV = PU∩V.By restricting the codomain to the Hilbert space V, the orthogonal projection PV gives rise to a projection mapping π : H → V; it is the adjoint of the inclusion mappingmeaning thatfor all x ∈ V and y ∈ H.The operator norm of the orthogonal projection PV onto a nonzero closed subspace V is equal to 1:Every closed subspace V of a Hilbert space is therefore the image of an operator P of norm one such that P2 = P. The property of possessing appropriate projection operators characterizes Hilbert spaces:[64]While this result characterizes the metric structure of a Hilbert space, the structure of a Hilbert space as a topological vector space can itself be characterized in terms of the presence of complementary subspaces:[65]The orthogonal complement satisfies some more elementary results. It is a monotone function in the sense that if U ⊂ V, then V⊥ ⊆ U⊥ with equality holding if and only if V is contained in the closure of U. This result is a special case of the Hahn–Banach theorem. The closure of a subspace can be completely characterized in terms of the orthogonal complement: if V is a subspace of H, then the closure of V is equal to V⊥⊥. The orthogonal complement is thus a Galois connection on the partial order of subspaces of a Hilbert space. In general, the orthogonal complement of a sum of subspaces is the intersection of the orthogonal complements:[66]If the Vi are in addition closed, thenThere is a well-developed spectral theory for self-adjoint operators in a Hilbert space, that is roughly analogous to the study of symmetric matrices over the reals or self-adjoint matrices over the complex numbers.[67] In the same sense, one can obtain a "diagonalization" of a self-adjoint operator as a suitable sum (actually an integral) of orthogonal projection operators.The spectrum of an operator T, denoted σ(T), is the set of complex numbers λ such that T − λ lacks a continuous inverse. If T is bounded, then the spectrum is always a compact set in the complex plane, and lies inside the disc |z| ≤ ||T||. If T is self-adjoint, then the spectrum is real. In fact, it is contained in the interval [m,M] whereMoreover, m and M are both actually contained within the spectrum.The eigenspaces of an operator T are given byUnlike with finite matrices, not every element of the spectrum of T must be an eigenvalue: the linear operator T − λ may only lack an inverse because it is not surjective. Elements of the spectrum of an operator in the general sense are known as spectral values. Since spectral values need not be eigenvalues, the spectral decomposition is often more subtle than in finite dimensions.However, the spectral theorem of a self-adjoint operator T takes a particularly simple form if, in addition, T is assumed to be a compact operator. The spectral theorem for compact self-adjoint operators states:[68]This theorem plays a fundamental role in the theory of integral equations, as many integral operators are compact, in particular those that arise from Hilbert–Schmidt operators.The general spectral theorem for self-adjoint operators involves a kind of operator-valued Riemann–Stieltjes integral, rather than an infinite summation.[69] The spectral family associated to T associates to each real number λ an operator Eλ, which is the projection onto the nullspace of the operator (T − λ)+, where the positive part of a self-adjoint operator is defined byThe operators Eλ are monotone increasing relative to the partial order defined on self-adjoint operators; the eigenvalues correspond precisely to the jump discontinuities. One has the spectral theorem, which assertsThe integral is understood as a Riemann–Stieltjes integral, convergent with respect to the norm on B(H). In particular, one has the ordinary scalar-valued integral representationA somewhat similar spectral decomposition holds for normal operators, although because the spectrum may now contain non-real complex numbers, the operator-valued Stieltjes measure dEλ must instead be replaced by a resolution of the identity.A major application of spectral methods is the spectral mapping theorem, which allows one to apply to a self-adjoint operator T any continuous complex function f defined on the spectrum of T by forming the integralThe resulting continuous functional calculus has applications in particular to pseudodifferential operators.[70]The spectral theory of unbounded self-adjoint operators is only marginally more difficult than for bounded operators. The spectrum of an unbounded operator is defined in precisely the same way as for bounded operators: λ is a spectral value if the resolvent operatorfails to be a well-defined continuous operator. The self-adjointness of T still guarantees that the spectrum is real. Thus the essential idea of working with unbounded operators is to look instead at the resolvent Rλ where λ is nonreal. This is a bounded normal operator, which admits a spectral representation that can then be transferred to a spectral representation of T itself. A similar strategy is used, for instance, to study the spectrum of the Laplace operator: rather than address the operator directly, one instead looks as an associated resolvent such as a Riesz potential or Bessel potential.A precise version of the spectral theorem in this case is:[71]There is also a version of the spectral theorem that applies to unbounded normal operators.Thomas Pynchon introduced the fictional character, Sammy Hilbert-Spaess (a pun on "Hilbert Space"), in his 1973 novel, Gravity's Rainbow.  Hilbert-Spaess is first described as a "a ubiquitous double agent" and later as "at least a double agent".  The novel had earlier referenced the work of fellow German mathematician Kurt Gödel's Incompleteness Theorems which showed that Hilbert's Program, Hilbert's formalized plan to unify mathematics into a single set of axioms, was not possible.[72][73][74]
Matrix norm
In mathematics, a matrix norm is a vector norm in a vector space whose elements (vectors) are matrices (of given dimensions).Additionally, in the case of square matrices (thus, m = n), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors:There are three types of matrix norms which will be discussed below:Moreover, any induced norm satisfies the inequality    (1)which is simply the maximum absolute column sum of the matrix;which is simply the maximum absolute row sum of the matrix;then we haveandFor example, using the p-norm for vectors, p ≥ 1, we get:This is a different norm from the induced p-norm (see above) and the Schatten p-norm (see below), but the notation is the same.The special case p = 2 is the Frobenius norm, and p = ∞ yields the maximum norm.andIt also satisfies and The max norm is the elementwise norm with p = q = ∞:This norm is not sub-multiplicative.The Schatten p-norms arise when applying the p-norm to the vector of singular values of a matrix. If the singular values are denoted by σi, then the Schatten p-norm is defined byThese norms again share the notation with the induced and entrywise p-norms, but they are different.The most familiar cases are p = 1, 2, ∞. The case p = 2 yields the Frobenius norm, introduced before. The case p = ∞ yields the spectral norm, which is the operator norm induced by the vector 2-norm (see above). Finally, p = 1 yields the nuclear norm (also known as the trace norm, or the Ky Fan 'n'-norm[3]), defined asAnother useful inequality between matrix norms iswhich is a special case of Hölder's inequality.
Diagonal matrix
As stated above, the off-diagonal entries are zero.  That is, the matrix D = (di,j) with n columns and n rows is diagonal ifHowever, the main diagonal entries are unrestricted.The term diagonal matrix may sometimes refer to a rectangular diagonal matrix, which is an m-by-n matrix with all the entries not of the form di,i being zero. For example:The following matrix is a symmetric diagonal matrix:If the entries are real numbers or complex numbers, then it is a normal matrix as well.In the remainder of this article we will consider only square matrices.A square diagonal matrix with all its main diagonal entries equal is a scalar matrix, that is, a scalar multiple λI of the identity matrix I. Its effect on a vector is scalar multiplication by λ. For example, a 3×3 scalar matrix has the form:The scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size. All other diagonal matrices which are not scalar only commute with  other diagonal matrices  and not with any matrix unlike scalar matrices.[1] Intuitively, this stems from the fact that scalar matrices are Identity matrices multiplied with scalars.The operations of matrix addition and matrix multiplication are especially simple for symmetric diagonal matrices. Write diag(a1, ..., an) for a diagonal matrix whose diagonal entries starting in the upper left corner are a1, ..., an. Then, for addition, we haveand for matrix multiplication,The diagonal matrix diag(a1, ..., an) is invertible if and only if the entries a1, ..., an are all non-zero. In this case, we haveIn particular, the diagonal matrices form a subring of the ring of all n-by-n matrices.Multiplying an n-by-n matrix A from the left with diag(a1, ..., an) amounts to multiplying the ith row of A by ai for all i; multiplying the matrix A from the right with diag(a1, ..., an) amounts to multiplying the ith column of A by ai for all i.In other words, the eigenvalues of diag(λ1, ..., λn) are λ1, ..., λn with associated eigenvectors of e1, ..., en.The determinant of diag(a1, ..., an) is the product a1...an.The adjugate of a diagonal matrix is again diagonal.A square matrix is diagonal if and only if it is triangular and normal.Any square diagonal matrix is also a symmetric matrix.A symmetric diagonal matrix can be defined as a matrix that is both upper- and lower-triangular. The identity matrix In and any square zero matrix are diagonal. A one-dimensional matrix is always diagonal.Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or linear map by a diagonal matrix.In fact, a given n-by-n matrix A is similar to a diagonal matrix (meaning that there is a matrix X such that X−1AX is diagonal) if and only if it has n linearly independent eigenvectors. Such matrices are said to be diagonalizable.Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if AA∗ = A∗A then there exists a unitary matrix U such that UAU∗ is diagonal). Furthermore, the singular value decomposition implies that for any matrix A, there exist unitary matrices U and V such that UAV∗ is diagonal with positive entries.In operator theory, particularly the study of PDEs, operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation. Therefore, a key technique to understanding operators is a change of coordinates–in the language of operators, an integral transform–which changes the basis to an eigenbasis of eigenfunctions: which makes the equation separable. An important example of this is the Fourier transform, which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation.Especially easy are multiplication operators, which are defined as multiplication by (the values of) a fixed function–the values of the function at each point correspond to the diagonal entries of a matrix.
Linear span
In linear algebra, the linear span (also called the linear hull or just span) of a set of vectors in a vector space is the intersection of all linear subspaces which each contain every vector in that set. The linear span of a set of vectors is therefore a vector space. Spans can be generalized to matroids and modules.For expressing that a vector space V is a span of a set S, one commonly uses the following phrases: S spans V; V is spanned by S; S is a spanning set of V; S is a generating set of V.Given a vector space V over a field K, the span of a set S of vectors (not necessarily infinite) is defined to be the intersection W of all subspaces of V that contain S. W is referred to as the subspace spanned by S, or by the vectors in S. Conversely, S is called a spanning set of W, and we say that S spans W.Alternatively, the span of S may be defined as the set of all finite linear combinations of elements of S, which follows from the above definition.In particular, if S is a finite subset of V, then the span of S is the set of all linear combinations of the elements of S. In the case of infinite S, infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming such sums are defined somehow, e.g. if V is a Banach space) are excluded by the definition; a generalization that allows these is not equivalent.The real vector space R3 has {(-1,0,0), (0,1,0), (0,0,1)} as a spanning set. This particular spanning set is also a basis. If (-1,0,0) were replaced by (1,0,0), it would also form the canonical basis of R3.Another spanning set for the same space is given by {(1,2,3), (0,1,2), (−1,1/2,3), (1,1,1)}, but this set is not a basis, because it is linearly dependent.The set {(1,0,0), (0,1,0), (1,1,0)} is not a spanning set of R3; instead its span is the space of all vectors in R3 whose last component is zero. That space (the space of all vectors in R3 whose last component is zero) is also spanned by the set {(1,0,0), (0,1,0)}, as (1,1,0) is a linear combination of (1,0,0) and (0,1,0). It does, however, span R2.The empty set is a spanning set of {(0, 0, 0)} since the empty set is a subset of all possible vector spaces in R3, and {(0, 0, 0)} is the intersection of all of these vector spaces.The set of functions xn where n is a non-negative integer spans the space of polynomials.Theorem 1: The subspace spanned by a non-empty subset S of a vector space V is the set of all linear combinations of vectors in S.This theorem is so well known that at times it is referred to as the definition of span of a set.Theorem 2: Every spanning set S of a vector space V must contain at least as many elements as any linearly independent set of vectors from V.Theorem 3: Let V be a finite-dimensional vector space. Any set of vectors that spans V can be reduced to a basis for V by discarding vectors if necessary (i.e. if there are linearly dependent vectors in the set). If the axiom of choice holds, this is true without the assumption that V has finite dimension.This also indicates that a basis is a minimal spanning set when V is finite-dimensional.Generalizing the definition of the span of points in space, a subset X of the ground set of a matroid is called a spanning set if the rank of X equals the rank of the entire ground set[citation needed].The vector space definition can also be generalized to modules.[1] Given an R-module A and any collection of elements a1,…,an of A, then the sum of cyclic modules,consisting of all R-linear combinations of the given elements ai, is called the submodule of A spanned by a1,…,an. As with the case of vector spaces, the submodule of A spanned by any subset of A is the intersection of all the submodules containing that subset.In functional analysis, a closed linear span of a set of vectors is the minimal closed set which contains the linear span of that set.One mathematical formulation of this isThe closed linear span of the set of functions xn on the interval [0, 1], where n is a non-negative integer, depends on the norm used. If the L2 norm is used, then the closed linear span is the Hilbert space of square-integrable functions on the interval. But if the maximum norm is used, the closed linear span will be the space of continuous functions on the interval. In either case, the closed linear span contains functions that are not polynomials, and so are not in the linear span itself. However, the cardinality of the set of functions in the closed linear span is the cardinality of the continuum, which is the same cardinality as for the set of polynomials.The linear span of a set is dense in the closed linear span. Moreover, as stated in the lemma below, the closed linear span is indeed the closure of the linear span.Closed linear spans are important when dealing with closed linear subspaces (which are themselves highly important, consider Riesz's lemma).Let X be a normed space and let E be any non-empty subset of X. Then(So the usual way to find the closed linear span is to find the linear span first, and then the closure of that linear span.)
Gaussian elimination
In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 A.D. (see History section).To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the leftmost nonzero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (unreduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called forward elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.There are three types of elementary row operations which may be performed on the rows of a matrix:If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.For each row in a matrix, if the row does not consist of only zeros, then the leftmost nonzero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).Suppose the goal is to find and describe the set of solutions to the following system of linear equations:The table below is the row reduction process applied simultaneously to the system of equations, and its associated augmented matrix. In practice, one does not usually deal with the systems in terms of equations but instead makes use of the augmented matrix, which is more suitable for computer manipulations. The row reduction procedure may be summarized as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.The second column describes which row operations have just been performed. So for the first step, the x is eliminated from L2 by adding 3/2L1 to L2. Next x is eliminated from L3 by adding L1 to L3. These row operations are labelled in the table asOnce y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = −1, y = 3, and x = 2. So there is a unique solution to the original system of equations.Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss–Jordan elimination, to distinguish it from stopping after reaching echelon form.The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied.  Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life.  The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century.  Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5]  The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss–Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.[7]The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B:Computationally, for an n × n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.A variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists.  If A is an n × n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n × n identity matrix is augmented to the right of A, forming an n × 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n × 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A−1. If the algorithm is unable to reduce the left block to I, then A is not invertible.For example, consider the following matrixTo find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 × 6 matrix:By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A−1. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.The Gaussian elimination algorithm can be applied to any m × n matrix A. In this way, for example, some 6 × 9 matrices can be transformed to a matrix that has a row echelon form likewhere the stars are arbitrary entries and a, b, c, d, e are nonzero entries. This echelon matrix T contains a wealth of information about A: the rank of A is 5 since there are 5 nonzero rows in T; the vector space spanned by the columns of A has a basis consisting of the first, third, fourth, seventh and ninth column of A (the columns of a, b, c, d, e in T), and the stars show how the other columns of A can be written as linear combinations of the basis columns. This is a consequence of the distributivity of the dot product in the expression of a linear map as a matrix.All of this applies also to the reduced row echelon form, which is a particular row echelon format.The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n + 1)/2 divisions, 2n3 + 3n2 − 5n/6 multiplications, and 2n3 + 3n2 − 5n/6 subtractions,[8] for a total of approximately 2n3/3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating-point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9]However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5).This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).To put an n × n matrix into reduced echelon form by row operations, one needs n3 arithmetic operations, which is approximately 50% more computation steps.[10]One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row-reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means that any error existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]The Gaussian elimination can be performed over any field, not just the real numbers.Gaussian elimination does not generalize in any way to higher-order tensors (matrices are array representations of order-2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]As explained above, Gaussian elimination transforms a given m × n matrix A into a matrix in row-echelon form.In the following pseudocode, A[i, j] denotes the entry of the matrix A in row i and column j with the indices starting from 1. The transformation is performed in place, meaning that the original matrix is lost for being eventually replaced by its row-echelon form.This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.
Simpson's rule
For unequally spaced points, see Cartwright.[1]Simpson's rule also corresponds to the three-point Newton-Cotes quadrature rule.In English, the method is credited to the mathematician Thomas Simpson (1710–1761) of Leicestershire, England. However, Johannes Kepler used similar formulas over 100 years prior, and for this reason the method is sometimes called Kepler's rule, or Keplersche Fassregel (Kepler's barrel rule) in German.Using integration by substitution one can show that[2]Another derivation constructs Simpson's rule from two simpler approximations: the midpoint ruleand the trapezoidal ruleThe errors in these approximations areThis weighted average is exactly Simpson's rule.Using another approximation (for example, the trapezoidal rule with twice as many points), it is possible to take a suitable weighted average and eliminate another error term.  This is Romberg's method.The third derivation starts from the ansatzThe coefficients α, β and γ can be fixed by requiring that this approximation be exact for all quadratic polynomials. This yields Simpson's rule.The error in approximating an integral by Simpson's rule isThe error committed by the composite Simpson's rule isSimpson's 3/8 rule is another method for numerical integration proposed by Thomas Simpson. It is based upon a cubic interpolation rather than a quadratic interpolation. Simpson's 3/8 rule is as follows:where b − a = 3h. The error of this method is:A further generalization of this concept for interpolation with arbitrary-degree polynomials are the Newton–Cotes formulas.While the remainder for the rule is shown as:A simplified version of Simpson's rules is used in naval architecture. The 3/8th rule is also called Simpson's second rule.This is another formulation of a composite Simpson's rule: instead of applying Simpson's rule to disjoint segments of the integral to be approximated, Simpson's rule is applied to overlapping segments, yielding:[7]The formula above is obtained by combining the original composite Simpson's rule with the one consisting of using Simpson's 3/8 rule in the extreme subintervals and the standard 3-point rule in the remaining subintervals. The result is then obtained by taking the mean of the two formulas. In the task of estimation of full area of narrow peak-like functions, Simpson's rules are much less efficient than trapezoidal rule. Namely, composite Simpson's 1/3 rule requires 1.8 times more points to achieve the same accuracy[8] as trapezoidal rule. Composite Simpson's 3/8 rule is even less accurate. Integral by Simpson's 1/3 rule can be represented as a sum of 2/3 of integral by trapezoidal rule with step h and 1/3 of integral by rectangle rule with step 2h. No wonder that error of the sum corresponds lo less accurate term. Averaging of Simpson's 1/3 rule composite sums with properly shifted frames produces following rules:where two points outside of integrated region are exploited andThose rules are very much similar to Press's alternative extended Simpson's rule. Coefficients within the major part of the region being integrated equal one, differences are only at the edges. These three rules can be associated with Euler-MacLaurin formula with the first derivative term and named Euler-MacLaurin integration rules[8]. They differ only in the way, how the first derivative at the region end is calculated.  This article incorporates material from Code for Simpson's rule on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
Spectral theory
In mathematics, spectral theory is an inclusive term for theories extending the eigenvector and eigenvalue theory of a single square matrix to a much broader theory of the structure of operators in a variety of mathematical spaces.[1] It is a result of studies of linear algebra and the solutions of systems of linear equations and their generalizations.[2] The theory is connected to that of analytic functions because the spectral properties of an operator are related to analytic functions of the spectral parameter.[3]The name spectral theory was introduced by David Hilbert in his original formulation of Hilbert space theory, which was cast in terms of quadratic forms in infinitely many variables. The original spectral theorem was therefore conceived as a version of the theorem on principal axes of an ellipsoid, in an infinite-dimensional setting. The later discovery in quantum mechanics that spectral theory could explain features of atomic spectra was therefore fortuitous. Hilbert himself was surprised by the unexpected application of this theory, noting that "I developed my theory of infinitely many variables from purely mathematical interests, and even called it 'spectral analysis' without any presentiment that it would later find application to the actual spectrum of physics."[4]There have been three main ways to formulate spectral theory, all of which retain their usefulness.[clarification needed] After Hilbert's initial formulation, the later development of abstract Hilbert space and the spectral theory of a single normal operator on it did very much go in parallel with the requirements of physics; particularly in the hands of von Neumann.[5] The further theory built on this to include Banach algebras, which can be given abstractly. This development leads to the Gelfand representation, which covers the commutative case, and further into non-commutative harmonic analysis.The difference can be seen in making the connection with Fourier analysis. The Fourier transform on the real line is in one sense the spectral theory of differentiation qua differential operator. But for that to cover the phenomena one has already to deal with generalized eigenfunctions (for example, by means of a rigged Hilbert space). On the other hand it is simple to construct a group algebra, the spectrum of which captures the Fourier transform's basic properties, and this is carried out by means of Pontryagin duality.One can also study the spectral properties of operators on Banach spaces. For example, compact operators on Banach spaces have many spectral properties similar to that of matrices.The background in the physics of vibrations has been explained in this way:[6]The mathematical theory is not dependent on such physical ideas on a technical level, but there are examples of mutual influence (see for example Mark Kac's question Can you hear the shape of a drum?). Hilbert's adoption of the term "spectrum" has been attributed to an 1897 paper of Wilhelm Wirtinger on Hill differential equation (by Jean Dieudonné), and it was taken up by his students during the first decade of the twentieth century, among them Erhard Schmidt and Hermann Weyl. The conceptual basis for Hilbert space was developed from Hilbert's ideas by Erhard Schmidt and Frigyes Riesz.[7][8]  It was almost twenty years later, when quantum mechanics was formulated in terms of the Schrödinger equation, that the connection was made to atomic spectra; a connection with the mathematical physics of vibration had been suspected before, as remarked by Henri Poincaré, but rejected for simple quantitative reasons, absent an explanation of the Balmer series.[9] The later discovery in quantum mechanics that spectral theory could explain features of atomic spectra was therefore fortuitous, rather than being an object of Hilbert's spectral theory.Consider a bounded linear transformation T defined everywhere over a general Banach space. We form the transformation:Here I is the identity operator and ζ is a complex number. The inverse of an operator T, that is T−1, is defined by:If the inverse exists, T is called regular. If it does not exist, T is called singular.With these definitions, the resolvent set of T is the set of all complex numbers ζ such that Rζ exists and is bounded. This set often is denoted as ρ(T). The spectrum of T is the set of all complex numbers ζ such that Rζ fails to exist or is unbounded. Often the spectrum of T is denoted by σ(T). The function Rζ for all ζ in ρ(T) (that is, wherever Rζ exists as a bounded operator) is called the resolvent of T. The spectrum of T is therefore the complement of the resolvent set of T in the complex plane.[10] Every eigenvalue of T belongs to σ(T), but σ(T) may contain non-eigenvalues.[11]This definition applies to a Banach space, but of course other types of space exist as well, for example, topological vector spaces include Banach spaces, but can be more general.[12][13] On the other hand, Banach spaces include Hilbert spaces, and it is these spaces that find the greatest application and the richest theoretical results.[14] With suitable restrictions, much can be said about the structure of the spectra of transformations in a Hilbert space. In particular, for self-adjoint operators, the spectrum lies on the real line and (in general) is a spectral combination of a point spectrum of discrete eigenvalues and a continuous spectrum.[15]In functional analysis and linear algebra the spectral theorem establishes conditions under which an operator can be expressed in simple form as a sum of simpler operators. As a full rigorous presentation is not appropriate for this article, we take an approach that avoids much of the rigor and satisfaction of a formal treatment with the aim of being more comprehensible to a non-specialist.This topic is easiest to describe by introducing the bra–ket notation of Dirac for operators.[16][17] As an example, a very particular linear operator L might be written as a dyadic product:[18][19]and the magnitude of f bywhere the notation '*' denotes a complex conjugate. This inner product choice defines a very specific inner product space, restricting the generality of the arguments that follow.[14]The effect of L upon a function f is then described as:A more general linear operator L might be expressed as:Some natural questions are: under what circumstances does this formalism work, and for what operators L are expansions in series of other operators like this possible? Can any function f be expressed in terms of the eigenfunctions (are they a Schauder basis) and under what circumstances does a point spectrum or a continuous spectrum arise? How do the formalisms for infinite-dimensional spaces and finite-dimensional spaces differ, or do they differ? Can these ideas be extended to a broader class of spaces? Answering such questions is the realm of spectral theory and requires considerable background in functional analysis and matrix algebra.This section continues in the rough and ready manner of the above section using the bra–ket notation, and glossing over the many important details of a rigorous treatment.[21] A rigorous mathematical treatment may be found in various references.[22] In particular, the dimension n of the space will be finite.Using the bra–ket notation of the above section, the identity operator may be written as:This expression of the identity operation is called a representation or a resolution of the identity.[21],[22] This formal representation satisfies the basic property of the identity:valid for every positive integer k.Given some operator equation of the form:with h in the space, this equation can be solved in the above basis through the formal manipulations:The role of spectral theory arises in establishing the nature and existence of the basis and the reciprocal basis. In particular, the basis might consist of the eigenfunctions of some linear operator L:with the { λi } the eigenvalues of L from the spectrum of L. Then the resolution of the identity above provides the dyad expansion of L:Using spectral theory, the resolvent operator R:can be evaluated  in terms of the eigenfunctions and eigenvalues of L, and the Green's function corresponding to L can be found.This function has poles in the complex λ-plane at each eigenvalue of L. Thus, using the calculus of residues:where the line integral is over a contour C that includes all the eigenvalues of L.Suppose our functions are defined over some coordinates {xj}, that is:Introducing the notationwhere δ(x − y) = δ(x1 − y1, x2 − y2, x3 − y3,  ...) is the Dirac delta function,[24]we can writeThen:The function G(x, y; λ) defined by:is called the Green's function for operator L, and satisfies:[25]Consider the operator equation:in terms of coordinates:A particular case is λ = 0.The Green's function of the previous section is:and satisfies:Using this Green's function property:Then, multiplying both sides of this equation by h(z) and integrating:which suggests the solution is:That is, the function ψ(x) satisfying the operator equation is found if we can  find the spectrum of O, and construct G, for example by using:There are many other ways to find G, of course.[26] See the articles on Green's functions and on Fredholm integral equations. It must be kept in mind that the above mathematics is purely formal, and a rigorous treatment involves some pretty sophisticated mathematics, including a good background knowledge of functional analysis, Hilbert spaces, distributions and so forth. Consult these articles and the references for more detail.Optimization problems may be the most useful examples about the combinatorial significance of the eigenvalues and eigenvectors in symmetric matrices, especially for the Rayleigh quotient with respect to a matrix M.Theorem Let M be a symmetric matrix and let x be the non-zero vector that maximizes the Rayleigh quotient with respect to M. Then, x is an eigenvector of M with eigenvalue equal to the Rayleigh quotient. Moreover, this eigenvalue is the largest eigenvalue of M. The way to prove this formula is pretty easy. Namely,evaluate the Rayleigh quotient with respect to x:where we used Parseval's identity in the last line.Finally we obtain that [27]
Quadratic form
In mathematics, a quadratic form is a homogeneous polynomial of degree two in a number of variables.  For example,is a quadratic form in the variables x and y.Quadratic forms occupy a central place in various branches of mathematics, including number theory, linear algebra, group theory (orthogonal group), differential geometry (Riemannian metric, second fundamental form), differential topology (intersection forms of four-manifolds), and Lie theory (the Killing form).Quadratic forms are homogeneous quadratic polynomials in n variables. In the cases of one, two, and three variables they are called unary, binary, and ternary and have the following explicit form:where a, ..., f are the coefficients.[1] Note that quadratic functions, such as ax2 + bx + c in the one variable case, are not quadratic forms, as they are typically not homogeneous (unless b and c are both 0).The theory of quadratic forms and methods used in their study depend in a large measure on the nature of the coefficients, which may be real or complex numbers, rational numbers, or integers. In linear algebra, analytic geometry, and in the majority of applications of quadratic forms, the coefficients are real or complex numbers. In the algebraic theory of quadratic forms, the coefficients are elements of a certain field. In the arithmetic theory of quadratic forms, the coefficients belong to a fixed commutative ring, frequently the integers Z or the p-adic integers Zp.[2] Binary quadratic forms have been extensively studied in number theory, in particular, in the theory of quadratic fields, continued fractions, and modular forms. The theory of integral quadratic forms in n variables has important applications to algebraic topology.Using homogeneous coordinates, a non-zero quadratic form in n variables defines an (n−2)-dimensional quadric in the (n−1)-dimensional projective space. This is a basic construction in projective geometry. In this way one may visualize 3-dimensional real quadratic forms as conic sections.A closely related notion with geometric overtones is a quadratic space, which is a pair (V, q), with V a vector space over a field K, and q : V → K a quadratic form on V. An example is given by the three-dimensional Euclidean space and the square of the Euclidean norm expressing the distance between a point with coordinates (x, y, z) and the origin:The study of particular quadratic forms, in particular the question of whether a given integer can be the value of a quadratic form over the integers, dates back many centuries. One such case is Fermat's theorem on sums of two squares, which determines when an integer may be expressed in the form x2 + y2, where x, y are integers. This problem is related to the problem of finding Pythagorean triples, which appeared in the second millennium B.C.[3]In 628, the Indian mathematician Brahmagupta wrote Brāhmasphuṭasiddhānta which includes, among many other things, a study of equations of the form x2 − ny2 = c. In particular he considered what is now called Pell's equation, x2 − ny2 = 1, and found a method for its solution.[4] In Europe this problem was studied by Brouncker, Euler and Lagrange.In 1801 Gauss published Disquisitiones Arithmeticae, a major portion of which was devoted to a complete theory of binary quadratic forms over the integers. Since then, the concept has been generalized, and the connections with quadratic number fields, the modular group, and other areas of mathematics have been further elucidated.Any n×n real symmetric matrix A determines a quadratic form qA in n variables by the formulaConversely, given a quadratic form in n variables, its coefficients can be arranged into an n × n symmetric matrix. An important question in the theory of quadratic forms is how to simplify a quadratic form q by a homogeneous linear change of variables.  A fundamental theorem due to Jacobi asserts that a real quadratic form q has an orthogonal diagonalization.[5]so that the corresponding symmetric matrix is diagonal, and this is accomplished with a change of variables given by an orthogonal matrix – in this case the coefficients λ1, λ2, ..., λn are determined uniquely up to a permutation.There always exists a change of variables given by an invertible matrix, not necessarily orthogonal, such that the coefficients λi are 0, 1, and −1. Sylvester's law of inertia states that the numbers of each 1 and −1 are invariants of the quadratic form, in the sense that any other diagonalization will contain the same number of each. The signature of the quadratic form is the triple (n0, n+, n−), where n0 is the number of 0s and n± is the number of ±1s. Sylvester's law of inertia shows that this is a well-defined quantity attached to the quadratic form. The case when all λi have the same sign is especially important: in this case the quadratic form is called positive definite (all 1) or negative definite (all −1).  If none of the terms are 0, then the form is called nondegenerate; this includes positive definite, negative definite, and indefinite (a mix of 1 and −1); equivalently, a nondegenerate quadratic form is one whose associated symmetric form is a nondegenerate bilinear form. A real vector space with an indefinite nondegenerate quadratic form of index (p, q) (denoting p 1s and q −1s) is often denoted as Rp,q particularly in the physical theory of spacetime.These results are reformulated in a different way below.Let q be a quadratic form defined on an n-dimensional real vector space. Let A be the matrix of the quadratic form q in a given basis. This means that A is a symmetric n × n matrix such thatwhere x is the column vector of coordinates of v in the chosen basis. Under a change of basis, the column x is multiplied on the left by an n × n invertible matrix S, and the symmetric square matrix A is transformed into another symmetric square matrix B of the same size according to the formulaAny symmetric matrix A can be transformed into a diagonal matrixby a suitable choice of an orthogonal matrix S, and the diagonal entries of B are uniquely determined – this is Jacobi's theorem. If S is allowed to be any invertible matrix then B can be made to have only 0,1, and −1 on the diagonal, and the number of the entries of each type (n0 for 0, n+ for 1, and n− for −1) depends only on A. This is one of the formulations of Sylvester's law of inertia and the numbers n+ and n− are called the positive and negative indices of inertia. Although their definition involved a choice of basis and consideration of the corresponding real symmetric matrix A, Sylvester's law of inertia means that they are invariants of the quadratic form q.The quadratic form q is positive definite (resp., negative definite) if q(v) > 0 (resp., q(v) < 0) for every nonzero vector v.[6] When q(v) assumes both positive and negative values, q is an indefinite quadratic form. The theorems of Jacobi and Sylvester show that any positive definite quadratic form in n variables can be brought to the sum of n squares by a suitable invertible linear transformation: geometrically, there is only one positive definite real quadratic form of every dimension. Its isometry group is a compact orthogonal group O(n). This stands in contrast with the case of indefinite forms, when the corresponding group, the indefinite orthogonal group O(p, q), is non-compact. Further, the isometry groups of Q and −Q are the same (O(p, q) ≈ O(q, p)), but the associated Clifford algebras (and hence pin groups) are different.An n-ary quadratic form over a field K is a homogeneous polynomial of degree 2 in n variables with coefficients in K:This formula may be rewritten using matrices: let x be the column vector with components x1, ..., xn and A = (aij) be the n×n matrix over K whose entries are the coefficients of q. ThenTwo n-ary quadratic forms φ and ψ over K are equivalent if there exists a nonsingular linear transformation C ∈ GL(n, K) such thatLet the characteristic of K be different from 2.[7] The coefficient matrix A of q may be replaced by the symmetric matrix (A + AT)/2 with the same quadratic form, so it may be assumed from the outset that A is symmetric. Moreover, a symmetric matrix A is uniquely determined by the corresponding quadratic form. Under an equivalence C, the symmetric matrix A of φ and the symmetric matrix B of ψ are related as follows:The associated bilinear form of a quadratic form q is defined byThus, bq is a symmetric bilinear form over K with matrix A. Conversely, any symmetric bilinear form b defines a quadratic formand these two processes are the inverses of one another.  As a consequence, over a field of characteristic not equal to 2, the theories of symmetric bilinear forms and of quadratic forms in n variables are essentially the same.A quadratic form q in n variables over K induces a map from the n-dimensional coordinate space Kn into K:The map Q is a homogeneous function of degree 2, which means that it has the property that, for all a in K and v in V:When the characteristic of K is not 2, the map B : V × V → K defined below is bilinear over K:This bilinear form B is symmetric, i.e. B(x, y) = B(y, x) for all x, y in V, and it determines Q: Q(x) = B(x, x)  for all x in V.When the characteristic of K is 2, so that 2 is not a unit, it is still possible to use a quadratic form to define a symmetric bilinear form B′(x, y) = Q(x + y) − Q(x) − Q(y). However, Q(x) can no longer be recovered from this B′ in the same way, since B′(x, x) = 0 for all x (and is thus alternating[8]). Alternately, there always exists a bilinear form B″ (not in general either unique or symmetric) such that B″(x, x) = Q(x).The pair (V, Q) consisting of a finite-dimensional vector space V over K and a quadratic map Q from V to K is called a quadratic space, and B as defined here is the associated symmetric bilinear form of Q. The notion of a quadratic space is a coordinate-free version of the notion of quadratic form. Sometimes, Q is also called a quadratic form.Two n-dimensional quadratic spaces (V, Q) and (V′, Q′) are isometric if there exists an invertible linear transformation T : V → V′ (isometry) such thatThe isometry classes of n-dimensional quadratic spaces over K correspond to the equivalence classes of n-ary quadratic forms over K.Let R be a commutative ring, M be an R-module and b : M × M → R be an R-bilinear form.[9]  A mapping Q : M → R : v ↦ b(v, v) is the associated quadratic form of b, and B : M × M → R : (u, v) ↦ Q(u + v) − Q(u) − Q(v) is the polar form of Q.Alternatively, a quadratic form Q : M → R may be characterized as follows:Two elements v and w of V are called orthogonal if B(v, w) = 0. The kernel of a bilinear form B consists of the elements that are orthogonal to every element of V. Q is non-singular if the kernel of its associated bilinear form is {0}. If there exists a non-zero v in V such that Q(v) = 0, the quadratic form Q is isotropic, otherwise it is anisotropic. This terminology also applies to vectors and subspaces of a quadratic space. If the restriction of Q to a subspace U of V is identically zero, U is totally singular.The orthogonal group of a non-singular quadratic form Q is the group of the linear automorphisms of V that preserve Q, i.e. the group of isometries of (V, Q) into itself.If a quadratic space (A, Q) has a product so that A is an algebra over a field, and satisfiesEvery quadratic form q in n variables over a field of characteristic not equal to 2 is equivalent to a diagonal formSuch a diagonal form is often denoted byClassification of all quadratic forms up to equivalence can thus be reduced to the case of diagonal forms.Quadratic forms over the ring of integers are called integral quadratic forms, whereas the corresponding modules are quadratic lattices (sometimes, simply lattices). They play an important role in number theory and topology.An integral quadratic form has integer coefficients, such as x2 + xy + y2; equivalently, given a lattice Λ in a vector space V (over a field with characteristic 0, such as Q or R), a quadratic form Q is integral with respect to Λ if and only if it is integer-valued on Λ, meaning Q(x, y) ∈ Z if x, y ∈ Λ.This is the current use of the term; in the past it was sometimes used differently, as detailed below.Historically there was some confusion and controversy over whether the notion of integral quadratic form should mean:This debate was due to the confusion of quadratic forms (represented by polynomials) and symmetric bilinear forms (represented by matrices), and "twos out" is now the accepted convention; "twos in" is instead the theory of integral symmetric bilinear forms (integral symmetric matrices).this is the convention Gauss uses in Disquisitiones Arithmeticae.Several points of view mean that twos out has been adopted as the standard convention. Those include:There are also forms whose image consists of all but one of the positive integers.  For example, {1,2,5,5} has 15 as the exception.  Recently, the 15 and 290 theorems have completely characterized universal integral quadratic forms: if all coefficients are integers, then it represents all positive integers if and only if it represents all integers up through 290; if it has an integral matrix, it represents all positive integers if and only if it represents all integers up through 15.
Linear inequality
In mathematics  a linear inequality is an inequality which involves a linear function. A linear inequality contains one of the symbols of inequality:[1]. It shows the data which is not equal in graph form.A linear inequality looks exactly like a linear equation, with the inequality sign replacing the equality sign.Two-dimensional linear inequalities are expressions in two variables of the form:where the inequalities may either be strict or not. The solution set of such an inequality can be graphically represented by a half-plane (all the points on one "side" of a fixed line) in the Euclidean plane.[2] The line that determines the half-planes (ax + by = c) is not included in the solution set when the inequality is strict. A simple procedure to determine which half-plane is in the solution set is to calculate the value of ax + by at a point (x0, y0) which is not on the line and observe whether or not the inequality is satisfied.For example,[3] to draw the solution set of x + 3y < 9, one first draws the line with equation x + 3y = 9 as a dotted line, to indicate that the line is not included in the solution set since the inequality is strict. Then, pick a convenient point not on the line, such as (0,0). Since 0 + 3(0) = 0 < 9, this point is in the solution set, so the half-plane containing this point (the half-plane "below" the line) is the solution set of this linear inequality.In Rn linear inequalities are the expressions that may be written in the formMore concretely, this may be written out asorAlternatively, these may be written aswhere g is an affine function.[4]That isorNote that any inequality containing a "greater than" or a "greater than or equal" sign can be rewritten with a "less than" or "less than or equal" sign, so there is no need to define linear inequalities using those signs.A system of linear inequalities is a set of linear inequalities in the same variables:This can be concisely written as the matrix inequalitywhere A is an m×n matrix, x is an n×1 column vector of variables, and b is an m×1 column vector of constants.In the above systems both strict and non-strict inequalities may be used.The set of solutions of a real linear inequality constitutes a half-space of the 'n'-dimensional real space, one of the two defined by the corresponding linear equation.The set of solutions of a system of linear inequalities corresponds to the intersection of the half-spaces defined by individual inequalities. It is a convex set, since the half-spaces are convex sets, and the intersection of a set of convex sets is also convex. In the non-degenerate cases this convex set is a convex polyhedron (possibly unbounded, e.g., a half-space, a slab between two parallel half-spaces or a polyhedral cone). It may also be empty or a convex polyhedron of lower dimension confined to an affine subspace of the n-dimensional space Rn.A linear programming problem seeks to optimize (find a maximum or minimum value) a function (called the objective function) subject to a number of constraints on the variables which, in general, are linear inequalities.[5] The list of constraints is a system of linear inequalities.The above definition requires well-defined operations of addition, multiplication and comparison; therefore, the notion of a linear inequality may be extended to ordered rings, and in particular to ordered fields.
Majorization
The majorization partial order on finite sets, described here, can be generalized to the Lorenz ordering, a partial order on distribution functions.Various generalizations of majorization are discussed in chapters 14 and 15 of the reference work Inequalities: Theory of Majorization and Its Applications. Albert W. Marshall, Ingram Olkin, Barry Arnold. Second edition. Springer Series in Statistics. Springer, New York, 2011. .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output q{quotes:"\"""\"""'""'"}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-limited a,.mw-parser-output .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}ISBN 978-0-387-40087-7
Rotation of axes
Coordinate systems are essential for studying the equations of curves using the methods of analytic geometry.  To use the method of coordinate geometry, the axes are placed at a convenient position with respect to the curve under consideration.  For example, to study the equations of ellipses and hyperbolas, the foci are usually located on one of the axes and are situated symmetrically with respect to the origin.  If the curve (hyperbola, parabola, ellipse, etc.) is not situated conveniently with respect to the axes, the coordinate system should be changed to place the curve at a convenient and familiar location and orientation.  The process of making this change is called a transformation of coordinates.[6]The solutions to many problems can be simplified by rotating the coordinate axes to obtain new axes through the same origin.We have    (1)    (2)and    (3)    (4)Substituting equations (1) and (2) into equations (3) and (4), we obtain    (5)    (6)Equations (5) and (6) can be represented in matrix form aswhich is the standard matrix equation of a rotation of axes in two dimensions.[8]The inverse transformation is    (7)    (8)orSolution:Solution:The most general equation of the second degree has the form    (9)Through a change of coordinates (a rotation of axes and a translation of axes), equation (9) can be put into a standard form, which is usually easier to work with.  It is always possible to rotate the coordinates in such a way that in the new system there is no x'y' term.  Substituting equations (7) and (8) into equation (9), we obtain    (10)where    (11)When a problem arises with B, D and E all different from zero, they can be eliminated by performing in succession a rotation (eliminating B) and a translation (eliminating the D and E terms).[12]Solution:
Invertible matrix
In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such thatwhere In denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A−1.A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.Non-square matrices (m-by-n matrices for which m ≠ n) do not have an inverse.  However, in some cases such a matrix may have a left inverse or right inverse.  If A is m-by-n and the rank of A is equal to n, then A has a left inverse: an n-by-m matrix B such that BA = In.  If A has rank m, then it has a right inverse: an n-by-m matrix B such that AB = Im.Matrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.While the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. However, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. For a noncommutative ring, the usual determinant is not defined.  The conditions for existence of left-inverse or right-inverse are more complicated since a notion of rank does not exist over rings.The set of n × n invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree n.Let A be a square n by n matrix over a field K (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:Furthermore, the following properties hold for an invertible matrix A:A matrix that is its own inverse, i.e. such that A = A−1 and A2 = I, is called an involutory matrix.It follows from the associativity of matrix multiplication that iffor finite square matrices A and B, then alsoOver the field of real numbers, the set of singular n-by-n matrices, considered as a subset of Rn×n, is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all n-by-n matrices are invertible.Furthermore, the n-by-n invertible matrices are a dense open set in the topological space of all n-by-n matrices.  Equivalently, the set of singular matrices is closed and nowhere dense in the space of n-by-n matrices.In practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.Consider the following 2-by-2 matrix:As an example of a non-invertible, or singular, matrix, consider the matrixGauss-Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.A generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:Victor Pan and John Reif have done work that includes ways of generating a starting seed.[2][3] Byte magazine summarised one of their approaches.[4]Newton's method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton's method is also useful for "touch up" corrections to the Gauss–Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given byIf matrix A is positive definite, then its inverse can be obtained aswhere L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.Writing the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of small matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:so thatwhere |A| is the determinant of A, C is the matrix of cofactors, and CT represents the matrix transpose.The cofactor equation listed above yields the following result for 2 × 2 matrices. Inversion of these matrices can be done as follows:[6]This is possible because 1/(ad − bc) is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.The Cayley–Hamilton method givesA computationally efficient 3 × 3 matrix inversion is given by(where the scalar A is not to be confused with the matrix A).If the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given byThe determinant of A can be computed by applying the rule of Sarrus as follows:The Cayley–Hamilton decomposition givesWith increasing dimension, expressions for the inverse of A get complicated. For n = 4, the Cayley–Hamilton method leads to an expression that is still tractable:Matrices can also be inverted blockwise by using the following analytic inversion formula:    ( 1)where A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and D − CA−1B must be nonsingular.[7]) This strategy is particularly advantageous if A is diagonal and D − CA−1B (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion.This technique was reinvented several times and is due to Hans Boltz (1923),[citation needed] who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.The nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.The inversion procedure that led to Equation (1) performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and A − BD−1C are nonsingular,[8] the result is    ( 2)Equating Equations (1) and (2) leads to    ( 3)where Equation (3) is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.Since a blockwise inversion of an n × n matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.[9] There exist matrix multiplication algorithms with a complexity of O(n2.3727) operations, while the best proven lower bound is Ω(n2 log n).[10]If a matrix A has the property thatthen A is nonsingular and its inverse may be expressed by a Neumann series:[11]Truncating the sum results in an "approximate" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfies More generally, if A is "near" the invertible matrix X in the sense thatthen A is nonsingular and its inverse isIf it is also the case that A − X has rank 1 then this simplifies toSuppose that the invertible matrix A depends on a parameter t. Then the derivative of the inverse of A with respect to t is given byMore generally, ifthen,Therefore,Some of the properties of inverse matrices are shared by generalized inverses (e.g., the Moore–Penrose inverse), which can be defined for any m-by-n matrix.For most practical applications, it is not necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it is necessary that the matrix involved be invertible.Decomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.Although an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.Matrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.Matrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.
Unit vector
The same construct is used to specify spatial directions in 3D.  As illustrated, each unique direction is equivalent numerically to a point on the unit sphere.The normalized vector or versor û of a non-zero vector u is the unit vector in the direction of u, i.e.,where |u| is the norm (or length) of u.  The term normalized vector is sometimes used as a synonym for unit vector.Unit vectors are often chosen to form the basis of a vector space. Every vector in the space may be written as a linear combination of unit vectors.By definition, in a Euclidean space the dot product of two unit vectors is a scalar value amounting to the cosine of the smaller subtended angle. In three-dimensional Euclidean space, the cross product of two arbitrary unit vectors is a third vector orthogonal to both of them having length equal to the sine of the smaller subtended angle. The normalized cross product corrects for this varying length, and yields the mutually orthogonal unit vector to the two inputs, applying the right-hand rule to resolve one of two possible directions.Unit vectors may be used to represent the axes of a Cartesian coordinate system. For instance, the unit vectors in the direction of the x, y, and z axes of a three dimensional Cartesian coordinate system areThey are sometimes referred to as the versors of the coordinate system, and they form a set of mutually orthogonal unit vectors, typically referred to as a standard basis in linear algebra.When a unit vector in space is expressed, with Cartesian notation, as a linear combination of i, j, k, its three scalar components can be referred to as direction cosines. The value of each component is equal to the cosine of the angle formed by the unit vector with the respective basis vector. This is one of the methods used to describe the orientation (angular position) of a straight line, segment of straight line, oriented axis, or segment of oriented axis (vector).The three orthogonal unit vectors appropriate to cylindrical symmetry are: Common general themes of unit vectors occur throughout physics and geometry:[2]Unit vector at acute deviation angle φ (including 0 or π/2 rad) relative to a principal direction.
Polynomial
In mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate, x, is x2 − 4x + 7. An example in three variables is x3 + 2xyz2 − yz + 1.Polynomials appear in many areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.The word polynomial joins two diverse roots: the Greek poly, meaning "many," and the Latin nomen, or name. It was derived from the term binomial by replacing the Latin root bi- with the Greek poly-. The word polynomial was first used in the 17th century.[1]The x occurring in a polynomial is commonly called either a variable or an indeterminate. When the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is "indeterminate"). It is thus more correct to call it an "indeterminate".[citation needed] However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a "variable". Many authors use these two words interchangeably.It is a common usage to use uppercase letters for the indeterminates and the corresponding lowercase letters for the variables (arguments) of the associated function.A polynomial P in the indeterminate x is commonly denoted either as P or as P(x). Formally, the name of the polynomial is P, not P(x), but the use of the functional notation P(x) date from the time where the distinction between a polynomial and the associated function was unclear. Moreover the functional notation is often useful for specifying, in a single phrase, a polynomial and its indeterminate. For example, "let P(x) be a polynomial" is a shorthand for "let P be a polynomial in the indeterminate x". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.The ambiguity of having two notations for a single mathematical object may be formally resolved by considering the general meaning of the functional notation for polynomials.If a denotes a number, a variable, another polynomial, or, more generally any expression, then P(a) denotes, by convention, the result of substituting a for x in P. Thus, the polynomial P defines the functionwhich is the polynomial function associated to P.Frequently, when using this notation, one supposes that a is a number. However one may use it over any domain where addition and multiplication are defined (that is, any ring). In particular, if a is a polynomial then P(a) is also a polynomial.More specifically, when a is the indeterminate x, then the image of x by this function is the polynomial P itself (substituting x to x does not change anything). In other words,which justifies formally the existence of two notations for the same polynomial.A polynomial is an expression that can be built from constants and symbols called indeterminates or variables by means of addition, multiplication and exponentiation to a non-negative integer power. Two such expressions that may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication are considered as defining the same polynomial.A polynomial in a single indeterminate x can always be written (or rewritten) in the formThis can be expressed more concisely by using summation notation:That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number—called the coefficient of the term[2]—and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because x = x1, the degree of an indeterminate without a written exponent is one.A term with no indeterminates and a polynomial with no indeterminates are called, respectively, a constant term and a constant polynomial.[3] The degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial, 0, (which has no terms at all) is generally treated as not defined (but see below).[4]For example:is a term. The coefficient is −5, the indeterminates are x and y, the degree of x is two, while the degree of y is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is 2 + 1 = 3.Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.Polynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials. For higher degrees the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in x2 + 2x + 1 the term 2x is a linear term in a quadratic polynomial.The polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either −1 or −∞).[5] These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. The graph of the zero polynomial, f(x) = 0, is the X-axis.In the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of degree n if all its non-zero terms have degree n. The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined.[6] For example, x3y2 + 7x2y3 − 3x5 is homogeneous of degree 5. For more details, see homogeneous polynomial.The commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in "descending powers of x", with the term of largest degree first, or in "ascending powers of x". The polynomial in the example above is written in descending powers of x. The first term has coefficient 3, indeterminate x, and exponent 2. In the second term, the coefficient is −5. The third term is a constant. Because the degree of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.[7]Two terms with the same indeterminates raised to the same powers are called "similar terms" or "like terms", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0.[8] Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial,[9] a two-term polynomial is called a binomial, and a three-term polynomial is called a trinomial. The term "quadrinomial" is occasionally used for a four-term polynomial.A real polynomial is a polynomial with real coefficients. The argument of the polynomial is not necessarily so restricted, for instance the s-plane variable in Laplace transforms. A real polynomial function is a function from the reals to the reals that is defined by a real polynomial. Similarly, an integer polynomial is a polynomial with integer coefficients, and a complex polynomial is a polynomial with complex coefficients.A polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply "polynomials in x, y, and z", listing the indeterminates allowed.The evaluation of a polynomial consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:Polynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms.[8][10] For example, ifthenwhich can be simplified toTo work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other.[8] For example, ifthenwhich can be simplified toPolynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of f(x) by (x − a) is f(a); see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.As for the integers, two kinds of divisions are considered for the polynomials. The Euclidean division of polynomials that generalizes the Euclidean division of the integers. It results in two polynomials, a quotient and a remainder that are characterized by the following property of the polynomials: given two polynomials a and b such that b ≠ 0, there exists a unique pair of polynomials, q, the quotient, and r, the remainder, such that a = b q + r and degree(r) < degree(b) (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.[12]All polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree.[13] For example, the factored form ofisover the integers and the reals andover the complex numbers.The computation of the factored form, called factorization is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.A formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a "rational expression" or "rational fraction" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example, x3/12 is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to (1/12)x3 and 1/12 is just a constant. When this expression is used as a term, its coefficient is therefore 1/12. For similar reasons, if complex coefficients are allowed, one may have a single term like (2 + 3i) x3; even though it looks like it should be expanded to two terms, the complex number 2 + 3i is one complex number, and is the coefficient of that term. The expression 1/(x2 + 1) is not a polynomial because it includes division by a non-constant polynomial. The expression (5 + y)x is not a polynomial, because it contains an indeterminate used as exponent.Because subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.A polynomial function is a function that can be defined by evaluating a polynomial. More precisely, a function f of one argument from a given domain is a polynomial function if there exists a polynomialGenerally, unless otherwise specified, polynomial functions have complex coefficients, arguments, and values. In particular, a polynomial, restricted to have real coefficients, defines a function from the complex numbers to the complex numbers. If the domain of this function is also restricted to the reals, the resulting function maps reals to reals.For example, the function f, defined byis a polynomial function of one variable. Polynomial functions of several variables are similarly defined, using polynomials in more than one indeterminate, as inEvery polynomial function is continuous, smooth, and entire. Polynomial of degree 2:f(x) = x2 − x − 2= (x + 1)(x − 2)Polynomial of degree 3:f(x) = x3/4 + 3x2/4 − 3x/2 − 2= 1/4 (x + 4)(x + 1)(x − 2)Polynomial of degree 4:f(x) = 1/14 (x + 4)(x + 1)(x − 1)(x − 3) + 0.5Polynomial of degree 5:f(x) = 1/20 (x + 4)(x + 2)(x + 1 )(x − 1)(x − 3)+ 2Polynomial of degree 6:f(x) = 1/100 (x6 − 2x 5 − 26x4 + 28x3+ 145x2 - 26x - 80)Polynomial of degree 7:f(x) = (x − 3)(x − 2)(x − 1)(x)(x + 1)(x + 2)(x + 3)A polynomial function in one real variable can be represented by a graph.A non-constant polynomial function tends to infinity when the variable increases indefinitely (in absolute value). If the degree is higher than one, the graph does not have any asymptote. It has two parabolic branches with vertical direction (one branch for positive x and one for negative x).Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.A polynomial equation, also called algebraic equation, is an equation of the form[14]For example,is a polynomial equation.When considering equations, the indeterminates (variables) of polynomials are also called unknowns, and the solutions are the possible values of the unknowns for which the equality is true (in general more than one solution may exist). A polynomial equation stands in contrast to a polynomial identity like (x + y)(x − y) = x2 − y2, where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.In elementary algebra, methods such as the quadratic formula are taught for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel–Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.The number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.A number a is a root of a polynomial P if and only if the linear polynomial x − a divides P, that is if there is another polynomial Q such that P = (x – a) Q. It may happen that x − a divides P more than once: if (x − a)2 divides P then a is called a multiple root of P, and otherwise a is called a simple root of P. If P is a nonzero polynomial, there is a highest power m such that (x − a)m divides P, which is called the multiplicity of the root a in P. When P is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots, as, with the above definitions, every number is a root of the zero polynomial, with an undefined multiplicity. With this exception made, the number of roots of P, even counted with their respective multiplicities, cannot exceed the degree of P.[15] The relation between the coefficients of a polynomial and its roots is described by Vieta's formulas.Some polynomials, such as x2 + 1, do not have any roots among the real numbers. If, however, the set of accepted solutions is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors x − a, one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree 1; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.When there is no algebraic expression for the roots, and when such an algebraic expression exists but is too complicated to be useful, the unique way of solving is to compute numerical approximations of the solutions.[16] There are many methods for that; some are restricted to polynomials and others may apply to any continuous function. The most efficient algorithms allow solving easily (on a computer) polynomial equations of degree higher than 1,000 (see Root-finding algorithm).For polynomials in more than one indeterminate, the combinations of values for the variables for which the polynomial function takes the value zero are generally called zeros instead of "roots". The study of the sets of zeros of polynomials is the object of algebraic geometry. For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions, and, if this number is finite, for computing the solutions. See System of polynomial equations.The special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.A polynomial equation for which one is interested only in the solutions which are integers is called a Diophantine equation. Solving Diophantine equations is generally a very hard task. It has been proved that there cannot be any general algorithm for solving them, and even for deciding whether the set of solutions is empty (see Hilbert's tenth problem). Some of the most famous problems that have been solved during the fifty last years are related to Diophantine equations, such as Fermat's Last Theorem.There are several generalizations of the concept of polynomials.A trigonometric polynomial is a finite linear combination of functions sin(nx) and cos(nx) with n taking on the values of one or more natural numbers.[17] The coefficients may be taken as real numbers, for real-valued functions.If sin(nx) and cos(nx) are expanded in terms of sin(x) and cos(x), a trigonometric polynomial becomes a polynomial in the two variables sin(x) and cos(x) (using List of trigonometric identities#Multiple-angle formulae). Conversely, every polynomial in sin(x) and cos(x) may be converted, with Product-to-sum identities, into a linear combination of functions sin(nx) and cos(nx). This equivalence explains why linear combinations are called polynomials.For complex coefficients, there is no difference between such a function and a finite Fourier series.Trigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.A matrix polynomial is a polynomial with square matrices as variables.[18] Given an ordinary, scalar-valued polynomialthis polynomial evaluated at a matrix A iswhere I is the identity matrix.[19]A matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices A in a specified matrix ring Mn(R).Laurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.A rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.The rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.Formal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like irrational numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.The simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone–Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.Calculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial functionthe derivative with respect to x isand the indefinite integral isIn abstract algebra, one distinguishes between polynomials and polynomial functions. A polynomial f in one indeterminate x over a ring R is defined as a formal expression of the formwhere n is a natural number, the coefficients a0, . . ., an are elements of R, and x is a formal symbol, whose powers xi are just placeholders for the corresponding coefficients ai, so that the given formal expression is just a way to encode the sequence (a0, a1, . . .), where there is an n such that ai = 0 for all i > n. Two polynomials sharing the same value of n are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of n obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term aixi is interpreted as a polynomial that has zero coefficients at all powers of x other than xi. Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the ruleThus the set of all polynomials with coefficients in the ring R forms itself a ring, the ring of polynomials over R, which is denoted by R[x]. The map from R to R[x] sending r to rx0 is an injective homomorphism of rings, by which R is viewed as a subring of R[x]. If R is commutative, then R[x] is an algebra over R.One can think of the ring R[x] as arising from R by adding one new element x to R, and extending in a minimal way to a ring in which x satisfies no other relations than the obligatory ones, plus commutation with all elements of R (that is xr = rx). To do this, one must add all powers of x and their linear combinations as well.Formation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring R[x] over the real numbers by factoring out the ideal of multiples of the polynomial x2 + 1. Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring R (see modular arithmetic).If R is commutative, then one can associate to every polynomial P in R[x], a polynomial function f with domain and range equal to R (more generally one can take domain and range to be the same unital associative algebra over R). One obtains the value f(r) by substitution of the value r for the symbol x in P. One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p). This is not the case when R is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for x.In commutative algebra, one major focus of study is divisibility among polynomials. If R is an integral domain and f and g are polynomials in R[x], it is said that f divides g or f is a divisor of g if there exists a polynomial q in R[x] such that f q = g. One can show that every zero gives rise to a linear divisor, or more formally, if f is a polynomial in R[x] and r is an element of R such that f(r) = 0, then the polynomial (x − r) divides f. The converse is also true. The quotient can be computed using the polynomial long division.[20][21]If F is a field and f and g are polynomials in F[x] with g ≠ 0, then there exist unique polynomials q and r in F[x] withand such that the degree of r is smaller than the degree of g (using the convention that the polynomial 0 has a negative degree). The polynomials q and r are uniquely determined by f and g. This is called Euclidean division, division with remainder or polynomial long division and shows that the ring F[x] is a Euclidean domain.Analogously, prime polynomials (more correctly, irreducible polynomials) can be defined as non-zero polynomials which cannot be factorized into the product of two non-constant polynomials. In the case of coefficients in a ring, "non-constant" must be replaced by "non-constant or non-unit" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non-unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand-written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.Polynomials serve to approximate other functions,[22] such as the use of splines.Polynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.The term "polynomial", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase polynomial time means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.Determining the roots of polynomials, or "solving algebraic equations", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins "Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou." We would write 3x + 2y + z = 29.The earliest known use of the equal sign is in Robert Recorde's The Whetstone of Witte, 1557. The signs + for addition, − for subtraction, and the use of a letter for an unknown appear in Michael Stifel's Arithemetica integra, 1544. René Descartes, in La géometrie, 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the a's denote constants and x denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.[23]
Weyl's inequality
In mathematics, there are at least two results known as Weyl's inequality.In number theory, Weyl's inequality, named for Hermann Weyl, states that if M, N, a and q are integers, with a and q coprime, q > 0, and f is a real polynomial of degree k whose leading coefficient c satisfiesThis inequality will only be useful whenThe theorem says that if any two of M, H and P are n by n Hermitian matrices, where M has eigenvaluesand H has eigenvaluesand P has eigenvaluesNote that we can order the eigenvalues because the matrices are Hermitian and therefore the eigenvalues are real.The singular values {σk} of a square matrix M are the square roots of eigenvalues of M*M (equivalently MM*). Since Hermitian matrices follow Weyl's inequality, if we take any matrix A then its singular values will be the square root of the eigenvalues of B=A*A which is a Hermitian matrix. Now since Weyl's inequality hold for B, therefore for the singular values of A.[3]This result gives the bound for the perturbation in the singular values of a matrix A  due to perturbation in A.
Geometry
Geometry (from the Ancient Greek: γεωμετρία; geo- "earth", -metron "measurement") is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. A mathematician who works in the field of geometry is called a geometer.Geometry arose independently in a number of early cultures as a practical way for dealing with lengths, areas, and volumes. Geometry began to see elements of formal mathematical science emerging in the West as early as the 6th century BC.[1] By the 3rd century BC, geometry was put into an axiomatic form by Euclid, whose treatment, Euclid's Elements, set a standard for many centuries to follow.[2] Geometry arose independently in India, with texts providing rules for geometric constructions appearing as early as the 3rd century BC.[3] Islamic scientists preserved Greek ideas and expanded on them during the Middle Ages.[4] By the early 17th century, geometry had been put on a solid analytic footing by mathematicians such as René Descartes and Pierre de Fermat. Since then, and into modern times, geometry has expanded into non-Euclidean geometry and manifolds, describing spaces that lie beyond the normal range of human experience.[5]While geometry has evolved significantly throughout the years, there are some general concepts that are more or less fundamental to geometry. These include the concepts of points, lines, planes, surfaces, angles, and curves, as well as the more advanced notions of manifolds and topology or metric.[6]Geometry has applications to many fields, including art, architecture, physics, as well as to other branches of mathematics.Contemporary geometry has many subfields:The earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC.[8][9] Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000–1800 BC) and Moscow Papyrus (c. 1890 BC), the Babylonian clay tablets such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum.[10] Later clay tablets (350–50 BC) demonstrate that Babylonian astronomers implemented trapezoid procedures for computing Jupiter's position and motion within time-velocity space. These geometric procedures anticipated the Oxford Calculators, including the mean speed theorem, by 14 centuries.[11] South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.[12][13]In the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore.  He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.[1]  Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem,[14]  though the statement of the theorem has a long history.[15][16] Eudoxus (408–c. 355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures,[17] as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time,[18] introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[19] The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today.[20] Archimedes (c. 287–212 BC) of Syracuse used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of Pi.[21] He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.Indian mathematicians also made many important contributions in geometry. The Satapatha Brahmana (3rd century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras.[3] According to (Hayashi 2005, p. 363), the Śulba Sūtras contain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples,[22] which are particular cases of Diophantine equations.[23]In the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero."[24] Aryabhata's Aryabhatiya (499) includes the computation of areas and volumes.Brahmagupta wrote his astronomical work Brāhma Sphuṭa Siddhānta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain).[25] In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).[25]In the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry.[26][27] Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra.[28] Thābit ibn Qurra (known as Thebit in Latin) (836–901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry.[4] Omar Khayyám (1048–1131) found geometric solutions to cubic equations.[29]  The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were early results in hyperbolic geometry, and along with their alternative postulates, such as Playfair's axiom, these works had a considerable influence on the development of non-Euclidean geometry among later European geometers, including Witelo (c. 1230–c. 1314), Gersonides (1288–1344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri.[30]In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661). Projective geometry is a geometry without measurement or parallel lines, just the study of how points are related to each other.Two developments in geometry in the 19th century changed the way it had been studied previously. These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, János Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen Programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826–1866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincaré, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of "space" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.The following are some of the most important concepts in geometry.[6][7]Euclid took an abstract approach to geometry in his Elements, one of the most influential books ever written. Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes. He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry. At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792–1856), János Bolyai (1802–1860), Carl Friedrich Gauss (1777–1855) and others led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862–1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.Points are considered fundamental objects in Euclidean geometry. They have been defined in a variety of ways, including Euclid's definition as 'that which has no part'[31] and through the use of algebra or nested sets.[32] In many areas of geometry, such as analytic geometry, differential geometry, and topology, all objects are considered to be built up from points. However, there has been some study of geometry without reference to points.[33]Euclid described a line as "breadthless length" which "lies equally with respect to the points on itself".[31] In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation,[34] but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.[35] In differential geometry, a  geodesic is a generalization of the notion of a line  to curved spaces.[36]A plane is a flat, two-dimensional surface that extends infinitely far.[31] Planes are used in every area of geometry. For instance, planes can be studied as a topological surface without reference to distances or angles;[37] it can be studied as an affine space, where collinearity and ratios can be studied but not distances;[38] it can be studied as the complex plane using techniques of complex analysis;[39] and so on.Euclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other.[31] In modern terms, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.[40]In Euclidean geometry, angles are used to study polygons and triangles, as well as forming an object of study in their own right.[31] The study of the angles of a triangle or of angles in a unit circle forms the basis of trigonometry.[41]In differential geometry and calculus, the angles between plane curves or space curves or surfaces can be calculated using the derivative.[42][43]A curve is a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are called plane curves and those in 3-dimensional space are called space curves.[44]In topology, a curve is defined by a function from an interval of the real numbers to another space.[37] In differential geometry, the same definition is used, but the defining function is required to be differentiable [45] Algebraic geometry studies algebraic curves, which are defined as algebraic varieties of dimension one.[46]A surface is a two-dimensional object, such as a sphere or paraboloid.[47] In differential geometry[45] and topology,[37] surfaces are described by two-dimensional 'patches' (or neighborhoods) that are assembled by diffeomorphisms or homeomorphisms, respectively. In algebraic geometry, surfaces are described by polynomial equations.[46]A manifold is a generalization of the concepts of curve and surface. In topology, a manifold is a topological space where every point has a neighborhood that is homeomorphic to Euclidean space.[37] In differential geometry, a differentiable manifold is a space where each neighborhood is diffeomorphic to Euclidean space.[45]Manifolds are used extensively in physics, including in general relativity and string theory[48]A topology is a mathematical structure on a set that tells how elements of the set relate spatially to each other.[37] The best-known examples of topologies come from metrics, which are ways of measuring distances between points.[49] For instance, the Euclidean metric measures the distance between points in the Euclidean plane, while the hyperbolic metric measures the distance in the hyperbolic plane. Other important examples of metrics include the Lorentz metric of special relativity and the semi-Riemannian metrics of general relativity.[50]Classical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments allowed in geometric constructions are the compass and straightedge. Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using parabolas and other curves, as well as mechanical devices, were found.Where the traditional geometry allowed dimensions 1 (a line), 2 (a plane) and 3 (our ambient world conceived of as three-dimensional space), mathematicians have used higher dimensions for nearly two centuries. The concept of dimension has gone through stages of being any natural number n, to being possibly infinite with the introduction of Hilbert space, to being any positive real number in fractal geometry. Dimension theory is a technical area, initially within general topology, that discusses definitions; in common with most mathematical ideas, dimension is now defined rather than an intuition. Connected topological manifolds have a well-defined dimension; this is a theorem (invariance of domain) rather than anything a priori.The issue of dimension still matters to geometry as many classic questions still lack complete answers. For instance, many open problems in topology depend on the dimension of an object for the result. In physics, dimensions 3 of space and 4 of space-time are special cases in geometric topology, and dimensions 10 and 11 are key ideas in string theory. Currently, the existence of the theoretical dimensions is purely defined by technical reasons; it is likely that further research may result in a geometric reason for the significance of 10 or 11 dimensions in the theory, lending credibility or possibly disproving string theory.The theme of symmetry in geometry is nearly as old as the science of geometry itself. Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers and were investigated in detail before the time of Euclid. Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of M.C. Escher. Nonetheless, it was not until the second half of 19th century that the unifying role of symmetry in foundations of geometry was recognized. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is. Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines. However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' proved most influential. Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory, the latter in Lie theory and Riemannian geometry.A different type of symmetry is the principle of duality in projective geometry (see Duality (projective geometry)) among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and you will get an equally true theorem. A similar and closely related form of duality exists between a vector space and its dual space.In the nearly two thousand years since Euclid, while the range of geometrical questions asked and answered inevitably expanded, the basic understanding of space remained essentially the same. Immanuel Kant argued that there is only one, absolute, geometry, which is known to be true a priori by an inner faculty of mind: Euclidean geometry was synthetic a priori.[51]  This dominant view was overturned by the revolutionary discovery of non-Euclidean geometry in the works of Bolyai, Lobachevsky, and Gauss (who never published his theory). They demonstrated that ordinary Euclidean space is only one possibility for development of geometry. A broad vision of the subject of geometry was then expressed  by Riemann in his 1867 inauguration lecture Über die Hypothesen, welche der Geometrie zu Grunde liegen (On the hypotheses on which geometry is based),[52] published only after his death. Riemann's new idea of space proved crucial in Einstein's general relativity theory, and Riemannian geometry, that considers very general spaces in which the notion of length is defined, is a mainstay of modern geometry.Euclidean geometry has become closely connected with computational geometry, computer graphics, convex geometry, incidence geometry, finite geometry, discrete geometry, and some areas of combinatorics. Attention was given to further work on Euclidean geometry and the Euclidean groups by crystallography and the work of H. S. M. Coxeter, and can be seen in theories of Coxeter groups and polytopes. Geometric group theory is an expanding area of the theory of more general discrete groups, drawing on geometric models and algebraic techniques.Differential geometry has been of increasing importance to mathematical physics due to Einstein's general relativity postulation that the universe is curved. Contemporary differential geometry is intrinsic, meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point, and not a priori parts of some ambient flat Euclidean space.The field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms. This has often been expressed in the form of the dictum 'topology is rubber-sheet geometry'. Contemporary geometric topology and differential topology, and particular subfields such as Morse theory, would be counted by most mathematicians as part of geometry. Algebraic topology and general topology have gone their own ways.[citation needed][dubious  – discuss]The field of algebraic geometry is the modern incarnation of the Cartesian geometry of co-ordinates. From late 1950s through mid-1970s it had undergone major foundational development, largely due to work of Jean-Pierre Serre and Alexander Grothendieck. This led to the introduction of schemes and greater emphasis on topological methods, including various cohomology theories. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.The study of low-dimensional algebraic varieties, algebraic curves, algebraic surfaces and algebraic varieties of dimension 3 ("algebraic threefolds"), has been far advanced. Gröbner basis theory and real algebraic geometry are among more applied subfields of modern algebraic geometry. Arithmetic geometry is an active field combining algebraic geometry and number theory. Other directions of research involve moduli spaces and complex geometry. Algebro-geometric methods are commonly applied in string and brane  theory.Geometry has found applications in many fields, some of which are described below.Mathematics and art are related in a variety of ways. For instance, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry.Mathematics and architecture are related, since, as with other arts, architects use mathematics for several reasons. Apart from the mathematics needed when engineering buildings, architects use geometry: to define the spatial form of a building; from the Pythagoreans of the sixth century BC onwards, to create forms considered harmonious, and thus to lay out buildings and their surroundings according to mathematical, aesthetic and sometimes religious principles; to decorate buildings with mathematical objects such as tessellations; and to meet environmental goals, such as to minimise wind speeds around the bases of tall buildings.The field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.Modern geometry has many ties to physics as is exemplified by the links between pseudo-Riemannian geometry and general relativity. One of the youngest physical theories, string theory, is also very geometric in flavour.Geometry has also had a large effect on other areas of mathematics. For instance, the introduction of coordinates by René Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century.  The subject of geometry was further enriched by the study of the intrinsic structure of geometric objects that originated with Euler and Gauss and led to the creation of topology and differential geometry.An important area of application is number theory. In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths, which contradicted their philosophical views, made them abandon abstract numbers in favor of concrete geometric quantities, such as length and area of figures. Since the 19th century, geometry has been used for solving problems in number theory, for example through the geometry of numbers or, more recently, scheme theory, which is used in Wiles's proof of Fermat's Last Theorem.While the visual nature of geometry makes it initially more accessible than other mathematical areas such as algebra or number theory, geometric language is also used in contexts far removed from its traditional, Euclidean provenance (for example, in fractal geometry and algebraic geometry).[53]Analytic geometry applies methods of algebra to geometric questions, typically by relating geometric curves to algebraic equations. These ideas played a key role in the development of calculus in the 17th century and led to the discovery of many new properties of plane curves. Modern algebraic geometry considers similar questions on a vastly more abstract level.Leonhard Euler, in studying problems like the Seven Bridges of Königsberg, considered the most fundamental properties of geometric figures based solely on shape, independent of their metric properties. Euler called this new branch of geometry geometria situs (geometry of place), but it is now known as topology. Topology grew out of geometry, but turned into a large independent discipline. It does not differentiate between objects that can be continuously deformed into each other. The objects may nevertheless retain some geometry, as in the case of hyperbolic knots."Three scientists, Ibn al-Haytham, Khayyam, and al-Tusi, had made the most considerable contribution to this branch of geometry whose importance came to be completely recognized only in the 19th century. In essence, their propositions concerning the properties of quadrangles which they considered, assuming that some of the angles of these figures were acute of obtuse, embodied the first few theorems of the hyperbolic and the elliptic geometries. Their other proposals showed that various geometric statements were equivalent to the Euclidean postulate V. It is extremely important that these scholars established the mutual connection between this postulate and the sum of the angles of a triangle and a quadrangle. By their works on the theory of parallel lines Arab mathematicians directly influenced the relevant investigations of their European counterparts. The first European attempt to prove the postulate on parallel lines – made by Witelo, the Polish scientists of the 13th century, while revising Ibn al-Haytham's Book of Optics (Kitab al-Manazir) – was undoubtedly prompted by Arabic sources. The proofs put forward in the 14th century by the Jewish scholar Levi ben Gerson, who lived in southern France, and by the above-mentioned Alfonso from Spain directly border on Ibn al-Haytham's demonstration. Above, we have demonstrated that Pseudo-Tusi's Exposition of Euclid had stimulated both J. Wallis's and G. Saccheri's studies of the theory of parallel lines."
Three-dimensional space
Three-dimensional space (also: 3-space or, rarely, tri-dimensional space) is a geometric setting in which three values (called parameters) are required to determine the position of an element (i.e., point). This is the informal meaning of the term dimension.In physics and mathematics, a sequence of n numbers can be understood as a location in n-dimensional space. When n = 3, the set of all such locations is called three-dimensional Euclidean space. It is commonly represented by the symbol ℝ3. This serves as a three-parameter model of the physical universe (that is, the spatial part, without considering time) in which all known matter exists. However, this space is only one example of a large variety of spaces in three dimensions called 3-manifolds. In this classical example, when the three values refer to measurements in different directions (coordinates), any three directions can be chosen, provided that vectors in these directions do not all lie in the same 2-space (plane). Furthermore, in this case, these three values can be labeled by any combination of three chosen from the terms width, height, depth, and length.In mathematics, analytic geometry (also called Cartesian geometry) describes every point in three-dimensional space by means of three coordinates. Three coordinate axes are given, each perpendicular to the other two at the origin, the point at which they cross.  They are usually labeled x, y, and z. Relative to these axes, the position of any point in three-dimensional space is given by an ordered triple of real numbers, each number giving the distance of that point from the origin measured along the given axis, which is equal to the distance of that point from the plane determined by the other two axes.[1]Other popular methods of describing the location of a point in three-dimensional space include cylindrical coordinates and spherical coordinates, though there are an infinite number of possible methods. See Euclidean space.Below are images of the above-mentioned systems.Cartesian coordinate systemCylindrical coordinate systemSpherical coordinate systemTwo distinct points always determine a (straight) line. Three distinct points are either collinear or determine a unique plane. Four distinct points can either be collinear, coplanar or determine the entire space.Two distinct lines can either intersect, be parallel or be skew. Two parallel lines, or two intersecting lines, lie in a unique plane, so skew lines are lines that do not meet and do not lie in a common plane.Two distinct planes can either meet in a common line or are parallel (do not meet). Three distinct planes, no pair of which are parallel, can either meet in a common line, meet in a unique common point or have no point in common. In the last case, the three lines of intersection of each pair of planes are mutually parallel.A line can lie in a given plane, intersect that plane in a unique point or be parallel to the plane. In the last case, there will be lines in the plane that are parallel to the given line.A hyperplane is a subspace of one dimension less than the dimension of the full space. The hyperplanes of a three-dimensional space are the two-dimensional subspaces, that is, the planes. In terms of cartesian coordinates, the points of a hyperplane satisfy a single linear equation, so planes in this 3-space are described by linear equations. A line can be described by a pair of independent linear equations, each representing a plane having this line as a common intersection.Varignon's theorem states that the midpoints of any quadrilateral in ℝ3 form a parallelogram, and so, are coplanar.A sphere in 3-space (also called a 2-sphere because it is a 2-dimensional object) consists of the set of all points in 3-space at a fixed distance r from a central point P. The solid enclosed by the sphere is called a ball (or, more precisely a 3-ball). The volume of the ball is given byAnother type of sphere arises from a 4-ball, whose three-dimensional surface is the 3-sphere: points equidistant to the origin of the euclidean space ℝ4. If a point has coordinates, P(x, y, z, w), then x2 + y2 + z2 + w2 = 1 characterizes those points on the unit 3-sphere centered at the origin.In three dimensions, there are nine regular polytopes: the five convex Platonic solids and the four nonconvex Kepler-Poinsot polyhedra.A surface generated by revolving a plane curve about a fixed line in its plane as an axis is called a surface of revolution. The plane curve is called the generatrix of the surface. A section of the surface, made by intersecting the surface with a plane that is perpendicular (orthogonal) to the axis, is a circle.Simple examples occur when the generatrix is a line. If the generatrix line intersects the axis line, the surface of revolution is a right circular cone with vertex (apex) the point of intersection. However, if the generatrix and axis are parallel, the surface of revolution is a circular cylinder.In analogy with the conic sections, the set of points whose cartesian coordinates satisfy the general equation of the second degree, namely,where A, B, C, F, G, H, J, K, L and M are real numbers and not all of A, B, C, F, G and H are zero is called a quadric surface.[2]There are six types of non-degenerate quadric surfaces:The degenerate quadric surfaces are the empty set, a single point, a single line, a single plane, a pair of planes or a quadratic cylinder (a surface consisting of a non-degenerate conic section in a plane π and all the lines of ℝ3 through that conic that are normal to π).[2] Elliptic cones are sometimes considered to be degenerate quadric surfaces as well.Both the hyperboloid of one sheet and the hyperbolic paraboloid are ruled surfaces, meaning that they can be made up from a family of straight lines. In fact, each has two families of generating lines, the members of each family are disjoint and each member one family intersects, with just one exception, every member of the other family.[3] Each family is called a regulus.Another way of viewing three-dimensional space is found in linear algebra, where the idea of independence is crucial. Space has three dimensions because the length of a box is independent of its width or breadth. In the technical language of linear algebra, space is three-dimensional because every point in space can be described by a linear combination of three independent vectors.A vector can be pictured as an arrow. The vector's magnitude is its length, and its direction is the direction the arrow points. A vector in ℝ3 can be represented by an ordered triple of real numbers. These numbers are called the components of the vector.The dot product of two vectors A = [A1, A2, A3] and B = [B1, B2, B3] is defined as:[4]The magnitude of a vector A is denoted by ||A||. The dot product of a vector A = [A1, A2, A3] with itself iswhich givesthe formula for the Euclidean length of the vector.Without reference to the components of the vectors, the dot product of two non-zero Euclidean vectors A and B is given by[5]where θ is the angle between A and B.The cross product or vector product is a binary operation on two vectors in three-dimensional space and is denoted by the symbol  ×. The cross product a × b of the vectors a and b is a vector that is perpendicular to both and therefore normal to the plane containing them. It has many applications in mathematics, physics, and engineering.The space and product form an algebra over a field, which is neither commutative nor associative, but is a Lie algebra with the cross product being the Lie bracket.One can in n dimensions take the product of n − 1 vectors to produce a vector perpendicular to all of them. But if the product is limited to non-trivial binary products with vector results, it exists only in three and seven dimensions.[6]In a rectangular coordinate system, the gradient is given byThe divergence of a continuously differentiable vector field F = U i + V j + W k is equal to the scalar-valued function:Expanded in Cartesian coordinates (see Del in cylindrical and spherical coordinates for spherical and cylindrical coordinate representations), the curl ∇ × F is, for F composed of [Fx, Fy, Fz]:where i, j, and k are the unit vectors for the x-, y-, and z-axes, respectively. This expands as follows:[7]For some scalar field f : U ⊆ Rn → R, the line integral along a piecewise smooth curve C ⊂  U is defined asFor a vector field F : U ⊆ Rn → Rn, the line integral along a piecewise smooth curve C ⊂ U, in the direction of r, is defined aswhere · is the dot product and r: [a, b] → C is a bijective parametrization of the curve C such that r(a) and r(b) give the endpoints of C.A surface integral  is a generalization of multiple integrals to integration over surfaces. It can be thought of as the double integral analog of the line integral. To find an explicit formula for the surface integral, we need to parameterize the surface of interest, S, by considering a system of curvilinear coordinates on S, like the latitude and longitude on a sphere. Let such a parameterization be x(s, t), where (s, t) varies in some region T in the plane. Then, the surface integral is given bywhere the expression between bars on the right-hand side is the magnitude of the cross product of the partial derivatives of x(s, t), and is known as the surface element. Given a vector field v on S, that is a function that assigns to each x in S a vector v(x), the surface integral can be defined component-wise according to the definition of the surface integral of a scalar field; the result is a vector.A volume integral refers to an integral over a  3-dimensional domain.The fundamental theorem of line integrals, says that a line integral through a gradient field can be evaluated by evaluating the original scalar field at the endpoints of the curve.Stokes' theorem relates the surface integral of the curl of a vector field F over a surface Σ in Euclidean three-space to the line integral of the vector field over its boundary ∂Σ:The left side is a volume integral over the volume V, the right side is the surface integral over the boundary of the volume V. The closed manifold ∂V is quite generally the boundary of V oriented by outward-pointing normals, and n is the outward pointing unit normal field of the boundary ∂V. (dS may be used as a shorthand for ndS.)Three-dimensional space has a number of topological properties that distinguish it from spaces of other dimension numbers. For example, at least three dimensions are required to tie a knot in a piece of string.[9]
Fundamental matrix (computer vision)
Being of rank two and determined only up to scale, the fundamental matrix can be estimated given at least seven point correspondences. Its seven parameters represent the only geometric information about cameras that can be obtained through point correspondences alone.The term "fundamental matrix" was coined by QT Luong in his influential PhD thesis. It is sometimes also referred to as the "bifocal tensor". As a tensor it is a two-point tensor in that it is a bilinear form relating points in distinct coordinate systems.The fundamental matrix is a relationship between any two images of the same scene that constrains where the projection of points from the scene can occur in both images. Given the projection of a scene point into one of the images the corresponding point in the other image is constrained to a line, helping the search, and allowing for the detection of wrong correspondences.  The relation between corresponding image points which the fundamental matrix represents is referred to as epipolar constraint, matching constraint, discrete matching constraint, or incidence relation.The fundamental matrix can be determined by a set of point correspondences.  Additionally, these corresponding image points may be triangulated to world points with the help of camera matrices derived directly from this fundamental matrix.  The scene composed of these world points is within a projective transformation of the true scene.[1]The cameras then transform asFundamental matrix can be derived using the coplanarity condition. [2]The fundamental matrix is of rank 2. Its kernel defines the epipole.
Affine space
In mathematics, an affine space is a geometric structure that generalizes some of the properties of Euclidean spaces in such a way that these are independent of the concepts of distance and measure of angles, keeping only the properties related to parallelism and ratio of lengths for parallel line segments.In an affine space, there is no distinguished point that serves as an origin. Hence, no vector has a fixed origin and no vector can be uniquely associated to a point. In an affine space, there are instead displacement vectors, also called translation vectors or simply translations, between two points of the space.[1] Thus it makes sense to subtract two points of the space, giving a translation vector, but it does not make sense to add two points of the space. Likewise, it makes sense to add a displacement vector to a point of an affine space, resulting in a new point translated from the starting point by that vector.Any vector space may be considered as an affine space, and this amounts to forgetting the special role played by the zero vector. In this case, the elements of the vector space may be viewed either as points of the affine space or as displacement vectors or translations. When considered as a point, the zero vector is called the origin. Adding a fixed vector to the elements of a linear subspace of a vector space produces an affine subspace. One commonly says that this affine subspace has been obtained by translating (away from the origin) the linear subspace by the translation vector. In finite dimensions, such an affine subspace is the solution set of an inhomogeneous linear system. The displacement vectors for that affine space are the solutions of the corresponding homogeneous linear system, which is a linear subspace. Linear subspaces, in contrast, always contain the origin of the vector space.The dimension of an affine space is defined as the dimension of the vector space of its translations. An affine space of dimension one is an affine line. An affine space of dimension 2 is an affine plane. An affine subspace of dimension n – 1 in an affine space or a vector space of dimension n is an affine hyperplane.The following characterization may be easier to understand than the usual formal definition: an affine space is what is left of a vector space after you've forgotten which point is the origin (or, in the words of the French mathematician Marcel Berger, "An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps"[2]). Imagine that Alice knows that a certain point is the actual origin, but Bob believes that another point — call it p — is the origin. Two vectors, a and b, are to be added. Bob draws an arrow from point p to point a and another arrow from point p to point b, and completes the parallelogram to find what Bob thinks is a + b, but Alice knows that he has actually computedSimilarly, Alice and Bob may evaluate any linear combination of a and b, or of any finite set of vectors, and will generally get different answers. However, if the sum of the coefficients in a linear combination is 1, then Alice and Bob will arrive at the same answer.If Alice travels tothen Bob can similarly travel toUnder this condition, for all coefficients λ + (1 − λ) = 1, Alice and Bob describe the same point with the same linear combination, despite using different origins.While only Alice knows the "linear structure", both Alice and Bob know the "affine structure"—i.e. the values of affine combinations, defined as linear combinations in which the sum of the coefficients is 1. A set with an affine structure is an affine space.that has the following properties.[4][5][6]The first two properties are simply defining properties of a (right) group action. The third property characterizes free and transitive actions, the onto character coming from transitivity, and then the injective character follows from the action being free. There is a fourth property that follows from 1, 2, 3 above:Property 3 is often used in the following equivalent form.Another way to express the definition is that an affine space is a principal homogeneous space for the action of the additive group of a vector space. Homogeneous spaces are by definition endowed with a transitive group action, and for a principal homogeneous space such a transitive action is by definition free.Existence follows from the transitivity of the action, and uniqueness follows because the action is free.This subtraction has the two following properties, called Weyl's axioms:[7]In Euclidean geometry, the second Weyl's axiom is commonly called the parallelogram rule.The affine subspaces of A are the subsets of A of the formThe linear subspace associated with an affine subspace is often called its direction, and two subspaces that share the same direction are said to be parallel.This implies the following generalization of  Playfair's axiom: Given a direction V, for any point a of A there is one and only one affine subspace of direction V, which passes through a, namely the subspace a + V.The term parallel is also used for two affine subspaces such that the direction of one is included in the direction of the other.such thatEvery vector space V may be considered as an affine space over itself. This means that every element of V may be considered either as a point or as a vector. This affine set is sometimes denoted (V, V) for emphasizing the double role of the elements of V. When considered as a point, the zero vector is commonly denoted o (or O, when upper-case letters are used for points) and called the origin.Euclidean spaces (including the one-dimensional line, two-dimensional plane, and three-dimensional space commonly studied in elementary geometry, as well as higher-dimensional analogues) are affine spaces.Indeed, in most modern definitions, a Euclidean space is defined to be an affine space, such that the associated vector space is a real inner product space of finite dimension, that is a vector space over the reals with a positive-definite quadratic form q(x). The inner product of two vectors x and y is the value of the symmetric bilinear formThe usual Euclidean distance between two points A and B is In older definition of Euclidean spaces through synthetic geometry, vectors are defined as equivalence classes of ordered pairs of points under equipollence (the pairs (A, B) and (C, D) are equipollent if the points A, B, D, C (in this order) form a parallelogram). It is straightforward to verify that the vectors form a vector space, the square of the Euclidean distance is a quadratic form on the space of vectors, and the two definitions of Euclidean spaces are equivalent.In Euclidean geometry, the common phrase "affine property" refers to a property that can be proved in affine spaces, that is, it can be proved without using the quadratic form and its associated inner product. In other words, an affine property is a property that does not involve lengths and angles. Typical examples are parallelism, and the definition of a tangent. A non-example is the definition of a normal.Equivalently, an affine property is a property that is invariant under affine transformations of the Euclidean space.Thus this sum is independent of the choice of the origin, and the resulting vector is denotedone writesFor any subset X of an affine space A, there is a smallest affine subspace that contains it, called the affine span of X. It is the intersection of all affine subspaces containing X, and its direction is the intersection of the directions of the affine subspaces that contain X.The affine span of X is the set of all (finite) affine combinations of points of X, and its direction is the linear span of the x − y for x and y in X. If one chooses a particular point x0, the direction of the affine span of X is also the linear span of the x – x0 for x in X.One says also that the affine span of X is generated by X and that X is a generating set of its affine span.A set X of points of an affine space is said affinely independent or, simply, independent, if the affine span of any strict subset of X is a strict subset of the affine span of X. An affine basis, or barycentric frame (see § Barycentric coordinates, below) of an affine space is a generating set that is also independent (that is a minimal generating set).Recall the dimension of an affine space is the dimension of its associated vector space. The bases of an affine space of finite dimension n are the independent subsets of n + 1 elements, or, equivalently, the generating subsets of n + 1 elements. Equivalently, {x0, …, xn} is an affine basis of an affine space if and only if {x1 − x0, …, xn − x0} is a linear basis of the associated vector space.There are two strongly related kinds of coordinate systems that may be defined on affine spaces.andFor affine spaces of infinite dimension, the same definition applies, using only finite sums. This means that for each point, only a finite number of coordinates are non-zero.or equivalentlyExample: In Euclidean geometry, Cartesian coordinates are affine coordinates relative to an orthonormal frame, that is an affine frame (o, v1, …, vn) such that (v1, …, vn) is an orthonormal basis.Barycentric coordinates and affine coordinates are strongly related, and may be considered as equivalent.In fact, given a barycentric frameone deduces immediately the affine frameand, ifare the barycentric coordinates of a point over the barycentric frame, then the affine coordinates of the same point over the affine frame areConversely, ifis an affine frame, thenis a barycentric frame. Ifare the affine coordinates of a point over the affine frame, then its barycentric coordinates over the barycentric frame areTherefore, barycentric and affine coordinates are almost equivalent. In most applications, affine coordinates are preferred, as involving less coordinates that are independent. However, in the situations where the important points of the studied problem are affinity independent, barycentric coordinates may lead to simpler computation, as in the following example.The vertices of a non-flat triangle form an affine basis of the Euclidean plane. The barycentric coordinates allows easy characterization of the elements of the triangle that do not involve angles or distance:The vertices are the points of barycentric coordinates (1, 0, 0),  (0, 1, 0) and  (0, 0, 1). The lines supporting the edges are the points that have a zero coordinate. The edges themselves are the points that have a zero coordinate and two nonnegative coordinates. The interior of the triangle are the points whose all coordinates are positive. The medians are the points that have two equal coordinates, and the centroid is the point of coordinates (1/3, 1/3, 1/3).Letbe an affine homomorphism, withas associated linear map.An important example is the projection parallel to some direction onto an affine subspace. The importance of this example lies in the fact that Euclidean spaces are affine spaces, and that this kind of projections is fundamental in Euclidean geometry.for x and y in E.The image of this projection is  F, and its fibers are the subspaces of direction  D.Although kernels are not defined for affine spaces, quotient spaces are defined. This results from the fact that "belonging to the same fiber of an affine homomorphism" is an equivalence relation.Affine space is usually studied as analytic geometry using coordinates, or equivalently vector spaces. It can also be studied as synthetic geometry by writing down axioms, though this approach is much less common. There are several different systems of axioms for affine space.Coxeter (1969, p. 192) axiomatizes affine geometry (over the reals) as ordered geometry together with an affine form of Desargues's theorem and an axiom stating that in a plane there is at most one line through a given point not meeting a given line.Affine planes satisfy the following axioms (Cameron 1991, chapter 2):(in which two lines are called parallel if they are equal ordisjoint):As well as affine planes over fields (or division rings), there are also many non-Desarguesian planes satisfying these axioms. (Cameron 1991, chapter 3) gives axioms for higher-dimensional affine spaces.Affine spaces are subspaces of projective spaces: an affine plane can be obtained from any projective plane by removing a line and all the points on it, and conversely any affine plane can be used to construct a projective plane as a closure by adding a line at infinity whose points correspond to equivalence classes of parallel lines.Further, transformations of projective space that preserve affine space (equivalently, that leave the hyperplane at infinity invariant as a set) yield transformations of affine space. Conversely, any affine linear transformation extends uniquely to a projective linear transformation, so the affine group is a subgroup of the projective group. For instance, Möbius transformations (transformations of the complex projective line, or Riemann sphere) are affine (transformations of the complex plane) if and only if they fix the point at infinity.In algebraic geometry, an affine variety (or, more generally, an affine algebraic set) is defined as the subset of an affine space that is the set of the common zeros of a set of so-called polynomial functions over the affine space. For defining a polynomial function over the affine space, one has to choose an affine coordinate system. Then, a polynomial function is a function such that the image of any point is the value of some multivariate polynomial function of the coordinates of the point. As a change of affine coordinates may be expressed by linear functions (more precisely affine functions) of the coordinates, this definition is independent of a particular choice of coordinates.As the whole affine space is the set of the common zeros of the zero polynomial, affine spaces are affine algebraic varieties.Affine spaces over topological fields, such as the real or the complex numbers, have a natural topology. The Zariski topology, which is defined for affine spaces over any field, allows use of topological methods in any case. Zariski topology is the unique topology on an affine space whose closed sets are affine algebraic sets (that is sets of the common zeros of polynomials functions over the affine set). As, over a topological field, polynomial functions are continuous, every Zariski closed set is closed for the usual topology, if any. In other words, over a topological field, Zariski topology is coarser than the natural topology.The case of an algebraically closed ground field is especially important in algebraic geometry, because, in this case, the homeomorphism above is a map between the affine space and the set of all maximal ideals of the ring of functions (this is Hilbert's Nullstellensatz).This is the starting idea of scheme theory of Grothendieck, which consists, for studying algebraic varieties, of considering as "points", not only the points of the affine space, but also all the prime ideals of the spectrum. This allows gluing together algebraic varieties in a similar way as, for manifolds, charts are glued together for building a manifold.
Projective space
In mathematics, a projective space can be thought of as the set of lines through the origin of a vector space V. The cases when V = R2 and V = R3 are the real projective line and the real projective plane, respectively, where R denotes the field of real numbers, R2 denotes ordered pairs of real numbers, and R3 denotes ordered triplets of real numbers.The idea of a projective space relates to perspective, more precisely to the way an eye or a camera projects a 3D scene to a 2D image. All points that lie on a projection line (i.e., a "line of sight"), intersecting with the entrance pupil of the camera, are projected onto a common image point. In this case, the vector space is R3 with the camera entrance pupil at the origin, and the projective space corresponds to the image points.Projective spaces can be studied as a separate field in mathematics, but are also used in various applied fields, geometry in particular. Geometric objects, such as points, lines, or planes, can be given a representation as elements in projective spaces based on homogeneous coordinates. As a result, various relations between these objects can be described in a simpler way than is possible without homogeneous coordinates. Furthermore, various statements in geometry can be made more consistent and without exceptions. For example, in the standard Euclidean geometry for the plane, two lines always intersect at a point except when parallel. In a projective representation of lines and points, however, such an intersection point exists even for parallel lines, and it can be computed in the same way as other intersection points.Other mathematical fields where projective spaces play a significant role are topology, the theory of Lie groups and algebraic groups, and their representation theories.As outlined above, projective space is a geometric object that formalizes statements like "Parallel lines intersect at infinity." For concreteness, we give the construction of the real projective plane P2(R) in some detail.  There are three equivalent definitions:The last formula goes under the name of homogeneous coordinates.In homogeneous coordinates, any point [x : y : z] with z ≠ 0 is equivalent to [x/z : y/z : 1].  So there are two disjoint subsets of the projective plane: that consisting of the points [x : y : z] = [x/z : y/z : 1] for z ≠ 0, and that consisting of the remaining points [x : y : 0].  The latter set can be subdivided similarly into two disjoint subsets, with points [x/y : 1 : 0] and [x : 0 : 0].  In the last case, x is necessarily nonzero, because the origin was not part of P2(R).  This last point is equivalent to [1 : 0 : 0].  Geometrically, the first subset, which is isomorphic (not only as a set, but also as a manifold, as seen later) to R2, is in the image the yellow upper hemisphere (without the equator), or equivalently the lower hemisphere.  The second subset, isomorphic to R1, corresponds to the green line (without the two marked points), or, again, equivalently the light green line.  Finally we have the red point or the equivalent light red point.  We thus have a disjoint decompositionIntuitively, and made precise below, R1 ⊔ point is itself the real projective line P1(R).  Considered as a subset of P2(R), it is called line at infinity, whereas R2 ⊂ P2(R) is called affine plane, i.e., just the usual plane.The next objective is to make the saying "parallel lines meet at infinity" precise.  A natural bijection between the plane z = 1 (which meets the sphere at the north pole N = (0, 0, 1)) and the sphere of the projective plane is accomplished by the  gnomonic projection.  Each point P on this plane is mapped to the two intersection points of the sphere with the line through its center and P.  These two points are identified in the projective plane.  Lines (blue) in the plane are mapped to great circles if one also includes one pair of antipodal points on the equator.  Any two great circles intersect precisely in two antipodal points (identified in the projective plane).  Great circles corresponding to parallel lines intersect on the equator.  So any two lines have exactly one intersection point inside P2(R).  This phenomenon is axiomatized in projective geometry.The real projective space of dimension n or projective n-space, Pn(R), is roughly the set of the lines in Rn+1 passing through the origin. For defining it as a topological space and as an algebraic variety it is better to define it as the quotient space of Rn+1 by the equivalence relation "to be aligned with the origin". More precisely,where ~ is the equivalence relation defined by: (x0, ..., xn) ~ (y0, ..., yn) if there is a non-zero real number λ such that (x0, ..., xn) = (λy0, ..., λyn).The elements of the projective space are commonly called points. The projective coordinates of a point P are x0, ..., xn, where (x0, ..., xn) is any element of the corresponding equivalence class. This is denoted P = [x0 : ... : xn], the colons and the brackets emphasizing that the right-hand side is an equivalence class, which is defined up to the multiplication by a non zero constant.Instead of R, one may take any field, or even a division ring, K. In these cases it is common[1] to use the notation PG(n, K) for Pn(K). If K is a finite field of order q, the notation is further simplified to PG(n, q). Taking the complex numbers or the quaternions, one obtains the complex projective space Pn(C) and quaternionic projective space Pn(H).If n is one or two, it is also called projective line or projective plane, respectively. The complex projective line is also called the Riemann sphere.Slightly more generally, for a vector space V (over some field k, or even more generally a module V over some division ring), P(V) is defined as (V ∖ {0}) / ~, where two non-zero vectors v1, v2 in V are equivalent if they differ by a non-zero scalar λ, i.e., v1 = λv2. The vector space need not be finite-dimensional; thus, for example, there is the theory of projective Hilbert spaces.The above definition of projective space gives a set. For purposes of differential geometry, which deals with manifolds, it is useful to endow this set with a (real or complex) manifold structure.Namely, identifying a point of the projective space with its homogeneous coordinates, let us consider the following subsets of the projective space:By the definition of projective space, their union is the whole projective space. Furthermore, Ui is in bijection with Rn (or Cn) via the following maps:(the hat means that the i-th entry is missing).The example image shows P1(R). (Antipodal points are identified in P1(R), though). It is covered by two copies of the real line R, each of which covers the projective line except one point, which is "the" (or "a") point at infinity.We first define a topology on projective space by declaring that these maps shall be homeomorphisms, that is, a subset of Ui is open iff its image under the above isomorphism is an open subset (in the usual sense) of Rn. An arbitrary subset A of  Pn(R) is open if all intersections A ∩ Ui are open. This defines a topological space.The manifold structure is given by the above maps, too.Another way to think about the projective line is the following: take two copies of the affine line with coordinates x and y, respectively, and glue them together along the subsets x ≠ 0 and y ≠ 0 via the mapsThe resulting manifold is the projective line. The charts given by this construction are the same as the ones above. Similar presentations exist for higher-dimensional projective spaces.The above decomposition in disjoint subsets reads in this generality:this so-called cell-decomposition can be used to calculate the singular cohomology of projective space.All of the above holds for complex projective space, too. The complex projective line P1(C) is an example of a Riemann surface.The covering by the above open subsets also shows that projective space is an algebraic variety (or scheme); it is covered by n + 1 affine n-spaces. The construction of a projective scheme is an instance of the Proj construction.Real projective n-space has a quite straightforward CW complex structure where Pn(R) is obtained from Pn-1(R) by attaching an n-cell with the quotient projection Sn-1→Pn-1(R) as the attaching map.There are some advantages of the projective space compared with affine space (e.g., Pn(R) vs. An(R)). For these reasons it is important to know when a given manifold or variety is projective, i.e., embeds into (is a closed subset of) projective space. (Very) ample line bundles are designed to tackle this question.Note that a projective space can be formed by the projectivization of a vector space, as lines through the origin, but cannot be formed from an affine space without a choice of basepoint. That is, affine spaces are open subspaces of projective spaces, which are quotients of vector spaces.A projective space S can be defined axiomatically as a set P (the set of points), together with a set L of subsets of P (the set of lines), satisfying these axioms:[2]The last axiom eliminates reducible cases that can be written as a disjoint union of projective spaces together with 2-point lines joining any two points in distinct projective spaces. More abstractly, it can be defined as an incidence structure (P, L, I) consisting of a set P of points, a set L of lines, and an incidence relation I that states which points lie on which lines.The structures defined by these axioms are more general than those obtained from the vector space construction given above. If the (projective) dimension is at least three then, by the Veblen–Young theorem, there is no difference. However, for dimension two, there are examples that satisfy these axioms that can not be constructed from vector spaces (or even modules over division rings). These examples do not satisfy the Theorem of Desargues and are known as Non-Desarguesian planes. In dimension one, any set with at least three elements satisfies the axioms, so it is usual to assume additional structure for projective lines defined axiomatically.[4]It is possible to avoid the troublesome cases in low dimensions by adding or modifying axioms that define a projective space. Coxeter (1969, p. 231) gives such an extension due to Bachmann.[5] To ensure that the dimension is at least two, replace the three point per line axiom above by;To avoid the non-Desarguesian planes, include Pappus's theorem as an axiom;[6]And, to ensure that the vector space is defined over a field that does not have even characteristic include Fano's axiom;[7]A subspace of the projective space is a subset X, such that any line containing two points of X is a subset of X (that is, completely contained in X).  The full space and the empty space are always  subspaces.The geometric dimension of the space is said to be n if that is the largest number for which there is a strictly ascending chain of subspaces of this form:A finite projective space is a projective space where P is a finite set of points. In any finite projective space, each line contains the same number of points and the order of the space is defined as one less than this common number. For finite projective spaces of dimension at least three, Wedderburn's theorem implies that the division ring over which the projective space is defined must be a finite field, GF(q), whose order (that is, number of elements) is q (a prime power). A finite projective space defined over such a finite field has q + 1 points on a line, so the two concepts of order coincide. Notationally, PG(n, GF(q)) is usually written as PG(n, q).All finite fields of the same order are isomorphic, so, up to isomorphism, there is only one finite projective space for each dimension greater than or equal to three, over a given finite field. However, in dimension two there are non-Desarguesian planes. Up to isomorphism there arefinite projective planes of orders 2, 3, 4, ..., 10, respectively. The numbers beyond this are very difficult to calculate and are not determined except for some zero values due to the Bruck–Ryser theorem.The smallest projective plane is the Fano plane, PG(2, 2) with 7 points and 7 lines.Injective linear maps T ∈ L(V, W) between two vector spaces V and W over the same field k induce mappings of the corresponding projective spaces P(V) → P(W) via:where v is a non-zero element of V and [...] denotes the equivalence classes of a vector under the defining identification of the respective projective spaces.  Since members of the equivalence class differ by a scalar factor, and linear maps preserve scalar factors, this induced map is well-defined. (If T is not injective, it has a null space larger than {0}; in this case the meaning of the class of T(v) is problematic if v is non-zero and in the null space. In this case one obtains a so-called rational map, see also birational geometry).Two linear maps S and T in L(V, W) induce the same map between P(V) and P(W) if and only if they differ by a scalar multiple, that is if T = λS for some λ ≠ 0.  Thus if one identifies the scalar multiples of the identity map with the underlying field K, the set of K-linear morphisms from P(V) to P(W) is simply P(L(V, W)).The automorphisms P(V) → P(V) can be described more concretely. (We deal only with automorphisms preserving the base field K). Using the notion of sheaves generated by global sections, it can be shown that any algebraic (not necessarily linear) automorphism must be linear, i.e., coming from a (linear) automorphism of the vector space V. The latter form the group GL(V). By identifying maps that differ by a scalar, one concludes thatthe quotient group of GL(V) modulo the matrices that are scalar multiples of the identity. (These matrices form the center of Aut(V).) The groups PGL are called projective linear groups. The automorphisms of the complex projective line P1(C) are called Möbius transformations.When the construction above is applied to the dual space V∗ rather than V, one obtains the dual projective space, which can be canonically identified with the space of hyperplanes through the origin of V.  That is, if V is n dimensional, then P(V∗) is the Grassmannian of n − 1 planes in V.In algebraic geometry, this construction allows for greater flexibility in the construction of projective bundles.  One would like to be able associate a projective space to every quasi-coherent sheaf E over a scheme Y, not just the locally free ones.[clarification needed] See EGAII, Chap. II, par. 4 for more details.Severi–Brauer varieties are algebraic varieties over a field k, which become isomorphic to projective spaces after an extension of the base field k.Another generalization of projective spaces are weighted projective spaces; these are themselves special cases of toric varieties.[8]
Orthogonality
In mathematics, orthogonality is the generalization of the notion of perpendicularity to the linear algebra of bilinear forms.  Two elements u and v of a vector space with bilinear form B are orthogonal when B(u, v) = 0.  Depending on the bilinear form, the vector space may contain nonzero self-orthogonal vectors. In the case of function spaces, families of orthogonal functions are used to form a basis.By extension, orthogonality is also used to refer to the separation of specific features of a system. The term also has specialized meanings in other fields including art and chemistry.The word comes from the Greek ὀρθός (orthos), meaning "upright", and γωνία (gonia), meaning "angle".The ancient Greek ὀρθογώνιον orthogōnion (< ὀρθός orthos 'upright'[1] + γωνία gōnia 'angle'[2]) and classical Latin orthogonium originally denoted a rectangle.[3] Later, they came to mean a right triangle.  In the 12th century, the post-classical Latin word orthogonalis came to mean a right angle or something related to a right angle.[4]A set of vectors in an inner product space is called pairwise orthogonal if each pairing of them is orthogonal. Such a set is called an orthogonal set.In certain cases, the word normal is used to mean orthogonal, particularly in the geometric sense as in the normal to a surface. For example, the y-axis is normal to the curve y = x2 at the origin.  However, normal may also refer to the magnitude of a vector. In particular, a set is called orthonormal (orthogonal plus normal) if it is an orthogonal set of unit vectors. As a result, use of the term normal to mean "orthogonal" is often avoided. The word "normal" also has a different meaning in probability and statistics.A vector space with a bilinear form generalizes the case of an inner product. When the bilinear form applied to two vectors results in zero, then they are orthogonal. The case of a pseudo-Euclidean plane uses the term hyperbolic orthogonality. In the diagram, axes x′ and t′ are hyperbolic-orthogonal for any given ϕ.In Euclidean space, two vectors are orthogonal if and only if their dot product is zero, i.e. they make an angle of 90° (π/2 radians), or one of the vectors is zero.[8] Hence orthogonality of vectors is an extension of the concept of perpendicular vectors to spaces of any dimension.The orthogonal complement of a subspace is the space of all vectors that are orthogonal to every vector in the subspace.  In a three-dimensional Euclidean vector space, the orthogonal complement of a line through the origin is the plane through the origin perpendicular to it, and vice versa.[9]Note that the geometric concept two planes being perpendicular does not correspond to the orthogonal complement, since in three dimensions a pair of vectors, one from each of a pair of perpendicular planes, might meet at any angle.In four-dimensional Euclidean space, the orthogonal complement of a line is a hyperplane and vice versa, and that of a plane is a plane.[9]By using integral calculus, it is common to use the following to define the inner product of two functions f and g with respect to a nonnegative weight function w over an interval [a, b]:In simple cases, w(x) = 1.We say that functions f and g are orthogonal if their inner product (equivalently, the value of this integral) is zero:Orthogonality of two functions with respect to one inner product does not imply orthogonality with respect to another inner product.We write the norm with respect to this inner product asThe members of a set of functions {fi : i = 1, 2, 3, ...} are orthogonal with respect to w on the interval [a, b] ifThe members of such a set of functions are orthonormal with respect to w on the interval [a, b] ifwhereis the Kronecker delta.In other words, every pair of them (excluding pairing of a function with itself) is orthogonal, and the norm of each is 1. See in particular the  orthogonal polynomials.In art, the perspective (imaginary) lines pointing to the vanishing point are referred to as "orthogonal lines".The term "orthogonal line" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as Piet Mondrian and Burgoyne Diller are noted for their exclusive use of "orthogonal lines" — not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay at the Web site of the Thyssen-Bornemisza Museum states that "Mondrian ... dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours." [1]Orthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results.[10] This usage was introduced by Van Wijngaarden in the design of Algol 68:The number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied “orthogonally” in order to maximize the expressive power of the language while trying to avoid deleterious superfluities.[11]Orthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the separation of concerns and encapsulation, and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them.An instruction set is said to be orthogonal if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task)[12] and is designed such that instructions can use any register in any addressing mode. This terminology results from considering an instruction as a vector whose components are the instruction fields.  One field identifies the registers to be operated upon and another specifies the addressing mode. An orthogonal instruction set uniquely encodes all combinations of registers and addressing modes.[citation needed]In communications, multiple-access schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different basis functions. One such scheme is TDMA, where the orthogonal basis functions are nonoverlapping rectangular pulses ("time slots").Another scheme is orthogonal frequency-division multiplexing (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other.  Well known examples include (a, g, and n) versions of 802.11 Wi-Fi; WiMAX; ITU-T G.hn, DVB-T, the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of ADSL.In OFDM, the subcarrier frequencies are chosen so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required.When performing statistical analysis, independent variables that affect a particular dependent variable are said to be orthogonal if they are uncorrelated,[13] since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables  individually with simple regression or simultaneously with multiple regression. If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions).One econometric formalism that is alternative to the maximum likelihood framework, the Generalized Method of Moments, relies on orthogonality conditions. In particular, the Ordinary Least Squares estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals.In taxonomy, an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive.In combinatorics, two n×n Latin squares are said to be orthogonal if their superimposition yields all possible n2 combinations of entries.[14]In synthetic organic chemistry orthogonal protection is a strategy allowing the deprotection of functional groups independently of each other. In chemistry and biochemistry, an orthogonal interaction occurs when there are two pairs of substances and each substance can interact with their respective partner, but does not interact with either substance of the other pair. For example, DNA has two orthogonal pairs: cytosine and guanine form a base-pair, and adenine and thymine form another base-pair, but other base-pair combinations are strongly disfavored. As a chemical example, tetrazine reacts with transcyclooctene and azide reacts with cyclooctyne without any cross-reaction, so these are mutually orthogonal reactions, and so, can be performed simultaneously and selectively.[15] Bioorthogonal chemistry refers to chemical reactions occurring inside living systems without reacting with naturally present cellular components. In supramolecular chemistry the notion of orthogonality refers to the possibility of two or more supramolecular, often non-covalent, interactions being compatible; reversibly forming without interference from the other.In analytical chemistry, analyses are "orthogonal" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement.  This is often required as a part of a new drug application.In the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure.In neuroscience, a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map.In board games such as chess which feature a grid of squares, 'orthogonal' is used to mean "in the same row/'rank' or column/'file'".  This is the counterpart to squares which are "diagonally adjacent".[16] In the ancient Chinese board game Go a player can capture the stones of an opponent by occupying all orthogonally-adjacent points.Stereo vinyl records encode both the left and right stereo channels in a single groove.  The V-shaped groove in the vinyl has walls that are 90 degrees to each other, with variations in each wall separately encoding one of the two analogue channels that make up the stereo signal.  The cartridge senses the motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side.[17]  A pure horizontal motion corresponds to a mono signal, equivalent to a stereo signal in which both channels carry identical (in-phase) signals.
Riemann solver
Numerical analysis · SimulationFinite element · Boundary element Lattice Boltzmann · Riemann solverDissipative particle dynamicsSmoothed particle hydrodynamicsA Riemann solver is a numerical method used to solve a Riemann problem. They are heavily used in computational fluid dynamics and computational magnetohydrodynamics.Godunov is credited with introducing the first exact Riemann solver for the Euler equations,[1] by extending the previous CIR (Courant-Isaacson-Rees) method to non-linear systems of hyperbolic conservation laws. Modern solvers are able to simulate relativistic effects and magnetic fields.For the hydrodynamic case latest research results showed the possibility to avoid the iterations to calculate the exact solution for theEuler equations.[2]As iterative solutions are too costly, especially in Magnetohydrodynamics, some approximations have to be made. The most popular solvers are:Roe used the linearisation of the Jacobian, which he then solves exactly.[3]The HLLE[4] (Harten, Lax, van Leer and  Einfeldt) solver is an approximate solution to the Riemann problem,  which is only based on the integral form of the conservation laws and the largest and smallest signal velocities at the interface.  The stability and robustness of the HLLE solver is closely related to the signal velocities and a single central average state, as proposed by Einfeldt in the original paper. The description of the HLLE scheme in the book mentioned below is incomplete and partially wrong[citation needed]. The reader is referred to the original paper. Actually, the HLLE scheme is based on a new stability theory for discontinuities in fluids, which was never published.[5]The HLLC (Harten-Lax-van Leer-Contact) solver was introduced by Toro.[6] It restores the missing Rarefaction wave by some estimates, like linearisations, these can be simple but also more advanced exists like using the Roe average velocity for the middle wave speed. They are quite robust and efficient but somewhat more diffusive.[7]These solvers were introduced by Nishikawa and Kitamura,[8] in order to overcome the carbuncle problems of the Roe solver and the excessive diffusion of the HLLE solver at the same time. They developed robust and accurate Riemann solvers by combining the Roe solver and the HLLE/Rusanov solvers: they show that being applied in two orthogonal directions the two Riemann solvers can be combined into a single Roe-type solver (the Roe solver with modified wave speeds). In particular, the one derived from the Roe and HLLE solvers, called Rotated-RHLL solver, is extremely robust (carbuncle-free for all possible test cases on both structured and unstructured grids) and accurate (as accurate as the Roe solver for the boundary layer calculation).
Lp space
In mathematics, the Lp spaces are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. They are sometimes called Lebesgue spaces, named after Henri Lebesgue (Dunford & Schwartz 1958, III.3), although according to the Bourbaki group (Bourbaki 1987) they were first introduced by Frigyes Riesz (Riesz 1910).Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces.Because of their key role in the mathematical analysis of measure and probability spaces, Lebesgue spaces are used also in the theoretical discussion of problems in physics, statistics, finance, engineering, and other disciplines.In statistics, measures of central tendency and statistical dispersion, such as the mean, median, and standard deviation, are defined in terms of Lp metrics, and measures of central tendency can be characterized as solutions to variational problems.In penalized regression, 'L1 penalty' and 'L2 penalty' refer to penalizing either the L1 norm of a solution's vector of parameter values (i.e. the sum of its absolute values), or its L2 norm (its Euclidean length). Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small. Elastic net regularization uses a penalty term that is a combination of the L1 norm and the L2 norm of the parameter vector.The Fourier transform for the real line (or, for periodic functions, see Fourier series), maps Lp(R) to Lq(R) (or Lp(T) to ℓq) respectively, where 1 ≤ p ≤ 2 and 1/p + 1/q = 1. This is a consequence of the Riesz–Thorin interpolation theorem, and is made precise with the Hausdorff–Young inequality.By contrast, if p > 2, the Fourier transform does not map into Lq.Hilbert spaces are central to many applications, from quantum mechanics to stochastic calculus. The spaces L2 and ℓ2 are both Hilbert spaces. In fact, by choosing a Hilbert basis (i.e., a maximal orthonormal subset of L2 or any Hilbert space), one sees that all Hilbert spaces are isometric to ℓ2(E), where E is a set with an appropriate cardinality.The length of a vector x = (x1, x2, ..., xn) in the n-dimensional real vector space Rn is usually given by the Euclidean norm:The Euclidean distance between two points x and y is the length ||x − y||2 of the straight line between the two points. In many situations, the Euclidean distance is insufficient for capturing the actual distances in a given space. An analogy to this is suggested by taxi drivers in a grid street plan who should measure distance not in terms of the length of the straight line to their destination, but in terms of the rectilinear distance, which takes into account that streets are either orthogonal or parallel to each other. The class of p-norms generalizes these two examples and has an abundance of applications in many parts of mathematics, physics, and computer science.For a real number p ≥ 1, the p-norm or Lp-norm of x is defined byThe absolute value bars are unnecessary when p is a rational number and, in reduced form, has an even numerator.The Euclidean norm from above falls into this class and is the 2-norm, and the 1-norm is the norm that corresponds to the rectilinear distance.See L-infinity.For all p ≥ 1, the p-norms and maximum norm as defined above indeed satisfy the properties of a "length function" (or norm), which are that:Abstractly speaking, this means that Rn together with the p-norm is a Banach space. This Banach space is the Lp-space over Rn.The grid distance or rectilinear distance (sometimes called the "Manhattan distance") between two points is never shorter than the length of the line segment between them (the Euclidean or "as the crow flies" distance). Formally, this means that the Euclidean norm of any vector is bounded by its 1-norm:This fact generalizes to p-norms in that the p-norm ||x||p of any given vector x does not grow with p:For the opposite direction, the following relation between the 1-norm and the 2-norm is known:This inequality depends on the dimension n of the underlying vector space and follows directly from the Cauchy–Schwarz inequality.In general, for vectors in Cn where 0 < r < p:In Rn for n > 1, the formuladefines an absolutely homogeneous function for 0 < p < 1; however, the resulting function does not define a norm, because it is not subadditive. On the other hand, the formuladefines a subadditive function at the cost of losing absolute homogeneity. It does define an F-norm, though, which is homogeneous of degree p.Hence, the functiondefines a metric. The metric space (Rn, dp) is denoted by ℓnp.Although the p-unit ball Bnp around the origin in this metric is "concave", the topology defined on Rn by the metric dp is the usual vector space topology of Rn, hence ℓnp is a locally convex topological vector space. Beyond this qualitative statement, a quantitative way to measure the lack of convexity of ℓnp is to denote by Cp(n) the smallest constant C such that the multiple C Bnp of the p-unit ball contains the convex hull of Bnp, equal to Bn1. The fact that for fixed p < 1 we haveshows that the infinite-dimensional sequence space ℓp defined below, is no longer locally convex.[citation needed]There is one ℓ0 norm and another function called the ℓ0 "norm" (with quotation marks).The mathematical definition of the ℓ0 norm was established by Banach's Theory of Linear Operations. The space of sequences has a complete metric topology provided by the F-normwhich is discussed by Stefan Rolewicz in Metric Linear Spaces.[1] The ℓ0-normed space is studied in functional analysis, probability theory, and harmonic analysis.Another function was called the ℓ0 "norm" by David Donoho—whose quotation marks warn that this function is not a proper norm—is the number of non-zero entries of the vector x. Many authors abuse terminology by omitting the quotation marks. Defining 00 = 0, the zero "norm" of x is equal toThis is not a norm because it is not homogeneous. Despite these defects as a mathematical norm, the non-zero counting "norm" has uses in scientific computing, information theory, and statistics–notably in compressed sensing in signal processing and computational harmonic analysis.The p-norm can be extended to vectors that have an infinite number of components, which yields the space ℓ p. This contains as special cases:The space of sequences has a natural vector space structure by applying addition and scalar multiplication coordinate by coordinate. Explicitly, the vector sum and the scalar action for infinite sequences of real (or complex) numbers are given by:Define the p-norm:Here, a complication arises, namely that the series on the right is not always convergent, so for example, the sequence made up of only ones, (1, 1, 1, ...), will have an infinite p-norm for 1 ≤ p < ∞. The space ℓ p is then defined as the set of all infinite sequences of real (or complex) numbers such that the p-norm is finite.One can check that as p increases, the set ℓ p grows larger. For example, the sequenceis not in ℓ 1, but it is in ℓ p for p > 1, as the seriesdiverges for p = 1 (the harmonic series), but is convergent for p > 1.One also defines the ∞-norm using the supremum:and the corresponding space ℓ ∞ of all bounded sequences. It turns out that[2]if the right-hand side is finite, or the left-hand side is infinite. Thus, we will consider ℓ p spaces for 1 ≤ p ≤ ∞.The p-norm thus defined on ℓ p is indeed a norm, and ℓ p together with this norm is a Banach space. The fully general Lp space is obtained—as seen below — by considering vectors, not only with finitely or countably-infinitely many components, but with "arbitrarily many components"; in other words, functions. An integral instead of a sum is used to define the p-norm.An Lp space may be defined as a space of functions for which the p-th power of the absolute value is Lebesgue integrable,[3] where functions which agree almost everywhere are identified. More generally, let 1 ≤ p < ∞ and (S, Σ, μ) be a measure space. Consider the set of all measurable functions from S to C or R whose absolute value raised to the p-th power has a finite integral, or equivalently, thatThe set of such functions forms a vector space, with the following natural operations:for every scalar λ.That the sum of two p-th power integrable functions is again p-th power integrable follows from the inequalityThis can be made into a normed vector space in a standard way; one simply takes the quotient space with respect to the kernel of || · ||p. Since for any measurable function  f , we have that || f ||p = 0 if and only if  f  = 0 almost everywhere, the kernel of || · ||p does not depend upon p,In the quotient space, two functions  f  and g are identified if  f  = g almost everywhere. The resulting normed vector space is, by definition,For p = ∞, the space L∞(S, μ) is defined as follows. We start with the set of all measurable functions from S to C or R which are bounded. Again two such functions are identified if they are equal almost everywhere. Denote this set by L∞(S, μ). For a function  f  in this set, the essential supremum of its absolute value serves as an appropriate norm:As before, if there exists q < ∞ such that  f  ∈ L∞(S, μ) ∩ Lq(S, μ), thenFor 1 ≤ p ≤ ∞, Lp(S, μ) is a Banach space. The fact that Lp is complete is often referred to as the Riesz-Fischer theorem. Completeness can be checked using the convergence theorems for Lebesgue integrals.When the underlying measure space S is understood, Lp(S, μ) is often abbreviated Lp(μ), or just Lp. The above definitions generalize to Bochner spaces.Similar to the ℓp spaces, L2 is the only Hilbert space among Lp spaces. In the complex case, the inner product on L2 is defined byThe additional inner product structure allows for a richer theory, with applications to, for instance, Fourier series and quantum mechanics. Functions in L2 are sometimes called quadratically integrable functions, square-integrable functions or square-summable functions, but sometimes these terms are reserved for functions that are square-integrable in some other sense, such as in the sense of a Riemann integral (Titchmarsh 1976).If we use complex-valued functions, the space L∞ is a commutative C*-algebra with pointwise multiplication and conjugation. For many measure spaces, including all sigma-finite ones, it is in fact a commutative von Neumann algebra. An element of L∞ defines a bounded operator on any Lp space by multiplication.For 1 ≤ p ≤ ∞ the ℓp spaces are a special case of Lp spaces, when S = N, and μ is the counting measure on N. More generally, if one considers any set S with the counting measure, the resulting Lp space is denoted ℓp(S). For example, the space ℓp(Z) is the space of all sequences indexed by the integers, and when defining the p-norm on such a space, one sums over all the integers. The space ℓp(n), where n is the set with n elements, is Rn with its p-norm as defined above. As any Hilbert space, every space L2 is linearly isometric to a suitable ℓ2(I), where the cardinality of the set I is the cardinality of an arbitrary Hilbertian basis for this particular L2.The fact that κp(g) is well defined and continuous follows from Hölder's inequality. κp : Lq(μ) → Lp(μ)∗ is a linear mapping which is an isometry by the extremal case of Hölder's inequality. It is also possible to show (for example with the Radon–Nikodym theorem, see[4]) that any G ∈ Lp(μ)∗ can be expressed this way: i.e., that κp is onto. Since κp is onto and isometric, it is an isomorphism of Banach spaces. With this (isometric) isomorphism in mind, it is usual to say simply that Lq  is the dual Banach space of Lp.For 1 < p < ∞, the space Lp(μ) is reflexive. Let κp be as above and let κq : Lp(μ) → Lq(μ)∗ be the corresponding linear isometry. Consider the map from Lp(μ) to Lp(μ)∗∗, obtained by composing κq with the transpose (or adjoint) of the inverse of κp:This map coincides with the canonical embedding J of Lp(μ) into its bidual. Moreover, the map jp is onto, as composition of two onto isometries, and this proves reflexivity.If the measure μ on S is sigma-finite, then the dual of L1(μ) is isometrically isomorphic to L∞(μ) (more precisely, the map κ1 corresponding to p = 1 is an isometry from L∞(μ) onto L1(μ)∗).The dual of L∞ is subtler. Elements of L∞(μ)∗ can be identified with bounded signed finitely additive measures on S that are absolutely continuous with respect to μ. See ba space for more details. If we assume the axiom of choice, this space is much bigger than L1(μ) except in some trivial cases. However, Saharon Shelah proved that there are relatively consistent extensions of Zermelo–Fraenkel set theory (ZF + DC + "Every subset of the real numbers has the Baire property") in which the dual of ℓ∞ is ℓ1.[5]Colloquially, if 1 ≤ p < q ≤ ∞, then Lp(S, μ) contains functions that are more locally singular, while elements of Lq(S, μ) can be more spread out. Consider the Lebesgue measure on the half line (0, ∞). A continuous function in L1 might blow up near 0 but must decay sufficiently fast toward infinity. On the other hand, continuous functions in L∞ need not decay at all but no blow-up is allowed. The precise technical result is the following.[6] Suppose that 0 < p < q ≤ ∞. Then:Neither condition holds for the real line with the Lebesgue measure. In both cases the embedding is continuous, in that the identity operator is a bounded linear map fromLq to Lp in the first case,and Lp to Lq in the second.(This is a consequence of the closed graph theoremand properties of Lp spaces.) Indeed, if the domain S has finite measure,one can make the following explicit calculation using Hölder's inequalityleading toThe constant appearing in the above inequality is optimal, in the sense that the operator norm of the identity I : Lq(S, μ) → Lp(S, μ) is preciselythe case of equality being achieved exactly when  f  = 1 μ-a.e.Throughout this section we assume that: 1 ≤ p < ∞.Let (S, Σ, μ) be a measure space. An integrable simple function  f  on S is one of the formMore can be said when S is a metrizable topological space and Σ its Borel σ–algebra, i.e., the smallest σ–algebra of subsets of S containing the open sets.Suppose V ⊂ S is an open set with μ(V) < ∞. It can be proved that for every Borel set A ∈ Σ contained in V, and for every ε > 0, there exist a closed set F and an open set U such thatIt follows that there exists φ continuous on S such thatIf S can be covered by an increasing sequence (Vn) of open sets that have finite measure, then the space of p–integrable continuous functions is dense in Lp(S, Σ, μ). More precisely, one can use bounded continuous functions that vanish outside one of the open sets Vn.This applies in particular when S = Rd and when μ is the Lebesgue measure. The space of continuous and compactly supported functions is dense in Lp(Rd). Similarly, the space of integrable step functions is dense in Lp(Rd); this space is the linear span of indicator functions of bounded intervals when d = 1, of bounded rectangles when d = 2 and more generally of products of bounded intervals.Several properties of general functions in Lp(Rd) are first proved for continuous and compactly supported functions (sometimes for step functions), then extended by density to all functions. For example, it is proved this way that translations are continuous on Lp(Rd), in the following sense:whereLet (S, Σ, μ) be a measure space. If 0 < p < 1, then Lp(μ) can be defined as above: it is the vector space of those measurable functions  f  such thatAs before, we may introduce the p-norm || f ||p = Np( f )1/p, but || · || p does not satisfy the triangle inequality in this case, and defines only a quasi-norm.  The inequality (a + b) p ≤ a p + b p, valid for a, b ≥ 0 implies that (Rudin 1991, §1.47)and so the functionis a metric on Lp(μ). The resulting metric space is complete; the verification is similar to the familiar case when p ≥ 1.In this setting Lp satisfies a reverse Minkowski inequality, that is for u, v in LpThis result may be used to prove Clarkson's inequalities, which are in turn used to establish the uniform convexity of the spaces Lp for 1 < p < ∞ (Adams & Fournier 2003).The space Lp for 0 < p < 1 is an F-space: it admits a complete translation-invariant metric with respect to which the vector space operations are continuous. It is also locally bounded, much like the case p ≥ 1. It is the prototypical example of an F-space that, for most reasonable measure spaces, is not locally convex: in ℓ p or Lp([0, 1]), every open convex set containing the 0 function is unbounded for the p-quasi-norm; therefore, the 0 vector does not possess a fundamental system of convex neighborhoods. Specifically, this is true if the measure space S contains an infinite family of disjoint measurable sets of finite positive measure.The only nonempty convex open set in Lp([0, 1]) is the entire space (Rudin 1991, §1.47). As a particular consequence, there are no nonzero linear functionals on Lp([0, 1]): the dual space is the zero space. In the case of the counting measure on the natural numbers (producing the sequence space Lp(μ) = ℓ p), the bounded linear functionals on ℓ p are exactly those that are bounded on ℓ 1, namely those given by sequences in ℓ ∞. Although ℓ p does contain non-trivial convex open sets, it fails to have enough of them to give a base for the topology.The situation of having no linear functionals is highly undesirable for the purposes of doing analysis. In the case of the Lebesgue measure on Rn, rather than work with Lp for 0 < p < 1, it is common to work with the Hardy space H p whenever possible, as this has quite a few linear functionals: enough to distinguish points from one another. However, the Hahn–Banach theorem still fails in H p for p < 1 (Duren 1970, §7.5).The vector space of (equivalence classes of) measurable functions on (S, Σ, μ) is denoted L0(S, Σ, μ) (Kalton, Peck & Roberts 1984). By definition, it contains all the Lp, and is equipped with the topology of convergence in measure. When μ is a probability measure (i.e., μ(S) = 1), this mode of convergence is named convergence in probability.The description is easier when μ is finite. If μ is a finite measure on (S, Σ), the 0 function admits for the convergence in measure the following fundamental system of neighborhoodsThe topology can be defined by any metric d of the formwhere φ is bounded continuous concave and non-decreasing on [0, ∞), with φ(0) = 0 and φ(t) > 0 when t > 0 (for example, φ(t) = min(t, 1)). Such a metric is called Lévy-metric for L0. Under this metric the space L0 is complete (it is again an F-space). The space L0 is in general not locally bounded, and not locally convex.For the infinite Lebesgue measure λ on Rn, the definition of the fundamental system of neighborhoods could be modified as followsThe resulting space L0(Rn, λ) coincides as topological vector space with L0(Rn, g(x) dλ(x)), for any positive λ–integrable density g.Let (S, Σ, μ) be a measure space, and f a measurable function with real or complex values on S. The distribution function of f is defined for t > 0 byIf f is in Lp(S, μ) for some p with 1 ≤ p < ∞, then by Markov's inequality,A function f is said to be in the space weak Lp(S, μ), or Lp,w(S, μ), if there is a constant C > 0 such that, for all t > 0,The best constant C for this inequality is the Lp,w-norm of f, and is denoted byThe weak Lp coincide with the Lorentz spaces Lp,∞, so this notation is also used to denote them.The Lp,w-norm is not a true norm, since the triangle inequality fails to hold. Nevertheless, for f in Lp(S, μ),and in particular Lp(S, μ) ⊂ Lp,w(S, μ). In fact, one hasUnder the convention that two functions are equal if they are equal μ almost everywhere, then the spaces Lp,w are complete (Grafakos 2004).For any 0 < r < p the expressionis comparable to the Lp,w-norm. Further in the case p > 1, this expression defines a norm if r = 1. Hence for p > 1 the weak Lp spaces are Banach spaces (Grafakos 2004).A major result that uses the Lp,w-spaces is the Marcinkiewicz interpolation theorem, which has broad applications to harmonic analysis and the study of singular integrals.As before, consider a measure space (S, Σ, μ). Let w : S → [0, ∞) be a measurable function. The w-weighted Lp space is defined as Lp(S, w dμ), where w dμ means the measure ν defined byor, in terms of the Radon–Nikodym derivative, w = dν/dμ  the norm for Lp(S, w dμ) is explicitlyAs Lp-spaces, the weighted spaces have nothing special, since Lp(S, w dμ) is equal to Lp(S, dν). But they are the natural framework for several results in harmonic analysis (Grafakos 2004); they appear for example in the Muckenhoupt theorem: for 1 < p < ∞, the classical Hilbert transform is defined on Lp(T, λ) where T denotes the unit circle and λ the Lebesgue measure; the (nonlinear) Hardy–Littlewood maximal operator is bounded on Lp(Rn, λ). Muckenhoupt's theorem describes weights w such that the Hilbert transform remains bounded on Lp(T, w dλ) and the maximal operator on Lp(Rn, w dλ).One may also define spaces Lp(M) on a manifold, called the intrinsic Lp spaces of the manifold, using densities.
Matrix pencil

Transpose of a linear map
In linear algebra, the transpose of a linear map between two vector spaces, defined over the same field, is an induced map between the dual spaces of the two vector spaces. The transpose of a linear map is often used to study the original linear map. This concept is generalised by adjoint functors.Let V and W be vector spaces over the same field. If f : V → W is a linear map, then the transpose[1] (or dual, or adjoint[2]), is defined to beThe resulting functional tf(φ) is called the pullback of φ along f.The following identity, which characterises the transpose,[3] holds for all φ ∈ W∗ and v ∈ V:where the bracket [·,·]V is the natural pairing of V's dual space with V, and [·,·]W is the same with W.The assignment f ↦ tf produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W∗ to V∗. If V = W then the space of linear maps is an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that t(fg) = tg tf. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify t(tf) with f using the natural injection into the double dual.If the linear map f is represented by the matrix A with respect to two bases of V and W, then tf is represented by the transpose matrix AT with respect to the dual bases of W∗ and V∗, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, tf is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.The identity that characterizes the transpose, that is, [f∗(φ), v] = [φ, f(v)], is formally similar to the definition of the Hermitian adjoint, however, the transpose and the Hermitian adjoint are not the same map. The difference stems from the fact that transpose is defined by a bilinear form while the Hermitian adjoint is defined by a sesquilinear form. Furthermore, while the transpose can be defined on any vector space, the Hermitian adjoint is defined on Hilbert spaces.If X and Y are Hilbert spaces and u : X → Y is a linear map then the transpose of u and the Hermitian adjoint of u, which we will denote respectively by tu and u∗, are related. Denote by I : X → X∗ and J : Y → Y∗ the canonical antilinear isometries of the Hilbert spaces X and Y onto their duals. Then u∗ is the following composition of maps:[5]Suppose that X and Y are topological vector spaces and that u : X → Y is a linear map, then many of u's properties are reflected in u∗.
Linear span
In linear algebra, the linear span (also called the linear hull or just span) of a set of vectors in a vector space is the intersection of all linear subspaces which each contain every vector in that set. The linear span of a set of vectors is therefore a vector space. Spans can be generalized to matroids and modules.For expressing that a vector space V is a span of a set S, one commonly uses the following phrases: S spans V; V is spanned by S; S is a spanning set of V; S is a generating set of V.Given a vector space V over a field K, the span of a set S of vectors (not necessarily infinite) is defined to be the intersection W of all subspaces of V that contain S. W is referred to as the subspace spanned by S, or by the vectors in S. Conversely, S is called a spanning set of W, and we say that S spans W.Alternatively, the span of S may be defined as the set of all finite linear combinations of elements of S, which follows from the above definition.In particular, if S is a finite subset of V, then the span of S is the set of all linear combinations of the elements of S. In the case of infinite S, infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming such sums are defined somehow, e.g. if V is a Banach space) are excluded by the definition; a generalization that allows these is not equivalent.The real vector space R3 has {(-1,0,0), (0,1,0), (0,0,1)} as a spanning set. This particular spanning set is also a basis. If (-1,0,0) were replaced by (1,0,0), it would also form the canonical basis of R3.Another spanning set for the same space is given by {(1,2,3), (0,1,2), (−1,1/2,3), (1,1,1)}, but this set is not a basis, because it is linearly dependent.The set {(1,0,0), (0,1,0), (1,1,0)} is not a spanning set of R3; instead its span is the space of all vectors in R3 whose last component is zero. That space (the space of all vectors in R3 whose last component is zero) is also spanned by the set {(1,0,0), (0,1,0)}, as (1,1,0) is a linear combination of (1,0,0) and (0,1,0). It does, however, span R2.The empty set is a spanning set of {(0, 0, 0)} since the empty set is a subset of all possible vector spaces in R3, and {(0, 0, 0)} is the intersection of all of these vector spaces.The set of functions xn where n is a non-negative integer spans the space of polynomials.Theorem 1: The subspace spanned by a non-empty subset S of a vector space V is the set of all linear combinations of vectors in S.This theorem is so well known that at times it is referred to as the definition of span of a set.Theorem 2: Every spanning set S of a vector space V must contain at least as many elements as any linearly independent set of vectors from V.Theorem 3: Let V be a finite-dimensional vector space. Any set of vectors that spans V can be reduced to a basis for V by discarding vectors if necessary (i.e. if there are linearly dependent vectors in the set). If the axiom of choice holds, this is true without the assumption that V has finite dimension.This also indicates that a basis is a minimal spanning set when V is finite-dimensional.Generalizing the definition of the span of points in space, a subset X of the ground set of a matroid is called a spanning set if the rank of X equals the rank of the entire ground set[citation needed].The vector space definition can also be generalized to modules.[1] Given an R-module A and any collection of elements a1,…,an of A, then the sum of cyclic modules,consisting of all R-linear combinations of the given elements ai, is called the submodule of A spanned by a1,…,an. As with the case of vector spaces, the submodule of A spanned by any subset of A is the intersection of all the submodules containing that subset.In functional analysis, a closed linear span of a set of vectors is the minimal closed set which contains the linear span of that set.One mathematical formulation of this isThe closed linear span of the set of functions xn on the interval [0, 1], where n is a non-negative integer, depends on the norm used. If the L2 norm is used, then the closed linear span is the Hilbert space of square-integrable functions on the interval. But if the maximum norm is used, the closed linear span will be the space of continuous functions on the interval. In either case, the closed linear span contains functions that are not polynomials, and so are not in the linear span itself. However, the cardinality of the set of functions in the closed linear span is the cardinality of the continuum, which is the same cardinality as for the set of polynomials.The linear span of a set is dense in the closed linear span. Moreover, as stated in the lemma below, the closed linear span is indeed the closure of the linear span.Closed linear spans are important when dealing with closed linear subspaces (which are themselves highly important, consider Riesz's lemma).Let X be a normed space and let E be any non-empty subset of X. Then(So the usual way to find the closed linear span is to find the linear span first, and then the closure of that linear span.)
Dimension (vector space)
In mathematics, the dimension of a vector space V is the cardinality (i.e. the number of vectors) of a basis of V over its base field.[1] It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension.For every vector space there exists a basis,[a] and all bases of a vector space have equal cardinality;[b] as a result, the dimension of a vector space is uniquely defined. We say V is finite-dimensional if the dimension of V is finite, and infinite-dimensional if its dimension is infinite.The dimension of the vector space V over the field F can be written as dimF(V) or as [V : F], read "dimension of V over F". When F can be inferred from context, dim(V) is typically written.The vector space R3 has as a basis, and therefore we have dimR(R3) = 3. More generally, dimR(Rn) = n, and even more generally, dimF(Fn) = n for any field F.The complex numbers C are both a real and complex vector space; we have dimR(C) = 2 and dimC(C) = 1. So the dimension depends on the base field.The only vector space with dimension 0 is {0}, the vector space consisting only of its zero element.If W is a linear subspace of V, then dim(W) ≤ dim(V).To show that two finite-dimensional vector spaces are equal, one often uses the following criterion: if V is a finite-dimensional vector space and W is a linear subspace of V with dim(W) = dim(V), then W = V.Rn has the standard basis {e1, ..., en}, where ei is the i-th column of the corresponding identity matrix. Therefore Rn has dimension n.Any two vector spaces over F having the same dimension are isomorphic. Any bijective map between their bases can be uniquely extended to a bijective linear map between the vector spaces. If B is some set, a vector space with dimension |B| over F can be constructed as follows: take the set F(B) of all functions f : B → F such that f(b) = 0 for all but finitely many b in B. These functions can be added and multiplied with elements of F, and we obtain the desired F-vector space. An important result about dimensions is given by the rank–nullity theorem for linear maps.If F/K is a field extension, then F is in particular a vector space over K. Furthermore, every F-vector space V is also a K-vector space. The dimensions are related by the formulaIn particular, every complex vector space of dimension n is a real vector space of dimension 2n.Some simple formulae relate the dimension of a vector space with the cardinality of the base field and the cardinality of the space itself.If V is a vector space over a field F then, denoting the dimension of V by dim V, we have:One can see a vector space as a particular case of a matroid, and in the latter there is a well-defined notion of dimension. The length of a module and the rank of an abelian group both have several properties similar to the dimension of vector spaces.The Krull dimension of a commutative ring, named after Wolfgang Krull (1899–1971), is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring.Alternatively, one may be able to take the trace of operators on an infinite-dimensional space; in this case a (finite) trace is defined, even though no (finite) dimension exists, and gives a notion of "dimension of the operator". These fall under the rubric of "trace class operators" on a Hilbert space, or more generally nuclear operators on a Banach space.
Dual basis
In linear algebra, given a vector space V with a basis B of vectors indexed by an index set I (the cardinality of I is the dimensionality of V), its dual set is a set B∗ of vectors in the dual space V∗ with the same index set I such that B and B∗ form a biorthogonal system. The dual set is always linearly independent but does not necessarily span V∗. If it does span V∗, then B∗ is called the dual basis or reciprocal basis for the basis B.The dual set always exists and gives an injection from V into V∗, namely the mapping that sends vi to vi. This says, in particular, that the dual space has dimension greater or equal to that of V.The dual of an infinite-dimensional space has greater dimensionality (this being a greater infinite cardinality) than the original space has, and thus these cannot have a basis with the same indexing set. However, a dual set of vectors exists, which defines a subspace of the dual isomorphic to the original space. Further, for topological vector spaces, a continuous dual space can be defined, in which case a dual basis may exist.In the case of finite-dimensional vector spaces, the dual set is always a dual basis and it is unique. These bases are denoted by B = { e1, …, en } and B∗ = { e1, …, en }. If one denotes the evaluation of a covector on a vector as a pairing, the biorthogonality condition becomes:The association of a dual basis with a basis gives a map from the space of bases of V to the space of bases of V∗, and this is also an isomorphism. For topological fields such as the real numbers, the space of duals is a topological space, and this gives a homeomorphism between the Stiefel manifolds of bases of these spaces.To perform operations with a vector, we must have a straightforward method of calculating its components. In a Cartesian frame the necessary operation is the dot product of the vector and the base vector.[1] E.g.,In a non-Cartesian frame, we do not necessarily have ei · ej = 0 for all i ≠ j. However, it is always possible to find a vector ei such thatthe equality holds when ei is the dual base of eiFor example, the standard basis vectors of R2 (the Cartesian plane) areand the standard basis vectors of its dual space R2* areIn 3-dimensional Euclidean space, for a given basis {e1, e2, e3}, you can find the biorthogonal (dual) basis {e1, e2, e3} by formulas below:where T denotes the transpose and
Kempner series
The Kempner series is a modification of the harmonic series, formed by omitting all terms whose denominator expressed in base 10 contains the digit 9. That is, it is the sumwhere the prime indicates that n takes only values whose decimal expansion has no nines. The series was first studied by A. J. Kempner in 1914.[1] The series is counter-intuitive because, unlike the harmonic series, it converges. Kempner showed the sum of this series is less than 80. Baillie[2] showed that, rounded to 20 decimals, the actual sum is 22.92067 66192 64150 34816(sequence A082838 in the OEIS).Heuristically, this series converges because most large integers contain all digits. For example, a random 100-digit integer is very likely to contain at least one '9', causing it to be excluded from the above sum.Schmelzer and Baillie[3] found an efficient algorithm for the more general problem of any omitted string of digits. For example, the sum of 1/n where n has no "42" is about 228.44630 41592 30813 25415. Another example: the sum of 1/n where n has no occurrence of the digit string "314159" is about 2302582.33386 37826 07892 02376. (All values are rounded in the last decimal place).Kempner's proof of convergence[1] is repeated in many textbooks, for example Hardy and Wright[4]:120 and Apostol.[5]:212 We group the terms of the sum by the number of digits in the denominator. The number of n-digit positive integers that have no digit equal to '9' is 8(9n−1) because there are 8 choices (1 through 8) for the first digit, and 9 independent choices (0 through 8) for each of the other n−1 digits. Each of these numbers having no '9' is greater than or equal to 10n−1, so the reciprocal of each of these numbers is less than or equal to 101−n. Therefore, the contribution of this group to the sum of reciprocals is less than 8(9/10)n−1. Therefore the whole sum of reciprocals is at mostThe same argument works for any omitted non-zero digit. The number of n-digit positive integers that have no '0' is 9n, so the sum of 1/n where n has no digit '0' is at mostThe series also converge if strings of k digits are omitted, for example if we omit all denominators that have a decimal substring of 42. This can be proved in almost the same way.[3] First we observe that we can work with numbers in base 10k and omit all denominators that have the given string as a "digit". The analogous argument to the base 10 case shows that this series converges. Now switching back to base 10, we see that this series contains all denominators that omit the given string, as well as denominators that include it if it is not on a "k-digit" boundary. For example, if we are omitting 42, the base-100 series would omit 4217 and 1742, but not 1427, so it is larger than the series that omits all 42s.Farhi[6] considered generalized Kempner series, namely, the sums S(d, n) of the reciprocals of the positive integers that have exactly n instances of the digit d where 0 ≤ d ≤ 9 (so that the original Kempner series is S(9, 0)). He showed that for each d the sequence of values S(d, n) for n ≥ 1 is decreasing and converges to 10 ln 10. The sequence is not in general decreasing starting with n = 0; for example, for the original Kempner series we have S(9, 0) ≈ 22.921 < 23.026 ≈ 10 ln 10 < S(9, n) for n ≥ 1.The series converges extremely slowly. Baillie[2] remarks that after summing 1027 terms the remainder is still larger than 1.The upper bound of 80 is very crude, and Irwin showed[7] by a slightly finer analysis of the bounds that the value of the Kempner series is near 23, since refined to about 22.92067.[8]Baillie[2] developed a recursion that expresses the contribution from each k+1-digit block in terms of the contributions of the k-digit blocks for all choices of omitted digit. This permits a very accurate estimate with a small amount of computation.Most authors do not name this series. The name "Kempner series" is used in MathWorld[9] and in Havil's book Gamma on the Euler–Mascheroni constant.[10]:31–33
Generalized eigenvector
butHere are some examples to illustrate the concept of generalized eigenvectors.  Some of the details will be described later.This example is simple but clearly illustrates the point.  This type of matrix is used frequently in textbooks.[38][39][40]SupposeWriting out the values:This simplifies toNote thatThis example is more complex than Example 1.  Unfortunately, it is a little difficult to construct an interesting example of low order.[41]The matrix    (1)Thus, in general,    (2)Definition:  A set of n linearly independent generalized eigenvectors is a canonical basis if it is composed entirely of Jordan chains.Now defineThe matrixWe now define    (3)but    (4)ThenandandFind a matrix in Jordan normal form that is similar toandandwiththenConsider the problem of solving the system of linear ordinary differential equations    (5)where    (6)In this case, the general solution is given by    (7)where    (8)The solution of (7) is    (9)
Predictor–corrector method
In numerical analysis, predictor–corrector methods belong to a class of algorithms designed to integrate ordinary differential equations – to find an unknown function that satisfies a given differential equation.  All such algorithms proceed in two steps: When considering the numerical solution of ordinary differential equations (ODEs), a predictor–corrector method typically uses an explicit method for the predictor step and an implicit method for the corrector step.A simple predictor–corrector method (known as Heun's method) can be constructed from the Euler method (an explicit method) and the trapezoidal rule (an implicit method).Consider the differential equationNext, the corrector step: improve the initial guess using trapezoidal rule,That value is used as the next step.There are different variants of a predictor–corrector method, depending on how often the corrector method is applied. The Predict–Evaluate–Correct–Evaluate (PECE) mode refers to the variant in the above example:It is also possible to evaluate the function f only once per step by using the method in Predict–Evaluate–Correct (PEC) mode:Additionally, the corrector step can be repeated in the hope that this achieves an even better approximation to the true solution. If the corrector method is run twice, this yields the PECECE mode:The PECEC mode has one fewer function evaluation. More generally, if the corrector is run k times, the method is in P(EC)kor P(EC)kE mode. If the corrector method is iterated until it converges, this could be called PE(CE)∞.[1]
Computing the permanent
In linear algebra, the computation of the permanent of a matrix is a problem that is thought to be more difficult than the computation of the determinant of a matrix despite the apparent similarity of the definitions.The permanent is defined similarly to the determinant, as a sum of products of sets of matrix entries that lie in distinct rows and columns. However, where the determinant weights each of these products with a ±1 sign based on the parity of the set, the permanent weights them all with a +1 sign.While the determinant can be computed in polynomial time by Gaussian elimination, it is generally believed that the permanent cannot be computed in polynomial time. In computational complexity theory, a theorem of Valiant states that computing permanents is #P-hard, and even #P-complete for matrices in which all entries are 0 or 1.Valiant (1979)  This puts the computation of the permanent in a class of problems believed to be even more difficult to compute than NP. It is known that computing the permanent is impossible for logspace-uniform ACC0 circuits.(Allender & Gore 1994)The development of both exact and approximate algorithms for computing the permanent of a matrix is an active area of research.The permanent of an n-by-n matrix A = (ai,j) is defined asThe sum here extends over all elements σ of the symmetric group Sn, i.e. over all permutations of the numbers 1, 2, ..., n. This formula differs from the corresponding formula for the determinant only in that, in the determinant, each product is multiplied by the sign of the permutation σ while in this formula each product is unsigned. The formula may be directly translated into an algorithm that naively expands the formula, summing over all permutations and within the sum multiplying out each matrix entry. This requires n! n arithmetic operations.It may be rewritten in terms of the matrix entries as follows[3]Another formula that appears to be as fast as Ryser's (or perhaps even twice as fast) is to be found in the two Ph.D. theses; see (Balasubramanian 1980), (Bax 1998); also(Bax & Franklin 1996). The methods to find the formula are quite different, being related to the combinatorics of the Muir algebra, and to finite difference theory respectively.  Another way, connected with invariant theory is via the polarization identity for a symmetric tensor (Glynn 2010).  The formula generalizes to infinitely many others, as found by all these authors, although it is not clear if they are any faster than the basic one. See (Glynn 2013).The simplest known formula of this type (when the characteristic of the field is not two) isThe number of perfect matchings in a bipartite graph is counted by the permanent of the graph's biadjacency matrix, and the permanent of any 0-1 matrix can be interpreted in this way as the number of perfect matchings in a graph. For planar graphs (regardless of bipartiteness), the FKT algorithm computes the number of perfect matchings in polynomial time by changing the signs of a carefully chosen subset of the entries in the Tutte matrix of the graph, so that the Pfaffian of the resulting skew-symmetric matrix (the square root of its determinant) is the number of perfect matchings. This technique can be generalized to graphs that contain no subgraph homeomorphic to the complete bipartite graph K3,3.[5]There are various formulae given by Glynn (2010) for the computation modulo a prime p.Firstly, there is one using symbolic calculations with partial derivatives.(Actually the above expansion can be generalized in an arbitrary characteristic p as the following pair of dual identities:This formula implies the following identities over fields of characteristic 3:and it allows to polynomial-time reduce the computation of the permanent of an nxn-matrix with a subset of k or k-1 rows expressible as linear combinations of another (disjoint) subset of k rows to the computation of the permanent of an (n-k)x(n-k)- or (n-k+1)x(n-k+1)-matrix correspondingly, hence having introduced a compression operator (analogical to the Gaussian modification applied for calculating the determinant) that "preserves" the permanent in characteristic 3. (Analogically, it would be worth noting that the Hamiltonian cycle polynomial in characteristic 2 does possess its invariant matrix compressions as well, taking into account the fact that ham(A) = 0 for any nxn-matrix A having three equal rows or, if n > 2, a pair of indexes i,j such that its i-th and j-th rows are identical and its i-th and j-th columns are identical too.) The closure of that operator defined as the limit of its sequential application together with the transpose transformation (utilized each time the operator leaves the matrix intact) is also an operator mapping, when applied to classes of matrices, one class to another. While the compression operator maps the class of 1-semi-unitary matrices into itself and the classes of unitary and 2-semi-unitary ones, the compression-closure of the 1-semi-unitary class (as well as the class of matrices received from unitary ones through replacing one row by an arbitrary row vector — the permanent of such a matrix is, via the Laplace expansion, the sum of the permanents of 1-semi-unitary matrices and, accordingly, polynomial-time computable) is yet unknown and tensely related to the general problem of the permanent's computational complexity in characteristic 3 and the chief question of P versus NP: as it was shown in (Knezevic & Cohen (2017)), if such a compression-closure is the set of all square matrices over a field of characteristic 3 or, at least, contains a matrix class the permanent's computation on is #3-P-complete (like the class of 2-semi-unitary matrices) then the permanent is computable in polynomial time in this characteristic.When the entries of A are nonnegative, the permanent can be computed approximately in probabilistic polynomial time, up to an error of εM, where M is the value of the permanent and ε > 0 is arbitrary. In other words, there exists a fully polynomial-time randomized approximation scheme (FPRAS) (Jerrum, Vigoda & Sinclair (2001)).The most difficult step in the computation is the construction of an algorithm to sample almost uniformly from the set of all perfect matchings in a given bipartite graph: in other words, a fully polynomial almost uniform sampler (FPAUS). This can be done using a Markov chain Monte Carlo algorithm that uses a Metropolis rule to define and run a Markov chain whose distribution is close to uniform, and whose mixing time is polynomial.Another class of matrices for which the permanent can be computed approximately, is the set of positive-semidefinite matrices (the complexity-theoretic problem of approximating the permanent of such matrices to within a multiplicative error is considered open[7]). The corresponding randomized algorithm is based on the model of boson sampling and it uses the tools proper to quantum optics, to represent the permanent of positive-semidefinite matrices as the expected value of a specific random variable. The latter is then approximated by its sample mean.[8] This algorithm, for a certain set of positive-semidefinite matrices, approximates their permanent in polynomial time up to an additive error, which is more reliable than that of the standard classical polynomial-time algorithm by Gurvits.[9]
Sesquilinear form
In mathematics, a sesquilinear form is a generalization of a bilinear form that, in turn, is a generalization of the concept of the dot product of Euclidean space. A bilinear form is linear in each of its arguments, but a sesquilinear form allows one of the arguments to be "twisted" in a semilinear manner, thus the name; which originates from the Latin numerical prefix sesqui- meaning "one and a half". The basic concept of the dot product – producing a scalar from a pair of vectors – can be generalized by allowing a broader range of scalar values and, perhaps simultaneously, by widening the definition of what a vector is.A motivating special case is a sesquilinear form on a complex vector space, V. This is a map V × V → C that is linear in one argument and "twists" the linearity of the other argument by complex conjugation (referred to as being antilinear in the other argument). This case arises naturally in mathematical physics applications. Another important case allows the scalars to come from any field and the twist is provided by a field automorphism.An application in projective geometry requires that the scalars come from a division ring (skewfield), K, and this means that the "vectors" should be replaced by elements of a K-module. In a very general setting, sesquilinear forms can be defined over R-modules for arbitrary rings R.Conventions differ as to which argument should be linear. In the commutative case, we shall take the first to be linear, as is common in the mathematical literature, except in the section devoted to sesquilinear forms on complex vector spaces. There we use the other convention and take the first argument to be conjugate-linear (i.e. antilinear) and the second to be linear. This is the convention used mostly by mathematical physicists[1] and originates in Dirac's bra–ket notation in quantum mechanics.In the more general noncommutative setting, with right modules we take the second argument to be linear and with left modules we take the first argument to be linear.Over a complex vector space V a map φ : V × V → C is sesquilinear iffor all x, y, z, w ∈ V and all a, b ∈ C. a is the complex conjugate of a.A complex sesquilinear form can also be viewed as a complex bilinear mapwhere V is the complex conjugate vector space to V. By the universal property of tensor products these are in one-to-one correspondence with complex linear mapsFor a fixed z in V the map w ↦ φ(z, w) is a linear functional on V (i.e. an element of the dual space V∗). Likewise, the w ↦ φ(w, z) is a conjugate-linear functional on V.Given any complex sesquilinear form φ on V we can define a second complex sesquilinear form ψ via the conjugate transpose:In general, ψ and φ will be different. If they are the same then φ is said to be Hermitian. If they are negatives of one another, then φ is said to be skew-Hermitian. Every sesquilinear form can be written as a sum of a Hermitian form and a skew-Hermitian form.If V is a finite-dimensional complex vector space, then relative to any basis { ei } of V, a sesquilinear form is represented by a matrix Φ, w by the column vector w, and z by the column vector z:The components of Φ are given by Φij = φ(ei, ej).A complex Hermitian form (also called a symmetric sesquilinear form), is a sesquilinear form h : V × V → C such thatThe standard Hermitian form on Cn is given (again, using the "physics" convention of linearity in the second and conjugate linearity in the first variable) byMore generally, the inner product on any complex Hilbert space is a Hermitian form.A vector space with a Hermitian form (V, h) is called a Hermitian space.The matrix representation of a complex Hermitian form is a Hermitian matrix.A complex Hermitian form applied to a single vectoris always real. One can show that a complex sesquilinear form is Hermitian iff the associated quadratic form is real for all z ∈ V.A complex skew-Hermitian form (also called an antisymmetric sesquilinear form), is a complex sesquilinear form s : V × V → C such thatEvery complex skew-Hermitian form can be written as i times a Hermitian form.The matrix representation of a complex skew-Hermitian form is a skew-Hermitian matrix.A complex skew-Hermitian form applied to a single vectoris always pure imaginary.This section applies unchanged when the division ring K is commutative.  More specific terminology then also applies: the division ring is a field, the anti-automorphism is also an automorphism, and the right module is a vector space.  The following applies to a left module with suitable reordering of expressions.A σ-sesquilinear form over a right K-module M is a bi-additive map φ : M × M → K with an associated anti-automorphism σ of a division ring K such that, for all x, y ∈ M and all α, β ∈ K,The associated anti-automorphism σ for any nonzero sesquilinear form φ is uniquely determined by φ.Given a sesquilinear form φ over a module M and a subspace W of M, the orthogonal complement of W with respect to φ isSimilarly, x ∈ M is orthogonal to y ∈ M with respect to φ, written x ⊥φ y (or simply x ⊥ y if φ can be inferred from the context), when φ(x, y) = 0.  This relation need not be symmetric, i.e. x ⊥ y does not imply y ⊥ x (but see § Reflexivity below).A sesquilinear form φ is reflexive if, for all x, y ∈ M,That is, a sesquilinear form is reflexive precisely when the derived orthogonality relation is symmetric.A σ-sesquilinear form φ is called (σ, ε)-Hermitian if there exists ε ∈ K such that, for all x, y ∈ M,If ε = 1, the form is called σ-Hermitian, and if ε = −1, it is called σ-anti-Hermitian.  (When σ is implied, respectively simply Hermitian or anti-Hermitian.)For a nonzero (σ, ε)-Hermitian form, it follows that, for all α ∈ K,It also follows that φ(x, x) is a fixed point of the map α ↦ σ(α)ε.  The fixed points of this map from a subgroup of the additive group of K.A (σ, ε)-Hermitian form is reflexive, and every reflexive σ-sesquilinear form is (σ, ε)-Hermitian for some ε.[2][3][4][5]In the special case that σ is the identity map (i.e., σ = id), K is commutative, φ is a bilinear form and ε2 = 1.  Then for ε = 1 the bilinear form is called symmetric, and for ε = −1 is called skew-symmetric.[6]Let V be the three dimensional vector space over the finite field F = GF(q2), where q is a prime power. With respect to the standard basis we can write x = (x1, x2, x3) and y = (y1, y2, y3) and define the map φ by:The map σ : t ↦ tq is an involutory automorphism of F. The map φ is then a σ-sesquilinear form. The matrix Mφ associated to this form is the identity matrix. This is a Hermitian form.In a projective geometry G, a permutation δ of the subspaces that inverts inclusion, i.e.is called a correlation. A result of Birkhoff and von Neumann (1936)[7] shows that the correlations of desarguesian projective geometries correspond to the nondegenerate sesquilinear forms on the underlying vector space.[5] A sesquilinear form φ is nondegenerate if φ(x, y) = 0 for all y in V (if and) only if x = 0.To achieve full generality of this statement, and since every desarguesian projective geometry may be coordinatized by a division ring, Reinhold Baer extended the definition of a sesquilinear form to a division ring, which requires replacing vector spaces by R-modules.[8] (In the geometric literature these are still referred to as either left or right vector spaces over skewfields.)[9] The specialization of the above section to skewfields was a consequence of the application to projective geometry, and not intrinsic to the nature of sesquilinear forms. Only the minor modifications needed to take into account the non-commutativity of multiplication are required to generalize the arbitrary field version of the definition to arbitrary rings. Let R be a ring, V an R-module and σ an antiautomorphism of R.A map φ : V × V → R is σ-sesquilinear iffor all x, y, z, w ∈ V and all c, d ∈ R.A element x is orthogonal to another element y with respect to the sesquilinear form φ (written x ⊥ y) if φ(x, y) = 0.  This relation need not be symmetric, i.e. x ⊥ y does not imply y ⊥ x.A sesquilinear form φ : V × V → R is reflexive (or orthosymmetric) if φ(x, y) = 0 implies φ(y, x) = 0 for all x, y ∈ V.A sesquilinear form φ : V × V → R is Hermitian if there exists σ such that[10]:325for all x, y ∈ V.  A Hermitian form is necessarily reflexive, and if it is nonzero, the associated antiautomorphism σ is an involution (i.e. of order 2).Since for an antiautomorphism σ we have σ(st) = σ(t) σ(s) for all s, t in R, if σ = id, then R must be commutative and φ is a bilinear form. In particular, if, in this case, R is a skewfield, then R is a field and V is a vector space with a bilinear form.An antiautomorphism σ : R → R can also be viewed as an isomorphism of R → Rop, the opposite ring based on the same set with the same addition, but whose multiplication operation (∗) is defined by a ∗ b = ba, where the product on the right is the product in R. It follows from this that a right (left) R-module V can be turned into a left (right) Rop-module, Vo.[11] Thus, the sesquilinear form φ : V × V → R can be viewed as a bilinear form φ′ : V × Vo → R.
Complex plane
In mathematics, the complex plane or z-plane is a geometric representation of the complex numbers established by the real axis and the perpendicular imaginary axis. It can be thought of as a modified Cartesian plane, with the real part of a complex number represented by a displacement along the x-axis, and the imaginary part by a displacement along the y-axis.[1]The concept of the complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed most easily in polar coordinates—the magnitude or modulus of the product is the product of the two absolute values, or moduli, and the angle or argument of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.The complex plane is sometimes known as the Argand plane. In complex analysis, the complex numbers are customarily represented by the symbol z, which can be separated into its real (x) and imaginary (y) parts:for example: z = 4 + 5i, where x and y are real numbers, and i is the imaginary unit. In this customary notation the complex number z corresponds to the point (x, y) in the Cartesian plane.In the Cartesian plane the point (x, y) can also be represented in polar coordinates asIn the Cartesian plane it may be assumed that the arctangent takes values from −π/2 to π/2 (in radians), and some care must be taken to define the real arctangent function for points (x, y) when x ≤ 0.[2] In the complex plane these polar coordinates take the formwhereHere |z| is the absolute value or modulus of the complex number z;  θ, the argument of z, is usually taken on the interval 0 ≤ θ < 2π; and the last equality (to |z|eiθ) is taken from Euler's formula. Notice that without the constraint on the range of θ, the argument of z is multi-valued, because the complex exponential function is periodic, with period 2π i. Thus, if θ is one value of arg(z), the other values are given by arg(z) = θ + 2nπ, where n is any integer ≠ 0.[4] The theory of contour integration comprises a major part of complex analysis. In this context the direction of travel around a closed curve is important – reversing the direction in which the curve is traversed multiplies the value of the integral by −1. By convention the positive direction is counterclockwise. For example, the unit circle is traversed in the positive direction when we start at the point z = 1, then travel up and to the left through the point z = i, then down and to the left through −1, then down and to the right through −i, and finally up and to the right to z = 1, where we started.Almost all of complex analysis is concerned with complex functions – that is, with functions that map some subset of the complex plane into some other (possibly overlapping, or even identical) subset of the complex plane. Here it is customary to speak of the domain of f(z) as lying in the z-plane, while referring to the range or image of f(z) as a set of points in the w-plane. In symbols we writeand often think of the function f as a transformation from the z-plane (with coordinates (x, y)) into the w-plane (with coordinates (u, v)).Argand diagram refers to a geometric plot of complex numbers as points z=x+iy using the x-axis as the real axis and y-axis as the imaginary axis.[5]. Such plots are named after Jean-Robert Argand (1768–1822), although they were first described by Norwegian–Danish land surveyor and mathematician Caspar Wessel (1745–1818).[6] Argand diagrams are frequently used to plot the positions of the zeros and poles of a function in the complex plane. It can be useful to think of the complex plane as if it occupied the surface of a sphere. Given a sphere of unit radius, place its center at the origin of the complex plane, oriented so that the equator on the sphere coincides with the unit circle in the plane, and the north pole is "above" the plane.We can establish a one-to-one correspondence between the points on the surface of the sphere minus the north pole and the points in the complex plane as follows. Given a point in the plane, draw a straight line connecting it with the north pole on the sphere. That line will intersect the surface of the sphere in exactly one other point. The point z = 0 will be projected onto the south pole of the sphere. Since the interior of the unit circle lies inside the sphere, that entire region (|z| < 1) will be mapped onto the southern hemisphere. The unit circle itself (|z| = 1) will be mapped onto the equator, and the exterior of the unit circle (|z| > 1) will be mapped onto the northern hemisphere, minus the north pole. Clearly this procedure is reversible – given any point on the surface of the sphere that is not the north pole, we can draw a straight line connecting that point to the north pole and intersecting the flat plane in exactly one point.Under this stereographic projection the north pole itself is not associated with any point in the complex plane. We perfect the one-to-one correspondence by adding one more point to the complex plane – the so-called point at infinity – and identifying it with the north pole on the sphere. This topological space, the complex plane plus the point at infinity, is known as the extended complex plane. We speak of a single "point at infinity" when discussing complex analysis. There are two points at infinity (positive, and negative) on the real number line, but there is only one point at infinity (the north pole) in the extended complex plane.[7]Imagine for a moment what will happen to the lines of latitude and longitude when they are projected from the sphere onto the flat plane. The lines of latitude are all parallel to the equator, so they will become perfect circles centered on the origin z = 0. And the lines of longitude will become straight lines passing through the origin (and also through the "point at infinity", since they pass through both the north and south poles on the sphere).This is not the only possible yet plausible stereographic situation of the projection of a sphere onto a plane consisting of two or more values. For instance, the north pole of the sphere might be placed on top of the origin z = −1 in a plane that is tangent to the circle. The details don't really matter. Any stereographic projection of a sphere onto a plane will produce one "point at infinity", and it will map the lines of latitude and longitude on the sphere into circles and straight lines, respectively, in the plane.When discussing functions of a complex variable it is often convenient to think of a cut in the complex plane. This idea arises naturally in several different contexts.Consider the simple two-valued relationshipBefore we can treat this relationship as a single-valued function, the range of the resulting value must be restricted  somehow. When dealing with the square roots of non-negative real numbers this is easily done. For instance, we can just defineto be the non-negative real number y such that y2 = x. This idea doesn't work so well in the two-dimensional complex plane. To see why, let's think about the way the value of f(z) varies as the point z moves around the unit circle. We can writeEvidently, as z moves all the way around the circle, w only traces out one-half of the circle. So one continuous motion in the complex plane has transformed the positive square root e0 = 1 into the negative square root eiπ = −1.This problem arises because the point z = 0 has just one square root, while every other complex number z ≠ 0 has exactly two square roots. On the real number line we could circumvent this problem by erecting a "barrier" at the single point x = 0. A bigger barrier is needed in the complex plane, to prevent any closed contour from completely encircling the branch point z = 0. This is commonly done by introducing a branch cut; in this case the "cut" might extend from the point z = 0 along the positive real axis to the point at infinity, so that the argument of the variable z in the cut plane is restricted to the range 0 ≤ arg(z) < 2π.We can now give a complete description of w = z½. To do so we need two copies of the z-plane, each of them cut along the real axis. On one copy we define the square root of 1 to be e0 = 1, and on the other we define the square root of 1 to be eiπ = −1. We call these two copies of the complete cut plane sheets. By making a continuity argument we see that the (now single-valued) function w = z½ maps the first sheet into the upper half of the w-plane, where 0 ≤ arg(w) < π, while mapping the second sheet into the lower half of the w-plane (where π ≤ arg(w) < 2π).[8]The branch cut in this example doesn't have to lie along the real axis. It doesn't even have to be a straight line. Any continuous curve connecting the origin z = 0 with the point at infinity would work. In some cases the branch cut doesn't even have to pass through the point at infinity. For example, consider the relationshipHere the polynomial z2 − 1 vanishes when z = ±1, so g evidently has two branch points. We can "cut" the plane along the real axis, from −1 to 1, and obtain a sheet on which g(z) is a single-valued function. Alternatively, the cut can run from z = 1 along the positive real axis through the point at infinity, then continue "up" the negative real axis to the other branch point, z = −1.This situation is most easily visualized by using the stereographic projection described above. On the sphere one of these cuts runs longitudinally through the southern hemisphere, connecting a point on the equator (z = −1) with another point on the equator (z = 1), and passing through the south pole (the origin, z = 0) on the way. The second version of the cut runs longitudinally through the northern hemisphere and connects the same two equatorial points by passing through the north pole (that is, the point at infinity).A meromorphic function is a complex function that is holomorphic and therefore analytic everywhere in its domain except at a finite, or countably infinite, number of points.[9] The points at which such a function cannot be defined are called the poles of the meromorphic function. Sometimes all these poles lie in a straight line. In that case mathematicians may say that the function is "holomorphic on the cut plane". Here's a simple example.The gamma function, defined bywhere γ is the Euler–Mascheroni constant, and has simple poles at 0, −1, −2, −3, ... because exactly one denominator in the infinite product vanishes when z is zero, or a negative integer.[10] Since all its poles lie on the negative real axis, from z = 0 to the point at infinity, this function might be described as "holomorphic on the cut plane, the cut extending along the negative real axis, from 0 (inclusive) to the point at infinity."Alternatively, Γ(z) might be described as "holomorphic in the cut plane with −π < arg(z) < π and excluding the point z = 0."Notice that this cut is slightly different from the branch cut we've already encountered, because it actually excludes the negative real axis from the cut plane. The branch cut left the real axis connected with the cut plane on one side (0 ≤ θ), but severed it from the cut plane along the other side (θ < 2π).Of course, it's not actually necessary to exclude the entire line segment from z = 0 to −∞ to construct a domain in which Γ(z) is holomorphic. All we really have to do is puncture the plane at a countably infinite set of points {0, −1, −2, −3, ...}. But a closed contour in the punctured plane might encircle one or more of the poles of Γ(z), giving a contour integral that is not necessarily zero, by the residue theorem. By cutting the complex plane we ensure not only that Γ(z) is holomorphic in this restricted domain – we also ensure that the contour integral of Γ over any closed curve lying in the cut plane is identically equal to zero.Many complex functions are defined by infinite series, or by continued fractions. A fundamental consideration in the analysis of these infinitely long expressions is identifying the portion of the complex plane in which they converge to a finite value. A cut in the plane may facilitate this process, as the following examples show.Consider the function defined by the infinite seriesSince z2 = (−z)2 for every complex number z, it's clear that f(z) is an even function of z, so the analysis can be restricted to one half of the complex plane. And since the series is undefined whenit makes sense to cut the plane along the entire imaginary axis and establish the convergence of this series where the real part of z is not zero before undertaking the more arduous task of examining f(z) when z is a pure imaginary number.[11]In this example the cut is a mere convenience, because the points at which the infinite sum is undefined are isolated, and the cut plane can be replaced with a suitably punctured plane. In some contexts the cut is necessary, and not just convenient. Consider the infinite periodic continued fractionIt can be shown that f(z) converges to a finite value if and only if z is not a negative real number such that z < −¼. In other words, the convergence region for this continued fraction is the cut plane, where the cut runs along the negative real axis, from −¼ to the point at infinity.[12]We have already seen how the relationshipcan be made into a single-valued function by splitting the domain of f into two disconnected sheets. It is also possible to "glue" those two sheets back together to form a single Riemann surface on which f(z) = z1/2 can be defined as a holomorphic function whose image is the entire w-plane (except for the point w = 0). Here's how that works.Imagine two copies of the cut complex plane, the cuts extending along the positive real axis from z = 0 to the point at infinity. On one sheet define 0 ≤ arg(z) < 2π, so that 11/2 = e0 = 1, by definition. On the second sheet define 2π ≤ arg(z) < 4π, so that 11/2 = eiπ = −1, again by definition. Now flip the second sheet upside down, so the imaginary axis points in the opposite direction of the imaginary axis on the first sheet, with both real axes pointing in the same direction, and "glue" the two sheets together (so that the edge on the first sheet labeled "θ = 0" is connected to the edge labeled "θ < 4π" on the second sheet, and the edge on the second sheet labeled "θ = 2π" is connected to the edge labeled "θ < 2π" on the first sheet). The result is the Riemann surface domain on which f(z) = z1/2 is single-valued and holomorphic (except when z = 0).[8]To understand why f is single-valued in this domain, imagine a circuit around the unit circle, starting with z = 1 on the first sheet. When 0 ≤ θ < 2π we are still on the first sheet. When θ = 2π we have crossed over onto the second sheet, and are obliged to make a second complete circuit around the branch point z = 0 before returning to our starting point, where θ = 4π is equivalent to θ = 0, because of the way we glued the two sheets together. In other words, as the variable z makes two complete turns around the branch point, the image of z in the w-plane traces out just one complete circle.Formal differentiation shows thatfrom which we can conclude that the derivative of f exists and is finite everywhere on the Riemann surface, except when z = 0 (that is, f is holomorphic, except when z = 0).How can the Riemann surface for the functionalso discussed above, be constructed? Once again we begin with two copies of the z-plane, but this time each one is cut along the real line segment extending from z = −1 to z = 1 – these are the two branch points of g(z). We flip one of these upside down, so the two imaginary axes point in opposite directions, and glue the corresponding edges of the two cut sheets together. We can verify that g is a single-valued function on this surface by tracing a circuit around a circle of unit radius centered at z = 1. Commencing at the point z = 2 on the first sheet we turn halfway around the circle before encountering the cut at z = 0. The cut forces us onto the second sheet, so that when z has traced out one full turn around the branch point z = 1, w has taken just one-half of a full turn, the sign of w has been reversed (since eiπ = −1), and our path has taken us to the point z = 2 on the second sheet of the surface. Continuing on through another half turn we encounter the other side of the cut, where z = 0, and finally reach our starting point (z = 2 on the first sheet) after making two full turns around the branch point.The natural way to label θ = arg(z) in this example is to set −π < θ ≤ π on the first sheet, with π < θ ≤ 3π on the second. The imaginary axes on the two sheets point in opposite directions so that the counterclockwise sense of positive rotation is preserved as a closed contour moves from one sheet to the other (remember, the second sheet is upside down). Imagine this surface embedded in a three-dimensional space, with both sheets parallel to the xy-plane. Then there appears to be a vertical hole in the surface, where the two cuts are joined together. What if the cut is made from z = −1 down the real axis to the point at infinity, and from z = 1, up the real axis until the cut meets itself? Again a Riemann surface can be constructed, but this time the "hole" is horizontal. Topologically speaking, both versions of this Riemann surface are equivalent – they are orientable two-dimensional surfaces of genus one.Another related use of the complex plane is with the Nyquist stability criterion.  This is a geometric principle which allows the stability of a closed-loop feedback system to be determined by inspecting a Nyquist plot of its open-loop magnitude and phase response as a function of frequency (or loop transfer function) in the complex plane.The 'z-plane' is a discrete-time version of the s-plane, where z-transforms are used instead of the Laplace transformation.The preceding sections of this article deal with the complex plane in terms of a geometric representation of the complex numbers. Although this usage of the term "complex plane" has a long and mathematically rich history, it is by no means the only mathematical concept that can be characterized as "the complex plane". There are at least three additional possibilities.While the terminology "complex plane" is historically accepted, the object could be more appropriately named "complex line" as it is a 1-dimensional complex vector space.
Determinant
In linear algebra, the determinant is a value that can be computed from the elements of a square matrix.  The determinant of a matrix A is denoted det(A), det A, or |A|. Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix.In the case of a 2 × 2 matrix the determinant may be defined as:Similarly, for a 3 × 3 matrix A, its determinant is:Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix A.  This procedure can be extended to give a recursive definition for the determinant of an n × n matrix, the minor expansion formula.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although other methods of solution are much more computationally efficient. In linear algebra, a matrix (with entries in a field) is invertible if and only if its determinant is non-zero, and correspondingly the matrix is singular if and only if its determinant is zero. This leads to the use of determinants in defining the characteristic polynomial of a matrix, whose roots are the eigenvalues. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. This leads to the use of determinants in calculus, the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants appear frequently in algebraic identities such as the Vandermonde identity.Determinants possess many algebraic properties, including that the determinant of a product of matrices is equal to the product of determinants. Special types of matrices have special determinants; for example, the determinant of an orthogonal matrix is always plus or minus one, and the determinant of a complex Hermitian matrix is always real.There are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns.  Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted.  For example, here is the result for a 4 × 4 matrix:Another way to define the determinant is expressed in terms of the columns of the matrix.  If we write an n × n matrix A in terms of its column vectorswhere b and c are scalars, v is any vector of size n and I is the identity matrix of size n.  These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar.  These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n × n matrix contributes n! terms), so it will first be given explicitly for the case of 2 × 2 matrices and 3 × 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.Assume A is a square matrix with n rows and n columns, so that it can be written asThe entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.The determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The Leibniz formula for the determinant of a 2 × 2 matrix isIf the matrix entries are real numbers, the matrix A can be used to represent two linear maps:  one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A.  In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping.  The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The absolute value of ad − bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).To show that ad − bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sinθ for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a⊥ = (−b, a), such that |a⊥||b|cosθ' , which can be determined by the pattern of the scalar product to be equal to ad − bc:Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.The object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) ∧ (c, d)) is the signed area, which is also the determinant ad − bc.[3]The Laplace formula for the determinant of a 3 × 3 matrix isthis can be expanded out to givewhich is the Leibniz formula for the determinant of a 3 × 3 matrix.The rule of Sarrus is a mnemonic for the 3 × 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 × 3 matrix does not carry over into higher dimensions.The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The Leibniz formula for the determinant of an n × n matrix A isHere the sum is computed over all permutations σ of the set {1, 2, …, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ1 = 2, σ2 = 3, and σ3 = 1.  The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.is notation for the product of the entries at positions (i, σi), where i ranges from 1 to n:For example, the determinant of a 3 × 3 matrix A (n = 3) isor using two epsilon symbols aswhere now each ir and each jr should be summed over 1, …, n.The determinant has many properties. Some basic properties of determinants areThis can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:Properties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from n × n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n ≥ 2,[6] so there is no good definition of the determinant in this setting.Property 2 above implies that properties for columns have their counterparts in terms of rows:Property 5 says that the determinant on n × n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 7; this is essentially the method of Gaussian elimination.For example, the determinant ofcan be computed using the following matrices:Here, B is obtained from A by adding −1/2×the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = −det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (−2) · 2 · 4.5 = −18. Therefore, det(A) = −det(D) = +18.The following identity holds for a Schur complement of a square matrix:The Schur complement arises as the result of performing a block Gaussian elimination by multiplying the matrix M from the right with a block lower triangular matrixHere Ip denotes a p×p identity matrix. After multiplication with the matrix L the Schur complement appears in the upper p×p block. The product matrix isThat is, we have effected a Gaussian decompositionThe first and last matrices on the RHS have determinant unity,  so we haveThis is Schur's determinant identity.The determinant of a matrix product of square matrices equals the product of their determinants:Thus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) → K that maps M ↦ det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized  to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n−1) × (n−1)-matrix that results from A by removing the i-th row and the j-th column. The expression (−1)i + jMi,j is known as a cofactor. The determinant of A is given byCalculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 × 3 matrixalong the second column (j = 2 and the sum runs over i) is given by,However, Laplace expansion is efficient for small matrices only.The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,In terms of the adjugate matrix, Laplace's expansion can be written as[7]Sylvester's determinant theorem states that for A, an m × n matrix, and B, an n × m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):where Im and In are the m × m and n × n identity matrices, respectively.From this general result several consequences follow.The product of all non-zero eigenvalues is referred to as pseudo-determinant.Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationwhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatricesbeing positive, for all k between 1 and n.The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,or, for real matrices A,Here exp(A) denotes the matrix exponential of A, because every eigenvalue λ of A corresponds to the eigenvalue exp(λ) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingthe determinant of A is given byFor example, for n = 2, n = 3, and n = 4, respectively,cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic n, detA = (−)nc0 the signed constant term of the characteristic polynomial, determined recursively fromIn the general case, this may also be obtained from[9]where the sum is taken over the set of all integers kl ≥ 0 satisfying the equationThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = −(l – 1)! tr(Al) asThis formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1, i2, …, ir) and J = (j1, j2, …, jr). The product and trace of such matrices are defined in a natural way asAn important arbitrary  dimension n  identity can be obtained from the  Mercator series expansion of the logarithm when the expansion converges.  If every eigenvalue of A is less than 1 in absolute value,where I is the identity matrix.  More generally, if is expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I + sA).For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantwith equality if and only if  A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.Also,These inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.For a matrix equationthe solution is given by Cramer's rule:where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.It has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.Suppose A, B, C, and D are matrices of dimension n × n, n × m, m × n, and m × m, respectively. ThenThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)When A is invertible, one hasas can be seen by employing the decompositionWhen the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 × 2 matrix holds:[12]Generally, if all pairs of n × n matrices of the np × np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix  obtained by computing the determinant of the block matrix considering its entries as the entries of a p × p matrix.[13] As the previous formula shows, for p = 2, this criterion is sufficient, but not necessary.When A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)When D is a 1×1 matrix, B is a column vector, and C is a row vector thenIt can be seen, e.g. using the Leibniz formula, that the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn × n to R, and so it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]where adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveExpressed in terms of the entries of A, these areYet another equivalent formulation isThis identity is used in describing the tangent space of certain matrix Lie groups.The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X−1BX. Indeed, repeatedly applying the above identities yieldsThe determinant is therefore also called a similarity invariant. The determinant of a linear transformationfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant of a linear transformation A : V → V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear mapAs ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThis definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 ∧ v2 ∧ v3 ∧ … ∧ vn to v2 ∧ v1 ∧ v3 ∧ … ∧ vn, say, also changes its sign.For this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with k < n.The vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one.  To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define (T′w)(x1, …, xn) = w(Tx1, …, Txn).  As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple.  We call this scalar the determinant of T.The determinant can also be characterized as the unique functionfrom the set of all n × n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.This fact also implies that every other n-linear alternating function F: Mn(K) → K satisfiesThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.The determinant defines a mappingbetween the group of invertible n × n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R → S, there is a map GLn(f): GLn(R) → GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityholds. In other words, the following diagram commutes:For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaAnother infinite-dimensional notion of determinant is the functional determinant.For operators in a finite factor, one may define a positive real-valued determinant called the Fuglede−Kadison determinant using the canonical trace. In fact, corresponding to every tracial state on a von Neumann algebra there is a notion of Fuglede−Kadison determinant.For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Computational geometry, however, does frequently use calculations related to determinants.[18]Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n × n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)The LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies toIf the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[19] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If two matrices of order n can be multiplied in time M(n), where M(n) ≥ na for some a > 2, then the determinant can be computed in time O(M(n)).[20] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation.  Unfortunately this interesting method does not always work in its original form.Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[21] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[22]Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, 2 × 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[23][24][25][26]In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).It was Vandermonde (1771) who first recognized determinants as independent functions.[23] Laplace (1772)[27][28] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,[29][30] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[23][31] With him begins the theory in its generality.The next important figure was Jacobi[24] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[32][33]The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 × 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be n − 1 times differentiable), the Wronskian is defined to beIt is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 × 2 or 3 × 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn → Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn → Rm is represented by the m × n matrix A, then the n-dimensional volume of f(S) is given by:By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)·|det(a − b, b − c, c − d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.For a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. Forthe Jacobian matrix is the n × n matrix whose entries are given byIts determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function φ: Rn → Rm is given byThe Jacobian also occurs in the inverse function theorem.The third order Vandermonde determinant isIn general, the nth-order Vandermonde determinant is[34]where the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.Second orderThird orderwhere ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is[34]where ωj is an nth root of 1.
Canonical basis
In mathematics, a canonical basis is a basis of an algebraic structure that is canonical in a sense that depends on the precise context:In representation theory there are several bases that are called "canonical", for example, Lusztig's canonical basis and closely related Kashiwara's crystal basis in quantum groups and their representations. There is a general concept underlying these basis:Definition:  A set of n linearly independent generalized eigenvectors is a canonical basis if it is composed entirely of Jordan chains.Now defineOnce we have determined the number of generalized eigenvectors of each rank that a canonical basis has, we can obtain the vectors explicitly (see generalized eigenvector).[3]This example illustrates a canonical basis with two Jordan chains.  Unfortunately, it is a little difficult to construct an interesting example of low order.[4]The matrix
Cauchy–Schwarz inequality
In mathematics, the Cauchy–Schwarz inequality, also known as the Cauchy–Bunyakovsky–Schwarz inequality, is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics.[1]The inequality for sums was published by Augustin-Louis Cauchy (1821), while the corresponding inequality for integrals was first proved byViktor Bunyakovsky (1859). The modern proof of the integral inequality was given by Hermann Amandus Schwarz (1888).[1]or LetThen, by linearity of the inner product in its first argument, one haswhich givesThis establishes the theorem.Titu's lemma (named after Titu Andreescu, also known as T2 Lemma, Engel's form, or Sedrakyan's inequality) states that for positive reals, we havewhich yields the Cauchy–Schwarz inequality.For the inner product space of square-integrable complex-valued functions, one hasA generalization of this is the Hölder inequality.The triangle inequality for the standard norm is often shown as a consequence of the Cauchy–Schwarz inequality, as follows: given vectors x and y:Taking square roots gives the triangle inequality.The Cauchy–Schwarz inequality is used to prove that the inner product is a continuous function with respect to the topology induced by the inner product itself.[8][9]The Cauchy–Schwarz inequality allows one to extend the notion of "angle between two vectors" to any real inner-product space by defining:[10][11]The Cauchy–Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [−1, 1] and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space. It can also be used to define an angle in complex inner-product spaces, by taking the absolute value or the real part of the right-hand side,[12][13] as is done when extracting a metric from quantum fidelity.Let X, Y be random variables, then the covariance inequality[14][15] is given byAfter defining an inner product on the set of random variables using the expectation of their product,then the Cauchy–Schwarz inequality becomesVarious generalizations of the Cauchy–Schwarz inequality exist in the context of operator theory, e.g. for operator-convex functions and operator algebras, where the domain and/or range are replaced by a C*-algebra or W*-algebra.which extends verbatim to positive functionals on C*-algebras:The next two theorems are further examples in operator algebra.Another generalization is a refinement obtained by interpolating between both sides the Cauchy-Schwarz inequality:  It can be easily proven by Hölder's inequality.[23] There are also non commutative versions for operators and tensor products of matrices.[24]
Defective matrix
In linear algebra, a defective matrix is a square matrix that does not have a complete basis of eigenvectors, and is therefore not diagonalizable.  In particular, an n × n matrix is defective if and only if it does not have n linearly independent eigenvectors.[1]  A complete basis is formed by augmenting the eigenvectors with generalized eigenvectors, which are necessary for solving defective systems of ordinary differential equations and other problems.A defective matrix always has fewer than n distinct eigenvalues, since distinct eigenvalues always have linearly independent eigenvectors.  In particular, a defective matrix has one or more eigenvalues λ with algebraic multiplicity m > 1 (that is, they are multiple roots of the characteristic polynomial), but fewer than m linearly independent eigenvectors associated with λ. If the algebraic multiplicity of λ exceeds its geometric multiplicity (that is, the number of linearly independent eigenvectors associated with λ), then λ is said to be a defective eigenvalue.[1]  However, every eigenvalue with algebraic multiplicity m always has m linearly independent generalized eigenvectors.A Hermitian matrix (or the special case of a real symmetric matrix) or a unitary matrix is never defective; more generally, a normal matrix (which includes Hermitian and unitary as special cases) is never defective.Any nontrivial Jordan block of size 2×2 or larger (that is, not completely diagonal) is defective.  (A diagonal matrix is a special case of the Jordan normal form and is not defective.)  For example, the n × n Jordan block,has an eigenvalue, λ, with algebraic multiplicity n, but only one distinct eigenvector,In fact, any defective matrix has a nontrivial Jordan normal form, which is as close as one can come to diagonalization of such a matrix.A simple example of a defective matrix is:which has a double eigenvalue of 3 but only one distinct eigenvector(and constant multiples thereof).
Möbius transformation
In geometry and complex analysis, a Möbius transformation of the complex plane is a rational function of the formof one complex variable z; here the coefficients a, b, c, d are complex numbers satisfying ad − bc ≠ 0.Geometrically, a Möbius transformation can be obtained by first performing stereographic projection from the plane to the unit two-sphere, rotating and moving the sphere to a new location and orientation in space, and then performing stereographic projection (from the new position of the sphere) to the plane.[1]These transformations preserve angles, map every straight line to a line or circle, and map every circle to a line or circle.The Möbius transformations are the projective transformations of the complex projective line. They form a group called the Möbius group, which is the projective linear group PGL(2,C). Together with its subgroups, it has numerous applications in mathematics and physics.Möbius transformations are named in honor of August Ferdinand Möbius; they are also variously named homographies, homographic transformations, linear fractional transformations, bilinear transformations, or fractional linear transformations.The Möbius group is isomorphic to the group of orientation-preserving isometries of hyperbolic 3-space and therefore plays an important role when studying hyperbolic 3-manifolds.In physics, the identity component of the Lorentz group acts on the celestial sphere in the same way that the Möbius group acts on the Riemann sphere. In fact, these two groups are isomorphic. An observer who accelerates to relativistic velocities will see the pattern of constellations as seen near the Earth continuously transform according to infinitesimal Möbius transformations. This observation is often taken as the starting point of twistor theory.Certain subgroups of the Möbius group form the automorphism groups of the other simply-connected Riemann surfaces (the complex plane and the hyperbolic plane). As such, Möbius transformations play an important role in the theory of Riemann surfaces. The fundamental group of every Riemann surface is a discrete subgroup of the Möbius group (see Fuchsian group and Kleinian group).A particularly important discrete subgroup of the Möbius group is the modular group; it is central to the theory of many fractals, modular forms, elliptic curves and Pellian equations.Möbius transformations can be more generally defined in spaces of dimension n>2 as the bijective conformal orientation-preserving maps from the n-sphere to the n-sphere. Such a transformation is the most general form of conformal mapping of a domain. According to Liouville's theorem a Möbius transformation can be expressed as a composition of translations, similarities, orthogonal transformations and inversions.The general form of a Möbius transformation is given bywhere a, b, c, d are any complex numbers satisfying ad − bc ≠ 0. If ad = bc, the rational function defined above is a constant sinceand is thus not considered a Möbius transformation.In case c ≠ 0, this definition is extended to the whole Riemann sphere by definingIf c = 0, we defineThus a Möbius transformation is always a bijective holomorphic function from the Riemann sphere to the Riemann sphere.The fixed points of the transformationare obtained by solving the fixed point equation f(γ) = γ. For c ≠ 0, this has two roots obtained by expanding this equation toand applying the quadratic formula. The roots arewith discriminantParabolic transforms have coincidental fixed points due to zero discriminant. For c nonzero and nonzero discriminant the transform is elliptic or hyperbolic.When c = 0, the quadratic equation degenerates into a linear equation and the transform is linear. This corresponds to the situation that one of the fixed points is the point at infinity. When a ≠ d the second fixed point is finite and is given byIn this case the transformation will be a simple transformation composed of translations, rotations, and dilations:If c = 0 and a = d, then both fixed points are at infinity, and the Möbius transformation corresponds to a pure translation:Topologically, the fact that (non-identity) Möbius transformations fix 2 points (with multiplicity) corresponds to the Euler characteristic of the sphere being 2:Firstly, the projective linear group PGL(2,K) is sharply 3-transitive – for any two ordered triples of distinct points, there is a unique map that takes one triple to the other, just as for Möbius transforms, and by the same algebraic proof (essentially dimension counting, as the group is 3-dimensional). Thus any map that fixes at least 3 points is the identity.Möbius transformations are also sometimes written in terms of their fixed points in so-called normal form. We first treat the non-parabolic case, for which there are two distinct fixed points.Non-parabolic case:Every non-parabolic transformation is conjugate to a dilation/rotation, i.e. a transformation of the form(k ∈ C) with fixed points at 0 and ∞. To see this define a mapwhich sends the points (γ1, γ2) to (0, ∞). Here we assume that γ1 and γ2 are distinct and finite. If one of them is already at infinity then g can be modified so as to fix infinity and send the other point to 0.Solving for f gives (in matrix form):or, if one of the fixed points is at infinity:From the above expressions one can calculate the derivatives of f at the fixed points:Observe that, given an ordering of the fixed points, we can distinguish one of the multipliers (k) of f as the characteristic constant of f. Reversing the order of the fixed points is equivalent to taking the inverse multiplier for the characteristic constant:For loxodromic transformations, whenever |k| > 1, one says that γ1 is the repulsive fixed point, and γ2 is the attractive fixed point. For |k| < 1, the roles are reversed.Parabolic case:In the parabolic case there is only one fixed point γ. The transformation sending that point to ∞ isHere, β is called the translation length. The fixed point formula for a parabolic transformation is thenSolving for f (in matrix form) givesor, if γ = ∞:Note that β is not the characteristic constant of f, which is always 1 for a parabolic transformation. From the above expressions one can calculate:These four points are the vertices of a parallelogram which is sometimes called the characteristic parallelogram of the transformation.which reduces down torepresenting the transform (compare the discussion in the preceding section about the characteristic constant of a transformation). Its characteristic polynomial is equal towhich has rootsA Möbius transformation can be composed as a sequence of simple transformations.The following simple transformations are also Möbius transformations:Let:Then these functions can be composed, givingThis decomposition makes many properties of the Möbius transformation obvious.A Möbius transformation is equivalent to a sequence of simpler transformations.The composition  makes many properties of the Möbius transformation obvious.The existence of the inverse Möbius transformation and its explicit formula are easily derived by the composition of the inverse functions of the simpler transformations. That is, define functions g1, g2, g3, g4 such that each gi is the inverse of fi. Then the compositionFrom this decomposition, we see that Möbius transformations carry over all non-trivial properties of circle inversion. For example, the preservation of angles is reduced to proving that circle inversion preserves angles since the other types of transformations are dilation and isometries (translation, reflection, rotation), which trivially preserve angles.Furthermore, Möbius transformations map generalized circles to generalized circles since circle inversion has this property. A generalized circle is either a circle or a line, the latter being considered as a circle through the point at infinity. Note that a Möbius transformation does not necessarily map circles to circles and lines to lines: it can mix the two. Even if it maps a circle to another circle, it does not necessarily map the first circle's center to the second circle's center.The cross ratio of four different points is real if and only if there is a line or a circle passing through them. This is another way to show that Möbius transformations preserve generalized circles.Two points z1 and z2 are conjugate with respect to a generalized circle C, if, given a generalized circle D passing through z1 and z2 and cutting C in two points a and b, (z1, z2; a, b) are in harmonic cross-ratio (i.e. their cross ratio is −1). This property does not depend on the choice of the circle D. This property is also sometimes referred to as being symmetric with respect to a line or circle.[2][3]Two points z, z∗ are conjugate with respect to a line, if they are symmetric with respect to the line. Two points are conjugate with respect to a circle if they are exchanged by the inversion with respect to this circle.The point z∗ conjugate to z when L is the line determined by the vector based eiθ at the point z0 can be explicitly given asThe point z∗ conjugate to z when C is the circle of radius r centered z0 can be explicitly given asSince Möbius transformations preserve generalized circles and cross-ratios, they preserve also the conjugation.With every invertible complex 2-by-2 matrixwe can associate the Möbius transformationThe condition ad − bc ≠ 0 is equivalent to the condition that the determinant of above matrix be nonzero, i.e. that the matrix be invertible.It is straightforward to check that then the product of two matrices will be associated with the composition of the two corresponding Möbius transformations. In other words, the mapThe same identification of PGL(2,K) with the group of fractional linear transformations and with the group of projective linear automorphisms of the projective line holds over any field K, a fact of algebraic interest, particularly for finite fields, though the case of the complex numbers has the greatest geometric interest.The natural action of PGL(2,C) on the complex projective line CP1 is exactly the natural action of the Möbius group on the Riemann sphere, where the projective line CP1 and the Riemann sphere are identified as follows:Here [z1:z2] are homogeneous coordinates on CP1; the point [1:0] corresponds to the point ∞ of the Riemann sphere.By using homogeneous coordinates, many concrete calculations involving Möbius transformations can be simplified, since no case distinctions dealing with ∞ are required.From this we see that the Möbius group is a 3-dimensional complex Lie group (or a 6-dimensional real Lie group). It is a semisimple non-compact Lie group.Note that there are precisely two matrices with unit determinant which can be used to represent any given Möbius transformation. That is, SL(2,C) is a double cover of PSL(2,C). Since SL(2,C) is simply-connected it is the universal cover of the Möbius group. Therefore, the fundamental group of the Möbius group is Z2.Given a set of three distinct points z1, z2, z3 on the Riemann sphere and a second set of distinct points w1, w2, w3, there exists precisely one Möbius transformation f(z)  with f(zi) = wi for i = 1,2,3. (In other words: the action of the Möbius group on the Riemann sphere is sharply 3-transitive.) There are several ways to determine f(z) from the given sets of points.It is easy to check that the Möbius transformationwith matrixThe stabilizer of {0, 1, ∞} (as an unordered set) is a subgroup known as the anharmonic group.The equationis equivalent to the equation of a standard hyperbolaby means of a Laplace expansion along the first row. This results in the determinant formulaeIf we require the coefficients a, b, c, d of a Möbius transformation to be real numbers with ad − bc = 1, we obtain a subgroup of theMöbius group denoted as PSL(2,R). This is the group of those Möbius transformations that map the upper half-plane H = x + iy : y > 0 to itself, and is equal to the group of all biholomorphic (or equivalently: bijective, conformal and orientation-preserving) maps H → H. If a proper metric is introduced, the upper half-plane becomes a model of the hyperbolic plane H 2, the Poincaré half-plane model, and PSL(2,R) is the group of all orientation-preserving isometries of H 2 in this model.The subgroup of all Möbius transformations that map the open disk D = z : |z| < 1 to itself consists of all transformations of the formSince both of the above subgroups serve as isometry groups of H 2, they are isomorphic. A concrete isomorphism is given by conjugation with the transformationwhich bijectively maps the open unit disk to the upper half plane.Alternatively, consider an open disk with radius r, centered at ri. The Poincaré disk model in this disk becomes identical to the upper-half-plane model as r approaches ∞.Icosahedral groups of Möbius transformations were used by Felix Klein to give an analytic solution to the quintic equation in (Klein 1888); a modern exposition is given in.[5]If we require the coefficients a, b, c, d of a Möbius transformation to be integers with ad − bc = 1, we obtain the modular group PSL(2,Z), a discrete subgroup of PSL(2,R) important in the study of lattices in the complex plane, elliptic functions and elliptic curves. The discrete subgroups of PSL(2,R) are known as Fuchsian groups; they are important in the study of Riemann surfaces.Non-identity Möbius transformations are commonly classified into four types, parabolic, elliptic, hyperbolic and loxodromic, with the hyperbolic ones being a subclass of the loxodromic ones. The classification has both algebraic and geometric significance. Geometrically, the different types result in different transformations of the complex plane, as the figures below illustrate.which describes a translation in the complex plane.this is an example of the unipotent radical of a Borel subgroup (of the Möbius group, or of SL(2,C) for the matrix group; the notion is defined for any reductive Lie group).All non-parabolic transformations have two fixed points and are defined by a matrix conjugate towith the complex number λ not equal to 0, 1 or −1, corresponding to a dilation/rotation through multiplication by the complex number k = λ2, called the characteristic constant or multiplier of the transformation.with α real.A transform is hyperbolic if and only if λ is real and positive.Historically, navigation by loxodrome or rhumb line refers to a path of constant bearing; the resulting path is a logarithmic spiral, similar in shape to the transformations of the complex plane that a loxodromic Möbius transformation makes. See the geometric figures below.Over the real numbers (if the coefficients must be real), there are no non-hyperbolic loxodromic transformations, and the classification is into elliptic, parabolic, and hyperbolic, as for real conics. The terminology is due to considering half the absolute value of the trace, |tr|/2, as the eccentricity of the transformation – division by 2 corrects for the dimension, so the identity has eccentricity 1 (tr/n is sometimes used as an alternative for the trace for this reason), and absolute value corrects for the trace only being defined up to a factor of ±1 due to working in PSL. Alternatively one may use half the trace squared as a proxy for the eccentricity squared, as was done above; these classifications (but not the exact eccentricity values, since squaring and absolute values are different) agree for real traces but not complex traces. The same terminology is used for the classification of elements of SL(2, R) (the 2-fold cover), and analogous classifications are used elsewhere. Loxodromic transformations are an essentially complex phenomenon, and correspond to complex eccentricities.The following picture depicts (after stereographic transformation from the sphere to the plane) the two fixed points of a Möbius transformation in the non-parabolic case:The characteristic constant can be expressed in terms of its logarithm:When expressed in this way, the real number ρ becomes an expansion factor. It indicates how repulsive the fixed point γ1 is, and how attractive γ2 is. The real number α is a rotation factor, indicating to what extent the transform rotates the plane anti-clockwise about γ1 and clockwise about γ2.If ρ = 0, then the fixed points are neither attractive nor repulsive but indifferent, and the transformation is said to be elliptic. These transformations tend to move all points in circles around the two fixed points. If one of the fixed points is at infinity, this is equivalent to doing an affine rotation around a point.If we take the one-parameter subgroup generated by any elliptic Möbius transformation, we obtain a continuous transformation, such that every transformation in the subgroup fixes the same two points. All other points flow along a family of circles which is nested between the two fixed points on the Riemann sphere. In general, the two fixed points can be any two distinct points.This has an important physical interpretation.Imagine that some observer rotates with constant angular velocity about some axis. Then we can take the two fixed points to be the North and South poles of the celestial sphere. The appearance of the night sky is now transformed continuously in exactly the manner described by the one-parameter subgroup of elliptic transformations sharing the fixed points 0, ∞, and with the number α corresponding to the constant angular velocity of our observer.Here are some figures illustrating the effect of an elliptic Möbius transformation on the Riemann sphere (after stereographic projection to the plane):These pictures illustrate the effect of a single Möbius transformation. The one-parameter subgroup which it generates continuously moves points along the family of circular arcs suggested by the pictures.If α is zero (or a multiple of 2π), then the transformation is said to be hyperbolic. These transformations tend to move points along circular paths from one fixed point toward the other.If we take the one-parameter subgroup generated by any hyperbolic Möbius transformation, we obtain a continuous transformation, such that every transformation in the subgroup fixes the same two points. All other points flow along a certain family of circular arcs away from the first fixed point and toward the second fixed point. In general, the two fixed points may be any two distinct points on the Riemann sphere.This too has an important physical interpretation. Imagine that an observer accelerates (with constant magnitude of acceleration) in the direction of the North pole on his celestial sphere. Then the appearance of the night sky is transformed in exactly the manner described by the one-parameter subgroup of hyperbolic transformations sharing the fixed points 0, ∞, with the real number ρ corresponding to the magnitude of his acceleration vector. The stars seem to move along longitudes, away from the South pole toward the North pole. (The longitudes appear as circular arcs under stereographic projection from the sphere to the plane.)Here are some figures illustrating the effect of a hyperbolic Möbius transformation on the Riemann sphere (after stereographic projection to the plane):These pictures resemble the field lines of a positive and a negative electrical charge located at the fixed points, because the circular flow lines subtend a constant angle between the two fixed points.If both ρ and α are nonzero, then the transformation is said to be loxodromic. These transformations tend to move all points in S-shaped paths from one fixed point to the other.The word "loxodrome" is from the Greek: "λοξος (loxos), slanting + δρόμος (dromos), course". When sailing on a constant bearing – if you maintain a heading of (say) north-east, you will eventually wind up sailing around the north pole in a logarithmic spiral. On the mercator projection such a course is a straight line, as the north and south poles project to infinity. The angle that the loxodrome subtends relative to the lines of longitude (i.e. its slope, the "tightness" of the spiral) is the argument of k. Of course, Möbius transformations may have their two fixed points anywhere, not just at the north and south poles. But any loxodromic transformation will be conjugate to a transform that moves all points along such loxodromes.If we take the one-parameter subgroup generated by any loxodromic Möbius transformation, we obtain a continuous transformation, such that every transformation in the subgroup fixes the same two points. All other points flow along a certain family of curves, away from the first fixed point and toward the second fixed point. Unlike the hyperbolic case, these curves are not circular arcs, but certain curves which under stereographic projection from the sphere to the plane appear as spiral curves which twist counterclockwise infinitely often around one fixed point and twist clockwise infinitely often around the other fixed point. In general, the two fixed points may be any two distinct points on the Riemann sphere.You can probably guess the physical interpretation in the case when the two fixed points are 0, ∞: an observer who is both rotating (with constant angular velocity) about some axis and moving along the same axis, will see the appearance of the night sky transform according to the one-parameter subgroup of loxodromic transformations with fixed points 0, ∞, and with ρ, α determined respectively by the magnitude of the actual linear and angular velocities.These images show Möbius transformations stereographically projected onto the Riemann sphere. Note in particular that when projected onto a sphere, the special case of a fixed point at infinity looks no different from having the fixed points in an arbitrary location.This can be used to iterate a transformation, or to animate one by breaking it up into steps.These images show three points (red, blue and black) continuously iterated under transformations with various characteristic constants.And these images demonstrate what happens when you transform a circle under Hyperbolic, Elliptical, and Loxodromic transforms. Note that in the elliptical and loxodromic images, the α value is 1/10 .The pointThe inverse poleis that point to which the point at infinity is transformed.The point midway between the two poles is always the same as the point midway between the two fixed points:These four points are the vertices of a parallelogram which is sometimes called the characteristic parallelogram of the transformation.which reduces down torepresenting the transform (compare the discussion in the preceding section about the characteristic constant of a transformation). Its characteristic polynomial is equal towhich has rootsThe orientation-preserving Möbius transformations form the connected component of the identity in the Möbius group.  In dimension n = 2, the orientation-preserving Möbius transformations are exactly the maps of the Riemann sphere covered here.  The orientation-reversing ones are obtained from these by complex conjugation.[8]An isomorphism of the Möbius group with the Lorentz group was noted by several authors: Based on previous work of Felix Klein (1893, 1897)[10] on automorphic functions related to hyperbolic geometry and Möbius geometry, Gustav Herglotz (1909)[11] showed that hyperbolic motions (i.e. isometric automorphisms of a hyperbolic space) transforming the unit sphere into itself correspond to Lorentz transformations, by which Herglotz was able to classify the one-parameter Lorentz transformations into loxodromic, elliptic, hyperbolic, and parabolic groups. Other authors include Emil Artin (1957),[12] H. S. M. Coxeter (1965),[13] and Roger Penrose and Wolfgang Rindler (1984).[14]Minkowski space consists of the four-dimensional real coordinate space R4 consisting of the space of ordered quadruples (x0,x1,x2,x3) of real numbers, together with a quadratic formBorrowing terminology from special relativity, points with Q > 0 are considered timelike; in addition, if x0 > 0, then the point is called future-pointing. Points with Q < 0 are called spacelike. The null cone S consists of those points where Q = 0; the future null cone N+ are those points on the null cone with x0 > 0. The celestial sphere is then identified with the collection of rays in N+ whose initial point is the origin of R4. The collection of linear transformations on R4 with positive determinant preserving the quadratic form Q and preserving the time direction form the restricted Lorentz group SO+(1,3).In connection with the geometry of the celestial sphere, the group of transformations SO+(1,3) is identified with the group PSL(2,C) of Möbius transformations of the sphere. To each (x0,x1,x2,x3) ∈ R4, associate the hermitian matrixThe determinant of the matrix X is equal to Q(x0,x1,x2,x3). The special linear group acts on the space of such matrices via    (1)for each A ∈ SL(2,C), and this action of SL(2,C) preserves the determinant of X because det A = 1. Since the determinant of X is identified with the quadratic form Q, SL(2,C) acts by Lorentz transformations. On dimensional grounds, SL(2,C) covers a neighborhood of the identity of SO(1,3). Since SL(2,C) is connected, it covers the entire restricted Lorentz group SO+(1,3). Furthermore, since the kernel of the action (1) is the subgroup {±I}, then passing to the quotient group gives the group isomorphism    (2)Focusing now attention on the case when (x0,x1,x2,x3) is null, the matrix X has zero determinant, and therefore splits as the outer product of a complex two-vector ξ with its complex conjugate:    (3)The two-component vector ξ is acted upon by SL(2,C) in a manner compatible with (1). It is now clear that the kernel of the representation of SL(2,C) on hermitian matrices is {±I}.The action of PSL(2,C) on the celestial sphere may also be described geometrically using stereographic projection. Consider first the hyperplane in R4 given by x0 = 1. The celestial sphere may be identified with the sphere S+ of intersection of the hyperplane with the future null cone N+. The stereographic projection from the north pole (1,0,0,1) of this sphere onto the plane x3 = 0 takes a point with coordinates (1,x1,x2,x3) withto the pointIntroducing the complex coordinatethe inverse stereographic projection gives the following formula for a point (x1, x2, x3) on S+:    (4)The action of SO+(1,3) on the points of N+ does not preserve the hyperplane S+, but acting on points in S+ and then rescaling so that the result is again in S+ gives an action of SO+(1,3) on the sphere which goes over to an action on the complex variable ζ. In fact, this action is by fractional linear transformations, although this is not easily seen from this representation of the celestial sphere. Conversely, for any fractional linear transformation of ζ variable goes over to a unique Lorentz transformation on N+, possibly after a suitable (uniquely determined) rescaling.A more invariant description of the stereographic projection which allows the action to be more clearly seen is to consider the variable ζ = z:w as a ratio of a pair of homogeneous coordinates for the complex projective line CP1. The stereographic projection goes over to a transformation from C2 − {0} to N+ which is homogeneous of degree two with respect to real scalings    (5)In summary, the action of the restricted Lorentz group SO+(1,3) agrees with that of the Möbius group PSL(2,C). This motivates the following definition. In dimension n ≥ 2, the Möbius group Möb(n) is the group of all orientation-preserving conformal isometries of the round sphere Sn to itself. By realizing the conformal sphere as the space of future-pointing rays of the null cone in the Minkowski space R1,n+1, there is an isomorphism of Möb(n) with the restricted Lorentz group SO+(1,n+1) of Lorentz transformations with positive determinant, preserving the direction of time.He identified the Lorentz group with transformations for which {x : Q(x) = -1} is stable. Then he interpreted the x's as homogeneous coordinates  and {x : Q(x) = 0}, the null cone, as the Cayley absolute for a hyperbolic space of points {x : Q(x) < 0}. Next, Coxeter introduced the variablesAs seen above, the Möbius group PSL(2,C) acts on Minkowski space as the group of those isometries that preserve the origin, the orientation of space and the direction of time. Restricting to the points where Q=1 in the positive light cone, which form a model of hyperbolic 3-space H 3, we see that the Möbius group acts on H 3 as a group of orientation-preserving isometries. In fact, the Möbius group is equal to the group of orientation-preserving isometries of hyperbolic 3-space.If we use the Poincaré ball model, identifying the unit ball in R3 with H 3, then we can think of the Riemann sphere as the "conformal boundary" of H 3. Every orientation-preserving isometry of H 3 gives rise to a Möbius transformation on the Riemann sphere and vice versa; this is the very first observation leading to the AdS/CFT correspondence conjectures in physics.SpecificGeneral
Runge–Kutta methods
In numerical analysis, the Runge–Kutta methods  are a family of implicit and explicit iterative methods, which include the well-known routine called the Euler Method, used in temporal discretization for the approximate solutions of ordinary differential equations.[1] These methods were developed around 1900 by the German mathematicians Carl Runge and Martin Kutta.See the article on numerical methods for ordinary differential equations for more background and other methods. See also List of Runge–Kutta methods.The most widely known member of the Runge–Kutta family is generally referred to as  "RK4", "classical Runge–Kutta method" or simply as "the Runge–Kutta method".Let an initial value problem be specified as follows:Now pick a step-size h > 0 and definefor n = 0, 1, 2, 3, ..., using[2]The family of explicit Runge–Kutta methods is a generalization of the RK4 method mentioned above. It is given bywhere[5]To specify a particular method, one needs to provide the integer s (the number of stages), and the coefficients aij (for 1 ≤ j < i ≤ s), bi (for i = 1, 2, ..., s) and ci (for i = 2, 3, ..., s). The matrix [aij] is called the Runge–Kutta matrix, while the bi and ci are known as the weights and the nodes.[6] These data are usually arranged in a mnemonic device, known as a Butcher tableau (after John C. Butcher):The Runge–Kutta method is consistent ifThere are also accompanying requirements if one requires the method to have a certain order p, meaning that the local truncation error is O(hp+1). These can be derived from the definition of the truncation error itself. For example, a two-stage method has order 2 if b1 + b2 = 1, b2c2 = 1/2, and a21 = c2.[7]The RK4 method falls in this framework. Its tableau is[10]A slight variation of "the" Runge–Kutta method is also due to Kutta in 1901 and is called the 3/8-rule.[11] The primary advantage this method has is that almost all of the error coefficients are smaller than in the popular method, but it requires slightly more FLOPs (floating-point operations) per time step. Its Butcher tableau isAn example of a second-order method with two stages is provided by the midpoint method:The corresponding tableau isThe midpoint method is not the only second-order Runge–Kutta method with two stages; there is a family of such methods, parameterized by α and given by the formula[12]Its Butcher tableau isAs an example, consider the two-stage second-order Runge–Kutta method with α = 2/3, also known as Ralston method. It is given by the tableauwith the corresponding equationsThis method is used to solve the initial-value problemwith step size h = 0.025, so the method needs to take four steps.The method proceeds as follows:The numerical solutions correspond to the underlined values.During the integration, the step size is adapted such that the estimated error stays below a user-defined threshold: If the error is too high, a step is repeated with a lower step size; if the error is much smaller, the step size is increased to save time. This results in an (almost) optimal step size, which saves computation time. Moreover, the user does not have to spend time on finding an appropriate step size.The lower-order step is given byThe Runge–Kutta–Fehlberg method has two methods of orders 5 and 4. Its extended Butcher tableau is:However, the simplest adaptive Runge–Kutta method involves combining Heun's method, which is order 2, with the Euler method, which is order 1. Its extended Butcher tableau is:Other adaptive Runge–Kutta methods are the Bogacki–Shampine method (orders 3 and 2), the Cash–Karp method and the Dormand–Prince method (both with orders 5 and 4).All Runge–Kutta methods mentioned up to now are explicit methods. Explicit Runge–Kutta methods are generally unsuitable for the solution of stiff equations because their region of absolute stability is small; in particular, it is bounded.[14]This issue is especially important in the solution of partial differential equations.The instability of explicit Runge–Kutta methods motivates the development of implicit methods. An implicit Runge–Kutta method has the formwhereThe consequence of this difference is that at every step, a system of algebraic equations has to be solved. This increases the computational cost considerably. If a method with s stages is used to solve a differential equation with m components, then the system of algebraic equations has ms components. This can be contrasted with implicit linear multistep methods (the other big family of methods for ODEs): an implicit s-step linear multistep method needs to solve a system of algebraic equations with only m components, so the size of the system does not increase as the number of steps increases.[16]The simplest example of an implicit Runge–Kutta method is the backward Euler method:The Butcher tableau for this is simply:This Butcher tableau corresponds to the formulaewhich can be re-arranged to get the formula for the backward Euler method listed above.Another example for an implicit Runge–Kutta method is the trapezoidal rule. Its Butcher tableau is:The trapezoidal rule is a collocation method (as discussed in that article). All collocation methods are implicit Runge–Kutta methods, but not all implicit Runge–Kutta methods are collocation methods.[17]The Gauss–Legendre methods form a family of collocation methods based on Gauss quadrature. A Gauss–Legendre method with s stages has order 2s (thus, methods with arbitrarily high order can be constructed).[18] The method with two stages (and thus order four) has Butcher tableau:where e stands for the vector of ones. The function r is called the stability function.[20] It follows from the formula that r is the quotient of two polynomials of degree s if the method has s stages. Explicit methods have a strictly lower triangular matrix A, which implies that det(I − zA) = 1 and that the stability function is a polynomial.[21]The numerical solution to the linear test equation decays to zero if | r(z) | < 1 with z = hλ. The set of such z is called the domain of absolute stability. In particular, the method is said to be A-stable if all z with Re(z) < 0 are in the domain of absolute stability. The stability function of an explicit Runge–Kutta method is a polynomial, so explicit Runge–Kutta methods can never be A-stable.[21]The Gauss–Legendre method with s stages has order 2s, so its stability function is the Padé approximant with m = n = s. It follows that the method is A-stable.[23] This shows that A-stable Runge–Kutta can have arbitrarily high order. In contrast, the order of A-stable linear multistep methods cannot exceed two.[24]where:where:If we now express the general formula using what we just derived we obtain:we obtain a system of constraints on the coefficients:
Basis (linear algebra)
In mathematics, a set B of elements (vectors) in a vector space V is called a basis, if every element of V may be written in a unique way as a (finite) linear combination of elements of B. The coefficients of this linear combination are referred to as components or coordinates on B of the vector. The elements of a basis are called basis vectors.Equivalently B is a basis if its elements are linearly independent and every element of V is a linear combination of elements of B.[1]  In more general terms, a basis is a linearly independent spanning set.A vector space can have generally several bases; however all the bases have the same number of elements, called the dimension of the vector space.A basis B of a vector space V over a field F (such as the real numbers R or the complex numbers C) is a linearly independent subset of V that spans V.This means that, a subset B of V is a basis if it satisfies the two following conditions:The scalars vi are called the coordinates of the vector v with respect to the basis B, and by the first property they are uniquely determined.A vector space that has a finite basis is called finite-dimensional. In this case, the subset {b1, ..., bn} that is considered (twice) in the above definition may be chosen as B itself.It is often convenient to ordering the basis vectors, typically when one consider the coefficients of a vector on a basis, without referring explicitly to the basis elements. In this case, the ordering is necessary for associating each coefficient to the corresponding basis element. Generally, this ordering is implicitly done by numbering the basis elements. For example, when using matrices, the ith row, and ith column refer to the ith element of a basis of some vector space. For emphasizing that an order has been chosen, one speaks of an ordered basis, which is therefore a sequence rather than a set; see Ordered bases and coordinates below.Many properties of finite bases result from the Steinitz exchange lemma, which states that, given a finite spanning set S and a linearly independent subset L of n elements of S, one may replace n well chosen elements of S by the elements of L for getting a spanning set containing L, having its other elements in S, and having the same number of elements as S.Most properties resulting from Steinitz exchange lemma remain true when there is no finite spanning set, but their proof in the infinite case requires generally the axiom of choice or a weaker form of it, such as the ultrafilter lemma.If V is a vector space over a field F, then:If V is a vector space of dimension n, then:Let V be a vector space of finite dimension n over a field F, and be a basis of V. By definition of a basis, for every v in V may be written, in a unique way,Typically, the new basis vectors are given by their coordinates over the old basis, that is, The formula for changing the coordinates with respect to the other basis results from the uniqueness of the decomposition of a vector over a basis, and is thusfor i = 1, ..., n.be the column vectors of the coordinates of v in the old and the new basis respectively, then the formula for changing coordinates isIf one replaces the field occurring in the definition of a vector space by a ring, one gets the definition of a module. For modules, linear independence and spanning sets are defined exactly as for vector spaces, although "generating set" is more commonly used than that of "spanning set".Like for vector spaces, a basis of a module is a linearly independent subset that is also a generating set. A major difference with the theory of vector spaces is that not every module has a basis. A module that has a basis is called a free module. Free modules play a fundamental role in module theory, as they may be used for describing the structure of non-free modules through free resolutions.The common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces – a large class of vector spaces including e.g. Hilbert spaces, Banach spaces, or Fréchet spaces.In the study of Fourier series, one learns that the functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval [0, 2π] that are square-integrable on this interval, i.e., functions f satisfyingThe functions {1} ∪ { sin(nx), cos(nx) : n = 1, 2, 3, ... } are linearly independent, and every function f that is square-integrable on [0, 2π] is an "infinite linear combination" of them, in the sense thatfor suitable (real or complex) coefficients ak, bk.  But many[2] square-integrable functions cannot be represented as finite linear combinations of these basis functions, which therefore do not comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.For a probability distribution in Rn with a probability density function, such as the equidistribution  in a n-dimensional ball with respect to Lebesgue measure, it can be shown that n randomly and independently chosen vectors will form a basis with probability one, which is due to the fact that n linearly dependent vectors x1, ..., xn in Rn should satisfy the equation det[x1, ..., xn] = 0 (zero determinant of the matrix with columns xi), and the set of zeros of a non-trivial polynomial has zero measure. This observation has led to techniques for approximating random bases.[5][6]In high dimensions, two independent random vectors are with high probability almost orthogonal, and the number of independent random vectors, which all are with given high probability pairwise almost orthogonal, grows exponentially with dimension. More precisely, consider equidistribution in n-dimensional ball. Choose N independent random vectors from a ball (they are independent and identically distributed). Let θ be a small positive number. Then for    (Eq. 1)The figure (right) illustrates distribution of lengths N of pairwise almost orthogonal chains of vectors that are independently randomly sampled from the n-dimensional cube [−1, 1]n as a function of dimension, n. A point is first randomly selected in the cube. The second point is randomly chosen in the same cube. If the angle between the vectors was within π/2 ± 0.037π/2 then the vector was retained. At the next step a new vector is generated in the same hypercube, and its angles with the previously generated vectors are evaluated. If these angles are within π/2 ± 0.037π/2 then the vector is retained. The process is repeated until the chain of almost orthogonality breaks, and the number of such pairwise almost orthogonal vectors (length of the chain) is recorded. For each n, 20 pairwise almost orthogonal chains where constructed numerically for each dimension. Distribution of the length of these chains is presented.Let V be any vector space over some field F.Let X be the set of all linearly independent subsets of V.The set X is nonempty since the empty set is an independent subset of V,and it is partially ordered by inclusion, which is denoted, as usual, by ⊆.Let Y be a subset of X that is totally ordered by ⊆,and let LY be the union of all the elements of Y (which are themselves certain subsets of V).Since (Y, ⊆) is totally ordered, every finite subset of LY is a subset of an element of Y,which is a linearly independent subset of V,and hence every finite subset of LY is linearly independent.Thus LY is linearly independent, so LY is an element of X.Therefore, LY is an upper bound for Y in (X, ⊆):it is an element of X, that contains every element Y.As X is nonempty, and every totally ordered subset of (X, ⊆) has an upper bound in X, Zorn's lemma asserts that X has a maximal element. In other words, there exists some element Lmax of X satisfying the condition that whenever Lmax ⊆ L for some element L of X, then L = Lmax.It remains to prove that Lmax is a basis of V.  Since Lmax belongs to X, we already know that Lmax is a linearly independent subset of V.If Lmax would not span V, there would exist some vector w of V that cannot be expressed as a linear combination of elements of Lmax (with coefficients in the field F). In particular, w cannot be an element of Lmax.Let Lw  =  Lmax ∪ {w}. This set is an element of X, that is, it is a linearly independent subset of V (because w is not in the span of Lmax, and Lmax is independent). As Lmax ⊆ Lw, and Lmax ≠ Lw (because Lw contains the vector w that is not contained in Lmax), this contradicts the maximality of Lmax. Thus this shows that Lmax spans V.Hence Lmax is linearly independent and spans V. It is thus a basis of V, and this proves that every vector space has a basis.This proof relies on Zorn's lemma, which is equivalent to the axiom of choice. Conversely, it may be proved that if every vector space has a basis, then the axiom of choice is true; thus the two assertions are equivalent.
Ring (mathematics)
In mathematics, a ring is one of the fundamental algebraic structures used in abstract algebra. It consists of a set equipped with two binary operations that generalize the arithmetic operations of addition and multiplication. Through this generalization, theorems from arithmetic are extended to non-numerical objects such as polynomials, series, matrices and functions.A ring is an abelian group with a second binary operation that is associative, is distributive over the abelian group operation, and has an identity element (this last property is not required by some authors, see § Notes on the definition). By extension from the integers, the abelian group operation is called addition and the second binary operation is called multiplication.Whether a ring is commutative or not (i.e., whether the order in which two elements are multiplied changes the result or not) has profound implications on its behavior as an abstract object. As a result, commutative ring theory, commonly known as commutative algebra, is a key topic in ring theory. Its development has been greatly influenced by problems and ideas occurring naturally in algebraic number theory and algebraic geometry. Examples of commutative rings include the set of integers equipped with the addition and multiplication operations, the set of polynomials equipped with their addition and multiplication, the coordinate ring of an affine algebraic variety, and the ring of integers of a number field. Examples of noncommutative rings include the ring of n × n real square matrices with n ≥ 2, group rings in representation theory, operator algebras in functional analysis, rings of differential operators in the theory of differential operators, and the cohomology ring of a topological space in topology.The conceptualization of rings began in the 1870s and was completed in the 1920s. Key contributors include Dedekind, Hilbert, Fraenkel, and Noether. Rings were first formalized as a generalization of Dedekind domains that occur in number theory, and of polynomial rings and rings of invariants that occur in algebraic geometry and invariant theory. Afterward, they also proved to be useful in other branches of mathematics such as geometry and mathematical analysis.The familiar properties for addition and multiplication of integers serve as a model for the axioms for rings.A ring is a set R equipped with two binary operations[1] + and · satisfying the following three sets of axioms, called the ring axioms[2][3][4]1. R is an abelian group under addition, meaning that:2. R is a monoid under multiplication, meaning that:3. Multiplication is distributive with respect to addition:As explained in § History below, many authors follow an alternative convention in which a ring is not defined to have a multiplicative identity. This article adopts the convention that, unless otherwise stated, a ring is assumed to have such an identity.  A structure satisfying all the axioms except the requirement that there exists a multiplicative identity element is called a rng (commonly pronounced "rung", and sometimes called a pseudo-ring). For example, the set of even integers with the usual + and ⋅ is a rng, but not a ring.The operations + and ⋅ are called addition and multiplication, respectively. The multiplication symbol ⋅ is often omitted, so the juxtaposition of ring elements is interpreted as multiplication. For example, xy means x ⋅ y.Although ring addition is commutative, ring multiplication is not required to be commutative: ab need not necessarily equal ba. Rings that also satisfy commutativity for multiplication (such as the ring of integers) are called commutative rings. Books on commutative algebra or algebraic geometry often adopt the convention that "ring" means "commutative ring", to simplify terminology.In a ring, multiplication does not have to have an inverse. A (non-trivial) commutative ring such that every nonzero element has a multiplicative inverse is called a field.The additive group of a ring is the ring equipped just with the structure of addition. Although the definition assumes that the additive group is abelian, this can be inferred from the other ring axioms.[6]Some basic properties of a ring follow immediately from the axioms:The set of 2-by-2 matrices with real number entries is writtenMore generally, for any ring R, commutative or not, and any nonnegative integer n, one may form the ring of n-by-n matrices with entries in R: see Matrix ring.The study of rings originated from the theory of polynomial rings and the theory of algebraic integers.[7] In 1871, Richard Dedekind defined the concept of the ring of integers of a number field.[8] In this context, he introduced the terms "ideal" (inspired by Ernst Kummer's notion of ideal number) and "module" and studied their properties. But Dedekind did not use the term "ring" and did not define the concept of a ring in a general setting.The term "Zahlring" (number ring) was coined by David Hilbert in 1892 and published in 1897.[9] In 19th century German, the word "Ring" could mean "association", which is still used today in English in a limited sense (e.g., spy ring),[10] so if that were the etymology then it would be similar to the way "group" entered mathematics by being a non-technical word for "collection of related things". According to Harvey Cohn, Hilbert used the term for a ring that had the property of "circling directly back" to an element of itself.[11] Specifically, in a ring of algebraic integers, all high powers of an algebraic integer can be written as an integral combination of a fixed set of lower powers, and thus the powers "cycle back". For instance, if a3 − 4a + 1 = 0 then a3 = 4a − 1, a4 = 4a2 − a, a5 = −a2 + 16a − 4, a6 = 16a2 − 8a + 1, a7 = −8a2 + 65a − 16, and so on; in general, an is going to be an integral linear combination of 1, a, and a2.The first axiomatic definition of a ring was given by Adolf Fraenkel in 1914,[12][13] but his axioms were stricter than those in the modern definition. For instance, he required every non-zero-divisor to have a multiplicative inverse.[14] In 1921, Emmy Noether gave the modern axiomatic definition of (commutative) ring and developed the foundations of commutative ring theory in her paper Idealtheorie in Ringbereichen.[15]Fraenkel required a ring to have a multiplicative identity 1,[16] whereas Noether did not.[15]Most or all books on algebra[17][18] up to around 1960 followed Noether's convention of not requiring a 1. Starting in the 1960s, it became increasingly common to see books including the existence of 1 in the definition of ring, especially in advanced books by notable authors such as Artin,[19] Atiyah and MacDonald,[20] Bourbaki,[21] Eisenbud,[22] and Lang.[23] But even today, there remain many books that do not require a 1.Faced with this terminological ambiguity, some authors have tried to impose their views, while others have tried to adopt more precise terms.In the first category, we find for instance Gardner and Wiegandt, who argue that if one requires all rings to have a 1, then some consequences include the lack of existence of infinite direct sums of rings, and the fact that proper direct summands of rings are not subrings. They conclude that "in many, maybe most, branches of ring theory the requirement of the existence of a unity element is not sensible, and therefore unacceptable."[24]In the second category, we find authors who use the following terms:[25][26]Commutative rings:Noncommutative rings:Non-rings:For example, the ring Z of integers is a subring of the field of real numbers and also a subring of the ring of polynomials Z[X] (in both cases, Z contains 1, which is the multiplicative identity of the larger rings). On the other hand, the subset of even integers 2Z does not contain the identity element 1 and thus does not qualify as a subring of Z.An intersection of subrings is a subring. The smallest subring containing a given subset E of R is called a subring generated by E. Such a subring exists since it is the intersection of all subrings containing E.The definition of an ideal in a ring is analogous to that of normal subgroup in a group. But, in actuality, it plays a role of an idealized generalization of an element in a ring; hence, the name "ideal". Like elements of rings, the study of ideals is central to structural understanding of a ring.Like a group, a ring is said to be simple if it is nonzero and it has no proper nonzero two-sided ideals. A commutative simple ring is precisely a field.Rings are often studied with special conditions set upon their ideals. For example, a ring in which there is no strictly increasing infinite chain of left ideals is called a left Noetherian ring. A ring in which there is no strictly decreasing infinite chain of left ideals is called a left Artinian ring. It is a somewhat surprising fact that a left Artinian ring is left Noetherian (the Hopkins–Levitzki theorem). The integers, however, form a Noetherian ring which is not Artinian.A homomorphism from a ring (R, +, ·) to a ring (S, ‡, *) is a function f from R to S that preserves the ring operations; namely, such that, for all a, b in R the following identities hold:If one is working with not necessarily unital rings, then the third condition is dropped.Examples:To give a ring homomorphism from a commutative ring R to a ring A with image contained in the center of A is the same as to give a structure of an algebra over R to A (in particular gives a structure of A-module).The quotient ring of a ring, is analogous to the notion of a quotient group of a group. More formally, given a ring (R, +, · ) and a two-sided ideal I of (R, +, · ), the quotient ring (or factor ring) R/I is the set of cosets of I (with respect to the additive group of (R, +, · ); i.e. cosets with respect to (R, +)) together with the operations:for every a, b in R.The concept of a module over a ring generalizes the concept of a vector space (over a field) by generalizing from multiplication of vectors with elements of a field (scalar multiplication) to multiplication with elements of a ring. More precisely, given a ring R with 1, an R-module M is an abelian group equipped with an operation R × M → M (associating an element of M to every pair of an element of R and an element of M) that satisfies certain axioms. This operation is commonly denoted multiplicatively and called multiplication. The axioms of modules are the following: for all a, b in R and all x, y in M, we have:When the ring is noncommutative these axioms define left modules; right modules are defined similarly by writing xa instead of ax. This is not only a change of notation, as the last axiom of right modules (that is x(ab) = (xa)b) becomes (ab)x = b(ax), if left multiplication (by ring elements) is used for a right module.Basic examples of modules are ideals, including the ring itself.Although similarly defined, the theory of modules is much more complicated than that of vector space, mainly, because, unlike vector spaces, modules are not characterized (up to an isomorphism) by a single invariant (the dimension of a vector space). In particular, not all modules have a basis.The axioms of modules imply that (−1)x = −x, where the first minus denotes the additive inverse in the ring and the second minus the additive inverse in the module. Using this and denoting repeated addition by a multiplication by a positive integer allows identifying abelian groups with modules over the ring of integers.Any ring homomorphism induces a structure of a module: if f : R → S is a ring homomorphism, then S is a left module over R by the multiplication: rs = f(r)s. If R is commutative or if f(R) is contained in the center of S, the ring S is called a R-algebra. In particular, every ring is an algebra over the integers.Let R and S be rings. Then the product R × S can be equipped with the following natural ring structure:as a direct sum of abelian groups (because for abelian groups finite products are the same as direct sums). Clearly the direct sum of such ideals also defines a product of rings that is isomorphic to R. Equivalently, the above can be done through central idempotents. Assume R has the above decomposition. Then we can writeAn important application of an infinite direct product is the construction of a projective limit of rings (see below). Another application is a restricted product of a family of rings (cf. adele ring).Given a symbol t (called a variable) and a commutative ring R, the set of polynomialsAny ring homomorphism R → S induces Mn(R) → Mn(S); in fact, any ring homomorphism between matrix rings arises in this way.[33]The Artin–Wedderburn theorem states any semisimple ring (cf. below) is of this form.A ring R and the matrix ring Mn(R) over it are Morita equivalent: the category of right modules of R is equivalent to the category of right modules over Mn(R).[33] In particular, two-sided ideals in R correspond in one-to-one to two-sided ideals in Mn(R).Examples:Examples of colimits:Any commutative ring is the colimit of finitely generated subrings.For an example of a projective limit, see § Completion.The most important properties of localization are the following: when R is a commutative ring and S a multiplicatively closed subsetA complete ring has much simpler structure than a commutative ring. This owns to the Cohen structure theorem, which says, roughly, that a complete local ring tends to look like a formal power series ring or a quotient of it. On the other hand, the interaction between the integral closure and completion has been among the most important aspects that distinguish modern commutative ring theory from the classical one developed by the likes of Noether. Pathological examples found by Nagata led to the reexamination of the roles of Noetherian rings and motivated, among other things, the definition of excellent ring.A nonzero ring with no nonzero zero-divisors is called a domain. A commutative domain is called an integral domain. The most important integral domains are principal ideals domains, PID for short, and fields. A principal ideal domain is an integral domain in which every ideal is principal. An important class of integral domains that contain a PID is a unique factorization domain (UFD), an integral domain in which every nonunit element is a product of prime elements (an element is prime if it generates a prime ideal.) The fundamental question in algebraic number theory is on the extent to which the ring of (generalized) integers in a number field, where an "ideal" admits prime factorization, fails to be a PID.In algebraic geometry, UFDs arise because of smoothness. More precisely, a point in a variety (over a perfect field) is smooth if the local ring at the point is a regular local ring. A regular local ring is a UFD.[42]The following is a chain of class inclusions that describes the relationship between rings, domains and fields:A division ring is a ring such that every non-zero element is a unit. A commutative division ring is a field. A prominent example of a division ring that is not a field is the ring of quaternions. Any centralizer in a division ring is also a division ring. In particular, the center of a division ring is a field. It turned out that every finite domain (in particular finite division ring) is a field; in particular commutative (the Wedderburn's little theorem).Every module over a division ring is a free module (has a basis); consequently, much of linear algebra can be carried out over a division ring instead of a field.The study of conjugacy classes figures prominently in the classical theory of division rings. Cartan famously asked the following question: given a division ring D and a proper sub-division-ring S that is not contained in the center, does each inner automorphism of D restrict to an automorphism of S? The answer is negative: this is the Cartan–Brauer–Hua theorem.A cyclic algebra, introduced by L. E. Dickson, is a generalization of a quaternion algebra.A ring is called a semisimple ring if it is semisimple as a left module (or right module) over itself; i.e., a direct sum of simple modules. A ring is called a semiprimitive ring if its Jacobson radical is zero. (The Jacobson radical is the intersection of all maximal left ideals.) A ring is semisimple if and only if it is artinian and is semiprimitive.An algebra over a field k is artinian if and only if it has finite dimension. Thus, a semisimple algebra over a field is necessarily finite-dimensional, while a simple algebra may have infinite dimension; e.g., the ring of differential operators.Any module over a semisimple ring is semisimple. (Proof: any free module over a semisimple ring is clearly semisimple and any module is a quotient of a free module.)Examples of semisimple rings:The Skolem–Noether theorem states any automorphism of a central simple algebra is inner.Azumaya algebras generalize the notion of central simple algebras to a commutative local ring.If K is a field, a valuation v is a group homomorphism from the multiplicative group K* to a totally ordered abelian group G such that, for any f, g in K with f + g nonzero, v(f + g) ≥ min{v(f), v(g)}. The valuation ring of v is the subring of K consisting of zero and all nonzero f such that v(f) ≥ 0.Examples:See also: Novikov ring and uniserial ring.A ring may be viewed as an abelian group (by using the addition operation), with extra structure: namely, ring multiplication. In the same way, there are other mathematical objects which may be considered as rings with extra structure. For example:Many different kinds of mathematical objects can be fruitfully analyzed in terms of some associated ring.To any topological space X one can associate its integral cohomology ringThe ring structure in cohomology provides the foundation for characteristic classes of fiber bundles, intersection theory on manifolds and algebraic varieties, Schubert calculus and much more.To any group is associated its Burnside ring which uses a ring to describe the various ways the group can act on a finite set. The Burnside ring's additive group is the free abelian group whose basis are the transitive actions of the group and whose addition is the disjoint union of the action. Expressing an action in terms of the basis is decomposing an action into its transitive constituents. The multiplication is easily expressed in terms of the representation ring: the multiplication in the Burnside ring is formed by writing the tensor product of two permutation modules as a permutation module. The ring structure allows a formal way of subtracting one action from another. Since the Burnside ring is contained as a finite index subring of the representation ring, one can pass easily from one to the other by extending the coefficients from integers to the rational numbers.To any group ring or Hopf algebra is associated its representation ring or "Green ring". The representation ring's additive group is the free abelian group whose basis are the indecomposable modules and whose addition corresponds to the direct sum. Expressing a module in terms of the basis is finding an indecomposable decomposition of the module. The multiplication is the tensor product. When the algebra is semisimple, the representation ring is just the character ring from character theory, which is more or less the Grothendieck group given a ring structure.To any irreducible algebraic variety is associated its function field. The points of an algebraic variety correspond to valuation rings contained in the function field and containing the coordinate ring. The study of algebraic geometry makes heavy use of commutative algebra to study geometric concepts in terms of ring-theoretic properties. Birational geometry studies maps between the subrings of the function field.Every simplicial complex has an associated face ring, also called its Stanley–Reisner ring. This ring reflects many of the combinatorial properties of the simplicial complex, so it is of particular interest in algebraic combinatorics. In particular, the algebraic geometry of the Stanley–Reisner ring was used to characterize the numbers of faces in each dimension of simplicial polytopes.Let (A, +) be an abelian group and let End(A) be its endomorphism ring (see above). Note that, essentially, End(A) is the set of all morphisms of A, where if f is in End(A), and g is in End(A), the following rules may be used to compute f + g and f · g:where + as in f(x) + g(x) is addition in A, and function composition is denoted from right to left. Therefore, associated to any abelian group, is a ring. Conversely, given any ring, (R, +, · ), (R, +) is an abelian group. Furthermore, for every r in R, right (or left) multiplication by r gives rise to a morphism of (R, +), by right (or left) distributivity. Let A = (R, +). Consider those endomorphisms of A, that "factor through" right (or left) multiplication of R. In other words, let EndR(A) be the set of all morphisms m of A, having the property that m(r · x) = r · m(x). It was seen that every r in R gives rise to a morphism of A: right multiplication by r. It is in fact true that this association of any element of R, to a morphism of A, as a function from R to EndR(A), is an isomorphism of rings. In this sense, therefore, any ring can be viewed as the endomorphism ring of some abelian X-group (by X-group, it is meant a group with X being its set of operators).[45] In essence, the most general form of a ring, is the endomorphism group of some abelian X-group.Any ring can be seen as a preadditive category with a single object. It is therefore natural to consider arbitrary preadditive categories to be generalizations of rings. And indeed, many definitions and theorems originally given for rings can be translated to this more general context. Additive functors between preadditive categories generalize the concept of ring homomorphism, and ideals in additive categories can be defined as sets of morphisms closed under addition and under composition with arbitrary morphisms.Algebraists have defined structures more general than rings by weakening or dropping some of ring axioms.A rng is the same as a ring, except that the existence of a multiplicative identity is not assumed.[46]A nonassociative ring is an algebraic structure that satisfies all of the ring axioms except the associative property and the existence of a multiplicative identity. A notable example is a Lie algebra. There exists some structure theory for such algebras that generalizes the analogous results for Lie algebras and associative algebras.[citation needed]A semiring is obtained by weakening the assumption that (R,+) is an abelian group to the assumption that (R,+) is a commutative monoid, and adding the axiom that 0 · a = a · 0 = 0 for all a in R (since it no longer follows from the other axioms).Example: a tropical semiring.In algebraic geometry, a ring scheme over a base scheme S is a ring object in the category of S-schemes. One example is the ring scheme Wn over Spec Z, which for any commutative ring A returns the ring Wn(A) of p-isotypic Witt vectors of length n over A.[47]Special types of rings:^ a: Some authors only require that a ring be a semigroup under multiplication; that is, do not require that there be a multiplicative identity (1). See the section Notes on the definition for more details.^ b: Elements which do have multiplicative inverses are called units, see Lang 2002, §II.1, p. 84.^ c: The closure axiom is already implied by the condition that +/• be a binary operation. Some authors therefore omit this axiom. Lang 2002^ d: The transition from the integers to the rationals by adding fractions is generalized by the quotient field.^ e: Many authors include commutativity of rings in the set of ring axioms (see above) and therefore refer to "commutative rings" as just "rings".
Chebyshev nodes
In numerical analysis, Chebyshev nodes are specific real algebraic numbers, namely the roots of the Chebyshev polynomials of the first kind. They are often used as nodes in polynomial interpolation because the resulting interpolation polynomial minimizes the effect of Runge's phenomenon.[2]For a given positive integer n the Chebyshev nodes in the interval (−1, 1) areThese are the roots of the Chebyshev polynomial of the first kind of degree n. For nodes over an arbitrary interval [a, b] an affine transformation can be used:This product is a monic polynomial of degree n.  It may be shown that the maximum absolute value (maximum norm) of any such polynomial is bounded from below by 21−n.  This bound is attained by the scaled Chebyshev polynomials 21−n Tn, which are also monic. (Recall that |Tn(x)| ≤ 1 for x ∈ [−1, 1].[4]) Therefore, when the interpolation nodes xi are the roots of Tn, the error satisfiesFor an arbitrary interval [a, b] a change of variable shows that
Non-negative matrix factorization
Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.NMF finds applications in such fields as astronomy,[3][4][5] computer vision, document clustering,[1] chemometrics, audio signal processing, recommender systems,[6][7] and bioinformatics.[8]In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[9]In this framework the vectors in the right matrix are continuous curves rather than discrete vectors.Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[10][11]It became more widely known as non-negative matrix factorization after Lee and Seung investigatedthe properties of the algorithm and published some simple and usefulalgorithms for two types of factorizations.[12][13]Let matrix V be the product of the matrices W and H,Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H.  That is, each column of V can be computed as follows:where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m × n matrix, W is an m × p matrix, and H is a p × n matrix then p can be significantly less than both m and n.Here is an example based on a text-mining application: This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H.When the error function to be used is Kullback–Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[15]Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V.  The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[17][18][19] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[20]There are different types of non-negative matrix factorizations.The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback–Leibler divergence to positive matrices (the original Kullback–Leibler divergence is defined on probability distributions).Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.Another type of NMF for images is based on the total variation norm.[21]When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[22][23]although it may also still be referred to as NMF.[24]Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[25][26][27]There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[13] has been a popular method due to the simplicity of implementation.  This algorithm is:Note that the updates are done on an element by element basis not matrix multiplication.We note that W and H multiplicative factor is identity matrix when V = W H.More recently other algorithms have been developed.Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[28] or different, as some NMF variants regularize one of W and H.[22] Specific approaches include the projected gradient descent methods,[28][29] the active set method,[6][30] the optimal gradient method,[31] and the block principal pivoting method[32] among several others.[33]Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[34] However, as in many other data mining applications, a local minimum may still prove to be useful.The contribution of the sequential NMF components can be compared with the Karhunen–Loève theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting.[36][37] For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA,[5] which is the indication of less over-fitting of sequential NMF.Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[38] Kalofolias and Gallopoulos (2012)[39] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[40]In Learning the parts of objects by non-negative matrix factorization Lee and Seung[41] proposed NMF mainly for parts-based decomposition of images.  It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[42]When NMF is obtained by minimizing the Kullback–Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[43]trained by maximum likelihood estimation.That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[14][44]  This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[16]NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[45]NMF extends beyond matrices to tensors of arbitrary order.[46][47][48] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[49]NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[50]The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[51]More control over the non-uniqueness of NMF is obtained with sparsity constraints.[52]In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3][4] and the direct imaging observations [5] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [4] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [35] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [5] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.Ren et al. (2018) [5] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10⁵ to 10¹⁰, various statistical methods have been adopted,[53][54][36] however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux.[55][37] Forward modeling is currently optimized for point sources,[37] however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors,[5] rather than a computationally intensive data re-reduction on generated models.NMF can be used for text mining applications.In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents.This matrix is factored into a term-feature and a feature-document matrix.The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[56]Another research group clustered parts of the Enron email dataset[57]with 65,033 messages and 91,133 terms into 50 clusters.[58]NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[59]Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[40]NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[60]Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[63] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[23][64][65][66] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[67]NMF, also referred in this field as factor analysis, has been used since the 1980s[68] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[69]Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,
Pohlke's theorem
Pohlke's theorem is the fundamental theorem of axonometry. It was established 1853 by the German painter and teacher of descriptive geometry Karl Wilhelm Pohlke. The first proof of the theorem was published 1864 by the German mathematician Hermann Amandus Schwarz, who was a student of Pohlke. Therefore the theorem is sometimes called theorem of Pohlke and Schwarz, too.Pohlke's theorem can be stated in terms of linear algebra as:Pohlke's theorem is the justification for the following easy procedure to construct a scaled parallel projection of a 3-dimensional object using coordinates,:[2][3]In order to get undistorted pictures, one has to choose the images of the axes and the forshortenings carefully (see Axonometry). In order to get an orthographic projection only the images of the axes are free and the forshortenings are determined. (see de:orthogonale Axonometrie).Schwarz formulated and proved the more general statement:and used a theorem of L’Huilier: 
Linear subspace
In linear algebra and related fields of mathematics, a linear subspace, also known as a vector subspace, or, in the older literature, a linear manifold,[1][2] is a vector space that is a subset of some other (higher-dimension) vector space. A linear subspace is usually called simply a subspace when the context serves to distinguish it from other kinds of subspace.Let K be a field (such as the real numbers), V be a vector space over K, and let W be a subset of V.Then W is a subspace if:Example I:Let the field K be the set R of real numbers, and let the vector space V be the real coordinate space R3.Take W to be the set of all vectors in V whose last component is 0.Then W is a subspace of V.Proof:Example II:Let the field be R again, but now let the vector space be the Cartesian plane R2.Take W to be the set of points (x, y) of R2 such that x = y.Then W is a subspace of R2.Proof:In general, any subset of the real coordinate space Rn that is defined by a system of homogeneous linear equations will yield a subspace.(The equation in example I was z = 0, and the equation in example II was x = y.)Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point 0.Example III:Again take the field to be R, but now let the vector space V be the set RR of all functions from R to R.Let C(R) be the subset consisting of continuous functions.Then C(R) is a subspace of RR.Proof:Example IV:Keep the same field and vector space as before, but now consider the set Diff(R) of all differentiable functions.The same sort of argument as before shows that this is a subspace too.Examples that extend these themes are common in functional analysis.A way to characterize subspaces is that they are closed under linear combinations.That is, a nonempty set W is a subspace if and only if every linear combination of (finitely many) elements of W also belongs to W.Conditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.In a topological vector space X, a subspace W need not be closed in general, but a finite-dimensional subspace is always closed.[3] The same is true for subspaces of finite codimension, i.e. determined by a finite number of continuous linear functionals.Descriptions of subspaces include the solution set to a homogeneous system of linear equations, the subset of Euclidean space described by a system of homogeneous linear parametric equations, the span of a collection of vectors, and the null space, column space, and row space of a matrix. Geometrically (especially, over the field of real numbers and its subfields), a subspace is a flat in an n-space that passes through the origin.A natural description of an 1-subspace is the scalar multiplication of one non-zero vector v to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:This idea is generalized for higher dimensions with linear span, but criteria for equality of k-spaces specified by sets of k vectors are not so simple.A dual description is provided with linear functionals (usually implemented as linear equations). One non-zero linear functional F specifies its kernel subspace F = 0 of codimension 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the dual space):It is generalized for higher codimensions with a system of equations. The following two subsections will present this latter description in details, and the remaining four subsections further describe the idea of linear span.The solution set to any homogeneous system of linear equations with n variables is a subspace in the coordinate space Kn:For example (over real or rational numbers), the set of all vectors (x, y, z) satisfying the equationsis a one-dimensional subspace. More generally, that is to say that given a set of n independent functions, the dimension of the subspace in Kk will be the dimension of the null set of A, the composite matrix of the n functions.In a finite-dimensional space, a homogeneous system of linear equations can be written as a single matrix equation:The set of solutions to this equation is known as the null space of the matrix. For example, the subspace described above is the null space of the matrixEvery subspace of Kn can be described as the null space of some matrix (see algorithms, below).The subset of Kn described by a system of homogeneous linear parametric equations is a subspace:For example, the set of all vectors (x, y, z) parameterized by the equationsis a two-dimensional subspace of K3, if K is a number field (such as real or rational numbers).[4]In linear algebra, the system of parametric equations can be written as a single vector equation:The expression on the right is called a linear combination of the vectors(2, 5, −1) and (3, −4, 2). These two vectors are said to span the resulting subspace.In general, a linear combination of vectors v1, v2, ... , vk is any vector of the formThe set of all possible linear combinations is called the span:If the vectors v1, ... , vk have n components, then their span is a subspace of Kn. Geometrically, the span is the flat through the origin in n-dimensional space determined by the points v1, ... , vk.A system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:In this case, the subspace consists of all possible values of the vector x. In linear algebra, this subspace is known as the column space (or image) of the matrix A. It is precisely the subspace of Kn spanned by the column vectors of A.The row space of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the orthogonal complement of the null space (see below).In general, a subspace of Kn determined by k parameters (or spanned by k vectors) has dimension k. However, there are exceptions to this rule. For example, the subspace of K3 spanned by the three vectors (1, 0, 0), (0, 0, 1), and (2, 0, 3) is just the xz-plane, with each point on the plane described by infinitely many different values of  t1, t2, t3.In general, vectors v1, ... , vk are called linearly independent iffor(t1, t2, ... , tk) ≠ (u1, u2, ... , uk).[5]If  v1, ..., vk  are linearly independent, then the coordinates  t1, ..., tk for a vector in the span are uniquely determined.A basis for a subspace S is a set of linearly independent vectors whose span is S. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see algorithms, below).The set-theoretical inclusion binary relation specifies a partial order on the set of all subspaces (of any dimension).A subspace cannot lie in any subspace of lesser dimension. If dim U = k, a finite number, and U ⊂ W, then dim W = k if and only if U = W.Given subspaces U and W of a vector space V, then their intersection U ∩ W := {v ∈ V : v is an element of both U and W} is also a subspace of V.[6]Proof:For every vector space V, the set {0} and V itself are subspaces of V.[7]If U and W are subspaces, their sum is the subspaceFor example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequalityHere the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:The operations intersection and sum make the set of all subspaces a bounded modular lattice, where the {0} subspace, the least element, is an identity element of the sum operation, and the identical subspace V, the greatest element, is an identity element of the intersection operation.If V is an inner product space, then the orthogonal complement ⊥ of any subspace of V is again a subspace. This operation, understood as negation (¬), makes the lattice of subspaces a (possibly infinite) orthocomplemented lattice (it is not a distributive lattice).In a pseudo-Euclidean space there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a Heyting algebra) because of null subspaces, for which N ∩ N⊥ = N ≠ {0}. The same case presents the ⊥ operation in symplectic vector spaces.Most algorithms for dealing with subspaces involve row reduction. This is the process of applying elementary row operations to a matrix until it reaches either row echelon form or reduced row echelon form. Row reduction has the following important properties:See the article on row space for an example.If we instead put the matrix A into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of Kn are equal.See the article on column space for an example.This produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.If the final column of the reduced row echelon form contains a pivot, then the input vector v does not lie in S.See the article on null space for an example.
Linear equation
In mathematics, a linear equation is an equation that may be put in the formIn the words of algebra, a linear equation is obtained by equating to zero a linear polynomial over some field, where the coefficients are taken from, and that does not contain the symbols for the indeterminates.The solutions of such an equation are the values that, when substituted to the unknowns, make the equality true.The case of just one variable is of particular importance, and it is frequent that the term linear equation refers implicitly to this particular case, in which the name unknown for the variable is sensibly used.All the pairs of numbers that are solutions of a linear equation in two variables form a line in the Euclidean plane, and every line may be defined as the solutions of a linear equation. This is the origin of the term linear for qualifying this type of equations. More generally, the solutions of a linear equation in n variables form a hyperplane (of dimension n – 1) in the Euclidean space of dimension n.Linear equations occur frequently in all mathematics and their applications in physics and engineering, partly because non-linear systems are often well approximated by linear equations.This article considers the case of a single equation with coefficients from the field of real numbers, for which one studies the real solutions. All its content applies for complex solutions and, more generally, for linear equations with coefficient and solutions in any field. For the case of several simultaneous linear equations, see System of linear equations.Frequently the term linear equation refers implicitly to the case of just one variable. This case, in which the name unknown for the variable is sensibly used, is of particular importance, since it offers a unique value as solution to the equation. According to the above definition such an equation has the formand, for a ≠ 0, a unique value as solution The above equation may always be rewritten toand the solution is of course the same in both cases: These equivalent variants are sometimes given generic names, like general form or standard form,[1] but contribute no new concepts.which is identical to the above form. The intercept form also works conveniently in higher dimensions for specifying (hyper)planes, when their intersections with all coordinate axes exist and are known.Expanding, regrouping, and appropriately factoring the products leads toThe products in the above equation result also from the evaluation of a 2-rowed determinant, inducing this form of the linear equation:The products on the left hand side of the expanded version can be reproduced by evaluating the 3-rowed determinant, designed for easy memorability:Equating the exterior product of these two vectors, as specified above, to zero, yields a linear equationwhich is identical to the determinant form above.Writing a linear equation in two unknowns in the formThis notation can easily expanded to more linear equations in more than two variables. For example, a system of two equations in two variablesA linear equation, written in the form y = f(x) whose  graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.An everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point–slope form or slope–intercept form.For an equation to have meaningful solutions, at least one coefficient must be non-zero. This can be formulated as 
Dual space
In mathematics, any vector space V has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants.The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the algebraic dual space.  When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the continuous dual space.Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.for all φ and ψ ∈ V∗, x ∈ V, and a ∈ F. Elements of the algebraic dual space V∗ are sometimes called covectors or one-forms.The pairing of a functional φ in the dual space V∗ and an element x of V is sometimes denoted by a bracket: φ(x) = [x,φ] [2]or φ(x) = ⟨φ,x⟩.[3] This pairing defines a nondegenerate bilinear mapping[4] ⟨·,·⟩ : V∗ × V → F called the natural pairing.If V is finite-dimensional, then V∗ has the same dimension as V. Given a basis {e1, ..., en}  in V, it is possible to construct a specific basis in V∗, called the dual basis. This dual basis is a set {e1, ..., en}  of linear functionals on V, defined by the relationfor any choice of coefficients ci ∈ F. In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equationsIn particular, if we interpret Rn as the space of columns of n real numbers, its dual space is typically written as the space of rows of n real numbers. Such a row acts on Rn as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a real number y. Then, seeing this functional as a matrix M, and x, y as a n × 1 matrix and a 1 × 1 matrix (trivially, a real number) respectively, if we have Mx = y, then, by dimension reasons, M must be a 1 × n matrix, i.e., M must be a row vector.If V consists of the space of geometrical vectors in the plane, then the level curves of an element of V∗ form a family of parallel lines in V, because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of V∗ can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if V is a vector space of any dimension, then the level sets of a linear functional in V∗ are parallel hyperplanes in V, and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.[5]If V is not finite-dimensional but has a basis[6] eα indexed by an infinite set A, then the same construction as in the finite-dimensional case yields linearly independent elements eα (α ∈ A) of the dual space, but they will not form a basis.Consider, for instance, the space R∞, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for i ∈ N, ei is the sequence consisting of all zeroes except in the ith position, which is 1. The dual space of R∞ is (isomorphic to) RN, the space of all sequences of real numbers: such a sequence (an) is applied to an element (xn) of R∞ to give the number ∑anxn, which is a finite sum because there are only finitely many nonzero xn. The dimension of R∞ is countably infinite, whereas RN does not have a countable basis.This observation generalizes to any[6] infinite-dimensional vector space V over any field F: a choice of basis {eα : α ∈ A}  identifies V with the space (FA)0 of functions f : A → F such that fα = f(α) is nonzero for only finitely many α ∈ A, where such a function f is identified with the vectorin V (the sum is finite by the assumption on f, and any v ∈ V may be written in this way by the definition of the basis).The dual space of V may then be identified with the space FA of all functions from A to F: a linear functional T on V is uniquely determined by the values θα = T(eα) it takes on the basis of V, and any function θ : A → F (with θ(α) = θα) defines a linear functional T on V byAgain the sum is finite because fα is nonzero for only finitely many α.Note that (FA)0 may be identified (essentially by definition) with the direct sumof infinitely many copies of F (viewed as a 1-dimensional vector space over itself) indexed by A, i.e., there are linear isomorphismsOn the other hand, FA is (again by definition), the direct product of infinitely many copies of F indexed by A, and so the identificationis a special case of a general result relating direct sums (of modules) to direct products.Thus if the basis is infinite, then the algebraic dual space is always of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.If V is finite-dimensional, then V is isomorphic to V∗. But there is in general no natural isomorphism between these two spaces.[7]  Any bilinear form ⟨·,·⟩ on V gives a mapping of V into its dual space viawhere the right hand side is defined as the functional on V taking each w ∈ V to ⟨v,w⟩.  In other words, the bilinear form determines a linear mappingdefined byThus there is a one-to-one correspondence between isomorphisms of V to subspaces of (resp., all of) V∗ and nondegenerate bilinear forms on V.If the vector space V is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms.  In that case, a given sesquilinear form ⟨·,·⟩ determines an isomorphism of V with the complex conjugate of the dual spaceThe conjugate space V∗ can be identified with the set of all additive complex-valued functionals f: V → C such thatIf f : V → W is a linear map, then the transpose (or dual) f∗ : W∗ → V∗ is defined byfor every φ ∈ W∗. The resulting functional f∗(φ) in V∗ is called the pullback of φ along f.The following identity holds for all φ ∈ W∗ and v ∈ V:where the bracket [·,·] on the left is the natural pairing of V with its dual space, and that on the right is the natural pairing of W with its dual.  This identity characterizes the transpose,[9] and is formally similar to the definition of the adjoint.The assignment f ↦ f∗ produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W∗ to V∗; this homomorphism is an isomorphism if and only if W is finite-dimensional. If V = W then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that (fg)∗ = g∗f∗. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify  (f∗)∗ with f using the natural injection into the double dual.If the linear map f is represented by the matrix A with respect to two bases of V and W, then f∗ is represented by the transpose matrix AT with respect to the dual bases of W∗ and V∗, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, f∗ is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.Let S be a subset of V. The annihilator of S in V∗, denoted here S0, is the collection of linear functionals f ∈ V∗ such that [f, s] = 0 for all s ∈ S. That is, S0 consists of all linear functionals f : V → F such that the restriction to S vanishes: f|S = 0.The annihilator of a subset is itself a vector space. In particular, ∅0 = V∗ is all of V∗ (vacuously), whereas V0 = 0 is the zero subspace. Furthermore, the assignment of an annihilator to a subset of V reverses inclusions, so that if S ⊂ T ⊂ V, thenMoreover, if A and B are two subsets of V, thenand equality holds provided V is finite-dimensional. If Ai is any family of subsets of V indexed by i belonging to some index set I, thenIn particular if A and B are subspaces of V, it follows thatIf V is finite-dimensional, and W is a vector subspace, thenafter identifying W with its image in the second dual space under the double duality isomorphism V ≈ V∗∗.  Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.If W is a subspace of V then the quotient space V/W is a vector space in its own right, and so has a dual.  By the first isomorphism theorem, a functional f : V → F factors through V/W if and only if W is in the kernel of f.  There is thus an isomorphismAs a particular consequence, if V is a direct sum of two subspaces A and B, then V∗ is a direct sum of A0 and B0.form its local base.Here are the three most important special cases.Let 1 < p < ∞ be a real number and consider the Banach space ℓ p of all sequences a = (an) for whichis finite. Define the number q by 1/p + 1/q = 1.  Then the continuous dual of ℓ p is naturally identified with ℓ q: given an element φ ∈ (ℓ p)′, the corresponding element of ℓ q is the sequence (φ(en)) where en denotes the sequence whose n-th term is 1 and all others are zero. Conversely, given an element a = (an) ∈ ℓ q, the corresponding continuous linear functional φ on ℓ p is defined by φ(b) = ∑n anbn for all b = (bn) ∈ ℓ p (see Hölder's inequality).In a similar manner, the continuous dual of ℓ 1 is naturally identified with ℓ ∞ (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces c (consisting of all convergent sequences, with the supremum norm) and c0 (the sequences converging to zero) are both naturally identified with ℓ 1.By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra–ket notation used by physicists in the mathematical formulation of quantum mechanics.By the Riesz–Markov–Kakutani representation theorem, the continuous dual of certain spaces of continuous functions can be described using measures.If T : V → W is a continuous linear map between two topological vector spaces, then the (continuous) transpose  T′ : W′ → V′ is defined by the same formula as before:The resulting functional T′(φ) is in V′. The assignment T → T′ produces a linear map between the space of continuous linear maps from V to W and the space of linear maps from W′ to V′.  When T and U are composable continuous linear maps, thenWhen V and W are normed spaces, the norm of the transpose in L(W′, V′) is equal to that of T in L(V, W). Several properties of transposition depend upon the Hahn–Banach theorem. For example, the bounded linear map T has dense range if and only if the transpose T′ is injective.When T is a compact linear map between two Banach spaces V and W, then the transpose T′ is compact. This can be proved using the Arzelà–Ascoli theorem.When V is a Hilbert space, there is an antilinear isomorphism iV from V onto its continuous dual V′. For every bounded linear map T on V, the transpose and the adjoint operators are linked byWhen T is a continuous linear map between two topological vector spaces V and W, then the transpose T′ is continuous when W′ and V′ are equipped with"compatible" topologies: for example when, for X = V and X = W, both duals X′ have the strong topology β(X′, X) of uniform convergence on bounded sets of X, or both have the weak-∗ topology σ(X′, X) of pointwise convergence on X.  The transpose T′ is continuous from β(W′, W) to β(V′, V), or from σ(W′, W) to σ(V′, V).Assume that W is a closed linear subspace of a normed space V, and consider the annihilator of W in V′,Then, the dual of the quotient V / W  can be identified with W⊥, and the dual of W can be identified with the quotient V′ / W⊥.[14] Indeed, let P denote the canonical surjection from V onto the quotient V / W ; then, the transpose P′ is an isometric isomorphism from (V / W )′ into V′, with range equal to W⊥. If j denotes the injection map from W into V, then the kernel of the transpose j′ is the annihilator of W:and it follows from the Hahn–Banach theorem that j′ induces an isometric isomorphismV′ / W⊥ → W′.If the dual of a normed space V is separable, then so is the space V itself. The converse is not true: for example the space ℓ 1 is separable, but its dual ℓ ∞ is not.The topology of V and the topology of real or complex numbers can be used to induce on V′ a dual space topology.In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator Ψ : V → V′′ from a normed space V into its continuous double dual V′′, defined byAs a consequence of the Hahn–Banach theorem, this map is in fact an isometry, meaning ‖ Ψ(x) ‖ = ‖ x ‖ for all x in V. Normed spaces for which the map Ψ is a bijection are called reflexive.When V is a topological vector space, one can still define Ψ(x) by the same formula, for every x ∈ V, however several difficulties arise. First, when V is not locally convex, the continuous dual may be equal to {0} and the map Ψ trivial.  However, if V is Hausdorff and locally convex, the map Ψ is injective from V to the algebraic dual V′∗ of the continuous dual, again as a consequence of the Hahn–Banach theorem.[15]Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual V′, so that the continuous double dual V′′ is not uniquely defined as a set. Saying that Ψ maps from V to V′′, or in other words, that Ψ(x) is continuous on V′ for every x ∈ V, is a reasonable minimal requirement on the topology of V′, namely that the evaluation mappingsbe continuous for the chosen topology on V′. Further, there is still a choice of a topology on V′′, and continuity of Ψ depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed  case.
Vector-valued function
A vector-valued function, also referred to as a vector function, is a mathematical function of one or more variables whose range is a set of multidimensional vectors or infinite-dimensional vectors. The input of a vector-valued function could be a scalar or a vector (that is, the dimension of the domain could be 1 or greater than 1); the dimension of the domain is not defined by the dimension of the range.A common example of a vector-valued function is one that depends on a single real number parameter t, often representing time, producing a vector v(t) as the result.  In terms of the standard unit vectors i, j, k of Cartesian 3-space, these specific type of vector-valued functions are given by expressions such aswhere f(t), g(t) and h(t) are the coordinate functions of the parameter t, and the domain of this vector-valued function is the intersection of the domain of the functions f, g, and h. It can also be referred to in a different notation:The vector r(t) has its tail at the origin and its head at the coordinates evaluated by the function.In 2D, We can analogously speak about vector-valued functions asIn the linear case the function can be expressed in terms of matrices:where y is an n × 1 output vector (n > 1), x is a k × 1 vector of inputs (k ≥ 1), A is an n × k matrix of parameters, and b is an n × 1 vector of parameters.in which X (playing the role of A in the previous generic form) is an n × k matrix of fixed (empirically based) numbers.A surface is a 2-dimensional set of points embedded in 3-dimensional space. One way to represent a surface is with parametric equations, in which two parameters s and t determine the three Cartesian coordinates of any point on the surface:Here F is a vector-valued function.Many vector-valued functions, like scalar-valued functions, can be differentiated by simply differentiating the components in the Cartesian coordinate system.  Thus, ifis a vector-valued function, thenThe vector derivative admits the following physical interpretation: if r(t) represents the position of a particle, then the derivative is the velocity of the particleLikewise, the derivative of the velocity is the accelerationThe partial derivative of a vector function a with respect to a scalar variable q is defined as[1] where ai is the scalar component of a in the direction of ei.  It is also called the direction cosine of a and ei or their dot product.  The vectors e1,e2,e3 form an orthonormal basis fixed in the reference frame in which the derivative is being taken.If a is regarded as a vector function of a single scalar variable, such as time t, then the equation above reduces to the first ordinary time derivative of a with respect to t,[1]If the vector a is a function of a number n of scalar variables qr (r = 1,...,n), and each qr is only a function of time t, then the ordinary derivative of a with respect to t can be expressed, in a form known as the total derivative, as[1]Some authors prefer to use capital D to indicate the total derivative operator, as in D/Dt.  The total derivative differs from the partial time derivative in that the total derivative accounts for changes in a due to the time variance of the variables qr.Whereas for scalar-valued functions there is only a single possible reference frame, to take the derivative of a vector-valued function requires the choice of a reference frame (at least when a fixed Cartesian coordinate system is not implied as such).  Once a reference frame has been chosen, the derivative of a vector-valued function can be computed using techniques similar to those for computing derivatives of scalar-valued functions. A different choice of reference frame will, in general, produce a different derivative function. The derivative functions in different reference frames have a specific kinematical relationship.The above formulas for the derivative of a vector function rely on the assumption that the basis vectors e1,e2,e3 are constant, that is, fixed in the reference frame in which the derivative of a is being taken, and therefore the e1,e2,e3 each has a derivative of identically zero.  This often holds true for problems dealing with vector fields in a fixed coordinate system, or for simple problems in physics.  However, many complex problems involve the derivative of a vector function in multiple moving reference frames, which means that the basis vectors will not necessarily be constant.  In such a case where the basis vectors e1,e2,e3 are fixed in reference frame E, but not in reference frame N, the more general formula for the ordinary time derivative of a vector in reference frame N is[1]where the superscript N to the left of the derivative operator indicates the reference frame in which the derivative is taken.  As shown previously, the first term on the right hand side is equal to the derivative of a in the reference frame where e1,e2,e3 are constant, reference frame E.  It also can be shown that the second term on the right hand side is equal to the relative angular velocity of the two reference frames cross multiplied with the vector a itself.[1]  Thus, after substitution, the formula relating the derivative of a vector function in two reference frames is[1]where NωE is the angular velocity of the reference frame E relative to the reference frame N.One common example where this formula is used is to find the velocity of a space-borne object, such as a rocket, in the inertial reference frame using measurements of the rocket's velocity relative to the ground.  The velocity NvR in inertial reference frame N of a rocket R located at position rR can be found using the formulawhere NωE is the angular velocity of the Earth relative to the inertial frame N.  Since velocity is the derivative of position, NvR and EvR are the derivatives of rR in reference frames N and E, respectively.  By substitution, where EvR is the velocity vector of the rocket as measured from a reference frame E that is fixed to the Earth.The derivative of the products of vector functions behaves similarly to the derivative of the products of scalar functions.[2]  Specifically, in the case of scalar multiplication of a vector, if p is a scalar variable function of q,[1]In the case of dot multiplication, for two vectors a and b that are both functions of q,[1]Similarly, the derivative of the cross product of two vector functions is[1]If the values of a function f lie in an infinite-dimensional vector space X, such as a Hilbert space,then f may be called an infinite-dimensional vector function.If the argument of f is a real number and X is a Hilbert space, then the derivative of f at a point t can be defined as in the finite-dimensional case:N.B. If X is a Hilbert space, then one can easily show that any derivative (and any other limit) can be computed componentwise: ifHowever, the existence of a componentwise derivative does not guarantee the existence of a derivative, as componentwise convergence in a Hilbert space does not guarantee convergence with respect to the actual topology of the Hilbert space.Most of the above hold for other topological vector spaces X too. However, not as many classical results hold in the Banach space setting, e.g., an absolutely continuous function with values in a suitable Banach space need not have a derivative anywhere. Moreover, in most Banach spaces setting there are no orthonormal bases.
Algorithm
As an effective method, an algorithm can be expressed within a finite amount of space and time[1] and in a well-defined formal language[2] for calculating a function.[3] Starting from an initial state and initial input (perhaps empty),[4] the instructions describe a computation that, when executed, proceeds through a finite[5] number of well-defined successive states, eventually producing "output"[6] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[7] The concept of algorithm has existed for centuries. Greek mathematicians used algorithms in, for example, the sieve of Eratosthenes for finding prime numbers and the Euclidean algorithm for finding the greatest common divisor of two numbers.[8] The word algorithm itself derives from the 9th century mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized Algoritmi. A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define "effective calculability"[9] or "effective method".[10] Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.The word 'algorithm' has its roots in Latinizing the name of Muhammad ibn Musa al-Khwarizmi in a first step to algorismus.[11][12] Al-Khwārizmī (Persian: خوارزمی‎, c. 780–850) was a Persian  mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in Uzbekistan.[13][14]About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title Algoritmi de numero Indorum. This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name.[15] Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra.[16] In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word ἀριθμός 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.[17]In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English.Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins thus:Haec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.which translates as:Algorism is the art by which at present we use those Indian figures, which number two times five.The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.An informal definition could be "a set of rules that precisely defines a sequence of operations."[18] which would include all computer programs, including programs that do not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.[19]A prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word in the following quotation:No human being can write fast enough, or long enough, or small enough† ( †"smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.[20]An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can be an algebraic equation such as y = m + n – two arbitrary "input variables" m and n that produce an output y. But various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000): Minsky: "But we will also maintain, with Turing . . . that any procedure which could "naturally" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments . . . in its favor are hard to refute".[26]Gurevich: "...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine ... according to Savage [1987], an algorithm is a computational process defined by a Turing machine".[27]Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by flow of control.So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer but are often used as a way to define or document algorithms.There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see more at Turing machine).Representations of algorithms can be classed into three accepted levels of Turing machine description:[28]For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.Algorithm design refers to a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns,[29] such as the template method pattern and decorator pattern.One of the most important aspects of algorithm design is creating an algorithm that has an efficient run-time, also known as its Big O.Typical steps in the development of algorithms:Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.In computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended "target" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology."Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'"—such proof would solve the Halting problem (ibid).Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is . . . important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by the procedure. The same function may have several different algorithms".[32]Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.Computers (and computors), models of computation: A computer (or human "computor"[33]) is a restricted type of machine, a "discrete deterministic mechanical device"[34] that blindly follows its instructions.[35] Melzak's and Lambek's primitive models[36] reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters[37] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[38]Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability".[39] Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution)[40] operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1).[41] Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.[42]Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example".[43] But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.[44]This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters".[45] When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".[46] Tausworthe augments the three Böhm-Jacopini canonical structures:[47] SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.[48] An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.[49]Canonical flowchart symbols[50]: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description of English prose, as:High-level description:(Quasi-)formal description:Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:Euclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his Elements.[51] Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero. To "measure" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s.[52] In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the "modulus", the integer-fractional part left over after the division.[53]For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero).Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest.[54] While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.Only a few instruction types are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:INPUT:E0: [Ensure r ≥ s.]E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.OUTPUT:DONE:The following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by "Inelegant"; worse, "Inelegant" requires more types of instructions. The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.The following version can be used with Object Oriented languages:How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.Does an algorithm do what its author wants it to do? A few test cases usually suffice to confirm core functionality. One source[55] uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.But exceptional cases must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[56] Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.[57]Elegance (compactness) versus goodness (speed): With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[58] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.Can the algorithms be improved?: Once the programmer judges a program "fit" and "effective"—that is, it computes the function intended by its author—then the question becomes, can it be improved?The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm;[59] rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.[60]To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[61] In general, speed improvements depend on special properties of the problem, which are very common in practical applications.[62] Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.There are various ways to classify algorithms, each with its own merits.One way to classify algorithms is by implementation means.Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.Algorithms can be classified by the amount of time they need to complete compared to their input size:Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.The adjective "continuous" when applied to the word "algorithm" can mean:Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).Algorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus,[69][8]:Ch 9.2 and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[8]:Ch 9.1 Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[70]Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.The work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al-Khwarizmi (from whose name the terms "algorism" and "algorithm" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.[71]The clock: Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular, the verge escapement[72] that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine"[73] led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century.[74] Lovelace is credited with the first creation of an algorithm intended for processing on a computer – Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator – and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.Logical machines 1870—Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically . . . More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc] . . .". With this machine he could analyze a "syllogism or any other simple logical argument".[75]This machine he displayed in 1870 before the Fellows of the Royal Society.[76] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".[77]Jacquard loom, Hollerith punch cards, telegraphy and telephony—the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers.[78] By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, it's discrete and distinguishable encoding of letters as "dots and dashes" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".[79]Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):Symbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".[81]But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules".[82] The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox.[83] The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J. B. Rosser's λ-calculus[84] a finely honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene.[85] Church's proof[86] that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction.[87] Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"[88]—in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine".[89] S. C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I",[90] and a few years later Kleene's renaming his Thesis "Church's Thesis"[91] and proposing "Turing's Thesis".[92]Here is a remarkable coincidence[according to whom?] of two men not knowing each other but describing a process of men-as-computers working on computations—and they yield virtually identical definitions.Emil Post (1936) described the actions of a "computer" (human being) as follows:His symbol space would beAlan Turing's work[94] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'".[95] Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we[who?] might conjecture that all were influences.Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.[96]Turing's reduction yields the following:"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church–Turing thesis. But he did this in the following context (boldface in original):A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.
Linear algebra
Linear algebra is the branch of mathematics concerning linear equations such as linear functions such asand their representations through matrices and vector spaces.[1][2][3]Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first-order approximation.The procedure for solving simultaneous linear equations now called Gaussian elimination appears in the ancient Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. [4]Systems of linear equations arose in Europe with the introduction in 1637 by René Descartes of coordinates in geometry. In fact, in this new geometry, now called Cartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.The first systematic methods for solving linear systems used determinants, first considered by Leibniz in 1693. In 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's Rule. Later, Gauss further described the method of elimination, which was initially listed as an advancement in geodesy.[5]In 1844 Hermann Grassmann published his "Theory of Extension" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for womb. Arthur Cayley introduced matrix multiplication  and the inverse matrix in 1856, making possible the general linear group. The mechanism of group representation became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".[5]Benjamin Peirce published his Linear Associative Algebra (1872), and his son Charles Sanders Peirce extended the work later.[6]The telegraph required an explanatory system, and the 1873 publication of A Treatise on Electricity and Magnetism instituted a field theory of forces and required differential geometry for expression. Linear algebra is flat differential geometry and serves in tangent spaces to manifolds. Electromagnetic symmetries of spacetime are expressed by the Lorentz transformations, and much of the history of linear algebra is the history of Lorentz transformations.The first modern and more precise definition of a vector space was introduced by Peano in 1888;[5] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.[5]See also Determinant § History and Gaussian elimination § History.Until 19th century, linear algebra was introduced through systems of linear equations and matrices. In modern mathematics, the presentation through vector spaces is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms.  Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The axioms that addition and scalar multiplication must satisfy are the following (in the list below, u, v and w are arbitrary elements of V, and a and b are arbitrary scalars in the field F.[7]The first four axioms mean that V is an abelian group under addition.Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.Linear maps are mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear map (also called, in some contexts, linear transformation, linear mapping or linear operator) is a mapthat is compatible with addition and scalar multiplication, that isfor any vectors u,v in V and scalar a in F.This implies that for any vectors u, v in V and scalars a, b in F, one hasWhen a bijective linear map exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its range (or image) and the set of elements that are mapped to the zero vector, called the kernel of the map. All these questions can be solved by using Gaussian elimination or some variant of this algorithm.The study of subsets of vector spaces that are themselves vector spaces for the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called linear subspaces. More precisely, a linear subspace of a vector space V over a field F is a subset W of V such that u + v and au are in W, for every u, v in W, and every a in F. (These conditions suffices for implying that W is a vector space.)For example, the image of a linear map, and the inverse image of 0 by a linear map (called kernel or null space) are linear subspaces.Another important way of forming a subspace is to consider linear combinations of a set S of vectors: the set of all sums where v1, v2, ..., vk are in V, and a1, a2, ..., ak are in F form a linear subspace called the span of S. The span of S is also the intersection of all linear subspaces containing S. In other words, it is the (smallest for the inclusion relation) linear subspace containing S.Any two bases of a vector space V have the same cardinality, which is called the dimension of V; this is the dimension theorem for vector spaces. Moreover, two vector spaces over the same field F are isomorphic if and only if they have the same dimension.[8]If any basis of V (and therefore every basis) has a finite number of elements, V is a finite-dimensional vector space. If U is a subspace of V, then dim U ≤ dim V. In the case where V is finite-dimensional, the equality of the dimensions implies U = V.If U1 and U2 are subspaces of V, thenMatrices allow explicit manipulation of finite-dimensional vector spaces and linear maps. Their theory is thus an essential part of linear algebra.Let V be a finite-dimensional vector space over a field F, and (v1, v2, ..., vm) be a basis of V (thus m is the dimension of V). By definition of a basis, the mapfor j = 1, ..., n, then f is represented by the matrixwith m rows and n columns.Matrix multiplication is defined in such a way that the product of two matrices is the matrix of the composition of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing exactly the same concepts.Two matrices that encode the same linear transformation in different bases are called similar. Equivalently, two matrices are similar if one can transform one in the other by elementary row and column operations. For a matrix representing a linear map from W to V, the row operations correspond to change of bases in V and the column operations correspond to change of bases in W. Every matrix is similar to an identity matrix possibly bordered by zero rows and zero columns. In terms of vector space, this means that, for any linear map from W to V, there are bases such a part of the basis of W is mapped bijectively on a part of the basis of V, and that the remaining basis elements of W, if any, are mapped to zero (this is a way of expressing the fundamental theorem of linear algebra). Gaussian elimination is the basic algorithm for finding these elementary operations, and proving this theorem.Systems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory has been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.For example, letbe a linear system.To such a system, one may associate its matrix and its right member vectorLet T be the linear transformation associated to the matrix M. A solution of the system (S) is a vector such that that is an element of the preimage of v by T.Let (S') be the associated homogeneous system, where the right-hand sides of the equations are put to zero. The solutions of (S') are exactly the elements of the kernel of T or, equivalently, M.The Gaussian-elimination consists of performing elementary row operations on the augmented matrixfor putting it in reduced row echelon form. These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is showing that the system (S) has the unique solutionIt follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the ranks, kernels, matrix inverses.A linear endomorphism is a linear map that maps a vector space V to itself. If V has a basis of n elements, such an endomorphism is represented by a square matrix of size n.With respect to general linear maps, linear endomorphisms and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including geometric transformations, coordinate changes, quadratic forms, and many other part of mathematics.The determinant of a square matrix is a polynomial function of the entries of the matrix, such that the matrix is invertible if and only if the determinant is not zero. This results from the fact that the determinant of a product of matrices is the product of the determinants, and thus that a matrix is invertible if and only if its determinant is invertible.Cramer's rule is a closed-form expression, in terms of determinants, of the solution of a system of n linear equations in n unknowns. Cramer's rule is useful for reasoning about the solution, but, except for n = 2 or 3, it is rarely used for computing a solution, since Gaussian elimination is a faster algorithm.The determinant of an endomorphism is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense, since this determinant is independent of the choice of the basis.If f is a linear endomorphism of a vector space V over a field F, an eigenvector of f is a nonzero vector v of V such that f(v) = av for some scalar a in F. This scalar a is an eigenvalue of f.If the dimension of V is finite, and a basis has been chosen, f and v may be represented, respectively, by a square matrix M and a column matrix and z; the equation defining eigenvectors and eigenvalues becomesUsing the identity matrix I, whose all entries are zero, except those of the main diagonal, which are equal to one, this may be rewrittenIf V is of dimension n, this is a monic polynomial of degree n, called the characteristic polynomial of the matrix (or of the endomorphism), and there are, at most, n eigenvalues.If a basis exists that consists only of eigenvectors, the matrix of f on this basis has a very simple structure: it is a diagonal matrix such that the entries on the main diagonal are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said diagonalizable. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after extending the field of scalars. In this extended sense, if the characteristic polynomial is square-free, then the matrix is diagonalizable.A symmetric matrix is always diagonalizable. There are non-diagonizable matrices, the simplest being(it cannot be diagonalizable since its square is the zero matrix, and the square of a nonzero diagonal matrix is never zero).When an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The Frobenius normal form does not need of extending the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The Jordan normal form requires to extend the field of scalar for containing all eigenvalues, and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.For v in V, the mapThere is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the bra–ket notationfor denoting f (x).Let be a linear map. For every linear form h on W, the composite function f ∘ h is a linear form on V. This defines a linear mapbetween the dual spaces, which is called the dual or the transpose of f.If elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in bra–ket notation by For highlighting this symmetry, the two members of this equality are sometimes written Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a mapthat satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[10][11]Note that in R, it is symmetric.We can define the length of a vector v in V byand we can prove the Cauchy–Schwarz inequality:In particular, the quantityand so we can call this quantity the cosine of the angle between the two vectors.The inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfyingIf T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.There is a strong relationship between linear algebra and geometry, which started with the introduction by René Descartes, in 1637, of Cartesian coordinates. In this new (at that time) geometry, now called Cartesian geometry, points are represented by Cartesian coordinates, which are sequences of three real numbers (in the case of the usual three-dimensional space). The basic objects of geometry, which are lines and planes are represented by linear equations. Thus, computing intersections of lines and planes amounts solving systems of linear equations. This was one of the main motivations for developing linear algebra.Most geometric transformation, such as translations, rotations, reflections, rigid motions, isometries, and projections transform lines into lines. It follows that they can be defined, specified and studied in terms of linear maps. This is also the case of homographies and Möbius transformations, when considered as transformations of a projective space. Until the end of 19th century, geometric spaces were defined by axioms relating points, lines and planes (synthetic geometry). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, Projective space and Affine space) It has been shown that the two approaches are essentially equivalent.[12] In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including finite fields. Presently, most textbooks, introduce geometric spaces from linear algebra, and geometry is often presented, at elementary level, as a subfield of linear algebra.Linear algebra is used in almost all areas of mathematics, and therefore in almost all scientific domains that use mathematics. These applications may be divided into several wide categories.The modeling of our ambient space is based on geometry. Sciences concerned with this space use geometry widely. This is the case with mechanics and robotics, for describing rigid body dynamics; geodesy for describing Earth shape; perspectivity, computer vision, and computer graphics, for describing the relationship between a scene and its plane representation; and many other scientific domains.In all these applications, synthetic geometry is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with coordinates. This requires the heavy use of linear algebra.Functional analysis studies function spaces. These are vector spaces with additional structure, such as Hilbert spaces. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, quantum mechanics (wave functions).Most physical phenomena are modeled by partial differential equations. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting cells. For linear systems this interaction involves linear functions. For nonlinear systems, this interaction is often approximated by linear functions.[13] In both cases, very large matrices are generally involved. Weather forecasting is a typical example, where the whole Earth atmosphere is divided in cells of, say, 100 km of width and 100 m of height.Nearly all scientific computations involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. BLAS and LAPACK are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, for adapting them to the specificities of the computer (cache size, number of available cores, ...).Some processors, typically graphics processing units (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra.This section presents several related topics that do not appear generally in elementary textbooks on linear algebra, but are commonly considered, in advanced mathematics, as parts of linear algebra.The existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a ring R, and this gives a structure called module over R, or R-module.The concepts of linear independence, span, basis, and linear maps (also called module homomorphisms) are defined for modules exactly as for vector spaces, with the essential difference that, if R is not a field, there are modules that do not have any basis. The modules that have a basis are the free modules, and those that are spanned by a finite set are the finitely generated modules. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that determinants exist only if the ring is commutative, and that a square matrix over a commutative ring is invertible only if its determinant has a multiplicative inverse in the ring.Vector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a cokernel of a homomorphism of free modules.Modules over the integers can be identified with abelian groups, since the multiplication by an integer may identified to a repeated addition. Most of the theory of abelian groups may be extended to modules over a principal ideal domain. In particular, over a principal ideal domain, every submodule of a free module is free, and the fundamental theorem of finitely generated abelian groups may be extended straightforwardly to finitely generated modules over a principal ring.There are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a computational complexity that is much higher than the similar algorithms over a field. For more details, see Linear equation over a ring.In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V∗ consisting of linear maps f: V → F where F is the field of scalars. Multilinear maps T: Vn → F can be described via tensor products of elements of V∗.If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V × V → V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces.
Bra–ket notation
In quantum mechanics, bra–ket notation is a standard notation for describing quantum states. It can also be used to denote abstract vectors and linear functionals in mathematics. The notation uses angle brackets (the ⟨ and ⟩ symbols) and a vertical bar (the | symbol), to denote the scalar product of vectors or the action of a linear functional on a vector in a complex vector space.  The scalar product or action is written asThe right part is called the ket /kɛt/; it is a vector, typically represented as a column vector and written The left part is called the bra, /brɑː/; it is the Hermitian conjugate of the ket with the same label, typically represented as a row vector and is writtenA combination of bras, kets, and operators is interpreted using matrix multiplication. A bra and a ket with the same label are Hermitian conjugates of each other.Bra-ket notation was introduced in 1939 by Paul Dirac[1][2] and is also known as the Dirac notation.  Bra–ket notation is a notation for linear algebra, particularly focused on vectors, inner products, linear operators, Hermitian conjugation, and the dual space, for both finite-dimensional and infinite-dimensional complex vector spaces. It is specifically designed to ease the types of calculations that frequently come up in quantum mechanics.Its use in quantum mechanics is quite widespread. Many phenomena that are explained using quantum mechanics are usually explained using bra–ket notation.In simple cases, a ket |m⟩ can be described as a column vector, a bra with the same label ⟨m| is its conjugate transpose (which is a row vector), and writing bras, kets, and linear operators next to each other implies matrix multiplication.[4] However, kets may also exist in uncountably-infinite-dimensional vector spaces, such that they cannot be literally written as a column vector. Also, writing a column vector as a list of numbers requires picking a basis, whereas one can write "|m⟩" without committing to any particular basis. This is helpful because quantum mechanics calculations involve frequently switching between different bases (e.g. position basis, momentum basis, energy eigenbasis, etc.), so it is better to have the basis vectors (if any) written out explicitly.  In some situations involving two important basis vectors they will be referred to simply as "|-⟩" and "|+⟩".The standard mathematical notation for the inner product, preferred as well by some physicists, expresses exactly the same thing as the bra–ket notation,Bras and kets can also be configured in other ways, such as the outer productwhich can also be represented as a matrix multiplication (i.e., a column vector times a row vector equals a matrix).If the ket is an element of a vector space, the bra is technically an element of its dual space—see Riesz representation theorem.In mathematics, the term "vector" is used to refer generally to any element of any vector space. In physics, however, the term "vector" is much more specific: "Vector" refers almost exclusively to quantities like displacement or velocity, which have three components that relate directly to the three dimensions of the real world. Such vectors are typically denoted with over arrows (r→) or boldface (r).In quantum mechanics, a quantum state is typically represented as an element of an abstract complex vector space—for example the infinite-dimensional vector space of all possible wavefunctions (functions mapping each point of 3D space to a complex number). Since the term "vector" is already used for something else (see previous paragraph), it is very common to refer to these elements of abstract complex vector spaces as "kets", and to write them using ket notation.Ket notation, invented by Dirac, uses vertical bars and angular brackets: |A⟩. When this notation is used, these quantities are called "kets", and |A⟩ is read as "ket-A".[5] These kets can be manipulated using the usual rules of linear algebra, for example:Note how any symbols, letters, numbers, or even words—whatever serves as a convenient label—can be used as the label inside a ket. For example, the last line above involves infinitely many different kets, one for each real number x. In other words, the symbol "|A⟩" has a specific and universal mathematical meaning, while just the "A" by itself does not. For example, |1⟩ + |2⟩ might or might not be equal to |3⟩. Nevertheless, for convenience, there is usually some logical scheme behind the labels inside kets, such as the common practice of labeling energy eigenkets in quantum mechanics through a listing of their quantum numbers.An inner product is a generalization of the dot product. The inner product of two vectors is a scalar. In neutral notation (notation dedicated to the inner product only), this might be written (A, B), where A and B are elements of the abstract vector space, i.e. both are kets.Bra–ket notation uses a specific notation for inner products:Bra–ket notation splits this inner product (also called a "bracket") into two pieces, the "bra" and the "ket":where ⟨A| is called a bra, read as "bra-A", and |B⟩ is a ket as above.The purpose of "splitting" the inner product into a bra and a ket is that both the bra ⟨A| and the ket |B⟩ are meaningful on their own, and can be used in other contexts besides within an inner product. There are two main ways to think about the meanings of separate bras and kets. Accordingly, the interpretation of the expression ⟨A|B⟩ has a second interpretation, namely that of the action of a linear functional per below.For a finite-dimensional vector space, using a fixed orthonormal basis, the inner product can be written as a matrix multiplication of a row vector with a column vector:Based on this, the bras and kets can be defined as:and then it is understood that a bra next to a ket implies matrix multiplication.The conjugate transpose (also called Hermitian conjugate) of a bra is the corresponding ket and vice versa:because if one starts with the brathen performs a complex conjugation, and then a matrix transpose, one ends up with the ketA more abstract definition, which is equivalent but more easily generalized to infinite-dimensional spaces, is to say that bras are linear functionals on the space of kets, i.e. linear transformations that input a ket and output a complex number. The bra linear functionals are defined to be consistent with the inner product. Thus, if ⟨A| is the linear functional corresponding to |A⟩ under the Riesz representation theorem, theni.e. it produces the same complex number as the inner product does. The terminology for the right hand side is though not inner product, which always involves two kets. Confusing this is harmless, since the same number is produced in the end.In mathematics terminology, the vector space of bras is the dual space to the vector space of kets, and corresponding bras and kets are related by the Riesz representation theorem.Bra–ket notation can be used even if the vector space is not a Hilbert space.In quantum mechanics, it is common practice to write down kets which have infinite norm, i.e. non-normalizable wavefunctions. Examples include states whose wavefunctions are Dirac delta functions or infinite plane waves. These do not, technically, belong to the Hilbert space itself. However, the definition of "Hilbert space" can be broadened to accommodate these states (see the Gelfand–Naimark–Segal construction or rigged Hilbert spaces). The bra–ket notation continues to work in an analogous way in this broader context.Banach spaces are a different generalization of Hilbert spaces. In a Banach space B, the vectors may be notated by kets and the continuous linear functionals by bras. Over any vector space without topology, we may also notate the vectors by kets and the linear functionals by bras. In these more general contexts, the bracket does not have the meaning of an inner product, because the Riesz representation theorem does not apply.The mathematical structure of quantum mechanics is based in large part on linear algebra:Since virtually every calculation in quantum mechanics involves vectors and linear operators, it can involve, and often does involve, bra–ket notation. A few examples follow:Starting from any ket |Ψ⟩ in this Hilbert space,  one may define a complex scalar function of r, known as a wavefunction,On the left-hand side, Ψ(r) is a function mapping any point in space to a complex number; on the right-hand side, |Ψ⟩ = ∫ d3r Ψ(r) |r⟩ is a ket consisting of a superposition of kets with relative coefficients specified by that function.It is then customary to define linear operators acting on wavefunctions in terms of linear operators acting on kets, byFor instance, the momentum operator p has the following form,One occasionally encounters an expression such asthough this is something of an abuse of notation. The differential operator must be understood to be an abstract operator, acting on kets, that has the effect of differentiating wavefunctions once the expression is projected into the position basis,even though, in the momentum basis, the operator amounts to a mere multiplication operator (by iħp).In quantum mechanics the expression ⟨φ|ψ⟩ is typically interpreted as the probability amplitude for the state ψ to collapse into the state φ. Mathematically, this means the coefficient for the projection of ψ onto φ.  It is also described as the projection of state ψ onto state φ.A stationary spin-1/2 particle has a two-dimensional Hilbert space. One orthonormal basis is:where |↑z⟩ is the state with a definite value of the spin operator Sz equal to +1/2 and  |↓z⟩ is the state with a definite value of the spin operator Sz equal to −1/2.Since these are a basis, any quantum state of the particle can be expressed as a linear combination (i.e., quantum superposition) of these two states:where aψ and bψ are complex numbers.A different basis for the same Hilbert space is:defined in terms of Sx rather than Sz.Again, any state of the particle can be expressed as a linear combination of these two:In vector form, you might writedepending on which basis you are using. In other words, the "coordinates" of a vector depend on the basis used.There is a mathematical relationship between aψ, bψ, cψ and dψ; see change of basis.There are a few conventions and abuses of notation that are generally accepted by the physics community, but which might confuse the non-initiated.It is common to use the same symbol for labels and constants in the same equation. For example, α̂ |α⟩ = α |α⟩, where the symbol α is used simultaneously as the name of the operator α̂, its eigenvector |α⟩ and the associated eigenvalue α.Something similar occurs in component notation of vectors. While Ψ (uppercase) is traditionally associated with wavefunctions, ψ (lowercase) may be used to denote a label, a wave function or complex constant in the same context, usually differentiated only by a subscript.The main abuses are including operations inside the vector labels. This is done for a fast notation of scaling vectors. E.g. if the vector |α⟩ is scaled by √2, it might be denoted by |α/√2⟩, which makes no sense since α is a label, not a function or a number, so you can't perform operations on it.This is especially common when denoting vectors as tensor products, where part of the labels are moved outside the designed slot, e.g. |α⟩ = |α/√21⟩ ⊗ |α/√22⟩. Here part of the labeling that should state that all three vectors are different was moved outside the kets, as subscripts 1 and 2. And a further abuse occurs, since α is meant to refer to the norm of the first vector—which is a label denoting a value.A linear operator is a map that inputs a ket and outputs a ket. (In order to be called "linear", it is required to have certain properties.) In other words, if A is a linear operator and |ψ⟩ is a ket, then A|ψ⟩ is another ket.In an N-dimensional Hilbert space, |ψ⟩ can be written as an N × 1 column vector, and then A is an N × N matrix with complex entries. The ket A|ψ⟩ can be computed by normal matrix multiplication.Linear operators are ubiquitous in the theory of quantum mechanics. For example, observable physical quantities are represented by self-adjoint operators, such as energy or momentum, whereas transformative processes are represented by unitary linear operators such as rotation or the progression of time.Operators can also be viewed as acting on bras from the right hand side. Specifically, if A is a linear operator and ⟨φ| is a bra, then ⟨φ|A is another bra defined by the rule(in other words, a function composition). This expression is commonly written as (cf. energy inner product)In an N-dimensional Hilbert space, ⟨φ| can be written as a 1 × N row vector, and A (as in the previous section) is an N × N matrix. Then the bra ⟨φ|A can be computed by normal matrix multiplication.If the same state vector appears on both bra and ket side,then this expression gives the expectation value, or mean or average value, of the observable represented by operator A for the physical system in the state |ψ⟩.A convenient way to define linear operators on a Hilbert space H is given by the outer product: if ⟨ϕ| is a bra and |ψ⟩ is a ket, the outer productdenotes the rank-one operator with the rule For a finite-dimensional vector space, the outer product can be understood as simple matrix multiplication:The outer product is an N × N matrix, as expected for a linear operator.One of the uses of the outer product is to construct projection operators. Given a ket |ψ⟩ of norm 1, the orthogonal projection onto the subspace spanned by |ψ⟩ isJust as kets and bras can be transformed into each other (making |ψ⟩ into ⟨ψ|), the element from the dual space corresponding to A|ψ⟩ is ⟨ψ|A†,  where A† denotes the Hermitian conjugate (or adjoint) of the operator A. In other words,If A is expressed as an N × N matrix, then A† is its conjugate transpose.Self-adjoint operators, where A = A†, play an important role in quantum mechanics; for example, an observable is always described by a self-adjoint operator. If A is a self-adjoint operator, then ⟨ψ|A|ψ⟩ is always a real number (not complex). This implies that expectation values of observables are real.Bra–ket notation was designed to facilitate the formal manipulation of linear-algebraic expressions. Some of the properties that allow this manipulation are listed herein. In what follows, c1 and c2 denote arbitrary complex numbers, c* denotes the complex conjugate of c, A and B denote arbitrary linear operators, and these properties are to hold for any choice of bras and kets.Given any expression involving complex numbers, bras, kets, inner products, outer products, and/or linear operators (but not addition), written in bra–ket notation, the parenthetical groupings do not matter (i.e., the associative property holds). For example:and so forth. The expressions on the right (with no parentheses whatsoever) are allowed to be written unambiguously because of the equalities on the left. Note that the associative property does not hold for expressions that include nonlinear operators, such as the antilinear time reversal operator in physics.Bra–ket notation makes it particularly easy to compute the Hermitian conjugate (also called dagger, and denoted †) of expressions. The formal rules are:These rules are sufficient to formally write the Hermitian conjugate of any such expression; some examples are as follows:Two Hilbert spaces V and W may form a third space V ⊗ W by a tensor product. In quantum mechanics, this is used for describing composite systems. If a system is composed of two subsystems described in V and W respectively, then the Hilbert space of the entire system is the tensor product of the two spaces. (The exception to this is if the subsystems are actually identical particles. In that case, the situation is a little more complicated.)If |ψ⟩ is a ket in V and |φ⟩ is a ket in W, the direct product of the two kets is a ket in V ⊗ W. This is written in various notations:See quantum entanglement and the EPR paradox for applications of this product.Consider a complete orthonormal system (basis),for a Hilbert space H, with respect to the norm from an inner product ⟨·,·⟩. From basic functional analysis, it is  known that any ket |ψ⟩ can also be written aswith ⟨·|·⟩ the inner product on the Hilbert space.From the commutativity of kets with (complex) scalars, it follows thatmust be the identity operator, which sends each vector to itself. This, then,  can be inserted in any expression without affecting its value; for examplewhere, in the last identity, the Einstein summation convention has been used.In quantum mechanics, it often occurs that little or no information about the inner product ⟨ψ|φ⟩ of two arbitrary (state) kets is present, while it is still possible to say something about the expansion coefficients ⟨ψ|ei⟩ = ⟨ei|ψ⟩* and ⟨ei|φ⟩ of those vectors with respect to a specific (orthonormalized) basis. In this case, it is particularly useful to insert the unit operator into the bracket one time or more.For more information, see Resolution of the identity, Since ⟨x′|x⟩ = δ(x − x′), plane waves follow, ⟨x|p⟩ = eixp/ħ/√2πħ.[7]Typically, when all matrix elements of an operator such as are available,this resolution serves to reconstitute the full operator,The object physicists are considering when using bra–ket notation is a Hilbert space (a complete inner product space).Let H be a Hilbert space and h ∈ H a vector in H. What physicists would denote by |h⟩ is the vector itself. That is,Let H* be the dual space of H. This is the space of linear functionals on H. The isomorphism Φ : H → H* is defined by Φ(h) = φh, where for every g ∈ H we definewhere IP(·,·), (·,·), ⟨·,·⟩ and ⟨·|·⟩ are just different notations for expressing an inner product between two elements in a Hilbert space (or for the first three, in any inner product space). Notational confusion arises when identifying φh and g with ⟨h| and |g⟩ respectively. This is because of literal symbolic substitutions. Let φh = H = ⟨h| and let g = G = |g⟩. This givesOne ignores the parentheses and removes the double bars. Some properties of this notation are convenient since we are dealing with linear operators and composition acts like a ring multiplication.Moreover, mathematicians usually write the dual entity not at the first place, as the physicists do, but at the second one, and they usually use not an asterisk but an overline (which the physicists reserve for averages and the Dirac spinor adjoint) to denote complex conjugate numbers; i.e., for scalar products mathematicians usually writewhereas physicists would write for the same quantity
Dot product
In mathematics, the dot product or scalar product[note 1] is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number.  In Euclidean geometry, the dot product of the Cartesian coordinates of two vectors is widely used and often called "the" inner product (or rarely projection product) of Euclidean space even though it is not the only inner product that can be defined on Euclidean space; see also inner product space.Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. These definitions are equivalent when using Cartesian coordinates. In modern geometry, Euclidean spaces are often defined by using vector spaces. In this case, the dot product is used for defining lengths (the length of a vector is the square root of the dot product of the vector by itself) and angles (the cosine of the angle of two vectors is the quotient of their dot product by the product of their lengths).The name "dot product" is derived from the centered dot " · " that is often used to designate this operation; the alternative name "scalar product" emphasizes that the result is a scalar, rather than a vector, as is the case for the vector product in three-dimensional space.The dot product may be defined algebraically or geometrically.  The geometric definition is based on the notions of angle and distance (magnitude of vectors). The equivalence of these two definitions relies on having a Cartesian coordinate system for Euclidean space.In modern presentations of Euclidean geometry, the points of space are defined in terms of their Cartesian coordinates, and Euclidean space itself is commonly identified with the real coordinate space Rn. In such a presentation, the notions of length and angles are defined by means of the dot product. The length of a vector is defined as the square root of the dot product of the vector by itself, and the cosine of the (non oriented) angle of two vectors of length one is defined as their dot product.  So the equivalence of the two definitions of the dot product is a part of the equivalence of the classical and the modern formulations of Euclidean geometry.The dot product of two vectors a  = [a1, a2, …, an] and b = [b1, b2, …, bn] is defined as:[1]where Σ denotes summation and n is the dimension of the vector space. For instance, in three-dimensional space, the dot product of vectors [1, 3, −5] and [4, −2, −1] is:If vectors are identified with row matrices, the dot product can also be written as a matrix productExpressing  the above example in this way, a 1 × 3 matrix (row vector) is multiplied by a 3 × 1 matrix (column vector) to get a 1 × 1 matrix that is identified with its unique entry: where θ is the angle between a and b.At the other extreme, if they are codirectional, then the angle between them is 0° andThis implies that the dot product of a vector a with itself iswhich givesthe formula for the Euclidean length of the vector.The scalar projection (or scalar component) of a Euclidean vector a in the direction of a Euclidean vector b is given bywhere θ is the angle between a and b.In terms of the geometric definition of the dot product, this can be rewrittenThe dot product is thus characterized geometrically by[4]The dot product, defined in this manner, is homogeneous under scaling in each variable, meaning that for any scalar α,It also satisfies a distributive law, meaning thatIf e1, ..., en are the standard basis vectors in Rn, then we may writeThe vectors ei are an orthonormal basis, which means that they have unit length and are at right angles to each other.  Hence since these vectors have unit lengthand since they form right angles with each other, if i ≠ j,Thus in general we can say that:Where δ  ij  is the Kronecker delta.Also, by the geometric definition, for any vector ei and a vector a, we notewhere ai is the component of vector a in the direction of ei.Now applying the distributivity of the geometric version of the dot product giveswhich is precisely the algebraic definition of the dot product.  So the geometric dot product equals the algebraic dot product.The dot product fulfills the following properties if a, b, and c are real vectors and r is a scalar.[1][2]Given two vectors a and b separated by angle θ (see image right), they form a triangle with a third side c = a − b. The dot product of this with itself is:which is the law of cosines.There are two ternary operations involving dot product and cross product.The scalar triple product of three vectors is defined asIts value is the determinant of the matrix whose columns are the Cartesian coordinates of the three vectors. It is the signed volume of the parallelogram defined by the three vectors.The vector triple product is defined by[1][2]This identity, also known as Lagrange's formula may be remembered as "BAC minus CAB", keeping in mind which vectors are dotted together. This formula finds application in simplifying vector calculations in physics.In physics, vector magnitude is a scalar in the physical sense, i.e. a physical quantity independent of the coordinate system, expressed as the product  of a numerical value and a physical unit, not just a number. The dot product is also a scalar in this sense, given by the formula, independent of the coordinate system.  Examples include:[8][9]For vectors with complex entries, using the given definition of the dot product would lead to quite different properties. For instance the dot product of a vector with itself would be an arbitrary complex number, and could be zero without the vector being the zero vector (such vectors are called isotropic); this in turn would have consequences for notions like length and angle. Properties such as the positive-definite norm can be salvaged at the cost of giving up the symmetric and bilinear properties of the scalar product, through the alternative definition[10][1]where ai is the complex conjugate of ai. Then the scalar product of any vector with itself is a non-negative real number, and it is nonzero except for the zero vector. However this scalar product is thus sesquilinear rather than bilinear: it is conjugate linear and not linear in a, and the scalar product is not symmetric, sinceThe angle between two complex vectors is then given byThis type of scalar product is nevertheless useful, and leads to the notions of Hermitian form and of general inner product spaces.The inner product of two vectors over the field of complex numbers is, in general, a complex number, and  is sesquilinear instead of bilinear. An inner product space is a normed vector space, and the inner product of a vector with itself is real and positive-definite.The dot product is defined for vectors that have a finite number of entries. Thus these vectors can be regarded as discrete functions: a length-n vector u is, then, a function with domain {k ∈ ℕ ∣ 1 ≤ k ≤ n}, and ui is a notation for the image of i by the function/vector u.This notion can be generalized to continuous functions: just as the inner product on vectors uses a sum over corresponding components, the inner product on functions is defined as an integral over some interval a ≤ x ≤ b (also denoted [a, b]):[1]Generalized further to complex functions ψ(x) and χ(x), by analogy with the complex inner product above, gives[1]Inner products can have a weight function, i.e. a function which weights each term of the inner product with a value.Matrices have the Frobenius inner product, which is analogous to the vector inner product. It is defined as the sum of the products of the corresponding components of two matrices A and B having the same size:Dyadics have a dot product and "double" dot product defined on them, see Dyadics (Product of dyadic and dyadic) for their definitions.The inner product between a tensor of order n and a tensor of order m is a tensor of order n + m − 2, see tensor contraction for details.The straightforward algorithm for calculating a floating-point dot product of vectors can suffer from catastrophic cancellation. To avoid this, approaches such as the Kahan summation algorithm are used.A dot product function is included in BLAS level 1.
Cartesian tensor
In geometry and linear algebra, a Cartesian tensor uses an orthonormal basis to represent a tensor in a Euclidean space in the form of components. Converting a tensor's components from one such basis to another is through an orthogonal transformation.The most familiar coordinate systems are the two-dimensional and three-dimensional Cartesian coordinate systems. Cartesian tensors may be used with any Euclidean space, or more technically, any finite-dimensional vector space over the field of real numbers that has an inner product.Use of Cartesian tensors occurs in physics and engineering, such as with the Cauchy stress tensor and the moment of inertia tensor in rigid body dynamics. Sometimes general curvilinear coordinates are convenient, as in high-deformation continuum mechanics, or even necessary, as in general relativity. While orthonormal bases may be found for some such coordinate systems (e.g. tangent to spherical coordinates), Cartesian tensors may provide considerable simplification for applications in which rotations of rectilinear coordinate axes suffice. The transformation is a passive transformation, since the coordinates are changed and not the physical system.In 3d Euclidean space, ℝ3, the standard basis is ex, ey, ez. Each basis vector points along the x-, y-, and z-axes, and the vectors are all unit vectors (or normalized), so the basis is orthonormal.Throughout, when referring to Cartesian coordinates in three dimensions, a right-handed system is assumed and this is much more common than a left-handed system in practice, see orientation (vector space) for details.simiFor Cartesian tensors of order 1, a Cartesian vector a can be written algebraically as a linear combination of the basis vectors ex, ey, ez:where the coordinates of the vector with respect to the Cartesian basis are denoted ax, ay, az. It is common and helpful to display the basis vectors as column vectorswhen we have a coordinate vector in a column vector representation:A row vector representation is also legitimate, although in the context of general curvilinear coordinate systems the row and column vector representations are used separately for specific reasons – see Einstein notation and covariance and contravariance of vectors for why.The term "component" of a vector is ambiguous: it could refer to:A more general notation is tensor index notation, which has the flexibility of numerical values rather than fixed coordinate labels. The Cartesian labels are replaced by tensor indices in the basis vectors ex ↦ e1, ey ↦ e2, ez ↦ e3 and coordinates Ax ↦ A1, Ay ↦ A2, Az ↦ A3. In general, the notation e1, e2, e3 refers to any basis, and A1, A2, A3 refers to the corresponding coordinate system; although here they are restricted to the Cartesian system. Then:It is standard to use the Einstein notation—the summation sign for summation over an index that is present exactly twice within a term may be suppressed for notational conciseness:An advantage of the index notation over coordinate-specific notations is the independence of the dimension of the underlying vector space, i.e. the same expression on the right hand side takes the same form in higher dimensions (see below). Previously, the Cartesian labels x, y, z were just labels and not indices. (It is informal to say "i = x, y, z").A dyadic tensor T is an order 2 tensor formed by the tensor product ⊗ of two Cartesian vectors a and b, written T = a ⊗ b. Analogous to vectors, it can be written as a linear combination of the tensor basis ex ⊗ ex ≡ exx, ex ⊗ ey ≡ exy, ..., ez ⊗ ez ≡ ezz (the right hand side of each identity is only an abbreviation, nothing more):Representing each basis tensor as a matrix:then T can be represented more systematically as a matrix:See matrix multiplication for the notational correspondence between matrices and the dot and tensor products.More generally, whether or not T is a tensor product of two vectors, it is always a linear combination of the basis tensors with coordinates Txx, Txy, ... Tzz:while in terms of tensor indices:and in matrix form:Second order tensors occur naturally in physics and engineering when physical quantities have directional dependence in the system, often in a "stimulus-response" way. This can be mathematically seen through one aspect of tensors - they are multilinear functions. A second order tensor T which takes in a vector u of some magnitude and direction will return a vector v; of a different magnitude and in a different direction to u, in general. The notation used for functions in mathematical analysis leads us to write v = T(u),[1] while the same idea can be expressed in matrix and index notations[2] (including the summation convention), respectively:By "linear", if u = ρr + σs for two scalars ρ and σ and vectors r and s, then in function and index notations:and similarly for the matrix notation. The function, matrix, and index notations all mean the same thing. The matrix forms provide a clear display of the components, while the index form allows easier tensor-algebraic manipulation of the formulae in a compact manner. Both provide the physical interpretation of directions; vectors have one direction, while second order tensors connect two directions together. One can associate a tensor index or coordinate label with a basis vector direction.The use of second order tensors are the minimum to describe changes in magnitudes and directions of vectors, as the dot product of two vectors is always a scalar, while the cross product of two vectors is always a pseudovector perpendicular to the plane defined by the vectors, so these products of vectors alone cannot obtain a new vector of any magnitude in any direction. (See also below for more on the dot and cross products). The tensor product of two vectors is a second order tensor, although this has no obvious directional interpretation by itself.The previous idea can be continued: if T takes in two vectors p and q, it will return a scalar r. In function notation we write r = T(p, q), while in matrix and index notations (including the summation convention) respectively:The tensor T is linear in both input vectors. When vectors and tensors are written without reference to components, and indices are not used, sometimes a dot · is placed where summations over indices (known as tensor contractions) are taken. For the above cases:[1][2]motivated by the dot product notation:More generally, a tensor of order m which takes in n vectors (where n is between 0 and m inclusive) will return a tensor of order m − n, see Tensor: As multilinear maps for further generalizations and details. The concepts above also apply to pseudovectors in the same way as for vectors. The vectors and tensors themselves can vary within throughout space, in which case we have vector fields and tensor fields, and can also depend on time.Following are some examples:For the electrical conduction example, the index and matrix notations would be:while for the rotational kinetic energy T:See also constitutive equation for more specialized examples.In n-dimensional Euclidean space over the real numbers, ℝn, the standard basis is denoted e1, e2, e3, ... en. Each basis vector ei points along the positive xi axis, with the basis being orthonormal. Component j of ei is given by the Kronecker delta:A vector in ℝn takes the form:Similarly for the order 2 tensor above, for each vector a and b in ℝn:or more generally:The position vector x in ℝn is a simple and common example of a vector, and can be represented in any coordinate system. Consider the case of rectangular coordinate systems with orthonormal bases only. It is possible to have a coordinate system with rectangular geometry if the basis vectors are all mutually perpendicular and not normalized, in which case the basis is orthogonal but not orthonormal. However, orthonormal bases are easier to manipulate and are often used in practice. The following results are true for orthonormal bases, not orthogonal ones.In one rectangular coordinate system, x as a contravector has coordinates xi and basis vectors ei, while as a covector it has coordinates xi and basis covectors ei, and we have:In another rectangular coordinate system, x as a contravector has coordinates xi and bases ei, while as a covector it has coordinates xi and bases ei, and we have:Each new coordinate is a function of all the old ones, and vice versa for the inverse function:and similarly each new basis vector is a function of all the old ones, and vice versa for the inverse function:for all i, j.A vector is invariant under any change of basis, so if coordinates transform according to a transformation matrix L, the bases transform according to the matrix inverse L−1, and conversely if the coordinates transform according to inverse L−1, the bases transform according to the matrix L. The difference between each of these transformations is shown conventionally through the indices as superscripts for contravariance and subscripts for covariance, and the coordinates and bases are linearly transformed according to the following rules:where Lij represents the entries of the transformation matrix (row number is i and column number is j) and (L−1)ik denotes the entries of the inverse matrix of the matrix Lik.If L is an orthogonal transformation (orthogonal matrix), the objects transforming by it are defined as Cartesian tensors. This geometrically has the interpretation that a rectangular coordinate system is mapped to another rectangular coordinate system, in which the norm of the vector x is preserved (and distances are preserved).The determinant of L is det(L) = ±1, which corresponds to two types of orthogonal transformation: (+1) for rotations and (−1) for improper rotations (including reflections).There are considerable algebraic simplifications, the matrix transpose is the inverse from the definition of an orthogonal transformation:From the previous table, orthogonal transformations of covectors and contravectors are identical. There is no need to differ between raising and lowering indices, and in this context and applications to physics and engineering the indices are usually all subscripted to remove confusion for exponents. All indices will be lowered in the remainder of this article. One can determine the actual raised and lowered indices by considering which quantities are covectors or contravectors, and the relevant transformation rules.Exactly the same transformation rules apply to any vector a, not only the position vector. If its components ai do not transform according to the rules, a is not a vector.Despite the similarity between the expressions above, for the change of coordinates such as xj = Lijxi, and the action of a tensor on a vector like bi = Tijaj, L is not a tensor, but T is. In the change of coordinates, L is a matrix, used to relate two rectangular coordinate systems with orthonormal bases together. For the tensor relating a vector to a vector, the vectors and tensors throughout the equation all belong to the same coordinate system and basis.The entries of L are partial derivatives of the new or old coordinates with respect to the old or new coordinates, respectively.Differentiating xi with respect to xk:sois an element of the Jacobian matrix. There is a (partially mnemonical) correspondence between index positions attached to L and in the partial derivative: i at the top and j at the bottom, in each case, although for Cartesian tensors the indices can be lowered.Conversely, differentiating xj with respect to xi:sois an element of the inverse Jacobian matrix, with a similar index correspondence.Many sources state transformations in terms of the partial derivatives:and the explicit matrix equations in 3d are:similarly forAs with all linear transformations, L depends on the basis chosen. For two orthonormal basesHence the components reduce to direction cosines between the xi and xj axes:where θij and θji are the angles between the xi and xj axes. In general, θij is not equal to θji, because for example θ12 and θ21 are two different angles.The transformation of coordinates can be written:and the explicit matrix equations in 3d are:similarly forThe geometric interpretation is the xi components equal to the sum of projecting the xj components onto the xj axes.The numbers ei⋅ej arranged into a matrix would form a symmetric matrix (a matrix equal to its own transpose) due to the symmetry in the dot products, in fact it is the metric tensor g. By contrast ei⋅ej or ei⋅ej do not form symmetric matrices in general, as displayed above. Therefore, while the L matrices are still orthogonal, they are not symmetric.Apart from a rotation about any one axis, in which the xi and xi for some i coincide, the angles are not the same as Euler angles, and so the L matrices are not the same as the rotation matrices.The dot product and cross product occur very frequently, in applications of vector analysis to physics and engineering, examples include:How these products transform under orthogonal transformations is illustrated below.The dot product ⋅ of each possible pairing of the basis vectors follows from the basis being orthonormal. For perpendicular pairs we havewhile for parallel pairs we haveReplacing Cartesian labels by index notation as shown above, these results can be summarized bywhere δij are the components of the Kronecker delta. The Cartesian basis can be used to represent δ in this way.In addition, each metric tensor component gij with respect to any basis is the dot product of a pairing of basis vectors:For the Cartesian basis the components arranged into a matrix are:so are the simplest possible for the metric tensor, namely the δ:This is not true for general bases: orthogonal coordinates have diagonal metrics containing various scale factors (i.e. not necessarily 1), while general curvilinear coordinates could also have nonzero entries for off-diagonal components.The dot product of two vectors a and b transforms according towhich is intuitive, since the dot product of two vectors is a single scalar independent of any coordinates. This also applies more generally to any coordinate systems, not just rectangular ones; the dot product in one coordinate system is the same in any other.For the cross product × of two vectors, the results are (almost) the other way round. Again, assuming a right-handed 3d Cartesian coordinate system, cyclic permutations in perpendicular directions yield the next vector in the cyclic collection of vectors:while parallel vectors clearly vanish:and replacing Cartesian labels by index notation as above, these can be summarized by:where i, j, k are indices which take values 1, 2, 3. It follows that:These permutation relations and their corresponding values are important, and there is an object coinciding with this property: the Levi-Civita symbol, denoted by ε. The Levi-Civita symbol entries can be represented by the Cartesian basis:which geometrically corresponds to the volume of a cube spanned by the orthonormal basis vectors, with sign indicating orientation (and not a "positive or negative volume"). Here, the orientation is fixed by ε123 = +1, for a right-handed system. A left-handed system would fix ε123 = −1 or equivalently ε321 = +1.The scalar triple product can now be written:with the geometric interpretation of volume (of the parallelepiped spanned by a, b, c) and algebraically is a determinant:[3]This in turn can be used to rewrite the cross product of two vectors as follows:Contrary to its appearance, the Levi-Civita symbol is not a tensor, but a pseudotensor, the components transform according to:Therefore, the transformation of the cross product of a and b is:and so a × b transforms as a pseudovector, because of the determinant factor.The tensor index notation applies to any object which has entities that form multidimensional arrays – not everything with indices is a tensor by default. Instead, tensors are defined by how their coordinates and basis elements change under a transformation from one coordinate system to another.Note the cross product of two vectors is a pseudovector, while the cross product of a pseudovector with a vector is another vector.Other identities can be formed from the δ tensor and ε pseudotensor, a notable and very useful identity is one that converts two Levi-Civita symbols adjacently contracted over two indices into an antisymmetrized combination of Kronecker deltas:The index forms of the dot and cross products, together with this identity, greatly facilitate the manipulation and derivation of other identities in vector calculus and algebra, which in turn are used extensively in physics and engineering. For instance, it is clear the dot and cross products are distributive over vector addition:without resort to any geometric constructions - the derivation in each case is a quick line of algebra. Although the procedure is less obvious, the vector triple product can also be derived. Rewriting in index notation:and because cyclic permutations of indices in the ε symbol does not change its value, cyclically permuting indices in εkℓm to obtain εℓmk allows us to use the above δ-ε identity to convert the ε symbols into δ tensors:thusly:Note this is antisymmetric in b and c, as expected from the left hand side. Similarly, via index notation or even just cyclically relabelling a, b, and c in the previous result and taking the negative:and the difference in results show that the cross product is not associative. More complex identities, like quadruple products;and so on, can be derived in a similar manner.Tensors are defined as quantities which transform in a certain way under linear transformations of coordinates.Text below contradics introduced above contravariant and covariant vectors (tensors)Let a = aiei and b = biei be two vectors, so that they transform according to aj = aiLij, bj = biLij.Taking the tensor product gives:then applying the transformation to the componentsand to the basesgives the transformation law of an order-2 tensor. The tensor a⊗b is invariant under this transformation:More generally, for any order-2 tensorthe components transform according to;and the basis transforms by:If R does not transform according to this rule - whatever quantity R may be, it's not an order 2 tensor.More generally, for any order p tensorthe components transform according to;and the basis transforms by:For a pseudotensor S of order p, the components transform according to;The antisymmetric nature of the cross product can be recast into a tensorial form as follows.[4] Let c be a vector, a be a pseudovector, b be another vector, and T be a second order tensor such that:As the cross product is linear in a and b, the components of T can be found by inspection, and they are:so the pseudovector a can be written as an antisymmetric tensor. This transforms as a tensor, not a pseudotensor. For the mechanical example above for the tangential velocity of a rigid body, given by v = ω × x, this can be rewritten as v = Ω · x where Ω is the tensor corresponding to the pseudovector ω:For an example in electromagnetism, while the electric field E is a vector field, the magnetic field B is a pseudovector field. These fields are defined from the Lorentz force for a particle of electric charge q traveling at velocity v:and considering the second term containing the cross product of a pseudovector B and velocity vector v, it can be written in matrix form, with F, E, and v as column vectors and B as an antisymmetric matrix:If a pseudovector is explicitly given by a cross product of two vectors (as opposed to entering the cross product with another vector), then such pseudovectors can also be written as antisymmetric tensors of second order, with each entry a component of the cross product. The angular momentum of a classical pointlike particle orbiting about an axis, defined by J = x × p, is another example of a pseudovector, with corresponding antisymmetric tensor:Although Cartesian tensors do not occur in the theory of relativity; the tensor form of orbital angular momentum J enters the spacelike part of the relativistic angular momentum tensor, and the above tensor form of the magnetic field B enters the spacelike part of the electromagnetic tensor.It should be emphasized the following formulae are only so simple in Cartesian coordinates - in general curvilinear coordinates there are factors of the metric and its determinant - see tensors in curvilinear coordinates for more general analysis.Following are the differential operators of vector calculus. Throughout, left Φ(r, t) be a scalar field, andbe vector fields, in which all scalar and vector fields are functions of the position vector r and time t.The gradient operator in Cartesian coordinates is given by:and in index notation, this is usually abbreviated in various ways:This operator acts on a scalar field Φ to obtain the vector field directed in the maximum rate of increase of Φ:The index notation for the dot and cross products carries over to the differential operators of vector calculus.[5]The directional derivative of a scalar field Φ is the rate of change of Φ along some direction vector a (not necessarily a unit vector), formed out of the components of a and the gradient:The divergence of a vector field A is:Note the interchange of the components of the gradient and vector field yields a different differential operatorwhich could act on scalar or vector fields. In fact, if A is replaced by the velocity field u(r, t) of a fluid, this is a term in the material derivative (with many other names) of continuum mechanics, with another term being the partial time derivative:which usually acts on the velocity field leading to the non-linearity in the Navier-Stokes equations.As for the curl of a vector field A, this can be defined as a pseudovector field by means of the ε symbol:which is only valid in three dimensions, or an antisymmetric tensor field of second order via antisymmetrization of indices, indicated by delimiting the antisymmetrized indices by square brackets (see Ricci calculus):which is valid in any number of dimensions. In each case, the order of the gradient and vector field components should not be interchanged as this would result in a different differential operator:which could act on scalar or vector fields.Finally, the Laplacian operator is defined in two ways, the divergence of the gradient of a scalar field Φ:or the square of the gradient operator, which acts on a scalar field Φ or a vector field A:In physics and engineering, the gradient, divergence, curl, and Laplacian operator arise inevitably in fluid mechanics, Newtonian gravitation, electromagnetism, heat conduction, and even quantum mechanics.Vector calculus identities can be derived in a similar way to those of vector dot and cross products and combinations. For example, in three dimensions, the curl of a cross product of two vector fields A and B:where the product rule was used, and throughout the differential operator was not interchanged with A or B. Thus:One can continue the operations on tensors of higher order. Let T = T(r, t) denote a second order tensor field, again dependent on the position vector r and time t.For instance, the gradient of a vector field in two equivalent notations ("dyadic" and "tensor", respectively) is:which is a tensor field of second order.The divergence of a tensor is:which is a vector field. This arises in continuum mechanics in Cauchy's laws of motion - the divergence of the Cauchy stress tensor σ is a vector field, related to body forces acting on the fluid.Cartesian tensors are as in tensor algebra, but Euclidean structure of and restriction of the basis brings some simplifications compared to the general theory.The general tensor algebra consists of general mixed tensors of type (p, q):with basis elements:the components transform according to:as for the bases:For Cartesian tensors, only the order p + q of the tensor matters in a Euclidean space with an orthonormal basis, and all p + q indices can be lowered.  A Cartesian basis does not exist unless the vector space has a positive-definite metric, and thus cannot be used in relativistic contexts.Dyadic tensors were historically the first approach to formulating second-order tensors, similarly triadic tensors for third-order tensors, and so on. Cartesian tensors use tensor index notation, in which the variance may be glossed over and is often ignored, since the components remain unchanged by raising and lowering indices.
Shanks transformation
In numerical analysis, the Shanks transformation is a non-linear series acceleration method to increase the rate of convergence of a sequence. This method is named after Daniel Shanks, who rediscovered this sequence transformation in 1955. It was first derived and published by R. Schmidt in 1941.[1]Milton D. Van Dyke (1975) Perturbation methods in fluid mechanics, p. 202.As an example, consider the slowly convergent series[3]The generalized kth-order Shanks transformation is given as the ratio of the determinants:[4]The generalized Shanks transformation is closely related to Padé approximants and Padé tables.[4]
