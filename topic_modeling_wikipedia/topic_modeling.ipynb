{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n",
      "694\n",
      "\n",
      "\n",
      "616\n",
      "612\n"
     ]
    }
   ],
   "source": [
    "# Splitting text data and storing them in a list (of articles)\n",
    "import io\n",
    "docs = io.open(\"raw_data.txt\", mode=\"r\", encoding=\"utf-8\", errors=\"ignore\").read().split('\\n') # list of strings \n",
    "titles = [docs[i] for i in range(len(docs)) if i % 2 == 0] # list of string titles\n",
    "contents = [docs[i] for i in range(len(docs)) if i % 2 == 1] # list of string contents\n",
    "print(len(titles))\n",
    "print(len(contents))\n",
    "print('\\n')\n",
    "print(len(list(set(titles))))\n",
    "print(len(list(set(contents))))\n",
    "\n",
    "# import collections\n",
    "# print(len([item for item, count in collections.Counter(contents).items() if count > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing/ cleaning the data\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# remove text between parenthesis\n",
    "# contents = list(map(lambda x: re.sub(r\"\\(.*\\)\",\"\",x), contents))\n",
    "\n",
    "# remove all digits from text\n",
    "contents = list(map(lambda x: re.sub(r\"\\d+\",\"\",x), contents))\n",
    "\n",
    "stop = set(stopwords.words('english')) # set of stopwords\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    # remove stopwords and words that are too short\n",
    "    return [lemma.lemmatize(i, 'v') for i in word_tokenize(doc) if i not in stop and len(i) > 2]\n",
    "cleaned = [clean(page.lower()) for page in contents]\n",
    "\n",
    "print(len(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(16197 unique tokens: ['-tuples', '.the', 'abbreviation', 'above.according', 'above.this']...)\n",
      "Dictionary(4085 unique tokens: ['.the', 'abbreviation', 'abstraction', 'abuse', 'act']...)\n",
      "Dictionary(4050 unique tokens: ['.the', 'abbreviation', 'abstraction', 'abuse', 'act']...)\n",
      "Dictionary(4000 unique tokens: ['.the', 'abbreviation', 'abstraction', 'abuse', 'act']...)\n"
     ]
    }
   ],
   "source": [
    "# Building word dicitonary\n",
    "from gensim import corpora\n",
    "# create the term dictionary of our corpus; terms are unique; each term is assigned an index\n",
    "dictionary = corpora.Dictionary(cleaned)\n",
    "print(dictionary)\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.7)\n",
    "print(dictionary)\n",
    "stoplist = set('also use make people know many call include part find become like mean often different usually take wikt come give well get since type list say change see refer actually iii aisne kinds pas ask would way something need things want every str'.split())\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "dictionary.filter_tokens(stop_ids)\n",
    "print(dictionary)\n",
    "dictionary.filter_n_most_frequent(50)\n",
    "print(dictionary)\n",
    "\n",
    "# This saves the dictionary to the local disk\n",
    "dictionary.save_as_text('./dictionary.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n",
      "[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 2), (10, 2), (11, 2), (12, 1), (13, 5), (14, 1), (15, 5), (16, 3), (17, 2), (18, 2), (19, 1), (20, 1), (21, 2), (22, 4), (23, 2), (24, 1), (25, 4), (26, 1), (27, 1), (28, 5), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 5), (37, 1), (38, 2), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 5), (46, 2), (47, 1), (48, 1), (49, 1), (50, 1), (51, 6), (52, 1), (53, 1), (54, 1), (55, 20), (56, 1), (57, 1), (58, 1), (59, 1), (60, 3), (61, 5), (62, 1), (63, 1), (64, 2), (65, 2), (66, 1), (67, 1), (68, 1), (69, 1), (70, 4), (71, 2), (72, 7), (73, 1), (74, 3), (75, 2), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 2), (85, 1), (86, 1), (87, 2), (88, 1), (89, 1), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 2), (97, 5), (98, 3), (99, 1), (100, 1), (101, 1), (102, 1), (103, 38), (104, 5), (105, 3), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1), (113, 12), (114, 1), (115, 1), (116, 2), (117, 2), (118, 1), (119, 1), (120, 1), (121, 2), (122, 1), (123, 2), (124, 1), (125, 2), (126, 2), (127, 1), (128, 1), (129, 1), (130, 1), (131, 4), (132, 8), (133, 2), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 3), (142, 1), (143, 3), (144, 1), (145, 1), (146, 1), (147, 1), (148, 3), (149, 1), (150, 2), (151, 3), (152, 1), (153, 9), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 4), (167, 8), (168, 2), (169, 2), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 2), (178, 4), (179, 8), (180, 1), (181, 1), (182, 8), (183, 1), (184, 1), (185, 2), (186, 7), (187, 3), (188, 1), (189, 1), (190, 5), (191, 1), (192, 1), (193, 1), (194, 2), (195, 1), (196, 2), (197, 1), (198, 2), (199, 1), (200, 1), (201, 1), (202, 1), (203, 4), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 9), (212, 1), (213, 3), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 2), (220, 1), (221, 1), (222, 2), (223, 1), (224, 2), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 10), (231, 1), (232, 1), (233, 1), (234, 2), (235, 1), (236, 1), (237, 2), (238, 3), (239, 2), (240, 2), (241, 1), (242, 1), (243, 2), (244, 3), (245, 1), (246, 13), (247, 2), (248, 1), (249, 1), (250, 1), (251, 3), (252, 1), (253, 2), (254, 1), (255, 1), (256, 2), (257, 1), (258, 1), (259, 1), (260, 1), (261, 2), (262, 1), (263, 1), (264, 3), (265, 4), (266, 1), (267, 1), (268, 2), (269, 1), (270, 2), (271, 3), (272, 1), (273, 4), (274, 3), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 3), (282, 7), (283, 2), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 2), (291, 1), (292, 1), (293, 1), (294, 2), (295, 2), (296, 1), (297, 1), (298, 2), (299, 1), (300, 2), (301, 1), (302, 1), (303, 1), (304, 3), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 4), (311, 1), (312, 1), (313, 3), (314, 1), (315, 1), (316, 1), (317, 1), (318, 2), (319, 4), (320, 1), (321, 1), (322, 2), (323, 3), (324, 1), (325, 4), (326, 1), (327, 1), (328, 1), (329, 1), (330, 3), (331, 1), (332, 1), (333, 3), (334, 1), (335, 1), (336, 3), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 3), (343, 1), (344, 4), (345, 1), (346, 1), (347, 1), (348, 1), (349, 2), (350, 6), (351, 1), (352, 1), (353, 1), (354, 1), (355, 1), (356, 1), (357, 3), (358, 2), (359, 1), (360, 1), (361, 2), (362, 1), (363, 1), (364, 1), (365, 1), (366, 2), (367, 1), (368, 1), (369, 1), (370, 4), (371, 2), (372, 7), (373, 5), (374, 1), (375, 1), (376, 1), (377, 3), (378, 1), (379, 1), (380, 2), (381, 1), (382, 2), (383, 3), (384, 4), (385, 2), (386, 1), (387, 5), (388, 1), (389, 1), (390, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Creating document-term matrix from vocabulary (dictionary)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in cleaned]\n",
    "print(len(doc_term_matrix))\n",
    "print(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.015*\"group\" + 0.012*\"line\" + 0.009*\"geometry\" + 0.008*\"coordinate\" + 0.008*\"manifold\" + 0.006*\"euclidean\" + 0.005*\"determinant\" + 0.005*\"tensor\" + 0.005*\"plane\" + 0.005*\"equation\" \n",
      "\n",
      "2   0.007*\"ring\" + 0.006*\"affine\" + 0.006*\"system\" + 0.006*\"cross\" + 0.005*\"group\" + 0.005*\"coordinate\" + 0.005*\"time\" + 0.005*\"map\" + 0.004*\"row\" + 0.004*\"solution\" \n",
      "\n",
      "3   0.007*\"group\" + 0.006*\"inequality\" + 0.006*\"sequence\" + 0.006*\"ring\" + 0.006*\"method\" + 0.005*\"finite\" + 0.005*\"polynomial\" + 0.004*\"determinant\" + 0.004*\"methods\" + 0.004*\"element\" \n",
      "\n",
      "4   0.015*\"group\" + 0.008*\"map\" + 0.006*\"system\" + 0.006*\"equations\" + 0.006*\"ring\" + 0.006*\"kernel\" + 0.005*\"hilbert\" + 0.005*\"element\" + 0.005*\"row\" + 0.005*\"equation\" \n",
      "\n",
      "5   0.016*\"ring\" + 0.006*\"map\" + 0.006*\"system\" + 0.006*\"dual\" + 0.005*\"equations\" + 0.005*\"hilbert\" + 0.004*\"element\" + 0.004*\"transform\" + 0.004*\"continuous\" + 0.004*\"coordinate\" \n",
      "\n",
      "6   0.011*\"map\" + 0.005*\"element\" + 0.005*\"hilbert\" + 0.005*\"group\" + 0.005*\"equations\" + 0.005*\"operator\" + 0.005*\"finite\" + 0.005*\"system\" + 0.004*\"coordinate\" + 0.004*\"independent\" \n",
      "\n",
      "7   0.008*\"row\" + 0.006*\"map\" + 0.006*\"equation\" + 0.005*\"line\" + 0.005*\"system\" + 0.005*\"determinant\" + 0.005*\"ring\" + 0.005*\"hilbert\" + 0.005*\"time\" + 0.005*\"leibniz\" \n",
      "\n",
      "8   0.007*\"equations\" + 0.007*\"projective\" + 0.007*\"row\" + 0.006*\"coordinate\" + 0.006*\"system\" + 0.006*\"line\" + 0.006*\"group\" + 0.005*\"map\" + 0.005*\"column\" + 0.005*\"jordan\" \n",
      "\n",
      "9   0.011*\"determinant\" + 0.007*\"group\" + 0.006*\"sequence\" + 0.005*\"system\" + 0.005*\"equations\" + 0.005*\"algorithm\" + 0.004*\"method\" + 0.004*\"compute\" + 0.004*\"finite\" + 0.004*\"map\" \n",
      "\n",
      "10   0.009*\"polynomial\" + 0.007*\"root\" + 0.006*\"system\" + 0.005*\"map\" + 0.005*\"hilbert\" + 0.005*\"equations\" + 0.004*\"finite\" + 0.004*\"translation\" + 0.004*\"ring\" + 0.004*\"element\" \n",
      "\n",
      "11   0.009*\"row\" + 0.007*\"ring\" + 0.006*\"quadratic\" + 0.005*\"equation\" + 0.005*\"group\" + 0.005*\"equations\" + 0.005*\"normal\" + 0.004*\"line\" + 0.004*\"projective\" + 0.004*\"coefficients\" \n",
      "\n",
      "12   0.010*\"span\" + 0.008*\"equations\" + 0.008*\"row\" + 0.006*\"affine\" + 0.006*\"transformation\" + 0.006*\"dual\" + 0.006*\"solution\" + 0.005*\"coordinate\" + 0.005*\"method\" + 0.005*\"line\" \n",
      "\n",
      "13   0.008*\"group\" + 0.006*\"quantum\" + 0.005*\"hilbert\" + 0.005*\"sequence\" + 0.005*\"system\" + 0.004*\"state\" + 0.004*\"determinant\" + 0.004*\"coordinate\" + 0.004*\"orthogonal\" + 0.004*\"map\" \n",
      "\n",
      "14   0.018*\"polynomial\" + 0.008*\"polynomials\" + 0.007*\"ring\" + 0.005*\"degree\" + 0.005*\"geometry\" + 0.005*\"system\" + 0.004*\"equation\" + 0.004*\"coefficients\" + 0.004*\"algebraic\" + 0.004*\"group\" \n",
      "\n",
      "15   0.007*\"row\" + 0.007*\"map\" + 0.005*\"operator\" + 0.005*\"plane\" + 0.005*\"frame\" + 0.005*\"norm\" + 0.004*\"rank\" + 0.004*\"jordan\" + 0.004*\"column\" + 0.004*\"tensor\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training LDA model\n",
    "# LDA automatically finds the mixture of similar words together, thus forming the topic or theme. we use this \n",
    "# unsupervised learning technique to identify the categories to which these articles belong, and the groups/clusters\n",
    "# within the collection. \n",
    "\n",
    "from gensim.models.ldamodel import LdaModel as Lda\n",
    "\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=15, id2word = dictionary)\n",
    "\n",
    "# Showing the 15 identified topics after the model is trained, where top 10 key terms are listed for each topic\n",
    "for topic in ldamodel.print_topics(num_topics=15, num_words=10):\n",
    "    print(topic[0]+1, \" \", topic[1],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles(ID) in Cluster 1: 1, 10, 16, 37, 39, 61, 66, 67, 69, 94, 95, 113, 121, 133, 135, 154, 157, 158, 180, 195\n",
      "\n",
      "Articles(ID) in Cluster 2: 2, 12, 17, 30, 86, 92, 102, 103, 106, 117, 150, 156, 190\n",
      "\n",
      "Articles(ID) in Cluster 3: 9, 46, 75, 78, 91, 107, 178, 181, 185\n",
      "\n",
      "Articles(ID) in Cluster 4: 5, 24, 29, 32, 33, 48, 49, 50, 56, 58, 60, 62, 63, 64, 65, 68, 71, 73, 79, 81, 87, 90, 98, 114, 118, 126, 141, 142, 144, 152, 153, 170, 171, 177, 189, 192\n",
      "\n",
      "Articles(ID) in Cluster 5: 3, 47, 74, 112, 120, 174, 183\n",
      "\n",
      "Articles(ID) in Cluster 6: 0, 8, 14, 31, 42, 45, 59, 76, 85, 97, 108, 129, 139, 166, 182, 193\n",
      "\n",
      "Articles(ID) in Cluster 7: 15, 23, 35, 53, 88, 105, 140, 147\n",
      "\n",
      "Articles(ID) in Cluster 8: 25, 40, 82, 101, 115, 122, 132, 137, 159, 161, 179\n",
      "\n",
      "Articles(ID) in Cluster 9: 4, 7, 11, 18, 22, 41, 54, 93, 96, 128, 131, 163, 173, 176, 191\n",
      "\n",
      "Articles(ID) in Cluster 10: 34, 44, 77, 100, 138\n",
      "\n",
      "Articles(ID) in Cluster 11: 19, 20, 26, 51, 84, 89, 99, 111, 124, 146, 149\n",
      "\n",
      "Articles(ID) in Cluster 12: 6, 13, 28, 36, 38, 55, 70, 72, 104, 130, 136, 145, 160, 167, 169, 172, 187, 188, 196\n",
      "\n",
      "Articles(ID) in Cluster 13: 21, 57, 83, 109, 110, 116, 123, 125, 127, 134, 148, 162, 164, 186\n",
      "\n",
      "Articles(ID) in Cluster 14: 43, 151, 155, 184\n",
      "\n",
      "Articles(ID) in Cluster 15: 27, 52, 80, 119, 143, 168, 175, 194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clustering documents based on topics extracted from LDA model \n",
    "from operator import itemgetter\n",
    "def cluster(doc_term_matrix, num):\n",
    "    doc_topics = ldamodel.get_document_topics(doc_term_matrix, minimum_probability=0.20)\n",
    "    result = [[] for i in range(num)]\n",
    "    for k,topic in enumerate(doc_topics):\n",
    "        # Some articles do not have a topic\n",
    "        if topic:\n",
    "            topic.sort(key = itemgetter(1), reverse=True)\n",
    "            result[topic[0][0]].append(k)\n",
    "    for k in range(len(result)):\n",
    "        print('Articles(ID) in Cluster ' + str(k+1) + ': ' + ', '.join(map(str, result[k])))\n",
    "        print()\n",
    "    return result\n",
    "cluster_result = cluster(doc_term_matrix, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles in Cluster 1: Quadruple product, Norm (mathematics), Homogeneous coordinates, Trace (linear algebra), Real number, Flat (geometry), Semi-simple operator, Synthetic geometry, Endomorphism, Levi-Civita symbol, Line (geometry), Matrix calculus, Manifold, Abelian group, Diagonalizable matrix, Unit vector, Geometry, Three-dimensional space, Möbius transformation, Cartesian tensor\n",
      "\n",
      "Articles in Cluster 2: MATLAB, Compressed sensing, Euclidean vector, Cross product, Field extension, Pseudovector, Seven-dimensional cross product, Row and column vectors, Lorentz transformation, René Descartes, Linear inequality, Weyl's inequality, Vector-valued function\n",
      "\n",
      "Articles in Cluster 3: Finite field, Sequence, Numerical analysis, Triangle inequality, Cauchy–Schwarz inequality, The Nine Chapters on the Mathematical Art, Cauchy–Schwarz inequality, Runge–Kutta methods, Non-negative matrix factorization\n",
      "\n",
      "Articles in Cluster 4: Butcher group, Generalizations of Pauli matrices, Vector space, Skew-Hermitian matrix, General linear group, Module homomorphism, Linear complementarity problem, Identity matrix, Linear function, Translation of axes, Free module, Kernel (linear algebra), Kernel (algebra), Hilbert space, Matrix analysis, Dual space, Symmetric matrix, Singular value decomposition, Matrix multiplication, Matrix addition, Invertible matrix, Kernel (linear algebra), General linear group, Field (mathematics), Algebra over a field, Spectral theorem, Complex conjugate, Hilbert space, Diagonal matrix, Rotation of axes, Invertible matrix, Kempner series, Generalized eigenvector, Canonical basis, Dual space, Linear algebra\n",
      "\n",
      "Articles in Cluster 5: Multi-core processor, Principal ideal domain, Linear form, Linear form, Commutative ring, Sesquilinear form, Ring (mathematics)\n",
      "\n",
      "Articles in Cluster 6: Function (mathematics), Basis (linear algebra), Woodbury matrix identity, Dimension theorem for vector spaces, Bra–ket notation, Triple product, Skew-Hamiltonian matrix, Isomorphism, Linear map, Rank factorization, Linear map, Tensor product, Function composition, Transpose of a linear map, Basis (linear algebra), Bra–ket notation\n",
      "\n",
      "Articles in Cluster 7: Linear approximation, Remez algorithm, Reflection (mathematics), Benjamin Peirce, Rank (linear algebra), Material point method, James Joseph Sylvester, Simpson's rule\n",
      "\n",
      "Articles in Cluster 8: Signal-flow graph, System of linear equations, System of linear equations, Jordan normal form, Jordan normal form, De Casteljau's algorithm, System of linear equations, Homography, Fundamental matrix (computer vision), Projective space, Defective matrix\n",
      "\n",
      "Articles in Cluster 9: Aitken's delta-squared process, Monte Carlo method, Gottfried Wilhelm Leibniz, Linear programming, Tensor operator, Horner's method, Orthant, Determinant, Permanent (mathematics), Translation, Overlap–save method, Riemann solver, Computing the permanent, Determinant, Algorithm\n",
      "\n",
      "Articles in Cluster 10: Wilkinson's polynomial, Binary operation, Relative change and difference, Bernstein polynomial, Square-free polynomial\n",
      "\n",
      "Articles in Cluster 11: Discrete wavelet transform, Savitzky–Golay filter, Identifiability analysis, Discrete Fourier transform, Gaussian elimination, Quadratic form, Pointwise, Normal matrix, Perspectivity, Gaussian elimination, Quadratic form\n",
      "\n",
      "Articles in Cluster 12: Vectorization (mathematics), Plane (geometry), Partial differential equation, Row and column spaces, Affine space, Linear subspace, Levinson recursion, Multigrid method, Mathematical analysis, Convex cone, Dual basis, Linear span, Affine space, Linear span, Dual basis, Predictor–corrector method, Linear subspace, Linear equation, Shanks transformation\n",
      "\n",
      "Articles in Cluster 13: Matrix (mathematics), Mathematics, Padé table, Range (mathematics), Quantum mechanics, Set (mathematics), Wave function, Projection (linear algebra), Quaternion, Multiplicative inverse, Spectral theory, Orthogonality, Lp space, Pohlke's theorem\n",
      "\n",
      "Articles in Cluster 14: Eigenvalues and eigenvectors, Majorization, Polynomial, Chebyshev nodes\n",
      "\n",
      "Articles in Cluster 15: Dimension (vector space), Shear matrix, Dual norm, Frame (linear algebra), Matrix norm, Dimension (vector space), Complex plane, Dot product\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the exact document titles in each cluster\n",
    "for k in range(len(cluster_result)):\n",
    "    print('Articles in Cluster ' + str(k+1) + ': ' + ', '.join(map(lambda x: titles[x], cluster_result[k])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.00070352375), (1, 0.0013643978), (2, 0.00025655841), (3, 0.00050678279), (4, 0.00044210179), (5, 0.00096805347), (6, 0.00038708164), (7, 0.0001171499), (8, 0.00076638913), (9, 0.00036176638), (10, 0.0002326205), (11, 0.0015267399), (12, 0.00048151685), (13, 0.00038440333), (14, 0.00032594844)]\n"
     ]
    }
   ],
   "source": [
    "term_topics = ldamodel.get_term_topics('convex', minimum_probability=0.000001)\n",
    "print(term_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Top 7 articles related to convex -------\n",
      "Convex cone \n",
      " 0.996011 \n",
      "\n",
      "Linear span \n",
      " 0.995534 \n",
      "\n",
      "Linear span \n",
      " 0.995534 \n",
      "\n",
      "Dual basis \n",
      " 0.990476 \n",
      "\n",
      "Dual basis \n",
      " 0.990476 \n",
      "\n",
      "Vectorization (mathematics) \n",
      " 0.990378 \n",
      "\n",
      "Predictor–corrector method \n",
      " 0.989513 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting related documents based on a term \n",
    "def get_related_documents(term, top, doc_term_matrix):\n",
    "    print('------- Top', top, 'articles related to',term,'-------')\n",
    "    related_docs = []\n",
    "    doc_topics = ldamodel.get_document_topics(doc_term_matrix, minimum_probability=0.20)\n",
    "    term_topics = ldamodel.get_term_topics(term, minimum_probability=0.000001)\n",
    "    term_topics.sort(key = itemgetter(1), reverse=True)\n",
    "    for k,topic in enumerate(doc_topics):\n",
    "        if topic:\n",
    "            topic.sort(key = itemgetter(1), reverse=True)\n",
    "            if topic[0][0] == term_topics[0][0]:\n",
    "                related_docs.append((k,topic[0][1]))\n",
    "    related_docs.sort(key = itemgetter(1), reverse=True)\n",
    "    result = []\n",
    "    for j,doc in enumerate(related_docs):\n",
    "        print(titles[doc[0]],\"\\n\",doc[1],\"\\n\")   \n",
    "        result.append(titles[doc[0]])\n",
    "        if j == top - 1:\n",
    "            break\n",
    "related_docs = get_related_documents('convex', 7, doc_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Absolutely convex set' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0267897c0da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcluster_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_theme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Absolutely convex set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-0267897c0da4>\u001b[0m in \u001b[0;36mget_theme\u001b[0;34m(doc, cluster_result)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_theme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document not found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'Absolutely convex set' is not in list"
     ]
    }
   ],
   "source": [
    "def get_theme(doc, cluster_result):\n",
    "    doc_id = titles.index(doc)\n",
    "    if doc_id == -1:\n",
    "        print('Document not found.')\n",
    "        return\n",
    "    for i, cluster in enumerate(cluster_result):\n",
    "        if doc_id in cluster:\n",
    "            return i+1\n",
    "    return 0\n",
    "cluster_num = get_theme('Absolutely convex set', cluster_result)\n",
    "print(cluster_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=197, num_nnz=77881)\n",
      "(0, 0.01622820264288196)\n"
     ]
    }
   ],
   "source": [
    "# Implementing tf-idf model; the only information needed from the previous part is the doc_term_matrix\n",
    "from gensim.models import TfidfModel, LsiModel\n",
    "tfidf_model = TfidfModel(doc_term_matrix, dictionary = dictionary)\n",
    "print(tfidf_model)\n",
    "vector = tfidf_model[doc_term_matrix[0]]\n",
    "print(vector[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsiModel(num_terms=4000, num_topics=200, decay=1.0, chunksize=20000)\n"
     ]
    }
   ],
   "source": [
    "# Implementing LSI model; the only information needed from the previous part is the doc_term_matrix\n",
    "lsi_model = LsiModel(doc_term_matrix, id2word=dictionary)\n",
    "print(lsi_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-70c2e0d8d368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m693\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 694 * 694 matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Creating the similarity matrix from simple bag-of-words model (# of documents * # of documents)\n",
    "from gensim import similarities\n",
    "\n",
    "index = similarities.MatrixSimilarity(doc_term_matrix, num_features=len(dictionary))\n",
    "print(len(index[doc_term_matrix[693]])) # 694 * 694 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training tf-idf model from bag-of-word dataset\n",
    "model_tfidf = TfidfModel(doc_term_matrix, id2word=dictionary, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying tf-idf model to all vectors\n",
    "from gensim.corpora import MmCorpus\n",
    "MmCorpus.serialize('./corpus_tfidf.mm', model_tfidf[doc_term_matrix], progress_cnt=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixSimilarity<694 docs, 6533 features>\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = MmCorpus('./corpus_tfidf.mm') # Loading back the corpus file after applying tf-idf\n",
    "model_lsi = LsiModel(corpus_tfidf, num_topics=15, id2word=dictionary)\n",
    "# Applying LSI model to all vectors\n",
    "index = similarities.MatrixSimilarity(model_lsi[corpus_tfidf], num_features=len(dictionary))\n",
    "print(index)\n",
    "index.save('./lsi_index.mm') # Saving the similarity matrix to a local matrix market file named './lsi_model.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n"
     ]
    }
   ],
   "source": [
    "# Loading the similarity matrix back from the local file\n",
    "similarity_matrix = similarities.MatrixSimilarity.load('./lsi_index.mm')\n",
    "print(len(similarity_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
