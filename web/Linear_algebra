Jordan normal form
In linear algebra, a Jordan normal form (often called Jordan canonical form)[1]of a linear operator on a finite-dimensional vector space is an upper triangular matrix of a particular form called a Jordan matrix, representing the operator with respect to some basis. Such a matrix has each non-zero off-diagonal entry equal to 1, immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them.Let V be a vector space over a field K. Then a basis with respect to which the matrix has the required form exists if and only if all eigenvalues of the matrix lie in K, or equivalently if the characteristic polynomial of the operator splits into linear factors over K. This condition is always satisfied if K is algebraically closed (for instance, if it is the field of complex numbers). The diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue.[2][3][4]If the operator is originally given by a square matrix M, then its Jordan normal form is also called the Jordan normal form of M. Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given M is not entirely unique, as it is a block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.[2][3][4]The Jordan–Chevalley decomposition is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the Jordan normal form.[5][6][7]The Jordan normal form is named after Camille Jordan, who first stated the Jordan decomposition theorem in 1870.[8]Some textbooks have the ones on the subdiagonal, i.e., immediately below the main diagonal instead of on the superdiagonal.  The eigenvalues are still on the main diagonal.[9][10]An n × n matrix A is diagonalizable if and only if the sum of the dimensions of the eigenspaces is n. Or, equivalently, if and only if A has n linearly independent eigenvectors. Not all matrices are diagonalizable. Consider the following matrix:Including multiplicity, the eigenvalues of A are λ = 1, 2, 4, 4. The dimension of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so A is not diagonalizable. However, there is an invertible matrix P such that A = PJP−1, whereThe matrix J is almost diagonal. This is the Jordan normal form of A. The section Example below fills in the details of the computation.In general, a square complex matrix A is similar to a block diagonal matrixwhere each block Ji is a square matrix of the formSo there exists an invertible matrix P such that P−1AP = J is such that the only non-zero entries of J are on the diagonal and the superdiagonal. J is called the Jordan normal form of A. Each Ji is called a Jordan block of A. In a given Jordan block, every entry on the superdiagonal is 1.Assuming this result, we can deduce the following properties:Consider the matrix A from the example in the previous section. The Jordan normal form is obtained by some similarity transformation P−1AP = J, i.e.,Let P have column vectors pi, i = 1, ..., 4, thenWe see thatThus, given an eigenvalue λ, its corresponding Jordan block gives rise to a Jordan chain. The generator, or lead vector, say pr, of the chain is a generalized eigenvector such that (A − λ I)rpr = 0, where r is the size of the Jordan block. The vector p1 =  (A − λ I)r−1pr is an eigenvector corresponding to λ. In general, pi is a preimage of pi−1 under A − λ I. So the lead vector generates the chain via multiplication by (A − λ I).[12][13]Therefore, the statement that every square matrix A can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of A.We give a proof by induction that any complex-valued matrix A may be put in Jordan normal form. The 1 × 1 case is trivial. Let A be an n × n matrix. Take any eigenvalue λ of A. The range of A − λ I, denoted by Ran(A − λ I), is an invariant subspace of A. Also, since λ is an eigenvalue of A, the dimension of Ran(A − λ I), r, is strictly less than n. Let A'  denote the restriction of A to Ran(A − λ I), By inductive hypothesis, there exists a basis {p1, ..., pr} such that A' , expressed with respect to this basis, is in Jordan normal form.Next consider the subspace Ker(A − λ I). Ifthe desired result follows immediately from the rank–nullity theorem. This would be the case, for example, if A was Hermitian.Otherwise, iflet the dimension of Q be s ≤ r. Each vector in Q is an eigenvector of A'  corresponding to eigenvalue λ. So the Jordan form of A'  must contain s Jordan chains corresponding to s linearly independent eigenvectors. So the basis {p1, ..., pr} must contain s vectors, say {pr−s+1, ..., pr}, that are lead vectors in these Jordan chains from the Jordan normal form of A'. We can "extend the chains" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(A − λ I).) Let qi be such thatClearly no non-trivial linear combination of the qi can lie in Ker(A − λ I). Furthermore, no non-trivial linear combination of the qi can be in Ran(A − λ I), for that would contradict the assumption that each pi is a lead vector in a Jordan chain. The set {qi}, being preimages of the linearly independent set {pi} under A − λ I, is also linearly independent.Finally, we can pick any linearly independent set {z1, ..., zt} that spansBy construction, the union of the three sets {p1, ..., pr}, {qr−s +1, ..., qr}, and  {z1, ..., zt} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of A. Finally, by the rank–nullity theorem, the cardinality of the union is n. In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of A, and this shows A can be put in Jordan normal form.It can be shown that the Jordan normal form of a given matrix A is unique up to the order of the Jordan blocks.Knowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of A. Assuming the algebraic multiplicity m(λ) of an eigenvalue λ is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (A − λ I)m(λ). To see this, suppose an n × n matrix A has only one eigenvalue λ. So m(λ) = n. The smallest integer k1 such thatis the size of the largest Jordan block in the Jordan form of A. (This number k1 is also called the index of λ. See discussion in a following section.) The rank ofis the number of Jordan blocks of size k1. Similarly, the rank ofis twice the number of Jordan blocks of size k1 plus the number of Jordan blocks of size k1−1. The general case is similar.This can be used to show the uniqueness of the Jordan form. Let J1 and J2 be two Jordan normal forms of A. Then J1 and J2 are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of J1 and J2. This proves the uniqueness part of the statement.This real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form complex conjugate pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.One can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.Using the Jordan normal form, direct calculation gives a spectral mapping theorem for the polynomial functional calculus: Let A be an n × n matrix with eigenvalues λ1, ..., λn, then for any polynomial p, p(A) has eigenvalues p(λ1), ..., p(λn).The Cayley–Hamilton theorem asserts that every matrix A satisfies its characteristic equation: if p is the characteristic polynomial of A, then p(A) = 0. This can be shown via direct calculation in the Jordan form, since any Jordan block for λ is annihilated by (X − λ)m where m is the multiplicity of the root λ of p, the sum of the sizes of the Jordan blocks for λ, and therefore no less than the size of the block in question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the splitting field of p; this field extension does not change the matrix p(A) in any way.The minimal polynomial P of a square matrix A is the unique monic polynomial of least degree, m, such that P(A) = 0. Alternatively, the set of polynomials that annihilate a given A form an ideal I in C[x], the principal ideal domain of polynomials with complex coefficients. The monic element that generates I is precisely P.Let λ1, ..., λq be the distinct eigenvalues of A, and si be the size of the largest Jordan block corresponding to λi. It is clear from the Jordan normal form that the minimal polynomial of A has degree Σsi.While the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of elementary divisors. The elementary divisors of a square matrix A are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial m are the elementary divisors of the largest degree corresponding to distinct eigenvalues.The degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, A is diagonalizable.The Jordan form of a n × n matrix A is block diagonal, and therefore gives a decomposition of the n dimensional Euclidean space into invariant subspaces of A. Every Jordan block Ji corresponds to an invariant subspace Xi. Symbolically, we putwhere each Xi is the span of the corresponding Jordan chain, and k is the number of Jordan chains.One can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue λi, the size of its largest corresponding Jordan block si is called the index of  λi and denoted by ν(λi). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace Yi byThis gives the decompositionwhere l is the number of distinct eigenvalues of A. Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where A is a multiple of the identity matrix we have k = n and l = 1.The projection onto Yi and along all the other Yj ( j ≠ i ) is called the spectral projection of A at λi and is usually denoted by P(λi ; A). Spectral projections are mutually orthogonal in the sense that P(λi ; A) P(λj ; A) = 0 if i ≠ j. Also they commute with A and their sum is the identity matrix. Replacing every λi in the Jordan matrix J by one and zeroising all other entries gives P(λi ; J), moreover if U J U−1 is the similarity transformation such that A = U J U−1 then P(λi ; A) = U P(λi ; J) U−1. They are not confined to finite dimensions. See below for their application to compact operators, and in holomorphic functional calculus for a more general discussion.Comparing the two decompositions, notice that, in general, l ≤ k. When A is normal, the subspaces Xi's in the first decomposition are one-dimensional and mutually orthogonal. This is the spectral theorem for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.It might be of interest here to note some properties of the index, ν(λ). More generally, for a complex number λ, its index can be defined as the least non-negative integer ν(λ) such thatSo ν(λ) > 0 if and only if λ is an eigenvalue of A. In the finite-dimensional case, ν(λ) ≤ the algebraic multiplicity of λ.Jordan reduction can be extended to any square matrix M whose entries lie in a field K.  The result states that any M can be written as a sum D + N where D is semisimple, N is nilpotent, and DN = ND. This is called the Jordan–Chevalley decomposition. Whenever K contains the eigenvalues of M, in particular when K is algebraically closed, the normal form can be expressed explicitly as the direct sum of Jordan blocks. Similar to the case when K is the complex numbers, knowing the dimensions of the kernels of (M − λI)k for 1 ≤ k ≤ m, where m is the algebraic multiplicity of the eigenvalue λ, allows one to determine the Jordan form of M. We may view the underlying vector space V as a K[x]-module by regarding the action of x on V as application of M and extending by K-linearity. Then the polynomials (x − λ)k are the elementary divisors of M, and the Jordan normal form is concerned with representing M in terms of blocks associated to the elementary divisors.The proof of the Jordan normal form is usually carried out as an application to the ring K[x] of the structure theorem for finitely generated modules over a principal ideal domain, of which it is a corollary.In a different direction, for compact operators on a Banach space, a result analogous to the Jordan normal form holds. One restricts to compact operators because every point x in the spectrum of a compact operator T, the only exception being when x is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.Let X be a Banach space, L(X) be the bounded operators on X, and σ(T) denote the spectrum of T ∈ L(X). The holomorphic functional calculus is defined as follows:Fix a bounded operator T. Consider the family Hol(T) of complex functions that is holomorphic on some open set G containing σ(T). Let Γ = {γi} be a finite collection of Jordan curves such that σ(T) lies in the inside of Γ, we define f(T) byThe open set G could vary with f and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar case. Although the integral makes sense for continuous f, we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that σ(T) lie in the inside of Γ ensures f(T) is well defined; it does not depend on the choice of Γ. The functional calculus is the mapping Φ from Hol(T) to L(X) given byWe will require the following properties of this functional calculus:In the finite-dimensional case, σ(T) = {λi} is a finite discrete set in the complex plane. Let ei be the function that is 1 in some open neighborhood of λi and 0 elsewhere. By property 3 of the functional calculus, the operatoris a projection. Moreoever, let νi be the index of λi andThe spectral mapping theorem tells ushas spectrum {0}. By property 1, f(T) can be directly computed in the Jordan form, and by inspection, we see that the operator f(T)ei(T) is the zero matrix.By property 3, f(T) ei(T) = ei(T) f(T). So ei(T) is precisely the projection ontothe subspaceThe relationimplieswhere the index i runs through the distinct eigenvalues of T. This is exactly the invariant subspace decompositiongiven in a previous section. Each ei(T) is the projection onto the subspace spanned by the Jordan chains corresponding to λi and along the subspaces spanned by the Jordan chains corresponding to λj for j ≠ i. In other words, ei(T) = P(λi;T). This explicit identification of the operators ei(T) in turn gives an explicit form of holomorphic functional calculus for matrices:Notice that the expression of f(T) is a finite sum because, on each neighborhood of λi, we have chosen the Taylor series expansion of f centered at λi.Let T be a bounded operator λ be an isolated point of σ(T). (As stated above, when T is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)The point λ is called a pole of operator T with order ν if the resolvent function RT defined byhas a pole of order ν at λ.We will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators.Consider the annular region A centered at the eigenvalue λ with sufficiently small radius ε such that the intersection of the open disc Bε(λ) and σ(T) is {λ}. The resolvent function RT is holomorphic on A.Extending a result from classical function theory, RT has a Laurent series representation on A:whereBy the previous discussion on the functional calculus,But we have shown that the smallest positive integer m such thatis precisely the index of λ, ν(λ). In other words, the function RT has a pole of order ν(λ) at λ.This example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.Consider the matrixwhich is mentioned in the beginning of the article.The characteristic polynomial of A isThis shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation Av = λ v. It is spanned by the column vector v = (−1, 1, 0, 0)T. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by w = (1, −1, 0, 1)T. Finally, the eigenspace corresponding to the eigenvalue 4 is also one-dimensional (even though this is a double eigenvalue) and is spanned by x = (1, 0, −1, 1)T. So, the geometric multiplicity (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix A is the direct sumThere are three chains. Two have length one: {v} and {w}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculatewhere I is the 4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of A − 4I, e.g., y = (1,0,0,0)T. Now, (A − 4I)y = x and (A − 4I)x = 0, so {y, x} is a chain of length two corresponding to the eigenvalue 4.The transition matrix P such that P−1AP = J is formed by putting these vectors next to each other as followsA computation shows that the equation P−1AP = J indeed holds.If we had interchanged the order of which the chain vectors appeared, that is, changing the order of v, w and {x, y} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.If the matrix A has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrixIf ε = 0, then the Jordan normal form is simplyHowever, for ε ≠ 0, the Jordan normal form isThis ill conditioning makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in numerical analysis; the stable Schur decomposition[15] or pseudospectra[16] are better alternatives.The Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let f(z) be an analytical function of a complex argument. Applying the function on a n×n Jordan block J with eigenvalue λ results in an upper triangular matrix:The following example shows the application to the power function f(z)=zn:
Norm (mathematics)
In linear algebra, functional analysis, and related areas of mathematics, a norm is a function that assigns a strictly positive length or size to each vector in a vector space—except for the zero vector, which is assigned a length of zero. A seminorm, on the other hand, is allowed to assign zero length to some non-zero vectors (in addition to the zero vector).A norm must also satisfy certain properties pertaining to scalability and additivity which are given in the formal definition below.A simple example is two dimensional Euclidean space R2 equipped with the "Euclidean norm" (see below). Elements in this vector space (e.g., (3, 7)) are usually drawn as arrows in a 2-dimensional cartesian coordinate system starting at the origin (0, 0). The Euclidean norm assigns to each vector the length of its arrow. Because of this, the Euclidean norm is often known as the magnitude.A vector space on which a norm is defined is called a normed vector space. Similarly, a vector space with a seminorm is called a seminormed vector space. It is often possible to supply a norm for a given vector space in more than one way.Given a vector space V over a subfield F of the complex numbers, a norm on V is a nonnegative-valued scalar function p: V → [0,+∞) with the following properties:[1]For all a ∈ F and all u, v ∈ V,A seminorm on V is a function p : V → R with the properties 1 and 2 above.Every vector space V with seminorm p induces a normed space V/W, called the quotient space, where W is the subspace of V consisting of all vectors v in V with p(v) = 0. The induced norm on V/W is defined by:Two norms (or seminorms) p and q on a vector space V are equivalent if there exist two real constants c and C, with c > 0, such thatA topological vector space is called normable (seminormable) if the topology of the space can be induced by a norm (seminorm).If a norm p : V → R is given on a vector space V then the norm of a vector v ∈ V is usually denoted by enclosing it within double vertical lines: ‖v‖ = p(v). Such notation is also sometimes used if p is only a seminorm.For the length of a vector in Euclidean space (which is an example of a norm, as explained below), the notation |v| with single vertical lines is also widespread.In Unicode, the code point of the "double vertical line" character ‖ is U+2016. The double vertical line should not be confused with the "parallel to" symbol, Unicode U+2225 ( ∥ ). This is usually not a problem because the former is used in parenthesis-like fashion, whereas the latter is used as an infix operator. The double vertical line used here should also not be confused with the symbol used to denote lateral clicks in linguistics, Unicode U+01C1 ( ǁ ). The single vertical line | is called "vertical line" in Unicode and its code point is U+007C.In LaTeX and related markup languages, the macros '\|' and '\parallel' are often used to denote a norm.The absolute valueis a norm on the one-dimensional vector spaces formed by the real or complex numbers.The absolute value norm is a special case of the L1 norm.On an n-dimensional Euclidean space Rn, the intuitive notion of length of the vector x = (x1, x2, ..., xn) is captured by the formulaThis gives the ordinary distance from the origin to the point X, a consequence of the Pythagorean theorem.  This operation may also be referred to as "SRSS" which is an acronym for the square root of the sum of squares.[2]The Euclidean norm is by far the most commonly used norm on Rn, but there are other norms on this vector space as will be shown below. However, all these norms are equivalent in the sense that they all define the same topology.On an n-dimensional complex space Cn the most common norm isIn both cases the norm can be expressed as the square root of the inner product of the vector and itself:where x is represented as a column vector ([x1; x2; ...; xn]), and x∗ denotes its conjugate transpose.This formula is valid for any inner product space, including Euclidean and complex spaces. For Euclidean spaces, the inner product is equivalent to the dot product. Hence, in this specific case the formula can be also written with the following notation:The Euclidean norm is also called the Euclidean length, L2 distance, ℓ2 distance, L2 norm, or ℓ2 norm; see Lp space.The set of vectors in Rn+1 whose Euclidean norm is a given positive constant forms an n-sphere.The name relates to the distance a taxi has to drive in a rectangular street grid to get from the origin to the point x.The 1-norm is simply the sum of the absolute values of the columns.In contrast,is not a norm because it may yield negative results.The p-norm is related to the generalized mean or power mean.This definition is still of some interest for 0 < p < 1, but the resulting function does not define a norm,[3] because it violates the triangle inequality. What is true for this case of 0 < p < 1, even in the measurable analog, is that the corresponding Lp class is a vector space, and it is also true that the function(without pth root) defines a distance that makes Lp(X) into a complete metric topological vector space. These spaces are of great interest in functional analysis, probability theory, and harmonic analysis.However, outside trivial cases, this topological vector space is not locally convex and has no continuous nonzero linear forms. Thus the topological dual space contains only the zero functional.The partial derivative of the p-norm is given byThe derivative with respect to x, therefore, isFor the special case of p = 2, this becomesorThe set of vectors whose infinity norm is a given constant, c, forms the surface of a hypercube with edge length 2c.In metric geometry, the discrete metric takes the value one for distinct points and zero otherwise. When applied coordinate-wise to the elements of a vector space, the discrete distance defines the Hamming distance, which is important in coding and information theory. In the field of real or complex numbers, the distance of the discrete metric from zero is not homogeneous in the non-zero point; indeed, the distance from zero remains one as its non-zero argument approaches zero. However, the discrete distance of a number from zero does satisfy the other properties of a norm, namely the triangle inequality and positive definiteness. When applied component-wise to vectors, the discrete distance from zero behaves like a non-homogeneous "norm", which counts the number of non-zero components in its vector argument; again, this non-homogeneous "norm" is discontinuous.In signal processing and statistics, David Donoho referred to the zero "norm" with quotation marks. Following Donoho's notation, the zero "norm" of x is simply the number of non-zero coordinates of x, or the Hamming distance of the vector from zero. When this "norm" is localized to a bounded set, it is the limit of p-norms as p approaches 0. Of course, the zero "norm" is not truly a norm, because it is not positive homogeneous. Indeed, it is not even an F-norm in the sense described above, since it is discontinuous, jointly and severally, with respect to the scalar argument in scalar–vector multiplication and with respect to its vector argument. Abusing terminology, some engineers[who?] omit Donoho's quotation marks and inappropriately call the number-of-nonzeros function the L0 norm, echoing the notation for the Lebesgue space of measurable functions.Other norms on Rn can be constructed by combining the above; for exampleis a norm on R4.For any norm and any injective linear transformation A we can define a new norm of x, equal toIn 2D, with A a rotation by 45° and a suitable scaling, this changes the taxicab norm into the maximum norm. In 2D, each A applied to the taxicab norm, up to inversion and interchanging of axes, gives a different unit ball: a parallelogram of a particular shape, size and orientation. In 3D this is similar but different for the 1-norm (octahedrons) and the maximum norm (prisms with parallelogram base).There are examples of norms that are not defined by "entrywise" formulas. For instance, the Minkowski functional of a centrally-symmetric convex body in Rn (centered at zero) defines a norm on Rn.All the above formulas also yield norms on Cn without modification.There are also norms on spaces of matrices (with real or complex entries), the so-called matrix norms.The generalization of the above norms to an infinite number of components leads to ℓ p and L p spaces, with normsOther examples of infinite dimensional normed vector spaces can be found in the Banach space article.Two norms ‖•‖α and ‖•‖β on a vector space V are called equivalent if there exist positive real numbers C and D such that for all x in VIn particular,i.e., If the vector space is a finite-dimensional real or complex one, all norms are equivalent. On the other hand, in the case of infinite-dimensional vector spaces, not all norms are equivalent.Equivalent norms define the same notions of continuity and convergence and for many purposes do not need to be distinguished. To be more precise the uniform structure defined by equivalent norms on the vector space is uniformly isomorphic.Every (semi)-norm is a sublinear function, which implies that every norm is a convex function. As a result, finding a global optimum of a norm-based objective function is often tractable.Given a finite family of seminorms pi on a vector space the sumis again a seminorm.For any norm p on a vector space V, we have that for all u and v ∈ V:Thus, p(u ± v) ≥ |p(u) − p(v)|.For the Lp norms, we have Hölder's inequality[6]A special case of this is the Cauchy–Schwarz inequality:[6]All seminorms on a vector space V can be classified in terms of absolutely convex absorbing subsets A of V. To each such subset corresponds a seminorm pA called the gauge of A, defined aswith the property thatConversely:Any locally convex topological vector space has a local basis consisting of absolutely convex sets. A common method to construct such a basis is to use a family (p) of seminorms p that separates points: the collection of all finite intersections of sets {p < 1/n} turns the space into a locally convex topological vector space so that every p is continuous.Such a method is used to design weak and weak* topologies.norm case:There are several generalizations of norms and semi-norms. If p is absolute homogeneity but in place of subadditivity we require thatthen p satisfies the triangle inequality but is called a quasi-seminorm and the smallest value of b for which this holds is called the multiplier of p; if in addition p separates points then it is called a quasi-norm.On the other hand, if p satisfies the triangle inequality but in place of absolute homogeneity we require thatthen p is called a k-seminorm.We have the following relationship between quasi-seminorms and k-seminorms:The concept of norm in composition algebras does not share the usual properties of a norm. A composition algebra (A, *, N) consists of an algebra over a field A, an involution *, and a quadratic form N, which is called the "norm". In several cases N is an isotropic quadratic form so that A has at least one null vector, contrary to the separation of points required for the usual norm discussed in this article.
Dimension (vector space)
In mathematics, the dimension of a vector space V is the cardinality (i.e. the number of vectors) of a basis of V over its base field.[1] It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension.For every vector space there exists a basis,[a] and all bases of a vector space have equal cardinality;[b] as a result, the dimension of a vector space is uniquely defined. We say V is finite-dimensional if the dimension of V is finite, and infinite-dimensional if its dimension is infinite.The dimension of the vector space V over the field F can be written as dimF(V) or as [V : F], read "dimension of V over F". When F can be inferred from context, dim(V) is typically written.The vector space R3 has as a basis, and therefore we have dimR(R3) = 3. More generally, dimR(Rn) = n, and even more generally, dimF(Fn) = n for any field F.The complex numbers C are both a real and complex vector space; we have dimR(C) = 2 and dimC(C) = 1. So the dimension depends on the base field.The only vector space with dimension 0 is {0}, the vector space consisting only of its zero element.If W is a linear subspace of V, then dim(W) ≤ dim(V).To show that two finite-dimensional vector spaces are equal, one often uses the following criterion: if V is a finite-dimensional vector space and W is a linear subspace of V with dim(W) = dim(V), then W = V.Rn has the standard basis {e1, ..., en}, where ei is the i-th column of the corresponding identity matrix. Therefore Rn has dimension n.Any two vector spaces over F having the same dimension are isomorphic. Any bijective map between their bases can be uniquely extended to a bijective linear map between the vector spaces. If B is some set, a vector space with dimension |B| over F can be constructed as follows: take the set F(B) of all functions f : B → F such that f(b) = 0 for all but finitely many b in B. These functions can be added and multiplied with elements of F, and we obtain the desired F-vector space. An important result about dimensions is given by the rank–nullity theorem for linear maps.If F/K is a field extension, then F is in particular a vector space over K. Furthermore, every F-vector space V is also a K-vector space. The dimensions are related by the formulaIn particular, every complex vector space of dimension n is a real vector space of dimension 2n.Some simple formulae relate the dimension of a vector space with the cardinality of the base field and the cardinality of the space itself.If V is a vector space over a field F then, denoting the dimension of V by dim V, we have:One can see a vector space as a particular case of a matroid, and in the latter there is a well-defined notion of dimension. The length of a module and the rank of an abelian group both have several properties similar to the dimension of vector spaces.The Krull dimension of a commutative ring, named after Wolfgang Krull (1899–1971), is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring.Alternatively, one may be able to take the trace of operators on an infinite-dimensional space; in this case a (finite) trace is defined, even though no (finite) dimension exists, and gives a notion of "dimension of the operator". These fall under the rubric of "trace class operators" on a Hilbert space, or more generally nuclear operators on a Banach space.
Fundamental matrix (computer vision)
Being of rank two and determined only up to scale, the fundamental matrix can be estimated given at least seven point correspondences. Its seven parameters represent the only geometric information about cameras that can be obtained through point correspondences alone.The term "fundamental matrix" was coined by QT Luong in his influential PhD thesis. It is sometimes also referred to as the "bifocal tensor". As a tensor it is a two-point tensor in that it is a bilinear form relating points in distinct coordinate systems.The fundamental matrix is a relationship between any two images of the same scene that constrains where the projection of points from the scene can occur in both images. Given the projection of a scene point into one of the images the corresponding point in the other image is constrained to a line, helping the search, and allowing for the detection of wrong correspondences.  The relation between corresponding image points which the fundamental matrix represents is referred to as epipolar constraint, matching constraint, discrete matching constraint, or incidence relation.The fundamental matrix can be determined by a set of point correspondences.  Additionally, these corresponding image points may be triangulated to world points with the help of camera matrices derived directly from this fundamental matrix.  The scene composed of these world points is within a projective transformation of the true scene.[1]The cameras then transform asFundamental matrix can be derived using the coplanarity condition. [2]The fundamental matrix is of rank 2. Its kernel defines the epipole.
Algebra over a field
In mathematics, an algebra over a field (often simply called an algebra) is a vector space equipped with a bilinear product. Thus, an algebra is an algebraic structure, which consists of a set, together with operations of multiplication, addition, and scalar multiplication by elements of the underlying field, and satisfies the axioms implied by "vector space" and "bilinear".[1]The multiplication operation in an algebra may or may not be associative, leading to the notions of associative algebras and nonassociative algebras. Given an integer n, the ring of real square matrices of order n is an example of an associative algebra over the field of real numbers under matrix addition and matrix multiplication since matrix multiplication is associative. Three-dimensional Euclidean space with multiplication given by the vector cross product is an example of a nonassociative algebra over the field of real numbers since the vector cross product is nonassociative, satisfying the Jacobi identity instead.An algebra is unital or unitary if it has an identity element with respect to the multiplication. The ring of real square matrices of order n forms a unital algebra since the identity matrix of order n is the identity element with respect to matrix multiplication. It is an example of a unital associative algebra, a (unital) ring that is also a vector space.Many authors use the term algebra to mean associative algebra, or unital associative algebra, or in  some subjects such as algebraic geometry, unital associative commutative algebra.Replacing the field of scalars by a commutative ring leads to the more general notion of an algebra over a ring. Algebras are not to be confused with vector spaces equipped with a bilinear form, like inner product spaces, as, for such a space, the result of a product is not in the space, but rather in the field of coefficients.Any complex number may be written a + bi, where a and b are real numbers and i is the imaginary unit. In other words, a complex number is represented by the vector (a, b) over the field of real numbers. So the complex numbers form a two-dimensional real vector space, where addition is given by (a, b) + (c, d) = (a + c, b + d) and scalar multiplication is given by c(a, b) = (ca, cb), where all of a, b, c and d are real numbers. We use the symbol · to multiply two vectors together, which we use complex multiplication to define: (a, b) · (c, d) = (ac − bd, ad + bc).The following statements are basic properties of the complex numbers. If x, y, z are complex numbers and a, b are real numbers, thenThis example fits into the following definition by taking the field K to be the real numbers, and the vector space A to be the complex numbers.Let K be a field, and let A be a vector space over K equipped with an additional binary operation from A × A to A, denoted here by · (i.e. if x and y are any two elements of A, x · y is the product of x and y).  Then A is an algebra over K if the following identities hold for all elements x, y, and z of A, and all elements (often called scalars) a and b of K:These three axioms are another way of saying that the binary operation is bilinear. An algebra over K is sometimes also called a K-algebra, and K is called the base field of A. The binary operation is often referred to as multiplication in A. The convention adopted in this article is that multiplication of elements of an algebra is not necessarily associative, although some authors use the term algebra to refer to an associative algebra.Notice that when a binary operation on a vector space is commutative, as in the above example of the complex numbers, it is left distributive exactly when it is right distributive. But in general, for non-commutative operations (such as the next example of the quaternions), they are not equivalent, and therefore require separate axioms.The real numbers may be viewed as a one-dimensional vector space with a compatible multiplication, and hence a one-dimensional algebra over itself. Likewise, as we saw above, the complex numbers form a two-dimensional vector space over the field of real numbers, and hence form a two dimensional algebra over the reals. In both these examples, every non-zero vector has an inverse, making them both division algebras. Although there are no division algebras in 3 dimensions, in 1843, the quaternions were defined and provided the now famous 4-dimensional example of an algebra over the real numbers, where one can not only multiply vectors, but also divide. Any quaternion may be written as (a, b, c, d) = a + bi + cj + dk. Unlike the complex numbers, the quaternions are an example of a non-commutative algebra: for instance, (0,1,0,0) · (0,0,1,0) = (0,0,0,1) but (0,0,1,0) · (0,1,0,0) = (0,0,0,−1).The quaternions were soon followed by several other hypercomplex number systems, which were the early examples of algebras over a field.Previous examples are associative algebras. An example of a non-associative algebra is a three dimensional vector space equipped with the cross product. This is a simple example of a class of nonassociative algebras, which is widely used in mathematics and physics, the Lie algebras.Given K-algebras A and B, a K-algebra homomorphism is a K-linear map f: A → B such that f(xy) = f(x) f(y) for all x,y in A. The space of all K-algebra homomorphisms between A and B is frequently written asA K-algebra isomorphism is a bijective K-algebra homomorphism. For all practical purposes, isomorphic algebras differ only by notation.A subalgebra of an algebra over a field K is a linear subspace that has the property that the product of any two of its elements is again in the subspace. In other words, a subalgebra of an algebra is a subset of elements that is closed under addition, multiplication, and scalar multiplication. In symbols, we say that a subset L of a K-algebra A is a subalgebra if for every x, y in L and c in K, we have that x · y, x + y, and cx are all in L.In the above example of the complex numbers viewed as a two-dimensional algebra over the real numbers, the one-dimensional real line is a subalgebra.A left ideal of a K-algebra is a linear subspace that has the property that any element of the subspace multiplied on the left by any element of the algebra produces an element of the subspace. In symbols, we say that a subset L of a K-algebra A is a left ideal if for every x and y in L, z in A and c in K, we have the following three statements.If (3) were replaced with x · z is in L, then this would define a right ideal. A two-sided ideal is a subset that is both a left and a right ideal. The term ideal on its own is usually taken to mean a two-sided ideal. Of course when the algebra is commutative, then all of these notions of ideal are equivalent. Notice that conditions (1) and (2) together are equivalent to L being a linear subspace of A. It follows from condition (3) that every left or right ideal is a subalgebra.It is important to notice that this definition is different from the definition of an ideal of a ring, in that here we require the condition (2). Of course if the algebra is unital, then condition (3) implies condition (2).Algebras over fields come in many different types. These types are specified by insisting on some further axioms, such as commutativity or associativity of the multiplication operation, which are not required in the broad definition of an algebra. The theories corresponding to the different types of algebras are often very different.An algebra is unital or unitary if it has a unit or identity element I with Ix = x = xI for all x in the algebra.An algebra is called zero algebra if uv = 0 for all u, v in the algebra,[2] not to be confused with the algebra with one element.  It is inherently non-unital (except in the case of only one element), associative and commutative.One may define a unital zero algebra by taking the direct sum of modules of a field (or more generally a ring) K and a K-vector space (or module) V, and defining the product of every pair of elements of V to be zero. That is, if λ, μ ∈ K and u, v ∈ V, then (λ + u) (μ + v) = λμ + (λv + μu). If e1, ... ed is a basis of V, the unital zero algebra is the quotient of the polynomial ring K[E1, ..., En] by the ideal generated by the EiEj for every pair (i, j).An example of unital zero algebra is the algebra of dual numbers, the unital zero R-algebra built from a one dimensional real vector space.These unital zero algebras may be more generally useful, as they allow to translate any general property of the algebras to properties of vector spaces or modules. For example, the theory of Gröbner bases was introduced by Bruno Buchberger for ideals in a polynomial ring R = K[x1, ..., xn] over a field. The construction of the unital zero algebra over a free R-module allows extending this theory as a Gröbner basis theory for sub modules of a free module. This extension allows, for computing a Gröbner basis of a submodule, to use, without any modification, any algorithm and any software for computing Gröbner bases of ideals.Examples detailed in the main article include:The definition of an associative K-algebra with unit is also frequently given in an alternative way. In this case, an algebra over a field K is a ring A together with a ring homomorphismwhere Z(A) is the center of A. Since η is a ring morphism, then one must have either that A is the zero ring, or that η is injective.  This definition is equivalent to that above, with scalar multiplicationgiven byGiven two such associative unital K-algebras A and B, a unital K-algebra morphism f: A → B is a ring morphism that commutes with the scalar multiplication defined by η, which one may write asFor algebras over a field, the bilinear multiplication from A × A to A is completely determined by the multiplication of basis elements of A.Conversely, once a basis for A has been chosen, the products of basis elements can be set arbitrarily, and then extended in a unique way to a bilinear operator on A, i.e., so the resulting multiplication satisfies the algebra laws.Thus, given the field K, any finite-dimensional algebra can be specified up to isomorphism by giving its dimension (say n), and specifying n3 structure coefficients ci,j,k, which are scalars.These structure coefficients determine the multiplication in A via the following rule:where e1,...,en form a basis of A.Note however that several different sets of structure coefficients can give rise to isomorphic algebras.When the algebra can be endowed with a metric, then the structure coefficients are generally written with upper and lower indices, so as to distinguish their transformation properties under coordinate transformations. Specifically, lower indices are covariant indices, and transform via pullbacks, while upper indices are contravariant, transforming under pushforwards. Thus, in mathematical physics, the structure coefficients are often written ci,jk, and their defining rule is written using the Einstein notation asIf you apply this to vectors written in index notation, then this becomesIf K is only a commutative ring and not a field, then the same process works if A is a free module over K. If it isn't, then the multiplication is still completely determined by its action on a set that spans A; however, the structure constants can't be specified arbitrarily in this case, and knowing only the structure constants does not specify the algebra up to isomorphism.Two-dimensional, three-dimensional and four-dimensional unital associative algebras over the field of complex numbers were completely classified up to isomorphism by Eduard Study.[4]There exist two two-dimensional algebras. Each algebra consists of linear combinations (with complex coefficients) of two basis elements, 1 (the identity element) and a. According to the definition of an identity element,It remains to specifyThere exist five three-dimensional algebras. Each algebra consists of linear combinations of three basis elements, 1 (the identity element), a and b. Taking into account the definition of an identity element, it is sufficient to specifyThe fourth algebra is non-commutative, others are commutative.In some areas of mathematics, such as commutative algebra, it is common to consider the more general concept of an algebra over a ring, where a commutative unital ring R replaces the field K. The only part of the definition that changes is that A is assumed to be an R-module (instead of a vector space over K). 
Rank factorization
Consider the matrixIt is straightforward to check that
System of linear equations
In mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1]  For example,is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given bysince it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.The simplest kind of linear system involves two equations and two variables:Now substitute this expression for x into the bottom equation:A general system of m linear equations with n unknowns can be written asOften the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.The vector equation is equivalent to a matrix equation of the formwhere A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The number of vectors in a basis for the span is now expressed as the rank of the matrix.A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.A linear system may behave in any one of three possible ways:For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.The following pictures illustrate this trichotomy in the case of two variables:The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.For example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For a more complicated example, the equationsare not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.There are several algorithms for solving a system of linear equations.To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.For example, consider the following system:The solution set to this system can be described by the following equations:Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Here x is the free variable, and y and z are dependent.The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:For example, consider the following system:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsSolving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:The last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemis given byFor each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.)Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.There is also a quantum algorithm for linear systems of equations.[3]A system of linear equations is homogeneous if all of the constant terms are zero:A homogeneous system is equivalent to a matrix equation of the formwhere A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution.  If the system has a singular matrix then there is a solution set with an infinite number of solutions.  This solution set has the following additional properties:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A.Numerical solutions to a homogeneous system can be found with a singular value decomposition.There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asGeometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.
Complex plane
In mathematics, the complex plane or z-plane is a geometric representation of the complex numbers established by the real axis and the perpendicular imaginary axis. It can be thought of as a modified Cartesian plane, with the real part of a complex number represented by a displacement along the x-axis, and the imaginary part by a displacement along the y-axis.[1]The concept of the complex plane allows a geometric interpretation of complex numbers. Under addition, they add like vectors. The multiplication of two complex numbers can be expressed most easily in polar coordinates—the magnitude or modulus of the product is the product of the two absolute values, or moduli, and the angle or argument of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.The complex plane is sometimes known as the Argand plane. In complex analysis, the complex numbers are customarily represented by the symbol z, which can be separated into its real (x) and imaginary (y) parts:for example: z = 4 + 5i, where x and y are real numbers, and i is the imaginary unit. In this customary notation the complex number z corresponds to the point (x, y) in the Cartesian plane.In the Cartesian plane the point (x, y) can also be represented in polar coordinates asIn the Cartesian plane it may be assumed that the arctangent takes values from −π/2 to π/2 (in radians), and some care must be taken to define the real arctangent function for points (x, y) when x ≤ 0.[2] In the complex plane these polar coordinates take the formwhereHere |z| is the absolute value or modulus of the complex number z;  θ, the argument of z, is usually taken on the interval 0 ≤ θ < 2π; and the last equality (to |z|eiθ) is taken from Euler's formula. Notice that without the constraint on the range of θ, the argument of z is multi-valued, because the complex exponential function is periodic, with period 2π i. Thus, if θ is one value of arg(z), the other values are given by arg(z) = θ + 2nπ, where n is any integer ≠ 0.[4] The theory of contour integration comprises a major part of complex analysis. In this context the direction of travel around a closed curve is important – reversing the direction in which the curve is traversed multiplies the value of the integral by −1. By convention the positive direction is counterclockwise. For example, the unit circle is traversed in the positive direction when we start at the point z = 1, then travel up and to the left through the point z = i, then down and to the left through −1, then down and to the right through −i, and finally up and to the right to z = 1, where we started.Almost all of complex analysis is concerned with complex functions – that is, with functions that map some subset of the complex plane into some other (possibly overlapping, or even identical) subset of the complex plane. Here it is customary to speak of the domain of f(z) as lying in the z-plane, while referring to the range or image of f(z) as a set of points in the w-plane. In symbols we writeand often think of the function f as a transformation from the z-plane (with coordinates (x, y)) into the w-plane (with coordinates (u, v)).Argand diagram refers to a geometric plot of complex numbers as points z=x+iy using the x-axis as the real axis and y-axis as the imaginary axis.[5]. Such plots are named after Jean-Robert Argand (1768–1822), although they were first described by Norwegian–Danish land surveyor and mathematician Caspar Wessel (1745–1818).[6] Argand diagrams are frequently used to plot the positions of the zeros and poles of a function in the complex plane. It can be useful to think of the complex plane as if it occupied the surface of a sphere. Given a sphere of unit radius, place its center at the origin of the complex plane, oriented so that the equator on the sphere coincides with the unit circle in the plane, and the north pole is "above" the plane.We can establish a one-to-one correspondence between the points on the surface of the sphere minus the north pole and the points in the complex plane as follows. Given a point in the plane, draw a straight line connecting it with the north pole on the sphere. That line will intersect the surface of the sphere in exactly one other point. The point z = 0 will be projected onto the south pole of the sphere. Since the interior of the unit circle lies inside the sphere, that entire region (|z| < 1) will be mapped onto the southern hemisphere. The unit circle itself (|z| = 1) will be mapped onto the equator, and the exterior of the unit circle (|z| > 1) will be mapped onto the northern hemisphere, minus the north pole. Clearly this procedure is reversible – given any point on the surface of the sphere that is not the north pole, we can draw a straight line connecting that point to the north pole and intersecting the flat plane in exactly one point.Under this stereographic projection the north pole itself is not associated with any point in the complex plane. We perfect the one-to-one correspondence by adding one more point to the complex plane – the so-called point at infinity – and identifying it with the north pole on the sphere. This topological space, the complex plane plus the point at infinity, is known as the extended complex plane. We speak of a single "point at infinity" when discussing complex analysis. There are two points at infinity (positive, and negative) on the real number line, but there is only one point at infinity (the north pole) in the extended complex plane.[7]Imagine for a moment what will happen to the lines of latitude and longitude when they are projected from the sphere onto the flat plane. The lines of latitude are all parallel to the equator, so they will become perfect circles centered on the origin z = 0. And the lines of longitude will become straight lines passing through the origin (and also through the "point at infinity", since they pass through both the north and south poles on the sphere).This is not the only possible yet plausible stereographic situation of the projection of a sphere onto a plane consisting of two or more values. For instance, the north pole of the sphere might be placed on top of the origin z = −1 in a plane that is tangent to the circle. The details don't really matter. Any stereographic projection of a sphere onto a plane will produce one "point at infinity", and it will map the lines of latitude and longitude on the sphere into circles and straight lines, respectively, in the plane.When discussing functions of a complex variable it is often convenient to think of a cut in the complex plane. This idea arises naturally in several different contexts.Consider the simple two-valued relationshipBefore we can treat this relationship as a single-valued function, the range of the resulting value must be restricted  somehow. When dealing with the square roots of non-negative real numbers this is easily done. For instance, we can just defineto be the non-negative real number y such that y2 = x. This idea doesn't work so well in the two-dimensional complex plane. To see why, let's think about the way the value of f(z) varies as the point z moves around the unit circle. We can writeEvidently, as z moves all the way around the circle, w only traces out one-half of the circle. So one continuous motion in the complex plane has transformed the positive square root e0 = 1 into the negative square root eiπ = −1.This problem arises because the point z = 0 has just one square root, while every other complex number z ≠ 0 has exactly two square roots. On the real number line we could circumvent this problem by erecting a "barrier" at the single point x = 0. A bigger barrier is needed in the complex plane, to prevent any closed contour from completely encircling the branch point z = 0. This is commonly done by introducing a branch cut; in this case the "cut" might extend from the point z = 0 along the positive real axis to the point at infinity, so that the argument of the variable z in the cut plane is restricted to the range 0 ≤ arg(z) < 2π.We can now give a complete description of w = z½. To do so we need two copies of the z-plane, each of them cut along the real axis. On one copy we define the square root of 1 to be e0 = 1, and on the other we define the square root of 1 to be eiπ = −1. We call these two copies of the complete cut plane sheets. By making a continuity argument we see that the (now single-valued) function w = z½ maps the first sheet into the upper half of the w-plane, where 0 ≤ arg(w) < π, while mapping the second sheet into the lower half of the w-plane (where π ≤ arg(w) < 2π).[8]The branch cut in this example doesn't have to lie along the real axis. It doesn't even have to be a straight line. Any continuous curve connecting the origin z = 0 with the point at infinity would work. In some cases the branch cut doesn't even have to pass through the point at infinity. For example, consider the relationshipHere the polynomial z2 − 1 vanishes when z = ±1, so g evidently has two branch points. We can "cut" the plane along the real axis, from −1 to 1, and obtain a sheet on which g(z) is a single-valued function. Alternatively, the cut can run from z = 1 along the positive real axis through the point at infinity, then continue "up" the negative real axis to the other branch point, z = −1.This situation is most easily visualized by using the stereographic projection described above. On the sphere one of these cuts runs longitudinally through the southern hemisphere, connecting a point on the equator (z = −1) with another point on the equator (z = 1), and passing through the south pole (the origin, z = 0) on the way. The second version of the cut runs longitudinally through the northern hemisphere and connects the same two equatorial points by passing through the north pole (that is, the point at infinity).A meromorphic function is a complex function that is holomorphic and therefore analytic everywhere in its domain except at a finite, or countably infinite, number of points.[9] The points at which such a function cannot be defined are called the poles of the meromorphic function. Sometimes all these poles lie in a straight line. In that case mathematicians may say that the function is "holomorphic on the cut plane". Here's a simple example.The gamma function, defined bywhere γ is the Euler–Mascheroni constant, and has simple poles at 0, −1, −2, −3, ... because exactly one denominator in the infinite product vanishes when z is zero, or a negative integer.[10] Since all its poles lie on the negative real axis, from z = 0 to the point at infinity, this function might be described as "holomorphic on the cut plane, the cut extending along the negative real axis, from 0 (inclusive) to the point at infinity."Alternatively, Γ(z) might be described as "holomorphic in the cut plane with −π < arg(z) < π and excluding the point z = 0."Notice that this cut is slightly different from the branch cut we've already encountered, because it actually excludes the negative real axis from the cut plane. The branch cut left the real axis connected with the cut plane on one side (0 ≤ θ), but severed it from the cut plane along the other side (θ < 2π).Of course, it's not actually necessary to exclude the entire line segment from z = 0 to −∞ to construct a domain in which Γ(z) is holomorphic. All we really have to do is puncture the plane at a countably infinite set of points {0, −1, −2, −3, ...}. But a closed contour in the punctured plane might encircle one or more of the poles of Γ(z), giving a contour integral that is not necessarily zero, by the residue theorem. By cutting the complex plane we ensure not only that Γ(z) is holomorphic in this restricted domain – we also ensure that the contour integral of Γ over any closed curve lying in the cut plane is identically equal to zero.Many complex functions are defined by infinite series, or by continued fractions. A fundamental consideration in the analysis of these infinitely long expressions is identifying the portion of the complex plane in which they converge to a finite value. A cut in the plane may facilitate this process, as the following examples show.Consider the function defined by the infinite seriesSince z2 = (−z)2 for every complex number z, it's clear that f(z) is an even function of z, so the analysis can be restricted to one half of the complex plane. And since the series is undefined whenit makes sense to cut the plane along the entire imaginary axis and establish the convergence of this series where the real part of z is not zero before undertaking the more arduous task of examining f(z) when z is a pure imaginary number.[11]In this example the cut is a mere convenience, because the points at which the infinite sum is undefined are isolated, and the cut plane can be replaced with a suitably punctured plane. In some contexts the cut is necessary, and not just convenient. Consider the infinite periodic continued fractionIt can be shown that f(z) converges to a finite value if and only if z is not a negative real number such that z < −¼. In other words, the convergence region for this continued fraction is the cut plane, where the cut runs along the negative real axis, from −¼ to the point at infinity.[12]We have already seen how the relationshipcan be made into a single-valued function by splitting the domain of f into two disconnected sheets. It is also possible to "glue" those two sheets back together to form a single Riemann surface on which f(z) = z1/2 can be defined as a holomorphic function whose image is the entire w-plane (except for the point w = 0). Here's how that works.Imagine two copies of the cut complex plane, the cuts extending along the positive real axis from z = 0 to the point at infinity. On one sheet define 0 ≤ arg(z) < 2π, so that 11/2 = e0 = 1, by definition. On the second sheet define 2π ≤ arg(z) < 4π, so that 11/2 = eiπ = −1, again by definition. Now flip the second sheet upside down, so the imaginary axis points in the opposite direction of the imaginary axis on the first sheet, with both real axes pointing in the same direction, and "glue" the two sheets together (so that the edge on the first sheet labeled "θ = 0" is connected to the edge labeled "θ < 4π" on the second sheet, and the edge on the second sheet labeled "θ = 2π" is connected to the edge labeled "θ < 2π" on the first sheet). The result is the Riemann surface domain on which f(z) = z1/2 is single-valued and holomorphic (except when z = 0).[8]To understand why f is single-valued in this domain, imagine a circuit around the unit circle, starting with z = 1 on the first sheet. When 0 ≤ θ < 2π we are still on the first sheet. When θ = 2π we have crossed over onto the second sheet, and are obliged to make a second complete circuit around the branch point z = 0 before returning to our starting point, where θ = 4π is equivalent to θ = 0, because of the way we glued the two sheets together. In other words, as the variable z makes two complete turns around the branch point, the image of z in the w-plane traces out just one complete circle.Formal differentiation shows thatfrom which we can conclude that the derivative of f exists and is finite everywhere on the Riemann surface, except when z = 0 (that is, f is holomorphic, except when z = 0).How can the Riemann surface for the functionalso discussed above, be constructed? Once again we begin with two copies of the z-plane, but this time each one is cut along the real line segment extending from z = −1 to z = 1 – these are the two branch points of g(z). We flip one of these upside down, so the two imaginary axes point in opposite directions, and glue the corresponding edges of the two cut sheets together. We can verify that g is a single-valued function on this surface by tracing a circuit around a circle of unit radius centered at z = 1. Commencing at the point z = 2 on the first sheet we turn halfway around the circle before encountering the cut at z = 0. The cut forces us onto the second sheet, so that when z has traced out one full turn around the branch point z = 1, w has taken just one-half of a full turn, the sign of w has been reversed (since eiπ = −1), and our path has taken us to the point z = 2 on the second sheet of the surface. Continuing on through another half turn we encounter the other side of the cut, where z = 0, and finally reach our starting point (z = 2 on the first sheet) after making two full turns around the branch point.The natural way to label θ = arg(z) in this example is to set −π < θ ≤ π on the first sheet, with π < θ ≤ 3π on the second. The imaginary axes on the two sheets point in opposite directions so that the counterclockwise sense of positive rotation is preserved as a closed contour moves from one sheet to the other (remember, the second sheet is upside down). Imagine this surface embedded in a three-dimensional space, with both sheets parallel to the xy-plane. Then there appears to be a vertical hole in the surface, where the two cuts are joined together. What if the cut is made from z = −1 down the real axis to the point at infinity, and from z = 1, up the real axis until the cut meets itself? Again a Riemann surface can be constructed, but this time the "hole" is horizontal. Topologically speaking, both versions of this Riemann surface are equivalent – they are orientable two-dimensional surfaces of genus one.Another related use of the complex plane is with the Nyquist stability criterion.  This is a geometric principle which allows the stability of a closed-loop feedback system to be determined by inspecting a Nyquist plot of its open-loop magnitude and phase response as a function of frequency (or loop transfer function) in the complex plane.The 'z-plane' is a discrete-time version of the s-plane, where z-transforms are used instead of the Laplace transformation.The preceding sections of this article deal with the complex plane in terms of a geometric representation of the complex numbers. Although this usage of the term "complex plane" has a long and mathematically rich history, it is by no means the only mathematical concept that can be characterized as "the complex plane". There are at least three additional possibilities.While the terminology "complex plane" is historically accepted, the object could be more appropriately named "complex line" as it is a 1-dimensional complex vector space.
Shanks transformation
In numerical analysis, the Shanks transformation is a non-linear series acceleration method to increase the rate of convergence of a sequence. This method is named after Daniel Shanks, who rediscovered this sequence transformation in 1955. It was first derived and published by R. Schmidt in 1941.[1]Milton D. Van Dyke (1975) Perturbation methods in fluid mechanics, p. 202.As an example, consider the slowly convergent series[3]The generalized kth-order Shanks transformation is given as the ratio of the determinants:[4]The generalized Shanks transformation is closely related to Padé approximants and Padé tables.[4]
Linear span
In linear algebra, the linear span (also called the linear hull or just span) of a set of vectors in a vector space is the intersection of all linear subspaces which each contain every vector in that set. The linear span of a set of vectors is therefore a vector space. Spans can be generalized to matroids and modules.For expressing that a vector space V is a span of a set S, one commonly uses the following phrases: S spans V; V is spanned by S; S is a spanning set of V; S is a generating set of V.Given a vector space V over a field K, the span of a set S of vectors (not necessarily infinite) is defined to be the intersection W of all subspaces of V that contain S. W is referred to as the subspace spanned by S, or by the vectors in S. Conversely, S is called a spanning set of W, and we say that S spans W.Alternatively, the span of S may be defined as the set of all finite linear combinations of elements of S, which follows from the above definition.In particular, if S is a finite subset of V, then the span of S is the set of all linear combinations of the elements of S. In the case of infinite S, infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming such sums are defined somehow, e.g. if V is a Banach space) are excluded by the definition; a generalization that allows these is not equivalent.The real vector space R3 has {(-1,0,0), (0,1,0), (0,0,1)} as a spanning set. This particular spanning set is also a basis. If (-1,0,0) were replaced by (1,0,0), it would also form the canonical basis of R3.Another spanning set for the same space is given by {(1,2,3), (0,1,2), (−1,1/2,3), (1,1,1)}, but this set is not a basis, because it is linearly dependent.The set {(1,0,0), (0,1,0), (1,1,0)} is not a spanning set of R3; instead its span is the space of all vectors in R3 whose last component is zero. That space (the space of all vectors in R3 whose last component is zero) is also spanned by the set {(1,0,0), (0,1,0)}, as (1,1,0) is a linear combination of (1,0,0) and (0,1,0). It does, however, span R2.The empty set is a spanning set of {(0, 0, 0)} since the empty set is a subset of all possible vector spaces in R3, and {(0, 0, 0)} is the intersection of all of these vector spaces.The set of functions xn where n is a non-negative integer spans the space of polynomials.Theorem 1: The subspace spanned by a non-empty subset S of a vector space V is the set of all linear combinations of vectors in S.This theorem is so well known that at times it is referred to as the definition of span of a set.Theorem 2: Every spanning set S of a vector space V must contain at least as many elements as any linearly independent set of vectors from V.Theorem 3: Let V be a finite-dimensional vector space. Any set of vectors that spans V can be reduced to a basis for V by discarding vectors if necessary (i.e. if there are linearly dependent vectors in the set). If the axiom of choice holds, this is true without the assumption that V has finite dimension.This also indicates that a basis is a minimal spanning set when V is finite-dimensional.Generalizing the definition of the span of points in space, a subset X of the ground set of a matroid is called a spanning set if the rank of X equals the rank of the entire ground set[citation needed].The vector space definition can also be generalized to modules.[1] Given an R-module A and any collection of elements a1,…,an of A, then the sum of cyclic modules,consisting of all R-linear combinations of the given elements ai, is called the submodule of A spanned by a1,…,an. As with the case of vector spaces, the submodule of A spanned by any subset of A is the intersection of all the submodules containing that subset.In functional analysis, a closed linear span of a set of vectors is the minimal closed set which contains the linear span of that set.One mathematical formulation of this isThe closed linear span of the set of functions xn on the interval [0, 1], where n is a non-negative integer, depends on the norm used. If the L2 norm is used, then the closed linear span is the Hilbert space of square-integrable functions on the interval. But if the maximum norm is used, the closed linear span will be the space of continuous functions on the interval. In either case, the closed linear span contains functions that are not polynomials, and so are not in the linear span itself. However, the cardinality of the set of functions in the closed linear span is the cardinality of the continuum, which is the same cardinality as for the set of polynomials.The linear span of a set is dense in the closed linear span. Moreover, as stated in the lemma below, the closed linear span is indeed the closure of the linear span.Closed linear spans are important when dealing with closed linear subspaces (which are themselves highly important, consider Riesz's lemma).Let X be a normed space and let E be any non-empty subset of X. Then(So the usual way to find the closed linear span is to find the linear span first, and then the closure of that linear span.)
Principal ideal domain
In abstract algebra, a principal ideal domain, or PID, is an integral domain in which every ideal is principal, i.e., can be generated by a single element. More generally, a principal ideal ring is a nonzero commutative ring whose ideals are principal, although some authors (e.g., Bourbaki) refer to PIDs as principal rings. The distinction is that a principal ideal ring may have zero divisors whereas a principal ideal domain cannot.Principal ideal domains are thus mathematical objects that behave somewhat like the integers, with respect to divisibility: any element of a PID has a unique decomposition into prime elements (so an analogue of the fundamental theorem of arithmetic holds); any two elements of a PID have a greatest common divisor (although it may not be possible to find it using the Euclidean algorithm). If x and y are elements of a PID without common divisors, then every element of the PID can be written in the form ax + by.Principal ideal domains are noetherian, they are  integrally closed, they are unique factorization domains and Dedekind domains.  All Euclidean domains and all fields are principal ideal domains.Principal ideal domains appear in the following chain of class inclusions:Examples include:Examples of integral domains that are not PIDs:In a principal ideal domain, any two elements a,b have a greatest common divisor, which may be obtained as a generator of the ideal (a,b).The previous three statements give the definition of a Dedekind domain, and hence every principal ideal domain is a Dedekind domain.Let A be an integral domain. Then the following are equivalent.A field norm is a Dedekind-Hasse norm; thus, (5) shows that a Euclidean domain is a PID. (4) compares to:An integral domain is a Bézout domain if and only if any two elements in it have a gcd that is a linear combination of the two. A Bézout domain is thus a GCD domain, and (4) gives yet another proof that a PID is a UFD.
Linear approximation
In mathematics, a linear approximation is an approximation of a general function using a linear function (more precisely, an affine function). They are widely used in the method of finite differences to produce first order methods for solving or approximating solutions to equations.In the more general case of Banach spaces, one hasGaussian optics is a technique in geometrical optics that describes the behaviour of light rays in optical systems by using the paraxial approximation, in which only rays which make small angles with the optical axis of the system are considered.[2] In this approximation, trigonometric functions can be expressed as linear functions of the angles. Gaussian optics applies to systems in which all the optical surfaces are either flat or are portions of a sphere. In this case, simple explicit formulae can be given for parameters of an imaging system such as focal distance, magnification and brightness, in terms of the geometrical shapes and material properties of the constituent elements.The period of swing of a simple gravity pendulum depends on its length, the local strength of gravity, and to a small extent on the maximum angle that the pendulum swings away from vertical, θ0, called the amplitude.[3]  It is independent of the mass of the bob. The true period T of a simple pendulum, the time taken for a complete cycle of an ideal simple gravity pendulum, can be written in several different forms (see Pendulum (mathematics) ), one example being the infinite series:[4][5]where L is the length of the pendulum and g is the local acceleration of gravity.However, if one takes the linear approximation (i.e. if the amplitude is limited to small swings,[Note 1] ) the period is:[6]In the linear approximation, the period of swing is approximately the same for different size swings: that is, the period is independent of amplitude. This property, called isochronism, is the reason pendulums are so useful for timekeeping.[7]  Successive swings of the pendulum, even if changing in amplitude, take the same amount of time.The electrical resistivity of most materials changes with temperature. If the temperature T does not vary too much, a linear approximation is typically used:
Wilkinson's polynomial
In numerical analysis, Wilkinson's polynomial is a specific polynomial which was used by James H. Wilkinson in 1963 to illustrate a difficulty when finding the root of a polynomial: the location of the roots can be very sensitive to perturbations in the coefficients of the polynomial.The polynomial isSometimes, the term Wilkinson's polynomial is also used to refer to some other polynomials appearing in Wilkinson's discussion.Wilkinson's polynomial arose in the study of algorithms for finding the roots of a polynomialIt is a natural question in numerical analysis to ask whether the problem of finding the roots of p from the coefficients ci is well-conditioned.  That is, we hope that a small change in the coefficients will lead to a small change in the roots.  Unfortunately, this is not the case here.The problem is ill-conditioned when the polynomial has a multiple root. For instance, the polynomial x2 has a double root at x = 0. However, the polynomial x2 − ε (a perturbation of size ε) has roots at ±√ε, which is much bigger than ε when ε is small.It is therefore natural to expect that ill-conditioning also occurs when the polynomial has zeros which are very close. However, the problem may also be extremely ill-conditioned for polynomials with well-separated zeros. Wilkinson used the polynomial w(x) to illustrate this point (Wilkinson 1963).In 1984, he described the personal impact of this discovery:Wilkinson's polynomial is often used to illustrate the undesirability of naively computing eigenvalues of a matrix by first calculating the coefficients of the matrix's characteristic polynomial and then finding its roots, since using the coefficients as an intermediate step may introduce an extreme ill-conditioning even if the original problem was well conditioned.[2]Wilkinson's polynomialclearly has 20 roots, located at x = 1, 2, …, 20. These roots are far apart. However, the polynomial is still very ill-conditioned.Expanding the polynomial, one findsIf the coefficient of x19 is decreased from −210 by 2−23 to −210.0000001192, then the polynomial value w(20) decreases from 0 to −2−232019 = −6.25×1017, and the root at x = 20 grows to x ≈ 20.8 . The roots at x = 18 and x = 19 collide into a double root at  x ≈ 18.62 which turns into a pair of complex conjugate roots at x ≈ 19.5 ± 1.9i as the perturbation increases further. The 20 roots become (to 5 decimals)Some of the roots are greatly displaced, even though the change to the coefficient is tiny and the original roots seem widely spaced. Wilkinson showed by the stability analysis discussed in the next section that this behavior is related to the fact that some roots α (such as α = 15) have many roots β that are "close" in the sense that |α − β| is smaller than |α|.Wilkinson chose the perturbation of 2−23 because his Pilot ACE computer had 30-bit floating point significands, so for numbers around 210, 2−23 was an error in the first bit position not represented in the computer. The two real numbers, −210 and −210 − 2−23, are represented by the same floating point number, which means that 2−23 is the unavoidable error in representing a real coefficient close to −210 by a floating point number on that computer. The perturbation analysis shows that 30-bit coefficient precision is insufficient for separating the roots of Wilkinson's polynomial.Suppose that we perturb a polynomial p(x) = Π (x − αj)with roots αj by adding a small multiple t·c(x) of a polynomial c(x), and ask how this affects the roots αj. To first order, the change in the roots will be controlled by the derivativeWhen the derivative is large, the roots will be less stable under variations of t, and conversely if this derivative is small the roots will be stable. In particular,if αj is a multiple root, then the denominator vanishes. In this case, αj is usually not differentiable with respect to t (unless c happens to vanish there), and the roots will be extremely unstable.For small values of t the perturbed root is given by the power series expansion in tand one expects problems when |t| is larger than the radius of convergence of this power series, which is given by the smallest value of |t| such that the root αj becomes multiple. A very crude estimate for this radius takes half the distance from αj to the nearest root, and divides by the derivative above.In the example of Wilkinson's polynomial of degree 20, the roots are given by αj = j for j = 1, …, 20, and c(x) is equal to x19.So the derivative is given byThis shows that the root αj will be less stable if there are many rootsαk close to αj, in the sense that the distance|αj − αk| between them is smaller than |αj|.Example. For the root α1 = 1, the derivative is equal to1/19! which is very small; this root is stable even for large changes in t. This is because all the other roots β are a long way from it, in the sense that |α1 − β| = 1, 2, 3, ..., 19 is larger than |α1| = 1.For example even if t is as large as –10000000000, the root α1 only changes from 1 to about  0.99999991779380 (which is very close to the first order approximation 1 + t/19! ≈ 0.99999991779365). Similarly, the other small roots of Wilkinson's polynomial are insensitive to changes in t.Example. On the other hand, for the root α20 = 20, the derivative is equal to −2019/19!  which is huge (about 43000000), so this root is very sensitive to small changes in t. The other roots β are close to α20, in the sense that |β − α20| = 1, 2, 3, ..., 19 is less than |α20| = 20. For t = −2 − 23 the first-order approximation 20 − t·2019/19! = 25.137... to the perturbed root 20.84... is terrible; this is even more obvious for the root α19 where the perturbed root has a large imaginary part but the first-order approximation (and for that matter all higher-order approximations) are real. The reason for this discrepancy is that |t|  ≈ 0.000000119 is greater than the radius of convergence of the power series mentioned above (which is about 0.0000000029, somewhat smaller than the value  0.00000001 given by the crude estimate) so the linearized theory does not apply. For a value such as t = 0.000000001 that is significantly smaller than this radius of convergence, the first-order approximation  19.9569... is reasonably close to the root  19.9509...At first sight the roots α1 = 1 and α20 = 20 of Wilkinson's polynomial appear to be similar, as they are on opposite ends of a symmetric line of roots, and have the same set of distances 1, 2, 3, ..., 19 from other roots. However the analysis above shows that this is grossly misleading: the root α20 = 20 is less stable than α1 = 1  (to small perturbations in the coefficient of x19) by a factor of 2019 = 5242880000000000000000000.The second example considered by Wilkinson isThe twenty zeros of this polynomial are in a geometric progression with common ratio 2, and hence the quotientcannot be large. Indeed, the zeros of w2 are quite stable to large relative changes in the coefficients.The expansionexpresses the polynomial in a particular basis, namely that of the monomials. If the polynomial is expressed in another basis, then the problem of finding its roots may cease to be ill-conditioned. For example, in a Lagrange form, a small change in one (or several) coefficients need not change the roots too much.  Indeed, the basis polynomials for interpolation at the points 0, 1, 2, …, 20 areEvery polynomial (of degree 20 or less) can be expressed in this basis:For Wilkinson's polynomial, we findGiven the definition of the Lagrange basis polynomial ℓ0(x), a change in the coefficient d0 will produce no change in the roots of w.  However, a perturbation in the other coefficients (all equal to zero) will slightly change the roots. Therefore, Wilkinson's polynomial is well-conditioned in this basis.Wilkinson discussed "his" polynomial inIt is mentioned in standard text books in numerical analysis, likeOther references:A high-precision numerical computation is presented in:
Horner's method
In mathematics, Horner's rule (also known as Horner scheme or Horner's method)[1][2] is a way of expressing and evaluating polynomials, which optimizes the needed number of arithmetic operations. More precisely Horner's rule consists of expressing the polynomialasThis allows evaluating a polynomial of degree n with only n – 1 multiplications and n – 1 additions. This is optimal, since there are polynomials of degree n that cannot be evaluated with fewer arithmetic operations.[citation needed]Horner's method may also refer to the use of Horner's rule in the process of solving a polynomial equation with Newton's method.[3] These methods are named after the British mathematician William George Horner, although they were known before him by Paolo Ruffini[4] , six hundred years earlier, by the Chinese mathematician Qin Jiushao[5] and seven hundred years earlier, by the Persian mathematician Sharaf al-Dīn al-Ṭūsī.[6]Given the polynomialTo accomplish this, we define a new sequence of constants as follows:To see why this works, note that the polynomial can be written in the formWe use synthetic division as follows:The third row is the sum of the first two rows, divided by 2. Each entry in the second row is the product of 1 with the third-row entry to the left. The answer isHorner's method is a fast, code-efficient method for multiplication and division of binary numbers on a microcontroller with no hardware multiplier.  One of the binary numbers to be multiplied is represented as a trivial polynomial, where (using the above notation) ai = 1, and x = 2.  Then, x (or x to some power) is repeatedly factored out.  In this binary numeral system (base 2), x = 2, so powers of 2 are repeatedly factored out.For example, to find the product of two numbers (0.15625) and m:To find the product of two binary numbers d and m:At this stage in the algorithm, it is required that terms with zero-valued coefficients are dropped, so that only binary coefficients equal to one are counted, thus the problem of multiplication or division by zero is not an issue, despite this implication in the factored equation:The denominators all equal one (or the term is absent), so this reduces toor equivalently (as consistent with the "method" described above)In binary (base-2) math, multiplication by a power of 2 is merely a register shift operation.  Thus, multiplying by 2 is calculated in base-2 by an arithmetic shift.  The factor (2−1) is a right arithmetic shift, a (0) results in no operation (since 20 = 1 is the multiplicative identity element), and a (21) results in a left arithmetic shift.The multiplication product can now be quickly calculated using only arithmetic shift operations, addition and subtraction.The method is particularly fast on processors supporting a single-instruction shift-and-addition-accumulate.  Compared to a C floating-point library, Horner's method sacrifices some accuracy, however it is nominally 13 times faster (16 times faster when the "canonical signed digit" (CSD) form is used) and uses only 20% of the code space.[7]These two steps are repeated until all real zeros are found for the polynomial. If the approximated zeros are not precise enough, the obtained values can be used as initial guesses for Newton's method but using the full polynomial rather than the reduced polynomials.[8]Consider the polynomialwhich can be expanded towhich is shown in yellow. The zero for this polynomial is found at 2 again using Newton's method and is circled in yellow. Horner's method is now used to obtainwhich is shown in green and found to have a zero at −3. This polynomial is further reduced toThe following Octave code was used in the example above to implement Horner's method.The following Python code implements Horner's method.The following C code implements Horner's method.Here is a slightly optimized version using explicit fused Multiply–accumulate operation, often execute faster than the above when running on a computer built with a processor supporting FMA instruction:The following C# code implements Horner's method.Evaluation using the monomial form of a degree-n polynomial requires at most n additions and (n2 + n)/2 multiplications, if powers are calculated by repeated multiplication and each monomial is evaluated individually.  (This can be reduced to n additions and 2n − 1 multiplications by evaluating the powers of x iteratively.)  If numerical data are represented in terms of digits (or bits), then the naive algorithm also entails storing approximately 2n times the number of bits of x (the evaluated polynomial has approximate magnitude xn, and one must also store xn itself).  By contrast, Horner's method requires only n additions and n multiplications, and its storage requirements are only n times the number of bits of x. Alternatively, Horner's method can be computed with n fused multiply–adds.  Horner's method can also be extended to evaluate the first k derivatives of the polynomial with kn additions and multiplications.[10]Horner's method is optimal, in the sense that any algorithm to evaluate an arbitrary polynomial must use at least as many operations. Alexander Ostrowski proved in 1954 that the number of additions required is minimal.[11] Victor Pan proved in 1966 that the number of multiplications is minimal.[12] However, when x is a matrix, Horner's method is not optimal.[citation needed]This assumes that the polynomial is evaluated in monomial form and no preconditioning of the representation is allowed, which makes sense if the polynomial is evaluated only once. However, if preconditioning is allowed and the polynomial is to be evaluated many times, then faster algorithms are possible. They involve a transformation of the representation of the polynomial. In general, a degree-n polynomial can be evaluated using only ⌊n/2⌋+2 multiplications and n additions.[13]A disadvantage of Horner's rule is that all of the operations are sequentially dependent, so it is not possible to take advantage of instruction level parallelism on modern computers.  In most applications where the efficiency of polynomial evaluation matters, many low-order polynomials are evaluated simultaneously (for each pixel or polygon in computer graphics, or for each grid square in a numerical simulation), so it is not necessary to find parallelism within a single polynomial evaluation.If, however, one is evaluating a single polynomial of very high order, it may be useful to break it up as follows:More generally, the summation can be broken into k parts:where the inner summations may be evaluated using separate parallel instances of Horner's method.  This requires slightly more operations than the basic Horner's method, but allows k-way SIMD execution of most of them.proceed as follows[14]At completion, we haveHorner's paper entitled "A new method of solving numerical equations of all orders, by continuous approximation"[16] was read before the Royal Society of London, at its meeting on July 1, 1819, with Davies Gilbert, Vice-President and Treasurer, in the chair; this was the final meeting of the session before the Society adjorned for its Summer recess. When a sequel was read before the Society in 1823, it was again at the final meeting of the session. On both occasions, papers by James Ivory, FRS, were also read. In 1819, it was Horner's paper that got through to publication in the "Philosophical Transactions".[16] later in the year, Ivory's paper falling by the way, despite Ivory being a Fellow; in 1823, when a total of ten papers were read, fortunes as regards publication, were reversed. But Gilbert, who had strong connections with the West of England and may have had social contact with Horner, resident as Horner was in Bristol and Bath, published his own survey of Horner-type methods earlier in 1823.Horner's paper in Part II of Philosophical Transactions of the Royal Society of London for 1819 was warmly and expansively welcomed by a reviewer in the issue of The Monthly Review: or, Literary Journal for April, 1820; in comparison, a technical paper by Charles Babbage is dismissed curtly in this review. However, the reviewer noted that another, similar method had also recently been published by the architect and mathematical expositor, Peter Nicholson. This theme is developed in a further review of some of Nicholson's books in the issue of The Monthly Review for December, 1820, which in turn ends with notice of the appearance of a booklet by Theophilus Holdred, from whom Nicholson acknowledges he obtained the gist of his approach in the first place, although claiming to have improved upon it. The sequence of reviews is concluded in the issue of The Monthly Review for September, 1821, with the reviewer concluding that whereas Holdred was the first person to discover a direct and general practical solution of numerical equations, he had not reduced it to its simplest form by the time of Horner's publication, and saying that had Holdred published forty years earlier when he first discovered his method, his contribution could be more easily recognized. The reviewer is exceptionally well-informed, even having sighted Horner's preparatory correspondence with Peter Barlow in 1818, seeking work of Budan. The Bodlean Library, Oxford has the Editor's annotated copy of The Monthly Review from which it is clear that the most active reviewer in mathematics in 1814 and 1815 (the last years for which this information has been published) was none other than Peter Barlow,one of the foremost specialists on approximation theory of the period, suggesting that it was Barlow, who wrote this sequence of reviews. As it also happened, Henry Atkinson, of Newcastle, devised a similar approximation scheme in 1809; he had consulted his fellow Geordie, Charles Hutton, another specialist and a senior colleague of Barlow at the Royal Military Academy, Woolwich, only to be advised that, while his work was publishable, it was unlikely to have much impact. J. R. Young, writing in the mid-1830s, concluded that Holdred's first method replicated Atkinson's while his improved method was only added to Holdred's booklet some months after its first appearance in 1820, when Horner's paper was already in circulation.The feature of Horner's writing that most distinguishes it from his English contemporaries is the way he draws on the Continental literature, notably the work of Arbogast. The advocacy, as well as the detraction, of Horner's Method has this as an unspoken subtext. Quite how he gained that familiarity has not been determined. Horner is known to have made a close reading of John Bonneycastle's book on algebra. Bonneycastle recognizes that Arbogast has the general, combinatorial expression for the reversion of series, a project going back at least to Newton. But Bonneycastle's main purpose in mentioning Arbogast is not to praise him, but to observe that Arbogast's notation is incompatible with the approach he adopts. The gap in Horner's reading was the work of Paolo Ruffini, except that, as far as awareness of Ruffini goes, citations of Ruffini's work by authors, including medical authors, in Philosophical Transactions speak volumes: there are none - Ruffini's name only appears in 1814, recording a work he donated to the Royal Society. Ruffini might have done better if his work had appeared in French, as had Malfatti's Problem in the reformulation of Joseph Diaz Gergonne, or had he written in French, as had Antonio Cagnoli, a source quoted by Bonneycastle on series reversion (today, Cagnoli is in the Italian Wikipedia, as shown, but has yet to make it into either French or English).Fuller[17] showed that the method in Horner's 1819 paper differs from what afterwards became known as 'Horner's method' and that in consequence the priority for this method should go to Holdred (1920). This view may be compared with the remarks concerning the works of Horner and Holdred in the previous paragraph. Fuller also takes aim at Augustus De Morgan. Precocious though Augustus de Morgan was, he was not the reviewer for The Monthly Review, while several others - Thomas Stephens Davies, J. R. Young, Stephen Fenwick, T. T. Wilkinson - wrote Horner firmly into their records, not least Horner himself, as he published extensively up until the year of his death in 1837. His paper in 1819 was one that would have been difficult to miss. In contrast, the only other mathematical sighting of Holdred is a single named contribution to The Gentleman's Mathematical Companion, an answer to a problem.It is questionable to what extent it was De Morgan's advocacy of Horner's priority in discovery[4][18] that led to "Horner's method" being so called in textbooks, but it is true that those suggesting this tend themselves to know of Horner largely through intermediaries, of whom De Morgan made himself a prime example. However, this method qua method was known long before Horner. In reverse chronological order, Horner's method was already known to:However, this observation on its own masks significant differences in conception and also, as noted with Ruffini's work, issues of accessibility.Qin Jiushao, in his Shu Shu Jiu Zhang (Mathematical Treatise in Nine Sections; 1247), presents a portfolio of methods of Horner-type for solving polynomial equations, which was based on earlier works of the 11th century Song dynasty mathematician Jia Xian; for example, one method is specifically suited to bi-quintics, of which Qin gives an instance, in keeping with the then Chinese custom of case studies. The first person writing in English to note the connection with Horner's method was Alexander Wylie, writing in The North China Herald in 1852; perhaps conflating and misconstruing different Chinese phrases, Wylie calls the method Harmoniously Alternating Evolution (which does not agree with his Chinese, linglong kaifang, not that at that date he uses pinyin), working the case of one of Qin's quartics and giving, for comparison, the working with Horner's method. Yoshio Mikami in Development of Mathematics in China and Japan published in Leipzig in 1913, gave a detailed description of Qin's method, using the quartic illustrated to the above right in a worked example; he wrote: "who can deny the fact of Horner's illustrious process being used in China at least nearly six long centuries earlier than in Europe ... We of course don't intend in any way to ascribe Horner's invention to a Chinese origin, but the lapse of time sufficiently makes it not altogether impossible that the Europeans could have known of the Chinese method in a direct or indirect way.".[21] However, as Mikami is also aware, it was not altogether impossible that a related work, Si Yuan Yu Jian (Jade Mirror of the Four Unknowns; 1303) by Zhu Shijie might make the shorter journey across to Japan, but seemingly it never did, although another work of Zhu, Suan Xue Qi Meng, had a seminal influence on the development of traditional mathematics in the Edo period, starting in the mid-1600s. Ulrich Libbrecht (at the time teaching in school, but subsequently a professor of comparative philosophy) gave a detailed description in his doctoral thesis of Qin's method, he concluded: It is obvious that this procedure is a Chinese invention....the method was not known in India. He said, Fibonacci probably learned of it from Arabs, who perhaps borrowed from the Chinese.[22] Here, the problems is that there is no more evidence for this speculation than there is of the method being known in India. Of course, the extraction of square and cube roots along similar lines is already discussed by Liu Hui in connection with Problems IV.16 and 22 in Jiu Zhang Suan Shu, while Wang Xiaotong in the 7th century supposes his readers can solve cubics by an approximation method described in his book Jigu Suanjing.
Matrix (mathematics)
In mathematics, a matrix (plural: matrices) is a rectangular array[1] of numbers, symbols, or expressions, arranged in rows and columns.[2][3] For example, the dimensions of the matrix below are 2 × 3 (read "two by three"), because there are two rows and three columns:The individual items in an m × n matrix A, often denoted by ai,j, where max i = m and max j = n, are called its elements or entries.[4] Provided that they have the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for Am,n × Bn,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to manipulate 3D models and project them onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. Matrices are used in economics to describe systems of economic relationships.A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.A matrix is a rectangular array of numbers or other mathematical objects for which operations such as addition and multiplication are defined.[6]  Most commonly, a matrix over a field F is a rectangular array of scalars each of which is a member of F.[7][8] Most of this article focuses on real and complex matrices, that is, matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m × n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 × 2 matrix.Matrices with a single row are called row vectors, and those with a single column are called column vectors. A matrix with the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.Matrices are commonly written in box brackets or parentheses:The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i − j. In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m × n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m × n as a subscript. For instance, the matrix A above is 3 × 4 and can be defined as A = [i − j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i − j]3×4.Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-×-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n − 1.[9] This article follows the more common convention in mathematical writing where enumeration starts from 1.An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai,∗ refers to the ith row of A, and a∗,j refers to the jth column of A. The set of all m-by-n matrices is denoted 𝕄(m, n).There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[11]This operation is called scalar multiplication, but its result is not named "scalar product" to avoid confusion, since "scalar product" is sometimes used as a synonym for "inner product".Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A + B = B + A.[12]The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:where 1 ≤ i ≤ m and 1 ≤ j ≤ p.[13] For example, the underlined entry 2340 in the product is calculated as (2 × 1000) + (3 × 100) + (4 × 10) = 2340:Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A + B)C = AC + BC as well as C(A + B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[14] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they need not be equal, that is, generally that is, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:whereasBesides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.[15] They arise in solving matrix equations such as the Sylvester equation.There are three types of row operations:These operations are used in a number of ways, including solving linear equations and finding matrix inverses.A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[16][17][18] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[18][19]A principal submatrix is a square submatrix obtained by removing certain rows and columns.  The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[20][21] Other authors define a principal submatrix as one in which the first k rows and columns, for some number k, are the ones that remain;[22] this type of submatrix has also been called a leading principal submatrix.[23]Matrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n×1-matrix) of n variables x1, x2, ..., xn, and b is an m×1-column vector, then the matrix equationis equivalent to the system of linear equations[24]Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, this can be done by writingwhere A−1 is the inverse matrix of A. If A has no inverse, solutions if any can be found using its generalized inverse.Matrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn → Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.For example, the 2×2 matrixThe following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[25] if a k-by-m matrix B represents another linear map g : Rm → Rk, then the composition g ∘ f is represented by BA sinceThe last equality follows from the above-mentioned associativity of matrix multiplication.The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[26] Equivalently it is the dimension of the image of the linear map represented by A.[27] The rank–nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[28]A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line that runs from the top left corner to the bottom right corner of the matrix.If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged: A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix.  If instead, A is equal to the negative of its transpose, that is, A = −AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A∗ = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[29] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.A square matrix A is called invertible or non-singular if there exists a matrix B such thatwhere In is the n×n identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A−1.A symmetric n×n-matrix A is called positive-definite if the associated quadratic form has a positive value for every nonzero vector x in Rn. If f (x) takes only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[32] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[33] The table at the right shows two possibilities for 2-by-2 matrices.Allowing as input two different vectors instead yields the bilinear form associated to A:An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:which entailswhere I is the identity matrix of size n.An orthogonal matrix A is necessarily invertible (with inverse A−1 = AT), unitary (A−1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or −1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.The complex analogue of an orthogonal matrix is a unitary matrix.The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors: This is immediate from the definition of matrix multiplication:It follows that the trace of the product of more than two matrices is independent of cyclic permutations of the matrices, however this does not in general apply for arbitrary permutations (for example, tr(ABC) ≠ tr(BAC), in general). Also, the trace of a matrix is equal to that of its transpose, that is,The determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.The determinant of 2-by-2 matrices is given byThe determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]The determinant of a product of square matrices equals the product of their determinants: Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by −1.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]A number λ and a non-zero vector v satisfyingare called an eigenvalue and an eigenvector of A, respectively.[40][41] The number λ is an eigenvalue of an n×n-matrix A if and only if A−λIn is not invertible, which is equivalent toThe polynomial pA in an indeterminate X given by evaluation the determinant det(XIn−A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(λ) = 0 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the Cayley–Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]To choose the most appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace expansion (Adj (A) denotes the adjugate matrix of A)may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[49]There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[50] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[51] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV∗, where U and V are unitary matrices and D is a diagonal matrix.The eigendecomposition or diagonalization expresses A as a product VDV−1, where D is a diagonal matrix and V is a suitable invertible matrix.[52]  If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues λ1 to λn of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[53] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated viaand the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[54] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[55]Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[56] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings  known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.More generally, matrices with entries in a ring R are widely used in mathematics.[57] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[58] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[59] Matrices over superrings are called supermatrices.[60]Matrices do not always have all their entries in the same ring – or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be square matrices, and thus need not be members of any ring; but their sizes must fulfil certain compatibility conditions.Linear maps Rn → Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V → W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such thatIn other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[61] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[62]More generally, the set of m×n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity.  When n = m composition of these maps is possible, and this gives rise to the matrix ring of n×n matrices representing the endomorphism ring of Rn.A group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[63] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[64][65] Since in a group every element must be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[66] Orthogonal matrices, determined by the conditionform the orthogonal group.[67] Every orthogonal matrix has determinant 1 or −1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[68] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[69]It is also possible to consider matrices with infinitely many rows and/or columns[70] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication, and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns does. Products of two matrices of the given type is well defined (provided that the column-index and row-index sets match), is of the same type, and corresponds to the composition of linear maps.If R is a normed ring, then the condition of row or column finiteness can be relaxed.  With the norm in place, absolutely convergent series can be used instead of finite sums.  For example, the matrices whose column sums are absolutely convergent sequences form a ring.  Analogously, the matrices whose row sums are absolutely convergent series also form a ring.Infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that must be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[71] and the abstract and more powerful tools of functional analysis can be used instead.An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[72][73] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[74] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[75]Complex numbers can be represented by particular real 2-by-2 matrices viaunder which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[76] and Clifford algebras in general.Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[77] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[78] Matrices over a polynomial ring are important in the study of control theory.Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree–Fock method.The adjacency matrix of a finite graph is a basic notion of graph theory.[79] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[80] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.The Hessian matrix of a differentiable function ƒ: Rn → R consists of the second derivatives of ƒ with respect to the several coordinate directions, that is,[81]Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn → Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as [83]If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[84]Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.[85]The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.[86]Stochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[87] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[88]Statistics also makes use of matrices in many different forms.[89] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[90] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), …, (xN, yN), by a linear functionwhich can be formulated in terms of matrices, related to the singular value decomposition of matrices.[91]Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[92][93]Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group.  Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[94] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo–Kobayashi–Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[95]The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[96] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[97]Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states.  The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[98]A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[99] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[100]Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies.The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[101]Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by  B = H · A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th–2nd century BCE is the first example of the use of array methods to solve simultaneous equations,[102] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano brought the method to Europe when he published Ars Magna.[103] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[104]  The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[105]  Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[103] Cramer presented his rule in 1750.The term "matrix" (Latin for "womb", derived from mater—mother[106]) was coined by James Joseph Sylvester in 1850,[107] who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[103]  Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[109][110] in which he proposed and demonstrated the Cayley–Hamilton theorem.[103]An English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[103]The modern study of determinants sprang from several sources.[111] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy − 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomialwhere Π denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[112] Jacobi studied "functional determinants"—later called Jacobi determinants by Sylvester—which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen über die Theorie der Determinanten[113] and Weierstrass' Zur Determinantentheorie,[114] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.Many theorems were first established for small matrices only, for example the Cayley–Hamilton theorem was proved for 2×2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4×4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss–Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra, [115] partially due to their use in classification of the hypercomplex number systems of the previous century.The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[116] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.The word has been used in unusual ways by at least two authors of historical importance.Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910–1913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:For example, a function Φ(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ∀ai: Φ(ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[118]
Sequence
In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed.  Like a set, it contains members (also called elements, or terms).  The number of elements (possibly infinite) is called the length of the sequence.  Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order matters.  Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first n natural numbers (for a sequence of finite length n).  The position of an element in a sequence is its rank or index; it is the natural number from which the element is the image. It depends on the context or a specific convention, if the first element has index 0 or 1.  When a symbol has been chosen for denoting a sequence, the nth element of the sequence is denoted by this symbol with n as subscript; for example, the nth element of the Fibonacci sequence is generally denoted Fn.For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last.  This sequence differs from (A, R, M, Y).  Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence.  Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...).  In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams.  The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.A sequence can be thought of as a list of elements with a particular order.  Sequences are useful in a number of mathematical disciplines for studying functions, spaces, and other mathematical structures using the convergence properties of sequences.  In particular, sequences are the basis for series, which are important in differential equations and analysis.  Sequences are also of interest in their own right and can be studied as patterns or puzzles, such as in the study of prime numbers.There are a number of ways to denote a sequence, some of which are more useful for specific types of sequences.  One way to specify a sequence is to list the elements.  For example, the first four odd numbers form the sequence (1, 3, 5, 7).  This notation can be used for infinite sequences as well.  For instance, the infinite sequence of positive odd integers can be written (1, 3, 5, 7, ...).  Listing is most useful for infinite sequences with a pattern that can be easily discerned from the first few elements.  Other ways to denote a sequence are discussed after the examples.The prime numbers are the natural numbers bigger than 1 that have no divisors but 1 and themselves.  Taking these in their natural order gives the sequence (2, 3, 5, 7, 11, 13, 17, ...).  The prime numbers are widely used in mathematics and specifically in number theory.The Fibonacci numbers are the integer sequence whose elements are the sum of the previous two elements.  The first two elements are either 0 and 1 or 1 and 1 so that the sequence is (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...).For a large list of examples of integer sequences, see On-Line Encyclopedia of Integer Sequences.Other examples of sequences include ones made up of rational numbers, real numbers, and complex numbers.  The sequence (.9, .99, .999, .9999, ...) approaches the number 1.  In fact, every real number can be written as the limit of a sequence of rational numbers, e.g. via its decimal expansion.  For instance, π is the limit of the sequence (3, 3.1, 3.14, 3.141, 3.1415, ...).  A related sequence is the sequence of decimal digits of π, i.e. (3, 1, 4, 1, 5, 9, ...).  This sequence does not have any pattern that is easily discernible by eye, unlike the preceding sequence, which is increasing.In some cases the elements of the sequence are related naturally to a sequence of integers whose pattern can be easily inferred.  In these cases the index set may be implied by a listing of the first few abstract elements.  For instance, the sequence of squares of odd numbers could be denoted in any of the following ways.Sequences whose elements are related to the previous elements in a straightforward way are often defined using recursion.  This is in contrast to the definition of sequences of elements as functions of their positions.To define a sequence by recursion, one needs a rule to construct each element in terms of the ones before it.  In addition, enough initial elements must be provided so that all subsequent elements of the sequence can be computed by the rule.  The principle of mathematical induction can be used to prove that in this case, there is exactly one sequence that satisfies both the recursion rule and the initial conditions.  Induction can also be used to prove properties about a sequence, especially for sequences whose most natural description is recursive.The Fibonacci sequence can be defined using a recursive rule along with two initial elements.  The rule is that each element is the sum of the previous two elements, and the first two elements are 0 and 1.The first ten terms of this sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34.  A more complicated example of a sequence that is defined recursively is Recaman's sequence.[1]  We can define Recaman's sequence byNot all sequences can be specified by a rule in the form of an equation, recursive or not, and some can be quite complicated.  For example, the sequence of prime numbers is the set of prime numbers in their natural order, i.e. (2, 3, 5, 7, 11, 13, 17, ...).There are many different notions of sequences in mathematics, some of which (e.g., exact sequence) are not covered by the definitions and notations introduced below.For the purposes of this article, we define a sequence to be a function whose domain is an interval of integers.  This definition covers several different uses of the word "sequence", including one-sided infinite sequences, bi-infinite sequences, and finite sequences (see below for definitions).  However, many authors use a narrower definition by requiring the domain of a sequence to be the set of natural numbers.  The narrower definition has the disadvantage that it rules out finite sequences and bi-infinite sequences, both of which are usually called sequences in standard mathematical practice.   In some contexts, to shorten exposition, the codomain of the sequence is fixed by context, for example by requiring it to be the set R of real numbers,[2] the set C of complex numbers,[3] or a topological space.[4]Sequences and their limits (see below) are important concepts for studying topological spaces.  An important generalization of sequences is the concept of nets.  A net is a function from a (possibly uncountable) directed set to a topological space.  The notational conventions for sequences normally apply to nets as well.The length of a sequence is defined as the number of terms in the sequence.A sequence of a finite length n is also called an n-tuple.  Finite sequences include the empty sequence ( ) that has no elements.The terms nondecreasing and nonincreasing are often used in place of increasing and decreasing in order to avoid any possible confusion with strictly increasing and strictly decreasing, respectively.If the sequence of real numbers (an) is such that all the terms are less than some real number M, then the sequence is said to be bounded from above.  In other words, this means that there exists M such that for all n, an ≤ M.  Any such M is called an upper bound.  Likewise, if, for some real m, an ≥ m for all n greater than some N, then the sequence is bounded from below and any such m is called a lower bound.  If a sequence is both bounded from above and bounded from below, then the sequence is said to be bounded.A subsequence of a given sequence is a sequence formed from the given sequence by deleting some of the elements without disturbing the relative positions of the remaining elements.  For instance, the sequence of positive even integers (2, 4, 6, ...) is a subsequence of the positive integers (1, 2, 3, ...).  The positions of some elements change when other elements are deleted.  However, the relative positions are preserved.Some other types of sequences that are easy to define include:An important property of a sequence is convergence.  If a sequence converges, it converges to a particular value known as the limit. If a sequence converges to some limit, then it is convergent. A sequence that does not converge is divergent.Moreover:A Cauchy sequence is a sequence whose terms become arbitrarily close together as n gets very large.  The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis.  One particularly important result in real analysis is Cauchy characterization of convergence for sequences:In contrast, there are Cauchy sequences of rational numbers that are not convergent in the rationals, e.g. the sequence defined byx1 = 1 and xn+1 = xn + 2/xn/2is Cauchy, but has no rational limit, cf. here.  More generally, any sequence of rational numbers that converges to an irrational number is Cauchy, but not convergent when interpreted as a sequence in the set of rational numbers.Metric spaces that satisfy the Cauchy characterization of convergence for sequences are called complete metric spaces and are particularly nice for analysis.In this case we say that the sequence diverges, or that it converges to infinity. An example of such a sequence is an = n.and say that the sequence diverges or converges to negative infinity.Sequences play an important role in topology, especially in the study of metric spaces.  For instance:Sequences can be generalized to nets or filters.  These generalizations allow one to extend some of the above theorems to spaces without metrics.The topological product of a sequence of topological spaces is the cartesian product of those spaces, equipped with a natural topology called the product topology.In analysis, when talking about sequences, one will generally consider sequences of the formwhich is to say, infinite sequences of elements indexed by natural numbers.It may be convenient to have the sequence start with an index different from 1 or 0.  For example, the sequence defined by xn = 1/log(n) would be defined only for n ≥ 2.  When talking about such infinite sequences, it is usually sufficient (and does not change much for most considerations) to assume that the members of the sequence are defined at least for all indices large enough, that is, greater than some given N.The most elementary type of sequences are numerical ones, that is, sequences of real or complex numbers.  This type can be generalized to sequences of elements of some vector space.  In analysis, the vector spaces considered are often function spaces.  Even more generally, one can study sequences with elements in some topological space.A sequence space is a vector space whose elements are infinite sequences of real or complex numbers.  Equivalently, it is a function space whose elements are functions from the natural numbers to the field K, where K is either the field of real numbers or the field of complex numbers.  The set of all such functions is naturally identified with the set of all possible infinite sequences with elements in K, and can be turned into a vector space under the operations of pointwise addition of functions and pointwise scalar multiplication.  All sequence spaces are linear subspaces of this space.  Sequence spaces are typically equipped with a norm, or at least the structure of a topological vector space.The most important sequences spaces in analysis are the ℓp spaces, consisting of the p-power summable sequences, with the p-norm.  These are special cases of Lp spaces for the counting measure on the set of natural numbers.  Other important classes of sequences like convergent sequences or null sequences form sequence spaces, respectively denoted c and c0, with the sup norm.  Any sequence space can also be equipped with the topology of pointwise convergence, under which it becomes a special kind of Fréchet space called an FK-space.Sequences over a field may also be viewed as vectors in a vector space.  Specifically, the set of F-valued sequences (where F is a field) is a function space (in fact, a product space) of F-valued functions over the set of natural numbers.Abstract algebra employs several types of sequences, including sequences of mathematical objects such as groups or rings.If A is a set, the free monoid over A (denoted A*, also called Kleene star of A) is a monoid containing all the finite sequences (or strings) of zero or more elements of A, with the binary operation of concatenation.  The free semigroup A+ is the subsemigroup of A* containing all elements except the empty sequence.In the context of group theory, a sequenceof groups and group homomorphisms is called exact, if the image (or range) of each homomorphism is equal to the kernel of the next:Note that the sequence of groups and homomorphisms may be either finite or infinite.A similar definition can be made for certain other algebraic structures.  For example, one could have an exact sequence of vector spaces and linear maps, or of modules and module homomorphisms.In homological algebra and algebraic topology, a spectral sequence is a means of computing homology groups by taking successive approximations.  Spectral sequences are a generalization of exact sequences, and since their introduction by Jean Leray (1946), they have become an important research tool, particularly in homotopy theory.An ordinal-indexed sequence is a generalization of a sequence.  If α is a limit ordinal and X is a set, an α-indexed sequence of elements of X is a function from α to X.  In this terminology an ω-indexed sequence is an ordinary sequence.Automata or finite state machines can typically be thought of as directed graphs, with edges labeled using some specific alphabet, Σ.  Most familiar types of automata transition from state to state by reading input letters from Σ, following edges with matching labels; the ordered input for such an automaton forms a sequence called a word (or input word).  The sequence of states encountered by the automaton when processing a word is called a run.  A nondeterministic automaton may have unlabeled or duplicate out-edges for any state, giving more than one successor for some input letter.  This is typically thought of as producing multiple possible runs for a given word, each being a sequence of single states, rather than producing a single run that is a sequence of sets of states; however, 'run' is occasionally used to mean the latter.Infinite sequences of digits (or characters) drawn from a finite alphabet are of particular interest in theoretical computer science.  They are often referred to simply as sequences or streams, as opposed to finite strings.  Infinite binary sequences, for instance, are infinite sequences of bits (characters drawn from the alphabet {0, 1}).  The set C = {0, 1}∞ of all infinite binary sequences is sometimes called the Cantor space.An infinite binary sequence can represent a formal language (a set of strings) by setting the n th bit of the sequence to 1 if and only if the n th string (in shortlex order) is in the language.  This representation is useful in the diagonalization method for proofs.[8]
Polynomial
In mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate, x, is x2 − 4x + 7. An example in three variables is x3 + 2xyz2 − yz + 1.Polynomials appear in many areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.The word polynomial joins two diverse roots: the Greek poly, meaning "many," and the Latin nomen, or name. It was derived from the term binomial by replacing the Latin root bi- with the Greek poly-. The word polynomial was first used in the 17th century.[1]The x occurring in a polynomial is commonly called either a variable or an indeterminate. When the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is "indeterminate"). It is thus more correct to call it an "indeterminate".[citation needed] However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a "variable". Many authors use these two words interchangeably.It is a common usage to use uppercase letters for the indeterminates and the corresponding lowercase letters for the variables (arguments) of the associated function.A polynomial P in the indeterminate x is commonly denoted either as P or as P(x). Formally, the name of the polynomial is P, not P(x), but the use of the functional notation P(x) date from the time where the distinction between a polynomial and the associated function was unclear. Moreover the functional notation is often useful for specifying, in a single phrase, a polynomial and its indeterminate. For example, "let P(x) be a polynomial" is a shorthand for "let P be a polynomial in the indeterminate x". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.The ambiguity of having two notations for a single mathematical object may be formally resolved by considering the general meaning of the functional notation for polynomials.If a denotes a number, a variable, another polynomial, or, more generally any expression, then P(a) denotes, by convention, the result of substituting a for x in P. Thus, the polynomial P defines the functionwhich is the polynomial function associated to P.Frequently, when using this notation, one supposes that a is a number. However one may use it over any domain where addition and multiplication are defined (that is, any ring). In particular, if a is a polynomial then P(a) is also a polynomial.More specifically, when a is the indeterminate x, then the image of x by this function is the polynomial P itself (substituting x to x does not change anything). In other words,which justifies formally the existence of two notations for the same polynomial.A polynomial is an expression that can be built from constants and symbols called indeterminates or variables by means of addition, multiplication and exponentiation to a non-negative integer power. Two such expressions that may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication are considered as defining the same polynomial.A polynomial in a single indeterminate x can always be written (or rewritten) in the formThis can be expressed more concisely by using summation notation:That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number—called the coefficient of the term[2]—and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because x = x1, the degree of an indeterminate without a written exponent is one.A term with no indeterminates and a polynomial with no indeterminates are called, respectively, a constant term and a constant polynomial.[3] The degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial, 0, (which has no terms at all) is generally treated as not defined (but see below).[4]For example:is a term. The coefficient is −5, the indeterminates are x and y, the degree of x is two, while the degree of y is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is 2 + 1 = 3.Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.Polynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials. For higher degrees the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in x2 + 2x + 1 the term 2x is a linear term in a quadratic polynomial.The polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either −1 or −∞).[5] These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. The graph of the zero polynomial, f(x) = 0, is the X-axis.In the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of degree n if all its non-zero terms have degree n. The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined.[6] For example, x3y2 + 7x2y3 − 3x5 is homogeneous of degree 5. For more details, see homogeneous polynomial.The commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in "descending powers of x", with the term of largest degree first, or in "ascending powers of x". The polynomial in the example above is written in descending powers of x. The first term has coefficient 3, indeterminate x, and exponent 2. In the second term, the coefficient is −5. The third term is a constant. Because the degree of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.[7]Two terms with the same indeterminates raised to the same powers are called "similar terms" or "like terms", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0.[8] Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial,[9] a two-term polynomial is called a binomial, and a three-term polynomial is called a trinomial. The term "quadrinomial" is occasionally used for a four-term polynomial.A real polynomial is a polynomial with real coefficients. The argument of the polynomial is not necessarily so restricted, for instance the s-plane variable in Laplace transforms. A real polynomial function is a function from the reals to the reals that is defined by a real polynomial. Similarly, an integer polynomial is a polynomial with integer coefficients, and a complex polynomial is a polynomial with complex coefficients.A polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply "polynomials in x, y, and z", listing the indeterminates allowed.The evaluation of a polynomial consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:Polynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms.[8][10] For example, ifthenwhich can be simplified toTo work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other.[8] For example, ifthenwhich can be simplified toPolynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of f(x) by (x − a) is f(a); see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.As for the integers, two kinds of divisions are considered for the polynomials. The Euclidean division of polynomials that generalizes the Euclidean division of the integers. It results in two polynomials, a quotient and a remainder that are characterized by the following property of the polynomials: given two polynomials a and b such that b ≠ 0, there exists a unique pair of polynomials, q, the quotient, and r, the remainder, such that a = b q + r and degree(r) < degree(b) (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.[12]All polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree.[13] For example, the factored form ofisover the integers and the reals andover the complex numbers.The computation of the factored form, called factorization is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.A formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a "rational expression" or "rational fraction" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example, x3/12 is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to (1/12)x3 and 1/12 is just a constant. When this expression is used as a term, its coefficient is therefore 1/12. For similar reasons, if complex coefficients are allowed, one may have a single term like (2 + 3i) x3; even though it looks like it should be expanded to two terms, the complex number 2 + 3i is one complex number, and is the coefficient of that term. The expression 1/(x2 + 1) is not a polynomial because it includes division by a non-constant polynomial. The expression (5 + y)x is not a polynomial, because it contains an indeterminate used as exponent.Because subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.A polynomial function is a function that can be defined by evaluating a polynomial. More precisely, a function f of one argument from a given domain is a polynomial function if there exists a polynomialGenerally, unless otherwise specified, polynomial functions have complex coefficients, arguments, and values. In particular, a polynomial, restricted to have real coefficients, defines a function from the complex numbers to the complex numbers. If the domain of this function is also restricted to the reals, the resulting function maps reals to reals.For example, the function f, defined byis a polynomial function of one variable. Polynomial functions of several variables are similarly defined, using polynomials in more than one indeterminate, as inEvery polynomial function is continuous, smooth, and entire. Polynomial of degree 2:f(x) = x2 − x − 2= (x + 1)(x − 2)Polynomial of degree 3:f(x) = x3/4 + 3x2/4 − 3x/2 − 2= 1/4 (x + 4)(x + 1)(x − 2)Polynomial of degree 4:f(x) = 1/14 (x + 4)(x + 1)(x − 1)(x − 3) + 0.5Polynomial of degree 5:f(x) = 1/20 (x + 4)(x + 2)(x + 1 )(x − 1)(x − 3)+ 2Polynomial of degree 6:f(x) = 1/100 (x6 − 2x 5 − 26x4 + 28x3+ 145x2 - 26x - 80)Polynomial of degree 7:f(x) = (x − 3)(x − 2)(x − 1)(x)(x + 1)(x + 2)(x + 3)A polynomial function in one real variable can be represented by a graph.A non-constant polynomial function tends to infinity when the variable increases indefinitely (in absolute value). If the degree is higher than one, the graph does not have any asymptote. It has two parabolic branches with vertical direction (one branch for positive x and one for negative x).Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.A polynomial equation, also called algebraic equation, is an equation of the form[14]For example,is a polynomial equation.When considering equations, the indeterminates (variables) of polynomials are also called unknowns, and the solutions are the possible values of the unknowns for which the equality is true (in general more than one solution may exist). A polynomial equation stands in contrast to a polynomial identity like (x + y)(x − y) = x2 − y2, where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.In elementary algebra, methods such as the quadratic formula are taught for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel–Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.The number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.A number a is a root of a polynomial P if and only if the linear polynomial x − a divides P, that is if there is another polynomial Q such that P = (x – a) Q. It may happen that x − a divides P more than once: if (x − a)2 divides P then a is called a multiple root of P, and otherwise a is called a simple root of P. If P is a nonzero polynomial, there is a highest power m such that (x − a)m divides P, which is called the multiplicity of the root a in P. When P is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots, as, with the above definitions, every number is a root of the zero polynomial, with an undefined multiplicity. With this exception made, the number of roots of P, even counted with their respective multiplicities, cannot exceed the degree of P.[15] The relation between the coefficients of a polynomial and its roots is described by Vieta's formulas.Some polynomials, such as x2 + 1, do not have any roots among the real numbers. If, however, the set of accepted solutions is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors x − a, one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree 1; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.When there is no algebraic expression for the roots, and when such an algebraic expression exists but is too complicated to be useful, the unique way of solving is to compute numerical approximations of the solutions.[16] There are many methods for that; some are restricted to polynomials and others may apply to any continuous function. The most efficient algorithms allow solving easily (on a computer) polynomial equations of degree higher than 1,000 (see Root-finding algorithm).For polynomials in more than one indeterminate, the combinations of values for the variables for which the polynomial function takes the value zero are generally called zeros instead of "roots". The study of the sets of zeros of polynomials is the object of algebraic geometry. For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions, and, if this number is finite, for computing the solutions. See System of polynomial equations.The special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.A polynomial equation for which one is interested only in the solutions which are integers is called a Diophantine equation. Solving Diophantine equations is generally a very hard task. It has been proved that there cannot be any general algorithm for solving them, and even for deciding whether the set of solutions is empty (see Hilbert's tenth problem). Some of the most famous problems that have been solved during the fifty last years are related to Diophantine equations, such as Fermat's Last Theorem.There are several generalizations of the concept of polynomials.A trigonometric polynomial is a finite linear combination of functions sin(nx) and cos(nx) with n taking on the values of one or more natural numbers.[17] The coefficients may be taken as real numbers, for real-valued functions.If sin(nx) and cos(nx) are expanded in terms of sin(x) and cos(x), a trigonometric polynomial becomes a polynomial in the two variables sin(x) and cos(x) (using List of trigonometric identities#Multiple-angle formulae). Conversely, every polynomial in sin(x) and cos(x) may be converted, with Product-to-sum identities, into a linear combination of functions sin(nx) and cos(nx). This equivalence explains why linear combinations are called polynomials.For complex coefficients, there is no difference between such a function and a finite Fourier series.Trigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.A matrix polynomial is a polynomial with square matrices as variables.[18] Given an ordinary, scalar-valued polynomialthis polynomial evaluated at a matrix A iswhere I is the identity matrix.[19]A matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices A in a specified matrix ring Mn(R).Laurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.A rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.The rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.Formal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like irrational numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.The simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone–Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.Calculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial functionthe derivative with respect to x isand the indefinite integral isIn abstract algebra, one distinguishes between polynomials and polynomial functions. A polynomial f in one indeterminate x over a ring R is defined as a formal expression of the formwhere n is a natural number, the coefficients a0, . . ., an are elements of R, and x is a formal symbol, whose powers xi are just placeholders for the corresponding coefficients ai, so that the given formal expression is just a way to encode the sequence (a0, a1, . . .), where there is an n such that ai = 0 for all i > n. Two polynomials sharing the same value of n are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of n obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term aixi is interpreted as a polynomial that has zero coefficients at all powers of x other than xi. Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the ruleThus the set of all polynomials with coefficients in the ring R forms itself a ring, the ring of polynomials over R, which is denoted by R[x]. The map from R to R[x] sending r to rx0 is an injective homomorphism of rings, by which R is viewed as a subring of R[x]. If R is commutative, then R[x] is an algebra over R.One can think of the ring R[x] as arising from R by adding one new element x to R, and extending in a minimal way to a ring in which x satisfies no other relations than the obligatory ones, plus commutation with all elements of R (that is xr = rx). To do this, one must add all powers of x and their linear combinations as well.Formation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring R[x] over the real numbers by factoring out the ideal of multiples of the polynomial x2 + 1. Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring R (see modular arithmetic).If R is commutative, then one can associate to every polynomial P in R[x], a polynomial function f with domain and range equal to R (more generally one can take domain and range to be the same unital associative algebra over R). One obtains the value f(r) by substitution of the value r for the symbol x in P. One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p). This is not the case when R is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for x.In commutative algebra, one major focus of study is divisibility among polynomials. If R is an integral domain and f and g are polynomials in R[x], it is said that f divides g or f is a divisor of g if there exists a polynomial q in R[x] such that f q = g. One can show that every zero gives rise to a linear divisor, or more formally, if f is a polynomial in R[x] and r is an element of R such that f(r) = 0, then the polynomial (x − r) divides f. The converse is also true. The quotient can be computed using the polynomial long division.[20][21]If F is a field and f and g are polynomials in F[x] with g ≠ 0, then there exist unique polynomials q and r in F[x] withand such that the degree of r is smaller than the degree of g (using the convention that the polynomial 0 has a negative degree). The polynomials q and r are uniquely determined by f and g. This is called Euclidean division, division with remainder or polynomial long division and shows that the ring F[x] is a Euclidean domain.Analogously, prime polynomials (more correctly, irreducible polynomials) can be defined as non-zero polynomials which cannot be factorized into the product of two non-constant polynomials. In the case of coefficients in a ring, "non-constant" must be replaced by "non-constant or non-unit" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non-unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand-written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.Polynomials serve to approximate other functions,[22] such as the use of splines.Polynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.The term "polynomial", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase polynomial time means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.Determining the roots of polynomials, or "solving algebraic equations", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins "Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou." We would write 3x + 2y + z = 29.The earliest known use of the equal sign is in Robert Recorde's The Whetstone of Witte, 1557. The signs + for addition, − for subtraction, and the use of a letter for an unknown appear in Michael Stifel's Arithemetica integra, 1544. René Descartes, in La géometrie, 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the a's denote constants and x denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.[23]
General linear group
In mathematics, the general linear group of degree n is the set of n×n invertible matrices, together with the operation of ordinary matrix multiplication. This forms a group, because the product of two invertible matrices is again invertible, and the inverse of an invertible matrix is invertible. The group is so named because the columns of an invertible matrix are linearly independent, hence the vectors/points they define are in general linear position, and matrices in the general linear group take points in general linear position to points in general linear position.To be more precise, it is necessary to specify what kind of objects may appear in the entries of the matrix. For example, the general linear group over R (the set of real numbers) is the group of n×n invertible matrices of real numbers, and is denoted by GLn(R) or GL(n, R).More generally, the general linear group of degree n over any field F (such as the complex numbers), or a ring R (such as the ring of integers), is the set of n×n invertible matrices with entries from F (or R), again with matrix multiplication as the group operation.[1] Typical notation is GLn(F) or GL(n, F), or simply GL(n) if the field is understood.More generally still, the general linear group of a vector space GL(V) is the abstract automorphism group, not necessarily written as matrices.The special linear group, written SL(n, F) or SLn(F), is the subgroup of GL(n, F) consisting of matrices with a determinant of 1.The group GL(n, F) and its subgroups are often called linear groups or matrix groups (the abstract group GL(V) is a linear group but not a matrix group). These groups are important in the theory of group representations, and also arise in the study of spatial symmetries and symmetries of vector spaces in general, as well as the study of polynomials. The modular group may be realised as a quotient of the special linear group SL(2, Z).If n ≥ 2, then the group GL(n, F) is not abelian.If V is a vector space over the field F, the general linear group of V, written GL(V) or Aut(V), is the group of all automorphisms of V, i.e. the set of all bijective linear transformations V → V, together with functional composition as group operation. If V has finite dimension n, then GL(V) and GL(n, F) are isomorphic. The isomorphism is not canonical; it depends on a choice of basis in V. Given a basis (e1, ..., en) of V and an automorphism T in GL(V), we have then for every basis vector ei thatfor some constants aij in F; the matrix corresponding to T is then just the matrix with entries given by the aij.In a similar way, for a commutative ring R the group GL(n, R) may be interpreted as the group of automorphisms of a free R-module M of rank n. One can also define GL(M) for any R-module, but in general this is not isomorphic to GL(n, R) (for any n).Over a field F, a matrix is invertible if and only if its determinant is nonzero. Therefore, an alternative definition of GL(n, F) is as the group of matrices with nonzero determinant.Over a commutative ring R, more care is needed: a matrix over R is invertible if and only if its determinant is a unit in R, that is, if its determinant is invertible in R. Therefore, GL(n, R) may be defined as the group of matrices whose determinants are units.Over a non-commutative ring R, determinants are not at all well behaved. In this case, GL(n, R) may be defined as the unit group of the matrix ring M(n, R).The general linear group GL(n, R) over the field of real numbers is a real Lie group of dimension n2. To see this, note that the set of all n×n real matrices, Mn(R), forms a real vector space of dimension n2. The subset GL(n, R) consists of those matrices whose determinant is non-zero. The determinant is a polynomial map, and hence GL(n, R) is an open affine subvariety of Mn(R) (a non-empty open subset of Mn(R) in the Zariski topology), and therefore[2]a smooth manifold of the same dimension.As a manifold, GL(n, R) is not connected but rather has two connected components: the matrices with positive determinant and the ones with negative determinant. The identity component, denoted by GL+(n, R), consists of the real n×n matrices with positive determinant. This is also a Lie group of dimension n2; it has the same Lie algebra as GL(n, R).The group GL(n, R) is also noncompact. "The"[3] maximal compact subgroup of GL(n, R) is the orthogonal group O(n), while "the" maximal compact subgroup of GL+(n, R) is the special orthogonal group SO(n). As for SO(n), the group GL+(n, R) is not simply connected (except when n = 1), but rather has a fundamental group isomorphic to Z for n = 2 or Z2 for n > 2.The general linear group over the field of complex numbers, GL(n, C), is a complex Lie group of complex dimension n2. As a real Lie group (through realification) it has dimension 2n2. The set of all real matrices forms a real Lie subgroup. These correspond to the inclusionswhich have real dimensions n2, 2n2, and 4n2 = (2n)2. Complex n-dimensional matrices can be characterized as real 2n-dimensional matrices that preserve a linear complex structure — concretely, that commute with a matrix J such that J2 = −I, where J corresponds to multiplying by the imaginary unit i.The Lie algebra corresponding to GL(n, C) consists of all n×n complex matrices with the commutator serving as the Lie bracket.Unlike the real case, GL(n, C) is connected. This follows, in part, since the multiplicative group of complex numbers C∗ is connected. The group manifold GL(n, C) is not compact; rather its maximal compact subgroup is the unitary group U(n). As for U(n), the group manifold GL(n, C) is not simply connected but has a fundamental group isomorphic to Z.If F is a finite field with q elements, then we sometimes write GL(n, q) instead of GL(n, F). When p is prime, GL(n, p) is the outer automorphism group  of the group Zpn, and also the automorphism group, because Zpn is abelian, so the inner automorphism group is trivial.The order of GL(n, q) is: For example, GL(3, 2) has order (8 − 1)(8 − 2)(8 − 4) = 168. It is the automorphism group of the Fano plane and of the group Z23, and is also known as PSL(2, 7).More generally, one can count points of Grassmannian over F: in other words the number of subspaces of a given dimension k. This requires only finding the order of the stabilizer subgroup of one such subspace and dividing into the formula just given, by the orbit-stabilizer theorem.These formulas are connected to the Schubert decomposition of the Grassmannian, and are q-analogs of the Betti numbers of complex Grassmannians. This was one of the clues leading to the Weil conjectures.Note that in the limit q ↦ 1 the order of GL(n, q) goes to 0! – but under the correct procedure (dividing by (q − 1)n) we see that it is the order of the symmetric group (See Lorscheid's article) – in the philosophy of the field with one element, one thus interprets the symmetric group as the general linear group over the field with one element: Sn ≅ GL(n, 1).The general linear group over a prime field, GL(ν, p), was constructed and its order computed by Évariste Galois in 1832, in his last letter (to Chevalier) and second (of three) attached manuscripts, which he used in the context of studying the Galois group of the general equation of order pν.[4]The special linear group, SL(n, F), is the group of all matrices with determinant 1. They are special in that they lie on a subvariety – they satisfy a polynomial equation (as the determinant is a polynomial in the entries). Matrices of this type form a group as the determinant of the product of two matrices is the product of the determinants of each matrix. SL(n, F) is a normal subgroup of GL(n, F).If we write F× for the multiplicative group of F (excluding 0), then the determinant is a group homomorphismthat is surjective and its kernel is the special linear group. Therefore, by the first isomorphism theorem, GL(n, F)/SL(n, F) is isomorphic to F×. In fact, GL(n, F) can be written as a semidirect product:When F is R or C, SL(n, F) is a Lie subgroup of GL(n, F) of dimension n2 − 1. The Lie algebra of SL(n, F) consists of all n×n matrices over F with vanishing trace. The Lie bracket is given by the commutator.The special linear group SL(n, R) can be characterized as the group of volume and orientation preserving linear transformations of Rn.The group SL(n, C) is simply connected, while SL(n, R) is not. SL(n, R) has the same fundamental group as GL+(n, R), that is, Z for n = 2 and Z2 for n > 2.The set of all invertible diagonal matrices forms a subgroup of GL(n, F) isomorphic to (F×)n. In fields like R and C, these correspond to rescaling the space; the so-called dilations and contractions.A scalar matrix is a diagonal matrix which is a constant times the identity matrix. The set of all nonzero scalar matrices forms a subgroup of GL(n, F) isomorphic to F× . This group is the center of GL(n, F). In particular, it is a normal, abelian subgroup.The center of SL(n, F) is simply the set of all scalar matrices with unit determinant, and is isomorphic to the group of nth roots of unity in the field F.The so-called classical groups are subgroups of GL(V) which preserve some sort of bilinear form on a vector space V. These include theThese groups provide important examples of Lie groups.The projective linear group PGL(n, F) and the projective special linear group PSL(n, F) are the quotients of GL(n, F) and SL(n, F) by their centers (which consist of the multiples of the identity matrix therein); they are the induced action on the associated projective space.The affine group Aff(n, F) is an extension of GL(n, F) by the group of translations in Fn. It can be written as a semidirect product:where GL(n, F) acts on Fn in the natural manner. The affine group can be viewed as the group of all affine transformations of the affine space underlying the vector space Fn.One has analogous constructions for other subgroups of the general linear group: for instance, the special affine group is the subgroup defined by the semidirect product, SL(n, F) ⋉ Fn, and the Poincaré group is the affine group associated to the Lorentz group, O(1, 3, F) ⋉ Fn.The general semilinear group ΓL(n, F) is the group of all invertible semilinear transformations, and contains GL. A semilinear transformation is a transformation which is linear "up to a twist", meaning "up to a field automorphism under scalar multiplication". It can be written as a semidirect product:where Gal(F) is the Galois group of F (over its prime field), which acts on GL(n, F) by the Galois action on the entries.The main interest of ΓL(n, F) is that the associated projective semilinear group PΓL(n, F) (which contains PGL(n, F)) is the collineation group of projective space, for n > 2, and thus semilinear maps are of interest in projective geometry.If one removes the restriction of the determinant being non-zero, the resulting algebraic structure is a monoid, usually called the full linear monoid,[6][7][8] but occasionally also full linear semigroup,[9] general linear monoid[10][11] etc. It is actually a regular semigroup.[7]The infinite general linear group or stable general linear group is the direct limit of the inclusions GL(n, F) → GL(n + 1, F) as the upper left block matrix. It is denoted by either GL(F) or GL(∞, F), and can also be interpreted as invertible infinite matrices which differ from the identity matrix in only finitely many places.[12]It is used in algebraic K-theory to define K1, and over the reals has a well-understood topology, thanks to Bott periodicity.It should not be confused with the space of (bounded) invertible operators on a Hilbert space, which is a larger group, and topologically much simpler, namely contractible – see Kuiper's theorem.
Savitzky–Golay filter
A Savitzky–Golay filter is a digital filter that can be applied to a set of digital data points for the purpose of smoothing the data, that is, to increase the signal-to-noise ratio without greatly distorting the signal. This is achieved, in a process known as convolution, by fitting successive sub-sets of adjacent data points with a low-degree polynomial by the method of linear least squares. When the data points are equally spaced, an analytical solution to the least-squares equations can be found, in the form of a single set of "convolution coefficients" that can be applied to all data sub-sets, to give estimates of the smoothed signal, (or derivatives of the smoothed signal) at the central point of each sub-set. The method, based on established mathematical procedures,[1][2] was popularized by Abraham Savitzky and Marcel J. E. Golay who published tables of convolution coefficients for various polynomials and sub-set sizes in 1964.[3][4] Some errors in the tables have been corrected.[5] The method has been extended for the treatment of 2- and 3-dimensional data.Savitzky and Golay's paper is one of the most widely cited papers in the journal Analytical Chemistry[6] and is classed by that journal as one of its "10 seminal papers" saying "it can be argued that the dawn of the computer-controlled analytical instrument can be traced to this article".[7]The data consists of a set of n  {xj, yj} points (j = 1, ..., n), where x is an independent variable and yj is an observed value. They are treated with a set of m convolution coefficients, Ci, according to the expression  It is easy to apply this formula in a spreadsheet. Selected convolution coefficients are shown in the tables, below. For example, for smoothing by a 5-point quadratic polynomial, m = 5, i = −2, −1, 0, 1, 2 and the jth smoothed data point, Yj, is given byA moving average filter is commonly used with time series data to smooth out short-term fluctuations and highlight longer-term trends or cycles. It is often used in technical analysis of financial data, like stock prices, returns or trading volumes. It is also used in economics to examine gross domestic product, employment or other macroeconomic time series.An unweighted moving average filter is the simplest convolution filter. Each subset of the data set is fitted by a straight horizontal line. It was not included in the Savitzsky-Golay tables of convolution coefficients as all the coefficient values are simply equal to 1/m.When the data points are equally spaced, an analytical solution to the least-squares equations can be found.[2] This solution forms the basis of the convolution method of numerical smoothing and differentiation. Suppose that the data consists of a set of n  points (xj, yj)  (j = 1, ..., n), where x is an independent variable and yj is a datum value. A polynomial will be fitted by linear least squares  to a set of m (an odd number) adjacent data points, each separated by an interval h. Firstly, a change of variable is madeThe coefficients a0, a1 etc. are obtained by solving the normal equations (bold a represents a vector, bold J represents a matrix).For example, for a cubic polynomial fitted to 5 points, z= −2, −1, 0, 1, 2 the normal equations are solved as follows.Now, the normal equations can be factored into two separate sets of equations, by rearranging rows and columns, withExpressions for the inverse of each of these matrices can be obtained using Cramer's ruleThe normal equations becomeandMultiplying out and removing common factors,The coefficients of y in these expressions are known as convolution coefficients. They are elements of the matrix In general,In matrix notation this example is written asTables of convolution coefficients, calculated in the same way for m up to 25, were published for the Savitzky–Golay smoothing filter in 1964,[3][5] The value of the central point, z = 0, is obtained from a single set of coefficients, a0 for smoothing, a1 for 1st. derivative etc. The numerical derivatives are obtained by differentiating Y. This means that the derivatives are calculated for the smoothed data curve.  For a cubic polynomialIn general, polynomials of degree (0 and 1),[note 3] (2 and 3), (4 and 5) etc. give the same coefficients for smoothing and even derivatives. Polynomials of degree (1 and 2), (3 and 4) etc. give the same coefficients for odd derivatives.It is not necessary always to use the Savitzky–Golay tables. The summations in the matrix JTJ can be evaluated in closed form, so that algebraic formulae can be derived for the convolution coefficients.[13][note 4] Functions that are suitable for use with a curve that has an inflection point are:Simpler expressions that can be used with curves that don't have an inflection point are:  Higher derivatives can be obtained. For example, a fourth derivative can be obtained by performing two passes of a second derivative function.[14]An alternative to fitting m data points by a simple polynomial in the subsidiary variable, z, is to use orthogonal polynomials. where P0, ..., Pk is a set of mutually orthogonal polynomials of degree 0, ..., k. Full details on how to obtain expressions for the orthogonal polynomials and the relationship between the coefficients b and a are given by Guest.[2]  Expressions for the convolution coefficients are easily obtained because the normal equations matrix, JTJ, is a diagonal matrix as the product of any two orthogonal polynomials is zero by virtue of their mutual orthogonality. Therefore, each non-zero element of its inverse is simply the reciprocal the corresponding element in the normal equation matrix. The calculation is further simplified by using recursion to build orthogonal Gram polynomials. The whole calculation can be coded in a few lines of PASCAL, a computer language well-adapted for calculations involving recursion.[15]Savitzky–Golay filters are most commonly used to obtain the smoothed or derivative value at the central point, z = 0, using a single set of convolution coefficients. (m − 1)/2 points at the start and end of the series cannot be calculated using this process. Various strategies can be employed to avoid this inconvenience.It is implicit in the above treatment that the data points are all given equal weight. Technically, the objective function being minimized in the least-squares process has unit weights, wi = 1. When weights are not all the same the normal equations becomeIf the same set of diagonal weights is used for all data subsets, W = diag(w1,w2,...,wm), an analytical solution to the normal equations can be written down. For example, with a quadratic polynomial,An explicit expression for the inverse of this matrix can be obtained using Cramer's rule. A set of convolution coefficients may then be derived as Alternatively the coefficients, C, could be calculated in a spreadsheet, employing a built-in matrix inversion routine to obtain the inverse of the normal equations matrix. This set of coefficients, once calculated and stored, can be used with all calculations in which the same weighting scheme applies. A different set of coefficients is needed for each different weighting scheme.Two-dimensional smoothing and differentiation can also be applied to tables of data values, such as intensity values in a photographic image which is composed of a rectangular grid of pixels.[16] [17] Such a grid is referred as a kernel, and the data points that constitute the kernel are referred as nodes. The trick is to transform the rectangular kernel into a single row by a simple ordering of the indices of the nodes.  Whereas the one-dimensional filter coefficients are found by fitting a polynomial in the subsidiary variable z to a set of m data points, the two-dimensional coefficients are found by fitting a polynomial in subsidiary variables v and w to a set of the values at the m × n kernel nodes. The following example, for a bi-cubic polynomial, m = 7, and n = 5, illustrates the process, which parallels the process for the one dimensional case, above.[18]The rectangular kernel of 35 data values, d1 − d35becomes a vector when the rows are placed one after another.The Jacobian has 10 columns, one for each of the parameters a00 − a03, and 35 rows, one for each pair of v and w values. Each row has the formThe convolution coefficients are calculated asNikitas and Pappa-Louisi showed that, depending on the format of the used polynomial, the quality of smoothing may vary significantly.[19] They recommend using the polynomial of the formbecause such polynomials can achieve good smoothing both in the central and in the near-boundary regions of a kernel, and therefore they can be confidently used in smoothing both at the internal and at the near-boundary data points of a sampled domain. In order to avoid ill-conditioning when solving the least-squares problem, p < m and q < n. For a software which calculates the two-dimensional coefficients and for a database of such C's, see the section on multi-dimensional convolution coefficients, below.The idea of two-dimensional convolution coefficients can be extended to the higher spatial dimensions as well, in a straightforward manner,[16][20] by arranging multidimensional distribution of the kernel nodes in a single row. Following the aforementioned finding by Nikitas and Pappa-Louisi[19] in two-dimensional cases, usage of the following form of the polynomial is recommended in multidimensional cases:Accurate computation of C in multidimensional cases becomes challenging, as precision of standard floating point numbers available in computer programming languages no longer remain sufficient. The insufficient precision causes the floating point truncation errors to become comparable to the magnitudes of some C elements, which, in turn, severely degrades its accuracy and renders it useless. Chandra Shekhar has brought forth two open source softwares, Advanced Convolution Coefficient Calculator (ACCC) and Precise Convolution Coefficient Calculator (PCCC), which handle these accuracy issues adequately. ACCC performs the computation by using floating point numbers, in an iterative manner.[21] The precision of the floating-point numbers is gradually increased in each iteration, by using GNU MPFR. Once the obtained C's in two consecutive iterations start having same significant digits until a pre-specified distance, the convergence is assumed to have reached. If the distance is sufficiently large, the computation yields a highly accurate C. PCCC employs rational number calculations, by using GNU Multiple Precision Arithmetic Library, and yields a fully accurate C, in the rational number format.[22] In the end, these rational numbers are converted into floating point numbers, until a pre-specified number of significant digits.A database of C's that are calculated by using ACCC, for symmetric kernels and both symmetric and asymmetric polynomials, on unity-spaced kernel nodes, in the 1, 2, 3, and 4 dimensional spaces, is made available.[23] Chandra Shekhar has also laid out a mathematical framework that describes usage of C calculated on unity-spaced kernel nodes to perform filtering and partial differentiations (of various orders) on non-uniformly spaced kernel nodes,[20] allowing usage of C provided in the aforementioned database. Although this method yields approximate results only, they are acceptable in most engineering applications, provided that non-uniformity of the kernel nodes is weak.It is inevitable that the signal will be distorted in the convolution process. From property 3 above, when data which has a peak is smoothed the peak height will be reduced and the half-width will be increased. Both the extent of the distortion and S/N (signal-to-noise ratio) improvement:For example, If the noise in all data points is uncorrelated and has a constant standard deviation, σ, the standard deviation on the noise will be decreased by convolution with an m-point smoothing function to[25][note 5]These functions are shown in the plot at the right. For example, with a 9-point linear function (moving average) two thirds of the noise is removed and with a 9-point quadratic/cubic smoothing function only about half the noise is removed. Most of the noise remaining is low-frequency noise(see Frequency characteristics of convolution filters, below).Although the moving average function gives the best noise reduction it is unsuitable for smoothing data which has curvature over m points. A quadratic filter function is unsuitable for getting a derivative of a data curve with an inflection point because a quadratic polynomial does not have one. The optimal choice of polynomial order and number of convolution coefficients will be a compromise between noise reduction and distortion.[27]One way to mitigate distortion and improve noise removal is to use a filter of smaller width and perform more than one convolution with it. For two passes of the same filter this is equivalent to one pass of a filter obtained by convolution of the original filter with itself.[28] For example, 2 passes of the filter with coefficients (1/3, 1/3, 1/3) is equivalent to 1 pass of the filter with coefficients(1/9, 2/9, 3/9, 2/9, 1/9).The disadvantage of multipassing is that the equivalent filter width for n passes of an m-point function is n(m − 1) + 1 so multipassing is subject to greater end-effects. Nevertheless, multipassing has been used to great advantage. For instance, some 40–80 passes on data with a signal-to-noise ratio of only 5 gave useful results.[29]  The noise reduction formulae given above do not apply because correlation between calculated data points increases with each pass.Convolution maps to multiplication in the Fourier co-domain. The discrete Fourier transform of a convolution filter is a real-valued function which can be represented asθ runs from 0 to 180 degrees, after which the function merely repeats itself. The plot for a 9-point quadratic/cubic smoothing function is typical. At very low angle, the plot is almost flat, meaning that low-frequency components of the data will be virtually unchanged by the smoothing operation. As the angle increases the value decreases so that higher frequency components are more and more attenuated. This shows that the convolution filter can be described as a low-pass filter: the noise that is removed is primarily high-frequency noise and low-frequency noise passes through the filter.[30] Some high-frequency noise components are attenuated more than others, as shown by undulations in the Fourier transform at large angles. This can give rise to small oscillations in the smoothed data.[31]Convolution affects the correlation between errors in the data. The effect of convolution can be expressed as a linear transformation.By the law of error propagation, the variance-covariance matrix of the data, A  will be transformed into B according toTo see how this applies in practice, consider the effect of a 3-point moving average on the first three calculated points, Y2 − Y4, assuming that the data points have equal variance and that there is no correlation between them. A will be an identity matrix multiplied by a constant, σ2, the variance at each point.In this case the correlation coefficients, between calculated points i and j will beIn general, the calculated values are correlated even when the observed values are not correlated.  The correlation extends over m − 1 calculated points at a time.[32]To illustrate the effect of multipassing on the noise and correlation of a set of data, consider the effects of a second pass of a 3-point moving average filter. For the second pass[note 6]Correlation now extends over a span of 4 sequential points with correlation coefficients The advantage obtained by performing two passes with the narrower smoothing function is that it introduces less distortion into the calculated data.Selected values of the convolution coefficients for polynomials of degree 1,2,3, 4 and 5 are given in the following tables. The values were calculated using the PASCAL code provided in Gorry.[15]
Runge–Kutta methods
In numerical analysis, the Runge–Kutta methods  are a family of implicit and explicit iterative methods, which include the well-known routine called the Euler Method, used in temporal discretization for the approximate solutions of ordinary differential equations.[1] These methods were developed around 1900 by the German mathematicians Carl Runge and Martin Kutta.See the article on numerical methods for ordinary differential equations for more background and other methods. See also List of Runge–Kutta methods.The most widely known member of the Runge–Kutta family is generally referred to as  "RK4", "classical Runge–Kutta method" or simply as "the Runge–Kutta method".Let an initial value problem be specified as follows:Now pick a step-size h > 0 and definefor n = 0, 1, 2, 3, ..., using[2]The family of explicit Runge–Kutta methods is a generalization of the RK4 method mentioned above. It is given bywhere[5]To specify a particular method, one needs to provide the integer s (the number of stages), and the coefficients aij (for 1 ≤ j < i ≤ s), bi (for i = 1, 2, ..., s) and ci (for i = 2, 3, ..., s). The matrix [aij] is called the Runge–Kutta matrix, while the bi and ci are known as the weights and the nodes.[6] These data are usually arranged in a mnemonic device, known as a Butcher tableau (after John C. Butcher):The Runge–Kutta method is consistent ifThere are also accompanying requirements if one requires the method to have a certain order p, meaning that the local truncation error is O(hp+1). These can be derived from the definition of the truncation error itself. For example, a two-stage method has order 2 if b1 + b2 = 1, b2c2 = 1/2, and a21 = c2.[7]The RK4 method falls in this framework. Its tableau is[10]A slight variation of "the" Runge–Kutta method is also due to Kutta in 1901 and is called the 3/8-rule.[11] The primary advantage this method has is that almost all of the error coefficients are smaller than in the popular method, but it requires slightly more FLOPs (floating-point operations) per time step. Its Butcher tableau isAn example of a second-order method with two stages is provided by the midpoint method:The corresponding tableau isThe midpoint method is not the only second-order Runge–Kutta method with two stages; there is a family of such methods, parameterized by α and given by the formula[12]Its Butcher tableau isAs an example, consider the two-stage second-order Runge–Kutta method with α = 2/3, also known as Ralston method. It is given by the tableauwith the corresponding equationsThis method is used to solve the initial-value problemwith step size h = 0.025, so the method needs to take four steps.The method proceeds as follows:The numerical solutions correspond to the underlined values.During the integration, the step size is adapted such that the estimated error stays below a user-defined threshold: If the error is too high, a step is repeated with a lower step size; if the error is much smaller, the step size is increased to save time. This results in an (almost) optimal step size, which saves computation time. Moreover, the user does not have to spend time on finding an appropriate step size.The lower-order step is given byThe Runge–Kutta–Fehlberg method has two methods of orders 5 and 4. Its extended Butcher tableau is:However, the simplest adaptive Runge–Kutta method involves combining Heun's method, which is order 2, with the Euler method, which is order 1. Its extended Butcher tableau is:Other adaptive Runge–Kutta methods are the Bogacki–Shampine method (orders 3 and 2), the Cash–Karp method and the Dormand–Prince method (both with orders 5 and 4).All Runge–Kutta methods mentioned up to now are explicit methods. Explicit Runge–Kutta methods are generally unsuitable for the solution of stiff equations because their region of absolute stability is small; in particular, it is bounded.[14]This issue is especially important in the solution of partial differential equations.The instability of explicit Runge–Kutta methods motivates the development of implicit methods. An implicit Runge–Kutta method has the formwhereThe consequence of this difference is that at every step, a system of algebraic equations has to be solved. This increases the computational cost considerably. If a method with s stages is used to solve a differential equation with m components, then the system of algebraic equations has ms components. This can be contrasted with implicit linear multistep methods (the other big family of methods for ODEs): an implicit s-step linear multistep method needs to solve a system of algebraic equations with only m components, so the size of the system does not increase as the number of steps increases.[16]The simplest example of an implicit Runge–Kutta method is the backward Euler method:The Butcher tableau for this is simply:This Butcher tableau corresponds to the formulaewhich can be re-arranged to get the formula for the backward Euler method listed above.Another example for an implicit Runge–Kutta method is the trapezoidal rule. Its Butcher tableau is:The trapezoidal rule is a collocation method (as discussed in that article). All collocation methods are implicit Runge–Kutta methods, but not all implicit Runge–Kutta methods are collocation methods.[17]The Gauss–Legendre methods form a family of collocation methods based on Gauss quadrature. A Gauss–Legendre method with s stages has order 2s (thus, methods with arbitrarily high order can be constructed).[18] The method with two stages (and thus order four) has Butcher tableau:where e stands for the vector of ones. The function r is called the stability function.[20] It follows from the formula that r is the quotient of two polynomials of degree s if the method has s stages. Explicit methods have a strictly lower triangular matrix A, which implies that det(I − zA) = 1 and that the stability function is a polynomial.[21]The numerical solution to the linear test equation decays to zero if | r(z) | < 1 with z = hλ. The set of such z is called the domain of absolute stability. In particular, the method is said to be A-stable if all z with Re(z) < 0 are in the domain of absolute stability. The stability function of an explicit Runge–Kutta method is a polynomial, so explicit Runge–Kutta methods can never be A-stable.[21]The Gauss–Legendre method with s stages has order 2s, so its stability function is the Padé approximant with m = n = s. It follows that the method is A-stable.[23] This shows that A-stable Runge–Kutta can have arbitrarily high order. In contrast, the order of A-stable linear multistep methods cannot exceed two.[24]where:where:If we now express the general formula using what we just derived we obtain:we obtain a system of constraints on the coefficients:
Rotation of axes
Coordinate systems are essential for studying the equations of curves using the methods of analytic geometry.  To use the method of coordinate geometry, the axes are placed at a convenient position with respect to the curve under consideration.  For example, to study the equations of ellipses and hyperbolas, the foci are usually located on one of the axes and are situated symmetrically with respect to the origin.  If the curve (hyperbola, parabola, ellipse, etc.) is not situated conveniently with respect to the axes, the coordinate system should be changed to place the curve at a convenient and familiar location and orientation.  The process of making this change is called a transformation of coordinates.[6]The solutions to many problems can be simplified by rotating the coordinate axes to obtain new axes through the same origin.We have    (1)    (2)and    (3)    (4)Substituting equations (1) and (2) into equations (3) and (4), we obtain    (5)    (6)Equations (5) and (6) can be represented in matrix form aswhich is the standard matrix equation of a rotation of axes in two dimensions.[8]The inverse transformation is    (7)    (8)orSolution:Solution:The most general equation of the second degree has the form    (9)Through a change of coordinates (a rotation of axes and a translation of axes), equation (9) can be put into a standard form, which is usually easier to work with.  It is always possible to rotate the coordinates in such a way that in the new system there is no x'y' term.  Substituting equations (7) and (8) into equation (9), we obtain    (10)where    (11)When a problem arises with B, D and E all different from zero, they can be eliminated by performing in succession a rotation (eliminating B) and a translation (eliminating the D and E terms).[12]Solution:
Function composition
In mathematics, function composition is the pointwise application of one function to the result of another to produce a third function. For instance, the functions f : X → Y and g : Y → Z can be composed to yield a function which maps x in X to g(f(x)) in Z.  Intuitively, if z is a function of y, and y is a function of x, then z is a function of x. The resulting composite function is denoted g ∘ f : X → Z, defined by (g ∘ f )(x) = g(f(x)) for all x in X.[note 1]The notation g ∘ f is read as "g circle f ", "g round f ", "g about f ", "g composed with f ", "g after f ", "g following f ", "g of f", or "g on f ". Intuitively, composing two functions is a chaining process in which the output of the inner function becomes the input of the outer function.The composition of functions is a special case of the composition of relations, so all properties of the latter are true of composition of functions.[1] The composition of functions has some additional properties.The composition of functions is always associative—a property inherited from the composition of relations.[1] That is, if f, g, and h are three functions with suitably chosen domains and codomains, then f ∘ (g ∘ h) = (f ∘ g) ∘ h, where the parentheses serve to indicate that composition is to be performed first for the parenthesized functions. Since there is no distinction between the choices of placement of parentheses, they may be left off without causing any ambiguity.In a strict sense, the composition g ∘ f can be built only if f's codomain equals g's domain; in a wider sense it is sufficient that the former is a subset of the latter.[note 2]Moreover, it is often convenient to tacitly restrict f's domain such that f produces only values in g's domain; for example, the composition g ∘ f of the functions f : ℝ → (−∞,+9]  defined by f(x) = 9 − x2 and g : [0,+∞) → ℝ defined by g(x) = √x can be defined on the interval [−3,+3].The functions g and f are said to commute with each other if g ∘ f = f ∘ g. Commutativity is a special property, attained only by particular functions, and often in special circumstances. For example, |x| + 3 = |x + 3| only when x ≥ 0. The picture shows another example.The composition of one-to-one functions is always one-to-one. Similarly, the composition of two onto functions is always onto. It follows that composition of two bijections is also a bijection. The inverse function of a composition (assumed invertible) has the property that (f ∘ g)−1 = ( g−1 ∘ f −1).[2]Derivatives of compositions involving differentiable functions can be found using the chain rule. Higher derivatives of such functions are given by Faà di Bruno's formula.Suppose one has two (or more) functions f: X → X, g: X → X having the same domain and codomain; these are often called transformations. Then one can form chains of transformations composed together, such as f ∘ f ∘ g ∘ f. Such chains have the algebraic structure of a monoid, called a transformation monoid or (much more seldom) composition monoid.  In general, transformation monoids can have remarkably complicated structure. One particular notable example is the de Rham curve. The set of all functions f: X → X is called the full transformation semigroup[3] or symmetric semigroup[4] on X. (One can actually define two semigroups depending how one defines the semigroup operation as the left or right composition of functions.[5])If the transformation are bijective (and thus invertible), then the set of all  possible combinations of these functions forms a transformation group; and one says that the group is generated by these functions.  A fundamental result in group theory, Cayley's theorem, essentially says that any group is in fact just a subgroup of a permutation group (up to isomorphism).[6]The set of all bijective functions f: X → X (called permutations) forms a group with respect to the composition operator. This is the symmetric group, also sometimes called the composition group.In the symmetric semigroup (of all transformations) one also finds a weaker, non-unique notion of inverse (called a pseudoinverse) because the symmetric semigroup is a regular semigroup.[7]If Y ⊆ X, then f: X→Y may compose with itself; this is sometimes denoted as f 2. That is:More generally, for any natural number n ≥ 2, the nth functional power can be defined inductively by f n = f ∘ f n−1 = f n−1 ∘ f. Repeated composition of such a function with itself is called iterated function.Note: If f takes its values in a ring (in particular for real or complex-valued f ), there is a risk of confusion, as f n could also stand for the n-fold product of f, e.g. f 2(x) = f(x) · f(x). For trigonometric functions, usually the latter is meant, at least for positive exponents. For example, in trigonometry, this superscript notation represents standard exponentiation when used with trigonometric functions:sin2(x) = sin(x) · sin(x).However, for negative exponents (especially −1), it nevertheless usually refers to the inverse function, e.g., tan−1 = arctan ≠ 1/tan.In some cases, when, for a given function f, the equation g ∘ g = f has a unique solution g, that function can be defined as the functional square root of f, then written as g = f 1/2.More generally, when gn = f has a unique solution for some natural number n > 0, then f m/n can be defined as gm.Under additional restrictions, this idea can be generalized so that the iteration count  becomes a continuous parameter; in this case, such a system is called a flow, specified through solutions of Schröder's equation. Iterated functions and flows occur naturally in the study of fractals and dynamical systems.To avoid ambiguity, some mathematicians choose to write  f °n for the n-th iterate  of the function f.Many mathematicians, particularly in group theory, omit the composition symbol, writing gf for g ∘ f.[8]In the mid-20th century, some mathematicians decided that writing "g ∘ f " to mean "first apply f, then apply g" was too confusing and decided to change notations. They write "xf " for "f(x)" and "(xf)g" for "g(f(x))".[9] This can be more natural and seem simpler than writing functions on the left in some areas – in linear algebra, for instance, when x is a row vector and f and g denote matrices and the composition is by matrix multiplication. This alternative notation is called postfix notation. The order is important because function composition is not necessarily commutative (e.g matrix multiplication). Successive transformations applying and composing to the right agrees with the left-to-right reading sequence.Mathematicians who use postfix notation may write "fg", meaning first apply f and then apply g, in keeping with the order the symbols occur in postfix notation, thus making the notation "fg" ambiguous.  Computer scientists may write "f ; g" for this,[10] thereby disambiguating the order of composition. To distinguish the left composition operator from a text semicolon, in the Z notation the ⨾  character is used for left relation composition.[11] Since all functions are  binary relations, it is correct to use the [fat] semicolon for function composition as well (see the article on composition of relations for further details on this notation).Given a function g, the composition operator Cg is defined as that operator which maps functions to functions asComposition operators are studied in the field of operator theory.Function composition appears in one form or another in numerous programming languages.Partial composition is possible for multivariate functions. The function resulting when some argument xi of the function f is replaced by the function g is called a composition of f and g in some computer engineering contexts, and is denoted f |xi = gWhen g is a simple constant b, composition degenerates into a (partial) valuation, whose result is also known as restriction or co-factor.[12]In general, the composition of multivariate functions may involve several other functions as arguments, as in the definition of primitive recursive function. Given f, a n-ary function, and n m-ary functions g1, ..., gn, the composition of f with g1, ..., gn, is the m-ary functionThis is sometimes called the generalized composite of f with g1, ..., gn.[13] The partial composition in only one argument mentioned previously can be instantiated from this more general scheme by setting all argument functions except one to be suitably chosen projection functions. Note also that g1, ..., gn can be seen as a single vector/tuple-valued function in this generalized scheme, in which case this is precisely the standard definition of function composition.[14]A set of finitary operations on some base set X is called a clone if it contains all projections and is closed under generalized composition. Note that a clone generally contains operations of various arities.[13] The notion of commutation also finds an interesting generalization in the multivariate case; a function f of arity n is said to commute with a function g of arity m if f is a homomorphism preserving g, and vice versa i.e.:[15]A unary operation always commutes with itself, but this is not necessarily the case for a binary (or higher arity) operation. A binary (or higher arity) operation that commutes with itself is called medial or entropic.[15]Composition can be generalized to arbitrary binary relations.If R ⊆ X × Y and S ⊆ Y × Z are two binary relations, then their composition S∘R is the relation defined as {(x, z) ∈ X × Z : ∃y ∈ Y. (x, y) ∈ R ∧ (y, z)  ∈ S}.Considering a function as a special case of a binary relation (namely functional relations), function composition satisfies the definition for relation composition.The composition is defined in the same way for partial functions and Cayley's theorem has its analogue called Wagner-Preston theorem.[16]The category of sets with functions as morphisms is the prototypical category. The axioms of a category are in fact inspired from the properties (and also the definition) of function composition.[17] The structures given by composition are axiomatized and generalized in category theory with the concept of morphism as the category-theoretical replacement of functions. The reversed order of composition in the formula (f ∘ g)−1 = (g−1 ∘ f −1) applies for composition of relations using converse relations, and thus in group theory. These structures form dagger categories.The composition symbol ∘  is encoded as .mw-parser-output .monospaced{font-family:monospace,monospace}U+2218 ∘ .mw-parser-output .smallcaps{font-variant:small-caps}RING OPERATOR (HTML &#8728;); see the Degree symbol article for similar-appearing Unicode characters. In TeX, it is written \circ.
Jordan normal form
In linear algebra, a Jordan normal form (often called Jordan canonical form)[1]of a linear operator on a finite-dimensional vector space is an upper triangular matrix of a particular form called a Jordan matrix, representing the operator with respect to some basis. Such a matrix has each non-zero off-diagonal entry equal to 1, immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them.Let V be a vector space over a field K. Then a basis with respect to which the matrix has the required form exists if and only if all eigenvalues of the matrix lie in K, or equivalently if the characteristic polynomial of the operator splits into linear factors over K. This condition is always satisfied if K is algebraically closed (for instance, if it is the field of complex numbers). The diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue.[2][3][4]If the operator is originally given by a square matrix M, then its Jordan normal form is also called the Jordan normal form of M. Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given M is not entirely unique, as it is a block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.[2][3][4]The Jordan–Chevalley decomposition is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the Jordan normal form.[5][6][7]The Jordan normal form is named after Camille Jordan, who first stated the Jordan decomposition theorem in 1870.[8]Some textbooks have the ones on the subdiagonal, i.e., immediately below the main diagonal instead of on the superdiagonal.  The eigenvalues are still on the main diagonal.[9][10]An n × n matrix A is diagonalizable if and only if the sum of the dimensions of the eigenspaces is n. Or, equivalently, if and only if A has n linearly independent eigenvectors. Not all matrices are diagonalizable. Consider the following matrix:Including multiplicity, the eigenvalues of A are λ = 1, 2, 4, 4. The dimension of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so A is not diagonalizable. However, there is an invertible matrix P such that A = PJP−1, whereThe matrix J is almost diagonal. This is the Jordan normal form of A. The section Example below fills in the details of the computation.In general, a square complex matrix A is similar to a block diagonal matrixwhere each block Ji is a square matrix of the formSo there exists an invertible matrix P such that P−1AP = J is such that the only non-zero entries of J are on the diagonal and the superdiagonal. J is called the Jordan normal form of A. Each Ji is called a Jordan block of A. In a given Jordan block, every entry on the superdiagonal is 1.Assuming this result, we can deduce the following properties:Consider the matrix A from the example in the previous section. The Jordan normal form is obtained by some similarity transformation P−1AP = J, i.e.,Let P have column vectors pi, i = 1, ..., 4, thenWe see thatThus, given an eigenvalue λ, its corresponding Jordan block gives rise to a Jordan chain. The generator, or lead vector, say pr, of the chain is a generalized eigenvector such that (A − λ I)rpr = 0, where r is the size of the Jordan block. The vector p1 =  (A − λ I)r−1pr is an eigenvector corresponding to λ. In general, pi is a preimage of pi−1 under A − λ I. So the lead vector generates the chain via multiplication by (A − λ I).[12][13]Therefore, the statement that every square matrix A can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of A.We give a proof by induction that any complex-valued matrix A may be put in Jordan normal form. The 1 × 1 case is trivial. Let A be an n × n matrix. Take any eigenvalue λ of A. The range of A − λ I, denoted by Ran(A − λ I), is an invariant subspace of A. Also, since λ is an eigenvalue of A, the dimension of Ran(A − λ I), r, is strictly less than n. Let A'  denote the restriction of A to Ran(A − λ I), By inductive hypothesis, there exists a basis {p1, ..., pr} such that A' , expressed with respect to this basis, is in Jordan normal form.Next consider the subspace Ker(A − λ I). Ifthe desired result follows immediately from the rank–nullity theorem. This would be the case, for example, if A was Hermitian.Otherwise, iflet the dimension of Q be s ≤ r. Each vector in Q is an eigenvector of A'  corresponding to eigenvalue λ. So the Jordan form of A'  must contain s Jordan chains corresponding to s linearly independent eigenvectors. So the basis {p1, ..., pr} must contain s vectors, say {pr−s+1, ..., pr}, that are lead vectors in these Jordan chains from the Jordan normal form of A'. We can "extend the chains" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(A − λ I).) Let qi be such thatClearly no non-trivial linear combination of the qi can lie in Ker(A − λ I). Furthermore, no non-trivial linear combination of the qi can be in Ran(A − λ I), for that would contradict the assumption that each pi is a lead vector in a Jordan chain. The set {qi}, being preimages of the linearly independent set {pi} under A − λ I, is also linearly independent.Finally, we can pick any linearly independent set {z1, ..., zt} that spansBy construction, the union of the three sets {p1, ..., pr}, {qr−s +1, ..., qr}, and  {z1, ..., zt} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of A. Finally, by the rank–nullity theorem, the cardinality of the union is n. In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of A, and this shows A can be put in Jordan normal form.It can be shown that the Jordan normal form of a given matrix A is unique up to the order of the Jordan blocks.Knowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of A. Assuming the algebraic multiplicity m(λ) of an eigenvalue λ is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (A − λ I)m(λ). To see this, suppose an n × n matrix A has only one eigenvalue λ. So m(λ) = n. The smallest integer k1 such thatis the size of the largest Jordan block in the Jordan form of A. (This number k1 is also called the index of λ. See discussion in a following section.) The rank ofis the number of Jordan blocks of size k1. Similarly, the rank ofis twice the number of Jordan blocks of size k1 plus the number of Jordan blocks of size k1−1. The general case is similar.This can be used to show the uniqueness of the Jordan form. Let J1 and J2 be two Jordan normal forms of A. Then J1 and J2 are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of J1 and J2. This proves the uniqueness part of the statement.This real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form complex conjugate pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.One can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.Using the Jordan normal form, direct calculation gives a spectral mapping theorem for the polynomial functional calculus: Let A be an n × n matrix with eigenvalues λ1, ..., λn, then for any polynomial p, p(A) has eigenvalues p(λ1), ..., p(λn).The Cayley–Hamilton theorem asserts that every matrix A satisfies its characteristic equation: if p is the characteristic polynomial of A, then p(A) = 0. This can be shown via direct calculation in the Jordan form, since any Jordan block for λ is annihilated by (X − λ)m where m is the multiplicity of the root λ of p, the sum of the sizes of the Jordan blocks for λ, and therefore no less than the size of the block in question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the splitting field of p; this field extension does not change the matrix p(A) in any way.The minimal polynomial P of a square matrix A is the unique monic polynomial of least degree, m, such that P(A) = 0. Alternatively, the set of polynomials that annihilate a given A form an ideal I in C[x], the principal ideal domain of polynomials with complex coefficients. The monic element that generates I is precisely P.Let λ1, ..., λq be the distinct eigenvalues of A, and si be the size of the largest Jordan block corresponding to λi. It is clear from the Jordan normal form that the minimal polynomial of A has degree Σsi.While the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of elementary divisors. The elementary divisors of a square matrix A are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial m are the elementary divisors of the largest degree corresponding to distinct eigenvalues.The degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, A is diagonalizable.The Jordan form of a n × n matrix A is block diagonal, and therefore gives a decomposition of the n dimensional Euclidean space into invariant subspaces of A. Every Jordan block Ji corresponds to an invariant subspace Xi. Symbolically, we putwhere each Xi is the span of the corresponding Jordan chain, and k is the number of Jordan chains.One can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue λi, the size of its largest corresponding Jordan block si is called the index of  λi and denoted by ν(λi). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace Yi byThis gives the decompositionwhere l is the number of distinct eigenvalues of A. Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where A is a multiple of the identity matrix we have k = n and l = 1.The projection onto Yi and along all the other Yj ( j ≠ i ) is called the spectral projection of A at λi and is usually denoted by P(λi ; A). Spectral projections are mutually orthogonal in the sense that P(λi ; A) P(λj ; A) = 0 if i ≠ j. Also they commute with A and their sum is the identity matrix. Replacing every λi in the Jordan matrix J by one and zeroising all other entries gives P(λi ; J), moreover if U J U−1 is the similarity transformation such that A = U J U−1 then P(λi ; A) = U P(λi ; J) U−1. They are not confined to finite dimensions. See below for their application to compact operators, and in holomorphic functional calculus for a more general discussion.Comparing the two decompositions, notice that, in general, l ≤ k. When A is normal, the subspaces Xi's in the first decomposition are one-dimensional and mutually orthogonal. This is the spectral theorem for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.It might be of interest here to note some properties of the index, ν(λ). More generally, for a complex number λ, its index can be defined as the least non-negative integer ν(λ) such thatSo ν(λ) > 0 if and only if λ is an eigenvalue of A. In the finite-dimensional case, ν(λ) ≤ the algebraic multiplicity of λ.Jordan reduction can be extended to any square matrix M whose entries lie in a field K.  The result states that any M can be written as a sum D + N where D is semisimple, N is nilpotent, and DN = ND. This is called the Jordan–Chevalley decomposition. Whenever K contains the eigenvalues of M, in particular when K is algebraically closed, the normal form can be expressed explicitly as the direct sum of Jordan blocks. Similar to the case when K is the complex numbers, knowing the dimensions of the kernels of (M − λI)k for 1 ≤ k ≤ m, where m is the algebraic multiplicity of the eigenvalue λ, allows one to determine the Jordan form of M. We may view the underlying vector space V as a K[x]-module by regarding the action of x on V as application of M and extending by K-linearity. Then the polynomials (x − λ)k are the elementary divisors of M, and the Jordan normal form is concerned with representing M in terms of blocks associated to the elementary divisors.The proof of the Jordan normal form is usually carried out as an application to the ring K[x] of the structure theorem for finitely generated modules over a principal ideal domain, of which it is a corollary.In a different direction, for compact operators on a Banach space, a result analogous to the Jordan normal form holds. One restricts to compact operators because every point x in the spectrum of a compact operator T, the only exception being when x is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.Let X be a Banach space, L(X) be the bounded operators on X, and σ(T) denote the spectrum of T ∈ L(X). The holomorphic functional calculus is defined as follows:Fix a bounded operator T. Consider the family Hol(T) of complex functions that is holomorphic on some open set G containing σ(T). Let Γ = {γi} be a finite collection of Jordan curves such that σ(T) lies in the inside of Γ, we define f(T) byThe open set G could vary with f and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar case. Although the integral makes sense for continuous f, we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that σ(T) lie in the inside of Γ ensures f(T) is well defined; it does not depend on the choice of Γ. The functional calculus is the mapping Φ from Hol(T) to L(X) given byWe will require the following properties of this functional calculus:In the finite-dimensional case, σ(T) = {λi} is a finite discrete set in the complex plane. Let ei be the function that is 1 in some open neighborhood of λi and 0 elsewhere. By property 3 of the functional calculus, the operatoris a projection. Moreoever, let νi be the index of λi andThe spectral mapping theorem tells ushas spectrum {0}. By property 1, f(T) can be directly computed in the Jordan form, and by inspection, we see that the operator f(T)ei(T) is the zero matrix.By property 3, f(T) ei(T) = ei(T) f(T). So ei(T) is precisely the projection ontothe subspaceThe relationimplieswhere the index i runs through the distinct eigenvalues of T. This is exactly the invariant subspace decompositiongiven in a previous section. Each ei(T) is the projection onto the subspace spanned by the Jordan chains corresponding to λi and along the subspaces spanned by the Jordan chains corresponding to λj for j ≠ i. In other words, ei(T) = P(λi;T). This explicit identification of the operators ei(T) in turn gives an explicit form of holomorphic functional calculus for matrices:Notice that the expression of f(T) is a finite sum because, on each neighborhood of λi, we have chosen the Taylor series expansion of f centered at λi.Let T be a bounded operator λ be an isolated point of σ(T). (As stated above, when T is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)The point λ is called a pole of operator T with order ν if the resolvent function RT defined byhas a pole of order ν at λ.We will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators.Consider the annular region A centered at the eigenvalue λ with sufficiently small radius ε such that the intersection of the open disc Bε(λ) and σ(T) is {λ}. The resolvent function RT is holomorphic on A.Extending a result from classical function theory, RT has a Laurent series representation on A:whereBy the previous discussion on the functional calculus,But we have shown that the smallest positive integer m such thatis precisely the index of λ, ν(λ). In other words, the function RT has a pole of order ν(λ) at λ.This example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.Consider the matrixwhich is mentioned in the beginning of the article.The characteristic polynomial of A isThis shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation Av = λ v. It is spanned by the column vector v = (−1, 1, 0, 0)T. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by w = (1, −1, 0, 1)T. Finally, the eigenspace corresponding to the eigenvalue 4 is also one-dimensional (even though this is a double eigenvalue) and is spanned by x = (1, 0, −1, 1)T. So, the geometric multiplicity (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix A is the direct sumThere are three chains. Two have length one: {v} and {w}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculatewhere I is the 4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of A − 4I, e.g., y = (1,0,0,0)T. Now, (A − 4I)y = x and (A − 4I)x = 0, so {y, x} is a chain of length two corresponding to the eigenvalue 4.The transition matrix P such that P−1AP = J is formed by putting these vectors next to each other as followsA computation shows that the equation P−1AP = J indeed holds.If we had interchanged the order of which the chain vectors appeared, that is, changing the order of v, w and {x, y} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.If the matrix A has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrixIf ε = 0, then the Jordan normal form is simplyHowever, for ε ≠ 0, the Jordan normal form isThis ill conditioning makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in numerical analysis; the stable Schur decomposition[15] or pseudospectra[16] are better alternatives.The Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let f(z) be an analytical function of a complex argument. Applying the function on a n×n Jordan block J with eigenvalue λ results in an upper triangular matrix:The following example shows the application to the power function f(z)=zn:
Semi-simple operator
In mathematics, a linear operator T on a finite-dimensional vector space is semi-simple if every T-invariant subspace has a  complementary T-invariant subspace.[1]An important result regarding semi-simple operators is that, a linear operator on a finite dimensional vector space over an algebraically closed field is semi-simple if and only if it is diagonalizable.[1] This is because such an operator always has an eigenvector; if it is, in addition, semi-simple, then it has a complementary invariant hyperplane, which itself has an eigenvector, and thus by induction is diagonalizable. Conversely, diagonalizable operators are easily seen to be semi-simple, as invariant subspaces are direct sums of eigenspaces, and any basis for this space can be extended to an eigenbasis.
Matrix addition
In mathematics, matrix addition is the operation of adding two matrices by adding the corresponding entries together. However, there are other operations which could also be considered as a kind of addition for matrices, the direct sum and the Kronecker sum.Two matrices must have an equal number of rows and columns to be added.[1] The sum of two matrices A and B will be a matrix which has the same number of rows and columns as do A and B. The sum of A and B, denoted A + B, is computed by adding corresponding elements of A and B:[2][3]For example:We can also subtract one matrix from another, as long as they have the same dimensions. A − B is computed by subtracting elements of B from corresponding elements of A, and has the same dimensions as A and B. For example:Another operation, which is used less often, is the direct sum (denoted by ⊕). Note the Kronecker sum is also denoted ⊕; the context should make the usage clear.  The direct sum of any pair of matrices A of size m × n and B of size p × q is a matrix of size (m + p) × (n + q) defined as [4][2]For instance,The direct sum of matrices is a special type of block matrix, in particular the direct sum of square matrices is a block diagonal matrix.The adjacency matrix of the union of disjoint graphs or multigraphs is the direct sum of their adjacency matrices. Any element in the direct sum of two vector spaces of matrices can be represented as a direct sum of two matrices.In general, the direct sum of n matrices is:[2]where the zeros are actually blocks of zeros, i.e. zero matrices.
Eigenvalues and eigenvectors
In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it. More formally, if T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v.  This condition can be written as the equationwhere λ is a scalar in the field F, known as the eigenvalue,  characteristic value, or characteristic root associated with the eigenvector v.If the vector space V is finite-dimensional, then the linear transformation T can be represented as a square matrix A, and the vector v by a column vector, rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the column vector on the right hand side in the equationThere is a direct correspondence between n-by-n square matrices and linear transformations from an n-dimensional vector space to itself, given any basis of the vector space. For this reason, it is equivalent to define eigenvalues and eigenvectors using either the language of matrices or the language of linear transformations.[1][2]Geometrically, an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction that is stretched by the transformation and the eigenvalue is the factor by which it is stretched.  If the eigenvalue is negative, the direction is reversed.[3]Eigenvalues and eigenvectors feature prominently in the analysis of linear transformations. The prefix eigen- is adopted from the German word eigen for "proper", "characteristic".[4] Originally utilized to study principal axes of the rotational motion of rigid bodies, eigenvalues and eigenvectors have a wide range of applications, for example in stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization.In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value λ, called an eigenvalue. This condition can be written as the equationreferred to as the eigenvalue equation or eigenequation. In general, λ may be any scalar. For example, λ may be negative, in which case the eigenvector reverses direction as part of the scaling, or it may be zero or complex.The Mona Lisa example pictured at right provides a simple illustration. Each point on the painting can be represented as a vector pointing from the center of the painting to that point. The linear transformation in this example is called a shear mapping. Points in the top half are moved to the right and points in the bottom half are moved to the left proportional to how far they are from the horizontal axis that goes through the middle of the painting. The vectors pointing to each point in the original image are therefore tilted right or left and made longer or shorter by the transformation. Notice that points along the horizontal axis do not move at all when this transformation is applied. Therefore, any vector that points directly to the right or left with no vertical component is an eigenvector of this transformation because the mapping does not change its direction. Moreover, these eigenvectors all have an eigenvalue equal to one because the mapping does not change their length, either.Alternatively, the linear transformation could take the form of an n by n matrix, in which case the eigenvectors are n by 1 matrices that are also referred to as eigenvectors. If the linear transformation is expressed in the form of an n by n matrix A, then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplicationwhere the eigenvector v is an n by 1 matrix. For a matrix, eigenvalues and eigenvectors can be used to decompose the matrix, for example by diagonalizing it.Eigenvalues and eigenvectors give rise to many closely related mathematical concepts, and the prefix eigen- is applied liberally when naming them:Eigenvalues are often introduced in the context of linear algebra or matrix theory. Historically, however, they arose in the study of quadratic forms and differential equations.In the 18th century Euler studied the rotational motion of a rigid body and discovered the importance of the principal axes.[9] Lagrange realized that the principal axes are the eigenvectors of the inertia matrix.[10] In the early 19th century, Cauchy saw how their work could be used to classify the quadric surfaces, and generalized it to arbitrary dimensions.[11] Cauchy also coined the term racine caractéristique (characteristic root) for what is now called eigenvalue; his term survives in characteristic equation.[12][13]Fourier used the work of Laplace and Lagrange to solve the heat equation by separation of variables in his famous 1822 book Théorie analytique de la chaleur.[14] Sturm developed Fourier's ideas further and brought them to the attention of Cauchy, who combined them with his own ideas and arrived at the fact that real symmetric matrices have real eigenvalues.[11] This was extended by Hermite in 1855 to what are now called Hermitian matrices.[12] Around the same time, Brioschi proved that the eigenvalues of orthogonal matrices lie on the unit circle,[11] and Clebsch found the corresponding result for skew-symmetric matrices.[12] Finally, Weierstrass clarified an important aspect in the stability theory started by Laplace by realizing that defective matrices can cause instability.[11]In the meantime, Liouville studied eigenvalue problems similar to those of Sturm; the discipline that grew out of their work is now called Sturm–Liouville theory.[15] Schwarz studied the first eigenvalue of Laplace's equation on general domains towards the end of the 19th century, while Poincaré studied Poisson's equation a few years later.[16]At the start of the 20th century, Hilbert studied the eigenvalues of integral operators by viewing the operators as infinite matrices.[17] He was the first to use the German word eigen, which means "own", to denote eigenvalues and eigenvectors in 1904,[18] though he may have been following a related usage by Helmholtz. For some time, the standard term in English was "proper value", but the more distinctive term "eigenvalue" is standard today.[19]The first numerical algorithm for computing eigenvalues and eigenvectors appeared in 1929, when Von Mises published the power method. One of the most popular methods today, the QR algorithm, was proposed independently by John G.F. Francis[20] and Vera Kublanovskaya[21] in 1961.[22]Eigenvalues and eigenvectors are often introduced to students in the context of linear algebra courses focused on matrices.[23][24] Furthermore, linear transformations can be represented using matrices,[1][2] which is especially common in numerical and computational applications.[25]Consider n-dimensional vectors that are formed as a list of n scalars, such as the three-dimensional vectorsThese vectors are said to be scalar multiples of each other, or parallel or collinear, if there is a scalar λ such thatIn this case λ = −1/20.Now consider the linear transformation of n-dimensional vectors defined by an n by n matrix A,orwhere, for each row,If it occurs that v and w are scalar multiples, that is if    (1)then v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to that eigenvector. Equation (1) is the eigenvalue equation for the matrix A.Equation (1) can be stated equivalently as    (2)where I is the n by n identity matrix.Equation (2) has a non-zero solution v if and only if the determinant of the matrix (A − λI) is zero. Therefore, the eigenvalues of A are values of λ that satisfy the equation    (3)Using Leibniz' rule for the determinant, the left hand side of Equation (3) is a polynomial function of the variable λ and the degree of this polynomial is n, the order of the matrix A. Its coefficients depend on the entries of A, except that its term of degree n is always (−1)nλn. This polynomial is called the characteristic polynomial of A. Equation (3) is called the characteristic equation or the secular equation of A.The fundamental theorem of algebra implies that the characteristic polynomial of an n-by-n matrix A, being a polynomial of degree n, can be factored into the product of n linear terms,    (4)where each λi may be real but in general is a complex number. The numbers λ1, λ2, … λn, which may not all have distinct values, are roots of the polynomial and are the eigenvalues of A.As a brief example, which is described in more detail in the examples section later, consider the matrixTaking the determinant of (M − λI), the characteristic polynomial of M isSetting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of M. The eigenvectors corresponding to each eigenvalue can be found by solving for the components of v in the equation Mv = λv. In this example, the eigenvectors are any non-zero scalar multiples ofIf the entries of the matrix A are all real numbers, then the coefficients of the characteristic polynomial will also be real numbers, but the eigenvalues may still have non-zero imaginary parts. The entries of the corresponding eigenvectors therefore may also have non-zero imaginary parts. Similarly, the eigenvalues may be irrational numbers even if all the entries of A are rational numbers or even if they are all integers. However, if the entries of A are all algebraic numbers, which include the rationals, the eigenvalues are complex algebraic numbers.The non-real roots of a real polynomial with real coefficients can be grouped into pairs of complex conjugates, namely with the two members of each pair having imaginary parts that differ only in sign and the same real part. If the degree is odd, then by the intermediate value theorem at least one of the roots is real. Therefore, any real matrix with odd order has at least one real eigenvalue, whereas a real matrix with even order may not have any real eigenvalues. The eigenvectors associated with these complex eigenvalues are also complex and also appear in complex conjugate pairs.Let λi be an eigenvalue of an n by n matrix A. The algebraic multiplicity μA(λi) of the eigenvalue is its multiplicity as a root of the characteristic polynomial, that is, the largest integer k such that (λ − λi)k divides evenly that polynomial.[8][26][27]Suppose a matrix A has dimension n and d ≤ n distinct eigenvalues. Whereas Equation (4) factors the characteristic polynomial of A into the product of n linear terms with some terms potentially repeating, the characteristic polynomial can instead be written as the product of d terms each corresponding to a distinct eigenvalue and raised to the power of the algebraic multiplicity,If d = n then the right hand side is the product of n linear terms and this is the same as Equation (4). The size of each eigenvalue's algebraic multiplicity is related to the dimension n asIf μA(λi) = 1, then λi is said to be a simple eigenvalue.[27] If μA(λi) equals the geometric multiplicity of λi, γA(λi), defined in the next section, then λi is said to be a semisimple eigenvalue.Given a particular eigenvalue λ of the n by n matrix A, define the set E to be all vectors v that satisfy Equation (2),On one hand, this set is precisely the kernel or nullspace of the matrix (A − λI). On the other hand, by definition, any non-zero vector that satisfies this condition is an eigenvector of A associated with λ. So, the set E is the union of the zero vector with the set of all eigenvectors of A associated with λ, and E equals the nullspace of (A − λI). E is called the eigenspace or characteristic space of A associated with λ.[7][8] In general λ is a complex number and the eigenvectors are complex n by 1 matrices. A property of the nullspace is that it is a linear subspace, so E is a linear subspace of ℂn.Because the eigenspace E is a linear subspace, it is closed under addition. That is, if two vectors u and v belong to the set E, written (u,v) ∈ E, then (u + v) ∈ E or equivalently A(u + v) = λ(u + v). This can be checked using the distributive property of matrix multiplication. Similarly, because E is a linear subspace, it is closed under scalar multiplication. That is, if v ∈ E and α is a complex number,  (αv) ∈ E or equivalently A(αv) = λ(αv). This can be checked by noting that multiplication of complex matrices by complex numbers is commutative. As long as u + v and αv are not zero, they are also eigenvectors of A associated with λ.The dimension of the eigenspace E associated with λ, or equivalently the maximum number of linearly independent eigenvectors associated with λ, is referred to as the eigenvalue's geometric multiplicity γA(λ). Because E is also the nullspace of (A − λI), the geometric multiplicity of λ is the dimension of the nullspace of (A − λI), also called the nullity of (A − λI), which relates to the dimension and rank of (A − λI) asBecause of the definition of eigenvalues and eigenvectors, an eigenvalue's geometric multiplicity must be at least one, that is, each eigenvalue has at least one associated eigenvector. Furthermore, an eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity. Additionally, recall that an eigenvalue's algebraic multiplicity cannot exceed n.Suppose A has d ≤ n distinct eigenvalues λ1, λ2, …, λd, where the geometric multiplicity of λi is γA(λi). The total geometric multiplicity of A,is the dimension of the union of all the eigenspaces of A's eigenvalues, or equivalently the maximum number of linearly independent eigenvectors of A. If γA = n, thenLet A be an arbitrary n by n matrix of complex numbers with eigenvalues λ1, λ2, ..., λn. Each eigenvalue appears μA(λi) times in this list, where μA(λi) is the eigenvalue's algebraic multiplicity. The following are properties of this matrix and its eigenvalues:Many disciplines traditionally represent vectors as matrices with a single column rather than as matrices with a single row. For that reason, the word "eigenvector" in the context of matrices almost always refers to a right eigenvector, namely a column vector that right multiplies the n by n matrix A in the defining equation, Equation (1),The eigenvalue and eigenvector problem can also be defined for row vectors that left multiply matrix A. In this formulation, the defining equation iswhere κ is a scalar and u is a 1 by n matrix. Any row vector u satisfying this equation is called a left eigenvector of A and κ is its associated eigenvalue. Taking the transpose of this equation,Comparing this equation to Equation (1), it follows immediately that a left eigenvector of A is the same as the transpose of a right eigenvector of AT, with the same eigenvalue. Furthermore, since the characteristic polynomial of AT is the same as the characteristic polynomial of A, the eigenvalues of the left eigenvectors of A are the same as the eigenvalues of the right eigenvectors of AT.Suppose the eigenvectors of A form a basis, or equivalently A has n linearly independent eigenvectors v1, v2, …, vn with associated eigenvalues λ1, λ2, …, λn. The eigenvalues need not be distinct. Define a square matrix Q whose columns are the n linearly independent eigenvectors of A,Since each column of Q is an eigenvector of A, right multiplying A by Q scales each column of Q by its associated eigenvalue,With this in mind, define a diagonal matrix Λ where each diagonal element Λii is the eigenvalue associated with the ith column of Q. ThenBecause the columns of Q are linearly independent, Q is invertible. Right multiplying both sides of the equation by Q−1,or by instead left multiplying both sides by Q−1,A can therefore be decomposed into a matrix composed of its eigenvectors, a diagonal matrix with its eigenvalues along the diagonal, and the inverse of the matrix of eigenvectors. This is called the eigendecomposition and it is a similarity transformation. Such a matrix A is said to be similar to the diagonal matrix Λ or diagonalizable. The matrix Q is the change of basis matrix of the similarity transformation. Essentially, the matrices A and Λ represent the same linear transformation expressed in two different bases. The eigenvectors are used as the basis when representing the linear transformation as Λ.Conversely, suppose a matrix A is diagonalizable. Let P be a non-singular square matrix such that P−1AP is some diagonal matrix D. Left multiplying both by P, AP = PD. Each column of P must therefore be an eigenvector of A whose eigenvalue is the corresponding diagonal element of D. Since the columns of P must be linearly independent for P to be invertible, there exist n linearly independent eigenvectors of A. It then follows that the eigenvectors of A form a basis if and only if A is diagonalizable.A matrix that is not diagonalizable is said to be defective. For defective matrices, the notion of eigenvectors generalizes to generalized eigenvectors and the diagonal matrix of eigenvalues generalizes to the Jordan normal form. Over an algebraically closed field, any matrix A has a Jordan normal form and therefore admits a basis of generalized eigenvectors and a decomposition into generalized eigenspaces.Consider the matrixThe figure on the right shows the effect of this transformation on point coordinates in the plane.The eigenvectors v of this transformation satisfy Equation (1), and the values of λ for which the determinant of the matrix (A − λI) equals zero are the eigenvalues.Taking the determinant to find characteristic polynomial of A,Setting the characteristic polynomial equal to zero, it has roots at λ = 1 and λ = 3, which are the two eigenvalues of A.For λ = 1, Equation (2) becomes,Any non-zero vector with v1 = −v2 solves this equation. Therefore,is an eigenvector of A corresponding to λ = 1, as is any scalar multiple of this vector.For λ = 3, Equation (2) becomesAny non-zero vector with v1 = v2 solves this equation. Therefore,is an eigenvector of A corresponding to λ = 3, as is any scalar multiple of this vector.Thus, the vectors vλ=1 and vλ=3 are eigenvectors of A associated with the eigenvalues λ = 1 and λ = 3, respectively.Consider the matrixThe characteristic polynomial of A isConsider the cyclic permutation matrixThis matrix shifts the coordinates of the vector up by one position and moves the first coordinate to the bottom. Its characteristic polynomial is 1 − λ3, whose roots areFor the real eigenvalue λ1 = 1, any vector with three equal non-zero entries is an eigenvector. For example,For the complex conjugate pair of imaginary eigenvalues, note thatThenandMatrices with entries only along the main diagonal are called diagonal matrices.  The eigenvalues of a diagonal matrix are the diagonal elements themselves. Consider the matrixThe characteristic polynomial of A iswhich has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.Each diagonal element corresponds to an eigenvector whose only non-zero component is in the same row as that diagonal element. In the example, the eigenvalues correspond to the eigenvectors,respectively, as well as scalar multiples of these vectors.A matrix whose elements above the main diagonal are all zero is called a lower triangular matrix, while a matrix whose elements below the main diagonal are all zero is called an upper triangular matrix.  As with diagonal matrices, the eigenvalues of triangular matrices are the elements of the main diagonal.Consider the lower triangular matrix,The characteristic polynomial of A iswhich has the roots λ1 = 1, λ2 = 2, and λ3 = 3. These roots are the diagonal elements as well as the eigenvalues of A.These eigenvalues correspond to the eigenvectors,respectively, as well as scalar multiples of these vectors.As in the previous example, the lower triangular matrixhas a characteristic polynomial that is the product of its diagonal elements,The roots of this polynomial, and hence the eigenvalues, are 2 and 3. The algebraic multiplicity of each eigenvalue is 2; in other words they are both double roots. The sum of the algebraic multiplicities of each distinct eigenvalue is μA = 4 = n, the order of the characteristic polynomial and the dimension of A.The definitions of eigenvalue and eigenvectors of a linear transformation T remains valid even if the underlying vector space is an infinite-dimensional Hilbert or Banach space. A widely used class of linear transformations acting on infinite-dimensional spaces are the differential operators on function spaces. Let D be a linear differential operator on the space C∞ of infinitely differentiable real functions of a real argument t. The eigenvalue equation for D is the differential equationThe functions that satisfy this equation are eigenvectors of D and are commonly called eigenfunctions.This differential equation can be solved by multiplying both sides by dt/f(t) and integrating. Its solution, the exponential functionis the eigenfunction of the derivative operator. Note that in this case the eigenfunction is itself a function of its associated eigenvalue. In particular, note that for λ = 0 the eigenfunction f(t) is a constant.The main eigenfunction article gives other examples.The concept of eigenvalues and eigenvectors extends naturally to arbitrary linear transformations on arbitrary vector spaces. Let V be any vector space over some field K of scalars, and let T be a linear transformation mapping V into V,We say that a non-zero vector v ∈ V is an eigenvector of T if and only if there exists a scalar λ ∈ K such that    ( 5)This equation is called the eigenvalue equation for T, and the scalar λ is the eigenvalue of T corresponding to the eigenvector v. Note that T(v) is the result of applying the transformation T to the vector v, while λv is the product of the scalar λ with v.[33]Given an eigenvalue λ, consider the setwhich is the union of the zero vector with the set of all eigenvectors associated with λ. E is called the eigenspace or characteristic space of T associated with λ.By definition of a linear transformation,for (x,y) ∈ V and α ∈ K. Therefore, if u and v are eigenvectors of T associated with eigenvalue λ, namely (u,v) ∈ E, thenSo, both u + v and αv are either zero or eigenvectors of T associated with λ, namely (u + v, αv) ∈ E, and E is closed under addition and scalar multiplication. The eigenspace E associated with λ is therefore a linear subspace of V.[8][34][35] If that subspace has dimension 1, it is sometimes called an eigenline.[36]The geometric multiplicity γT(λ) of an eigenvalue λ is the dimension of the eigenspace associated with λ, i.e., the maximum number of linearly independent eigenvectors associated with that eigenvalue.[8][27] By the definition of eigenvalues and eigenvectors, γT(λ) ≥ 1 because every eigenvalue has at least one eigenvector.The eigenspaces of T always form a direct sum. As a consequence, eigenvectors of different eigenvalues are always linearly independent. Therefore, the sum of the dimensions of the eigenspaces cannot exceed the dimension n of the vector space on which T operates, and there cannot be more than n distinct eigenvalues.[37]Any subspace spanned by eigenvectors of T is an invariant subspace of T, and the restriction of T to such a subspace is diagonalizable. Moreover, if the entire vector space V can be spanned by the eigenvectors of T, or equivalently if the direct sum of the eigenspaces associated with all the eigenvalues of T is the entire vector space V, then a basis of V called an eigenbasis can be formed from linearly independent eigenvectors of T. When T admits an eigenbasis, T is diagonalizable.While the definition of an eigenvector used in this article excludes the zero vector, it is possible to define eigenvalues and eigenvectors such that the zero vector is an eigenvector.[38]Consider again the eigenvalue equation, Equation (5). Define an eigenvalue to be any scalar λ ∈ K such that there exists a non-zero vector v ∈ V satisfying Equation (5). It is important that this version of the definition of an eigenvalue specify that the vector be non-zero, otherwise by this definition the zero vector would allow any scalar in K to be an eigenvalue. Define an eigenvector v associated with the eigenvalue λ to be any vector that, given λ, satisfies Equation (5). Given the eigenvalue, the zero vector is among the vectors that satisfy Equation (5), so the zero vector is included among the eigenvectors by this alternate definition.If λ is an eigenvalue of T, then the operator (T − λI) is not one-to-one, and therefore its inverse (T − λI)−1 does not exist. The converse is true for finite-dimensional vector spaces, but not for infinite-dimensional vector spaces. In general, the operator (T − λI) may not have an inverse even if λ is not an eigenvalue.For this reason, in functional analysis eigenvalues can be generalized to the spectrum of a linear operator T as the set of all scalars λ for which the operator (T − λI) has no bounded inverse. The spectrum of an operator always contains all its eigenvalues but is not limited to them.One can generalize the algebraic object that is acting on the vector space, replacing a single operator acting on a vector space with an algebra representation – an associative algebra acting on a module. The study of such actions is the field of representation theory.The representation-theoretical concept of weight is an analog of eigenvalues, while weight vectors and weight spaces are the analogs of eigenvectors and eigenspaces, respectively.The simplest difference equations have the formThe solution of this equation for x in terms of t is found by using its characteristic equationA similar procedure is used for solving a differential equation of the formThe calculation of eigenvalues and eigenvectors is a topic where theory, as presented in elementary linear algebra textbooks, is often very far from practice.The classical method is to first find the eigenvalues, and then calculate the eigenvectors for each eigenvalue. It is in several ways poorly suited for non-exact arithmetics such as floating-point.Once the (exact) value of an eigenvalue is known, the corresponding eigenvectors can be found by finding non-zero solutions of the eigenvalue equation, that becomes a system of linear equations with known coefficients. For example, once it is known that 6 is an eigenvalue of the matrixThis matrix equation is equivalent to two linear equationsEfficient, accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices were not known until the advent of the QR algorithm in 1961.[39] Combining the Householder transformation with the LU decomposition results in an algorithm with better convergence than the QR algorithm.[citation needed] For large Hermitian sparse matrices, the Lanczos algorithm is one example of an efficient iterative method to compute eigenvalues and eigenvectors, among several other possibilities.[39]Most numeric methods that compute the eigenvalues of a matrix also determine a set of corresponding eigenvectors as a by-product of the computation, although sometimes the implementors choose to discard the eigenvector information as soon as it is not needed anymore.The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors.A linear transformation that takes a square to a rectangle of the same area (a squeeze mapping) has reciprocal eigenvalues.In quantum mechanics, and in particular in atomic and molecular physics, within the Hartree–Fock theory, the atomic and molecular orbitals can be defined by the eigenvectors of the Fock operator. The corresponding eigenvalues are interpreted as ionization potentials via Koopmans' theorem. In this case, the term eigenvector is used in a somewhat more general meaning, since the Fock operator is explicitly dependent on the orbitals and their eigenvalues. Thus, if one wants to underline this aspect, one speaks of nonlinear eigenvalue problems. Such equations are usually solved by an iteration procedure, called in this case self-consistent field method. In quantum chemistry, one often represents the Hartree–Fock equation in a non-orthogonal basis set. This particular representation is a generalized eigenvalue problem called Roothaan equations.In geology, especially in the study of glacial till, eigenvectors and eigenvalues are used as a method by which a mass of information of a clast fabric's constituents' orientation and dip can be summarized in a 3-D space by six numbers. In the field, a geologist may collect such data for hundreds or thousands of clasts in a soil sample, which can only be compared graphically such as in a Tri-Plot (Sneed and Folk) diagram,[40][41] or as a Stereonet on a Wulff Net.[42]The eigendecomposition of a symmetric positive semidefinite (PSD) matrix yields an orthogonal basis of eigenvectors, each of which has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.Principal component analysis is used to study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, in the field of psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal component analysis can be used as a method of factor analysis in structural equation modeling.Eigenvalue problems occur naturally in the vibration analysis of mechanical structures with many degrees of freedom. The eigenvalues are the natural frequencies (or eigenfrequencies) of vibration, and the eigenvectors are the shapes of these vibrational modes. In particular, undamped vibration is governed byorleads to a so-called quadratic eigenvalue problem,This can be reduced to a generalized eigenvalue problem by algebraic manipulation at the cost of solving a larger system.The orthogonality properties of the eigenvectors allows decoupling of the differential equations so that the system can be represented as linear summation of the eigenvectors. The eigenvalue problem of complex structures is often solved using finite element analysis, but neatly generalize the solution to scalar-valued vibration problems.In image processing, processed images of faces can be seen as vectors whose components are the brightnesses of each pixel.[45] The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; this is an example of principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.Similar to this concept, eigenvoices represent the general direction of variability in human pronunciations of a particular utterance, such as a word in a language. Based on a linear combination of such eigenvoices, a new voice pronunciation of the word can be constructed. These concepts have been found useful in automatic speech recognition systems for speaker adaptation.In mechanics, the eigenvectors of the moment of inertia tensor define the principal axes of a rigid body. The tensor of moment of inertia is a key quantity required to determine the rotation of a rigid body around its center of mass.In solid mechanics, the stress tensor is symmetric and so can be decomposed into a diagonal tensor with the eigenvalues on the diagonal and eigenvectors as a basis. Because it is diagonal, in this orientation, the stress tensor has no shear components; the components it does have are the principal components.The principal eigenvector is used to measure the centrality of its vertices. An example is Google's PageRank algorithm. The principal eigenvector of a modified adjacency matrix of the World Wide Web graph gives the page ranks as its components. This vector corresponds to the stationary distribution of the Markov chain represented by the row-normalized adjacency matrix; however, the adjacency matrix must first be modified to ensure a stationary distribution exists. The second smallest eigenvector can be used to partition the graph into clusters, via spectral clustering. Other methods are also available for clustering.
Unit vector
The same construct is used to specify spatial directions in 3D.  As illustrated, each unique direction is equivalent numerically to a point on the unit sphere.The normalized vector or versor û of a non-zero vector u is the unit vector in the direction of u, i.e.,where |u| is the norm (or length) of u.  The term normalized vector is sometimes used as a synonym for unit vector.Unit vectors are often chosen to form the basis of a vector space. Every vector in the space may be written as a linear combination of unit vectors.By definition, in a Euclidean space the dot product of two unit vectors is a scalar value amounting to the cosine of the smaller subtended angle. In three-dimensional Euclidean space, the cross product of two arbitrary unit vectors is a third vector orthogonal to both of them having length equal to the sine of the smaller subtended angle. The normalized cross product corrects for this varying length, and yields the mutually orthogonal unit vector to the two inputs, applying the right-hand rule to resolve one of two possible directions.Unit vectors may be used to represent the axes of a Cartesian coordinate system. For instance, the unit vectors in the direction of the x, y, and z axes of a three dimensional Cartesian coordinate system areThey are sometimes referred to as the versors of the coordinate system, and they form a set of mutually orthogonal unit vectors, typically referred to as a standard basis in linear algebra.When a unit vector in space is expressed, with Cartesian notation, as a linear combination of i, j, k, its three scalar components can be referred to as direction cosines. The value of each component is equal to the cosine of the angle formed by the unit vector with the respective basis vector. This is one of the methods used to describe the orientation (angular position) of a straight line, segment of straight line, oriented axis, or segment of oriented axis (vector).The three orthogonal unit vectors appropriate to cylindrical symmetry are: Common general themes of unit vectors occur throughout physics and geometry:[2]Unit vector at acute deviation angle φ (including 0 or π/2 rad) relative to a principal direction.
Predictor–corrector method
In numerical analysis, predictor–corrector methods belong to a class of algorithms designed to integrate ordinary differential equations – to find an unknown function that satisfies a given differential equation.  All such algorithms proceed in two steps: When considering the numerical solution of ordinary differential equations (ODEs), a predictor–corrector method typically uses an explicit method for the predictor step and an implicit method for the corrector step.A simple predictor–corrector method (known as Heun's method) can be constructed from the Euler method (an explicit method) and the trapezoidal rule (an implicit method).Consider the differential equationNext, the corrector step: improve the initial guess using trapezoidal rule,That value is used as the next step.There are different variants of a predictor–corrector method, depending on how often the corrector method is applied. The Predict–Evaluate–Correct–Evaluate (PECE) mode refers to the variant in the above example:It is also possible to evaluate the function f only once per step by using the method in Predict–Evaluate–Correct (PEC) mode:Additionally, the corrector step can be repeated in the hope that this achieves an even better approximation to the true solution. If the corrector method is run twice, this yields the PECECE mode:The PECEC mode has one fewer function evaluation. More generally, if the corrector is run k times, the method is in P(EC)kor P(EC)kE mode. If the corrector method is iterated until it converges, this could be called PE(CE)∞.[1]
Computing the permanent
In linear algebra, the computation of the permanent of a matrix is a problem that is thought to be more difficult than the computation of the determinant of a matrix despite the apparent similarity of the definitions.The permanent is defined similarly to the determinant, as a sum of products of sets of matrix entries that lie in distinct rows and columns. However, where the determinant weights each of these products with a ±1 sign based on the parity of the set, the permanent weights them all with a +1 sign.While the determinant can be computed in polynomial time by Gaussian elimination, it is generally believed that the permanent cannot be computed in polynomial time. In computational complexity theory, a theorem of Valiant states that computing permanents is #P-hard, and even #P-complete for matrices in which all entries are 0 or 1.Valiant (1979)  This puts the computation of the permanent in a class of problems believed to be even more difficult to compute than NP. It is known that computing the permanent is impossible for logspace-uniform ACC0 circuits.(Allender & Gore 1994)The development of both exact and approximate algorithms for computing the permanent of a matrix is an active area of research.The permanent of an n-by-n matrix A = (ai,j) is defined asThe sum here extends over all elements σ of the symmetric group Sn, i.e. over all permutations of the numbers 1, 2, ..., n. This formula differs from the corresponding formula for the determinant only in that, in the determinant, each product is multiplied by the sign of the permutation σ while in this formula each product is unsigned. The formula may be directly translated into an algorithm that naively expands the formula, summing over all permutations and within the sum multiplying out each matrix entry. This requires n! n arithmetic operations.It may be rewritten in terms of the matrix entries as follows[3]Another formula that appears to be as fast as Ryser's (or perhaps even twice as fast) is to be found in the two Ph.D. theses; see (Balasubramanian 1980), (Bax 1998); also(Bax & Franklin 1996). The methods to find the formula are quite different, being related to the combinatorics of the Muir algebra, and to finite difference theory respectively.  Another way, connected with invariant theory is via the polarization identity for a symmetric tensor (Glynn 2010).  The formula generalizes to infinitely many others, as found by all these authors, although it is not clear if they are any faster than the basic one. See (Glynn 2013).The simplest known formula of this type (when the characteristic of the field is not two) isThe number of perfect matchings in a bipartite graph is counted by the permanent of the graph's biadjacency matrix, and the permanent of any 0-1 matrix can be interpreted in this way as the number of perfect matchings in a graph. For planar graphs (regardless of bipartiteness), the FKT algorithm computes the number of perfect matchings in polynomial time by changing the signs of a carefully chosen subset of the entries in the Tutte matrix of the graph, so that the Pfaffian of the resulting skew-symmetric matrix (the square root of its determinant) is the number of perfect matchings. This technique can be generalized to graphs that contain no subgraph homeomorphic to the complete bipartite graph K3,3.[5]There are various formulae given by Glynn (2010) for the computation modulo a prime p.Firstly, there is one using symbolic calculations with partial derivatives.(Actually the above expansion can be generalized in an arbitrary characteristic p as the following pair of dual identities:This formula implies the following identities over fields of characteristic 3:and it allows to polynomial-time reduce the computation of the permanent of an nxn-matrix with a subset of k or k-1 rows expressible as linear combinations of another (disjoint) subset of k rows to the computation of the permanent of an (n-k)x(n-k)- or (n-k+1)x(n-k+1)-matrix correspondingly, hence having introduced a compression operator (analogical to the Gaussian modification applied for calculating the determinant) that "preserves" the permanent in characteristic 3. (Analogically, it would be worth noting that the Hamiltonian cycle polynomial in characteristic 2 does possess its invariant matrix compressions as well, taking into account the fact that ham(A) = 0 for any nxn-matrix A having three equal rows or, if n > 2, a pair of indexes i,j such that its i-th and j-th rows are identical and its i-th and j-th columns are identical too.) The closure of that operator defined as the limit of its sequential application together with the transpose transformation (utilized each time the operator leaves the matrix intact) is also an operator mapping, when applied to classes of matrices, one class to another. While the compression operator maps the class of 1-semi-unitary matrices into itself and the classes of unitary and 2-semi-unitary ones, the compression-closure of the 1-semi-unitary class (as well as the class of matrices received from unitary ones through replacing one row by an arbitrary row vector — the permanent of such a matrix is, via the Laplace expansion, the sum of the permanents of 1-semi-unitary matrices and, accordingly, polynomial-time computable) is yet unknown and tensely related to the general problem of the permanent's computational complexity in characteristic 3 and the chief question of P versus NP: as it was shown in (Knezevic & Cohen (2017)), if such a compression-closure is the set of all square matrices over a field of characteristic 3 or, at least, contains a matrix class the permanent's computation on is #3-P-complete (like the class of 2-semi-unitary matrices) then the permanent is computable in polynomial time in this characteristic.When the entries of A are nonnegative, the permanent can be computed approximately in probabilistic polynomial time, up to an error of εM, where M is the value of the permanent and ε > 0 is arbitrary. In other words, there exists a fully polynomial-time randomized approximation scheme (FPRAS) (Jerrum, Vigoda & Sinclair (2001)).The most difficult step in the computation is the construction of an algorithm to sample almost uniformly from the set of all perfect matchings in a given bipartite graph: in other words, a fully polynomial almost uniform sampler (FPAUS). This can be done using a Markov chain Monte Carlo algorithm that uses a Metropolis rule to define and run a Markov chain whose distribution is close to uniform, and whose mixing time is polynomial.Another class of matrices for which the permanent can be computed approximately, is the set of positive-semidefinite matrices (the complexity-theoretic problem of approximating the permanent of such matrices to within a multiplicative error is considered open[7]). The corresponding randomized algorithm is based on the model of boson sampling and it uses the tools proper to quantum optics, to represent the permanent of positive-semidefinite matrices as the expected value of a specific random variable. The latter is then approximated by its sample mean.[8] This algorithm, for a certain set of positive-semidefinite matrices, approximates their permanent in polynomial time up to an additive error, which is more reliable than that of the standard classical polynomial-time algorithm by Gurvits.[9]
Matrix analysis
In mathematics, particularly in linear algebra and applications, matrix analysis is the study of matrices and their algebraic properties.[1] Some particular topics out of many include; operations defined on matrices (such as matrix addition, matrix multiplication and operations derived from these), functions of matrices (such as matrix exponentiation and matrix logarithm, and even sines and cosines etc. of matrices),[2] and the eigenvalues of matrices (eigendecomposition of a matrix, eigenvalue perturbation theory).The set of all m×n matrices over a number field F denoted in this article Mmn(F) form a vector space. Examples of F include the set of integers ℤ, the real numbers ℝ, and set of complex numbers ℂ. The spaces Mmn(F) and Mpq(F) are different spaces if m and p are unequal, and if n and q are unequal; for instance M32(F) ≠ M23(F). Two m×n matrices A and B in Mmn(F) can be added together to form another matrix in the space Mmn(F):and multiplied by a α in F, to obtain another matrix in Mmn(F):Combining these two properties, a linear combination of matrices A and B are in Mmn(F) is another matrix in Mmn(F):where α and β are numbers in F.Any matrix can be expressed as a linear combination of basis matrices, which play the role of the basis vectors for the matrix space. For example, for the set of 2×2 matrices over the field of real numbers, M22(ℝ), one legitimate basis set of matrices is:because any 2×2 matrix can be expressed as:where a, b, c,d are all real numbers. This idea applies to other fields and matrices of higher dimensions.The determinant of a square matrix is an important property. The determinant indicates if a matrix is invertible (i.e. the inverse of a matrix exists when the determinant is nonzero). Determinants are used for finding eigenvalues of matrices (see below), and for solving a system of linear equations (see Cramer's rule).An n×n matrix A has eigenvectors x and eigenvalues λ defined by the relation:In words, the matrix multiplication of A followed by an eigenvector x (here an n-dimensional column matrix), is the same as multiplying the eigenvector by the eigenvalue. For an n×n matrix, there are n eigenvalues. The eigenvalues are the roots of the characteristic polynomial:where I is the n×n identity matrix.Roots of polynomials, in this context the eigenvalues, can all be different, or some may be equal (in which case eigenvalue has multiplicity, the number of times an eigenvalue occurs). After solving for the eigenvalues, the eigenvectors corresponding to the eigenvalues can be found by the defining equation.Two n×n matrices A and B are similar if they are related by a similarity transformation:The matrix P is called a similarity matrix, and is necessarily invertible.LU decomposition splits a matrix into a matrix product of an upper triangular matrix and a lower triangle matrix.Since matrices form vector spaces, one can form axioms (analogous to those of vectors) to define a "size" of a particular matrix. The norm of a matrix is a positive real number.For all matrices A and B in Mmn(F), and all numbers α in F, a matrix norm, delimited by double vertical bars || ... ||, fulfills:[note 1]The Frobenius norm is analogous to the dot product of Euclidean vectors; multiply matrix elements entry-wise, add up the results, then take the positive square root:It is defined for matrices of any dimension (i.e. no restriction to square matrices).Matrix elements are not restricted to constant numbers, they can be mathematical variables.A functions of a matrix takes in a matrix, and return something else (a number, vector, matrix, etc...).A matrix valued function takes in something (a number, vector, matrix, etc...) and returns a matrix.
Real number
In mathematics, a real number is a value of a continuous quantity that can represent a distance along a line. The adjective real in this context was introduced in the 17th century by René Descartes, who distinguished between real and imaginary roots of polynomials. The real numbers include all the rational numbers, such as the integer −5 and the fraction 4/3, and all the irrational numbers, such as √2 (1.41421356..., the square root of 2, an irrational algebraic number).  Included within the irrationals are the transcendental numbers, such as π (3.14159265...).   In addition to measuring distance, real numbers can be used to measure quantities such as time, mass, energy, velocity, and many more.Real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. Any real number can be determined by a possibly infinite decimal representation, such as that of 8.632, where each consecutive digit is measured in units one tenth the size of the previous one. The real line can be thought of as a part of the complex plane, and complex numbers include real numbers.These descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. The discovery of a suitably rigorous definition of the real numbers – indeed, the realization that a better definition was needed – was one of the most important developments of 19th-century mathematics. The current standard axiomatic definition is that real numbers form the unique Dedekind-complete ordered field (R ; + ; · ; <), up to an isomorphism,[a] whereas popular constructive definitions of real numbers include declaring them as equivalence classes of Cauchy sequences of rational numbers, Dedekind cuts, or infinite decimal representations, together with precise interpretations for the arithmetic operations and the order relation. All these definitions satisfy the axiomatic definition and are thus equivalent.Simple fractions were used by the Egyptians around 1000 BC; the Vedic "Sulba Sutras" ("The rules of chords") in, c. 600 BC, include what may be the first "use" of irrational numbers. The concept of irrationality was implicitly accepted by early Indian mathematicians since Manava (c. 750–690 BC), who were aware that the square roots of certain numbers such as 2 and 61 could not be exactly determined.[1] Around 500 BC, the Greek mathematicians led by Pythagoras realized the need for irrational numbers, in particular the irrationality of the square root of 2.The Middle Ages brought the acceptance of zero, negative, integral, and fractional numbers, first by Indian and Chinese mathematicians, and then by Arabic mathematicians, who were also the first to treat irrational numbers as algebraic objects,[2] which was made possible by the development of algebra. Arabic mathematicians merged the concepts of "number" and "magnitude" into a more general idea of real numbers.[3] The Egyptian mathematician Abū Kāmil Shujā ibn Aslam (c. 850–930) was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots.[4]In the 16th century, Simon Stevin created the basis for modern decimal notation, and insisted that there is no difference between rational and irrational numbers in this regard.In the 17th century, Descartes introduced the term "real" to describe roots of a polynomial, distinguishing them from "imaginary" ones.In the 18th and 19th centuries, there was much work on irrational and transcendental numbers. Johann Heinrich Lambert (1761) gave the first flawed proof that π cannot be rational; Adrien-Marie Legendre (1794) completed the proof,[5] and showed that π is not the square root of a rational number.[6] Paolo Ruffini (1799) and Niels Henrik Abel (1842) both constructed proofs of the Abel–Ruffini theorem: that the general quintic or higher equations cannot be solved by a general formula involving only arithmetical operations and roots.Évariste Galois (1832) developed techniques for determining whether a given equation could be solved by radicals, which gave rise to the field of Galois theory. Joseph Liouville (1840) showed that neither e nor e2 can be a root of an integer quadratic equation, and then established the existence of transcendental numbers; Georg Cantor (1873) extended and greatly simplified this proof.[7] Charles Hermite (1873) first proved that e is transcendental, and Ferdinand von Lindemann (1882), showed that π is transcendental. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and has finally been made elementary by Adolf Hurwitz[8] and Paul Gordan.[9]The development of calculus in the 18th century used the entire set of real numbers without having defined them cleanly. The first rigorous definition was published by Georg Cantor in 1871. In 1874, he showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite. Contrary to widely held beliefs, his first method was not his famous diagonal argument, which he published in 1891. See Cantor's first uncountability proof.Let R denote the set of all real numbers. Then:The last property is what differentiates the reals from the rationals (and from other, more exotic ordered fields). For example, the set of rationals with square less than 2 has a rational upper bound (e.g., 1.5) but no rational least upper bound, because the square root of 2 is not rational.These properties imply Archimedean property (which is not implied by other definitions of completeness). That is, the set of integers is not upper-bounded in the reals. In fact, if this were false, then the integers would have a least upper bound N; then, N – 1 would not be an upper bound, and there would be an integer n such that n > N – 1, and thus n + 1 > N, which is a contradiction with the upper-bound property of N.The real numbers are uniquely specified by the above properties. More precisely, given any two Dedekind-complete ordered fields R1 and R2, there exists a unique field isomorphism from R1 to R2, allowing us to think of them as essentially the same mathematical object.For another axiomatization of ℝ, see Tarski's axiomatization of the reals.The real numbers can be constructed as a completion of the rational numbers in such a way that a sequence defined by a decimal or binary expansion like (3; 3.1; 3.14; 3.141; 3.1415; ...) converges to a unique real number, in this case π. For details and other constructions of real numbers, see construction of the real numbers.More formally, the real numbers have the two basic properties of being an ordered field, and having the least upper bound property. The first says that real numbers comprise a field, with addition and multiplication as well as division by non-zero numbers, which can be totally ordered on a number line in a way compatible with addition and multiplication. The second says that, if a non-empty set of real numbers has an upper bound, then it has a real least upper bound. The second condition distinguishes the real numbers from the rational numbers: for example, the set of rational numbers whose square is less than 2 is a set with an upper bound (e.g. 1.5) but no (rational) least upper bound: hence the rational numbers do not satisfy the least upper bound property.A main reason for using real numbers is that the reals contain all limits. More precisely, a sequence of real numbers has a limit, which is a real number, if (and only if) its elements eventually come and remain arbitrarily close to each other.This is formally defined in the following, and means that the reals are complete (in the sense of metric spaces or uniform spaces, which is a different sense than the Dedekind completeness of the order in the previous section). :A sequence (xn) of real numbers is called a Cauchy sequence if for any ε > 0 there exists an integer N (possibly depending on ε) such that the distance |xn − xm| is less than ε for all n and m that are both greater than N. This definition, originally provided by Cauchy, formalizes the fact that the xn eventually come and remain arbitrarily close to each other.A sequence (xn) converges to the limit x if its elements eventually come and remain arbitrarily close to x, that is, if for any ε > 0 there exists an integer N (possibly depending on ε) such that the distance |xn − x| is less than ε for n greater than N.Every convergent sequence is a Cauchy sequence, and the converse is true for real numbers, and this means that the topological space of the real numbers is complete.The set of rational numbers is not complete. For example, the sequence (1; 1.4; 1.41; 1.414; 1.4142; 1.41421; ...), where each term adds a digit of the decimal expansion of the positive square root of 2, is Cauchy but it does not converge to a rational number (in the real numbers, in contrast, it converges to the positive square root of 2).The completeness property of the reals is the basis on which calculus, and, more generally mathematical analysis are built. In particular, the test that a sequence is a Cauchy sequence allows proving that a sequence has a limit, without computing it, and even without knowing it.For example, the standard series of the exponential functionconverges to a real number for every x, because the sumsThe real numbers are often described as "the complete ordered field", a phrase that can be interpreted in several ways.First, an order can be lattice-complete. It is easy to see that no ordered field can be lattice-complete, because it can have no largest element (given any element z, z + 1 is larger), so this is not the sense that is meant.Additionally, an order can be Dedekind-complete, as defined in the section Axioms. The uniqueness result at the end of that section justifies using the word "the" in the phrase "complete ordered field" when this is the sense of "complete" that is meant. This sense of completeness is most closely related to the construction of the reals from Dedekind cuts, since that construction starts from an ordered field (the rationals) and then forms the Dedekind-completion of it in a standard way.These two notions of completeness ignore the field structure. However, an ordered group (in this case, the additive group of the field) defines a uniform structure, and uniform structures have a notion of completeness (topology); the description in the previous section Completeness is a special case. (We refer to the notion of completeness in uniform spaces rather than the related and better known notion for metric spaces, since the definition of metric space relies on already having a characterization of the real numbers.) It is not true that R is the only uniformly complete ordered field, but it is the only uniformly complete Archimedean field, and indeed one often hears the phrase "complete Archimedean field" instead of "complete ordered field". Every uniformly complete Archimedean field must also be Dedekind-complete (and vice versa), justifying using "the" in the phrase "the complete Archimedean field". This sense of completeness is most closely related to the construction of the reals from Cauchy sequences (the construction carried out in full in this article), since it starts with an Archimedean field (the rationals) and forms the uniform completion of it in a standard way.But the original use of the phrase "complete Archimedean field" was by David Hilbert, who meant still something else by it. He meant that the real numbers form the largest Archimedean field in the sense that every other Archimedean field is a subfield of R. Thus R is "complete" in the sense that nothing further can be added to it without making it no longer an Archimedean field. This sense of completeness is most closely related to the construction of the reals from surreal numbers, since that construction starts with a proper class that contains every ordered field (the surreals) and then selects from it the largest Archimedean subfield.The reals are uncountable; that is: there are strictly more real numbers than natural numbers, even though both sets are infinite. In fact, the cardinality of the reals equals that of the set of subsets (i.e. the power set) of the natural numbers, and Cantor's diagonal argument states that the latter set's cardinality is strictly greater than the cardinality of N. Since the set of algebraic numbers is countable, almost all real numbers are transcendental. The non-existence of a subset of the reals with cardinality strictly between that of the integers and the reals is known as the continuum hypothesis. The continuum hypothesis can neither be proved nor be disproved; it is independent from the axioms of set theory.As a topological space, the real numbers are separable. This is because the set of rationals, which is countable, is dense in the real numbers. The irrational numbers are also dense in the real numbers, however they are uncountable and have the same cardinality as the reals.The real numbers form a metric space: the distance between x and y is defined as the absolute value |x − y|. By virtue of being a totally ordered set, they also carry an order topology; the topology arising from the metric and the one arising from the order are identical, but yield different presentations for the topology – in the order topology as ordered intervals, in the metric topology as epsilon-balls. The Dedekind cuts construction uses the order topology presentation, while the Cauchy sequences construction uses the metric topology presentation. The reals are a contractible (hence connected and simply connected), separable and complete metric space of Hausdorff dimension 1. The real numbers are locally compact but not compact. There are various properties that uniquely specify them; for instance, all unbounded, connected, and separable order topologies are necessarily homeomorphic to the reals.Every nonnegative real number has a square root in R, although no negative number does. This shows that the order on R is determined by its algebraic structure. Also, every polynomial of odd degree admits at least one real root: these two properties make R the premier example of a real closed field. Proving this is the first half of one proof of the fundamental theorem of algebra.The reals carry a canonical measure, the Lebesgue measure, which is the Haar measure on their structure as a topological group normalized such that the unit interval [0;1] has measure 1. There exist sets of real numbers that are not Lebesgue measurable, e.g. Vitali sets.The supremum axiom of the reals refers to subsets of the reals and is therefore a second-order logical statement. It is not possible to characterize the reals with first-order logic alone: the Löwenheim–Skolem theorem implies that there exists a countable dense subset of the real numbers satisfying exactly the same sentences in first-order logic as the real numbers themselves. The set of hyperreal numbers satisfies the same first order sentences as R. Ordered fields that satisfy the same first-order sentences as R are called nonstandard models of R. This is what makes nonstandard analysis work; by proving a first-order statement in some nonstandard model (which may be easier than proving it in R), we know that the same statement must also be true of R.The field R of real numbers is an extension field of the field Q of rational numbers, and R can therefore be seen as a vector space over Q. Zermelo–Fraenkel set theory with the axiom of choice guarantees the existence of a basis of this vector space: there exists a set B of real numbers such that every real number can be written uniquely as a finite linear combination of elements of this set, using rational coefficients only, and such that no element of B is a rational linear combination of the others. However, this existence theorem is purely theoretical, as such a base has never been explicitly described.The well-ordering theorem implies that the real numbers can be well-ordered if the axiom of choice is assumed: there exists a total order on R with the property that every non-empty subset of R has a least element in this ordering. (The standard ordering ≤ of the real numbers is not a well-ordering since e.g. an open interval does not contain a least element in this ordering.) Again, the existence of such a well-ordering is purely theoretical, as it has not been explicitly described. If V=L is assumed in addition to the axioms of ZF, a well ordering of the real numbers can be shown to be explicitly definable by a formula.[10]A real number may be either computable or uncomputable; either algorithmically random or not; and either arithmetically random or not.The real numbers are most often formalized using the Zermelo–Fraenkel axiomatization of set theory, but some mathematicians study the real numbers with other logical foundations of mathematics. In particular, the real numbers are also studied in reverse mathematics and in constructive mathematics.[11]The hyperreal numbers as developed by Edwin Hewitt, Abraham Robinson and others extend the set of the real numbers by introducing infinitesimal and infinite numbers, allowing for building infinitesimal calculus in a way closer to the original intuitions of Leibniz, Euler, Cauchy and others.Edward Nelson's internal set theory enriches the Zermelo–Fraenkel set theory syntactically by introducing a unary predicate "standard". In this approach, infinitesimals are (non-"standard") elements of the set of the real numbers (rather than being elements of an extension thereof, as in Robinson's theory).In the physical sciences, most physical constants such as the universal gravitational constant, and physical variables, such as position, mass, speed, and electric charge, are modeled using real numbers. In fact, the fundamental physical theories such as classical mechanics, electromagnetism, quantum mechanics, general relativity and the standard model are described using mathematical structures, typically smooth manifolds or Hilbert spaces, that are based on the real numbers, although actual measurements of physical quantities are of finite accuracy and precision.Physicists have occasionally suggested that a more fundamental theory would replace the real numbers with quantities that do not form a continuum, but such proposals remain speculative.[12]With some exceptions, most calculators do not operate on real numbers. Instead, they work with finite-precision approximations called floating-point numbers. In fact, most scientific computation uses floating-point arithmetic. Real numbers satisfy the usual rules of arithmetic, but floating-point numbers do not.A real number is called computable if there exists an algorithm that yields its digits. Because there are only countably many algorithms,[14] but an uncountable number of reals, almost all real numbers fail to be computable. Moreover, the equality of two computable numbers is an undecidable problem. Some constructivists accept the existence of only those reals that are computable. The set of definable numbers is broader, but still only countable.In set theory, specifically descriptive set theory, the Baire space is used as a surrogate for the real numbers since the latter have some topological properties (connectedness) that are a technical inconvenience. Elements of Baire space are referred to as "reals".Mathematicians use the symbol R, or, alternatively, ℝ, the letter "R" in blackboard bold (encoded in Unicode as .mw-parser-output .monospaced{font-family:monospace,monospace}U+211D ℝ .mw-parser-output .smallcaps{font-variant:small-caps}DOUBLE-STRUCK CAPITAL R (HTML &#8477;)), to represent the set of all real numbers. As this set is naturally endowed with the structure of a field, the expression field of real numbers is frequently used when its algebraic properties are under consideration.The sets of positive real numbers and negative real numbers are often noted R+ and R−,[15] respectively; R+ and R− are also used.[16] The non-negative real numbers can be noted R≥0 but one often sees this set noted R+ ∪ {0}.[15] In French mathematics, the positive real numbers and negative real numbers commonly include zero, and these sets are noted respectively ℝ+ and ℝ−.[16] In this understanding, the respective sets without zero are called strictly positive real numbers and strictly negative real numbers, and are noted ℝ+* and ℝ−*.[16]The notation Rn refers to the cartesian product of n copies of R, which is an n-dimensional vector space over the field of the real numbers; this vector space may be identified to the n-dimensional space of Euclidean geometry as soon as a coordinate system has been chosen in the latter. For example, a value from R3 consists of three real numbers and specifies the coordinates of a point in 3‑dimensional space.In mathematics, real is used as an adjective, meaning that the underlying field is the field of the real numbers (or the real field). For example, real matrix, real polynomial and real Lie algebra. The word is also used as a noun, meaning a real number (as in "the set of all reals").The real numbers can be generalized and extended in several different directions:
Plane (geometry)
In mathematics, a plane is a flat, two-dimensional surface that extends infinitely far. A plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. Planes can arise as subspaces of some higher-dimensional space, as with a room's walls extended infinitely far, or they may enjoy an independent existence in their own right, as in the setting of Euclidean geometry.When working exclusively in two-dimensional Euclidean space, the definite article is used, so, the plane refers to the whole space. Many fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional space, or, in other words, in the plane.Euclid set forth the first great landmark of mathematical thought, an axiomatic treatment of geometry.[1] He selected a small core of undefined terms (called common notions) and postulates (or axioms) which he then used to prove various geometrical statements. Although the plane in its modern sense is not directly given a definition anywhere in the Elements, it may be thought of as part of the common notions.[2] Euclid never used numbers to measure length, angle, or area. In this way the Euclidean plane is not quite the same as the Cartesian plane.A plane is a ruled surface.This section is solely concerned with planes embedded in three dimensions: specifically, in R3.In a Euclidean space of any number of dimensions, a plane is uniquely determined by any of the following:The following statements hold in three-dimensional Euclidean space but not in higher dimensions, though they have higher-dimensional analogues:In a manner analogous to the way lines in a two-dimensional space are described using a point-slope form for their equations, planes in a three dimensional space have a natural description using a point in the plane and a vector orthogonal to it (the normal vector) to indicate its "inclination".Specifically, let r0 be the position vector of some point P0 = (x0, y0, z0), and let n = (a, b, c) be a nonzero vector. The plane determined by the point P0 and the vector n consists of those points P, with position vector r, such that the vector drawn from P0 to P is perpendicular to n. Recalling that two vectors are perpendicular if and only if their dot product is zero, it follows that the desired plane can be described as the set of all points r such that(The dot here means a dot (scalar) product.)Expanded this becomeswhich is the point-normal form of the equation of a plane.[3] This is just a linear equationwhereConversely, it is easily shown that if a, b, c and d are constants and a, b, and c are not all zero, then the graph of the equationis a plane having the vector n = (a, b, c) as a normal.[4] This familiar equation for a plane is called the general form of the equation of the plane.[5]Thus for example a regression equation of the form y = d + ax + cz (with b = −1) establishes a best-fit plane in three-dimensional space when there are two explanatory variables.Alternatively, a plane may be described parametrically as the set of all points of the formwhere s and t range over all real numbers, v and w are given linearly independent vectors defining the plane, and r0 is the vector representing the position of an arbitrary (but fixed) point on the plane. The vectors v and w can be visualized as vectors starting at r0 and pointing in different directions along the plane. Note that v and w can be perpendicular, but cannot be parallel.Let p1=(x1, y1, z1), p2=(x2, y2, z2), and p3=(x3, y3, z3) be non-collinear points.The plane passing through p1, p2, and p3 can be described as the set of all points (x,y,z) that satisfy the following determinant equations:This system can be solved using Cramer's rule and basic matrix manipulations. LetIf D is non-zero (so for planes not through the origin) the values for a, b and c can be calculated as follows:These equations are parametric in d. Setting d equal to any non-zero number and substituting it into these equations will yield one solution set.This plane can also be described by the "point and a normal vector" prescription above. A suitable normal vector is given by the cross productand the point r0 can be taken to be any of the given points p1,p2 or p3[6] (or any other point in the plane).Another vector form for the equation of a plane, known as the Hesse normal form relies on the parameter D. This form is:[5]whereIn addition to its familiar geometric structure, with isomorphisms that are isometries with respect to the usual inner product, the plane may be viewed at various other levels of abstraction. Each level of abstraction corresponds to a specific category.At one extreme, all geometrical and metric concepts may be dropped to leave the topological plane, which may be thought of as an idealized homotopically trivial infinite rubber sheet, which retains a notion of proximity, but has no distances. The topological plane has a concept of a linear path, but no concept of a straight line. The topological plane, or its equivalent the open disc, is the basic topological neighborhood used to construct surfaces (or 2-manifolds) classified in low-dimensional topology. Isomorphisms of the topological plane are all continuous bijections. The topological plane is the natural context for the branch of graph theory that deals with planar graphs, and results such as the four color theorem.The plane may also be viewed as an affine space, whose isomorphisms are combinations of translations and non-singular linear maps. From this viewpoint there are no distances, but collinearity and ratios of distances on any line are preserved.Differential geometry views a plane as a 2-dimensional real manifold, a topological plane which is provided with a differential structure. Again in this case, there is no notion of distance, but there is now a concept of smoothness of maps, for example a differentiable or smooth path (depending on the type of differential structure applied). The isomorphisms in this case are bijections with the chosen degree of differentiability.In the opposite direction of abstraction, we may apply a compatible field structure to the geometric plane, giving rise to the complex plane and the major area of complex analysis. The complex field has only two isomorphisms that leave the real line fixed, the identity and conjugation.In the same way as in the real case, the plane may also be viewed as the simplest, one-dimensional (over the complex numbers) complex manifold, sometimes called the complex line. However, this viewpoint contrasts sharply with the case of the plane as a 2-dimensional real manifold. The isomorphisms are all conformal bijections of the complex plane, but the only possibilities are maps that correspond to the composition of a multiplication by a complex number and a translation.In addition, the Euclidean geometry (which has zero curvature everywhere) is not the only geometry that the plane may have. The plane may be given a spherical geometry by using the stereographic projection. This can be thought of as placing a sphere on the plane (just like a ball on the floor), removing the top point, and projecting the sphere onto the plane from this point). This is one of the projections that may be used in making a flat map of part of the Earth's surface. The resulting geometry has constant positive curvature.Alternatively, the plane can also be given a metric which gives it constant negative curvature giving the hyperbolic plane. The latter possibility finds an application in the theory of special relativity in the simplified case where there are two spatial dimensions and one time dimension. (The hyperbolic plane is a timelike hypersurface in three-dimensional Minkowski space.)The one-point compactification of the plane is homeomorphic to a sphere (see stereographic projection); the open disk is homeomorphic to a sphere with the "north pole" missing; adding that point completes the (compact) sphere. The result of this compactification is a manifold referred to as the Riemann sphere or the complex projective line. The projection from the Euclidean plane to a sphere without a point is a diffeomorphism and even a conformal map.The plane itself is homeomorphic (and diffeomorphic) to an open disk. For the hyperbolic plane such diffeomorphism is conformal, but for the Euclidean plane it is not.
Linear form
In linear algebra, a linear functional or linear form (also called a one-form or covector) is a linear map from a vector space to its field of scalars. In ℝn, if vectors are represented as column vectors, then linear functionals are represented as row vectors, and their action on vectors is given by the dot product, or the matrix product with the row vector on the left and the column vector on the right.  In general, if V is a vector space over a field k, then a linear functional f is a function from V to k that is linear:The set of all linear functionals from V to k, Homk(V,k), forms a vector space over k with the addition of the operations of addition and scalar multiplication (defined pointwise).  This space is called the dual space of V, or sometimes the algebraic dual space, to distinguish it from the continuous dual space.  It is often written V∗, V′, or Vᐯ when the field k is understood.If V is a topological vector space, the space of continuous linear functionals —  the continuous dual — is often simply called the dual space.  If V is a Banach space, then so is its (continuous) dual.  To distinguish the ordinary dual space from the continuous dual space, the former is sometimes called the algebraic dual space.  In finite dimensions, every linear functional is continuous, so the continuous dual is the same as the algebraic dual, but in infinite dimensions the continuous dual is a proper subspace of the algebraic dual.Suppose that vectors in the real coordinate space Rn are represented as column vectorsFor each row vector [a1 … an] there is a linear functional f defined byand each linear functional can be expressed in this form.Linear functionals first appeared in functional analysis, the study of vector spaces of functions.  A typical example of a linear functional is integration: the linear transformation defined by the Riemann integralis a linear functional from the vector space C[a, b] of continuous functions on the interval [a, b] to the real numbers. The linearity of I follows from the standard facts about the integral:Let Pn denote the vector space of real-valued polynomial functions of degree ≤n defined on an interval [a, b].  If c ∈ [a, b], then let evc : Pn → R be the evaluation functionalThe mapping f → f(c) is linear sinceIf x0, ..., xn are n + 1 distinct points in [a, b], then the evaluation functionals evxi, i = 0, 1, ..., n form a basis of the dual space of Pn.  (Lax (1996) proves this last fact using Lagrange interpolation.)The integration functional I defined above defines a linear functional on the subspace Pn of polynomials of degree ≤ n. If x0, ..., xn are n + 1 distinct points in [a, b], then there are coefficients a0, ..., an for whichfor all f ∈ Pn. This forms the foundation of the theory of numerical quadrature.This follows from the fact that the linear functionals evxi : f → f(xi) defined above form a basis of the dual space of Pn.[1]Linear functionals are particularly important in quantum mechanics.  Quantum mechanical systems are represented by Hilbert spaces, which are anti–isomorphic to their own dual spaces.  A state of a quantum mechanical system can be  identified with a linear functional.  For more information see bra–ket notation.In the theory of generalized functions, certain kinds of generalized functions called distributions can be realized as linear functionals on spaces of test functions.In finite dimensions, a linear functional can be visualized in terms of its level sets.  In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel hyperplanes.  This method of visualizing linear functionals is sometimes introduced in general relativity texts, such as Gravitation by Misner, Thorne & Wheeler (1973).Every non-degenerate bilinear form on a finite-dimensional vector space V induces an isomorphism V → V∗ : v ↦ v∗ such that where the bilinear form on V is denoted ⟨ , ⟩ (for instance, in Euclidean space ⟨v, w⟩ = v ⋅ w is the dot product of v and w).The inverse isomorphism is V∗ → V : v∗ ↦ v, where v is the unique element of V such thatThe above defined vector v∗ ∈ V∗ is said to be the dual vector of v ∈ V.In an infinite dimensional Hilbert space, analogous results hold by the Riesz representation theorem.  There is a mapping V → V∗ into the continuous dual space V∗.  However, this mapping is antilinear rather than linear.Or, more succinctly,where δ is the Kronecker delta.  Here the superscripts of the basis functionals are not exponents but are instead contravariant indices.due to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals.  ThenSo each component of a linear functional can be extracted by applying the functional to the corresponding basis vector.In higher dimensions, this generalizes as follows
Homography
In projective geometry, a homography is an isomorphism of projective spaces, induced by an isomorphism of the vector spaces from which the projective spaces derive.[1] It is a bijection that maps lines to lines, and thus a collineation. In general, some collineations are not homographies, but the fundamental theorem of projective geometry asserts that is not so in the case of real projective spaces of dimension at least two. Synonyms include projectivity, projective transformation, and projective collineation.Historically, homographies (and projective spaces) have been introduced to study perspective and projections in Euclidean geometry, and the term homography, which, etymologically, roughly means "similar drawing" date from this time. At the end of the 19th century, formal definitions of projective spaces were introduced, which differed from extending Euclidean or affine spaces by adding points at infinity. The term "projective transformation" originated in these abstract constructions. These constructions divide into two classes that have been shown to be equivalent. A projective space may be constructed as the set of the lines of a vector space over a given field (the above definition is based on this version); this construction facilitates the definition of projective coordinates and allows using the tools of linear algebra for the study of homographies. The alternative approach consists in defining the projective space through a set of axioms, which do not involve explicitly any field (incidence geometry, see also synthetic geometry); in this context, collineations are easier to define than homographies, and homographies are defined as specific collineations, thus called "projective collineations".For sake of simplicity, unless otherwise stated, the projective spaces considered in this article are supposed to be defined over a (commutative) field. Equivalently Pappus's hexagon theorem and Desargues's theorem are supposed to be true. A large part of the results remain true, or may be generalized to projective geometries for which these theorems do not hold.Historically, the concept of homography had been introduced to understand, explain and study visual perspective, and, specifically, the difference in appearance of two plane objects viewed from different points of view.In the Euclidean space of dimension 3, a central projection from a point O (the center) onto a plane P that does not contain O is the mapping that sends a point A to the intersection (if it exists) of the line OA and the plane P. The projection is not defined if the point A belongs to the plane passing through O and parallel to P. The notion of projective space was originally introduced by extending the Euclidean space, that is, by adding points at infinity to it, in order to define the projection for every point except O.Given another plane Q, which does not contain O, the restriction to Q of the above projection is called a perspectivity.With these definitions, a perspectivity is only a partial function, but it becomes a bijection if extended to projective spaces. Therefore, this notion is normally defined for projective spaces. The notion is also easily generalized to projective spaces of any dimension, over any field, in the following way: Given two projective spaces P and Q of dimension n, a perspectivity is a bijection from P to Q that may be obtained by embedding P and Q in a projective space R of dimension n + 1 and restricting to P a central projection onto Q.If f is a perspectivity from P to Q, and g a perspectivity from Q to P, with a different center, then g ⋅ f is a homography from P to itself, which is called a central collineation, when the dimension of P is at least two. (see § Central collineation below and Perspectivity § Perspective collineations).Originally, a homography was defined as the composition of a finite number of perspectivities.[2] It is a part of the fundamental theorem of projective geometry (see below) that this definition coincides with the more algebraic definition sketched in the introduction and detailed below.When the projective spaces are defined by adding points at infinity to affine spaces (projective completion) the preceding formulas become, in affine coordinates,which generalizes the expression of the homographic function of the next section. This defines only a partial function between affine spaces, which is defined only outside the hyperplane where the denominator is zero.The projective line over a field K may be identified with the union of K and a point, called the "point at infinity" and denoted by ∞ (see projective line). With this representation of the projective line, the homographies are the mappingswhich are called homographic functions or linear fractional transformations.In the case of the complex projective line, which can be identified with the Riemann sphere, the homographies are called Möbius transformations.These correspond precisely with those bijections of the Riemann sphere that preserve orientation and are conformal.[3]In the study of collineations, the case of projective lines is special due to the small dimension. When the line is viewed as a projective space in isolation, any permutation of the points of a projective line is a collineation,[4] since every set of points are collinear. However, if the projective line is embedded in a higher-dimensional projective space, the geometric structure of that space can be used to impose a geometric structure on the line. Thus, in synthetic geometry, the homographies and the collineations of the projective line that are considered are those obtained by restrictions to the line of collineations and homographies of spaces of higher dimension. This means that the fundamental theorem of projective geometry (see below) remains valid in the one-dimensional setting. A homography of a projective line may also be properly defined by insisting that the mapping preserves cross-ratios.[5]A projective frame or projective basis of a projective space of dimension n is an ordered set of n + 2 points such no hyperplane contains n + 1 of them. A projective frame is sometimes called a simplex,[6] although a simplex in a space of dimension n has at most n + 1 vertices.It follows that, given two frames, there is exactly one homography mapping the first one onto the second one. In particular, the only homography fixing the points of a frame is the identity map. This result is much more difficult in synthetic geometry (where projective spaces are defined through axioms). It is sometimes called the first fundamental theorem of projective geometry.[7]One may also consider the projective space P(Kn+1). It has a canonical frame consisting of the image by p of the canonical basis of Kn+1 (consisting of the elements having only one nonzero entry, which is equal to 1), and (1, 1, ..., 1). On this basis, the homogeneous coordinates of p(v) are simply the entries (coefficients) of v. Given another projective space P(V) of the same dimension, and a frame F of it, there is one homography h mapping F onto the canonical frame of P(Kn+1). The projective coordinates of a point a on the frame F are the homogeneous coordinates of h(a) on the canonical frame of P(Kn+1).In above sections, homographies have been defined through linear algebra. In synthetic geometry, they are traditionally defined as the composition of one or several special homographies called central collineations. It is a part of the fundamental theorem of projective geometry that the two definitions are equivalent.In a projective space, P, of dimension n ≥ 2, a collineation of P is a bijection from P onto P that maps lines onto lines. A central collineation (traditionally these were called perspectivities,[8] but this term may be confusing, having another meaning; see Perspectivity) is a bijection α from P to P, such that there exists a hyperplane H (called the axis of α), which is fixed pointwise by α (that is, α(X) = X for all points X in H) and a point O (called the center of α), which is fixed linewise by α (any line through O is mapped to itself by α, but not necessarily pointwise).[9] There are two types of central collineations. Elations are the central collineations in which the center is incident with the axis and homologies are those in which the center is not incident with the axis. A central collineation is uniquely defined by its center, its axis, and the image α(P) of any given point P that differs from the center O and does not belong to the axis. (The image α(Q) of any other point Q is the intersection of the line defined by O and Q and the line passing through α(P) and the intersection with the axis of the line defined by P and Q.)A central collineation is a homography defined by a (n+1) × (n+1) matrix that has an eigenspace of dimension n. It is a homology, if the matrix has another eigenvalue and is therefore diagonalizable. It is an elation, if all the eigenvalues are equal and the matrix is not diagonalizable.The composition of two central collineations, while still a homography in general, is not a central collineation. In fact, every homography is the composition of a finite number of central collineations. In synthetic geometry, this property, which is a part of the fundamental theory of projective geometry is taken as the definition of homographies.[10]There are collineations besides the homographies. In particular, any field automorphism σ of a field F induces a collineation of every projective space over F by applying σ to all homogeneous coordinates (over a projective frame) of a point. These collineations are called automorphic collineations.The fundamental theorem of projective geometry consists of the three following theorems.If projective spaces are defined by means of axioms (synthetic geometry), the third part is simply a definition. On the other hand, if projective spaces are defined by means of linear algebra, the first part is an easy corollary of the definitions. Therefore, the proof of the first part in synthetic geometry, and the proof of the third part in terms of linear algebra both are fundamental steps of the proof of the equivalence of the two ways of defining projective spaces.As every homography has an inverse mapping and the composition of two homographies is another, the homographies of a given projective space form a group. For example, the Möbius group is the homography group of any complex projective line.As all the projective spaces of the same dimension over the same field are isomorphic, the same is true for their homography groups. They are therefore considered as a single group acting on several spaces, and only the dimension and the field appear in the notation, not the specific projective space.Homography groups also called projective linear groups are denoted PGL(n + 1, F) when acting on a projective space of dimension n over a field F. Above definition of homographies shows that PGL(n + 1, F) may be identified to the quotient group GL(n + 1, F) / F×I, where GL(n + 1, F) is the general linear group of the invertible matrices, and F×I is the group of the products by a nonzero element of F of the identity matrix of size (n + 1) × (n + 1).When F is a Galois field GF(q) then the homography group is written PGL(n, q). For example, PGL(2, 7) acts on the eight points in the projective line over the finite field GF(7), while PGL(2, 4), which is isomorphic to the alternating group A5, is the homography group of the projective line with five points.[12]The homography group PGL(n + 1, F) is a subgroup of the collineation group PΓL(n + 1, F) of the collineations of a projective space of dimension n. When the points and lines of the projective space are viewed as a block design, whose blocks are the sets of points contained in a line, it is common to call the collineation group the automorphism group of the design.The cross-ratio of four collinear points is an invariant under the homography that is fundamental for the study of the homographies of the lines.Three distinct points a, b and c on a projective line over a field F form a projective frame of this line. There is therefore a unique homography h of this line onto F ∪ ∞ that maps a to ∞, b to 0, and c to 1. Given a fourth point on the same line, the cross-ratio of the four points a, b, c and d, denoted [a, b; c, d], is the element h(d) of F ∪ ∞. In other words, if d has homogeneous coordinates [k : 1] over the projective frame (a, b, c), then [a, b; c, d] = k.[13]Suppose A is a ring and U is its group of units. Homographies act on a projective line over A, written P(A), consisting of points U(a, b) with homogeneous coordinates. The homographies on P(A) are described by matrix mappingsWhen A is a commutative ring, the homography may be writtenbut otherwise the linear fractional transformation is seen as an equivalence:Ring homographies have been used in quaternion analysis, and with dual quaternions to facilitate screw theory. When A is taken to be biquaternions the homographies exhibit conformal symmetry of an electromagnetic field. The homography group of the ring of integers Z is modular group PSL(2, Z).
Canonical basis
In mathematics, a canonical basis is a basis of an algebraic structure that is canonical in a sense that depends on the precise context:In representation theory there are several bases that are called "canonical", for example, Lusztig's canonical basis and closely related Kashiwara's crystal basis in quantum groups and their representations. There is a general concept underlying these basis:Definition:  A set of n linearly independent generalized eigenvectors is a canonical basis if it is composed entirely of Jordan chains.Now defineOnce we have determined the number of generalized eigenvectors of each rank that a canonical basis has, we can obtain the vectors explicitly (see generalized eigenvector).[3]This example illustrates a canonical basis with two Jordan chains.  Unfortunately, it is a little difficult to construct an interesting example of low order.[4]The matrix
Matrix calculus
In mathematics, matrix calculus is a specialized notation for doing multivariable calculus, especially over spaces of matrices.  It collects the various partial derivatives of a single function with respect to many variables, and/or of a multivariate function with respect to a single variable, into vectors and matrices that can be treated as single entities.  This greatly simplifies operations such as finding the maximum or minimum of a multivariate function and solving systems of differential equations. The notation used here is commonly used in statistics and engineering, while the tensor index notation is preferred in physics.Two competing notational conventions split the field of matrix calculus into two separate groups. The two groups can be distinguished by whether they write the derivative of a scalar with respect to a vector as a column vector or a row vector. Both of these conventions are possible even when the common assumption is made that vectors should be treated as column vectors when combined with matrices (rather than row vectors). A single convention can be somewhat standard throughout a single field that commonly uses matrix calculus (e.g. econometrics, statistics, estimation theory and machine learning). However, even within a given field different authors can be found using competing conventions. Authors of both groups often write as though their specific convention were standard. Serious mistakes can result when combining results from different authors without carefully verifying that compatible notations have been used. Definitions of these two conventions and comparisons between them are collected in the layout conventions section.Matrix calculus refers to a number of different notations that use matrices and vectors to collect the derivative of each component of the dependent variable with respect to each component of the independent variable. In general, the independent variable can be a scalar, a vector, or a matrix while the dependent variable can be any of these as well. Each different situation will lead to a different set of rules, or a separate calculus, using the broader sense of the term. Matrix notation serves as a convenient way to collect the many derivatives in an organized way.More complicated examples include the derivative of a scalar function with respect to a matrix, known as the gradient matrix, which collects the derivative with respect to each matrix element in the corresponding position in the resulting matrix. In that case the scalar must be a function of each of the independent variables in the matrix. As another example, if we have an n-vector of dependent variables, or functions, of m independent variables we might consider the derivative of the dependent vector with respect to the independent vector. The result could be collected in an m×n matrix consisting of all of the possible derivative combinations. There are, of course, a total of nine possibilities using scalars, vectors, and matrices. Notice that as we consider higher numbers of components in each of the independent and dependent variables we can be left with a very large number of possibilities.The six kinds of derivatives that can be most neatly organized in matrix form are collected in the following table.[1]Here, we have used the term "matrix" in its most general sense, recognizing that vectors and scalars are simply matrices with one column and then one row respectively. Moreover, we have used bold letters to indicate vectors and bold capital letters for matrices. This notation is used throughout.Notice that we could also talk about the derivative of a vector with respect to a matrix, or any of the other unfilled cells in our table. However, these derivatives are most naturally organized in a tensor of rank higher than 2, so that they do not fit neatly into a matrix. In the following three sections we will define each one of these derivatives and relate them to other branches of mathematics. See the layout conventions section for a more detailed table.The matrix derivative is a convenient notation for keeping track of partial derivatives for doing calculations. The Fréchet derivative is the standard way in the setting of functional analysis to take derivatives with respect to vectors. In the case that a matrix function of a matrix is Fréchet differentiable, the two derivatives will agree up to translation of notations. As is the case in general for partial derivatives, some formulae may extend under weaker analytic conditions than the existence of the derivative as approximating linear mapping.Matrix calculus is used for deriving optimal stochastic estimators, often involving the use of Lagrange multipliers. This includes the derivation of:The vector and matrix derivatives presented in the sections to follow take full advantage of matrix notation, using a single variable to represent a large number of variables. In what follows we will distinguish scalars, vectors and matrices by their typeface. We will let M(n,m) denote the space of real n×m matrices with n rows and m columns.  Such matrices will be denoted using bold capital letters: A, X, Y, etc.  An element of M(n,1), that is, a column vector, is denoted with a boldface lowercase letter: a, x, y, etc. An element of M(1,1) is a scalar, denoted with lowercase italic typeface: a, t, x, etc. XT denotes matrix transpose, tr(X) is the trace, and det(X) or |X| is the determinant. All functions are assumed to be of differentiability class C1 unless otherwise noted. Generally letters from the first half of the alphabet (a, b, c, …) will be used to denote constants, and from the second half (t, x, y, …) to denote variables.NOTE: As mentioned above, there are competing notations for laying out systems of partial derivatives in vectors and matrices, and no standard appears to be emerging yet.  The next two introductory sections use the numerator layout convention simply for the purposes of convenience, to avoid overly complicating the discussion.  The section after them discusses layout conventions in more detail.  It is important to realize the following:The tensor index notation with its Einstein summation convention is very similar to the matrix calculus, except one writes only a single component at a time.  It has the advantage that one can easily manipulate arbitrarily high rank tensors, whereas tensors of rank higher than two are quite unwieldy with matrix notation.  All of the work here can be done in this notation without use of the single-variable matrix notation. However, many problems in estimation theory and other areas of applied mathematics would result in too many indices to properly keep track of, pointing in favor of matrix calculus in those areas.  Also, Einstein notation can be very useful in proving the identities presented here (see section on differentiation) as an alternative to typical element notation, which can become cumbersome when the explicit sums are carried around. Note that a matrix can be considered a tensor of rank two.Because vectors are matrices with only one column, the simplest matrix derivatives are vector derivatives.The notations developed here can accommodate the usual operations of vector calculus by identifying the space M(n,1) of n-vectors with the Euclidean space Rn, and the scalar M(1,1) is identified with R. The corresponding concept from vector calculus is indicated at the end of each subsection.NOTE: The discussion in this section assumes the numerator layout convention for pedagogical purposes.  Some authors use different conventions.  The section on layout conventions discusses this issue in greater detail.  The identities given further down are presented in forms that can be used in conjunction with all common layout conventions.Example Simple examples of this include the velocity vector in Euclidean space, which is the tangent vector of the position vector (considered as a function of time). Also, the acceleration is the tangent vector of the velocity.In vector calculus, the gradient of a scalar field y in the space Rn (whose independent coordinates are the components of x) is the transpose of the derivative of a scalar by a vector. In physics, the electric field is the vector gradient of the electric potential.The directional derivative of a scalar function f(x) of the space vector x in the direction of the unit vector u is defined using the gradient as follows.Each of the previous two cases can be considered as an application of the derivative of a vector with respect to a vector, using a vector of size one appropriately. Similarly we will find that the derivatives involving matrices will reduce to derivatives involving vectors in a corresponding way.In vector calculus, the derivative of a vector function y with respect to a vector x whose components represent a space is known as the pushforward (or differential), or the Jacobian matrix.There are two types of derivatives with matrices that can be organized into a matrix of the same size. These are the derivative of a matrix by a scalar and the derivative of a scalar by a matrix. These can be useful in minimization problems found in many areas of applied mathematics and have adopted the names tangent matrix and gradient matrix respectively after their analogs for vectors.NOTE: The discussion in this section assumes the numerator layout convention for pedagogical purposes.  Some authors use different conventions.  The section on layout conventions discusses this issue in greater detail.  The identities given further down are presented in forms that can be used in conjunction with all common layout conventions.The derivative of a matrix function Y by a scalar x is known as the tangent matrix and is given (in numerator layout notation) byThe derivative of a scalar y function of a p×q matrix X of independent variables, with respect to the matrix X, is given (in numerator layout notation) byImportant examples of scalar functions of matrices include the trace of a matrix and the determinant.In analog with vector calculus this derivative is often written as the following.Also in analog with vector calculus, the directional derivative of a scalar f(X) of a matrix X in the direction of matrix Y is given byIt is the gradient matrix, in particular, that finds many uses in minimization problems in estimation theory, particularly in the derivation of the Kalman filter algorithm, which is of great importance in the field.The three types of derivatives that have not been considered are those involving vectors-by-matrices, matrices-by-vectors, and matrices-by-matrices. These are not as widely considered and a notation is not widely agreed upon. As for vectors, the other two types of higher matrix derivatives can be seen as applications of the derivative of a matrix by a matrix by using a matrix with one column in the correct place. For this reason, in this subsection we consider only how one can write the derivative of a matrix by another matrix.Note that this definition encompasses all of the preceding definitions as special cases.According to Jan R. Magnus and Heinz Neudecker, the following notations are both unsuitable, as the determinant of the second resulting matrix would have "no interpretation" and "a useful chain rule does not exist" if these notations are being used:[2]The Jacobian matrix, according to Magnus and Neudecker,[2] isThis section discusses the similarities and differences between notational conventions that are used in the various fields that take advantage of matrix calculus. Although there are largely two consistent conventions, some authors find it convenient to mix the two conventions in forms that are discussed below. After this section equations will be listed in both competing forms separately.Keep in mind that various authors use different combinations of numerator and denominator layouts for different types of derivatives, and there is no guarantee that an author will consistently use either numerator or denominator layout for all types.  Match up the formulas below with those quoted in the source to determine the layout used for that particular type of derivative, but be careful not to assume that derivatives of other types necessarily follow the same kind of layout.When taking derivatives with an aggregate (vector or matrix) denominator in order to find a maximum or minimum of the aggregate, it should be kept in mind that using numerator layout will produce results that are transposed with respect to the aggregate.  For example, in attempting to find the maximum likelihood estimate of a multivariate normal distribution using matrix calculus, if the domain is a kx1 column vector, then the result using the numerator layout will be in the form of a 1xk row vector. Thus, either the results should be transposed at the end or the denominator layout (or mixed layout) should be used.(denominator layout) size-m row vector (denominator layout) size-n column vector (denominator layout) n×m matrix (denominator layout) p×q matrix The results of operations will be transposed when switching between numerator-layout and denominator-layout notation.Using numerator-layout notation, we have:[1]The following definitions are only provided in numerator-layout notation:Using denominator-layout notation, we have:[3]As noted above, in general, the results of operations will be transposed when switching between numerator-layout and denominator-layout notation.To help make sense of all the identities below, keep in mind the most important rules: the chain rule, product rule and sum rule.  The sum rule applies universally, and the product rule applies in most of the cases below, provided that the order of matrix products is maintained, since matrix products are not commutative.  The chain rule applies in some of the cases, but unfortunately does not apply in matrix-by-scalar derivatives or scalar-by-matrix derivatives (in the latter case, mostly involving the trace operator applied to matrices).  In the latter case, the product rule can't quite be applied directly, either, but the equivalent can be done with a bit more work using the differential identities.This is presented first because all of the operations that apply to vector-by-vector differentiation apply directly to vector-by-scalar or scalar-by-vector differentiation simply by reducing the appropriate vector in the numerator or denominator to a scalar.The fundamental identities are placed above the thick black line.Note that exact equivalents of the scalar product rule and chain rule do not exist when applied to matrix-valued functions of matrices.  However, the product rule of this sort does apply to the differential form (see below), and this is the way to derive many of the identities below involving the trace function, combined with the fact that the trace function allows transposing and cyclic permutation, i.e.:Therefore,(For the last step, see the 'Conversion from differential to derivative form' section.)i.e. mixed layout if denominator layout for X is being used.Further see Derivative of the exponential map.It is often easier to work in differential form and then convert back to normal derivatives.  This only works well using the numerator layout. In these rules, "a" is a scalar.To convert to normal derivative form, first convert it to one of the following canonical forms, and then use these identities:Matrix differential calculus is used in statistics, particularly for the statistical analysis of multivariate distributions, especially the multivariate normal distribution and other elliptical distributions.[10][11][12]It is used in regression analysis to compute, for example, the ordinary least squares regression formula for the case of multiple explanatory variables.
Orthant
In geometry, an orthant[1] or hyperoctant[2] is the analogue in n-dimensional Euclidean space of a quadrant in the plane or an octant in three dimensions.In general an orthant in n-dimensions can be considered the intersection of n mutually orthogonal half-spaces. By independent selections of half-space signs, there are 2n orthants in n-dimensional space.More specifically, a closed orthant in Rn is a subset defined by constraining each Cartesian coordinate to be nonnegative or nonpositive.  Such a subset is defined by a system of inequalities:where each εi is +1 or −1.Similarly, an open orthant in Rn is a subset defined by a system of strict inequalitieswhere each εi is +1 or −1.By dimension:John Conway defined the term n-orthoplex from orthant complex as a regular polytope in n-dimensions with 2n simplex facets, one per orthant.[3]The nonnegative orthant is the generalization of the first quadrant to n dimensions and is important in many constrained optimization problems.[[Mohammad Amin Ahmadi]
Skew-Hermitian matrix
In linear algebra, a square matrix with complex entries is said to be skew-Hermitian or antihermitian if its conjugate transpose is the negative of the original matrix.[1] That is, the matrix A is skew-Hermitian if it satisfies the relationfor all i and j, where ai,j is the i,j-th entry of A, and the overline denotes complex conjugation.Skew-Hermitian matrices can be understood as the complex versions of real skew-symmetric matrices, or as the matrix analogue of the purely imaginary numbers.[2]  The set of all skew-Hermitian n×n matrices forms the u(n) Lie algebra, which corresponds to the Lie group U(n).The concept can be generalized to include linear transformations of any complex vector space with a sesquilinear norm.Imaginary numbers can be thought of as skew-adjoint (since they are like 1-by-1 matrices), whereas real numbers correspond to self-adjoint operators.For example, the following matrix is skew-Hermitian:
Transpose of a linear map
In linear algebra, the transpose of a linear map between two vector spaces, defined over the same field, is an induced map between the dual spaces of the two vector spaces. The transpose of a linear map is often used to study the original linear map. This concept is generalised by adjoint functors.Let V and W be vector spaces over the same field. If f : V → W is a linear map, then the transpose[1] (or dual, or adjoint[2]), is defined to beThe resulting functional tf(φ) is called the pullback of φ along f.The following identity, which characterises the transpose,[3] holds for all φ ∈ W∗ and v ∈ V:where the bracket [·,·]V is the natural pairing of V's dual space with V, and [·,·]W is the same with W.The assignment f ↦ tf produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W∗ to V∗. If V = W then the space of linear maps is an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that t(fg) = tg tf. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify t(tf) with f using the natural injection into the double dual.If the linear map f is represented by the matrix A with respect to two bases of V and W, then tf is represented by the transpose matrix AT with respect to the dual bases of W∗ and V∗, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, tf is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.The identity that characterizes the transpose, that is, [f∗(φ), v] = [φ, f(v)], is formally similar to the definition of the Hermitian adjoint, however, the transpose and the Hermitian adjoint are not the same map. The difference stems from the fact that transpose is defined by a bilinear form while the Hermitian adjoint is defined by a sesquilinear form. Furthermore, while the transpose can be defined on any vector space, the Hermitian adjoint is defined on Hilbert spaces.If X and Y are Hilbert spaces and u : X → Y is a linear map then the transpose of u and the Hermitian adjoint of u, which we will denote respectively by tu and u∗, are related. Denote by I : X → X∗ and J : Y → Y∗ the canonical antilinear isometries of the Hilbert spaces X and Y onto their duals. Then u∗ is the following composition of maps:[5]Suppose that X and Y are topological vector spaces and that u : X → Y is a linear map, then many of u's properties are reflected in u∗.
System of linear equations
In mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables.[1]  For example,is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given bysince it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.The simplest kind of linear system involves two equations and two variables:Now substitute this expression for x into the bottom equation:A general system of m linear equations with n unknowns can be written asOften the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.The vector equation is equivalent to a matrix equation of the formwhere A is an m×n matrix, x is a column vector with n entries, and b is a column vector with m entries.The number of vectors in a basis for the span is now expressed as the rank of the matrix.A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.A linear system may behave in any one of three possible ways:For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.[2]For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, and is a flat, which may have any dimension lower than n.In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, "in general" means that a different behavior may occur for specific values of the coefficients of the equations.In the first case, the dimension of the solution set is, in general, equal to n − m, where n is the number of variables and m is the number of equations.The following pictures illustrate this trichotomy in the case of two variables:The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.It must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns.The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.For example, the equationsare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.For a more complicated example, the equationsare not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten as the statement 0 = 1.For example, the equationsare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equationsare inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.Putting it another way, according to the Rouché–Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.There are several algorithms for solving a system of linear equations.To describe a set with an infinite number of solutions, typically some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.For example, consider the following system:The solution set to this system can be described by the following equations:Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:Here x is the free variable, and y and z are dependent.The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:For example, consider the following system:Solving the first equation for x gives x = 5 + 2z − 3y, and plugging this into the second and third equation yieldsSolving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = −15. Therefore, the solution set is the single point (x, y, z) = (−15, 8, 2).In row reduction (also known as Gaussian elimination), the linear system is represented as an augmented matrix:This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:The last matrix is in reduced row echelon form, and represents the system x = −15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the systemis given byFor each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.)Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.[citation needed]While systems of three or four equations can be readily solved by hand (see Cracovian), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.There is also a quantum algorithm for linear systems of equations.[3]A system of linear equations is homogeneous if all of the constant terms are zero:A homogeneous system is equivalent to a matrix equation of the formwhere A is an m × n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) ≠ 0) then it is also the only solution.  If the system has a singular matrix then there is a solution set with an infinite number of solutions.  This solution set has the following additional properties:These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A.Numerical solutions to a homogeneous system can be found with a singular value decomposition.There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described asGeometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.
Spectral theory
In mathematics, spectral theory is an inclusive term for theories extending the eigenvector and eigenvalue theory of a single square matrix to a much broader theory of the structure of operators in a variety of mathematical spaces.[1] It is a result of studies of linear algebra and the solutions of systems of linear equations and their generalizations.[2] The theory is connected to that of analytic functions because the spectral properties of an operator are related to analytic functions of the spectral parameter.[3]The name spectral theory was introduced by David Hilbert in his original formulation of Hilbert space theory, which was cast in terms of quadratic forms in infinitely many variables. The original spectral theorem was therefore conceived as a version of the theorem on principal axes of an ellipsoid, in an infinite-dimensional setting. The later discovery in quantum mechanics that spectral theory could explain features of atomic spectra was therefore fortuitous. Hilbert himself was surprised by the unexpected application of this theory, noting that "I developed my theory of infinitely many variables from purely mathematical interests, and even called it 'spectral analysis' without any presentiment that it would later find application to the actual spectrum of physics."[4]There have been three main ways to formulate spectral theory, all of which retain their usefulness.[clarification needed] After Hilbert's initial formulation, the later development of abstract Hilbert space and the spectral theory of a single normal operator on it did very much go in parallel with the requirements of physics; particularly in the hands of von Neumann.[5] The further theory built on this to include Banach algebras, which can be given abstractly. This development leads to the Gelfand representation, which covers the commutative case, and further into non-commutative harmonic analysis.The difference can be seen in making the connection with Fourier analysis. The Fourier transform on the real line is in one sense the spectral theory of differentiation qua differential operator. But for that to cover the phenomena one has already to deal with generalized eigenfunctions (for example, by means of a rigged Hilbert space). On the other hand it is simple to construct a group algebra, the spectrum of which captures the Fourier transform's basic properties, and this is carried out by means of Pontryagin duality.One can also study the spectral properties of operators on Banach spaces. For example, compact operators on Banach spaces have many spectral properties similar to that of matrices.The background in the physics of vibrations has been explained in this way:[6]The mathematical theory is not dependent on such physical ideas on a technical level, but there are examples of mutual influence (see for example Mark Kac's question Can you hear the shape of a drum?). Hilbert's adoption of the term "spectrum" has been attributed to an 1897 paper of Wilhelm Wirtinger on Hill differential equation (by Jean Dieudonné), and it was taken up by his students during the first decade of the twentieth century, among them Erhard Schmidt and Hermann Weyl. The conceptual basis for Hilbert space was developed from Hilbert's ideas by Erhard Schmidt and Frigyes Riesz.[7][8]  It was almost twenty years later, when quantum mechanics was formulated in terms of the Schrödinger equation, that the connection was made to atomic spectra; a connection with the mathematical physics of vibration had been suspected before, as remarked by Henri Poincaré, but rejected for simple quantitative reasons, absent an explanation of the Balmer series.[9] The later discovery in quantum mechanics that spectral theory could explain features of atomic spectra was therefore fortuitous, rather than being an object of Hilbert's spectral theory.Consider a bounded linear transformation T defined everywhere over a general Banach space. We form the transformation:Here I is the identity operator and ζ is a complex number. The inverse of an operator T, that is T−1, is defined by:If the inverse exists, T is called regular. If it does not exist, T is called singular.With these definitions, the resolvent set of T is the set of all complex numbers ζ such that Rζ exists and is bounded. This set often is denoted as ρ(T). The spectrum of T is the set of all complex numbers ζ such that Rζ fails to exist or is unbounded. Often the spectrum of T is denoted by σ(T). The function Rζ for all ζ in ρ(T) (that is, wherever Rζ exists as a bounded operator) is called the resolvent of T. The spectrum of T is therefore the complement of the resolvent set of T in the complex plane.[10] Every eigenvalue of T belongs to σ(T), but σ(T) may contain non-eigenvalues.[11]This definition applies to a Banach space, but of course other types of space exist as well, for example, topological vector spaces include Banach spaces, but can be more general.[12][13] On the other hand, Banach spaces include Hilbert spaces, and it is these spaces that find the greatest application and the richest theoretical results.[14] With suitable restrictions, much can be said about the structure of the spectra of transformations in a Hilbert space. In particular, for self-adjoint operators, the spectrum lies on the real line and (in general) is a spectral combination of a point spectrum of discrete eigenvalues and a continuous spectrum.[15]In functional analysis and linear algebra the spectral theorem establishes conditions under which an operator can be expressed in simple form as a sum of simpler operators. As a full rigorous presentation is not appropriate for this article, we take an approach that avoids much of the rigor and satisfaction of a formal treatment with the aim of being more comprehensible to a non-specialist.This topic is easiest to describe by introducing the bra–ket notation of Dirac for operators.[16][17] As an example, a very particular linear operator L might be written as a dyadic product:[18][19]and the magnitude of f bywhere the notation '*' denotes a complex conjugate. This inner product choice defines a very specific inner product space, restricting the generality of the arguments that follow.[14]The effect of L upon a function f is then described as:A more general linear operator L might be expressed as:Some natural questions are: under what circumstances does this formalism work, and for what operators L are expansions in series of other operators like this possible? Can any function f be expressed in terms of the eigenfunctions (are they a Schauder basis) and under what circumstances does a point spectrum or a continuous spectrum arise? How do the formalisms for infinite-dimensional spaces and finite-dimensional spaces differ, or do they differ? Can these ideas be extended to a broader class of spaces? Answering such questions is the realm of spectral theory and requires considerable background in functional analysis and matrix algebra.This section continues in the rough and ready manner of the above section using the bra–ket notation, and glossing over the many important details of a rigorous treatment.[21] A rigorous mathematical treatment may be found in various references.[22] In particular, the dimension n of the space will be finite.Using the bra–ket notation of the above section, the identity operator may be written as:This expression of the identity operation is called a representation or a resolution of the identity.[21],[22] This formal representation satisfies the basic property of the identity:valid for every positive integer k.Given some operator equation of the form:with h in the space, this equation can be solved in the above basis through the formal manipulations:The role of spectral theory arises in establishing the nature and existence of the basis and the reciprocal basis. In particular, the basis might consist of the eigenfunctions of some linear operator L:with the { λi } the eigenvalues of L from the spectrum of L. Then the resolution of the identity above provides the dyad expansion of L:Using spectral theory, the resolvent operator R:can be evaluated  in terms of the eigenfunctions and eigenvalues of L, and the Green's function corresponding to L can be found.This function has poles in the complex λ-plane at each eigenvalue of L. Thus, using the calculus of residues:where the line integral is over a contour C that includes all the eigenvalues of L.Suppose our functions are defined over some coordinates {xj}, that is:Introducing the notationwhere δ(x − y) = δ(x1 − y1, x2 − y2, x3 − y3,  ...) is the Dirac delta function,[24]we can writeThen:The function G(x, y; λ) defined by:is called the Green's function for operator L, and satisfies:[25]Consider the operator equation:in terms of coordinates:A particular case is λ = 0.The Green's function of the previous section is:and satisfies:Using this Green's function property:Then, multiplying both sides of this equation by h(z) and integrating:which suggests the solution is:That is, the function ψ(x) satisfying the operator equation is found if we can  find the spectrum of O, and construct G, for example by using:There are many other ways to find G, of course.[26] See the articles on Green's functions and on Fredholm integral equations. It must be kept in mind that the above mathematics is purely formal, and a rigorous treatment involves some pretty sophisticated mathematics, including a good background knowledge of functional analysis, Hilbert spaces, distributions and so forth. Consult these articles and the references for more detail.Optimization problems may be the most useful examples about the combinatorial significance of the eigenvalues and eigenvectors in symmetric matrices, especially for the Rayleigh quotient with respect to a matrix M.Theorem Let M be a symmetric matrix and let x be the non-zero vector that maximizes the Rayleigh quotient with respect to M. Then, x is an eigenvector of M with eigenvalue equal to the Rayleigh quotient. Moreover, this eigenvalue is the largest eigenvalue of M. The way to prove this formula is pretty easy. Namely,evaluate the Rayleigh quotient with respect to x:where we used Parseval's identity in the last line.Finally we obtain that [27]
Cartesian tensor
In geometry and linear algebra, a Cartesian tensor uses an orthonormal basis to represent a tensor in a Euclidean space in the form of components. Converting a tensor's components from one such basis to another is through an orthogonal transformation.The most familiar coordinate systems are the two-dimensional and three-dimensional Cartesian coordinate systems. Cartesian tensors may be used with any Euclidean space, or more technically, any finite-dimensional vector space over the field of real numbers that has an inner product.Use of Cartesian tensors occurs in physics and engineering, such as with the Cauchy stress tensor and the moment of inertia tensor in rigid body dynamics. Sometimes general curvilinear coordinates are convenient, as in high-deformation continuum mechanics, or even necessary, as in general relativity. While orthonormal bases may be found for some such coordinate systems (e.g. tangent to spherical coordinates), Cartesian tensors may provide considerable simplification for applications in which rotations of rectilinear coordinate axes suffice. The transformation is a passive transformation, since the coordinates are changed and not the physical system.In 3d Euclidean space, ℝ3, the standard basis is ex, ey, ez. Each basis vector points along the x-, y-, and z-axes, and the vectors are all unit vectors (or normalized), so the basis is orthonormal.Throughout, when referring to Cartesian coordinates in three dimensions, a right-handed system is assumed and this is much more common than a left-handed system in practice, see orientation (vector space) for details.simiFor Cartesian tensors of order 1, a Cartesian vector a can be written algebraically as a linear combination of the basis vectors ex, ey, ez:where the coordinates of the vector with respect to the Cartesian basis are denoted ax, ay, az. It is common and helpful to display the basis vectors as column vectorswhen we have a coordinate vector in a column vector representation:A row vector representation is also legitimate, although in the context of general curvilinear coordinate systems the row and column vector representations are used separately for specific reasons – see Einstein notation and covariance and contravariance of vectors for why.The term "component" of a vector is ambiguous: it could refer to:A more general notation is tensor index notation, which has the flexibility of numerical values rather than fixed coordinate labels. The Cartesian labels are replaced by tensor indices in the basis vectors ex ↦ e1, ey ↦ e2, ez ↦ e3 and coordinates Ax ↦ A1, Ay ↦ A2, Az ↦ A3. In general, the notation e1, e2, e3 refers to any basis, and A1, A2, A3 refers to the corresponding coordinate system; although here they are restricted to the Cartesian system. Then:It is standard to use the Einstein notation—the summation sign for summation over an index that is present exactly twice within a term may be suppressed for notational conciseness:An advantage of the index notation over coordinate-specific notations is the independence of the dimension of the underlying vector space, i.e. the same expression on the right hand side takes the same form in higher dimensions (see below). Previously, the Cartesian labels x, y, z were just labels and not indices. (It is informal to say "i = x, y, z").A dyadic tensor T is an order 2 tensor formed by the tensor product ⊗ of two Cartesian vectors a and b, written T = a ⊗ b. Analogous to vectors, it can be written as a linear combination of the tensor basis ex ⊗ ex ≡ exx, ex ⊗ ey ≡ exy, ..., ez ⊗ ez ≡ ezz (the right hand side of each identity is only an abbreviation, nothing more):Representing each basis tensor as a matrix:then T can be represented more systematically as a matrix:See matrix multiplication for the notational correspondence between matrices and the dot and tensor products.More generally, whether or not T is a tensor product of two vectors, it is always a linear combination of the basis tensors with coordinates Txx, Txy, ... Tzz:while in terms of tensor indices:and in matrix form:Second order tensors occur naturally in physics and engineering when physical quantities have directional dependence in the system, often in a "stimulus-response" way. This can be mathematically seen through one aspect of tensors - they are multilinear functions. A second order tensor T which takes in a vector u of some magnitude and direction will return a vector v; of a different magnitude and in a different direction to u, in general. The notation used for functions in mathematical analysis leads us to write v = T(u),[1] while the same idea can be expressed in matrix and index notations[2] (including the summation convention), respectively:By "linear", if u = ρr + σs for two scalars ρ and σ and vectors r and s, then in function and index notations:and similarly for the matrix notation. The function, matrix, and index notations all mean the same thing. The matrix forms provide a clear display of the components, while the index form allows easier tensor-algebraic manipulation of the formulae in a compact manner. Both provide the physical interpretation of directions; vectors have one direction, while second order tensors connect two directions together. One can associate a tensor index or coordinate label with a basis vector direction.The use of second order tensors are the minimum to describe changes in magnitudes and directions of vectors, as the dot product of two vectors is always a scalar, while the cross product of two vectors is always a pseudovector perpendicular to the plane defined by the vectors, so these products of vectors alone cannot obtain a new vector of any magnitude in any direction. (See also below for more on the dot and cross products). The tensor product of two vectors is a second order tensor, although this has no obvious directional interpretation by itself.The previous idea can be continued: if T takes in two vectors p and q, it will return a scalar r. In function notation we write r = T(p, q), while in matrix and index notations (including the summation convention) respectively:The tensor T is linear in both input vectors. When vectors and tensors are written without reference to components, and indices are not used, sometimes a dot · is placed where summations over indices (known as tensor contractions) are taken. For the above cases:[1][2]motivated by the dot product notation:More generally, a tensor of order m which takes in n vectors (where n is between 0 and m inclusive) will return a tensor of order m − n, see Tensor: As multilinear maps for further generalizations and details. The concepts above also apply to pseudovectors in the same way as for vectors. The vectors and tensors themselves can vary within throughout space, in which case we have vector fields and tensor fields, and can also depend on time.Following are some examples:For the electrical conduction example, the index and matrix notations would be:while for the rotational kinetic energy T:See also constitutive equation for more specialized examples.In n-dimensional Euclidean space over the real numbers, ℝn, the standard basis is denoted e1, e2, e3, ... en. Each basis vector ei points along the positive xi axis, with the basis being orthonormal. Component j of ei is given by the Kronecker delta:A vector in ℝn takes the form:Similarly for the order 2 tensor above, for each vector a and b in ℝn:or more generally:The position vector x in ℝn is a simple and common example of a vector, and can be represented in any coordinate system. Consider the case of rectangular coordinate systems with orthonormal bases only. It is possible to have a coordinate system with rectangular geometry if the basis vectors are all mutually perpendicular and not normalized, in which case the basis is orthogonal but not orthonormal. However, orthonormal bases are easier to manipulate and are often used in practice. The following results are true for orthonormal bases, not orthogonal ones.In one rectangular coordinate system, x as a contravector has coordinates xi and basis vectors ei, while as a covector it has coordinates xi and basis covectors ei, and we have:In another rectangular coordinate system, x as a contravector has coordinates xi and bases ei, while as a covector it has coordinates xi and bases ei, and we have:Each new coordinate is a function of all the old ones, and vice versa for the inverse function:and similarly each new basis vector is a function of all the old ones, and vice versa for the inverse function:for all i, j.A vector is invariant under any change of basis, so if coordinates transform according to a transformation matrix L, the bases transform according to the matrix inverse L−1, and conversely if the coordinates transform according to inverse L−1, the bases transform according to the matrix L. The difference between each of these transformations is shown conventionally through the indices as superscripts for contravariance and subscripts for covariance, and the coordinates and bases are linearly transformed according to the following rules:where Lij represents the entries of the transformation matrix (row number is i and column number is j) and (L−1)ik denotes the entries of the inverse matrix of the matrix Lik.If L is an orthogonal transformation (orthogonal matrix), the objects transforming by it are defined as Cartesian tensors. This geometrically has the interpretation that a rectangular coordinate system is mapped to another rectangular coordinate system, in which the norm of the vector x is preserved (and distances are preserved).The determinant of L is det(L) = ±1, which corresponds to two types of orthogonal transformation: (+1) for rotations and (−1) for improper rotations (including reflections).There are considerable algebraic simplifications, the matrix transpose is the inverse from the definition of an orthogonal transformation:From the previous table, orthogonal transformations of covectors and contravectors are identical. There is no need to differ between raising and lowering indices, and in this context and applications to physics and engineering the indices are usually all subscripted to remove confusion for exponents. All indices will be lowered in the remainder of this article. One can determine the actual raised and lowered indices by considering which quantities are covectors or contravectors, and the relevant transformation rules.Exactly the same transformation rules apply to any vector a, not only the position vector. If its components ai do not transform according to the rules, a is not a vector.Despite the similarity between the expressions above, for the change of coordinates such as xj = Lijxi, and the action of a tensor on a vector like bi = Tijaj, L is not a tensor, but T is. In the change of coordinates, L is a matrix, used to relate two rectangular coordinate systems with orthonormal bases together. For the tensor relating a vector to a vector, the vectors and tensors throughout the equation all belong to the same coordinate system and basis.The entries of L are partial derivatives of the new or old coordinates with respect to the old or new coordinates, respectively.Differentiating xi with respect to xk:sois an element of the Jacobian matrix. There is a (partially mnemonical) correspondence between index positions attached to L and in the partial derivative: i at the top and j at the bottom, in each case, although for Cartesian tensors the indices can be lowered.Conversely, differentiating xj with respect to xi:sois an element of the inverse Jacobian matrix, with a similar index correspondence.Many sources state transformations in terms of the partial derivatives:and the explicit matrix equations in 3d are:similarly forAs with all linear transformations, L depends on the basis chosen. For two orthonormal basesHence the components reduce to direction cosines between the xi and xj axes:where θij and θji are the angles between the xi and xj axes. In general, θij is not equal to θji, because for example θ12 and θ21 are two different angles.The transformation of coordinates can be written:and the explicit matrix equations in 3d are:similarly forThe geometric interpretation is the xi components equal to the sum of projecting the xj components onto the xj axes.The numbers ei⋅ej arranged into a matrix would form a symmetric matrix (a matrix equal to its own transpose) due to the symmetry in the dot products, in fact it is the metric tensor g. By contrast ei⋅ej or ei⋅ej do not form symmetric matrices in general, as displayed above. Therefore, while the L matrices are still orthogonal, they are not symmetric.Apart from a rotation about any one axis, in which the xi and xi for some i coincide, the angles are not the same as Euler angles, and so the L matrices are not the same as the rotation matrices.The dot product and cross product occur very frequently, in applications of vector analysis to physics and engineering, examples include:How these products transform under orthogonal transformations is illustrated below.The dot product ⋅ of each possible pairing of the basis vectors follows from the basis being orthonormal. For perpendicular pairs we havewhile for parallel pairs we haveReplacing Cartesian labels by index notation as shown above, these results can be summarized bywhere δij are the components of the Kronecker delta. The Cartesian basis can be used to represent δ in this way.In addition, each metric tensor component gij with respect to any basis is the dot product of a pairing of basis vectors:For the Cartesian basis the components arranged into a matrix are:so are the simplest possible for the metric tensor, namely the δ:This is not true for general bases: orthogonal coordinates have diagonal metrics containing various scale factors (i.e. not necessarily 1), while general curvilinear coordinates could also have nonzero entries for off-diagonal components.The dot product of two vectors a and b transforms according towhich is intuitive, since the dot product of two vectors is a single scalar independent of any coordinates. This also applies more generally to any coordinate systems, not just rectangular ones; the dot product in one coordinate system is the same in any other.For the cross product × of two vectors, the results are (almost) the other way round. Again, assuming a right-handed 3d Cartesian coordinate system, cyclic permutations in perpendicular directions yield the next vector in the cyclic collection of vectors:while parallel vectors clearly vanish:and replacing Cartesian labels by index notation as above, these can be summarized by:where i, j, k are indices which take values 1, 2, 3. It follows that:These permutation relations and their corresponding values are important, and there is an object coinciding with this property: the Levi-Civita symbol, denoted by ε. The Levi-Civita symbol entries can be represented by the Cartesian basis:which geometrically corresponds to the volume of a cube spanned by the orthonormal basis vectors, with sign indicating orientation (and not a "positive or negative volume"). Here, the orientation is fixed by ε123 = +1, for a right-handed system. A left-handed system would fix ε123 = −1 or equivalently ε321 = +1.The scalar triple product can now be written:with the geometric interpretation of volume (of the parallelepiped spanned by a, b, c) and algebraically is a determinant:[3]This in turn can be used to rewrite the cross product of two vectors as follows:Contrary to its appearance, the Levi-Civita symbol is not a tensor, but a pseudotensor, the components transform according to:Therefore, the transformation of the cross product of a and b is:and so a × b transforms as a pseudovector, because of the determinant factor.The tensor index notation applies to any object which has entities that form multidimensional arrays – not everything with indices is a tensor by default. Instead, tensors are defined by how their coordinates and basis elements change under a transformation from one coordinate system to another.Note the cross product of two vectors is a pseudovector, while the cross product of a pseudovector with a vector is another vector.Other identities can be formed from the δ tensor and ε pseudotensor, a notable and very useful identity is one that converts two Levi-Civita symbols adjacently contracted over two indices into an antisymmetrized combination of Kronecker deltas:The index forms of the dot and cross products, together with this identity, greatly facilitate the manipulation and derivation of other identities in vector calculus and algebra, which in turn are used extensively in physics and engineering. For instance, it is clear the dot and cross products are distributive over vector addition:without resort to any geometric constructions - the derivation in each case is a quick line of algebra. Although the procedure is less obvious, the vector triple product can also be derived. Rewriting in index notation:and because cyclic permutations of indices in the ε symbol does not change its value, cyclically permuting indices in εkℓm to obtain εℓmk allows us to use the above δ-ε identity to convert the ε symbols into δ tensors:thusly:Note this is antisymmetric in b and c, as expected from the left hand side. Similarly, via index notation or even just cyclically relabelling a, b, and c in the previous result and taking the negative:and the difference in results show that the cross product is not associative. More complex identities, like quadruple products;and so on, can be derived in a similar manner.Tensors are defined as quantities which transform in a certain way under linear transformations of coordinates.Text below contradics introduced above contravariant and covariant vectors (tensors)Let a = aiei and b = biei be two vectors, so that they transform according to aj = aiLij, bj = biLij.Taking the tensor product gives:then applying the transformation to the componentsand to the basesgives the transformation law of an order-2 tensor. The tensor a⊗b is invariant under this transformation:More generally, for any order-2 tensorthe components transform according to;and the basis transforms by:If R does not transform according to this rule - whatever quantity R may be, it's not an order 2 tensor.More generally, for any order p tensorthe components transform according to;and the basis transforms by:For a pseudotensor S of order p, the components transform according to;The antisymmetric nature of the cross product can be recast into a tensorial form as follows.[4] Let c be a vector, a be a pseudovector, b be another vector, and T be a second order tensor such that:As the cross product is linear in a and b, the components of T can be found by inspection, and they are:so the pseudovector a can be written as an antisymmetric tensor. This transforms as a tensor, not a pseudotensor. For the mechanical example above for the tangential velocity of a rigid body, given by v = ω × x, this can be rewritten as v = Ω · x where Ω is the tensor corresponding to the pseudovector ω:For an example in electromagnetism, while the electric field E is a vector field, the magnetic field B is a pseudovector field. These fields are defined from the Lorentz force for a particle of electric charge q traveling at velocity v:and considering the second term containing the cross product of a pseudovector B and velocity vector v, it can be written in matrix form, with F, E, and v as column vectors and B as an antisymmetric matrix:If a pseudovector is explicitly given by a cross product of two vectors (as opposed to entering the cross product with another vector), then such pseudovectors can also be written as antisymmetric tensors of second order, with each entry a component of the cross product. The angular momentum of a classical pointlike particle orbiting about an axis, defined by J = x × p, is another example of a pseudovector, with corresponding antisymmetric tensor:Although Cartesian tensors do not occur in the theory of relativity; the tensor form of orbital angular momentum J enters the spacelike part of the relativistic angular momentum tensor, and the above tensor form of the magnetic field B enters the spacelike part of the electromagnetic tensor.It should be emphasized the following formulae are only so simple in Cartesian coordinates - in general curvilinear coordinates there are factors of the metric and its determinant - see tensors in curvilinear coordinates for more general analysis.Following are the differential operators of vector calculus. Throughout, left Φ(r, t) be a scalar field, andbe vector fields, in which all scalar and vector fields are functions of the position vector r and time t.The gradient operator in Cartesian coordinates is given by:and in index notation, this is usually abbreviated in various ways:This operator acts on a scalar field Φ to obtain the vector field directed in the maximum rate of increase of Φ:The index notation for the dot and cross products carries over to the differential operators of vector calculus.[5]The directional derivative of a scalar field Φ is the rate of change of Φ along some direction vector a (not necessarily a unit vector), formed out of the components of a and the gradient:The divergence of a vector field A is:Note the interchange of the components of the gradient and vector field yields a different differential operatorwhich could act on scalar or vector fields. In fact, if A is replaced by the velocity field u(r, t) of a fluid, this is a term in the material derivative (with many other names) of continuum mechanics, with another term being the partial time derivative:which usually acts on the velocity field leading to the non-linearity in the Navier-Stokes equations.As for the curl of a vector field A, this can be defined as a pseudovector field by means of the ε symbol:which is only valid in three dimensions, or an antisymmetric tensor field of second order via antisymmetrization of indices, indicated by delimiting the antisymmetrized indices by square brackets (see Ricci calculus):which is valid in any number of dimensions. In each case, the order of the gradient and vector field components should not be interchanged as this would result in a different differential operator:which could act on scalar or vector fields.Finally, the Laplacian operator is defined in two ways, the divergence of the gradient of a scalar field Φ:or the square of the gradient operator, which acts on a scalar field Φ or a vector field A:In physics and engineering, the gradient, divergence, curl, and Laplacian operator arise inevitably in fluid mechanics, Newtonian gravitation, electromagnetism, heat conduction, and even quantum mechanics.Vector calculus identities can be derived in a similar way to those of vector dot and cross products and combinations. For example, in three dimensions, the curl of a cross product of two vector fields A and B:where the product rule was used, and throughout the differential operator was not interchanged with A or B. Thus:One can continue the operations on tensors of higher order. Let T = T(r, t) denote a second order tensor field, again dependent on the position vector r and time t.For instance, the gradient of a vector field in two equivalent notations ("dyadic" and "tensor", respectively) is:which is a tensor field of second order.The divergence of a tensor is:which is a vector field. This arises in continuum mechanics in Cauchy's laws of motion - the divergence of the Cauchy stress tensor σ is a vector field, related to body forces acting on the fluid.Cartesian tensors are as in tensor algebra, but Euclidean structure of and restriction of the basis brings some simplifications compared to the general theory.The general tensor algebra consists of general mixed tensors of type (p, q):with basis elements:the components transform according to:as for the bases:For Cartesian tensors, only the order p + q of the tensor matters in a Euclidean space with an orthonormal basis, and all p + q indices can be lowered.  A Cartesian basis does not exist unless the vector space has a positive-definite metric, and thus cannot be used in relativistic contexts.Dyadic tensors were historically the first approach to formulating second-order tensors, similarly triadic tensors for third-order tensors, and so on. Cartesian tensors use tensor index notation, in which the variance may be glossed over and is often ignored, since the components remain unchanged by raising and lowering indices.
Vector-valued function
A vector-valued function, also referred to as a vector function, is a mathematical function of one or more variables whose range is a set of multidimensional vectors or infinite-dimensional vectors. The input of a vector-valued function could be a scalar or a vector (that is, the dimension of the domain could be 1 or greater than 1); the dimension of the domain is not defined by the dimension of the range.A common example of a vector-valued function is one that depends on a single real number parameter t, often representing time, producing a vector v(t) as the result.  In terms of the standard unit vectors i, j, k of Cartesian 3-space, these specific type of vector-valued functions are given by expressions such aswhere f(t), g(t) and h(t) are the coordinate functions of the parameter t, and the domain of this vector-valued function is the intersection of the domain of the functions f, g, and h. It can also be referred to in a different notation:The vector r(t) has its tail at the origin and its head at the coordinates evaluated by the function.In 2D, We can analogously speak about vector-valued functions asIn the linear case the function can be expressed in terms of matrices:where y is an n × 1 output vector (n > 1), x is a k × 1 vector of inputs (k ≥ 1), A is an n × k matrix of parameters, and b is an n × 1 vector of parameters.in which X (playing the role of A in the previous generic form) is an n × k matrix of fixed (empirically based) numbers.A surface is a 2-dimensional set of points embedded in 3-dimensional space. One way to represent a surface is with parametric equations, in which two parameters s and t determine the three Cartesian coordinates of any point on the surface:Here F is a vector-valued function.Many vector-valued functions, like scalar-valued functions, can be differentiated by simply differentiating the components in the Cartesian coordinate system.  Thus, ifis a vector-valued function, thenThe vector derivative admits the following physical interpretation: if r(t) represents the position of a particle, then the derivative is the velocity of the particleLikewise, the derivative of the velocity is the accelerationThe partial derivative of a vector function a with respect to a scalar variable q is defined as[1] where ai is the scalar component of a in the direction of ei.  It is also called the direction cosine of a and ei or their dot product.  The vectors e1,e2,e3 form an orthonormal basis fixed in the reference frame in which the derivative is being taken.If a is regarded as a vector function of a single scalar variable, such as time t, then the equation above reduces to the first ordinary time derivative of a with respect to t,[1]If the vector a is a function of a number n of scalar variables qr (r = 1,...,n), and each qr is only a function of time t, then the ordinary derivative of a with respect to t can be expressed, in a form known as the total derivative, as[1]Some authors prefer to use capital D to indicate the total derivative operator, as in D/Dt.  The total derivative differs from the partial time derivative in that the total derivative accounts for changes in a due to the time variance of the variables qr.Whereas for scalar-valued functions there is only a single possible reference frame, to take the derivative of a vector-valued function requires the choice of a reference frame (at least when a fixed Cartesian coordinate system is not implied as such).  Once a reference frame has been chosen, the derivative of a vector-valued function can be computed using techniques similar to those for computing derivatives of scalar-valued functions. A different choice of reference frame will, in general, produce a different derivative function. The derivative functions in different reference frames have a specific kinematical relationship.The above formulas for the derivative of a vector function rely on the assumption that the basis vectors e1,e2,e3 are constant, that is, fixed in the reference frame in which the derivative of a is being taken, and therefore the e1,e2,e3 each has a derivative of identically zero.  This often holds true for problems dealing with vector fields in a fixed coordinate system, or for simple problems in physics.  However, many complex problems involve the derivative of a vector function in multiple moving reference frames, which means that the basis vectors will not necessarily be constant.  In such a case where the basis vectors e1,e2,e3 are fixed in reference frame E, but not in reference frame N, the more general formula for the ordinary time derivative of a vector in reference frame N is[1]where the superscript N to the left of the derivative operator indicates the reference frame in which the derivative is taken.  As shown previously, the first term on the right hand side is equal to the derivative of a in the reference frame where e1,e2,e3 are constant, reference frame E.  It also can be shown that the second term on the right hand side is equal to the relative angular velocity of the two reference frames cross multiplied with the vector a itself.[1]  Thus, after substitution, the formula relating the derivative of a vector function in two reference frames is[1]where NωE is the angular velocity of the reference frame E relative to the reference frame N.One common example where this formula is used is to find the velocity of a space-borne object, such as a rocket, in the inertial reference frame using measurements of the rocket's velocity relative to the ground.  The velocity NvR in inertial reference frame N of a rocket R located at position rR can be found using the formulawhere NωE is the angular velocity of the Earth relative to the inertial frame N.  Since velocity is the derivative of position, NvR and EvR are the derivatives of rR in reference frames N and E, respectively.  By substitution, where EvR is the velocity vector of the rocket as measured from a reference frame E that is fixed to the Earth.The derivative of the products of vector functions behaves similarly to the derivative of the products of scalar functions.[2]  Specifically, in the case of scalar multiplication of a vector, if p is a scalar variable function of q,[1]In the case of dot multiplication, for two vectors a and b that are both functions of q,[1]Similarly, the derivative of the cross product of two vector functions is[1]If the values of a function f lie in an infinite-dimensional vector space X, such as a Hilbert space,then f may be called an infinite-dimensional vector function.If the argument of f is a real number and X is a Hilbert space, then the derivative of f at a point t can be defined as in the finite-dimensional case:N.B. If X is a Hilbert space, then one can easily show that any derivative (and any other limit) can be computed componentwise: ifHowever, the existence of a componentwise derivative does not guarantee the existence of a derivative, as componentwise convergence in a Hilbert space does not guarantee convergence with respect to the actual topology of the Hilbert space.Most of the above hold for other topological vector spaces X too. However, not as many classical results hold in the Banach space setting, e.g., an absolutely continuous function with values in a suitable Banach space need not have a derivative anywhere. Moreover, in most Banach spaces setting there are no orthonormal bases.
Vectorization (mathematics)
In mathematics, especially in linear algebra and matrix theory, the vectorization of a matrix is a linear transformation which converts the matrix into a column vector. Specifically, the vectorization of an m × n matrix A, denoted vec(A), is the mn × 1 column vector obtained by stacking the columns of the matrix A on top of one another:The vectorization is frequently used together with the Kronecker product to express matrix multiplication as a linear transformation on matrices. In particular,There are two other useful formulations:More generally, it has been shown that vectorization is a self-adjunction in the monoidal closed structure of any category of matrices.[1]Vectorization is an algebra homomorphism from the space of n × n matrices with the Hadamard (entrywise) product to Cn2 with its Hadamard product:Vectorization is a unitary transformation from the space of n×n matrices with the Frobenius (or Hilbert–Schmidt) inner product to Cn2 :where the superscript H denotes the conjugate transpose.Bi consists of n block matrices of size m × m, stacked column-wise, and all these matrices are all-zero except for the i-th one, which is a m × m identity matrix Im.Then the vectorized version of X can be expressed as follows:Multiplication of X by ei extracts the i-th column, while multiplication by Bi puts it into the desired position in the final vector.Alternatively, the linear sum can be expressed using the Kronecker product:For a symmetric matrix A, the vector vec(A) contains more information than is strictly necessary, since the matrix is completely determined by the symmetry together with the lower triangular portion, that is, the n(n + 1)/2 entries on and below the main diagonal. For such matrices, the half-vectorization is sometimes more useful than the vectorization. The half-vectorization, vech(A), of a symmetric n × n matrix A is the n(n + 1)/2 × 1 column vector obtained by vectorizing only the lower triangular part of A:There exist unique matrices transforming the half-vectorization of a matrix to its vectorization and vice versa called, respectively, the duplication matrix and the elimination matrix.Programming languages that implement matrices may have easy means for vectorization.In Matlab/GNU Octave a matrix A can be vectorized by A(:).GNU Octave also allows vectorization and half-vectorization with vec(A) and vech(A) respectively. Julia has the vec(A) function as well.In Python NumPy arrays implement the 'flatten' method[1], while in R the desired effect can be achieved via the c() or as.vector() functions. In R, function vec() of package 'ks' allows vectorization and function vech() implemented in both packages 'ks' and 'sn' allows half-vectorization.
Butcher group
In mathematics,  the Butcher group, named after the New Zealand mathematician John C. Butcher by Hairer & Wanner (1974), is an infinite-dimensional  Lie group[1] first introduced in numerical analysis to study solutions of non-linear ordinary differential equations by the Runge–Kutta method. It arose from an algebraic formalism involving rooted trees that provides formal power series solutions of the differential equation modeling the flow of a vector field. It was Cayley (1857), prompted by the work of Sylvester on change of variables in differential calculus, who first noted that the derivatives of a composition of functions can be conveniently expressed in terms of rooted trees and their combinatorics.Connes & Kreimer (1999) pointed out that the Butcher group is the group of characters of the Hopf algebra of rooted trees that had arisen independently in their own work on renormalization in quantum field theory and Connes' work with Moscovici on local index theorems. This Hopf algebra, often called the Connes-Kreimer algebra, is essentially equivalent to the Butcher group, since its dual can be identified with the universal enveloping algebra of the Lie algebra of the Butcher group.[2] As they commented:A rooted tree is a graph with a distinguished node, called the root, in which every other node is connected to the root by a unique path.  If the root of a tree t is removed and the nodes connected to the original node by a single bond are taken as new roots, the tree t breaks up into rooted trees t1, t2, ... Reversing this process a new tree t = [t1, t2, ...] can be constructed by joining the roots of the trees to a new common root. The number of nodes in a tree is denoted by |t|. A heap-ordering of a rooted tree t is an allocation of the numbers 1 through |t| to the nodes so that the numbers increase on any path going away from the root. Two heap orderings are equivalent, if there is an automorphism of rooted trees mapping one of them on the other. The number of equivalence classes of heap-orderings on a particular tree is denoted by α(t) and can be computed using the Butcher's formula:[3][4]where St denotes the symmetry group of t and the tree factorial is defined recursively bywith the tree factorial of an isolated root defined to be 1The ordinary differential equation for the flow of a vector field on an open subset U of RN can be writtenwhere x(s) takes values in U, f is a smooth function from U to  RN and x0 is the starting point of the flow at time s = 0.Cayley (1857) gave a method to compute the higher order derivatives x(m)(s) in terms of rooted trees. His formula can be conveniently expressed using the elementary differentials introduced by Butcher. These are defined inductively byWith this notationgiving the power series expansionAs an example when N = 1, so that x and f are real-valued functions of a single real variable, the formula yieldswhere the four terms correspond to the four rooted trees from left to right in Figure 3 above.In a single variable this formula is the same as Faà di Bruno's formula of 1855; however in several variables it has to be written more carefully in the formwhere the tree structure is crucial.The Hopf algebra H of rooted trees was defined by Connes & Kreimer (1998) in connection with Kreimer's previous work on renormalization in quantum field theory. It was later discovered that the Hopf algebra was the dual of a Hopf algebra defined earlier by Grossman & Larsen (1989) in a different context. The characters of H, i.e. the homomorphisms of the underlying commutative algebra into R, form a group, called the Butcher group. It corresponds to the formal group structure discovered in numerical analysis by Butcher (1972).The Hopf algebra of rooted trees H is defined to be the polynomial ring in the variables t, where t runs through rooted trees.The Butcher group is defined to be the set of algebra homomorphisms φ of H into R with group structureThe inverse in the Butcher group is given byand the identity by the counit ε.Using complex coefficients in the construction of the Hopf algebra of rooted trees one obtains the complex Hopf algebra of rooted trees.Its C-valued characters form a group, called the complex Butcher group GC. The complex Butcher group GC is an infinite-dimensional complex Lie group[1] which appears as a toy model in the § Renormalization of quantum field theories.The non-linear ordinary differential equationcan be solved approximately by the Runge-Kutta method. This iterative scheme requires an m x m matrixand a vectorwith m components.The scheme defines vectors xn by first finding a solution X1, ... , Xm ofand then settingButcher (1963) showed that the solution of the corresponding ordinary differential equationshas the power series expansionwhere φj and φ are determined recursively byandThe power series above are called B-series or Butcher series.[3][5] The corresponding assignment φ is an element of the Butcher group. The homomorphism corresponding to the actual flow hasHairer & Wanner (1974) proved that the Butcher group acts naturally on the functions f. Indeed, settingthey proved thatthe formal tangent space of G at the identity ε. This forms a Lie algebra with Lie bracketfor each rooted tree t.Connes & Kreimer (1998) provided a general context for using Hopf algebraic methods to give a simple mathematical formulation of renormalization in quantum field theory. Renormalization was interpreted as Birkhoff factorization of loops in the character group of the associated Hopf algebra.  The models considered by Kreimer (1999) had Hopf algebra H and character group G, the Butcher group. Brouder (2000) has given an account of this renormalization process in terms of Runge-Kutta data.In this simplified setting, a renormalizable model has two pieces of input data:[6]Note that R satisfies the Rota-Baxter identity if and only if id –  R does. An important example is the minimal subtraction schemeIn addition there is a projection P of H onto the augmentation ideal ker ε given byTo define the renormalized Feynman rules, note that the antipode S satisfiesso thatFor the minimal subtraction scheme, this process can be interpreted in terms of Birkhoff factorization in the complex Butcher group. Φ can be regarded as a map γ of the unit circle into the complexification GC of G (maps into C instead of R). As such it has a Birkhoff factorizationIn example, the Feynman rules depend on additional parameter μ, a "unit of mass". Connes & Kreimer (2001) showed thatso that γμ– is independent of μ.The complex Butcher group comes with a natural one-parameter group λw of automorphisms, dual to that on Hfor w ≠ 0 in C.The loops γμ and λw · γμ have the same negative part and, for t real,defines a one-parameter subgroup of the complex Butcher group GC called the  renormalization group flow (RG).Its infinitesimal generator β is an element of the Lie algebra of GC and is defined byIt is called the beta-function of the model.In any given model, there is usually a finite-dimensional space of complex coupling constants. The complex Butcher group acts by diffeomorphims on this space. In particular the renormalization group defines a flow on the space of coupling constants, with the beta function giving the corresponding vector field.More general models in quantum field theory require rooted trees to be replaced by Feynman diagrams with vertices decorated by symbols from a finite index set. Connes and Kreimer have also defined Hopf algebras in this setting and have shown how they can be used to systematize standard computations in renormalization theory.Kreimer (2007) has given a "toy model" involving dimensional regularization for H and the algebra V. If c is a positive integer and qμ = q / μ is a dimensionless constant, Feynman rules can be defined recursively bywhere z = 1 – D/2 is the regularization parameter. These integrals can be computed explicitly in terms of the Gamma function using the formulaIn particular
Finite field
In mathematics, a finite field or Galois field (so-named in honor of Évariste Galois) is a field that contains a finite number of elements.  As with any field, a finite field is a set on which the operations of multiplication, addition, subtraction and division are defined and satisfy certain basic rules.  The most common examples of finite fields are given by the integers mod p when p is a prime number.Finite fields are fundamental in a number of areas of mathematics and computer science, including number theory, algebraic geometry, Galois theory, finite geometry, cryptography and coding theory.The number of elements of a finite field is called its order. A finite field of order q exists if and only if the order q is a prime power pk (where p is a prime number and k is a positive integer). All finite fields of a given order are isomorphic.[1] In a field of order pk, adding p copies of any element always results in zero; that is, the characteristic of the field is p. In a finite field of order q, the polynomial Xq − X has all q elements of the finite field as roots. The non-zero elements of a finite field form a multiplicative group. This group is cyclic, so all non-zero elements can be expressed as powers of a single element called a primitive element of the field.  (In general there will be several primitive elements for a given field.)The elements of the prime field of order p may be represented by integers in the range 0, ..., p − 1. The sum, the difference and the product are computed by taking the remainder by p of the integer result. The multiplicative inverse of an element may be computed by using the extended Euclidean algorithm (see Extended Euclidean algorithm § Modular integers).Let F be a finite field. For any element x in F and any integer n, denote by n ⋅ x the sum of n copies of x. The least positive n such that n ⋅ 1 = 0 exists and is a prime number; it is called the characteristic of the field.The identity(sometimes called the freshman's dream) is true in a field of characteristic p for every x and y.  This follows from the binomial theorem, as each binomial coefficient of the expansion of (x + y)p, except the first and the last, is a multiple of p.By Fermat's little theorem, if p is a prime number and x is in the field GF(p) then xp = x.  This implies the equalityfor polynomials over GF(p).  More generally, every element in GF(pn) satisfies the polynomial equation xpn − x = 0.Any finite field extension of a finite field is separable and simple.  That is, if E is a finite field and F is a subfield of E, then E is obtained from F by adjoining a single element whose minimal polynomial is separable. To use a jargon, finite fields are perfect.A more general algebraic structure that satisfies all the other axioms of a field, but whose multiplication is not required to be commutative, is called a division ring (or sometimes skew field). By Wedderburn's little theorem, any finite division ring is commutative, and hence is a finite field.Let q = pn be a prime power, and F be the splitting field of the polynomial over the prime field GF(p). This means that F is a finite field of lowest order, in which P has q distinct roots (the roots are distinct, as the formal derivative of P is equal to −1). The above identity shows that the sum and the product of two roots of P are roots of P, as well as the multiplicative inverse of a root of P. In other word, the roots of P form a field of order q, which is equal to F by the minimality of the splitting field.The uniqueness up to isomorphism of splitting fields implies thus that all fields of order q are isomorphic.In summary, we have the following classification theorem first proved in 1893 by E. H. Moore:[1]It follows that GF(pn) contains a subfield isomorphic to GF(pm) if and only if m is a divisor of n; in that case, this subfield is unique. In fact, the polynomial Xpm − X divides Xpn − X if and only if m is a divisor of n.Given a prime power q = pn with p prime and n > 1, the field GF(q) may be explicitly constructed in the following way. One chooses first an irreducible polynomial P in GF(p)[X] of degree n (such an irreducible polynomial always exists). Then the quotient ring of the polynomial ring GF(p)[X] by the ideal generated by P is a field of order q.More explicitly, the elements of GF(q) are the polynomials over GF(p) whose degree is strictly less than n. The addition and the subtraction are those of polynomials over GF(p). The product of two elements is the remainder of the Euclidean division by P of the product in GF(p)[X].The multiplicative inverse of a non-zero element may be computed with the extended Euclidean algorithm; see Extended Euclidean algorithm § Simple algebraic field extensions.Except in the construction of GF(4), there are several possible choices for P, which produce isomorphic results. To simplify the Euclidean division, for P one commonly chooses polynomials of the form which make the needed Euclidean divisions very efficient. However, for some fields, typically in characteristic 2, irreducible polynomials of the form Xn + aX + b may not exist. In characteristic 2, if the polynomial Xn + X + 1 is reducible, it is recommended to choose Xn + Xk + 1 with the lowest possible k that makes the polynomial irreducible. If all these trinomials are reducible, one chooses "pentanomials" Xn + Xa +  Xb +  Xc +  1, as polynomials of degree greater than 1, with an even number of terms, are never irreducible in characteristic 2, having 1 as a root.[3]In the next sections, we will show how this general construction method works for small finite fields.Over GF(2), there is only one irreducible polynomial of degree 2:Therefore, for GF(4) the construction of the preceding section must involve this polynomial, andIf one denotes a a root of this polynomial in GF(4), the tables of the operations in GF(4) are the following. There is no table for subtraction, because subtraction is identical to addition, as is the case for every field of characteristic 2. In the third table, for the division of x by y, x must be read on the left, and y on the top.For applying the above general construction of finite fields in the case of GF(p2), one has to find an irreducible polynomial of degree 2. For p = 2, this has been done in the preceding section. If p is an odd prime, there are always irreducible polynomials of the form X2 − r, with r in GF(p).Having chosen a quadratic non-residue r, let α be a symbolic square root of r, that is a symbol which has the property α2 = r, in the same way as the complex number i is a symbolic square root of −1. Then, the elements of GF(p2) are all the linear expressionswith a and b in GF(p). The operations on GF(p2) are defined as follows (the operations between elements of GF(p) represented by Latin letters are the operations in GF(p)):The polynomialis irreducible over GF(2) and GF(3), that is, it is irreducible modulo 2 and 3 (to show this it suffices to show that it has no root in GF(2) nor in GF(3)). It follows that the elements of GF(8) and GF(27) may be represented by expressionsThe addition, additive inverse and multiplication on GF(8) and GF(27) may thus be defined as follows; in following formulas, the operations between elements of GF(2) or GF(3), represented by Latin letters, are the operations in GF(2) or GF(3), respectively:The polynomialis irreducible over GF(2), that is, it is irreducible modulo 2. It follows that the elements of GF(16) may be represented by expressionswhere a, b, c, d are either 0 or 1 (elements of GF(2)), and α is a symbol such that As the characteristic of GF(2) is 2, each element is its additive inverse in GF(16).The addition and multiplication on GF(16) may be defined as follows; in following formulas, the operations between elements of GF(2), represented by Latin letters are the operations in GF(2).The set of non-zero elements in GF(q) is an abelian group under the multiplication, of order q – 1. By Lagrange's theorem, there exists a divisor k of q – 1 such that xk = 1 for every non-zero x in GF(q). As the equation Xk = 1 has at most k solutions in any field, q – 1 is the lowest possible value for k.The structure theorem of finite abelian groups implies that this multiplicative group is cyclic, that is, all non-zero elements are powers of a single element. In summary:Such an element a is called a primitive element. Unless q = 2, 3, the primitive element is not unique. The number of primitive elements is φ(q − 1) where φ is Euler's totient function.The result above implies that xq = x for every x in GF(q). The particular case where q is prime is Fermat's little theorem.If a is a primitive element in GF(q), then for any non-zero element x in F, there is a unique integer n with 0 ≤ n ≤ q − 2 such thatThis integer n is called the discrete logarithm of x to the base a.While an can be computed very quickly, for example using exponentiation by squaring, there is no known efficient algorithm for computing the inverse operation, the discrete logarithm. This has been used in various cryptographic protocols, see Discrete logarithm for details.When the nonzero elements of GF(q) are represented by their discrete logarithms, multiplication and division are easy, as they reduce to addition and subtraction modulo q – 1. However, addition amounts to computing the discrete logarithm of am + an. The identity allows one to solve this problem by constructing the table of the discrete logarithms of an + 1, called Zech's logarithms, for n = 0, ..., q − 2 (it is convenient to define the discrete logarithm of zero as being −∞).Zech's logarithms are useful for large computations, such as linear algebra over medium-sized fields, that is, fields that are sufficiently large for making natural algorithms inefficient, but not too large, as one has to pre-compute a table of the same size as the order of the field.Every nonzero element of a finite field is a root of unity, as xq−1 = 1 for every nonzero element of GF(q).If n is a positive integer, an nth primitive root of unity is a solution of the equation xn = 1 that is not a solution of the equation xm = 1 for any positive integer m < n. If a is a nth primitive root of unity in a field F, then F contains all the n roots of unity, which are 1, a, a2, ..., an−1.The field GF(q) contains a nth primitive root of unity if and only if n is a divisor of q − 1; if n is a divisor of q − 1, then the number of primitive nth roots of unity in GF(q) is φ(n) (Euler's totient function). The number of nth roots of unity in GF(q) is gcd(n, q − 1).In a field of characteristic p, every (np)th root of unity is also a nth root of unity. It follows that primitive (np)th roots of unity never exist in a field of characteristic p.The field GF(64) has several interesting properties that smaller fields do not share: it has two subfields such that neither is contained in the other; not all generators (elements with minimal polynomial of degree 6 over GF(2)) are primitive elements; and the primitive elements are not all conjugate under the Galois group.The order of this field being 26, and the divisors of 6 being 1, 2, 3, 6, the subfields of GF(64) are GF(2), GF(22) = GF(4), GF(23) = GF(8), and GF(64) itself. As 2 and 3 are coprime, the intersection of GF(4) and GF(8) in GF(64) is the prime field GF(2).The union of GF(4) and GF(8) has thus 10 elements. The remaining 54 elements of GF(64) generate GF(64) in the sense that no other subfield contains any of them. It follows that they are roots of irreducible polynomials of degree 6 over GF(2). This implies that, over GF(2), there are exactly 9 = 54/6 irreducible monic polynomials of degree 6. This may be verified by factoring X64 − X over GF(2).The elements of GF(64) are primitive nth roots of unity for some n dividing 63. As the 3rd and the 7th roots of unity belong to GF(4) and GF(8), respectively, the 54 generators are primitive nth roots of unity for some n in {9, 21, 63}. Euler's totient function shows that there are 6 primitive 9th roots of unity, 12 primitive 21st roots of unity, and 36 primitive 63rd roots of unity. Summing these numbers, one finds again 54 elements.By factoring the cyclotomic polynomials over GF(2), one finds that:This shows that the best choice to construct GF(64) is to define it as GF(2)[X]/(X6 + X + 1). In fact, this generator is a primitive element, and this polynomial is the irreducible polynomial that produces the easiest Euclidean division.In this section, p is a prime number, and q = pn is a power of p.In GF(q), the identity (x + y)p = xp + yp implies that the mapis a GF(p)-linear endomorphism and a field automorphism of GF(q), which fixes every element of the subfield GF(p). It is called the Frobenius automorphism, after Ferdinand Georg Frobenius.Denoting by φk the composition of φ with itself k times, we have It has been shown in the preceding section that φn is the identity. For 0 < k < n, the automorphism φk is not the identity, as, otherwise, the polynomial would have more than pk roots.There are no other GF(p)-automorphisms of GF(q). In other words, GF(pn) has exactly n GF(p)-automorphisms, which are In terms of Galois theory, this means that GF(pn) is a Galois extension of GF(p), which has a cyclic Galois group.The fact that the Frobenius map is surjective implies that every finite field is perfect.If F is a finite field, a non-constant monic polynomial with coefficients in F is irreducible over F, if it is not the product of two non-constant monic polynomials, with coefficients in F.As every polynomial ring over a field is a unique factorization domain, every monic polynomial over a finite field may be factored in a unique way (up to the order of the factors) into a product of irreducible monic polynomials.There are efficient algorithms for testing polynomial irreducibility and factoring polynomials over finite field. They are a key step for factoring polynomials over the integers or the rational numbers. At least for this reason, every computer algebra system has functions for factoring polynomials over finite fields, or, at least, over finite prime fields.The polynomialfactors into linear factors over a field of order q. More precisely, this polynomial is the product of all monic polynomials of degree one over a field of order q.This implies that, if q = pn then Xq − X is the product of all monic irreducible polynomials over GF(p), whose degree divides n. In fact, if P is an irreducible factor over GF(p) of Xq − X, its degree divides n, as its splitting field is contained in GF(pn). Conversely, if P is an irreducible monic polynomial over GF(p) of degree d dividing n, it defines a field extension of degree d, which is contained in GF(pn), and all roots of P belong to GF(pn), and are roots of Xq − X; thus P divides Xq − X. As Xq − X does not have any multiple factor, it is thus the product of all the irreducible monic polynomials that divide it.This property is used to compute the product of the irreducible factors of each degree of polynomials over GF(p); see Distinct degree factorization.The number N(q, n) of monic irreducible polynomials of degree n over GF(q) is given by[4]where μ is the Möbius function. This formula is almost a direct consequence of above property of Xq − X.By the above formula, the number of irreducible (not necessarily monic) polynomials of degree n over GF(q) is (q − 1)N(q, n).A (slightly simpler) lower bound for N(q, n) isOne may easily deduce that, for every q and every n, there is at least one irreducible polynomial of degree n over GF(q). This lower bound is sharp for q = n = 2.In cryptography, the difficulty of the discrete logarithm problem in finite fields or in elliptic curves is the basis of several widely used protocols, such as the Diffie–Hellman protocol. For example, in 2014 a secure internet connection to Wikipedia involved the elliptic curve Diffie–Hellman protocol (ECDHE) over a large finite field.[5] In coding theory, many codes are constructed as subspaces of vector spaces over finite fields.Finite fields are widely used in number theory, as many problems over the integers may be solved by reducing them modulo one or several prime numbers. For example, the fastest known algorithms for polynomial factorization and linear algebra over the field of rational numbers proceed by reduction modulo one or several primes, and then reconstruction of the solution by using Chinese remainder theorem, Hensel lifting or the LLL algorithm.Similarly many theoretical problems in number theory can be solved by considering their reductions modulo some or all prime numbers. See, for example, Hasse principle. Many recent developments of algebraic geometry were motivated by the need to enlarge the power of these modular methods. Wiles' proof of Fermat's Last Theorem is an example of a deep result involving many mathematical tools, including finite fields.A finite field F is not algebraically closed. To demonstrate this, consider the polynomialwhich has no roots in F, since f (α) = 1 for all α in F.The direct limit of the system:If we actually construct our finite fields in such a fashion that Fpn is contained in Fpm whenever n divides m, then this direct limit can be constructed as the union of all these fields. Even if we do not construct our fields this way, we can still speak of the algebraic closure, but some more delicacy is required in its construction.A division ring is a generalization of field. Division rings are not assumed to be commutative. There are no non-commutative finite division rings: Wedderburn's little theorem states that all finite division rings are commutative, hence finite fields. The result holds even if we relax associativity and consider alternative rings, by the Artin–Zorn theorem.[6]Finite fields appear in the following chain of inclusions:
Diagonal matrix
As stated above, the off-diagonal entries are zero.  That is, the matrix D = (di,j) with n columns and n rows is diagonal ifHowever, the main diagonal entries are unrestricted.The term diagonal matrix may sometimes refer to a rectangular diagonal matrix, which is an m-by-n matrix with all the entries not of the form di,i being zero. For example:The following matrix is a symmetric diagonal matrix:If the entries are real numbers or complex numbers, then it is a normal matrix as well.In the remainder of this article we will consider only square matrices.A square diagonal matrix with all its main diagonal entries equal is a scalar matrix, that is, a scalar multiple λI of the identity matrix I. Its effect on a vector is scalar multiplication by λ. For example, a 3×3 scalar matrix has the form:The scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size. All other diagonal matrices which are not scalar only commute with  other diagonal matrices  and not with any matrix unlike scalar matrices.[1] Intuitively, this stems from the fact that scalar matrices are Identity matrices multiplied with scalars.The operations of matrix addition and matrix multiplication are especially simple for symmetric diagonal matrices. Write diag(a1, ..., an) for a diagonal matrix whose diagonal entries starting in the upper left corner are a1, ..., an. Then, for addition, we haveand for matrix multiplication,The diagonal matrix diag(a1, ..., an) is invertible if and only if the entries a1, ..., an are all non-zero. In this case, we haveIn particular, the diagonal matrices form a subring of the ring of all n-by-n matrices.Multiplying an n-by-n matrix A from the left with diag(a1, ..., an) amounts to multiplying the ith row of A by ai for all i; multiplying the matrix A from the right with diag(a1, ..., an) amounts to multiplying the ith column of A by ai for all i.In other words, the eigenvalues of diag(λ1, ..., λn) are λ1, ..., λn with associated eigenvectors of e1, ..., en.The determinant of diag(a1, ..., an) is the product a1...an.The adjugate of a diagonal matrix is again diagonal.A square matrix is diagonal if and only if it is triangular and normal.Any square diagonal matrix is also a symmetric matrix.A symmetric diagonal matrix can be defined as a matrix that is both upper- and lower-triangular. The identity matrix In and any square zero matrix are diagonal. A one-dimensional matrix is always diagonal.Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or linear map by a diagonal matrix.In fact, a given n-by-n matrix A is similar to a diagonal matrix (meaning that there is a matrix X such that X−1AX is diagonal) if and only if it has n linearly independent eigenvectors. Such matrices are said to be diagonalizable.Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if AA∗ = A∗A then there exists a unitary matrix U such that UAU∗ is diagonal). Furthermore, the singular value decomposition implies that for any matrix A, there exist unitary matrices U and V such that UAV∗ is diagonal with positive entries.In operator theory, particularly the study of PDEs, operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation. Therefore, a key technique to understanding operators is a change of coordinates–in the language of operators, an integral transform–which changes the basis to an eigenbasis of eigenfunctions: which makes the equation separable. An important example of this is the Fourier transform, which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation.Especially easy are multiplication operators, which are defined as multiplication by (the values of) a fixed function–the values of the function at each point correspond to the diagonal entries of a matrix.
Wave function
A wave function in quantum physics is a mathematical description of the quantum state of an isolated quantum system.  The wave function is a  complex-valued probability amplitude, and the probabilities for the possible results of measurements made on the system can be derived from it.  The most common symbols for a wave function are the Greek letters ψ or Ψ (lower-case and capital psi, respectively).The wave function is a function of the degrees of freedom corresponding to some maximal set of commuting observables.  Once such a representation is chosen, the wave function can be derived from the quantum state.For a given system, the choice of which commuting degrees of freedom to use is not unique, and correspondingly the domain of the wave function is also not unique.  For instance, it may be taken to be a function of all the position coordinates of the particles over position space, or the momenta of all the particles over momentum space; the two are related by a Fourier transform.  Some particles, like electrons and photons, have nonzero spin, and the wave function for such particles includes spin as an intrinsic, discrete degree of freedom; other discrete variables can also be included, such as isospin.  When a system has internal degrees of freedom, the wave function at each point in the continuous degrees of freedom (e.g., a point in space) assigns a complex number for each possible value of the discrete degrees of freedom (e.g., z-component of spin) – these values are often displayed in a column matrix (e.g., a 2 × 1 column vector for a non-relativistic electron with spin ​1⁄2).According to the superposition principle of quantum mechanics, wave functions can be added together and multiplied by complex numbers to form new wave functions and form a Hilbert space.  The inner product between two wave functions is a measure of the overlap between the corresponding physical states, and is used in the foundational probabilistic interpretation of quantum mechanics, the Born rule, relating transition probabilities to inner products.  The Schrödinger equation determines how wave functions evolve over time, and a wave function behaves qualitatively like other waves, such as water waves or waves on a string, because the Schrödinger equation is mathematically a type of wave equation.  This explains the name "wave function," and gives rise to wave–particle duality.  However, the wave function in quantum mechanics describes a kind of physical phenomenon, still open to different interpretations, which fundamentally differs from that of classic mechanical waves.[1][2][3][4][5][6][7]In Born's statistical interpretation in non-relativistic quantum mechanics,[8][9][10] the squared modulus of the wave function, |ψ|2, is a real number interpreted as the probability density of measuring a particle's being detected at a given place – or having a given momentum – at a given time, and possibly having definite values for discrete degrees of freedom.  The integral of this quantity, over all the system's degrees of freedom, must be 1 in accordance with the probability interpretation.  This general requirement that a wave function must satisfy is called the normalization condition.  Since the wave function is complex valued, only its relative phase and relative magnitude can be measured—its value does not, in isolation, tell anything about the magnitudes or directions of measurable observables; one has to apply quantum operators, whose eigenvalues correspond to sets of possible results of measurements, to the wave function ψ and calculate the statistical distributions for measurable quantities.In the 1920s and 1930s, quantum mechanics was developed using calculus and linear algebra. Those who used the techniques of calculus included Louis de Broglie, Erwin Schrödinger, and others, developing "wave mechanics". Those who applied the methods of linear algebra included Werner Heisenberg, Max Born, and others, developing "matrix mechanics". Schrödinger subsequently showed that the two approaches were equivalent.[14]In 1926, Schrödinger published the famous wave equation now named after him, indeed the Schrödinger equation, based on classical conservation of energy using quantum operators and the de Broglie relations such that the solutions of the equation are the wave functions for the quantum system.[15] However, no one was clear on how to interpret it.[16] At first, Schrödinger and others thought that wave functions represent particles that are spread out with most of the particle being where the wave function is large.[17] This was shown to be incompatible with the elastic scattering of a wave packet (representing a particle) off a target; it spreads out in all directions.[8] While a scattered particle may scatter in any direction, it does not break up and take off in all directions. In 1926, Born provided the perspective of probability amplitude.[8][9][18] This relates calculations of quantum mechanics directly to probabilistic experimental observations.It is accepted as part of the Copenhagen interpretation of quantum mechanics. There are many other interpretations of quantum mechanics. In 1927, Hartree and Fock made the first step in an attempt to solve the N-body wave function, and developed the self-consistency cycle: an iterative algorithm to approximate the solution. Now it is also known as the Hartree–Fock method.[19] The Slater determinant and permanent (of a matrix) was part of the method, provided by John C. Slater.Schrödinger did encounter an equation for the wave function that satisfied relativistic energy conservation before he published the non-relativistic one, but discarded it as it predicted negative probabilities and negative energies. In 1927, Klein, Gordon and Fock also found it, but incorporated the electromagnetic interaction and proved that it was Lorentz invariant. De Broglie also arrived at the same equation in 1928. This relativistic wave equation is now most commonly known as the Klein–Gordon equation.[20]In 1927, Pauli phenomenologically found a non-relativistic equation to describe spin-1/2 particles in electromagnetic fields, now called the Pauli equation.[21] Pauli found the wave function was not described by a single complex function of space and time, but needed two complex numbers, which respectively correspond to the spin +1/2 and −1/2 states of the fermion. Soon after in 1928, Dirac found an equation from the first successful unification of special relativity and quantum mechanics applied to the electron, now called the Dirac equation. In this, the wave function is a spinor represented by four complex-valued components:[19] two for the electron and two for the electron's antiparticle, the positron. In the non-relativistic limit, the Dirac wave function resembles the Pauli wave function for the electron. Later, other relativistic wave equations were found.All these wave equations are of enduring importance. The Schrödinger equation and the Pauli equation are under many circumstances excellent approximations of the relativistic variants. They are considerably easier to solve in practical problems than the relativistic counterparts.The Klein–Gordon equation and the Dirac equation, while being relativistic, do not represent full reconciliation of quantum mechanics and special relativity. The branch of quantum mechanics where these equations are studied the same way as the Schrödinger equation, often called relativistic quantum mechanics, while very successful, has its limitations (see e.g. Lamb shift) and conceptual problems (see e.g. Dirac sea).Relativity makes it inevitable that the number of particles in a system is not constant. For full reconciliation, quantum field theory is needed.[22] In this theory, the wave equations and the wave functions have their place, but in a somewhat different guise. The main objects of interest are not the wave functions, but rather operators, so called field operators (or just fields where "operator" is understood) on the Hilbert space of states (to be described next section). It turns out that the original relativistic wave equations and their solutions are still needed to build the Hilbert space. Moreover, the free fields operators, i.e. when interactions are assumed not to exist, turn out to (formally) satisfy the same equation as do the fields (wave functions) in many cases.Thus the Klein–Gordon equation (spin 0) and the Dirac equation (spin ​1⁄2) in this guise remain in the theory. Higher spin analogues include the Proca equation (spin 1), Rarita–Schwinger equation (spin ​3⁄2), and, more generally, the Bargmann–Wigner equations. For massless free fields two examples are the free field Maxwell equation (spin 1) and the free field Einstein equation (spin 2) for the field operators.[23] All of them are essentially a direct consequence of the requirement of Lorentz invariance. Their solutions must transform under Lorentz transformation in a prescribed way, i.e. under a particular representation of the Lorentz group and that together with few other reasonable demands, e.g. the cluster decomposition principle,[24] with implications for causality is enough to fix the equations.It should be emphasized that this applies to free field equations; interactions are not included. If a Lagrangian density (including interactions) is available, then the Lagrangian formalism will yield an equation of motion at the classical level. This equation may be very complex and not amenable to solution. Any solution would refer to a fixed number of particles and would not account for the term "interaction" as referred to in these theories, which involves the creation and annihilation of particles and not external potentials as in ordinary "first quantized" quantum theory.In string theory, the situation remains analogous. For instance, a wave function in momentum space has the role of Fourier expansion coefficient in a general state of a particle (string) with momentum that is not sharply defined.[25]For now, consider the simple case of a non-relativistic single particle, without spin, in one spatial dimension. More general cases are discussed below.The state of such a particle is completely described by its wave function,where x is position and t is time. This is a complex-valued function of two real variables x and t.For one spinless particle in 1d, if the wave function is interpreted as a probability amplitude, the square modulus of the wave function, the positive real numberis interpreted as the probability density that the particle is at x. The asterisk indicates the complex conjugate. If the particle's position is measured, its location cannot be determined from the wave function, but is described by a probability distribution. The probability that its position x will be in the interval a ≤ x ≤ b is the integral of the density over this interval:where t is the time at which the particle was measured. This leads to the normalization condition:because if the particle is measured, there is 100% probability that it will be somewhere.For a given system, the set of all possible normalizable wave functions (at any given time) forms an abstract mathematical vector space, meaning that it is possible to add together different wave functions, and multiply wave functions by complex numbers (see vector space for details). Technically, because of the normalization condition, wave functions form a projective space rather than an ordinary vector space. This vector space is infinite-dimensional, because there is no finite set of functions which can be added together in various combinations to create every possible function. Also, it is a Hilbert space, because the inner product of two wave functions Ψ1 and Ψ2 can be defined as the complex number (at time t)[nb 1]More details are given below. Although the inner product of two wave functions is a complex number, the inner product of a wave function Ψ with itself,is always a positive real number. The number ||Ψ|| (not ||Ψ||2) is called the norm of the wave function Ψ.If (Ψ, Ψ) = 1, then Ψ is normalized. If Ψ is not normalized, then dividing by its norm gives the normalized function Ψ/||Ψ||. Two wave functions Ψ1 and Ψ2 are orthogonal if (Ψ1, Ψ2) = 0. If they are normalized and orthogonal, they are orthonormal. Orthogonality (hence also orthonormality) of wave functions is not a necessary condition wave functions must satisfy, but is instructive to consider since this guarantees linear independence of the functions. In a linear combination of orthogonal wave functions Ψn we have,If the wave functions Ψn were nonorthogonal, the coefficients would be less simple to obtain.In the Copenhagen interpretation, the modulus squared of the inner product (a complex number) gives a real numberwhich, assuming both wave functions are normalized, is interpreted as the probability of the wave function Ψ2 "collapsing" to the new wave function Ψ1 upon measurement of an observable, whose eigenvalues are the possible results of the measurement, with Ψ1 being an eigenvector of the resulting eigenvalue. This is the Born rule,[8] and is one of the fundamental postulates of quantum mechanics.At a particular instant of time, all values of the wave function Ψ(x, t) are components of a vector. There are uncountably infinitely many of them and integration is used in place of summation. In Bra–ket notation, this vector is writtenand is referred to as a "quantum state vector", or simply "quantum state". There are several advantages to understanding wave functions as representing elements of an abstract vector space:The time parameter is often suppressed, and will be in the following. The x coordinate is a continuous index. The |x⟩ are the basis vectors, which are orthonormal so their inner product is a delta function;thusandwhich illuminates the identity operatorFinding the identity operator in a basis allows the abstract state to be expressed explicitly in a basis, and more (the inner product between two state vectors, and other operators for observables, can be expressed in the basis).The particle also has a wave function in momentum space:where p is the momentum in one dimension, which can be any value from −∞ to +∞, and t is time.Analogous to the position case, the inner product of two wave functions Φ1(p, t) and Φ2(p, t) can be defined as:One particular solution to the time-independent Schrödinger equation isa plane wave, which can be used in the description of a particle with momentum exactly p, since it is an eigenfunction of the momentum operator. These functions are not normalizable to unity (they aren't square-integrable), so they are not really elements of physical Hilbert space. The setforms what is called the momentum basis. This "basis" is not a basis in the usual mathematical sense. For one thing, since the functions aren't normalizable, they are instead normalized to a delta function,For another thing, though they are linearly independent, there are too many of them (they form an uncountable set) for a basis for physical Hilbert space. They can still be used to express all functions in it using Fourier transforms as described next.The x and p representations areNow take the projection of the state Ψ onto eigenfunctions of momentum using the last expression in the two equations,[26]Then utilizing the known expression for suitably normalized eigenstates of momentum in the position representation solutions of the free Schrödinger equationone obtainsLikewise, using eigenfunctions of position,The position-space and momentum-space wave functions are thus found to be Fourier transforms of each other.[27] The two wave functions contain the same information, and either one alone is sufficient to calculate any property of the particle. As representatives of elements of abstract physical Hilbert space, whose elements are the possible states of the system under consideration, they represent the same state vector, hence identical physical states, but they are not generally equal when viewed as square-integrable functions.In practice, the position-space wave function is used much more often than the momentum-space wave function. The potential entering the relevant equation (Schrödinger, Dirac, etc.) determines in which basis the description is easiest. For the harmonic oscillator, x and p enter symmetrically, so there it doesn't matter which description one uses. The same equation (modulo constants) results. From this follows, with a little bit of afterthought, a factoid: The solutions to the wave equation of the harmonic oscillator are eigenfunctions of the Fourier transform in L2.[nb 2]Following are the general forms of the wave function for systems in higher dimensions and more particles, as well as including other degrees of freedom than position coordinates or momentum components.The position-space wave function of a single particle without spin in three spatial dimensions is similar to the case of one spatial dimension above:where r is the position vector in three-dimensional space, and t is time. As always Ψ(r, t) is a complex-valued function of real variables. As a single vector in Dirac notationAll the previous remarks on inner products, momentum space wave functions, Fourier transforms, and so on extend to higher dimensions.For a particle with spin, ignoring the position degrees of freedom, the wave function is a function of spin only (time is a parameter);where sz is the spin projection quantum number along the z axis. (The z axis is an arbitrary choice; other axes can be used instead if the wave function is transformed appropriately, see below.) The sz parameter, unlike r and t, is a discrete variable. For example, for a spin-1/2 particle, sz can only be +1/2 or −1/2, and not any other value. (In general, for spin s, sz can be s, s − 1, ... , −s + 1, −s). Inserting each quantum number gives a complex valued function of space and time, there are 2s + 1 of them. These can be arranged into a column vector[nb 3]In bra–ket notation, these easily arrange into the components of a vector[nb 4]The entire vector ξ is a solution of the Schrödinger equation (with a suitable Hamiltonian), which unfolds to a coupled system of 2s + 1 ordinary differential equations with solutions ξ(s, t), ξ(s − 1, t), ..., ξ(−s, t). The term "spin function" instead of "wave function" is used by some authors. This contrasts the solutions to position space wave functions, the position coordinates being continuous degrees of freedom, because then the Schrödinger equation does take the form of a wave equation.More generally, for a particle in 3d with any spin, the wave function can be written in "position–spin space" as:and these can also be arranged into a column vectorin which the spin dependence is placed in indexing the entries, and the wave function is a complex vector-valued function of space and time only.All values of the wave function, not only for discrete but continuous variables also, collect into a single vectorFor a single particle, the tensor product ⊗ of its position state vector |ψ⟩ and spin state vector |ξ⟩ gives the composite position-spin state vectorwith the identificationsThe tensor product factorization is only possible if the orbital and spin angular momenta of the particle are separable in the Hamiltonian operator underlying the system's dynamics (in other words, the Hamiltonian can be split into the sum of orbital and spin terms[28]). The time dependence can be placed in either factor, and time evolution of each can be studied separately. The factorization is not possible for those interactions where an external field or any space-dependent quantity couples to the spin; examples include a particle in a magnetic field, and spin–orbit coupling.The preceding discussion is not limited to spin as a discrete variable, the total angular momentum J may also be used.[29] Other discrete degrees of freedom, like isospin, can expressed similarly to the case of spin above.If there are many particles, in general there is only one wave function, not a separate wave function for each particle. The fact that one wave function describes many particles is what makes quantum entanglement and the EPR paradox possible. The position-space wave function for N particles is written:[19]where ri is the position of the ith particle in three-dimensional space, and t is time. Altogether, this is a complex-valued function of 3N + 1 real variables.In quantum mechanics there is a fundamental distinction between identical particles and distinguishable particles. For example, any two electrons are identical and fundamentally indistinguishable from each other; the laws of physics make it impossible to "stamp an identification number" on a certain electron to keep track of it.[27] This translates to a requirement on the wave function for a system of identical particles:where the + sign occurs if the particles are all bosons and − sign if they are all fermions. In other words, the wave function is either totally symmetric in the positions of bosons, or totally antisymmetric in the positions of fermions.[30] The physical interchange of particles corresponds to mathematically switching arguments in the wave function. The antisymmetry feature of fermionic wave functions leads to the Pauli principle. Generally, bosonic and fermionic symmetry requirements are the manifestation of particle statistics and are present in other quantum state formalisms.For N distinguishable particles (no two being identical, i.e. no two having the same set of quantum numbers), there is no requirement for the wave function to be either symmetric or antisymmetric.For a collection of particles, some identical with coordinates r1, r2, ... and others distinguishable x1, x2, ... (not identical with each other, and not identical to the aforementioned identical particles), the wave function is symmetric or antisymmetric in the identical particle coordinates ri only:Again, there is no symmetry requirement for the distinguishable particle coordinates xi.The wave function for N particles each with spin is the complex-valued functionAccumulating all these components into a single vector,For identical particles, symmetry requirements apply to both position and spin arguments of the wave function so it has the overall correct symmetry.The formulae for the inner products are integrals over all coordinates or momenta and sums over all spin quantum numbers. For the general case of N particles with spin in 3d,this is altogether N three-dimensional volume integrals and N sums over the spins. The differential volume elements d3ri are also written "dVi" or "dxi dyi dzi".The multidimensional Fourier transforms of the position or position–spin space wave functions yields momentum or momentum–spin space wave functions.For the general case of N particles with spin in 3d, if Ψ is interpreted as a probability amplitude, the probability density isand the probability that particle 1 is in region R1 with spin sz1 = m1 and particle 2 is in region R2 with spin sz2 = m2 etc. at time t is the integral of the probability density over these regions and evaluated at these spin numbers:For systems in time-independent potentials, the wave function can always be written as a function of the degrees of freedom multiplied by a time-dependent phase factor, the form of which is given by the Schrödinger equation. For N particles, considering their positions only and suppressing other degrees of freedom,where E is the energy eigenvalue of the system corresponding to the eigenstate Ψ. Wave functions of this form are called stationary states.The time dependence of the quantum state and the operators can be placed according to unitary transformations on the operators and states. For any quantum state |Ψ⟩ and operator O, in the Schrödinger picture |Ψ(t)⟩ changes with time according to the Schrödinger equation while O is constant. In the Heisenberg picture it is the other way round, |Ψ⟩ is constant while O(t) evolves with time according to the Heisenberg equation of motion. The Dirac (or interaction) picture is intermediate, time dependence is places in both operators and states which evolve according to equations of motion. It is useful primarily in computing S-matrix elements.[31]The following are solutions to the Schrödinger equation for one nonrelativistic spinless particle.One of most prominent features of the wave mechanics is a possibility for a particle to reach a location with a prohibitive (in classical mechanics) force potential. A common model is the "potential barrier", the one-dimensional case has the potentialand the steady-state solutions to the wave equation have the form (for some constants k, κ)Note that these wave functions are not normalized; see scattering theory for discussion.The standard interpretation of this is as a stream of particles being fired at the step from the left (the direction of negative x): setting Ar = 1 corresponds to firing particles singly; the terms containing Ar and Cr signify motion to the right, while Al and Cl – to the left. Under this beam interpretation, put Cl = 0 since no particles are coming from the right. By applying the continuity of wave functions and their derivatives at the boundaries, it is hence possible to determine the constants above.In a semiconductor crystallite whose radius is smaller than the size of its exciton Bohr radius, the excitons are squeezed, leading to quantum confinement. The energy levels can then be modeled using the particle in a box model in which the energy of different states is dependent on the length of the box.The wave functions for the quantum harmonic oscillator can be expressed in terms of Hermite polynomials Hn, they arewhere n = 0,1,2,....The wave functions of an electron in a Hydrogen atom are expressed in terms of spherical harmonics and generalized Laguerre polynomials (these are defined differently by different authors—see main article on them and the hydrogen atom).It is convenient to use spherical coordinates, and the wave function can be separated into functions of each coordinate,[32]where R are radial functions and Ymℓ(θ, φ) are spherical harmonics of degree ℓ and order m. This is the only atom for which the Schrödinger equation has been solved exactly. Multi-electron atoms require approximative methods. The family of solutions is:[33]where a0 = 4πε0ħ2/mee2 is the Bohr radius,L2ℓ + 1n − ℓ − 1 are the generalized Laguerre polynomials of degree n − ℓ − 1, n = 1, 2, ...  is the principal quantum number, ℓ = 0, 1, ... n − 1 the azimuthal quantum number, m = −ℓ, −ℓ + 1, ...,  ℓ − 1, ℓ the magnetic quantum number. Hydrogen-like atoms have very similar solutions.This solution does not take into account the spin of the electron.In the figure of the hydrogen orbitals, the 19 sub-images are images of wave functions in position space (their norm squared). The wave functions represent the abstract state characterized by the triple of quantum numbers (n, l, m), in the lower right of each image. These are the principal quantum number, the orbital angular momentum quantum number, and the magnetic quantum number. Together with one spin-projection quantum number of the electron, this is a complete set of observables.The figure can serve to illustrate some further properties of the function spaces of wave functions. The concept of function spaces enters naturally in the discussion about wave functions. A function space is a set of functions, usually with some defining requirements on the functions (in the present case that they are square integrable), sometimes with an algebraic structure on the set (in the present case a vector space structure with an inner product), together with a topology on the set. The latter will sparsely be used here, it is only needed to obtain a precise definition of what it means for a subset of a function space to be closed. It will be concluded below that the function space of wave functions is a Hilbert space. This observation is the foundation of the predominant mathematical formulation of quantum mechanics.A wave function is an element of a function space partly characterized by the following concrete and abstract descriptions.This similarity is of course not accidental. There are also a distinctions between the spaces to keep in mind.Basic states are characterized by a set of quantum numbers. This is a set of eigenvalues of a maximal set of commuting observables. Physical observables are represented by linear operators, also called observables, on the vectors space. Maximality means that there can be added to the set no further algebraically independent observables that commute with the ones already present. A choice of such a set may be called a choice of representation.The abstract states are "abstract" only in that an arbitrary choice necessary for a particular explicit description of it is not given. This is the same as saying that no choice of maximal set of commuting observables has been given. This is analogous to a vector space without a specified basis. Wave functions corresponding to a state are accordingly not unique. This non-uniqueness reflects the non-uniqueness in the choice of a maximal set of commuting observables. For one spin particle in one dimension, to a particular state there corresponds two wave functions, Ψ(x, Sz) and Ψ(p, Sy), both describing the same state. Each choice of representation should be thought of as specifying a unique function space in which wave functions corresponding to that choice of representation lives. This distinction is best kept, even if one could argue that two such function spaces are mathematically equal, e.g. being the set of square integrable functions. One can then think of the function spaces as two distinct copies of that set.There is an additional algebraic structure on the vector spaces of wave functions and the abstract state space.This motivates the introduction of an inner product on the vector space of abstract quantum states, compatible with the mathematical observations above when passing to a representation. It is denoted (Ψ, Φ), or in the Bra–ket notation ⟨Ψ|Φ⟩. It yields a complex number. With the inner product, the function space is an inner product space. The explicit appearance of the inner product (usually an integral or a sum of integrals) depends on the choice of representation, but the complex number (Ψ, Φ) does not. Much of the physical interpretation of quantum mechanics stems from the Born rule. It states that the probability p of finding upon measurement the state Φ given the system is in the state Ψ iswhere Φ and Ψ are assumed normalized. Consider a scattering experiment. In quantum field theory, if Φout describes a state in the "distant future" (an "out state") after interactions between scattering particles have ceased, and Ψin an "in state" in the "distant past", then the quantities (Φout, Ψin), with Φout and Ψin varying over a complete set of in states and out states respectively, is called the S-matrix or scattering matrix. Knowledge of it is, effectively, having solved the theory at hand, at least as far as predictions go. Measurable quantities such as decay rates and scattering cross sections are calculable from the S-matrix.[35]The above observations encapsulate the essence of the function spaces of which wave functions are elements. However, the description is not yet complete. There is a further technical requirement on the function space, that of completeness, that allows one to take limits of sequences in the function space, and be ensured that, if the limit exists, it is an element of the function space. A complete inner product space is called a Hilbert space. The property of completeness is crucial in advanced treatments and applications of quantum mechanics. For instance, the existence of projection operators or orthogonal projections relies on the completeness of the space.[36] These projection operators, in turn, are essential for the statement and proof of many useful theorems, e.g. the spectral theorem. It is not very important in introductory quantum mechanics, and technical details and links may be found in footnotes like the one that follows.[nb 7] The space L2 is a Hilbert space, with inner product presented later. The function space of the example of the figure is a subspace of L2. A subspace of a Hilbert space is a Hilbert space if it is closed.In summary, the set of all possible normalizable wave functions for a system with a particular choice of basis, together with the null vector, constitute a Hilbert space.Not all functions of interest are elements of some Hilbert space, say L2. The most glaring example is the set of functions e​2πip · x⁄h. These are plane wave solutions of the Schrödinger equation for a free particle, but are not normalizable, hence not in L2. But they are nonetheless fundamental for the description. One can, using them, express functions that are normalizable using wave packets. They are, in a sense, a basis (but not a Hilbert space basis, nor a Hamel basis) in which wave functions of interest can be expressed. There is also the artifact "normalization to a delta function" that is frequently employed for notational convenience, see further down. The delta functions themselves aren't square integrable either.The above description of the function space containing the wave functions is mostly mathematically motivated. The function spaces are, due to completeness, very large in a certain sense. Not all functions are realistic descriptions of any physical system. For instance, in the function space L2 one can find the function that takes on the value 0 for all rational numbers and -i for the irrationals in the interval [0, 1]. This is square integrable,[nb 8] but can hardly represent a physical state.While the space of solutions as a whole is a Hilbert space there are many other Hilbert spaces that commonly occur as ingredients.More generally, one may consider a unified treatment of all second order polynomial solutions to the Sturm–Liouville equations in the setting of Hilbert space. These include the Legendre and Laguerre polynomials as well as Chebyshev polynomials, Jacobi polynomials and Hermite polynomials. All of these actually appear in physical problems, the latter ones in the harmonic oscillator, and what is otherwise a bewildering maze of properties of special functions becomes an organized body of facts. For this, see Byron & Fuller (1992, Chapter 5).There occurs also finite-dimensional Hilbert spaces. The space ℂn is a Hilbert space of dimension n. The inner product is the standard inner product on these spaces. In it, the "spin part" of a single particle wave function resides.With more particles, the situations is more complicated. One has to employ tensor products and use representation theory of the symmetry groups involved (the rotation group and the Lorentz group respectively) to extract from the tensor product the spaces in which the (total) spin wave functions reside. (Further problems arise in the relativistic case unless the particles are free.[37] See the Bethe–Salpeter equation.) Corresponding remarks apply to the concept of isospin, for which the symmetry group is SU(2). The models of the nuclear forces of the sixties (still useful today, see nuclear force) used the symmetry group SU(3). In this case, as well, the part of the wave functions corresponding to the inner symmetries reside in some ℂn or subspaces of tensor products of such spaces.Due to the infinite-dimensional nature of the system, the appropriate mathematical tools are objects of study in functional analysis.Not all introductory textbooks take the long route and introduce the full Hilbert space machinery, but the focus is on the non-relativistic Schrödinger equation in position representation for certain standard potentials. The following constraints on the wave function are sometimes explicitly formulated for the calculations and physical interpretation to make sense:[38][39]It is possible to relax these conditions somewhat for special purposes.[nb 10] If these requirements are not met, it is not possible to interpret the wave function as a probability amplitude.[40]This does not alter the structure of the Hilbert space that these particular wave functions inhabit, but it should be pointed out that the subspace of the square-integrable functions L2, which is a Hilbert space, satisfying the second requirement is not closed in L2, hence not a Hilbert space in itself.[nb 11] The functions that does not meet the requirements are still needed for both technical and practical reasons.[nb 12][nb 13]As has been demonstrated, the set of all possible wave functions in some representation for a system constitute an in general infinite-dimensional Hilbert space. Due to the multiple possible choices of representation basis, these Hilbert spaces are not unique. One therefore talks about an abstract Hilbert space, state space, where the choice of representation and basis is left undetermined. Specifically, each state is represented as an abstract vector in state space.[41] A quantum state |Ψ⟩ in any representation is generally expressed as a vectorwhere These quantum numbers index the components of the state vector. More, all α are in an n-dimensional set A = A1 × A2 × ... An where each Ai is the set of allowed values for αi; all ω are in an m-dimensional "volume" Ω ⊆ ℝm where Ω = Ω1 × Ω2 × ... Ωm and each Ωi ⊆ ℝ is the set of allowed values for ωi, a subset of the real numbers ℝ. For generality n and m are not necessarily equal.Example: (a) For a single particle in 3d with spin s, neglecting other degrees of freedom, using Cartesian coordinates, we could take α = (sz) for the spin quantum number of the particle along the z direction, and ω = (x, y, z) for the particle's position coordinates. Here A = {−s, −s + 1, ..., s − 1, s}  is the set of allowed spin quantum numbers and Ω = ℝ3 is the set of all possible particle positions throughout 3d position space. (b) An alternative choice is α = (sy) for the spin quantum number along the y direction and ω = (px, py, pz) for the particle's momentum components. In this case A and Ω are the same as before.The probability of finding system with α in some or all possible discrete-variable configurations, D ⊆ A, and ω in some or all possible continuous-variable configurations, C ⊆ Ω, is the sum and integral over the density,[nb 14]Since the sum of all probabilities must be 1, the normalization conditionmust hold at all times during the evolution of the system.The normalization condition requires ρ dmω to be dimensionless, by dimensional analysis Ψ must have the same units as (ω1ω2...ωm)−1/2.Whether the wave function really exists, and what it represents, are major questions in the interpretation of quantum mechanics. Many famous physicists of a previous generation puzzled over this problem, such as Schrödinger, Einstein and Bohr. Some advocate formulations or variants of the Copenhagen interpretation (e.g. Bohr, Wigner and von Neumann) while others, such as Wheeler or Jaynes, take the more classical approach[42] and regard the wave function as representing information in the mind of the observer, i.e. a measure of our knowledge of reality.  Some, including Schrödinger, Bohm and Everett and others, argued that the wave function must have an objective, physical existence. Einstein thought that a complete description of physical reality should refer directly to physical space and time, as distinct from the wave function, which refers to an abstract mathematical space.[43]
Cauchy–Schwarz inequality
In mathematics, the Cauchy–Schwarz inequality, also known as the Cauchy–Bunyakovsky–Schwarz inequality, is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics.[1]The inequality for sums was published by Augustin-Louis Cauchy (1821), while the corresponding inequality for integrals was first proved byViktor Bunyakovsky (1859). The modern proof of the integral inequality was given by Hermann Amandus Schwarz (1888).[1]or LetThen, by linearity of the inner product in its first argument, one haswhich givesThis establishes the theorem.Titu's lemma (named after Titu Andreescu, also known as T2 Lemma, Engel's form, or Sedrakyan's inequality) states that for positive reals, we havewhich yields the Cauchy–Schwarz inequality.For the inner product space of square-integrable complex-valued functions, one hasA generalization of this is the Hölder inequality.The triangle inequality for the standard norm is often shown as a consequence of the Cauchy–Schwarz inequality, as follows: given vectors x and y:Taking square roots gives the triangle inequality.The Cauchy–Schwarz inequality is used to prove that the inner product is a continuous function with respect to the topology induced by the inner product itself.[8][9]The Cauchy–Schwarz inequality allows one to extend the notion of "angle between two vectors" to any real inner-product space by defining:[10][11]The Cauchy–Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [−1, 1] and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space. It can also be used to define an angle in complex inner-product spaces, by taking the absolute value or the real part of the right-hand side,[12][13] as is done when extracting a metric from quantum fidelity.Let X, Y be random variables, then the covariance inequality[14][15] is given byAfter defining an inner product on the set of random variables using the expectation of their product,then the Cauchy–Schwarz inequality becomesVarious generalizations of the Cauchy–Schwarz inequality exist in the context of operator theory, e.g. for operator-convex functions and operator algebras, where the domain and/or range are replaced by a C*-algebra or W*-algebra.which extends verbatim to positive functionals on C*-algebras:The next two theorems are further examples in operator algebra.Another generalization is a refinement obtained by interpolating between both sides the Cauchy-Schwarz inequality:  It can be easily proven by Hölder's inequality.[23] There are also non commutative versions for operators and tensor products of matrices.[24]
Levinson recursion
Levinson recursion or Levinson–Durbin recursion is a procedure in linear algebra to recursively calculate the solution to an equation involving a Toeplitz matrix. The algorithm runs in Θ(n2) time, which is a strong improvement over Gauss–Jordan elimination, which runs in Θ(n3).The Levinson–Durbin algorithm was proposed first by Norman Levinson in 1947, improved by James Durbin in 1960, and subsequently improved to 4n2 and then 3n2 multiplications by W. F. Trench and S. Zohar, respectively.Other methods to process data include Schur decomposition and Cholesky decomposition. In comparison to these, Levinson recursion (particularly split Levinson recursion) tends to be faster computationally, but more sensitive to computational inaccuracies like round-off errors.The Bareiss algorithm for Toeplitz matrices (not to be confused with the general Bareiss algorithm) runs about as fast as Levinson recursion, but it uses O(n2) space, whereas Levinson recursion uses only O(n) space.  The Bareiss algorithm, though, is numerically stable,[1][2] whereas Levinson recursion is at best only weakly stable (i.e. it exhibits numerical stability for well-conditioned linear systems).[3]Newer algorithms, called asymptotically fast or sometimes superfast Toeplitz algorithms, can solve in Θ(n logpn) for various p (e.g. p = 2,[4][5] p = 3 [6]). Levinson recursion remains popular for several reasons; for one, it is relatively easy to understand in comparison; for another, it can be faster than a superfast algorithm for small n (usually n < 256).[7]Matrix equations follow the form:For the sake of this article, êi is a vector made up entirely of zeroes, except for its ith place, which holds the value one. Its length will be implicitly determined by the surrounding context. The term N refers to the width of the matrix above – M is an N×N matrix. Finally, in this article, superscripts refer to an inductive index, whereas subscripts denote indices. For example (and definition), in this article, the matrix Tn is an n×n matrix which copies the upper left n×n block from M – that is, Tnij = Mij.Tn is also a Toeplitz matrix; meaning that it can be written as:The algorithm proceeds in two steps. In the first step, two sets of vectors, called the forward and backward vectors, are established. The forward vectors are used to help get the set of backward vectors; then they can be immediately discarded. The backwards vectors are necessary for the second step, where they are used to build the solution desired.An important simplification can occur when M is a symmetric matrix; then the two vectors are related by bni = fnn+1−i—that is, they are row-reversals of each other. This can save some extra computation in that special case.Even if the matrix is not symmetric, then the nth forward and backward vector may be found from the vectors of length n − 1 as follows. First, the forward vector may be extended with a zero to obtain:In going from Tn−1 to Tn, the extra column added to the matrix does not perturb the solution when a zero is used to extend the forward vector. However, the extra row added to the matrix has perturbed the solution; and it has created an unwanted error term εf which occurs in the last place. The above equation gives it the value of:This error will be returned to shortly and eliminated from the new forward vector; but first, the backwards vector must be extended in a similar (albeit reversed) fashion. For the backwards vector,As before, the extra column added to the matrix does not perturb this new backwards vector; but the extra row does. Here we have another unwanted error εb with value:If α and β are chosen so that the right hand side yields ê1 or ên, then the quantity in the parentheses will fulfill the definition of the nth forward or backward vector, respectively. With those alpha and beta chosen, the vector sum in the parentheses is simple and yields the desired result.Now, all the zeroes in the middle of the two vectors above being disregarded and collapsed, only the following equation is left:With these solved for (by using the Cramer 2×2 matrix inverse formula), the new forward and backward vectors are:Performing these vector summations, then, gives the nth forward and backward vectors from the prior ones. All that remains is to find the first of these vectors, and then some quick sums and multiplications give the remaining ones. The first forward and backward vectors are simply:The above steps give the N backward vectors for M. From there, a more arbitrary equation is:The solution is then built recursively by noticing that ifThen, extending with a zero again, and defining an error constant where necessary:We can then use the nth backward vector to eliminate the error term and replace it with the desired formula as follows:In practice, these steps are often done concurrently with the rest of the procedure, but they form a coherent unit and deserve to be treated as their own step.If M is not strictly Toeplitz, but block Toeplitz, the Levinson recursion can be derived in much the same way by regarding the block Toeplitz matrix as a Toeplitz matrix with matrix elements (Musicus 1988). Block Toeplitz matrices arise naturally in signal processing algorithms when dealing with multiple signal streams (e.g., in MIMO systems) or cyclo-stationary signals.Defining sourcesFurther workSummaries
Compressed sensing
Compressed sensing (also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Shannon-Nyquist sampling theorem. There are two conditions under which recovery is possible.[1] The first one is sparsity which requires the signal to be sparse in some domain. The second one is incoherence which is applied through the isometric property which is sufficient for sparse signals.[2][3]A common goal of the engineering field of signal processing is to reconstruct a signal from a series of sampling measurements. In general, this task is impossible because there is no way to reconstruct a signal during the times that the signal is not measured. Nevertheless, with prior knowledge or assumptions about the signal, it turns out to be possible to perfectly reconstruct a signal from a series of measurements (acquiring this series of measurements is called sampling). Over time, engineers have improved their understanding of which assumptions are practical and how they can be generalized.An early breakthrough in signal processing was the Nyquist–Shannon sampling theorem. It states that if a real signal's highest frequency is less than half of the sampling rate (or less than the sampling rate, if the signal is complex), then the signal can be reconstructed perfectly by means of sinc interpolation. The main idea is that with prior knowledge about constraints on the signal's frequencies, fewer samples are needed to reconstruct the signal.Around 2004, Emmanuel Candès, Justin Romberg, Terence Tao, and David Donoho proved that given knowledge about a signal's sparsity, the signal may be reconstructed with even fewer samples than the sampling theorem requires.[4][5] This idea is the basis of compressed sensing.At first glance, compressed sensing might seem to violate the sampling theorem, because compressed sensing depends on the sparsity of the signal in question and not its highest frequency. This is a misconception, because the sampling theorem guarantees perfect reconstruction given sufficient, not necessary, conditions. A sampling method fundamentally different from classical fixed-rate sampling cannot "violate" the sampling theorem. Sparse signals with high frequency components can be highly under-sampled using compressed sensing compared to classical fixed-rate sampling.[10]In order to choose a solution to such a system, one must impose extra constraints or conditions (such as smoothness) as appropriate. In compressed sensing, one adds the constraint of sparsity, allowing only solutions which have a small number of nonzero coefficients. Not all underdetermined systems of linear equations have a sparse solution. However, if there is a unique sparse solution to the underdetermined system, then the compressed sensing framework allows the recovery of that solution.Compressed sensing takes advantage of the redundancy in many interesting signals—they are not pure noise. In particular, many signals are sparse, that is, they contain many coefficients close to or equal to zero, when represented in some domain.[11] This is the same insight used in many forms of lossy compression.Compressed sensing typically starts with taking a weighted linear combination of samples also called compressive measurements in a basis different from the basis in which the signal is known to be sparse. The results found by  Emmanuel Candès, Justin Romberg,  Terence Tao and  David Donoho, showed that the number of these compressive measurements can be small and still contain nearly all the useful information. Therefore, the task of converting the image back into the intended domain involves solving an underdetermined matrix equation since the number of compressive measurements taken is smaller than the number of pixels in the full image. However, adding the constraint that the initial signal is sparse enables one to solve this underdetermined system of linear equations.Total variation can be seen as a non-negative real-valued functional defined on the space of real-valued functions (for the case of functions of one variable) or on the space of integrable functions (for the case of functions of several variables). For signals, especially, total variation refers to the integral of the absolute gradient of the signal. In signal and image reconstruction, it is applied as total variation regularization where the underlying principle is that signals with excessive details have high total variation and that removing these details, while retaining important information such as edges, would reduce the total variation of the signal and make the signal subject closer to the original signal in the problem.Recent progress on this problem involves using an iteratively directional TV refinement for CS reconstruction.[15] This method would have 2 stages: the first stage would estimate and refine the initial orientation field - which is defined as a noisy point-wise initial estimate, through edge-detection, of the given image. In the second stage, the CS reconstruction model is presented by utilizing directional TV regularizer. More details about these TV-based approaches - iteratively reweighted l1 minimization, edge-preserving TV and iterative model using directional orientation field and TV- are provided below.Early iterations may find inaccurate sample estimates, however this method will down-sample these at a later stage to give more weight to the smaller non-zero signal estimates. One of the disadvantages is the need for defining a valid starting point as a global minimum might not be obtained every time due to the concavity of the function. Another disadvantage is that this method tends to uniformly penalize the image gradient irrespective of the underlying image structures. This causes over-smoothing of edges, especially those of low contrast regions, subsequently leading to loss of low contrast information. The advantages of this method include: reduction of the sampling rate for sparse signals; reconstruction of the image while being robust to the removal of noise and other artifacts; and use of very few iterations. This can also help in recovering images with sparse gradients.Some of the disadvantages of this method are the absence of smaller structures in the reconstructed image and degradation of image resolution. This edge preserving TV algorithm, however, requires fewer iterations than the conventional TV algorithm.[14] Analyzing the horizontal and vertical intensity profiles of the reconstructed images, it can be seen that there are sharp jumps at edge points and negligible, minor fluctuation at non-edge points. Thus, this method leads to low relative error and higher correlation as compared to the TV method. It also effectively suppresses and removes any form of image noise and image artifacts such as streaking.To overcome this drawback, a refined orientation model is defined in which the data term reduces the effect of noise and improves accuracy while the second penalty term with the L2-norm is a fidelity term which ensures accuracy of initial coarse estimation.For the orientation field refinement model, the Lagrangian multipliers are updated in the iterative process as follows:For the iterative directional total variation refinement model, the Lagrangian multipliers are updated as follows:Based on Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics and known ground-truth images for testing performance, it is concluded that iterative directional total variation has a better reconstructed performance than the non-iterative methods in preserving edge and texture areas. The orientation field refinement model plays a major role in this improvement in performance as it increases the number of directionless pixels in the flat area while enhancing the orientation field consistency in the regions with edges.The field of compressive sensing is related to several topics in signal processing and computational mathematics, such as underdetermined linear-systems, group testing, heavy hitters, sparse coding, multiplexing, sparse sampling, and finite rate of innovation. Its broad scope and generality has enabled several innovative CS-enhanced approaches in signal processing and compression, solution of inverse problems, design of radiating systems, radar and through-the-wall imaging, and antenna characterization.[22]  Imaging techniques having a strong affinity with compressive sensing include coded aperture and computational photography. Implementations of compressive sensing in hardware at different technology readiness levels is available.[23]Compressed sensing is used in a mobile phone camera sensor. The approach allows a reduction in image acquisition energy per image by as much as a factor of 15 at the cost of complex decompression algorithms; the computation may require an off-device implementation.[26]Compressed sensing is used in single-pixel cameras from Rice University.[27] Bell Labs employed the technique in a lensless single-pixel camera that takes stills using repeated snapshots of randomly chosen apertures from a grid. Image quality improves with the number of snapshots, and generally requires a small fraction of the data of conventional imaging, while eliminating lens/focus-related aberrations.[28][29]Compressed sensing can be used to improve image reconstruction in holography by increasing the number of voxels one can infer from a single hologram.[30][31][32] It is also used for image retrieval from undersampled measurements in optical [33][34] and millimeter-wave [35] holography.Compressed sensing is being used in facial recognition applications.[36]Compressed sensing has been used [37][38]  to shorten magnetic resonance imaging scanning sessions on conventional hardware.[39][40][41] Reconstruction methods includeCompressed sensing addresses the issue of high scan time by enabling faster acquisition by measuring fewer Fourier coefficients. This produces a high-quality image with relatively lower scan time. Another application (also discussed ahead) is for CT reconstruction with fewer X-ray projections. Compressed sensing, in this case, removes the high spatial gradient parts - mainly, image noise and artifacts. This holds tremendous potential as one can obtain high-resolution CT images at low radiation doses (through lower current-mA settings).[45]Compressed sensing has showed outstanding results in the application of network tomography to network management. Network delay estimation and network congestion detection can both be modeled as underdetermined systems of linear equations where the coefficient matrix is the network routing matrix. Moreover, in the Internet, network routing matrices usually satisfy the criterion for using compressed sensing.[46]Commercial shortwave-infrared cameras based upon compressed sensing are available.[47] These cameras have light sensitivity from 0.9 µm to 1.7 µm, which are wavelengths invisible to the human eye.In the field of radio astronomy, compressed sensing has been proposed for deconvolving an interferometric image.[48] In fact, the Högbom CLEAN algorithm that has been in use for the deconvolution of radio images since 1974, is similar to compressed sensing's matching pursuit algorithm.Compressed sensing combined with a moving aperture has been used to increase the acquisition rate of images in a transmission electron microscope.[49] In scanning mode, compressive sensing combined with random scanning of the electron beam has enabled both faster acquisition and less electron dose, which allows for imaging of electron beam sensitive materials.[50]
Gaussian elimination
In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the corresponding matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 A.D. (see History section).To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the leftmost nonzero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (unreduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.The process of row reduction makes use of elementary row operations, and can be divided into two parts. The first part (sometimes called forward elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. The second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form.Another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. The elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. Alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a Frobenius matrix. Then the first part of the algorithm computes an LU decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix.There are three types of elementary row operations which may be performed on the rows of a matrix:If the matrix is associated to a system of linear equations, then these operations do not change the solution set. Therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier.For each row in a matrix, if the row does not consist of only zeros, then the leftmost nonzero entry is called the leading coefficient (or pivot) of that row. So if two leading coefficients are in the same column, then a row operation of type 3 could be used to make one of those coefficients zero. Then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. If this is the case, then matrix is said to be in row echelon form. So the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. The word "echelon" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom.For example, the following matrix is in row echelon form, and its leading coefficients are shown in red.It is in echelon form because the zero row is at the bottom, and the leading coefficient of the second row (in the third column), is to the right of the leading coefficient of the first row (in the second column).A matrix is said to be in reduced row echelon form if furthermore all of the leading coefficients are equal to 1 (which can be achieved by using the elementary row operation of type 2), and in every column containing a leading coefficient, all of the other entries in that column are zero (which can be achieved by using elementary row operations of type 3).Suppose the goal is to find and describe the set of solutions to the following system of linear equations:The table below is the row reduction process applied simultaneously to the system of equations, and its associated augmented matrix. In practice, one does not usually deal with the systems in terms of equations but instead makes use of the augmented matrix, which is more suitable for computer manipulations. The row reduction procedure may be summarized as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.The second column describes which row operations have just been performed. So for the first step, the x is eliminated from L2 by adding 3/2L1 to L2. Next x is eliminated from L3 by adding L1 to L3. These row operations are labelled in the table asOnce y is also eliminated from the third row, the result is a system of linear equations in triangular form, and so the first part of the algorithm is complete. From a computational point of view, it is faster to solve the variables in reverse order, a process known as back-substitution. One sees the solution is z = −1, y = 3, and x = 2. So there is a unique solution to the original system of equations.Instead of stopping once the matrix is in echelon form, one could continue until the matrix is in reduced row echelon form, as it is done in the table. The process of row reducing until the matrix is reduced is sometimes referred to as Gauss–Jordan elimination, to distinguish it from stopping after reaching echelon form.The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE.[1][2] It was commented on by Liu Hui in the 3rd century.The method in Europe stems from the notes of Isaac Newton.[3][4] In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied.  Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton had left academic life.  The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks by the end of the 18th century.  Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems.[5]  The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.[6]Some authors use the term Gaussian elimination to refer only to the procedure until the matrix is in echelon form, and use the term Gauss–Jordan elimination to refer to the procedure which ends in reduced echelon form. The name is used because it is a variation of Gaussian elimination as described by Wilhelm Jordan in 1888. However, the method also appears in an article by Clasen published in the same year. Jordan and Clasen probably discovered Gauss–Jordan elimination independently.[7]The historically first application of the row reduction method is for solving systems of linear equations. Here are some other important applications of the algorithm.To explain how Gaussian elimination allows the computation of the determinant of a square matrix, we have to recall how the elementary row operations change the determinant:If Gaussian elimination applied to a square matrix A produces a row echelon matrix B, let d be the product of the scalars by which the determinant has been multiplied, using the above rules. Then the determinant of A is the quotient by d of the product of the elements of the diagonal of B:Computationally, for an n × n matrix, this method needs only O(n3) arithmetic operations, while solving by elementary methods requires O(2n) or O(n!) operations. Even on the fastest computers, the elementary methods are impractical for n above 20.A variant of Gaussian elimination called Gauss–Jordan elimination can be used for finding the inverse of a matrix, if it exists.  If A is an n × n square matrix, then one can use row reduction to compute its inverse matrix, if it exists. First, the n × n identity matrix is augmented to the right of A, forming an n × 2n block matrix [A | I]. Now through application of elementary row operations, find the reduced echelon form of this n × 2n matrix. The matrix A is invertible if and only if the left block can be reduced to the identity matrix I; in this case the right block of the final matrix is A−1. If the algorithm is unable to reduce the left block to I, then A is not invertible.For example, consider the following matrixTo find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 × 6 matrix:By performing row operations, one can check that the reduced row echelon form of this augmented matrix is:One can think of each row operation as the left product by an elementary matrix. Denoting by B the product of these elementary matrices, we showed, on the left, that BA = I, and therefore, B = A−1. On the right, we kept a record of BI = B, which we know is the inverse desired. This procedure for finding the inverse works for square matrices of any size.The Gaussian elimination algorithm can be applied to any m × n matrix A. In this way, for example, some 6 × 9 matrices can be transformed to a matrix that has a row echelon form likewhere the stars are arbitrary entries and a, b, c, d, e are nonzero entries. This echelon matrix T contains a wealth of information about A: the rank of A is 5 since there are 5 nonzero rows in T; the vector space spanned by the columns of A has a basis consisting of the first, third, fourth, seventh and ninth column of A (the columns of a, b, c, d, e in T), and the stars show how the other columns of A can be written as linear combinations of the basis columns. This is a consequence of the distributivity of the dot product in the expression of a linear map as a matrix.All of this applies also to the reduced row echelon form, which is a particular row echelon format.The number of arithmetic operations required to perform row reduction is one way of measuring the algorithm's computational efficiency. For example, to solve a system of n equations for n unknowns by performing row operations on the matrix until it is in echelon form, and then solving for each unknown in reverse order, requires n(n + 1)/2 divisions, 2n3 + 3n2 − 5n/6 multiplications, and 2n3 + 3n2 − 5n/6 subtractions,[8] for a total of approximately 2n3/3 operations. Thus it has arithmetic complexity of O(n3); see Big O notation. This arithmetic complexity is a good measure of the time needed for the whole computation when the time for each arithmetic operation is approximately constant. This is the case when the coefficients are represented by floating-point numbers or when they belong to a finite field. If the coefficients are integers or rational numbers exactly represented, the intermediate entries can grow exponentially large, so the bit complexity is exponential.[9]However, there is a variant of Gaussian elimination, called Bareiss algorithm that avoids this exponential growth of the intermediate entries, and, with the same arithmetic complexity of O(n3), has a bit complexity of O(n5).This algorithm can be used on a computer for systems with thousands of equations and unknowns. However, the cost becomes prohibitive for systems with millions of equations. These large systems are generally solved using iterative methods. Specific methods exist for systems whose coefficients follow a regular pattern (see system of linear equations).To put an n × n matrix into reduced echelon form by row operations, one needs n3 arithmetic operations, which is approximately 50% more computation steps.[10]One possible problem is numerical instability, caused by the possibility of dividing by very small numbers. If, for example, the leading coefficient of one of the rows is very close to zero, then to row-reduce the matrix one would need to divide by that number so the leading coefficient is 1. This means that any error existed for the number which was close to zero would be amplified. Gaussian elimination is numerically stable for diagonally dominant or positive-definite matrices. For general matrices, Gaussian elimination is usually considered to be stable, when using partial pivoting, even though there are examples of stable matrices for which it is unstable.[11]The Gaussian elimination can be performed over any field, not just the real numbers.Gaussian elimination does not generalize in any way to higher-order tensors (matrices are array representations of order-2 tensors); even computing the rank of a tensor of order greater than 2 is NP-hard.[12]As explained above, Gaussian elimination transforms a given m × n matrix A into a matrix in row-echelon form.In the following pseudocode, A[i, j] denotes the entry of the matrix A in row i and column j with the indices starting from 1. The transformation is performed in place, meaning that the original matrix is lost for being eventually replaced by its row-echelon form.This algorithm differs slightly from the one discussed earlier, by choosing a pivot with largest absolute value. Such a partial pivoting may be required if, at the pivot place, the entry of the matrix is zero. In any case, choosing the largest possible absolute value of the pivot improves the numerical stability of the algorithm, when floating point is used for representing numbers.Upon completion of this procedure the matrix will be in row echelon form and the corresponding system may be solved by back substitution.
Determinant
In linear algebra, the determinant is a value that can be computed from the elements of a square matrix.  The determinant of a matrix A is denoted det(A), det A, or |A|. Geometrically, it can be viewed as the scaling factor of the linear transformation described by the matrix.In the case of a 2 × 2 matrix the determinant may be defined as:Similarly, for a 3 × 3 matrix A, its determinant is:Each determinant of a 2 × 2 matrix in this equation is called a "minor" of the matrix A.  This procedure can be extended to give a recursive definition for the determinant of an n × n matrix, the minor expansion formula.Determinants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although other methods of solution are much more computationally efficient. In linear algebra, a matrix (with entries in a field) is invertible if and only if its determinant is non-zero, and correspondingly the matrix is singular if and only if its determinant is zero. This leads to the use of determinants in defining the characteristic polynomial of a matrix, whose roots are the eigenvalues. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. This leads to the use of determinants in calculus, the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants appear frequently in algebraic identities such as the Vandermonde identity.Determinants possess many algebraic properties, including that the determinant of a product of matrices is equal to the product of determinants. Special types of matrices have special determinants; for example, the determinant of an orthogonal matrix is always plus or minus one, and the determinant of a complex Hermitian matrix is always real.There are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns.  Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted.  For example, here is the result for a 4 × 4 matrix:Another way to define the determinant is expressed in terms of the columns of the matrix.  If we write an n × n matrix A in terms of its column vectorswhere b and c are scalars, v is any vector of size n and I is the identity matrix of size n.  These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar.  These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.[2]Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n × n matrix contributes n! terms), so it will first be given explicitly for the case of 2 × 2 matrices and 3 × 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.Assume A is a square matrix with n rows and n columns, so that it can be written asThe entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.The determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:The Leibniz formula for the determinant of a 2 × 2 matrix isIf the matrix entries are real numbers, the matrix A can be used to represent two linear maps:  one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A.  In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping.  The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram.The absolute value of ad − bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).To show that ad − bc is the signed area, one may consider a matrix containing two vectors a = (a, b) and b = (c, d) representing the parallelogram's sides. The signed area can be expressed as |a||b|sinθ for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. a⊥ = (−b, a), such that |a⊥||b|cosθ' , which can be determined by the pattern of the scalar product to be equal to ad − bc:Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.The object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted by (a, b) ∧ (c, d)) is the signed area, which is also the determinant ad − bc.[3]The Laplace formula for the determinant of a 3 × 3 matrix isthis can be expanded out to givewhich is the Leibniz formula for the determinant of a 3 × 3 matrix.The rule of Sarrus is a mnemonic for the 3 × 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 × 3 matrix does not carry over into higher dimensions.The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.The Leibniz formula for the determinant of an n × n matrix A isHere the sum is computed over all permutations σ of the set {1, 2, …, n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ1 = 2, σ2 = 3, and σ3 = 1.  The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.is notation for the product of the entries at positions (i, σi), where i ranges from 1 to n:For example, the determinant of a 3 × 3 matrix A (n = 3) isor using two epsilon symbols aswhere now each ir and each jr should be summed over 1, …, n.The determinant has many properties. Some basic properties of determinants areThis can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.A number of additional properties relate to the effects on the determinant of changing particular rows or columns:Properties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from n × n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for n ≥ 2,[6] so there is no good definition of the determinant in this setting.Property 2 above implies that properties for columns have their counterparts in terms of rows:Property 5 says that the determinant on n × n matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 7; this is essentially the method of Gaussian elimination.For example, the determinant ofcan be computed using the following matrices:Here, B is obtained from A by adding −1/2×the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = −det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (−2) · 2 · 4.5 = −18. Therefore, det(A) = −det(D) = +18.The following identity holds for a Schur complement of a square matrix:The Schur complement arises as the result of performing a block Gaussian elimination by multiplying the matrix M from the right with a block lower triangular matrixHere Ip denotes a p×p identity matrix. After multiplication with the matrix L the Schur complement appears in the upper p×p block. The product matrix isThat is, we have effected a Gaussian decompositionThe first and last matrices on the RHS have determinant unity,  so we haveThis is Schur's determinant identity.The determinant of a matrix product of square matrices equals the product of their determinants:Thus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) → K that maps M ↦ det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized  to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given byIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n−1) × (n−1)-matrix that results from A by removing the i-th row and the j-th column. The expression (−1)i + jMi,j is known as a cofactor. The determinant of A is given byCalculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the 3 × 3 matrixalong the second column (j = 2 and the sum runs over i) is given by,However, Laplace expansion is efficient for small matrices only.The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,In terms of the adjugate matrix, Laplace's expansion can be written as[7]Sylvester's determinant theorem states that for A, an m × n matrix, and B, an n × m matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):where Im and In are the m × m and n × n identity matrices, respectively.From this general result several consequences follow.The product of all non-zero eigenvalues is referred to as pseudo-determinant.Conversely, determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equationwhere I is the identity matrix of the same dimension as A and x is a (scalar) number which solves the equation (there are no more than n solutions, where n is the dimension of A).A Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatricesbeing positive, for all k between 1 and n.The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,or, for real matrices A,Here exp(A) denotes the matrix exponential of A, because every eigenvalue λ of A corresponds to the eigenvalue exp(λ) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfyingthe determinant of A is given byFor example, for n = 2, n = 3, and n = 4, respectively,cf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic n, detA = (−)nc0 the signed constant term of the characteristic polynomial, determined recursively fromIn the general case, this may also be obtained from[9]where the sum is taken over the set of all integers kl ≥ 0 satisfying the equationThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = −(l – 1)! tr(Al) asThis formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1, i2, …, ir) and J = (j1, j2, …, jr). The product and trace of such matrices are defined in a natural way asAn important arbitrary  dimension n  identity can be obtained from the  Mercator series expansion of the logarithm when the expansion converges.  If every eigenvalue of A is less than 1 in absolute value,where I is the identity matrix.  More generally, if is expanded as a formal power series in s then all coefficients of sm for m > n are zero and the remaining polynomial is det(I + sA).For a positive definite matrix A, the trace operator gives the following tight lower and upper bounds on the log determinantwith equality if and only if  A=I. This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.Also,These inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.For a matrix equationthe solution is given by Cramer's rule:where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.It has recently been shown that Cramer's rule can be implemented in O(n3) time,[10] which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.Suppose A, B, C, and D are matrices of dimension n × n, n × m, m × n, and m × m, respectively. ThenThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)When A is invertible, one hasas can be seen by employing the decompositionWhen the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 × 2 matrix holds:[12]Generally, if all pairs of n × n matrices of the np × np block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix  obtained by computing the determinant of the block matrix considering its entries as the entries of a p × p matrix.[13] As the previous formula shows, for p = 2, this criterion is sufficient, but not necessary.When A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)When D is a 1×1 matrix, B is a column vector, and C is a row vector thenIt can be seen, e.g. using the Leibniz formula, that the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn × n to R, and so it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:[14]where adj(A) denotes the adjugate of A. In particular, if A is invertible, we haveExpressed in terms of the entries of A, these areYet another equivalent formulation isThis identity is used in describing the tangent space of certain matrix Lie groups.The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X−1BX. Indeed, repeatedly applying the above identities yieldsThe determinant is therefore also called a similarity invariant. The determinant of a linear transformationfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.The determinant of a linear transformation A : V → V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear mapAs ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to sayThis definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product v1 ∧ v2 ∧ v3 ∧ … ∧ vn to v2 ∧ v1 ∧ v3 ∧ … ∧ vn, say, also changes its sign.For this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with k < n.The vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one.  To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define (T′w)(x1, …, xn) = w(Tx1, …, Txn).  As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple.  We call this scalar the determinant of T.The determinant can also be characterized as the unique functionfrom the set of all n × n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that isfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, D(A) = 0. Finally, D(In) = 1, where In is the identity matrix.This fact also implies that every other n-linear alternating function F: Mn(K) → K satisfiesThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.The determinant defines a mappingbetween the group of invertible n × n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R → S, there is a map GLn(f): GLn(R) → GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identityholds. In other words, the following diagram commutes:For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant).[15] Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formulaAnother infinite-dimensional notion of determinant is the functional determinant.For operators in a finite factor, one may define a positive real-valued determinant called the Fuglede−Kadison determinant using the canonical trace. In fact, corresponding to every tracial state on a von Neumann algebra there is a notion of Fuglede−Kadison determinant.For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear form[clarify] with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.[16]The permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.[17] Computational geometry, however, does frequently use calculations related to determinants.[18]Naive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n × n matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)The LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies toIf the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of A + uvT, where u and v are column vectors.Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz[19] is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.If two matrices of order n can be multiplied in time M(n), where M(n) ≥ na for some a > 2, then the determinant can be computed in time O(M(n)).[20] This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.Charles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation.  Unfortunately this interesting method does not always work in its original form.Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long.[21] The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.[22]Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). In Europe, 2 × 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.[23][24][25][26]In Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by Bézout (1764).It was Vandermonde (1771) who first recognized determinants as independent functions.[23] Laplace (1772)[27][28] gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,[29][30] summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.[23][31] With him begins the theory in its generality.The next important figure was Jacobi[24] (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.[32][33]The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 × 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be n − 1 times differentiable), the Wronskian is defined to beIt is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 × 2 or 3 × 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn → Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn → Rm is represented by the m × n matrix A, then the n-dimensional volume of f(S) is given by:By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)·|det(a − b, b − c, c − d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.For a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. Forthe Jacobian matrix is the n × n matrix whose entries are given byIts determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function φ: Rn → Rm is given byThe Jacobian also occurs in the inverse function theorem.The third order Vandermonde determinant isIn general, the nth-order Vandermonde determinant is[34]where the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.Second orderThird orderwhere ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is[34]where ωj is an nth root of 1.
Overlap–save method
    (Eq.1)where h[m]=0 for m outside the region [1, M].The concept is to compute short segments of y[n] of an arbitrary length L, and concatenate the segments together.  Consider a segment that begins at n = kL + M, for any integer k, and define:Then, for kL + M  ≤  n  ≤  kL + L + M − 1, and equivalently M  ≤  n − kL  ≤  L + M − 1, we can write:The task is thereby reduced to computing yk[n], for M  ≤  n  ≤  L + M − 1.  The process described above is illustrated in the accompanying figure.Now note that if we periodically extend xk[n] with period N  ≥  L + M − 1, according to:The advantage is that the circular convolution can be computed very efficiently as follows, according to the circular convolution theorem:where:When the DFT and its inverse is implemented by the FFT algorithm, the pseudocode above requires about N log2(N) + N complex multiplications for the FFT, product of arrays, and IFFT.[note 1]  Each iteration produces N-M+1 output samples, so the number of complex multiplications per output sample is about:    (Eq.2)For example, when M=201 and N=1024, Eq.2 equals 13.67, whereas direct evaluation of Eq.1 would require up to 201 complex multiplications per output sample, the worst case being when both x and h are complex-valued.  Also note that for any given M, Eq.2 has a minimum with respect to N.  It diverges for both small and large block sizes.Overlap–discard[1] and Overlap–scrap[2] are less commonly used labels for the same method described here.  However, these labels are actually better (than overlap–save) to distinguish from overlap–add, because both methods "save", but only one discards.  "Save" merely refers to the fact that M − 1 input (or output) samples from segment k are needed to process segment k + 1.The overlap–save algorithm may be extended to include other common operations of a system:[note 2][3]
Frame (linear algebra)
In linear algebra, a frame of an inner product space is a generalization of a basis of a vector space to sets that may be linearly dependent. In the terminology of signal processing, a frame provides a redundant, stable way of representing a signal.[1] Frames are used in error detection and correction and the design and analysis of filter banks and more generally in applied mathematics, computer science, and engineering.[2].A set of vectors that satisfies the frame condition is a frame for the vector space.[3]The numbers A and B are called the lower and upper frame bounds, respectively.[3] The frame bounds are not unique because numbers less than A and greater than B are also valid frame bounds. The optimal lower bound is the supremum of all lower bounds and the optimal upper bound is the infimum of all upper bounds.A frame is called overcomplete (or redundant) if it is not a basis for the vector space.By using this definition we may rewrite the frame condition asBecause of the various mathematical components surrounding frames, frame theory has roots in harmonic and functional analysis, operator theory, linear algebra, and matrix theory.[6]The Fourier transform has been used for over a century as a way of decomposing and expanding signals. However, the Fourier transform masks key information regarding the moment of emission and the duration of a signal.  In 1946, Dennis Gabor was able to solve this using a technique that simultaneously reduced noise, provided resiliency, and created quantization while encapsulating important signal characteristics.[1] This discovery marked the first concerted effort towards frame theory.The frame condition was first described by Richard Duffin and Albert Charles Schaeffer in a 1952 article on nonharmonic Fourier series as a way of computing the coefficients in a linear combination of the vectors of a linearly dependent spanning set (in their terminology, a "Hilbert space frame").[7] In the 1980s, Stéphane Mallat, Ingrid Daubechies, and Yves Meyer used frames to analyze wavelets. Today frames are associated with wavelets, signal and image processing, and data compression.A frame satisfies a generalization of Parseval's identity, namely the frame condition, while still maintaining norm equivalence between a signal and its sequence of coefficients.In signal processing, each vector is interpreted as a signal. In this interpretation, a vector expressed as a linear combination of the frame vectors is a redundant signal. Using a frame, it is possible to create a simpler, more sparse representation of a signal as compared with a family of elementary signals (that is, representing a signal strictly with a set of linearly independent vectors may not always be the most compact form).[8] Frames, therefore, provide robustness. Because they provide a way of producing the same vector within a space, signals can be encoded in various ways. This facilitates fault tolerance and resilience to a loss of signal.  Finally, redundancy can be used to mitigate noise, which is relevant to the restoration, enhancement, and reconstruction of signals.In signal processing, it is common to assume the vector space is a Hilbert space.A frame is a tight frame if A = B; in other words, the frame satisfies a generalized version of Parseval's identity. For example, the union of k disjoint orthonormal bases of a vector space is a tight frame with A = B = k. A tight frame is a Parseval frame (sometimes called a normalized frame) if A = B = 1. Each orthonormal basis is a Parseval frame, but the converse is not always true.A frame is an exact frame if no proper subset of the frame spans the inner product space. Each basis for an inner product space is an exact frame for the space (so a basis is a special case of a frame).A Bessel Sequence is a set of vectors that satisfies only the upper bound of the frame condition.and we see that Continuous Frames are indeed the natural generalization of the frames mentioned above.Just like in the discrete case we can define the Analysis, Synthesis, and Frame operators when dealing with continuous frames.It is defined as follows:The adjoint operator of the Continuous Analysis Operator is the Continuous Synthesis Operator which is the map:which, when substituted in the frame condition inequality, yieldsThuswhich proves thatAlternatively, we can letwhich shows that
Cauchy–Schwarz inequality
In mathematics, the Cauchy–Schwarz inequality, also known as the Cauchy–Bunyakovsky–Schwarz inequality, is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics.[1]The inequality for sums was published by Augustin-Louis Cauchy (1821), while the corresponding inequality for integrals was first proved byViktor Bunyakovsky (1859). The modern proof of the integral inequality was given by Hermann Amandus Schwarz (1888).[1]or LetThen, by linearity of the inner product in its first argument, one haswhich givesThis establishes the theorem.Titu's lemma (named after Titu Andreescu, also known as T2 Lemma, Engel's form, or Sedrakyan's inequality) states that for positive reals, we havewhich yields the Cauchy–Schwarz inequality.For the inner product space of square-integrable complex-valued functions, one hasA generalization of this is the Hölder inequality.The triangle inequality for the standard norm is often shown as a consequence of the Cauchy–Schwarz inequality, as follows: given vectors x and y:Taking square roots gives the triangle inequality.The Cauchy–Schwarz inequality is used to prove that the inner product is a continuous function with respect to the topology induced by the inner product itself.[8][9]The Cauchy–Schwarz inequality allows one to extend the notion of "angle between two vectors" to any real inner-product space by defining:[10][11]The Cauchy–Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [−1, 1] and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space. It can also be used to define an angle in complex inner-product spaces, by taking the absolute value or the real part of the right-hand side,[12][13] as is done when extracting a metric from quantum fidelity.Let X, Y be random variables, then the covariance inequality[14][15] is given byAfter defining an inner product on the set of random variables using the expectation of their product,then the Cauchy–Schwarz inequality becomesVarious generalizations of the Cauchy–Schwarz inequality exist in the context of operator theory, e.g. for operator-convex functions and operator algebras, where the domain and/or range are replaced by a C*-algebra or W*-algebra.which extends verbatim to positive functionals on C*-algebras:The next two theorems are further examples in operator algebra.Another generalization is a refinement obtained by interpolating between both sides the Cauchy-Schwarz inequality:  It can be easily proven by Hölder's inequality.[23] There are also non commutative versions for operators and tensor products of matrices.[24]
Dual space
In mathematics, any vector space V has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants.The dual space as defined above is defined for all vector spaces, and to avoid ambiguity may also be called the algebraic dual space.  When defined for a topological vector space, there is a subspace of the dual space, corresponding to continuous linear functionals, called the continuous dual space.Dual vector spaces find application in many branches of mathematics that use vector spaces, such as in tensor analysis with finite-dimensional vector spaces. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.for all φ and ψ ∈ V∗, x ∈ V, and a ∈ F. Elements of the algebraic dual space V∗ are sometimes called covectors or one-forms.The pairing of a functional φ in the dual space V∗ and an element x of V is sometimes denoted by a bracket: φ(x) = [x,φ] [2]or φ(x) = ⟨φ,x⟩.[3] This pairing defines a nondegenerate bilinear mapping[4] ⟨·,·⟩ : V∗ × V → F called the natural pairing.If V is finite-dimensional, then V∗ has the same dimension as V. Given a basis {e1, ..., en}  in V, it is possible to construct a specific basis in V∗, called the dual basis. This dual basis is a set {e1, ..., en}  of linear functionals on V, defined by the relationfor any choice of coefficients ci ∈ F. In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equationsIn particular, if we interpret Rn as the space of columns of n real numbers, its dual space is typically written as the space of rows of n real numbers. Such a row acts on Rn as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every n-vector x into a real number y. Then, seeing this functional as a matrix M, and x, y as a n × 1 matrix and a 1 × 1 matrix (trivially, a real number) respectively, if we have Mx = y, then, by dimension reasons, M must be a 1 × n matrix, i.e., M must be a row vector.If V consists of the space of geometrical vectors in the plane, then the level curves of an element of V∗ form a family of parallel lines in V, because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of V∗ can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if V is a vector space of any dimension, then the level sets of a linear functional in V∗ are parallel hyperplanes in V, and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.[5]If V is not finite-dimensional but has a basis[6] eα indexed by an infinite set A, then the same construction as in the finite-dimensional case yields linearly independent elements eα (α ∈ A) of the dual space, but they will not form a basis.Consider, for instance, the space R∞, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for i ∈ N, ei is the sequence consisting of all zeroes except in the ith position, which is 1. The dual space of R∞ is (isomorphic to) RN, the space of all sequences of real numbers: such a sequence (an) is applied to an element (xn) of R∞ to give the number ∑anxn, which is a finite sum because there are only finitely many nonzero xn. The dimension of R∞ is countably infinite, whereas RN does not have a countable basis.This observation generalizes to any[6] infinite-dimensional vector space V over any field F: a choice of basis {eα : α ∈ A}  identifies V with the space (FA)0 of functions f : A → F such that fα = f(α) is nonzero for only finitely many α ∈ A, where such a function f is identified with the vectorin V (the sum is finite by the assumption on f, and any v ∈ V may be written in this way by the definition of the basis).The dual space of V may then be identified with the space FA of all functions from A to F: a linear functional T on V is uniquely determined by the values θα = T(eα) it takes on the basis of V, and any function θ : A → F (with θ(α) = θα) defines a linear functional T on V byAgain the sum is finite because fα is nonzero for only finitely many α.Note that (FA)0 may be identified (essentially by definition) with the direct sumof infinitely many copies of F (viewed as a 1-dimensional vector space over itself) indexed by A, i.e., there are linear isomorphismsOn the other hand, FA is (again by definition), the direct product of infinitely many copies of F indexed by A, and so the identificationis a special case of a general result relating direct sums (of modules) to direct products.Thus if the basis is infinite, then the algebraic dual space is always of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.If V is finite-dimensional, then V is isomorphic to V∗. But there is in general no natural isomorphism between these two spaces.[7]  Any bilinear form ⟨·,·⟩ on V gives a mapping of V into its dual space viawhere the right hand side is defined as the functional on V taking each w ∈ V to ⟨v,w⟩.  In other words, the bilinear form determines a linear mappingdefined byThus there is a one-to-one correspondence between isomorphisms of V to subspaces of (resp., all of) V∗ and nondegenerate bilinear forms on V.If the vector space V is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms.  In that case, a given sesquilinear form ⟨·,·⟩ determines an isomorphism of V with the complex conjugate of the dual spaceThe conjugate space V∗ can be identified with the set of all additive complex-valued functionals f: V → C such thatIf f : V → W is a linear map, then the transpose (or dual) f∗ : W∗ → V∗ is defined byfor every φ ∈ W∗. The resulting functional f∗(φ) in V∗ is called the pullback of φ along f.The following identity holds for all φ ∈ W∗ and v ∈ V:where the bracket [·,·] on the left is the natural pairing of V with its dual space, and that on the right is the natural pairing of W with its dual.  This identity characterizes the transpose,[9] and is formally similar to the definition of the adjoint.The assignment f ↦ f∗ produces an injective linear map between the space of linear operators from V to W and the space of linear operators from W∗ to V∗; this homomorphism is an isomorphism if and only if W is finite-dimensional. If V = W then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that (fg)∗ = g∗f∗. In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over F to itself. Note that one can identify  (f∗)∗ with f using the natural injection into the double dual.If the linear map f is represented by the matrix A with respect to two bases of V and W, then f∗ is represented by the transpose matrix AT with respect to the dual bases of W∗ and V∗, hence the name. Alternatively, as f is represented by A acting on the left on column vectors, f∗ is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on Rn, which identifies the space of column vectors with the dual space of row vectors.Let S be a subset of V. The annihilator of S in V∗, denoted here S0, is the collection of linear functionals f ∈ V∗ such that [f, s] = 0 for all s ∈ S. That is, S0 consists of all linear functionals f : V → F such that the restriction to S vanishes: f|S = 0.The annihilator of a subset is itself a vector space. In particular, ∅0 = V∗ is all of V∗ (vacuously), whereas V0 = 0 is the zero subspace. Furthermore, the assignment of an annihilator to a subset of V reverses inclusions, so that if S ⊂ T ⊂ V, thenMoreover, if A and B are two subsets of V, thenand equality holds provided V is finite-dimensional. If Ai is any family of subsets of V indexed by i belonging to some index set I, thenIn particular if A and B are subspaces of V, it follows thatIf V is finite-dimensional, and W is a vector subspace, thenafter identifying W with its image in the second dual space under the double duality isomorphism V ≈ V∗∗.  Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.If W is a subspace of V then the quotient space V/W is a vector space in its own right, and so has a dual.  By the first isomorphism theorem, a functional f : V → F factors through V/W if and only if W is in the kernel of f.  There is thus an isomorphismAs a particular consequence, if V is a direct sum of two subspaces A and B, then V∗ is a direct sum of A0 and B0.form its local base.Here are the three most important special cases.Let 1 < p < ∞ be a real number and consider the Banach space ℓ p of all sequences a = (an) for whichis finite. Define the number q by 1/p + 1/q = 1.  Then the continuous dual of ℓ p is naturally identified with ℓ q: given an element φ ∈ (ℓ p)′, the corresponding element of ℓ q is the sequence (φ(en)) where en denotes the sequence whose n-th term is 1 and all others are zero. Conversely, given an element a = (an) ∈ ℓ q, the corresponding continuous linear functional φ on ℓ p is defined by φ(b) = ∑n anbn for all b = (bn) ∈ ℓ p (see Hölder's inequality).In a similar manner, the continuous dual of ℓ 1 is naturally identified with ℓ ∞ (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces c (consisting of all convergent sequences, with the supremum norm) and c0 (the sequences converging to zero) are both naturally identified with ℓ 1.By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra–ket notation used by physicists in the mathematical formulation of quantum mechanics.By the Riesz–Markov–Kakutani representation theorem, the continuous dual of certain spaces of continuous functions can be described using measures.If T : V → W is a continuous linear map between two topological vector spaces, then the (continuous) transpose  T′ : W′ → V′ is defined by the same formula as before:The resulting functional T′(φ) is in V′. The assignment T → T′ produces a linear map between the space of continuous linear maps from V to W and the space of linear maps from W′ to V′.  When T and U are composable continuous linear maps, thenWhen V and W are normed spaces, the norm of the transpose in L(W′, V′) is equal to that of T in L(V, W). Several properties of transposition depend upon the Hahn–Banach theorem. For example, the bounded linear map T has dense range if and only if the transpose T′ is injective.When T is a compact linear map between two Banach spaces V and W, then the transpose T′ is compact. This can be proved using the Arzelà–Ascoli theorem.When V is a Hilbert space, there is an antilinear isomorphism iV from V onto its continuous dual V′. For every bounded linear map T on V, the transpose and the adjoint operators are linked byWhen T is a continuous linear map between two topological vector spaces V and W, then the transpose T′ is continuous when W′ and V′ are equipped with"compatible" topologies: for example when, for X = V and X = W, both duals X′ have the strong topology β(X′, X) of uniform convergence on bounded sets of X, or both have the weak-∗ topology σ(X′, X) of pointwise convergence on X.  The transpose T′ is continuous from β(W′, W) to β(V′, V), or from σ(W′, W) to σ(V′, V).Assume that W is a closed linear subspace of a normed space V, and consider the annihilator of W in V′,Then, the dual of the quotient V / W  can be identified with W⊥, and the dual of W can be identified with the quotient V′ / W⊥.[14] Indeed, let P denote the canonical surjection from V onto the quotient V / W ; then, the transpose P′ is an isometric isomorphism from (V / W )′ into V′, with range equal to W⊥. If j denotes the injection map from W into V, then the kernel of the transpose j′ is the annihilator of W:and it follows from the Hahn–Banach theorem that j′ induces an isometric isomorphismV′ / W⊥ → W′.If the dual of a normed space V is separable, then so is the space V itself. The converse is not true: for example the space ℓ 1 is separable, but its dual ℓ ∞ is not.The topology of V and the topology of real or complex numbers can be used to induce on V′ a dual space topology.In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator Ψ : V → V′′ from a normed space V into its continuous double dual V′′, defined byAs a consequence of the Hahn–Banach theorem, this map is in fact an isometry, meaning ‖ Ψ(x) ‖ = ‖ x ‖ for all x in V. Normed spaces for which the map Ψ is a bijection are called reflexive.When V is a topological vector space, one can still define Ψ(x) by the same formula, for every x ∈ V, however several difficulties arise. First, when V is not locally convex, the continuous dual may be equal to {0} and the map Ψ trivial.  However, if V is Hausdorff and locally convex, the map Ψ is injective from V to the algebraic dual V′∗ of the continuous dual, again as a consequence of the Hahn–Banach theorem.[15]Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual V′, so that the continuous double dual V′′ is not uniquely defined as a set. Saying that Ψ maps from V to V′′, or in other words, that Ψ(x) is continuous on V′ for every x ∈ V, is a reasonable minimal requirement on the topology of V′, namely that the evaluation mappingsbe continuous for the chosen topology on V′. Further, there is still a choice of a topology on V′′, and continuity of Ψ depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed  case.
Bra–ket notation
In quantum mechanics, bra–ket notation is a standard notation for describing quantum states. It can also be used to denote abstract vectors and linear functionals in mathematics. The notation uses angle brackets (the ⟨ and ⟩ symbols) and a vertical bar (the | symbol), to denote the scalar product of vectors or the action of a linear functional on a vector in a complex vector space.  The scalar product or action is written asThe right part is called the ket /kɛt/; it is a vector, typically represented as a column vector and written The left part is called the bra, /brɑː/; it is the Hermitian conjugate of the ket with the same label, typically represented as a row vector and is writtenA combination of bras, kets, and operators is interpreted using matrix multiplication. A bra and a ket with the same label are Hermitian conjugates of each other.Bra-ket notation was introduced in 1939 by Paul Dirac[1][2] and is also known as the Dirac notation.  Bra–ket notation is a notation for linear algebra, particularly focused on vectors, inner products, linear operators, Hermitian conjugation, and the dual space, for both finite-dimensional and infinite-dimensional complex vector spaces. It is specifically designed to ease the types of calculations that frequently come up in quantum mechanics.Its use in quantum mechanics is quite widespread. Many phenomena that are explained using quantum mechanics are usually explained using bra–ket notation.In simple cases, a ket |m⟩ can be described as a column vector, a bra with the same label ⟨m| is its conjugate transpose (which is a row vector), and writing bras, kets, and linear operators next to each other implies matrix multiplication.[4] However, kets may also exist in uncountably-infinite-dimensional vector spaces, such that they cannot be literally written as a column vector. Also, writing a column vector as a list of numbers requires picking a basis, whereas one can write "|m⟩" without committing to any particular basis. This is helpful because quantum mechanics calculations involve frequently switching between different bases (e.g. position basis, momentum basis, energy eigenbasis, etc.), so it is better to have the basis vectors (if any) written out explicitly.  In some situations involving two important basis vectors they will be referred to simply as "|-⟩" and "|+⟩".The standard mathematical notation for the inner product, preferred as well by some physicists, expresses exactly the same thing as the bra–ket notation,Bras and kets can also be configured in other ways, such as the outer productwhich can also be represented as a matrix multiplication (i.e., a column vector times a row vector equals a matrix).If the ket is an element of a vector space, the bra is technically an element of its dual space—see Riesz representation theorem.In mathematics, the term "vector" is used to refer generally to any element of any vector space. In physics, however, the term "vector" is much more specific: "Vector" refers almost exclusively to quantities like displacement or velocity, which have three components that relate directly to the three dimensions of the real world. Such vectors are typically denoted with over arrows (r→) or boldface (r).In quantum mechanics, a quantum state is typically represented as an element of an abstract complex vector space—for example the infinite-dimensional vector space of all possible wavefunctions (functions mapping each point of 3D space to a complex number). Since the term "vector" is already used for something else (see previous paragraph), it is very common to refer to these elements of abstract complex vector spaces as "kets", and to write them using ket notation.Ket notation, invented by Dirac, uses vertical bars and angular brackets: |A⟩. When this notation is used, these quantities are called "kets", and |A⟩ is read as "ket-A".[5] These kets can be manipulated using the usual rules of linear algebra, for example:Note how any symbols, letters, numbers, or even words—whatever serves as a convenient label—can be used as the label inside a ket. For example, the last line above involves infinitely many different kets, one for each real number x. In other words, the symbol "|A⟩" has a specific and universal mathematical meaning, while just the "A" by itself does not. For example, |1⟩ + |2⟩ might or might not be equal to |3⟩. Nevertheless, for convenience, there is usually some logical scheme behind the labels inside kets, such as the common practice of labeling energy eigenkets in quantum mechanics through a listing of their quantum numbers.An inner product is a generalization of the dot product. The inner product of two vectors is a scalar. In neutral notation (notation dedicated to the inner product only), this might be written (A, B), where A and B are elements of the abstract vector space, i.e. both are kets.Bra–ket notation uses a specific notation for inner products:Bra–ket notation splits this inner product (also called a "bracket") into two pieces, the "bra" and the "ket":where ⟨A| is called a bra, read as "bra-A", and |B⟩ is a ket as above.The purpose of "splitting" the inner product into a bra and a ket is that both the bra ⟨A| and the ket |B⟩ are meaningful on their own, and can be used in other contexts besides within an inner product. There are two main ways to think about the meanings of separate bras and kets. Accordingly, the interpretation of the expression ⟨A|B⟩ has a second interpretation, namely that of the action of a linear functional per below.For a finite-dimensional vector space, using a fixed orthonormal basis, the inner product can be written as a matrix multiplication of a row vector with a column vector:Based on this, the bras and kets can be defined as:and then it is understood that a bra next to a ket implies matrix multiplication.The conjugate transpose (also called Hermitian conjugate) of a bra is the corresponding ket and vice versa:because if one starts with the brathen performs a complex conjugation, and then a matrix transpose, one ends up with the ketA more abstract definition, which is equivalent but more easily generalized to infinite-dimensional spaces, is to say that bras are linear functionals on the space of kets, i.e. linear transformations that input a ket and output a complex number. The bra linear functionals are defined to be consistent with the inner product. Thus, if ⟨A| is the linear functional corresponding to |A⟩ under the Riesz representation theorem, theni.e. it produces the same complex number as the inner product does. The terminology for the right hand side is though not inner product, which always involves two kets. Confusing this is harmless, since the same number is produced in the end.In mathematics terminology, the vector space of bras is the dual space to the vector space of kets, and corresponding bras and kets are related by the Riesz representation theorem.Bra–ket notation can be used even if the vector space is not a Hilbert space.In quantum mechanics, it is common practice to write down kets which have infinite norm, i.e. non-normalizable wavefunctions. Examples include states whose wavefunctions are Dirac delta functions or infinite plane waves. These do not, technically, belong to the Hilbert space itself. However, the definition of "Hilbert space" can be broadened to accommodate these states (see the Gelfand–Naimark–Segal construction or rigged Hilbert spaces). The bra–ket notation continues to work in an analogous way in this broader context.Banach spaces are a different generalization of Hilbert spaces. In a Banach space B, the vectors may be notated by kets and the continuous linear functionals by bras. Over any vector space without topology, we may also notate the vectors by kets and the linear functionals by bras. In these more general contexts, the bracket does not have the meaning of an inner product, because the Riesz representation theorem does not apply.The mathematical structure of quantum mechanics is based in large part on linear algebra:Since virtually every calculation in quantum mechanics involves vectors and linear operators, it can involve, and often does involve, bra–ket notation. A few examples follow:Starting from any ket |Ψ⟩ in this Hilbert space,  one may define a complex scalar function of r, known as a wavefunction,On the left-hand side, Ψ(r) is a function mapping any point in space to a complex number; on the right-hand side, |Ψ⟩ = ∫ d3r Ψ(r) |r⟩ is a ket consisting of a superposition of kets with relative coefficients specified by that function.It is then customary to define linear operators acting on wavefunctions in terms of linear operators acting on kets, byFor instance, the momentum operator p has the following form,One occasionally encounters an expression such asthough this is something of an abuse of notation. The differential operator must be understood to be an abstract operator, acting on kets, that has the effect of differentiating wavefunctions once the expression is projected into the position basis,even though, in the momentum basis, the operator amounts to a mere multiplication operator (by iħp).In quantum mechanics the expression ⟨φ|ψ⟩ is typically interpreted as the probability amplitude for the state ψ to collapse into the state φ. Mathematically, this means the coefficient for the projection of ψ onto φ.  It is also described as the projection of state ψ onto state φ.A stationary spin-1/2 particle has a two-dimensional Hilbert space. One orthonormal basis is:where |↑z⟩ is the state with a definite value of the spin operator Sz equal to +1/2 and  |↓z⟩ is the state with a definite value of the spin operator Sz equal to −1/2.Since these are a basis, any quantum state of the particle can be expressed as a linear combination (i.e., quantum superposition) of these two states:where aψ and bψ are complex numbers.A different basis for the same Hilbert space is:defined in terms of Sx rather than Sz.Again, any state of the particle can be expressed as a linear combination of these two:In vector form, you might writedepending on which basis you are using. In other words, the "coordinates" of a vector depend on the basis used.There is a mathematical relationship between aψ, bψ, cψ and dψ; see change of basis.There are a few conventions and abuses of notation that are generally accepted by the physics community, but which might confuse the non-initiated.It is common to use the same symbol for labels and constants in the same equation. For example, α̂ |α⟩ = α |α⟩, where the symbol α is used simultaneously as the name of the operator α̂, its eigenvector |α⟩ and the associated eigenvalue α.Something similar occurs in component notation of vectors. While Ψ (uppercase) is traditionally associated with wavefunctions, ψ (lowercase) may be used to denote a label, a wave function or complex constant in the same context, usually differentiated only by a subscript.The main abuses are including operations inside the vector labels. This is done for a fast notation of scaling vectors. E.g. if the vector |α⟩ is scaled by √2, it might be denoted by |α/√2⟩, which makes no sense since α is a label, not a function or a number, so you can't perform operations on it.This is especially common when denoting vectors as tensor products, where part of the labels are moved outside the designed slot, e.g. |α⟩ = |α/√21⟩ ⊗ |α/√22⟩. Here part of the labeling that should state that all three vectors are different was moved outside the kets, as subscripts 1 and 2. And a further abuse occurs, since α is meant to refer to the norm of the first vector—which is a label denoting a value.A linear operator is a map that inputs a ket and outputs a ket. (In order to be called "linear", it is required to have certain properties.) In other words, if A is a linear operator and |ψ⟩ is a ket, then A|ψ⟩ is another ket.In an N-dimensional Hilbert space, |ψ⟩ can be written as an N × 1 column vector, and then A is an N × N matrix with complex entries. The ket A|ψ⟩ can be computed by normal matrix multiplication.Linear operators are ubiquitous in the theory of quantum mechanics. For example, observable physical quantities are represented by self-adjoint operators, such as energy or momentum, whereas transformative processes are represented by unitary linear operators such as rotation or the progression of time.Operators can also be viewed as acting on bras from the right hand side. Specifically, if A is a linear operator and ⟨φ| is a bra, then ⟨φ|A is another bra defined by the rule(in other words, a function composition). This expression is commonly written as (cf. energy inner product)In an N-dimensional Hilbert space, ⟨φ| can be written as a 1 × N row vector, and A (as in the previous section) is an N × N matrix. Then the bra ⟨φ|A can be computed by normal matrix multiplication.If the same state vector appears on both bra and ket side,then this expression gives the expectation value, or mean or average value, of the observable represented by operator A for the physical system in the state |ψ⟩.A convenient way to define linear operators on a Hilbert space H is given by the outer product: if ⟨ϕ| is a bra and |ψ⟩ is a ket, the outer productdenotes the rank-one operator with the rule For a finite-dimensional vector space, the outer product can be understood as simple matrix multiplication:The outer product is an N × N matrix, as expected for a linear operator.One of the uses of the outer product is to construct projection operators. Given a ket |ψ⟩ of norm 1, the orthogonal projection onto the subspace spanned by |ψ⟩ isJust as kets and bras can be transformed into each other (making |ψ⟩ into ⟨ψ|), the element from the dual space corresponding to A|ψ⟩ is ⟨ψ|A†,  where A† denotes the Hermitian conjugate (or adjoint) of the operator A. In other words,If A is expressed as an N × N matrix, then A† is its conjugate transpose.Self-adjoint operators, where A = A†, play an important role in quantum mechanics; for example, an observable is always described by a self-adjoint operator. If A is a self-adjoint operator, then ⟨ψ|A|ψ⟩ is always a real number (not complex). This implies that expectation values of observables are real.Bra–ket notation was designed to facilitate the formal manipulation of linear-algebraic expressions. Some of the properties that allow this manipulation are listed herein. In what follows, c1 and c2 denote arbitrary complex numbers, c* denotes the complex conjugate of c, A and B denote arbitrary linear operators, and these properties are to hold for any choice of bras and kets.Given any expression involving complex numbers, bras, kets, inner products, outer products, and/or linear operators (but not addition), written in bra–ket notation, the parenthetical groupings do not matter (i.e., the associative property holds). For example:and so forth. The expressions on the right (with no parentheses whatsoever) are allowed to be written unambiguously because of the equalities on the left. Note that the associative property does not hold for expressions that include nonlinear operators, such as the antilinear time reversal operator in physics.Bra–ket notation makes it particularly easy to compute the Hermitian conjugate (also called dagger, and denoted †) of expressions. The formal rules are:These rules are sufficient to formally write the Hermitian conjugate of any such expression; some examples are as follows:Two Hilbert spaces V and W may form a third space V ⊗ W by a tensor product. In quantum mechanics, this is used for describing composite systems. If a system is composed of two subsystems described in V and W respectively, then the Hilbert space of the entire system is the tensor product of the two spaces. (The exception to this is if the subsystems are actually identical particles. In that case, the situation is a little more complicated.)If |ψ⟩ is a ket in V and |φ⟩ is a ket in W, the direct product of the two kets is a ket in V ⊗ W. This is written in various notations:See quantum entanglement and the EPR paradox for applications of this product.Consider a complete orthonormal system (basis),for a Hilbert space H, with respect to the norm from an inner product ⟨·,·⟩. From basic functional analysis, it is  known that any ket |ψ⟩ can also be written aswith ⟨·|·⟩ the inner product on the Hilbert space.From the commutativity of kets with (complex) scalars, it follows thatmust be the identity operator, which sends each vector to itself. This, then,  can be inserted in any expression without affecting its value; for examplewhere, in the last identity, the Einstein summation convention has been used.In quantum mechanics, it often occurs that little or no information about the inner product ⟨ψ|φ⟩ of two arbitrary (state) kets is present, while it is still possible to say something about the expansion coefficients ⟨ψ|ei⟩ = ⟨ei|ψ⟩* and ⟨ei|φ⟩ of those vectors with respect to a specific (orthonormalized) basis. In this case, it is particularly useful to insert the unit operator into the bracket one time or more.For more information, see Resolution of the identity, Since ⟨x′|x⟩ = δ(x − x′), plane waves follow, ⟨x|p⟩ = eixp/ħ/√2πħ.[7]Typically, when all matrix elements of an operator such as are available,this resolution serves to reconstitute the full operator,The object physicists are considering when using bra–ket notation is a Hilbert space (a complete inner product space).Let H be a Hilbert space and h ∈ H a vector in H. What physicists would denote by |h⟩ is the vector itself. That is,Let H* be the dual space of H. This is the space of linear functionals on H. The isomorphism Φ : H → H* is defined by Φ(h) = φh, where for every g ∈ H we definewhere IP(·,·), (·,·), ⟨·,·⟩ and ⟨·|·⟩ are just different notations for expressing an inner product between two elements in a Hilbert space (or for the first three, in any inner product space). Notational confusion arises when identifying φh and g with ⟨h| and |g⟩ respectively. This is because of literal symbolic substitutions. Let φh = H = ⟨h| and let g = G = |g⟩. This givesOne ignores the parentheses and removes the double bars. Some properties of this notation are convenient since we are dealing with linear operators and composition acts like a ring multiplication.Moreover, mathematicians usually write the dual entity not at the first place, as the physicists do, but at the second one, and they usually use not an asterisk but an overline (which the physicists reserve for averages and the Dirac spinor adjoint) to denote complex conjugate numbers; i.e., for scalar products mathematicians usually writewhereas physicists would write for the same quantity
Identity matrix
In linear algebra, the identity matrix, or sometimes ambiguously called a unit matrix, of size n is the n × n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. (In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I.) Less frequently, some mathematics books use U or E to represent the identity matrix, meaning "unit matrix"[1] and the German word "Einheitsmatrix",[2] respectively.When A is m×n, it is a property of matrix multiplication thatIn particular, the identity matrix serves as the unit of the ring of all n×n matrices, and as the identity element of the general linear group GL(n) consisting of all invertible n×n matrices. (The identity matrix itself is invertible, being its own inverse.)Where n×n matrices are used to represent linear transformations from an n-dimensional vector space to itself, In represents the identity function, regardless of the basis.The ith column of an identity matrix is the unit vector ei.  It follows that the determinant of the identity matrix is 1 and the trace is n.Using the notation that is sometimes used to concisely describe diagonal matrices, we can write:It can also be written using the Kronecker delta notation:The identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.The identity matrix of a given size is the only idempotent matrix of that size having full rank.  That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are linearly independent.The principal square root of an identity matrix is itself, and this is its only positive definite square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.[3]
Generalized eigenvector
butHere are some examples to illustrate the concept of generalized eigenvectors.  Some of the details will be described later.This example is simple but clearly illustrates the point.  This type of matrix is used frequently in textbooks.[38][39][40]SupposeWriting out the values:This simplifies toNote thatThis example is more complex than Example 1.  Unfortunately, it is a little difficult to construct an interesting example of low order.[41]The matrix    (1)Thus, in general,    (2)Definition:  A set of n linearly independent generalized eigenvectors is a canonical basis if it is composed entirely of Jordan chains.Now defineThe matrixWe now define    (3)but    (4)ThenandandFind a matrix in Jordan normal form that is similar toandandwiththenConsider the problem of solving the system of linear ordinary differential equations    (5)where    (6)In this case, the general solution is given by    (7)where    (8)The solution of (7) is    (9)
Binary operation
In mathematics, a binary operation on a set is a calculation that combines two elements of the set (called operands) to produce another element of the set. More formally, a binary operation is an operation of arity of two whose two domains and one codomain are the same set.  Examples include the familiar elementary arithmetic operations of addition, subtraction, multiplication and division.  Other examples are readily found in different areas of mathematics, such as vector addition, matrix multiplication and conjugation in groups.More precisely, a binary operation on a set S is a map which sends elements of the Cartesian product S × S to S:[1][2][3]Because the result of performing the operation on a pair of elements of S is again an element of S, the operation is called a closed binary operation on S (or sometimes expressed as having the property of closure).[4]  If f is not a function, but is instead a partial function, it is called a partial binary operation.  For instance, division of real numbers is a partial binary operation, because one can't divide by zero: a/0 is not defined for any real a.  Note however that both in algebra and model theory the binary operations considered are defined on all of S × S.Sometimes, especially in computer science, the term is used for any binary function.Binary operations are the keystone of algebraic structures studied in abstract algebra: they are essential in the definitions of groups, monoids, semigroups, rings, and more.  Most generally, a magma is a set together with some binary operation defined on it.Typical examples of binary operations are the addition (+) and multiplication (×) of numbers and matrices as well as composition of functions on a single set.For instance,Many binary operations of interest in both algebra and formal logic are commutative, satisfying f(a, b) = f(b, a) for all elements a and b in S, or associative, satisfying f(f(a, b), c) = f(a, f(b, c)) for all a, b and c in S.  Many also have identity elements and inverse elements.The first three examples above are commutative and all of the above examples are associative.On the set of real numbers R, subtraction, that is, f(a, b) = a − b, is a binary operation which is not commutative since, in general, a − b ≠ b − a.  It is also not associative, since, in general, a − (b − c) ≠ (a − b) − c; for instance, 1 − (2 − 3) = 2 but (1 − 2) − 3 = −4.On the set of natural numbers N, the binary operation exponentiation, f(a,b) = ab, is not commutative since, in general, ab ≠ ba and is also not associative since f(f(a, b), c) ≠ f(a, f(b, c)).  For instance, with a = 2, b = 3 and c = 2, f(23,2) = f(8,2) = 82 = 64, but f(2,32) = f(2,9) = 29 = 512.  By changing the set N to the set of integers Z, this binary operation becomes a partial binary operation since it is now undefined when a = 0 and b is any negative integer.  For either set, this operation has a right identity (which is 1) since f(a, 1) = a for all a in the set, which is not an identity (two sided identity) since f(1, b) ≠ b in general.Division (/), a partial binary operation on the set of real or rational numbers, is not commutative or associative.  Tetration (↑↑), as a binary operation on the natural numbers, is not commutative or associative and has no identity element.Binary operations are often written using infix notation such as a ∗ b, a + b, a · b or (by juxtaposition with no symbol) ab rather than by functional notation of the form f(a, b).  Powers are usually also written without operator, but with the second argument as superscript.Binary operations sometimes use prefix or (probably more often) postfix notation, both of which dispense with parentheses.  They are also called, respectively, Polish notation and reverse Polish notation.A binary operation, ab, depends on the ordered pair (a, b) and so (ab)c (where the parentheses here mean first operate on the ordered pair (a, b) and then operate on the result of that using the ordered pair ((ab), c)) depends in general on the ordered pair ((a, b), c).  Thus, for the general, non-associative case, binary operations can be represented with binary trees.However:A binary operation f on a set S may be viewed as a ternary relation on S, that is, the set of triples (a, b, f(a,b)) in S × S × S for all a and b in S.An external binary operation is a binary function from K × S to S.  This differs from a binary operation in the strict sense in that K need not be S; its elements come from outside.An example of an external binary operation is scalar multiplication in linear algebra.  Here K is a field and S is a vector space over that field.An external binary operation may alternatively be viewed as an action; K is acting on S.Note that the dot product of two vectors is not a binary operation, external or otherwise, as it maps from S × S to K, where K is a field and S is a vector space over K.
Multiplicative inverse
In mathematics, a multiplicative inverse or reciprocal for a number x, denoted by 1/x or x−1, is a number which when multiplied by x yields the multiplicative identity, 1. The multiplicative inverse of a fraction a/b is b/a. For the multiplicative inverse of a real number, divide 1 by the number. For example, the reciprocal of 5 is one fifth (1/5 or 0.2), and the reciprocal of 0.25 is 1 divided by 0.25, or 4. The reciprocal function, the function f(x) that maps x to 1/x, is one of the simplest examples of a function which is its own inverse (an involution).The term reciprocal was in common use at least as far back as the third edition of Encyclopædia Britannica (1797) to describe two numbers whose product is 1; geometrical quantities in inverse proportion are described as reciprocall in a 1570 translation of Euclid's Elements.[1]In the phrase multiplicative inverse, the qualifier multiplicative is often omitted and then tacitly understood (in contrast to the additive inverse). Multiplicative inverses can be defined over many mathematical domains as well as numbers. In these cases it can happen that ab ≠ ba; then "inverse" typically implies that an element is both a left and right inverse.The notation f −1 is sometimes also used for the inverse function of the function f, which is not in general equal to the multiplicative inverse. For example, the multiplicative inverse 1/(sin x) = (sin x)−1 is the cosecant of x, and not the inverse sine of x denoted by sin−1 x or arcsin x. Only for linear maps are they strongly related (see below). The terminology difference reciprocal versus inverse is not sufficient to make this distinction, since many authors prefer the opposite naming convention, probably for historical reasons (for example in French, the inverse function is preferably called bijection réciproque).In the real numbers, zero does not have a reciprocal because no real number multiplied by 0 produces 1 (the product of any number with zero is zero). With the exception of zero, reciprocals of every real number are real, reciprocals of every rational number are rational, and reciprocals of every complex number are complex. The property that every element other than zero has a multiplicative inverse is part of the definition of a field, of which these are all examples. On the other hand, no integer other than 1 and −1 has an integer reciprocal, and so the integers are not a field.In modular arithmetic, the modular multiplicative inverse of a is also defined: it is the number x such that ax ≡ 1 (mod n).  This multiplicative inverse exists if and only if a and n are coprime.  For example, the inverse of 3 modulo 11 is 4 because 4 · 3 ≡ 1 (mod 11).  The extended Euclidean algorithm may be used to compute it.The sedenions are an algebra in which every nonzero element has a multiplicative inverse, but which nonetheless has divisors of zero, i.e. nonzero elements x, y such that xy = 0.A square matrix has an inverse if and only if its determinant has an inverse in the coefficient ring. The linear map that has the matrix A−1 with respect to some base is then the reciprocal function of the map having A as matrix in the same base. Thus, the two distinct notions of the inverse of a function are strongly related in this case, while they must be carefully distinguished in the general case (as noted above).The trigonometric functions are related by the reciprocal identity: the cotangent is the reciprocal of the tangent; the secant is the reciprocal of the cosine; the cosecant is the reciprocal of the sine.A ring in which every nonzero element has a multiplicative inverse is a division ring; likewise an algebra in which this holds is a division algebra.For a complex number in polar form z = r(cos φ + i sin φ), the reciprocal simply takes the reciprocal of the magnitude and the negative of the angle:In real calculus, the derivative of 1/x = x−1 is given by the power rule with the power −1:The power rule for integrals (Cavalieri's quadrature formula) cannot be used to compute the integral of 1/x, because doing so would result in division by 0:Instead the integral is given by:The reciprocal may be computed by hand with the use of long division.This continues until the desired precision is reached. For example, suppose we wish to compute 1/17 ≈ 0.0588 with 3 digits of precision. Taking x0 = 0.1, the following sequence is produced:A typical initial guess can be found by rounding b to a nearby power of 2, then using bit shifts to compute its reciprocal.In constructive mathematics, for a real number x to have a reciprocal, it is not sufficient that x ≠ 0. There must instead be given a rational number r such that 0 < r < |x|. In terms of the approximation algorithm described above, this is needed to prove that the change in y will eventually become arbitrarily small.This iteration can also be generalised to a wider sort of inverses, e.g. matrix inverses.If the multiplication is associative, an element x with a multiplicative inverse cannot be a zero divisor (x is a zero divisor if some nonzero y, xy = 0). To see this, it is sufficient to multiply the equation xy = 0 by the inverse of x (on the left), and then simplify using associativity. In the absence of associativity, the sedenions provide a counterexample.The converse does not hold: an element which is not a zero divisor is not guaranteed to have a multiplicative inverse.Within Z, all integers except −1, 0, 1 provide examples; they are not zero divisors nor do they have inverses in Z.If the ring or algebra is finite, however, then all elements a which are not zero divisors do have a (left and right) inverse. For, first observe that the map f(x) = ax must be injective: f(x) = f(y) implies x = y:Distinct elements map to distinct elements, so the image consists of the same finite number of elements, and the map is necessarily surjective. Specifically, ƒ (namely multiplication by a) must map some element x to 1, ax = 1, so that x is an inverse for a.The expansion of the reciprocal 1/q in any base can also act [3] as a source of pseudo-random numbers, if q is a "suitable" safe prime, a prime of the form 2p + 1 where p is also a prime. A sequence of pseudo-random numbers of length q − 1 will be produced by the expansion.
Set (mathematics)
In mathematics, a set is a collection of distinct objects, considered as an object in its own right. For example, the numbers 2, 4, and 6 are distinct objects when considered separately, but when they are considered collectively they form a single set of size three, written {2,4,6}. The concept of a set is one of the most fundamental in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived. In mathematics education, elementary topics from set theory such as Venn diagrams are taught at a young age, while more advanced concepts are taught as part of a university degree.The German word Menge, rendered as "set" in English, was coined by Bernard Bolzano in his work The Paradoxes of the Infinite.A set is a well-defined collection of distinct objects. The objects that make up a set (also known as the set's elements or members) can be anything: numbers, people, letters of the alphabet, other sets, and so on. Georg Cantor, one of the founders of set theory, gave the following definition of a set at the beginning of his Beiträge zur Begründung der transfiniten Mengenlehre:[1]A set is a gathering together into a whole of definite, distinct objects of our perception [Anschauung] or of our thought—which are called elements of the set.Sets are conventionally denoted with capital letters. Sets A and B are equal if and only if they have precisely the same elements.[2]For technical reasons, Cantor's definition turned out to be inadequate; today, in contexts where more rigor is required, one can use axiomatic set theory, in which the notion of a "set" is taken as a primitive notion and the properties of sets are defined by a collection of axioms. The most basic properties are that a set can have elements, and that two sets are equal (one and the same) if and only if every element of each set is an element of the other; this property is called the extensionality of sets.There are two ways of describing, or specifying the members of, a set. One way is by intensional definition, using a rule or semantic description:The second way is by extension – that is, listing each member of the set.  An extensional definition is denoted by enclosing the list of members in curly brackets:One often has the choice of specifying a set either intensionally or extensionally.  In the examples above, for instance, A = C and B = D.In an extensional definition, a set member can be listed two or more times, for example, {11, 6, 6}.  However, per extensionality, two definitions of sets which differ only in that one of the definitions lists members multiple times define the same set.  Hence, the set {11, 6, 6} is identical to the set {11, 6}. Moreover, the order in which the elements of a set are listed is irrelevant (unlike for a sequence or tuple).  We can illustrate these two important points with an example:For sets with many elements, the enumeration of members can be abbreviated. For instance, the set of the first thousand positive integers may be specified extensionally aswhere the ellipsis ("...") indicates that the list continues in the obvious way.The notation with braces may also be used in an intensional specification of a set.  In this usage, the braces have the meaning "the set of all ...". So, E = {playing card suits} is the set whose four members are ♠, ♦, ♥, and ♣.  A more general form of this is set-builder notation, through which, for instance, the set F of the twenty smallest integers that are four less than perfect square can be denotedIn this notation, the colon (":") means "such that", and the description can be interpreted as "F is the set of all numbers of the form n2 − 4, such that n is a whole number in the range from 0 to 19 inclusive." Sometimes the vertical bar ("|") is used instead of the colon.If B is a set and x is one of the objects of B, this is denoted x ∈ B, and is read as "x belongs to B", or "x is an element of B". If y is not a member of B then this is written as y ∉ B, and is read as "y does not belong to B".For example, with respect to the sets A = {1,2,3,4}, B = {blue, white, red}, and F = {n2 − 4 : n is an integer; and 0 ≤ n ≤ 19} defined above,If every member of set A is also a member of set B, then A is said to be a subset of B, written A ⊆ B (also pronounced A is contained in B). Equivalently, we can write B ⊇ A, read as B is a superset of A, B includes A, or B contains A. The relationship between sets established by ⊆ is called inclusion or containment.If A is a subset of, but not equal to, B, then A is called a proper subset of B, written A ⊊ B (A is a proper subset of B) or B ⊋ A (B is a proper superset of A).The expressions A ⊂ B and B ⊃ A are used differently by different authors; some authors use them to mean the same as A ⊆ B (respectively B ⊇ A), whereas others use them to mean the same as A ⊊ B (respectively B ⊋ A).Examples:The empty set is a subset of every set and every set is a subset of itself:Every set is a subset of the universal set:An obvious but useful identity, which can often be used to show that two seemingly different sets are equal:A partition of a set S is a set of nonempty subsets of S such that every element x in S is in exactly one of these subsets.The power set of a set S is the set of all subsets of S. The power set contains S itself and the empty set because these are both subsets of S. For example, the power set of the set {1, 2, 3} is {{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}, {2}, {3}, ∅}. The power set of a set S is usually written as P(S).The power set of a finite set with n elements has 2n elements. For example, the set {1, 2, 3} contains three elements, and the power set shown above contains 23 = 8 elements.The power set of an infinite (either countable or uncountable) set is always uncountable. Moreover, the power set of a set is always strictly "bigger" than the original set in the sense that there is no way to pair every element of S with exactly one element of P(S). (There is never an onto map or surjection from S onto P(S).)Every partition of a set S is a subset of the powerset of S.The cardinality | S | of a set S is "the number of members of S." For example, if  B = {blue, white, red}, then | B | = 3.There is a unique set with no members, called the empty set (or the null set), which is denoted by the symbol ∅ (other notations are used; see empty set).  The cardinality of the empty set is zero. For example, the set of all three-sided squares has zero members and thus is the empty set. Though it may seem trivial, the empty set, like the number zero, is important in mathematics. Indeed, the existence of this set is one of the fundamental concepts of axiomatic set theory.Some sets have infinite cardinality.  The set N of natural numbers, for instance, is infinite. Some infinite cardinalities are greater than others.  For instance, the set of real numbers has greater cardinality than the set of natural numbers. However, it can be shown that the cardinality of (which is to say, the number of points on) a straight line is the same as the cardinality of any segment of that line, of the entire plane, and indeed of any finite-dimensional Euclidean space.There are some sets or kinds of sets that hold great mathematical importance and are referred to with such regularity that they have acquired special names and notational conventions to identify them. One of these is the empty set, denoted {} or ∅. A set with exactly one element, x, is a unit set, or singleton, {x}.[2]Many of these sets are represented using blackboard bold or bold typeface. Special sets of numbers includePositive and negative sets are denoted by a superscript - or +. For example, ℚ+ represents the set of positive rational numbers.Each of the above sets of numbers has an infinite number of elements, and each can be considered to be a proper subset of the sets listed below it. The primes are used less frequently than the others outside of number theory and related fields.There are several fundamental operations for constructing new sets from given sets.Two sets can be "added" together. The union of A and B, denoted by A ∪ B, is the set of all things that are members of either A or B.Examples:Some basic properties of unions:A new set can also be constructed by determining which members two sets have "in common". The intersection of A and B, denoted by A ∩ B, is the set of all things that are members of both A and B. If A ∩ B = ∅, then A and B are said to be disjoint.Examples:Some basic properties of intersections:Two sets can also be "subtracted". The relative complement of B in A (also called the set-theoretic difference of A and B), denoted by A \ B (or A − B), is the set of all elements that are members of A but not members of B. Note that it is valid to "subtract" members of a set that are not in the set, such as removing the element green from the set {1, 2, 3}; doing so has no effect.In certain settings all sets under discussion are considered to be subsets of a given universal set U. In such cases, U \ A is called the absolute complement or simply complement of A, and is denoted by A′.Examples:Some basic properties of complements:An extension of the complement is the symmetric difference, defined for sets A, B asFor example, the symmetric difference of {7,8,9,10} and {9,10,11,12} is the set {7,8,11,12}. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring (with the empty set as neutral element) and intersection as the multiplication of the ring.A new set can be constructed by associating every element of one set with every element of another set.  The Cartesian product of two sets A and B, denoted by A × B is the set of all ordered pairs (a, b) such that a is a member of A and b is a member of B.Examples:Some basic properties of Cartesian products:Let A and B be finite sets; then the cardinality of the Cartesian product is the product of the cardinalities:Set theory is seen as the foundation from which virtually all of mathematics can be derived. For example, structures in abstract algebra, such as groups, fields and rings, are sets closed under one or more operations.One of the main applications of naive set theory is constructing relations.  A relation from a domain A to a codomain B is a subset of the Cartesian product A × B.  Given this concept, we are quick to see that the set F of all ordered pairs (x, x2), where x is real, is quite familiar.  It has a domain set R and a codomain set that is also R, because the set of all squares is subset of the set of all real numbers.  If placed in functional notation, this relation becomes f(x) = x2.  The reason these two are equivalent is for any given value, y that the function is defined for, its corresponding ordered pair, (y, y2) is a member of the set F.Although initially naive set theory, which defines a set merely as any well-defined collection, was well accepted, it soon ran into several obstacles. It was found that this definition spawned several paradoxes, most notably:The reason is that the phrase well-defined is not very well-defined. It was important to free set theory of these paradoxes because nearly all of mathematics was being redefined in terms of set theory. In an attempt to avoid these paradoxes, set theory was axiomatized based on first-order logic, and thus axiomatic set theory was born.For most purposes, however, naive set theory is still useful.The inclusion–exclusion principle is a counting technique that can be used to count the number of elements in a union of two sets, if the size of each set and the size of their intersection are known. It can be expressed symbolically asA more general form of the principle can be used to find the cardinality of any finite union of sets:Augustus De Morgan stated two laws about sets.If A and B are any two sets then,The complement of A union B equals the complement of A intersected with the complement of B.The complement of A intersected with B is equal to the complement of A union to the complement of B.
Quadruple product
In mathematics, the quadruple product is a product of four vectors in three-dimensional Euclidean space. The name "quadruple product" is used for two different products,[1] the scalar-valued scalar quadruple product and the vector-valued vector quadruple product or vector product of four vectors .The scalar quadruple product  is defined as the dot product of two cross products:where a, b, c, d are vectors in three-dimensional Euclidean space.[2] It can be evaluated using the identity:[2]or using the determinant:The vector quadruple product  is defined as the cross product of two cross products:where a, b, c, d are vectors in three-dimensional Euclidean space.[3] It can be evaluated using the identity:[4]This identity can also be written using tensor notation and the Einstein summation convention as follows:using the notation for the triple product:Equivalent forms can be obtained using the identity:[5]The quadruple products are useful for deriving various formulas in spherical and plane geometry.[3] For example, if four points are chosen on the unit sphere, A, B, C, D, and unit vectors drawn from the center of the sphere to the four points, a, b, c, d respectively, the identity:in conjunction with the relation for the magnitude of the cross product:and the dot product:where a = b = 1 for the unit sphere, results in the identity among the angles attributed to Gauss:where x is the angle between a × b and c × d, or equivalently, between the planes defined by these vectors.Josiah Willard Gibbs's pioneering work on vector calculus provides several other examples.[3]
Linear programming
Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.Linear programs are problems that can be expressed in canonical form asLinear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.The problem of solving a system of linear inequalities dates back at least as far as Fourier, who in 1827 published a method for solving them,[1] and after whom the method of Fourier–Motzkin elimination is named.In 1939 a linear programming formulation of a problem that is equivalent to the general linear programming problem was given by the Soviet economist Leonid Kantorovich, who also proposed a method for solving it.[2] It is a way he developed, during World War II, to plan expenditures and returns in order to reduce costs of the army and to increase losses imposed on the enemy.[citation needed] Kantorovich's work was initially neglected in the USSR.[3] About the same time as Kantorovich, the Dutch-American economist T. C. Koopmans formulated classical economic problems as linear programs. Kantorovich and Koopmans later shared the 1975 Nobel prize in economics.[1] In 1941, Frank Lauren Hitchcock also formulated transportation  problems as linear programs and gave a solution very similar to the later simplex method.[2] Hitchcock had died in 1957 and the Nobel prize is not awarded posthumously.During 1946–1947, George B. Dantzig independently developed general linear programming formulation to use for planning problems in US Air Force[citation needed]. In 1947, Dantzig also invented the simplex method that for the first time efficiently tackled the linear programming problem in most cases[citation needed]. When Dantzig arranged a meeting with John von Neumann to discuss his simplex method, Neumann immediately conjectured the theory of duality by realizing that the problem he had been working in game theory was equivalent[citation needed]. Dantzig provided formal proof in an unpublished report "A Theorem on Linear Inequalities" on January 5, 1948.[3] In the post-war years, many industries applied it in their daily planning.Dantzig's original example was to find the best assignment of 70 people to 70 jobs. The computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the number of particles in the observable universe. However, it takes only a moment to find the optimum solution by posing the problem as a linear program and applying the simplex algorithm. The theory behind linear programming drastically reduces the number of possible solutions that must be checked.The linear programming problem was first shown to be solvable in polynomial time by Leonid Khachiyan in 1979,[4] but a larger theoretical and practical breakthrough in the field came in 1984 when Narendra Karmarkar introduced a new interior-point method for solving linear-programming problems.[5]Linear programming is a widely used field of optimization for several reasons. Many practical problems in operations research can be expressed as linear programming problems.[3] Certain special cases of linear programming, such as network flow problems and multicommodity flow problems are considered important enough to have generated much research on specialized algorithms for their solution. A number of algorithms for other types of optimization problems work by solving LP problems as sub-problems. Historically, ideas from linear programming have inspired many of the central concepts of optimization theory, such as duality, decomposition, and the importance of convexity and its generalizations. Likewise, linear programming was heavily used in the early formation of microeconomics and it is currently utilized in company management, such as planning, production, transportation, technology and other issues. Although the modern management issues are ever-changing, most companies would like to maximize profits and minimize costs with limited resources. Therefore, many issues can be characterized as linear programming problems.Standard form is the usual and most intuitive form of describing a linear programming problem. It consists of the following three parts:The problem is usually expressed in matrix form, and then becomes:Other forms, such as minimization problems, problems with constraints on alternative forms, as well as problems involving negative variables can always be rewritten into an equivalent problem in standard form.Suppose that a farmer has a piece of farm land, say L km2, to be planted with either wheat or barley or some combination of the two. The farmer has a limited amount of fertilizer, F kilograms, and pesticide, P kilograms. Every square kilometer of wheat requires F1 kilograms of fertilizer and P1 kilograms of pesticide, while every square kilometer of barley requires F2 kilograms of fertilizer and P2 kilograms of pesticide. Let S1 be the selling price of wheat per square kilometer, and S2 be the selling price of barley. If we denote the area of land planted with wheat and barley by x1 and x2 respectively, then profit can be maximized by choosing optimal values for x1 and x2. This problem can be expressed with the following linear programming problem in the standard form:In matrix form this becomes:Linear programming problems can be converted into an augmented form in order to apply the common form of the simplex algorithm. This form introduces non-negative slack variables to replace inequalities with equalities in the constraints. The problems can then be written in the following block matrix form:The example above is converted into the following augmented form:In matrix form this becomes:Every linear programming problem, referred to as a primal problem, can be converted into a dual problem, which provides an upper bound to the optimal value of the primal problem. In matrix form, we can express the primal problem as:An alternative primal formulation is:There are two ideas fundamental to duality theory. One is the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program. Additionally, every feasible solution for a linear program gives a bound on the optimal value of the objective function of its dual.  The weak duality theorem states that the objective function value of the dual at any feasible solution is always greater than or equal to the objective function value of the primal at any feasible solution. The strong duality theorem states that if the primal has an optimal solution, x*, then the dual also has an optimal solution, y*, and cTx*=bTy*.A linear program can also be unbounded or infeasible. Duality theory tells us that if the primal is unbounded then the dual is infeasible by the weak duality theorem. Likewise, if the dual is unbounded, then the primal must be infeasible. However, it is possible for both the dual and the primal to be infeasible. As an example, consider the linear program:Revisit the above example of the farmer who may grow wheat and barley with the set provision of some L land, F fertilizer and P pesticide. Assume now that y unit prices for each of these means of production (inputs) are set by a planning board. The planning board's job is to minimize the total cost of procuring the set amounts of inputs while providing the farmer with a floor on the unit price of each of his crops (outputs), S1 for wheat and S2 for barley. This corresponds to the following linear programming problem:In matrix form this becomes:The primal problem deals with physical quantities. With all inputs available in limited quantities, and assuming the unit prices of all outputs is known, what quantities of outputs to produce so as to maximize total revenue? The dual problem deals with economic values. With floor guarantees on all output unit prices, and assuming the available quantity of all inputs is known, what input unit pricing scheme to set so as to minimize total expenditure?To each variable in the primal space corresponds an inequality to satisfy in the dual space, both indexed by output type. To each inequality to satisfy in the primal space corresponds a variable in the dual space, both indexed by input type.The coefficients that bound the inequalities in the primal space are used to compute the objective in the dual space, input quantities in this example. The coefficients used to compute the objective in the primal space bound the inequalities in the dual space, output unit prices in this example.Both the primal and the dual problems make use of the same matrix. In the primal space, this matrix expresses the consumption of physical quantities of inputs necessary to produce set quantities of outputs. In the dual space, it expresses the creation of the economic values associated with the outputs from set input unit prices.Since each inequality can be replaced by an equality and a slack variable, this means each primal variable corresponds to a dual slack variable, and each dual variable corresponds to a primal slack variable.  This relation allows us to speak about complementary slackness.Sometimes, one may find it more intuitive to obtain the dual program without looking at the program matrix. Consider the following linear program:We have m + n conditions and all variables are non-negative. We shall define m + n dual variables: yj and si. We get:Since this is a minimization problem, we would like to obtain a dual program that is a lower bound of the primal. In other words, we would like the sum of all right hand side of the constraints to be the maximal under the condition that for each primal variable the sum of its coefficients do not exceed its coefficient in the linear function. For example, x1 appears in n + 1 constraints. If we sum its constraints' coefficients we get a1,1y1 + a1,2y2 + ... + a1,nyn + f1s1. This sum must be at most c1. As a result, we get:Note that we assume in our calculations steps that the program is in standard form. However, any linear program may be transformed to standard form and it is therefore not a limiting factor.A covering LP is a linear program of the form:such that the matrix A and the vectors b and c are non-negative.The dual of a covering LP is a packing LP, a linear program of the form:such that the matrix A and the vectors b and c are non-negative.Covering and packing LPs commonly arise as a linear programming relaxation of a combinatorial problem and are important in the study of approximation algorithms.[6] For example, the LP relaxations of the set packing problem, the independent set problem, and the matching problem are packing LPs. The LP relaxations of the set cover problem, the vertex cover problem, and the dominating set problem are also covering LPs.Finding a fractional coloring of a graph is another example of a covering LP. In this case, there is one constraint for each vertex of the graph and one variable for each independent set of the graph.It is possible to obtain an optimal solution to the dual when only an optimal solution to the primal is known using the complementary slackness theorem. The theorem states:Suppose that x = (x1, x2, ... , xn) is primal feasible and that y = (y1, y2, ... , ym) is dual feasible. Let (w1, w2, ..., wm) denote the corresponding primal slack variables, and let (z1, z2, ... , zn) denote the corresponding dual slack variables. Then x and y are optimal for their respective problems if and only ifSo if the i-th slack variable of the primal is not zero, then the i-th variable of the dual is equal to zero. Likewise, if the j-th slack variable of the dual is not zero, then the j-th variable of the primal is equal to zero.This necessary condition for optimality conveys a fairly simple economic principle.  In standard form (when maximizing), if there is slack in a constrained primal resource (i.e., there are "leftovers"), then additional quantities of that resource must have no value.  Likewise, if there is slack in the dual (shadow) price non-negativity constraint requirement, i.e., the price is not zero, then there must be scarce supplies (no "leftovers").Geometrically, the linear constraints define the feasible region, which is a convex polyhedron. A linear function is a convex function, which implies that every local minimum is a global minimum; similarly, a linear function is a concave function, which implies that every local maximum is a global maximum.An optimal solution need not exist, for two reasons. First, if two constraints are inconsistent, then no feasible solution exists: For instance, the constraints x ≥ 2 and x ≤ 1 cannot be satisfied jointly; in this case, we say that the LP is infeasible. Second, when the polytope is unbounded in the direction of the gradient of the objective function (where the gradient of the objective function is the vector of the coefficients of the objective function), then no optimal value is attained because it is always possible to do better than any finite value of the objective function.Otherwise, if a feasible solution exists and if the constraint set is bounded, then the optimum value is always attained on the boundary of the constraint set, by the maximum principle for convex functions (alternatively, by the minimum principle for concave functions) since linear functions are both convex and concave. However, some problems have distinct optimal solutions; for example, the problem of finding a feasible solution to a system of linear inequalities is a linear programming problem in which the objective function is the zero function (that is, the constant function taking the value zero everywhere). For this feasibility problem with the zero-function for its objective-function, if there are two distinct solutions, then every convex combination of the solutions is a solution.The vertices of the polytope are also called basic feasible solutions. The reason for this choice of name is as follows. Let d denote the number of variables. Then the fundamental theorem of linear inequalities implies (for feasible problems) that for every vertex x* of the LP feasible region, there exists a set of d (or fewer) inequality constraints from the LP such that, when we treat those d constraints as equalities, the unique solution is x*. Thereby we can study these vertices by means of looking at certain subsets of the set of all constraints (a discrete set), rather than the continuum of LP solutions. This principle underlies the simplex algorithm for solving linear programs.The simplex algorithm, developed by George Dantzig in 1947, solves LP problems by constructing a feasible solution at a vertex of the polytope and then walking along a path on the edges of the polytope to vertices with non-decreasing values of the objective function until an optimum is reached for sure. In many practical problems, "stalling" occurs: many pivots are made with no increase in the objective function.[7][8] In rare practical problems, the usual versions of the simplex algorithm may actually "cycle".[8] To avoid cycles, researchers developed new pivoting rules.[9][10][7][8][11][12]In practice, the simplex algorithm is quite efficient and can be guaranteed to find the global optimum if certain precautions against cycling are taken. The simplex algorithm has been proved to solve "random" problems efficiently, i.e. in a cubic number of steps,[13] which is similar to its behavior on practical problems.[7][14]However, the simplex algorithm has poor worst-case behavior: Klee and Minty constructed a family of linear programming problems for which the simplex method takes a number of steps exponential in the problem size.[7][10][11] In fact, for some time it was not known whether the linear programming problem was solvable in polynomial time, i.e. of complexity class P.Like the simplex algorithm of Dantzig, the criss-cross algorithm is a basis-exchange algorithm that pivots between bases. However, the criss-cross algorithm need not maintain feasibility, but can pivot rather from a feasible basis to an infeasible basis. The criss-cross algorithm does not have polynomial time-complexity for linear programming. Both algorithms visit all 2D corners of a (perturbed) cube in dimension D, the Klee–Minty cube, in the worst case.[12][15]In contrast to the simplex algorithm, which finds an optimal solution by traversing the edges between vertices on a polyhedral set, interior-point methods move through the interior of the feasible region.This is the first worst-case polynomial-time algorithm ever found for linear programming.  To solve a problem which has n variables and can be encoded in L input bits, this algorithm uses O(n4L) pseudo-arithmetic operations on numbers with O(L) digits. Leonid Khachiyan solved this long-standing complexity issue in 1979 with the introduction of the ellipsoid method. The convergence analysis has (real-number) predecessors, notably the iterative methods developed by Naum Z. Shor and the approximation algorithms by Arkadi Nemirovski and D. Yudin.Khachiyan's algorithm was of landmark importance for establishing the polynomial-time solvability of linear programs.  The algorithm was not a computational break-through, as the simplex method is more efficient for all but specially constructed families of linear programs.Affine scaling is one of the oldest interior point methods to be developed. It was developed in the Soviet Union in the mid-1960s, but didn't receive much attention until the discovery of Karmarkar's algorithm, after which affine scaling was reinvented multiple times and presented as a simplified version of Karmarkar's. Affine scaling amounts to doing gradient descent steps within the feasible region, while rescaling the problem to make sure the steps move toward the optimum faster.[17]For both theoretical and practical purposes, barrier function or path-following methods have been the most popular interior point methods since the 1990s.[18]The current opinion is that the efficiencies of good implementations of simplex-based methods and interior point methods are similar for routine applications of linear programming.[18]  However, for specific types of LP problems, it may be that one type of solver is better than another (sometimes much better), and that the structure of the solutions generated by interior point methods versus simplex-based methods are significantly different with the support set of active variables being typically smaller for the later one.[19]Covering and packing LPs can be solved approximately in nearly-linear time. That is, if matrix A is of dimension n×m and has N non-zero entries, then there exist algorithms that run in time O(N·(log N)O(1)/εO(1)) and produce O(1±ε) approximate solutions to given covering and packing LPs. The best known sequential algorithm of this kind runs in time O(N + (log N)·(n+m)/ε2),[20] and the best known parallel algorithm of this kind runs in O((log N)2/ε3) iterations, each requiring only a matrix-vector multiplication which is highly parallelizable.[21]There are several open problems in the theory of linear programming, the solution of which would represent fundamental breakthroughs in mathematics and potentially major advances in our ability to solve large-scale linear programs.This closely related set of problems has been cited by Stephen Smale as among the 18 greatest unsolved problems of the 21st century.  In Smale's words, the third version of the problem "is the main unsolved problem of linear programming theory."  While algorithms exist to solve linear programming in weakly polynomial time, such as the ellipsoid methods and interior-point techniques, no algorithms have yet been found that allow strongly polynomial-time performance in the number of constraints and the number of variables.  The development of such algorithms would be of great theoretical interest, and perhaps allow practical gains in solving large LPs as well.Although the Hirsch conjecture was recently disproved for higher dimensions, it still leaves the following questions open.These questions relate to the performance analysis and development of simplex-like methods.  The immense efficiency of the simplex algorithm in practice despite its exponential-time theoretical performance hints that there may be variations of simplex that run in polynomial or even strongly polynomial time.  It would be of great practical and theoretical significance to know whether any such variants exist, particularly as an approach to deciding if LP can be solved in strongly polynomial time.The simplex algorithm and its variants fall in the family of edge-following algorithms, so named because they solve linear programming problems by moving from vertex to vertex along edges of a polytope.  This means that their theoretical performance is limited by the maximum number of edges between any two vertices on the LP polytope.  As a result, we are interested in knowing the maximum graph-theoretical diameter of polytopal graphs.  It has been proved that all polytopes have subexponential diameter. The recent disproof of the Hirsch conjecture is the first step to prove whether any polytope has superpolynomial diameter. If any such polytopes exist, then no edge-following variant can run in polynomial time. Questions about polytope diameter are of independent mathematical interest.Simplex pivot methods preserve primal (or dual) feasibility.  On the other hand, criss-cross pivot methods do not preserve (primal or dual) feasibility—they may visit primal feasible, dual feasible or primal-and-dual infeasible bases in any order.  Pivot methods of this type have been studied since the 1970s.  Essentially, these methods attempt to find the shortest pivot path on the arrangement polytope under the linear programming problem.  In contrast to polytopal graphs, graphs of arrangement polytopes are known to have small diameter, allowing the possibility of strongly polynomial-time criss-cross pivot algorithm without resolving questions about the diameter of general polytopes.[12]If all of the unknown variables are required to be integers, then the problem is called an integer programming (IP) or integer linear programming (ILP) problem.  In contrast to linear programming, which can be solved efficiently in the worst case, integer programming problems are in many practical situations (those with bounded variables) NP-hard. 0–1 integer programming or binary integer programming (BIP) is the special case of integer programming where variables are required to be 0 or 1 (rather than arbitrary integers). This problem is also classified as NP-hard, and in fact the decision version was one of Karp's 21 NP-complete problems.If only some of the unknown variables are required to be integers, then the problem is called a mixed integer programming (MIP) problem.  These are generally also NP-hard because they are even more general than ILP programs.There are however some important subclasses of IP and MIP problems that are efficiently solvable, most notably problems where the constraint matrix is totally unimodular and the right-hand sides of the constraints are integers or – more general – where the system has the total dual integrality (TDI) property.Advanced algorithms for solving integer linear programs include:Such integer-programming algorithms are discussed by Padberg and in Beasley.Integral linear programs are of central importance in the polyhedral aspect of combinatorial optimization since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective. Conversely, if we can prove that a linear programming relaxation is integral, then it is the desired description of the convex hull of feasible (integral) solutions.Note that terminology is not consistent throughout the literature, so one should be careful to distinguish the following two concepts,One common way of proving that a polyhedron is integral is to show that it is totally unimodular. There are other general methods including the integer decomposition property and total dual integrality. Other specific well-known integral LPs include the matching polytope, lattice polyhedra, submodular flow polyhedra, and the intersection of 2 generalized polymatroids/g-polymatroids – e.g. see Schrijver 2003.A bounded integral polyhedron is sometimes called a convex lattice polytope, particularly in two dimensions.Permissive licenses:Copyleft (reciprocal) licenses:MINTO (Mixed Integer Optimizer, an integer programming solver which uses branch and bound algorithm) has publicly available source code[24] but is not open source.Proprietary licenses:A reader may consider beginning with Nering and Tucker, with the first volume of Dantzig and Thapa, or with Williams.
Skew-Hamiltonian matrix
In linear algebra, skew-Hamiltonian matrices are special matrices which correspond to skew-symmetric bilinear forms on a symplectic vector space.The square of a Hamiltonian matrix is skew-Hamiltonian. The converse is also true: every skew-Hamiltonian matrix can be obtained as the square of a Hamiltonian matrix.[1][2]
Partial differential equation
In mathematics, a partial differential equation (PDE) is a differential equation that contains beforehand unknown multivariable functions and their partial derivatives. PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a computer model. A special case is ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.PDEs can be used to describe a wide variety of phenomena such as sound, heat, diffusion, electrostatics, electrodynamics, fluid dynamics, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.Partial differential equations (PDEs) are equations that involve rates of change with respect to continuous variables. The position of a rigid body is specified by six parameters,[1] but the configuration of a fluid is given by the continuous distribution of several parameters, such as the temperature, pressure, and so forth. The dynamics for the rigid body take place in a finite-dimensional configuration space; the dynamics for the fluid occur in an infinite-dimensional configuration space. This distinction usually makes PDEs much harder to solve than ordinary differential equations (ODEs), but here again, there will be simple solutions for linear problems. Classic domains where PDEs are used include acoustics, fluid dynamics, electrodynamics, and heat transfer.A partial differential equation (PDE) for the function u(x1,… xn) is an equation of the formIf f is a linear function of u and its derivatives, then the PDE is called linear. Common examples of linear PDEs include the heat equation, the wave equation, Laplace's equation, Helmholtz equation, Klein–Gordon equation, and Poisson's equation.A relatively simple PDE isThis relation implies that the function u(x,y) is independent of x. However, the equation gives no information on the function's dependence on the variable y. Hence the general solution of this equation iswhere f is an arbitrary function of y. The analogous ordinary differential equation iswhich has the solutionwhere c is any constant value. These two examples illustrate that general solutions of ordinary differential equations (ODEs) involve arbitrary constants, but solutions of PDEs involve arbitrary functions. A solution of a PDE is generally not unique; additional conditions must generally be specified on the boundary of the region where the solution is defined. For instance, in the simple example above, the function f(y) can be determined if u is specified on the line x = 0.Although the issue of existence and uniqueness of solutions of ordinary differential equations has a very satisfactory answer with the Picard–Lindelöf theorem, that is far from the case for partial differential equations. The Cauchy–Kowalevski theorem states that the Cauchy problem for any partial differential equation whose coefficients are analytic in the unknown function and its derivatives, has a locally unique analytic solution. Although this result might appear to settle the existence and uniqueness of solutions, there are examples of linear partial differential equations whose coefficients have derivatives of all orders (which are nevertheless not analytic) but which have no solutions at all: see Lewy (1957). Even if the solution of a partial differential equation exists and is unique, it may nevertheless have undesirable properties.  The mathematical study of these questions is usually in the more powerful context of weak solutions.An example of pathological behavior is the sequence (depending upon n) of Cauchy problems for the Laplace equationwith boundary conditionswhere n is an integer. The derivative of u with respect to y approaches zero uniformly in x as n increases, but the solution isThis solution approaches infinity if nx is not an integer multiple of π for any non-zero value of y. The Cauchy problem for the Laplace equation is called ill-posed or not well-posed, since the solution does not continuously depend on the data of the problem. Such ill-posed problems are not usually satisfactory for physical applications.The existence of solutions for the Navier–Stokes equations, a partial differential equation, is part of one of the Millennium Prize Problems.In PDEs, it is common to denote partial derivatives using subscripts. That is:Especially in physics, del or nabla (∇) is often used to denote spatial derivatives, and u̇, ü for time derivatives. For example, the wave equation (described below) can be written asorwhere Δ is the Laplace operator.Some linear, second-order partial differential equations can be classified as parabolic, hyperbolic and elliptic. Others, such as the Euler–Tricomi equation, have different types in different regions. The classification provides a guide to appropriate initial and boundary conditions and to the smoothness of the solutions.Assuming uxy = uyx, the general linear second-order PDE in two independent variables has the formwhere the coefficients A, B, C... may depend upon x and y. If A2 + B2 + C2 > 0 over a region of the xy-plane, the PDE is second-order in that region. This form is analogous to the equation for a conic section:More precisely, replacing ∂x by X, and likewise for other variables (formally this is done by a Fourier transform), converts a constant-coefficient PDE into a polynomial of the same degree, with the top degree (a homogeneous polynomial, here a quadratic form) being most significant for the classification.Just as one classifies conic sections and quadratic forms into parabolic, hyperbolic, and elliptic based on the discriminant B2 − 4AC, the same can be done for a second-order PDE at a given point.  However, the discriminant in a PDE is given by B2 − AC due to the convention of the xy term being 2B rather than B; formally, the discriminant (of the associated quadratic form) is (2B)2 − 4AC = 4(B2 − AC), with the factor of 4 dropped for simplicity.If there are n independent variables x1, x2 ,… xn, a general linear partial differential equation of second order has the formThe classification depends upon the signature of the eigenvalues of the coefficient matrix ai,j.The classification of partial differential equations can be extended to systems of first-order equations, where the unknown u is now a vector with m components, and the coefficient matrices Aν are m by m matrices for ν = 1, 2,… n. The partial differential equation takes the formwhere the coefficient matrices Aν and the vector B may depend upon x and u. If a hypersurface S is given in the implicit formwhere φ has a non-zero gradient, then S is a characteristic surface for the operator L at a given point if the characteristic form vanishes:The geometric interpretation of this condition is as follows: if data for u are prescribed on the surface S, then it may be possible to determine the normal derivative of u on S from the differential equation. If the data on S and the differential equation determine the normal derivative of u on S, then S is non-characteristic. If the data on S and the differential equation do not determine the normal derivative of u on S, then the surface is characteristic, and the differential equation restricts the data on S: the differential equation is internal to S.If a PDE has coefficients that are not constant, it is possible that it will not belong to any of these categories but rather be of mixed type. A simple but important example is the Euler–Tricomi equationwhich is called elliptic-hyperbolic because it is elliptic in the region x < 0, hyperbolic in the region x > 0, and degenerate parabolic on the line x = 0.In the phase space formulation of quantum mechanics,  one may consider the quantum Hamilton's equations for trajectories of quantum particles. These equations are infinite-order PDEs. However, in the semiclassical expansion, one has a finite system of ODEs at any fixed order of ħ.  The evolution equation of the Wigner function is also an infinite-order PDE. The quantum trajectories are quantum characteristics, with the use of which one could calculate the evolution of the Wigner function.Linear PDEs can be reduced to systems of ordinary differential equations by the important technique of separation of variables. This technique rests on a characteristic of solutions to differential equations: if one can find any solution that solves the equation and satisfies the boundary conditions, then it is the solution (this also applies to ODEs). We assume as an ansatz that the dependence of a solution on the parameters space and time can be written as a product of terms that each depend on a single parameter, and then see if this can be made to solve the problem.[2]In the method of separation of variables, one reduces a PDE to a PDE in fewer variables, which is an ordinary differential equation if in one variable – these are in turn easier to solve.This is possible for simple PDEs, which are called separable partial differential equations, and the domain is generally a rectangle (a product of intervals). Separable PDEs correspond to diagonal matrices – thinking of "the value for fixed x" as a coordinate, each coordinate can be understood separately.This generalizes to the method of characteristics, and is also used in integral transforms.In special cases, one can find characteristic curves on which the equation reduces to an ODE – changing coordinates in the domain to straighten these curves allows separation of variables, and is called the method of characteristics.More generally, one may find characteristic surfaces.An integral transform may transform the PDE to a simpler one, in particular, a separable PDE. This corresponds to diagonalizing an operator.An important example of this is Fourier analysis, which diagonalizes the heat equation using the eigenbasis of sinusoidal waves.If the domain is finite or periodic, an infinite sum of solutions such as a Fourier series is appropriate, but an integral of solutions such as a Fourier integral is generally required for infinite domains. The solution for a point source for the heat equation given above is an example of the use of a Fourier integral.Often a PDE can be reduced to a simpler form with a known solution by a suitable change of variables.  For example, the Black–Scholes PDEis reducible to the heat equationby the change of variables (for complete details see Solution of the Black Scholes Equation at the Wayback Machine (archived April 11, 2008))Inhomogeneous equations can often be solved (for constant coefficient PDEs, always be solved) by finding the fundamental solution (the solution for a point source), then taking the convolution with the boundary conditions to get the solution.This is analogous in signal processing to understanding a filter by its impulse response.The superposition principle applies to any linear system, including linear systems of PDEs. A common visualization of this concept is the interaction of two waves in phase being combined to result in a greater amplitude, for example sin x + sin x = 2 sin x. The same principle can be observed in PDEs where the solutions may be real or complex and additive. superpositionIf u1 and u2 are solutions of linear PDE in some function space R, then u = c1u1 + c2u2 with any constants c1 and c2 are also a solution of that PDE in the same function space.There are no generally applicable methods to solve nonlinear PDEs. Still, existence and uniqueness results (such as the Cauchy–Kowalevski theorem) are often possible, as are proofs of important qualitative and quantitative properties of solutions (getting these results is a major part of analysis). Computational solution to the nonlinear PDEs, the split-step method, exist for specific equations like nonlinear Schrödinger equation.Nevertheless, some techniques can be used for several types of equations. The h-principle is the most powerful method to solve underdetermined equations. The Riquier–Janet theory is an effective method for obtaining information about many analytic overdetermined systems.The method of characteristics (similarity transformation method) can be used in some very special cases to solve partial differential equations.In some cases, a PDE can be solved via perturbation analysis in which the solution is considered to be a correction to an equation with a known solution. Alternatives are numerical analysis techniques from simple finite difference schemes to the more mature multigrid and finite element methods. Many interesting problems in science and engineering are solved in this way using computers, sometimes high performance supercomputers.From 1870 Sophus Lie's work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called Lie groups, be referred to a common source; and that ordinary differential equations which admit the same infinitesimal transformations present comparable difficulties of integration. He also emphasized the subject of transformations of contact.A general approach to solving PDEs uses the symmetry property of differential equations, the continuous infinitesimal transformations of solutions to solutions (Lie theory). Continuous group theory, Lie algebras and differential geometry are used to understand the structure of linear and nonlinear partial differential equations for generating integrable equations, to find its Lax pairs, recursion operators, Bäcklund transform and finally finding exact analytic solutions to the PDE.Symmetry methods have been recognized to study differential equations arising in mathematics, physics, engineering, and many other disciplines.The adomian decomposition method, the Lyapunov artificial small parameter method, and He's homotopy perturbation method are all special cases of the more general homotopy analysis method. These are series expansion methods, and except for the Lyapunov method, are independent of small physical parameters as compared to the well known perturbation theory, thus giving these methods greater flexibility and solution generality.The three most widely used numerical methods to solve PDEs are the finite element method (FEM), finite volume methods (FVM) and finite difference methods (FDM), as well other kind of methods called Meshfree methods, which were made to solve problems where the before mentioned methods are limited. The FEM has a prominent position among these methods and especially its exceptionally efficient higher-order version hp-FEM. Other hybrid versions of FEM and Meshfree methods include the generalized finite element method (GFEM), extended finite element method (XFEM), spectral finite element method (SFEM), meshfree finite element method, discontinuous Galerkin finite element method (DGFEM), Element-Free Galerkin Method (EFGM), Interpolating Element-Free Galerkin Method (IEFGM), etc.The finite element method (FEM) (its practical application often known as finite element analysis (FEA)) is a numerical technique for finding approximate solutions of partial differential equations (PDE) as well as of integral equations. The solution approach is based either on eliminating the differential equation completely (steady state problems), or rendering the PDE into an approximating system of ordinary differential equations, which are then numerically integrated using standard techniques such as Euler's method, Runge–Kutta, etc.Finite-difference methods are numerical methods for approximating the solutions to differential equations using finite difference equations to approximate derivatives.Similar to the finite difference method or finite element method, values are calculated at discrete places on a meshed geometry. "Finite volume" refers to the small volume surrounding each node point on a mesh. In the finite volume method, surface integrals in a partial differential equation that contain a divergence term are converted to volume integrals, using the divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods conserve mass by design.
Tensor operator
In pure and applied mathematics, quantum mechanics and computer graphics, a tensor operator generalizes the notion of operators which are scalars and vectors. A special class of these are spherical tensor operators which apply the notion of the spherical basis and spherical harmonics. The spherical basis closely relates to the description of angular momentum in quantum mechanics and spherical harmonic functions. The coordinate-free generalization of a tensor operator  is known as a  representation operator.[1]In the same way, tensor quantities must be represented by tensor operators. An example of a tensorquantity (of rank two) is the electrical quadrupole moment of the above molecule. Likewise, the octupole and hexadecapole moments would be tensors of rank three and four, respectively.The rotation operator about the unit vector n (defining the axis of rotation) through angle θ iswhere J = (Jx, Jy, Jz) are the rotation generators (also the angular momentum matrices):Using the completeness condition:we haveIntroducing the Wigner D matrix elements:gives the matrix multiplication:For one basis ket:where Pℓm is an associated Legendre polynomial, ℓ is the orbital angular momentum quantum number, and m is the orbital magnetic quantum number which takes the values −ℓ, −ℓ + 1, ... ℓ − 1, ℓ The formalism of spherical harmonics have wide applications in applied mathematics, and are closely related to the formalism of spherical tensors, as shown below.Spherical harmonics are functions of the polar and azimuthal angles, ϕ and θ respectively, which can be conveniently collected into a unit vector n(θ, ϕ) pointing in the direction of those angles, in the Cartesian basis it is:Now as,we have,A scalar operator is invariant under rotations:[2]and we have a simple result, that the scalar operator commutes with the rotation generators:Examples of a scalar operators includeVector operators (as well as pseudovector operators) are a set of 3 operators that can be rotated according to:[2]where εijk is the Levi-Civita symbol, which all vector operators must satisfy, by construction. As the symbol εijk is a pseudotensor, pseudovector operators are invariant up to a sign: +1 for proper rotations and −1 for improper rotations.Vector operators includeand peusodovector operators includeIn Dirac notation:and since | Ψ > is any quantum state, the same result follows:Note that here, the term "vector" is used two different ways: kets such as |ψ⟩ are elements of abstract Hilbert spaces, while the vector operator is defined as a quantity whose components transform in a certain way under rotations.A vector operator in the spherical basis is V = (V+1, V0, V−1) where the components are:[2]and the commutators with the rotation generators are:where q is a placeholder for the spherical basis labels (+1, 0, −1), and:(some authors may place a factor of 1/2 on the left hand side of the equation) and raise (J+) or lower (J−) the total magnetic quantum number m by one unit. In the spherical basis the generators are:The rotation transformation in the spherical basis (originally written in the Cartesian basis) is then:One can generalize the vector operator concept easily to tensorial operators, shown next.A tensor operator can be rotated according to:[2]Consider a dyadic tensor with components Tij = aibj, this rotates infinitesimally according to:Cartesian dyadic tensors of the formwhere a and b are two vector operators:are reducible, which means they can be re-expressed in terms of a and b as a rank 0 tensor (scalar), plus a rank 1 tensor (an antisymmetric tensor), plus a rank 2 tensor (a symmetric tensor with zero trace):where the first termincludes just one component, a scalar equivalently written (a·b)/3,  the secondincludes three independent components, equivalently the components of (a×b)/2, and the thirdincludes five independent components. Throughout, δij is the Kronecker delta, the components of the identity matrix. The number in the superscripted brackets denotes the tensor rank. These three terms are irreducible, which means they cannot be decomposed further and still be tensors satisfying the defining transformation laws under which they must be invariant. These also correspond to the number of spherical harmonic functions 2ℓ + 1 for ℓ = 0, 1, 2, the same as the ranks for each tensor. Each of the irreducible representations T(1), T(2) ...  transform like angular momentum eigenstates according to the number of independent components.Example of a Tensor operator,in general,Note: This is just an example, in general, a tensor operator cannot be written as the product of two Tensor operators as given in the above example.Continuing the previous example of the second order dyadic tensor T = a ⊗ b, casting each of a and b into the spherical basis and substituting into T gives the spherical tensor operators of the second order, which are:Using the infinitesimal rotation operator and its Hermitian conjugate, one can derive the commutation relation in the spherical basis:and the finite rotation transformation in the spherical basis is:In general, tensor operators can be constructed from two perspectives.[3]One way is to specify how spherical tensors transform under a physical rotation - a group theoretical definition. A rotated angular momentum eigenstate can be decomposed into a linear combination of the initial eigenstates: the coefficients in the linear combination consist of Wigner rotation matrix entries. Spherical tensor operators are sometimes defined as the set of operators that transform just like the eigenkets under a rotation.A spherical tensor Tq(k) of rank k is defined to rotate into Tq′(k) according to:where q = k, k − 1, ..., −k + 1, −k. For spherical tensors, k and q are analogous labels to ℓ and m respectively, for spherical harmonics. Some authors write Tkq instead of Tq(k), with or without the parentheses enclosing the rank number k.Another related procedure requires that the spherical tensors satisfy certain commutation relations with respect to the rotation generators Jx, Jy, Jz - an algebraic definition.The commutation relations of the angular momentum components with the tensor operators are:For any 3d vector, not just a unit vector, and not just the position vector:a spherical tensor is a spherical harmonic as a function of this vector a, and in Dirac notation:(the super and subscripts switch places for the corresponding labels ℓ ↔ k and m ↔ q which spherical tensors and spherical harmonics use).Spherical harmonic states and spherical tensors can also be constructed out of the Clebsch–Gordan coefficients. Irreducible spherical tensors can build higher rank spherical tensors; if Aq1(k1) and Bq2(k2) are two spherical tensors of ranks k1 and k2 respectively, then:is a spherical tensor of rank k.The Hermitian adjoint of a spherical tensor may be defined asThere is some arbitrariness in the choice of the phase factor: any factor containing (−1)±q will satisfy the commutation relations.[4]  The above choice of phase has the advantages of being real and that the tensor product two commuting Hermitian operators is still Hermitian.[5]  Some authors define it with a different sign on q, without the k, or use only the floor of k[6].Orbital angular momentum operators have the ladder operators:which raise or lower the orbital magnetic quantum number mℓ by one unit. This has almost exactly the same form as the spherical basis, aside from constant multiplicative factors.Spherical tensors can also be formed from algebraic combinations of the spin operators Sx, Sy, Sz, as matrices, for a spin system with total quantum number j = ℓ + s (and ℓ = 0). Spin operators have the ladder operators:which raise or lower the spin magnetic quantum number ms by one unit.Spherical bases have broad applications in pure and applied mathematics and physical sciences where spherical geometries occur.The transition amplitude is proportional to matrix elements of the dipole operator between the initial and final states. We use an electrostatic, spinless model for the atom and we consider the transition from the initial energy level Enℓ to final level En′ℓ′. These levels are degenerate, since the energy does not depend on the magnetic quantum number m or m′. The wave functions have the form,The dipole operator is proportional to the position operator of the electron, so we must evaluate matrix elements of the form,where, the initial state is on the right and the final one on the left. The position operator r has three components, and the initial and final levels consist of 2ℓ + 1 and 2ℓ′ + 1 degenerate states, respectively. Therefore if we wish to evaluate the intensity of a spectral line as it would be observed, we really have to evaluate 3(2ℓ′+ 1)(2ℓ+ 1) matrix elements, for example, 3×3×5 = 45 in a 3d → 2p transition. This is actually an exaggeration, as we shall see, because many of the matrix elements vanish, but there are still many non-vanishing matrix elements to be calculated.A great simplification can be achieved by expressing the components of r, not with respect to the Cartesian basis, but with respect to the spherical basis. First we define,Next, by inspecting a table of the Yℓm′s, we find that for ℓ = 1 we have,where, we have multiplied each Y1m by the radius r. On the right hand side we see the spherical components rq of the position vector r. The results can be summarized by,for q = 1, 0, −1, where q appears explicitly as a magnetic quantum number. This equation reveals a relationship between vector operators and the angular momentum value ℓ = 1, something we will have more to say about presently. Now the matrix elements become a product of a radial integral times an angular integral,We see that all the dependence on the three magnetic quantum numbers (m′,q,m) is contained in the angular part of the integral. Moreover, the angular integral can be evaluated by the three-Yℓm formula, whereupon it becomes proportional to the Clebsch-Gordan coefficient,The radial integral is independent of the three magnetic quantum numbers (m′, q, m), and the trick we have just used does not help us to evaluate it. But it is only one integral, and after it has been done, all the other integrals can be evaluated just by computing or looking up Clebsch-Gordancoefficients.The selection rule m′ = q + m in the Clebsch-Gordan coefficient means that many of the integrals vanish, so we have exaggerated the total number of integrals that need to be done. But had we worked with the Cartesian components ri of r, this selection rule might not have been obvious. In any case, even with the selection rule, there may still be many nonzero integrals to be done (nine, in the case 3d → 2p).The example we have just given of simplifying the calculation of matrix elements for a dipole transition is really an application of the Wigner-Eckart theorem, which we take up later in these notes.The spherical tensor formalism provides a common platform for treating coherence and relaxation in nuclear magnetic resonance. In NMR and EPR, spherical tensor operators are employed to express the quantum dynamics of particle spin, by means of an equation of motion for the density matrix entries, or to formulate dynamics in terms of an equation of motion in Liouville space. The Liouville space equation of motion governs the observable averages of spin variables. When relaxation is formulated using a spherical tensor basis in Liouville space, insight is gained because the relaxation matrix exhibits the cross-relaxation of spin observables directly.[3]
Translation of axes
In mathematics, a translation of axes in two dimensions is a mapping from an xy-Cartesian coordinate system to an x'y'-Cartesian coordinate system in which the x' axis is parallel to the x axis and k units away, and the y' axis is parallel to the y axis and h units away.  This means that the origin O' of the new coordinate system has coordinates (h, k) in the original system.  The positive x' and y' directions are taken to be the same as the positive x and y directions.  A point P has coordinates (x, y) with respect to the original system and coordinates (x', y') with respect to the new system, where    (1)or equivalently    (2)In the new coordinate system, the point P will appear to have been translated in the opposite direction.  For example, if the xy-system is translated a distance h to the right and a distance k upward, then P will appear to have been translated a distance h to the left and a distance k downward in the x'y'-system .  A translation of axes in more than two dimensions is defined similarly.[3]   A translation of axes is a rigid transformation, but not a linear map.  (See Affine transformation.)Coordinate systems are essential for studying the equations of curves using the methods of analytic geometry.  To use the method of coordinate geometry, the axes are placed at a convenient position with respect to the curve under consideration.  For example, to study the equations of ellipses and hyperbolas, the foci are usually located on one of the axes and are situated symmetrically with respect to the origin.  If the curve (hyperbola, parabola, ellipse, etc.) is not situated conveniently with respect to the axes, the coordinate system should be changed to place the curve at a convenient and familiar location and orientation.  The process of making this change is called a transformation of coordinates.[4]The solutions to many problems can be simplified by translating the coordinate axes to obtain new axes parallel to the original ones.[5]Through a change of coordinates, the equation of a conic section can be put into a standard form, which is usually easier to work with.   For the most general equation of the second degree, it is always possible to perform a rotation of axes in such a way that in the new system the equation takes the form    (3)that is, there is no xy term.[6]  Next, a translation of axes can reduce an equation of the form (3) to an equation of the same form but with new variables (x', y') as coordinates, and with D and E both equal to zero (with certain exceptions—for example, parabolas).  The principal tool in this process is "completing the square."[7]  In the examples that follow, it is assumed that a rotation of axes has already been performed.Given the equationby using a translation of axes, determine whether the locus of the equation is a parabola, ellipse, or hyperbola.  Determine foci (or focus), vertices (or vertex), and eccentricity.Solution:  To complete the square in x and y, write the equation in the formComplete the squares and obtainDefine    (4)Divide equation (4) by 225 to obtainFor an xyz-Cartesian coordinate system in three dimensions, suppose that a second Cartesian coordinate system is introduced, with axes x', y' and z' so located that the x' axis is parallel to the x axis and h units from it, the y' axis is parallel to the y axis and k units from it, and the z' axis is parallel to the z axis and l units from it.  A point P in space will have coordinates in both systems.  If its coordinates are (x, y, z) in the original system and (x', y', z') in the second system, the equations    (5)hold.[9]  Equations (5) define a translation of axes in three dimensions where (h, k, l) are the xyz-coordinates of the new origin.[10]  A translation of axes in any finite number of dimensions is defined similarly.In three-space, the most general equation of the second degree in x, y and z has the form    (6)As in the case of plane analytic geometry, the method of translation of axes may be used to simplify second-degree equations, thereby making evident the nature of certain quadric surfaces.  The principal tool in this process is "completing the square."[12]Use a translation of coordinates to identify the quadric surfaceSolution:  Write the equation in the formComplete the square to obtainIntroduce the translation of coordinatesThe equation of the surface takes the formwhich is recognizable as the equation of an ellipsoid.[13]
Linear algebra
Linear algebra is the branch of mathematics concerning linear equations such as linear functions such asand their representations through matrices and vector spaces.[1][2][3]Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first-order approximation.The procedure for solving simultaneous linear equations now called Gaussian elimination appears in the ancient Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. [4]Systems of linear equations arose in Europe with the introduction in 1637 by René Descartes of coordinates in geometry. In fact, in this new geometry, now called Cartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.The first systematic methods for solving linear systems used determinants, first considered by Leibniz in 1693. In 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's Rule. Later, Gauss further described the method of elimination, which was initially listed as an advancement in geodesy.[5]In 1844 Hermann Grassmann published his "Theory of Extension" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for womb. Arthur Cayley introduced matrix multiplication  and the inverse matrix in 1856, making possible the general linear group. The mechanism of group representation became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".[5]Benjamin Peirce published his Linear Associative Algebra (1872), and his son Charles Sanders Peirce extended the work later.[6]The telegraph required an explanatory system, and the 1873 publication of A Treatise on Electricity and Magnetism instituted a field theory of forces and required differential geometry for expression. Linear algebra is flat differential geometry and serves in tangent spaces to manifolds. Electromagnetic symmetries of spacetime are expressed by the Lorentz transformations, and much of the history of linear algebra is the history of Lorentz transformations.The first modern and more precise definition of a vector space was introduced by Peano in 1888;[5] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.[5]See also Determinant § History and Gaussian elimination § History.Until 19th century, linear algebra was introduced through systems of linear equations and matrices. In modern mathematics, the presentation through vector spaces is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms.  Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The axioms that addition and scalar multiplication must satisfy are the following (in the list below, u, v and w are arbitrary elements of V, and a and b are arbitrary scalars in the field F.[7]The first four axioms mean that V is an abelian group under addition.Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.Linear maps are mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear map (also called, in some contexts, linear transformation, linear mapping or linear operator) is a mapthat is compatible with addition and scalar multiplication, that isfor any vectors u,v in V and scalar a in F.This implies that for any vectors u, v in V and scalars a, b in F, one hasWhen a bijective linear map exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its range (or image) and the set of elements that are mapped to the zero vector, called the kernel of the map. All these questions can be solved by using Gaussian elimination or some variant of this algorithm.The study of subsets of vector spaces that are themselves vector spaces for the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called linear subspaces. More precisely, a linear subspace of a vector space V over a field F is a subset W of V such that u + v and au are in W, for every u, v in W, and every a in F. (These conditions suffices for implying that W is a vector space.)For example, the image of a linear map, and the inverse image of 0 by a linear map (called kernel or null space) are linear subspaces.Another important way of forming a subspace is to consider linear combinations of a set S of vectors: the set of all sums where v1, v2, ..., vk are in V, and a1, a2, ..., ak are in F form a linear subspace called the span of S. The span of S is also the intersection of all linear subspaces containing S. In other words, it is the (smallest for the inclusion relation) linear subspace containing S.Any two bases of a vector space V have the same cardinality, which is called the dimension of V; this is the dimension theorem for vector spaces. Moreover, two vector spaces over the same field F are isomorphic if and only if they have the same dimension.[8]If any basis of V (and therefore every basis) has a finite number of elements, V is a finite-dimensional vector space. If U is a subspace of V, then dim U ≤ dim V. In the case where V is finite-dimensional, the equality of the dimensions implies U = V.If U1 and U2 are subspaces of V, thenMatrices allow explicit manipulation of finite-dimensional vector spaces and linear maps. Their theory is thus an essential part of linear algebra.Let V be a finite-dimensional vector space over a field F, and (v1, v2, ..., vm) be a basis of V (thus m is the dimension of V). By definition of a basis, the mapfor j = 1, ..., n, then f is represented by the matrixwith m rows and n columns.Matrix multiplication is defined in such a way that the product of two matrices is the matrix of the composition of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing exactly the same concepts.Two matrices that encode the same linear transformation in different bases are called similar. Equivalently, two matrices are similar if one can transform one in the other by elementary row and column operations. For a matrix representing a linear map from W to V, the row operations correspond to change of bases in V and the column operations correspond to change of bases in W. Every matrix is similar to an identity matrix possibly bordered by zero rows and zero columns. In terms of vector space, this means that, for any linear map from W to V, there are bases such a part of the basis of W is mapped bijectively on a part of the basis of V, and that the remaining basis elements of W, if any, are mapped to zero (this is a way of expressing the fundamental theorem of linear algebra). Gaussian elimination is the basic algorithm for finding these elementary operations, and proving this theorem.Systems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory has been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.For example, letbe a linear system.To such a system, one may associate its matrix and its right member vectorLet T be the linear transformation associated to the matrix M. A solution of the system (S) is a vector such that that is an element of the preimage of v by T.Let (S') be the associated homogeneous system, where the right-hand sides of the equations are put to zero. The solutions of (S') are exactly the elements of the kernel of T or, equivalently, M.The Gaussian-elimination consists of performing elementary row operations on the augmented matrixfor putting it in reduced row echelon form. These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is showing that the system (S) has the unique solutionIt follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the ranks, kernels, matrix inverses.A linear endomorphism is a linear map that maps a vector space V to itself. If V has a basis of n elements, such an endomorphism is represented by a square matrix of size n.With respect to general linear maps, linear endomorphisms and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including geometric transformations, coordinate changes, quadratic forms, and many other part of mathematics.The determinant of a square matrix is a polynomial function of the entries of the matrix, such that the matrix is invertible if and only if the determinant is not zero. This results from the fact that the determinant of a product of matrices is the product of the determinants, and thus that a matrix is invertible if and only if its determinant is invertible.Cramer's rule is a closed-form expression, in terms of determinants, of the solution of a system of n linear equations in n unknowns. Cramer's rule is useful for reasoning about the solution, but, except for n = 2 or 3, it is rarely used for computing a solution, since Gaussian elimination is a faster algorithm.The determinant of an endomorphism is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense, since this determinant is independent of the choice of the basis.If f is a linear endomorphism of a vector space V over a field F, an eigenvector of f is a nonzero vector v of V such that f(v) = av for some scalar a in F. This scalar a is an eigenvalue of f.If the dimension of V is finite, and a basis has been chosen, f and v may be represented, respectively, by a square matrix M and a column matrix and z; the equation defining eigenvectors and eigenvalues becomesUsing the identity matrix I, whose all entries are zero, except those of the main diagonal, which are equal to one, this may be rewrittenIf V is of dimension n, this is a monic polynomial of degree n, called the characteristic polynomial of the matrix (or of the endomorphism), and there are, at most, n eigenvalues.If a basis exists that consists only of eigenvectors, the matrix of f on this basis has a very simple structure: it is a diagonal matrix such that the entries on the main diagonal are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said diagonalizable. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after extending the field of scalars. In this extended sense, if the characteristic polynomial is square-free, then the matrix is diagonalizable.A symmetric matrix is always diagonalizable. There are non-diagonizable matrices, the simplest being(it cannot be diagonalizable since its square is the zero matrix, and the square of a nonzero diagonal matrix is never zero).When an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The Frobenius normal form does not need of extending the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The Jordan normal form requires to extend the field of scalar for containing all eigenvalues, and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.For v in V, the mapThere is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the bra–ket notationfor denoting f (x).Let be a linear map. For every linear form h on W, the composite function f ∘ h is a linear form on V. This defines a linear mapbetween the dual spaces, which is called the dual or the transpose of f.If elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in bra–ket notation by For highlighting this symmetry, the two members of this equality are sometimes written Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a mapthat satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:[10][11]Note that in R, it is symmetric.We can define the length of a vector v in V byand we can prove the Cauchy–Schwarz inequality:In particular, the quantityand so we can call this quantity the cosine of the angle between the two vectors.The inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfyingIf T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.There is a strong relationship between linear algebra and geometry, which started with the introduction by René Descartes, in 1637, of Cartesian coordinates. In this new (at that time) geometry, now called Cartesian geometry, points are represented by Cartesian coordinates, which are sequences of three real numbers (in the case of the usual three-dimensional space). The basic objects of geometry, which are lines and planes are represented by linear equations. Thus, computing intersections of lines and planes amounts solving systems of linear equations. This was one of the main motivations for developing linear algebra.Most geometric transformation, such as translations, rotations, reflections, rigid motions, isometries, and projections transform lines into lines. It follows that they can be defined, specified and studied in terms of linear maps. This is also the case of homographies and Möbius transformations, when considered as transformations of a projective space. Until the end of 19th century, geometric spaces were defined by axioms relating points, lines and planes (synthetic geometry). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, Projective space and Affine space) It has been shown that the two approaches are essentially equivalent.[12] In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including finite fields. Presently, most textbooks, introduce geometric spaces from linear algebra, and geometry is often presented, at elementary level, as a subfield of linear algebra.Linear algebra is used in almost all areas of mathematics, and therefore in almost all scientific domains that use mathematics. These applications may be divided into several wide categories.The modeling of our ambient space is based on geometry. Sciences concerned with this space use geometry widely. This is the case with mechanics and robotics, for describing rigid body dynamics; geodesy for describing Earth shape; perspectivity, computer vision, and computer graphics, for describing the relationship between a scene and its plane representation; and many other scientific domains.In all these applications, synthetic geometry is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with coordinates. This requires the heavy use of linear algebra.Functional analysis studies function spaces. These are vector spaces with additional structure, such as Hilbert spaces. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, quantum mechanics (wave functions).Most physical phenomena are modeled by partial differential equations. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting cells. For linear systems this interaction involves linear functions. For nonlinear systems, this interaction is often approximated by linear functions.[13] In both cases, very large matrices are generally involved. Weather forecasting is a typical example, where the whole Earth atmosphere is divided in cells of, say, 100 km of width and 100 m of height.Nearly all scientific computations involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. BLAS and LAPACK are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, for adapting them to the specificities of the computer (cache size, number of available cores, ...).Some processors, typically graphics processing units (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra.This section presents several related topics that do not appear generally in elementary textbooks on linear algebra, but are commonly considered, in advanced mathematics, as parts of linear algebra.The existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a ring R, and this gives a structure called module over R, or R-module.The concepts of linear independence, span, basis, and linear maps (also called module homomorphisms) are defined for modules exactly as for vector spaces, with the essential difference that, if R is not a field, there are modules that do not have any basis. The modules that have a basis are the free modules, and those that are spanned by a finite set are the finitely generated modules. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that determinants exist only if the ring is commutative, and that a square matrix over a commutative ring is invertible only if its determinant has a multiplicative inverse in the ring.Vector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a cokernel of a homomorphism of free modules.Modules over the integers can be identified with abelian groups, since the multiplication by an integer may identified to a repeated addition. Most of the theory of abelian groups may be extended to modules over a principal ideal domain. In particular, over a principal ideal domain, every submodule of a free module is free, and the fundamental theorem of finitely generated abelian groups may be extended straightforwardly to finitely generated modules over a principal ring.There are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a computational complexity that is much higher than the similar algorithms over a field. For more details, see Linear equation over a ring.In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V∗ consisting of linear maps f: V → F where F is the field of scalars. Multilinear maps T: Vn → F can be described via tensor products of elements of V∗.If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V × V → V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces.
Discrete Fourier transform
In mathematics, the discrete Fourier transform (DFT) converts a finite sequence of equally-spaced samples of a function into a same-length sequence of equally-spaced samples of the discrete-time Fourier transform (DTFT), which is a complex-valued function of frequency. The interval at which the DTFT is sampled is the reciprocal of the duration of the input sequence.  An inverse DFT is a Fourier series, using the DTFT samples as coefficients of complex sinusoids at the corresponding DTFT frequencies.  It has the same sample-values as the original input sequence.  The DFT is therefore said to be a frequency domain representation of the original input sequence.  If the original sequence spans all the non-zero values of a function, its DTFT is continuous (and periodic), and the DFT provides discrete samples of one cycle.  If the original sequence is one cycle of a periodic function, the DFT provides all the non-zero values of one DTFT cycle.The DFT is the most important discrete transform, used to perform Fourier analysis in many practical applications.[1]  In digital signal processing, the function is any quantity or signal that varies over time, such as the pressure of a sound wave, a radio signal, or daily temperature readings, sampled over a finite time interval (often defined by a window function[2]). In image processing, the samples can be the values of pixels along a row or column of a raster image. The DFT is also used to efficiently solve partial differential equations, and to perform other operations such as convolutions or multiplying large integers.Since it deals with a finite amount of data, it can be implemented in computers by numerical algorithms or even dedicated hardware. These implementations usually employ efficient fast Fourier transform (FFT) algorithms;[3] so much so that the terms "FFT" and "DFT" are often used interchangeably.  Prior to its current usage, the "FFT" initialism may have also been used for the ambiguous term "finite Fourier transform".    (Eq.1)where the last expression follows from the first one by Euler's formula.Eq.1 can be interpreted or derived in various ways, for example:    (Eq.2)The discrete Fourier transform is an invertible, linear transformationThe inverse transform is given by:where the star denotes complex conjugation.  Plancherel theorem is a special case of the Parseval's theorem and states:These theorems are also equivalent to the unitary condition below.The periodicity can be shown directly from the definition:Similarly, it can be shown that the IDFT formula leads to a periodic extension.It can also be shown that:The trigonometric interpolation polynomialAnother way of looking at the DFT is to note that in the above discussion, the DFT can be expressed as the DFT matrix, a Vandermonde matrix, introduced by Sylvester in 1867,whereis a primitive Nth root of unity.The inverse transform is then given by the inverse of the above matrix,The orthogonality of the DFT is now expressed as an orthonormality condition (which arises in many areas of mathematics as described in root of unity):If X  is defined as the unitary DFT of the vector x, thenand the Plancherel theorem is expressed asA consequence of the circular convolution theorem is that the DFT matrix F diagonalizes any circulant matrix.A useful property of the DFT is that the inverse DFT can be easily expressed in terms of the (forward) DFT, via several well-known "tricks".  (For example, in computations, it is often convenient to only implement a fast Fourier transform corresponding to one transform direction and then to get the other transform direction from the first.)First, we can compute the inverse DFT by reversing all but one of the inputs (Duhamel et al., 1988):Second, one can also conjugate the inputs and outputs:That is, the inverse transform is the same as the forward transform with the real and imaginary parts swapped for both input and output, up to a normalization (Duhamel et al., 1988).The eigenvalues of the DFT matrix are simple and well-known, whereas the eigenvectors are complicated, not unique, and are the subject of ongoing research.This matrix satisfies the matrix polynomial equation:The problem of their multiplicity was solved by McClellan and Parks (1972), although it was later shown to have been equivalent to a problem solved by Gauss (Dickinson and Steiglitz, 1982).  The multiplicity depends on the value of N modulo 4, and is given by the following table:No simple analytical formula for general eigenvectors is known.   Moreover, the eigenvectors are not unique because any linear combination of eigenvectors for the same eigenvalue is also an eigenvector for that eigenvalue.  Various researchers have proposed different choices of eigenvectors, selected to satisfy useful properties like orthogonality and to have "simple" forms (e.g., McClellan and Parks, 1972; Dickinson and Steiglitz, 1982; Grünbaum, 1982; Atakishiyev and Wolf, 1997; Candan et al., 2000; Hanna et al., 2004; Gurevich and Hadani, 2008).A straightforward approach is to discretize an eigenfunction of the continuous Fourier transform,of which the most famous is the Gaussian function.Since periodic summation of the function means discretizing its frequency spectrumand discretization means periodic summation of the spectrum,the discretized and periodically summed Gaussian function yields an eigenvector of the discrete transform:The closed form expression for the series can be expressed byJacobi theta functions asTwo other simple closed-form analytical eigenvectors for special DFT period N were found (Kong, 2008):For DFT period N = 2L + 1 = 4K +1, where K is an integer, the following is an eigenvector of DFT:For DFT period N = 2L = 4K, where K is an integer, the following is an eigenvector of DFT:The choice of eigenvectors of the DFT matrix has become important in recent years in order to define a discrete analogue of the fractional Fourier transform—the DFT matrix can be taken to fractional powers by exponentiating the eigenvalues (e.g., Rubio and Santhanam, 2005).  For the continuous Fourier transform, the natural orthogonal eigenfunctions are the Hermite functions, so various discrete analogues of these have been employed as the eigenvectors of the DFT, such as the Kravchuk polynomials (Atakishiyev and Wolf, 1997).  The "best" choice of eigenvectors to define a fractional discrete Fourier transform remains an open question, however.If the random variable Xk is constrained bythen may be considered to represent a discrete probability mass function of n, with an associated probability mass function constructed from the transformed variable,However, the Hirschman entropic uncertainty will have a useful analog for the case of the DFT.[7] The Hirschman uncertainty principle is expressed in terms of the Shannon entropy of the two probability functions.In the discrete case, the Shannon entropies are defined asandand the entropic uncertainty principle becomes[7]It is possible to shift the transform sampling in time and/or frequency domain by some real shifts a and b, respectively. This is sometimes known as a generalized DFT (or GDFT), also called the shifted DFT or offset DFT, and has analogous properties to the ordinary DFT:The term GDFT is also used for the non-linear phase extensions of DFT. Hence, GDFT method provides a generalization for constant amplitude orthogonal block transforms including linear and non-linear phase types. GDFT is a framework to improve time and frequency domain properties of the traditional DFT, e.g. auto/cross-correlations, by the addition of the properly designed phase shaping function (non-linear, in general) to the original linear phase functions (Akansu and Agirman-Tosun, 2010).[10]The discrete Fourier transform can be viewed as a special case of the z-transform, evaluated on the unit circle in the complex plane; more general z-transforms correspond to complex shifts a and b above.The inverse of the multi-dimensional DFT is, analogous to the one-dimensional case, given by:An algorithm to compute a one-dimensional DFT is thus sufficient to efficiently compute a multidimensional DFT.  This approach is known as the row-column algorithm. There are also intrinsically multidimensional FFT algorithms.The DFT has seen wide usage across a large number of fields; we only sketch a few examples below (see also the references at the end). All applications of the DFT depend crucially on the availability of a fast algorithm to compute discrete Fourier transforms and their inverses, a fast Fourier transform.A final source of distortion (or perhaps illusion) is the DFT itself, because it is just a discrete sampling of the DTFT, which is a function of a continuous frequency domain.  That can be mitigated by increasing the resolution of the DFT.  That procedure is illustrated at Sampling the DTFT.See FFT filter banks and Sampling the DTFT.The field of digital signal processing relies heavily on operations in the frequency domain (i.e. on the Fourier transform). For example, several lossy image and sound compression methods employ the discrete Fourier transform: the signal is cut into short segments, each is transformed, and then the Fourier coefficients of high frequencies, which are assumed to be unnoticeable, are discarded. The decompressor computes the inverse transform based on this reduced number of Fourier coefficients. (Compression applications often use a specialized form of the DFT, the discrete cosine transform or sometimes the modified discrete cosine transform.)Some relatively recent compression algorithms, however, use wavelet transforms, which give a more uniform compromise between time and frequency domain than obtained by chopping data into segments and transforming each segment.  In the case of JPEG2000, this avoids the spurious image features that appear when images are highly compressed with the original JPEG.Suppose we wish to compute the polynomial product c(x) = a(x) · b(x).  The ordinary product expression for the coefficients of c involves a linear (acyclic) convolution, where indices do not "wrap around."  This can be rewritten as a cyclic convolution by taking the coefficient vectors for a(x) and b(x) with constant term first, then appending zeros so that the resultant coefficient vectors a and b have dimension d > deg(a(x)) + deg(b(x)).  Then,But convolution becomes multiplication under the DFT:Here the vector product is taken elementwise.  Thus the coefficients of the product polynomial c(x) are just the terms 0, ..., deg(a(x)) + deg(b(x)) of the coefficient vectorWith a fast Fourier transform, the resulting algorithm takes O (N log N) arithmetic operations.  Due to its simplicity and speed, the Cooley–Tukey FFT algorithm, which is limited to composite sizes, is often chosen for the transform operation.  In this case, d should be chosen as the smallest integer greater than the sum of the input polynomial degrees that is factorizable into small prime factors (e.g. 2, 3, and 5, depending upon the FFT implementation).The fastest known algorithms for the multiplication of very large integers use the polynomial multiplication method outlined above.  Integers can be treated as the value of a polynomial evaluated specifically at the number base, with the coefficients of the polynomial corresponding to the digits in that base.  After polynomial multiplication, a relatively low-complexity carry-propagation step completes the multiplication.When data is convolved with a function with wide support, such as for downsampling by a large sampling ratio, because of the Convolution theorem and the FFT algorithm, it may be faster to transform it, multiply pointwise by the transform of the filter and then reverse transform it.  Alternatively, a good filter is obtained by simply truncating the transformed data and re-transforming the shortened data set.From this point of view, one may generalize the DFT to representation theory generally, or more narrowly to the representation theory of finite groups.More narrowly still, one may generalize the DFT by either changing the target (taking values in a field other than the complex numbers), or the domain (a group other than a finite cyclic group), as detailed in the sequel.The standard DFT acts on a sequence x0, x1, …, xN−1 of complex numbers, which can be viewed as a function {0, 1, …, N − 1} → C. The multidimensional DFT acts on multidimensional sequences, which can be viewed as functionsThis suggests the generalization to Fourier transforms on arbitrary finite groups, which act on functions G → C where G is a finite group. In this framework, the standard DFT is seen as the Fourier transform on a cyclic group, while the multidimensional DFT is a Fourier transform on a direct sum of cyclic groups.Further, Fourier transform can be on cosets of a group. There are various alternatives to the DFT for various applications, prominent among which are wavelets. The analog of the DFT is the discrete wavelet transform (DWT). From the point of view of time–frequency analysis, a key limitation of the Fourier transform is that it does not include location information, only frequency information, and thus has difficulty in representing transients. As wavelets have location as well as frequency, they are better able to represent location, at the expense of greater difficulty representing frequency. For details, see comparison of the discrete wavelet transform with the discrete Fourier transform.
Defective matrix
In linear algebra, a defective matrix is a square matrix that does not have a complete basis of eigenvectors, and is therefore not diagonalizable.  In particular, an n × n matrix is defective if and only if it does not have n linearly independent eigenvectors.[1]  A complete basis is formed by augmenting the eigenvectors with generalized eigenvectors, which are necessary for solving defective systems of ordinary differential equations and other problems.A defective matrix always has fewer than n distinct eigenvalues, since distinct eigenvalues always have linearly independent eigenvectors.  In particular, a defective matrix has one or more eigenvalues λ with algebraic multiplicity m > 1 (that is, they are multiple roots of the characteristic polynomial), but fewer than m linearly independent eigenvectors associated with λ. If the algebraic multiplicity of λ exceeds its geometric multiplicity (that is, the number of linearly independent eigenvectors associated with λ), then λ is said to be a defective eigenvalue.[1]  However, every eigenvalue with algebraic multiplicity m always has m linearly independent generalized eigenvectors.A Hermitian matrix (or the special case of a real symmetric matrix) or a unitary matrix is never defective; more generally, a normal matrix (which includes Hermitian and unitary as special cases) is never defective.Any nontrivial Jordan block of size 2×2 or larger (that is, not completely diagonal) is defective.  (A diagonal matrix is a special case of the Jordan normal form and is not defective.)  For example, the n × n Jordan block,has an eigenvalue, λ, with algebraic multiplicity n, but only one distinct eigenvector,In fact, any defective matrix has a nontrivial Jordan normal form, which is as close as one can come to diagonalization of such a matrix.A simple example of a defective matrix is:which has a double eigenvalue of 3 but only one distinct eigenvector(and constant multiples thereof).
Triangle inequality
In mathematics, the triangle inequality states that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side.[1][2] This statement permits the inclusion of degenerate triangles, but some authors, especially those writing about elementary geometry, will exclude this possibility, thus leaving out the possibility of equality.[3] If x, y, and z are the lengths of the sides of the triangle, with no side being greater than z, then the triangle inequality states thatwith equality only in the degenerate case of a triangle with zero area.In Euclidean geometry and some other geometries, the triangle inequality is a theorem about distances, and it is written using vectors and vector lengths (norms):where the length z of the third side has been replaced by the vector sum x + y. When x and y are real numbers, they can be viewed as vectors in ℝ1, and the triangle inequality expresses a relationship between absolute values.In Euclidean geometry, for right triangles the triangle inequality is a consequence of the Pythagorean theorem, and for general triangles a consequence of the law of cosines, although it may be proven without these theorems. The inequality can be viewed intuitively in either ℝ2 or ℝ3. The figure at the right shows three examples beginning with clear inequality (top) and approaching equality (bottom). In the Euclidean case, equality occurs only if the triangle has a 180° angle and two 0° angles, making the three vertices collinear, as shown in the bottom example. Thus, in Euclidean geometry, the shortest distance between two points is a straight line.In spherical geometry, the shortest distance between two points is an arc of a great circle, but the triangle inequality holds provided the restriction is made that the distance between two points on a sphere is the length of a minor spherical line segment (that is, one with central angle in [0, π]) with those endpoints.[4][5]The triangle inequality is a defining property of norms and measures of distance. This property must be established as a theorem for any function proposed for such purposes for each particular space: for example, spaces such as the real numbers, Euclidean spaces, the Lp spaces (p ≥ 1), and inner product spaces.Euclid proved the triangle inequality for distances in plane geometry using the construction in the figure.[6] Beginning with triangle ABC, an isosceles triangle is constructed with one side taken as BC and the other equal leg BD along the extension of side AB. It then is argued that angle β > α, so side AD > AC. But AD = AB + BD = AB + BC so the sum of sides AB + BC > AC. This proof appears in Euclid's Elements, Book 1, Proposition 20.[7]For a proper triangle, the triangle inequality, as stated in words, literally translates into three inequalities (given that a proper triangle has side lengths a, b, c that are all positive and excludes the degenerate case of zero area):A more succinct form of this inequality system can be shown to beAnother way to state it isimplyingand thus that the longest side length is less than the semiperimeter.A mathematically equivalent formulation is that the area of a triangle with sides a, b, c must be a real number greater than zero. Heron's formula for the area isIn terms of either area expression, the triangle inequality imposed on all sides is equivalent to the condition that the expression under the square root sign be real and greater than zero (so the area expression is real and greater than zero).In the case of right triangles, the triangle inequality specializes to the statement that the hypotenuse is greater than either of the two sides, and less than their sum.[9]The second part of this theorem is already established above for any side of any triangle. The first part is established using the lower figure. In the figure, consider the right triangle ADC. An isosceles triangle ABC is constructed with equal sides AB = AC. From the triangle postulate, the angles in the right triangle ADC satisfy:Likewise, in the isosceles triangle ABC, the angles satisfy:Therefore,and so, in particular,That means side AD opposite angle α is shorter than side AB opposite the larger angle β. But AB = AC. Hence:A similar construction shows AC > DC, establishing the theorem.An alternative proof (also based upon the triangle postulate) proceeds by considering three positions for point B:[10] (i) as depicted (which is to be proven), or (ii) B coincident with D (which would mean the isosceles triangle had two right angles as base angles plus the vertex angle γ, which would violate the triangle postulate), or lastly, (iii) B interior to the right triangle between points A and D (in which case angle ABC is an exterior angle of a right triangle BDC and therefore larger than π/2, meaning the other base angle of the isosceles triangle also is greater than π/2 and their sum exceeds π in violation of the triangle postulate).This theorem establishing inequalities is sharpened by Pythagoras' theorem to the equality that the square of the length of the hypotenuse equals  the sum of the squares of the other two sides.Consider a triangle whose sides are in an arithmetic progression and let the sides be a, a + d, a + 2d. Then the triangle inequality requires thatTo satisfy all these inequalities requiresWhen d is chosen such that d = a/3, it generates a right triangle that is always similar to the Pythagorean triple with sides 3, 4, 5.Now consider a triangle whose sides are in a geometric progression and let the sides be a, ar, ar2. Then the triangle inequality requires thatThe first inequality requires a > 0; consequently it can be divided through and eliminated. With a > 0, the middle inequality only requires r > 0. This now leaves the first and third inequalities needing to satisfyThe first of these quadratic inequalities requires r to range in the region beyond the value of the positive root of the quadratic equation r2 + r − 1 = 0, i.e. r > φ − 1  where φ is the golden ratio. The second quadratic inequality requires r to range between 0 and the positive root of the quadratic equation r2 − r − 1 = 0, i.e. 0 < r < φ. The combined requirements result in r being confined to the rangeWhen r the common ratio is chosen such that r = √φ it generates a right triangle that is always similar to the Kepler triangle.The triangle inequality can be extended by mathematical induction to arbitrary polygonal paths, showing that the total length of such a path is no less than the length of the straight line between its endpoints. Consequently, the length of any polygon side is always less than the sum of the other polygon side lengths.Consider a quadrilateral whose sides are in a geometric progression and let the sides be a, ar, ar2, ar3. Then the generalized polygon inequality requires thatThese inequalities for a > 0 reduce to the followingThe left-hand side polynomials of these two inequalities have roots that are the tribonacci constant and its reciprocal. Consequently, r is limited to the range 1/t < r < t where t is the tribonacci constant.This generalization can be used to prove that the shortest curve between two points in Euclidean geometry is a straight line.No polygonal path between two points is shorter than the line between them. This implies that no curve can have an arc length less than the distance between its endpoints. By definition, the arc length of a curve is the least upper bound of the lengths of all polygonal approximations of the curve. The result for polygonal paths shows that the straight line between the endpoints is shortest of all the polygonal approximations. Because the arc length of the curve is greater than or equal to the length of every polygonal approximation, the curve itself cannot be shorter than the straight line path.[14]The converse of the triangle inequality theorem is also true: if three real numbers are such that each is less than the sum of the others, then there exists a triangle with these numbers as its side lengths and with positive area; and if one number equals the sum of the other two, there exists a degenerate triangle (i.e., with zero area) with these numbers as its side lengths.In either case, if the side lengths are a, b, c we can attempt to place a triangle in the Euclidean plane as shown in the diagram. We need to prove that there exists a real number h consistent with the values a, b, and c, in which case this triangle exists.By the Pythagorean theorem we have b2 = h2 + d2 and a2 = h2 + (c − d)2 according to the figure at the right. Subtracting these yields a2 − b2 = c2 − 2cd. This equation allows us to express d in terms of the sides of the triangle:For the height of the triangle we have that h2 = b2 − d2. By replacing d with the formula given above, we havewhich holds if the triangle inequality is satisfied for all sides. Therefore there does exist a real number h consistent with the sides a, b, c, and the triangle exists. If each triangle inequality holds strictly, h > 0 and the triangle is non-degenerate (has positive area); but if one of the inequalities holds with equality, so h = 0, the triangle is degenerate.In Euclidean space, the hypervolume of an (n − 1)-facet of an n-simplex is less than or equal to the sum of the hypervolumes of the other n facets.  In particular, the area of a triangular face of a tetrahedron is less than or equal to the sum of the areas of the other three sides.In a normed vector space V, one of the defining properties of the norm is the triangle inequality:that is, the norm of the sum of two vectors is at most as large as the sum of the norms of the two vectors.  This is also referred to as subadditivity. For any proposed function to behave as a norm, it must satisfy this requirement.[15]Proof:[16]After adding, The triangle inequality is useful in mathematical analysis for determining the best upper estimate on the size of the sum of two numbers, in terms of the sizes of the individual numbers.There is also a lower estimate, which can be found using the reverse triangle inequality which states that for any real numbers x and y:In a metric space M with metric d, the triangle inequality is a requirement upon distance:for all x, y, z in M. That is, the distance from x to z is at most as large as the sum of the distance from x to y and the distance from y to z.The triangle inequality is responsible for most of the interesting structure on a metric space, namely, convergence.  This is because the remaining requirements for a metric are rather simplistic in comparison.  For example, the fact that any convergent sequence in a metric space is a Cauchy sequence is a direct consequence of the triangle inequality, because if we choose any xn and xm such that d(xn, x) < ε/2 and d(xm, x) < ε/2, where ε > 0 is given and arbitrary (as in the definition of a limit in a metric space), then by the triangle inequality, d(xn, xm) ≤ d(xn, x) + d(xm, x) < ε/2 + ε/2 = ε, so that the sequence {xn} is a Cauchy sequence, by definition.This version of the triangle inequality reduces to the one stated above in case of normed vector spaces where a metric is induced via d(x, y) ≔ ‖x − y‖, with x − y being the vector pointing from point y to x.The reverse triangle inequality is an elementary consequence of the triangle inequality that gives lower bounds instead of upper bounds. For plane geometry the statement is:[19]In the case of a normed vector space, the statement is:Combining these two statements gives:In Minkowski space, if x and y are both timelike vectors lying in the future light cone, the triangle inequality is reversed:A physical example of this inequality is the twin paradox in special relativity. The same reversed form of the inequality holds if both vectors lie in the past light cone, and if one or both are null vectors. The result holds in n+1 dimensions for any n≥1.  If the plane defined by x and y is spacelike (and therefore a euclidean subspace) then the usual triangle inequality holds.
Orthogonality
In mathematics, orthogonality is the generalization of the notion of perpendicularity to the linear algebra of bilinear forms.  Two elements u and v of a vector space with bilinear form B are orthogonal when B(u, v) = 0.  Depending on the bilinear form, the vector space may contain nonzero self-orthogonal vectors. In the case of function spaces, families of orthogonal functions are used to form a basis.By extension, orthogonality is also used to refer to the separation of specific features of a system. The term also has specialized meanings in other fields including art and chemistry.The word comes from the Greek ὀρθός (orthos), meaning "upright", and γωνία (gonia), meaning "angle".The ancient Greek ὀρθογώνιον orthogōnion (< ὀρθός orthos 'upright'[1] + γωνία gōnia 'angle'[2]) and classical Latin orthogonium originally denoted a rectangle.[3] Later, they came to mean a right triangle.  In the 12th century, the post-classical Latin word orthogonalis came to mean a right angle or something related to a right angle.[4]A set of vectors in an inner product space is called pairwise orthogonal if each pairing of them is orthogonal. Such a set is called an orthogonal set.In certain cases, the word normal is used to mean orthogonal, particularly in the geometric sense as in the normal to a surface. For example, the y-axis is normal to the curve y = x2 at the origin.  However, normal may also refer to the magnitude of a vector. In particular, a set is called orthonormal (orthogonal plus normal) if it is an orthogonal set of unit vectors. As a result, use of the term normal to mean "orthogonal" is often avoided. The word "normal" also has a different meaning in probability and statistics.A vector space with a bilinear form generalizes the case of an inner product. When the bilinear form applied to two vectors results in zero, then they are orthogonal. The case of a pseudo-Euclidean plane uses the term hyperbolic orthogonality. In the diagram, axes x′ and t′ are hyperbolic-orthogonal for any given ϕ.In Euclidean space, two vectors are orthogonal if and only if their dot product is zero, i.e. they make an angle of 90° (π/2 radians), or one of the vectors is zero.[8] Hence orthogonality of vectors is an extension of the concept of perpendicular vectors to spaces of any dimension.The orthogonal complement of a subspace is the space of all vectors that are orthogonal to every vector in the subspace.  In a three-dimensional Euclidean vector space, the orthogonal complement of a line through the origin is the plane through the origin perpendicular to it, and vice versa.[9]Note that the geometric concept two planes being perpendicular does not correspond to the orthogonal complement, since in three dimensions a pair of vectors, one from each of a pair of perpendicular planes, might meet at any angle.In four-dimensional Euclidean space, the orthogonal complement of a line is a hyperplane and vice versa, and that of a plane is a plane.[9]By using integral calculus, it is common to use the following to define the inner product of two functions f and g with respect to a nonnegative weight function w over an interval [a, b]:In simple cases, w(x) = 1.We say that functions f and g are orthogonal if their inner product (equivalently, the value of this integral) is zero:Orthogonality of two functions with respect to one inner product does not imply orthogonality with respect to another inner product.We write the norm with respect to this inner product asThe members of a set of functions {fi : i = 1, 2, 3, ...} are orthogonal with respect to w on the interval [a, b] ifThe members of such a set of functions are orthonormal with respect to w on the interval [a, b] ifwhereis the Kronecker delta.In other words, every pair of them (excluding pairing of a function with itself) is orthogonal, and the norm of each is 1. See in particular the  orthogonal polynomials.In art, the perspective (imaginary) lines pointing to the vanishing point are referred to as "orthogonal lines".The term "orthogonal line" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as Piet Mondrian and Burgoyne Diller are noted for their exclusive use of "orthogonal lines" — not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay at the Web site of the Thyssen-Bornemisza Museum states that "Mondrian ... dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours." [1]Orthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results.[10] This usage was introduced by Van Wijngaarden in the design of Algol 68:The number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied “orthogonally” in order to maximize the expressive power of the language while trying to avoid deleterious superfluities.[11]Orthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the separation of concerns and encapsulation, and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them.An instruction set is said to be orthogonal if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task)[12] and is designed such that instructions can use any register in any addressing mode. This terminology results from considering an instruction as a vector whose components are the instruction fields.  One field identifies the registers to be operated upon and another specifies the addressing mode. An orthogonal instruction set uniquely encodes all combinations of registers and addressing modes.[citation needed]In communications, multiple-access schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different basis functions. One such scheme is TDMA, where the orthogonal basis functions are nonoverlapping rectangular pulses ("time slots").Another scheme is orthogonal frequency-division multiplexing (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other.  Well known examples include (a, g, and n) versions of 802.11 Wi-Fi; WiMAX; ITU-T G.hn, DVB-T, the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of ADSL.In OFDM, the subcarrier frequencies are chosen so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required.When performing statistical analysis, independent variables that affect a particular dependent variable are said to be orthogonal if they are uncorrelated,[13] since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables  individually with simple regression or simultaneously with multiple regression. If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions).One econometric formalism that is alternative to the maximum likelihood framework, the Generalized Method of Moments, relies on orthogonality conditions. In particular, the Ordinary Least Squares estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals.In taxonomy, an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive.In combinatorics, two n×n Latin squares are said to be orthogonal if their superimposition yields all possible n2 combinations of entries.[14]In synthetic organic chemistry orthogonal protection is a strategy allowing the deprotection of functional groups independently of each other. In chemistry and biochemistry, an orthogonal interaction occurs when there are two pairs of substances and each substance can interact with their respective partner, but does not interact with either substance of the other pair. For example, DNA has two orthogonal pairs: cytosine and guanine form a base-pair, and adenine and thymine form another base-pair, but other base-pair combinations are strongly disfavored. As a chemical example, tetrazine reacts with transcyclooctene and azide reacts with cyclooctyne without any cross-reaction, so these are mutually orthogonal reactions, and so, can be performed simultaneously and selectively.[15] Bioorthogonal chemistry refers to chemical reactions occurring inside living systems without reacting with naturally present cellular components. In supramolecular chemistry the notion of orthogonality refers to the possibility of two or more supramolecular, often non-covalent, interactions being compatible; reversibly forming without interference from the other.In analytical chemistry, analyses are "orthogonal" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement.  This is often required as a part of a new drug application.In the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure.In neuroscience, a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map.In board games such as chess which feature a grid of squares, 'orthogonal' is used to mean "in the same row/'rank' or column/'file'".  This is the counterpart to squares which are "diagonally adjacent".[16] In the ancient Chinese board game Go a player can capture the stones of an opponent by occupying all orthogonally-adjacent points.Stereo vinyl records encode both the left and right stereo channels in a single groove.  The V-shaped groove in the vinyl has walls that are 90 degrees to each other, with variations in each wall separately encoding one of the two analogue channels that make up the stereo signal.  The cartridge senses the motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side.[17]  A pure horizontal motion corresponds to a mono signal, equivalent to a stereo signal in which both channels carry identical (in-phase) signals.
Pohlke's theorem
Pohlke's theorem is the fundamental theorem of axonometry. It was established 1853 by the German painter and teacher of descriptive geometry Karl Wilhelm Pohlke. The first proof of the theorem was published 1864 by the German mathematician Hermann Amandus Schwarz, who was a student of Pohlke. Therefore the theorem is sometimes called theorem of Pohlke and Schwarz, too.Pohlke's theorem can be stated in terms of linear algebra as:Pohlke's theorem is the justification for the following easy procedure to construct a scaled parallel projection of a 3-dimensional object using coordinates,:[2][3]In order to get undistorted pictures, one has to choose the images of the axes and the forshortenings carefully (see Axonometry). In order to get an orthographic projection only the images of the axes are free and the forshortenings are determined. (see de:orthogonale Axonometrie).Schwarz formulated and proved the more general statement:and used a theorem of L’Huilier: 
Commutative ring
In ring theory, a branch of abstract algebra, a commutative ring is a ring in which the multiplication operation is commutative. The study of commutative rings is called commutative algebra. Complementarily, noncommutative algebra is the study of noncommutative rings where multiplication is not required to be commutative.A ring is a set R equipped with two binary operations, i.e. operations combining any two elements of the ring to a third. They are called addition and multiplication and commonly denoted by "+" and "⋅"; e.g. a + b and a ⋅ b. To form a ring these two operations have to satisfy a number of properties: the ring has to be an abelian group under addition as well as a monoid under multiplication, where multiplication distributes over addition; i.e., a ⋅ (b + c) = (a ⋅ b) + (a ⋅ c). The identity elements for addition and multiplication are denoted 0 and 1, respectively.If the multiplication is commutative, i.e.then the ring R is called commutative. In the remainder of this article, all rings will be commutative, unless explicitly stated otherwise.An important example, and in some sense crucial, is the ring of integers Z with the two operations of addition and multiplication. As the multiplication of integers is a commutative operation, this is a commutative ring. It is usually denoted Z as an abbreviation of the German word Zahlen (numbers).If R is a given commutative ring, then the set of all polynomials in the variable X whose coefficients are in R forms the polynomial ring, denoted R[X]. The same holds true for several variables.If V is some topological space, for example a subset of some Rn, real- or complex-valued continuous functions on V form a commutative ring. The same is true for differentiable or holomorphic functions, when the two concepts are defined, such as for V a complex manifold.In contrast to fields, where every nonzero element is multiplicatively invertible, the concept of divisibility for rings is richer. An element a of ring R is called a unit if it possesses a multiplicative inverse. Another particular type of element is the zero divisors, i.e. a non-zero element a such that there exists a non-zero element b of the ring such that ab = 0. If R possesses no zero divisors, it is called an integral domain (or domain). An element a satisfying an = 0 for some positive integer n is called nilpotent.The localization of a ring is a process in which some elements are rendered invertible, i.e. multiplicative inverses are added to the ring. Concretely, if S is a multiplicatively closed subset of R (i.e. whenever s, t ∈ S then so is st) then the localization of R at S, or ring of fractions with denominators in S, usually denoted S−1R consists of symbolssubject to certain rules that mimic the cancellation familiar from rational numbers. Indeed, in this language Q is the localization of Z at all nonzero integers. This construction works for any integral domain R instead of Z. The localization (R \ {0})−1R is a field, called the quotient field of R.Many of the following notions also exist for not necessarily commutative rings, but the definitions and properties are usually more complicated. For example, all ideals in a commutative ring are automatically two-sided, which simplifies the situation considerably.For a ring R, an R-module M is like what a vector space is to a field. That is, elements in a module can be added; they can be multiplied by elements of R subject to the same axioms as for a vector space. The study of modules is significantly more involved than the one of vector spaces in linear algebra, since several features of vector spaces fail for modules in general: modules need not be free, i.e., of the formEven for free modules, the rank of a free module (i.e. the analog of the dimension of vector spaces) may not be well-defined. Finally, submodules of finitely generated modules need not be finitely generated (unless R is Noetherian, see below).Ideals of a ring R are the submodules of R, i.e., the modules contained in R. In more detail, an ideal I is a non-empty subset of R such that for all r in R, i and j in I, both ri and i + j are in I. For various applications, understanding the ideals of a ring is of particular importance, but often one proceeds by studying modules in general.Any ring has two ideals, namely the zero ideal {0} and R, the whole ring. These two ideals are the only ones precisely if R is a field. Given any subset F = {fj}j ∈ J of R (where J is some index set), the ideal generated by F is the smallest ideal that contains F. Equivalently, it is given by finite linear combinationsIf F consists of a single element r, the ideal generated by F consists of the multiples of r, i.e., the elements of the form rs for arbitrary elements s. Such an ideal is called a principal ideal. If every ideal is a principal ideal, R is called a principal ideal ring; two important cases are Z and k[X], the polynomial ring over a field k. These two are in addition domains, so they are called principal ideal domains.Unlike for general rings, for a principal ideal domain, the properties of individual elements are strongly tied to the properties of the ring as a whole. For example, any principal ideal domain R is a unique factorization domain (UFD) which means that any element is a product of irreducible elements, in a (up to reordering of factors) unique way. Here, an element a in a domain is called irreducible if the only way of expressing it as a productis by either b or c being a unit. An example, important in field theory, are irreducible polynomials, i.e., irreducible elements in k[X], for a field k. The fact that Z is a UFD can be stated more elementarily by saying that any natural number can be uniquely decomposed as product of powers of prime numbers. It is also known as the fundamental theorem of arithmetic.An element a is a prime element if whenever a divides a product bc, a divides b or c. In a domain, being prime implies being irreducible. The converse is true in a unique factorization domain, but false in general.The definition of ideals is such that "dividing" I "out" gives another ring, the factor ring R / I: it is the set of cosets of I together with the operationsFor example, the ring Z/nZ (also denoted Zn), where n is an integer, is the ring of integers modulo n. It is the basis of modular arithmetic.An ideal is proper if it is strictly smaller than the whole ring. An ideal that is not strictly contained in any proper ideal is called maximal.  An ideal m is maximal if and only if R / m is a field.  Except for the zero ring, any ring (with identity) possesses at least one maximal ideal; this follows from Zorn's lemma.A ring is called Noetherian (in honor of Emmy Noether, who developed this concept) if every ascending chain of idealsbecomes stationary, i.e. becomes constant beyond some index n. Equivalently, any ideal is generated by finitely many elements, or, yet equivalent, submodules of finitely generated modules are finitely generated.Being Noetherian is a highly important finiteness condition, and the condition is preserved under many operations that occur frequently in geometry. For example, if R is Noetherian, then so is the polynomial ring R[X1, X2, ..., Xn] (by Hilbert's basis theorem), any localization S−1R, and also any factor ring R / I.Any non-noetherian ring R is the union of its Noetherian subrings. This fact, known as Noetherian approximation, allows the extension of certain theorems to non-Noetherian rings.A ring is called Artinian (after Emil Artin), if every descending chain of idealsbecomes stationary eventually. Despite the two conditions appearing symmetric, Noetherian rings are much more general than Artinian rings. For example, Z is Noetherian, since every ideal can be generated by one element, but is not Artinian, as the chainshows. In fact, by the Hopkins–Levitzki theorem, every Artinian ring is Noetherian. More precisely, Artinian rings can be characterized as the Noetherian rings whose Krull dimension is zero.As was mentioned above, Z is a unique factorization domain. This is not true for more general rings, as algebraists realized in the 19th century. For example, inthere are two genuinely distinct ways of writing 6 as a product:Any maximal ideal is a prime ideal or, more briefly, is prime. Moreover, an ideal I is prime if and only if the factor ring R / I is an integral domain. Proving that an ideal is prime, or equivalently that a ring has no zero-divisors can be very difficult. Yet another way of expressing the same is to say that the complement R \ p is multiplicatively closed. The localisation (R \ p)−1R is important enough to have its own notation: Rp. This ring has only one maximal ideal, namely pRp. Such rings are called local.The spectrum of a ring R,[nb 1]  denoted by Spec R, is the set of all prime ideals of R. It is equipped with a topology, the Zariski topology, which reflects the algebraic properties of R: a basis of open subsets is given byInterpreting f as a function that takes the value f mod p (i.e., the image of f in the residue field R/p), this subset is the locus where f is non-zero. The spectrum also makes precise the intuition that localisation and factor rings are complementary: the natural maps R → Rf and R → R / fR correspond, after endowing the spectra of the rings in question with their Zariski topology, to complementary open and closed immersions respectively. Even for basic rings, such as illustrated for R = Z at the right, the Zariski topology is quite different from the one on the set of real numbers.The spectrum contains the set of maximal ideals, which is occasionally denoted mSpec (R). For an algebraically closed field k, mSpec (k[T1, ..., Tn] / (f1, ..., fm)) is in bijection with the setThus, maximal ideals reflect the geometric properties of solution sets of polynomials, which is an initial motivation for the study of commutative rings. However, the consideration of non-maximal ideals as part of the geometric properties of a ring is useful for several reasons. For example, the minimal prime ideals (i.e., the ones not strictly containing smaller ones) correspond to the irreducible components of Spec R. For a Noetherian ring R, Spec R has only finitely many irreducible components. This is a geometric restatement of primary decomposition, according to which any ideal can be decomposed as a product of finitely many primary ideals. This fact is the ultimate generalization of the decomposition into prime ideals in Dedekind rings.The resulting equivalence of the two said categories aptly reflects algebraic properties of rings in a geometrical manner.Similar to the fact that manifolds are locally given by open subsets of Rn, affine schemes are local models for schemes, which are the object of study in algebraic geometry. Therefore, several notions concerning commutative rings stem from geometric intuition.The Krull dimension (or dimension) dim R of a ring R measures the "size" of a ring by, roughly speaking, counting independent elements in R. The dimension of algebras over a field k can be axiomatized by four properties:The dimension is defined, for any ring R, as the supremum of lengths n of chains of prime idealsFor example, a field is zero-dimensional, since the only prime ideal is the zero ideal. The integers are one-dimensional, since chains are of the form (0) ⊊ (p), where p is a prime number. For non-Noetherian rings, and also non-local rings, the dimension may be infinite, but Noetherian local rings have finite dimension. Among the four axioms above, the first two are elementary consequences of the definition, whereas the remaining two hinge on important facts in commutative algebra, the going-up theorem and Krull's principal ideal theorem.A ring homomorphism or, more colloquially, simply a map, is a map f : R → S such thatThese conditions ensure f(0) = 0. Similarly as for other algebraic structures, a ring homomorphism is thus a map that is compatible with the structure of the algebraic objects in question. In such a situation S is also called an R-algebra, by understanding that s in S may be multiplied by some r of R, by settingThe kernel and image of f are defined by ker (f) = {r ∈ R, f(r) = 0} and im (f) = f(R) = {f(r), r ∈ R}. The kernel is an ideal of R, and the image is a subring of S.A ring homomorphism is called an isomorphism if it is bijective. An example of a ring isomorphism, known as the Chinese remainder theorem, iswhere n = p1p2...pk is a product of pairwise distinct prime numbers.Commutative rings, together with ring homomorphisms, form a category. The ring Z is the initial object in this category, which means that for any commutative ring R, there is a unique ring homomorphism Z → R. By means of this map, an integer n can be regarded as an element of R. For example, the binomial formulawhich is valid for any two elements a and b in any commutative ring R is understood in this sense by interpreting the binomial coefficients as elements of R using this map.Given two R-algebras S and T, their tensor productis again a commutative R-algebra. In some cases, the tensor product can serve to find a T-algebra which relates to Z as S relates to R. For example,An R-algebra S is called finitely generated (as an algebra) if there are finitely many elements s1, ..., sn such that any element of s is expressible as a polynomial in the si. Equivalently, S is isomorphic toA much stronger condition is that S is finitely generated as an R-module, which means that any s can be expressed as a R-linear combination of some finite set s1, ..., sn.A ring is called local if it has only a single maximal ideal, denoted by m. For any (not necessarily local) ring R, the localizationat a prime ideal p is local. This localization reflects the geometric properties of Spec R "around p". Several notions and problems in commutative algebra can be reduced to the case when R is local, making local rings a particularly deeply studied class of rings. The residue field of R is defined asAny R-module M yields a k-vector space given by M / mM. Nakayama's lemma shows this passage is preserving important information: a finitely generated module M is zero if and only if M / mM is zero.The k-vector space m/m2 is an algebraic incarnation of the cotangent space. Informally, the elements of m can be thought of as functions which vanish at the point p, whereas m2 contains the ones which vanish with order at least 2. For any Noetherian local ring R, the inequalityholds true, reflecting the idea that the cotangent (or equivalently the tangent) space has at least the dimension of the space Spec R. If equality holds true in this estimate, R is called a regular local ring. A Noetherian local ring is regular if and only if the ring (which is the ring of functions on the tangent cone)is isomorphic to a polynomial ring over k. Broadly speaking, regular local rings are somewhat similar to polynomial rings.[1] Regular local rings are UFD's.[2]Discrete valuation rings are equipped with a function which assign an integer to any element r. This number, called the valuation of r can be informally thought of as a zero or pole order of r. Discrete valuation rings are precisely the one-dimensional regular local rings. For example, the ring of germs of holomorphic functions on a Riemann surface is a discrete valuation ring.By Krull's principal ideal theorem, a foundational result in the dimension theory of rings, the dimension ofis at least r − n. A ring R is called a complete intersection ring if it can be presented in a way that attains this minimal bound. This notion is also mostly studied for local rings. Any regular local ring is a complete intersection ring, but not conversely.A ring R is a set-theoretic complete intersection if the reduced ring associated to R, i.e., the one obtained by dividing out all nilpotent elements, is a complete intersection. As of 2017, it is in general unknown, whether curves in three-dimensional space are set-theoretic complete intersections.[3]The depth of a local ring R is the number of elements in some (or, as can be shown, any) maximal regular sequence, i.e., a sequence a1, ..., an ∈ m such that all ai are non-zero divisors inFor any local Noetherian ring, the inequalityholds. A local ring in which equality takes place is called a Cohen–Macaulay ring. Local complete intersection rings, and a fortiori, regular local rings are Cohen–Macaulay, but not conversely. Cohen–Macaulay combine desirable properties of regular rings (such as the property of being universally catenary rings, which means that the (co)dimension of primes is well-behaved), but are also more robust under taking quotients than regular local rings.[4]There are several ways to construct new rings out of given ones. The aim of such constructions is often to improve certain properties of the ring so as to make it more readily understandable. For example, an integral domain that is integrally closed in its field of fractions is called normal. This is a desirable property, for example any normal one-dimensional ring is necessarily regular. Rendering[clarification needed] a ring normal is known as normalization.If I is an ideal in a commutative ring R, the powers of I form topological neighborhoods of 0 which allow R to be viewed as a topological ring. This topology is called the I-adic topology. R can then be completed with respect to this topology. Formally, the I-adic completion is the inverse limit of the rings R/In. For example, if k is a field, k[[X]], the formal power series ring in one variable over k, is the I-adic completion of k[X] where I is the principal ideal generated by X. This ring serves as an algebraic analogue of the disk. Analogously, the ring of p-adic integers is the completion of Z  with respect to the principal ideal (p). Any ring that is isomorphic to its own completion, is called complete.Complete local rings satisfy Hensel's lemma, which roughly speaking allows extending solutions (of various problems) over the residue field k to R.Several deeper aspects of commutative rings have been studied using methods from homological algebra. Hochster (2007) lists some open questions in this area of active research.Projective modules can be defined to be the direct summands of free modules. If R is local, any finitely generated projective module is actually free, which gives content to an analogy between projective modules and vector bundles.[5] The Quillen–Suslin theorem asserts that any finitely generated projective module over k[T1, ..., Tn] (k a field) is free, but in general these two concepts differ. A local Noetherian ring is regular if and only if its global dimension is finite, say n, which means that any finitely generated R-module has a resolution by projective modules of length at most n.The proof of this and other related statements relies on the usage of homological methods, such as theExt functor. This functor is the derived functor of the functorThe latter functor is exact if M is projective, but not otherwise: for a surjective map E → F of R-modules, a map M → F need not extend to a map M → E. The higher Ext functors measure the non-exactness of the Hom-functor. The importance of this standard construction in homological algebra stems can be seen from the fact that a local Noetherian ring R with residue field k is regular if and only ifvanishes for all large enough n. Moreover, the dimensions of these Ext-groups, known as Betti numbers, grow polynomially in n if and only if R is a local complete intersection ring.[6] A key argument in such considerations is the Koszul complex, which provides an explicit free resolution of the residue field k of a local ring R in terms of a regular sequence.The tensor product is another non-exact functor relevant in the context of commutative rings: for a general R-module M, the functoris only right exact. If it is exact, M is called flat. Despite being defined in terms of homological algebra, flatness has profound geometric implications. For example, if an R-algebra S is flat, the dimensions of the fibers(for prime ideals p in R) have the "expected" dimension, namely dim S − dim R + dim (R / p).By Wedderburn's theorem, every finite division ring is commutative, and therefore a finite field. Another condition ensuring commutativity of a ring, due to Jacobson, is the following: for every element r of R there exists an integer n > 1 such that rn = r.[7] If, r2 = r for every r, the ring is called Boolean ring. More general conditions which guarantee commutativity of a ring are also known.[8]A graded ring R = ⨁i∊Z Ri is called graded-commutative ifIf the Ri are connected by differentials ∂ such that an abstract form of the product rule holds, i.e.,R is called a commutative differential graded algebra (cdga). An example is the complex of differential forms on a manifold, with the multiplication given by the exterior product, is a cdga. The cohomology of a cdga is a graded-commutative ring, sometimes referred to as the cohomology ring. A broad range examples of graded rings arises in this way. For example, the Lazard ring is the ring of cobordism classes of complex manifolds.A graded-commutative ring with respect to a grading  by Z/2 (as opposed to Z) is called a superalgebra.A related notion is an almost commutative ring, which means that R is filtered in such a way that the associated graded ringis commutative. An example is the Weyl algebra and more general rings of differential operators.A simplicial commutative ring is a simplicial object in the category of commutative rings. They are building blocks for (connective) derived algebraic geometry. A closely related but more general notion is that of E∞-ring.The ring of matrices is not commutative, since matrix multiplication fails to be commutative.[9]However, any two matrices A and B that do commute can be simultaneously diagonalized, i.e., there is an invertible matrix P such that both PAP−1 and PBP−1 are diagonal matrices. This fact makes representations of commutative Lie groups particularly simpler to understand than in general.An example is the set of matrices of divided differences with respect to a fixed set of nodes.
Quadratic form
In mathematics, a quadratic form is a homogeneous polynomial of degree two in a number of variables.  For example,is a quadratic form in the variables x and y.Quadratic forms occupy a central place in various branches of mathematics, including number theory, linear algebra, group theory (orthogonal group), differential geometry (Riemannian metric, second fundamental form), differential topology (intersection forms of four-manifolds), and Lie theory (the Killing form).Quadratic forms are homogeneous quadratic polynomials in n variables. In the cases of one, two, and three variables they are called unary, binary, and ternary and have the following explicit form:where a, ..., f are the coefficients.[1] Note that quadratic functions, such as ax2 + bx + c in the one variable case, are not quadratic forms, as they are typically not homogeneous (unless b and c are both 0).The theory of quadratic forms and methods used in their study depend in a large measure on the nature of the coefficients, which may be real or complex numbers, rational numbers, or integers. In linear algebra, analytic geometry, and in the majority of applications of quadratic forms, the coefficients are real or complex numbers. In the algebraic theory of quadratic forms, the coefficients are elements of a certain field. In the arithmetic theory of quadratic forms, the coefficients belong to a fixed commutative ring, frequently the integers Z or the p-adic integers Zp.[2] Binary quadratic forms have been extensively studied in number theory, in particular, in the theory of quadratic fields, continued fractions, and modular forms. The theory of integral quadratic forms in n variables has important applications to algebraic topology.Using homogeneous coordinates, a non-zero quadratic form in n variables defines an (n−2)-dimensional quadric in the (n−1)-dimensional projective space. This is a basic construction in projective geometry. In this way one may visualize 3-dimensional real quadratic forms as conic sections.A closely related notion with geometric overtones is a quadratic space, which is a pair (V, q), with V a vector space over a field K, and q : V → K a quadratic form on V. An example is given by the three-dimensional Euclidean space and the square of the Euclidean norm expressing the distance between a point with coordinates (x, y, z) and the origin:The study of particular quadratic forms, in particular the question of whether a given integer can be the value of a quadratic form over the integers, dates back many centuries. One such case is Fermat's theorem on sums of two squares, which determines when an integer may be expressed in the form x2 + y2, where x, y are integers. This problem is related to the problem of finding Pythagorean triples, which appeared in the second millennium B.C.[3]In 628, the Indian mathematician Brahmagupta wrote Brāhmasphuṭasiddhānta which includes, among many other things, a study of equations of the form x2 − ny2 = c. In particular he considered what is now called Pell's equation, x2 − ny2 = 1, and found a method for its solution.[4] In Europe this problem was studied by Brouncker, Euler and Lagrange.In 1801 Gauss published Disquisitiones Arithmeticae, a major portion of which was devoted to a complete theory of binary quadratic forms over the integers. Since then, the concept has been generalized, and the connections with quadratic number fields, the modular group, and other areas of mathematics have been further elucidated.Any n×n real symmetric matrix A determines a quadratic form qA in n variables by the formulaConversely, given a quadratic form in n variables, its coefficients can be arranged into an n × n symmetric matrix. An important question in the theory of quadratic forms is how to simplify a quadratic form q by a homogeneous linear change of variables.  A fundamental theorem due to Jacobi asserts that a real quadratic form q has an orthogonal diagonalization.[5]so that the corresponding symmetric matrix is diagonal, and this is accomplished with a change of variables given by an orthogonal matrix – in this case the coefficients λ1, λ2, ..., λn are determined uniquely up to a permutation.There always exists a change of variables given by an invertible matrix, not necessarily orthogonal, such that the coefficients λi are 0, 1, and −1. Sylvester's law of inertia states that the numbers of each 1 and −1 are invariants of the quadratic form, in the sense that any other diagonalization will contain the same number of each. The signature of the quadratic form is the triple (n0, n+, n−), where n0 is the number of 0s and n± is the number of ±1s. Sylvester's law of inertia shows that this is a well-defined quantity attached to the quadratic form. The case when all λi have the same sign is especially important: in this case the quadratic form is called positive definite (all 1) or negative definite (all −1).  If none of the terms are 0, then the form is called nondegenerate; this includes positive definite, negative definite, and indefinite (a mix of 1 and −1); equivalently, a nondegenerate quadratic form is one whose associated symmetric form is a nondegenerate bilinear form. A real vector space with an indefinite nondegenerate quadratic form of index (p, q) (denoting p 1s and q −1s) is often denoted as Rp,q particularly in the physical theory of spacetime.These results are reformulated in a different way below.Let q be a quadratic form defined on an n-dimensional real vector space. Let A be the matrix of the quadratic form q in a given basis. This means that A is a symmetric n × n matrix such thatwhere x is the column vector of coordinates of v in the chosen basis. Under a change of basis, the column x is multiplied on the left by an n × n invertible matrix S, and the symmetric square matrix A is transformed into another symmetric square matrix B of the same size according to the formulaAny symmetric matrix A can be transformed into a diagonal matrixby a suitable choice of an orthogonal matrix S, and the diagonal entries of B are uniquely determined – this is Jacobi's theorem. If S is allowed to be any invertible matrix then B can be made to have only 0,1, and −1 on the diagonal, and the number of the entries of each type (n0 for 0, n+ for 1, and n− for −1) depends only on A. This is one of the formulations of Sylvester's law of inertia and the numbers n+ and n− are called the positive and negative indices of inertia. Although their definition involved a choice of basis and consideration of the corresponding real symmetric matrix A, Sylvester's law of inertia means that they are invariants of the quadratic form q.The quadratic form q is positive definite (resp., negative definite) if q(v) > 0 (resp., q(v) < 0) for every nonzero vector v.[6] When q(v) assumes both positive and negative values, q is an indefinite quadratic form. The theorems of Jacobi and Sylvester show that any positive definite quadratic form in n variables can be brought to the sum of n squares by a suitable invertible linear transformation: geometrically, there is only one positive definite real quadratic form of every dimension. Its isometry group is a compact orthogonal group O(n). This stands in contrast with the case of indefinite forms, when the corresponding group, the indefinite orthogonal group O(p, q), is non-compact. Further, the isometry groups of Q and −Q are the same (O(p, q) ≈ O(q, p)), but the associated Clifford algebras (and hence pin groups) are different.An n-ary quadratic form over a field K is a homogeneous polynomial of degree 2 in n variables with coefficients in K:This formula may be rewritten using matrices: let x be the column vector with components x1, ..., xn and A = (aij) be the n×n matrix over K whose entries are the coefficients of q. ThenTwo n-ary quadratic forms φ and ψ over K are equivalent if there exists a nonsingular linear transformation C ∈ GL(n, K) such thatLet the characteristic of K be different from 2.[7] The coefficient matrix A of q may be replaced by the symmetric matrix (A + AT)/2 with the same quadratic form, so it may be assumed from the outset that A is symmetric. Moreover, a symmetric matrix A is uniquely determined by the corresponding quadratic form. Under an equivalence C, the symmetric matrix A of φ and the symmetric matrix B of ψ are related as follows:The associated bilinear form of a quadratic form q is defined byThus, bq is a symmetric bilinear form over K with matrix A. Conversely, any symmetric bilinear form b defines a quadratic formand these two processes are the inverses of one another.  As a consequence, over a field of characteristic not equal to 2, the theories of symmetric bilinear forms and of quadratic forms in n variables are essentially the same.A quadratic form q in n variables over K induces a map from the n-dimensional coordinate space Kn into K:The map Q is a homogeneous function of degree 2, which means that it has the property that, for all a in K and v in V:When the characteristic of K is not 2, the map B : V × V → K defined below is bilinear over K:This bilinear form B is symmetric, i.e. B(x, y) = B(y, x) for all x, y in V, and it determines Q: Q(x) = B(x, x)  for all x in V.When the characteristic of K is 2, so that 2 is not a unit, it is still possible to use a quadratic form to define a symmetric bilinear form B′(x, y) = Q(x + y) − Q(x) − Q(y). However, Q(x) can no longer be recovered from this B′ in the same way, since B′(x, x) = 0 for all x (and is thus alternating[8]). Alternately, there always exists a bilinear form B″ (not in general either unique or symmetric) such that B″(x, x) = Q(x).The pair (V, Q) consisting of a finite-dimensional vector space V over K and a quadratic map Q from V to K is called a quadratic space, and B as defined here is the associated symmetric bilinear form of Q. The notion of a quadratic space is a coordinate-free version of the notion of quadratic form. Sometimes, Q is also called a quadratic form.Two n-dimensional quadratic spaces (V, Q) and (V′, Q′) are isometric if there exists an invertible linear transformation T : V → V′ (isometry) such thatThe isometry classes of n-dimensional quadratic spaces over K correspond to the equivalence classes of n-ary quadratic forms over K.Let R be a commutative ring, M be an R-module and b : M × M → R be an R-bilinear form.[9]  A mapping Q : M → R : v ↦ b(v, v) is the associated quadratic form of b, and B : M × M → R : (u, v) ↦ Q(u + v) − Q(u) − Q(v) is the polar form of Q.Alternatively, a quadratic form Q : M → R may be characterized as follows:Two elements v and w of V are called orthogonal if B(v, w) = 0. The kernel of a bilinear form B consists of the elements that are orthogonal to every element of V. Q is non-singular if the kernel of its associated bilinear form is {0}. If there exists a non-zero v in V such that Q(v) = 0, the quadratic form Q is isotropic, otherwise it is anisotropic. This terminology also applies to vectors and subspaces of a quadratic space. If the restriction of Q to a subspace U of V is identically zero, U is totally singular.The orthogonal group of a non-singular quadratic form Q is the group of the linear automorphisms of V that preserve Q, i.e. the group of isometries of (V, Q) into itself.If a quadratic space (A, Q) has a product so that A is an algebra over a field, and satisfiesEvery quadratic form q in n variables over a field of characteristic not equal to 2 is equivalent to a diagonal formSuch a diagonal form is often denoted byClassification of all quadratic forms up to equivalence can thus be reduced to the case of diagonal forms.Quadratic forms over the ring of integers are called integral quadratic forms, whereas the corresponding modules are quadratic lattices (sometimes, simply lattices). They play an important role in number theory and topology.An integral quadratic form has integer coefficients, such as x2 + xy + y2; equivalently, given a lattice Λ in a vector space V (over a field with characteristic 0, such as Q or R), a quadratic form Q is integral with respect to Λ if and only if it is integer-valued on Λ, meaning Q(x, y) ∈ Z if x, y ∈ Λ.This is the current use of the term; in the past it was sometimes used differently, as detailed below.Historically there was some confusion and controversy over whether the notion of integral quadratic form should mean:This debate was due to the confusion of quadratic forms (represented by polynomials) and symmetric bilinear forms (represented by matrices), and "twos out" is now the accepted convention; "twos in" is instead the theory of integral symmetric bilinear forms (integral symmetric matrices).this is the convention Gauss uses in Disquisitiones Arithmeticae.Several points of view mean that twos out has been adopted as the standard convention. Those include:There are also forms whose image consists of all but one of the positive integers.  For example, {1,2,5,5} has 15 as the exception.  Recently, the 15 and 290 theorems have completely characterized universal integral quadratic forms: if all coefficients are integers, then it represents all positive integers if and only if it represents all integers up through 290; if it has an integral matrix, it represents all positive integers if and only if it represents all integers up through 15.
James Joseph Sylvester
James Joseph Sylvester FRS HFRSE LLD (3 September 1814 – 15 March 1897) was an English mathematician. He made fundamental contributions to matrix theory, invariant theory, number theory, partition theory, and combinatorics.  He played a leadership role in American mathematics in the later half of the 19th century as a professor at the Johns Hopkins University and as founder of the American Journal of Mathematics.  At his death, he was professor at Oxford.James Joseph was born in London on 3 September 1814, the son of Abraham Joseph, a merchant.[1]  James later adopted the surname Sylvester when his older brother did so upon emigration to the United States—a country which at that time required all immigrants to have a given name, a middle name, and a surname. At the age of 14, Sylvester was a student of Augustus De Morgan at the University of London. His family withdrew him from the University after he was accused of stabbing a fellow student with a knife. Subsequently, he attended the Liverpool Royal Institution.Sylvester began his study of mathematics at St John's College, Cambridge in 1831,[2] where his tutor was John Hymers. Although his studies were interrupted for almost two years due to a prolonged illness, he nevertheless ranked second in Cambridge's famous mathematical examination, the tripos, for which he sat in 1837. However, Sylvester was not issued a degree, because graduates at that time were required to state their acceptance of the Thirty-Nine Articles of the Church of England, and Sylvester could not do so because he was Jewish. For the same reason, he was unable to compete for a Fellowship or obtain a Smith's prize.[3] In 1838, Sylvester became professor of natural philosophy at University College London and in 1839 a Fellow of the Royal Society of London. In 1841, he was awarded a BA and an MA by Trinity College, Dublin. In the same year he moved to the United States to become a professor of mathematics at the University of Virginia, but left after less than four months following a violent encounter with two students he had disciplined. He moved to New York City and began friendships with the Harvard mathematician Benjamin Peirce (father of Charles Sanders Peirce) and the Princeton physicist Joseph Henry.  However, he left in November 1843 after being denied appointment as Professor of Mathematics at Columbia College (now University), again for his Judaism, and returned to England.On his return to England, he was hired in 1844 by the Equity and Law Life Assurance Society for which he developed successful actuarial models and served as de facto CEO, a position that required a law degree. As a result, he studied for the Bar, meeting a fellow British mathematician studying law, Arthur Cayley, with whom he made significant contributions to invariant theory and also matrix theory during a long collaboration.[4][incomplete short citation]  He did not obtain a position teaching university mathematics until 1855, when he was appointed professor of mathematics at the Royal Military Academy, Woolwich, from which he retired in 1869, because the compulsory retirement age was 55. The Woolwich academy initially refused to pay Sylvester his full pension, and only relented after a prolonged public controversy, during which Sylvester took his case to the letters page of The Times.One of Sylvester's lifelong passions was for poetry; he read and translated works from the original French, German, Italian, Latin and Greek, and many of his mathematical papers contain illustrative quotes from classical poetry. Following his early retirement, Sylvester (1870) published a book entitled The Laws of Verse in which he attempted to codify a set of laws for prosody in poetry.In 1872, he finally received his B.A. and M.A. from Cambridge, having been denied the degrees due to his being a Jew.[2]In 1876[5] Sylvester again crossed the Atlantic Ocean to become the inaugural professor of mathematics at the new Johns Hopkins University in Baltimore, Maryland. His salary was $5,000 (quite generous for the time), which he demanded be paid in gold.  After negotiation, agreement was reached on a salary that was not paid in gold.[6]  In 1878 he founded the American Journal of Mathematics.  The only other mathematical journal in the US at that time was the Analyst, which eventually became the Annals of Mathematics.In 1883, he returned to England to take up the Savilian Professor of Geometry at Oxford University. He held this chair until his death, although in 1892 the University appointed a deputy professor to the same chair.Sylvester died in London on 15 March 1897. He is buried in Balls Pond Road Jewish Cemetery on Kingsbury Road in London.[7]Sylvester invented a great number of mathematical terms such as "matrix" (in 1850),[8] "graph" (combinatorics)[9] and "discriminant".[10]  He coined the term "totient" for Euler's totient function φ(n).[11]  His collected scientific work fills four volumes. In 1880, the Royal Society of London awarded Sylvester the Copley Medal, its highest award for scientific achievement; in 1901, it instituted the Sylvester Medal in his memory, to encourage mathematical research after his death in Oxford. In Discrete geometry he is remembered for Sylvester's Problem and a result on the orchard problem.Sylvester House, a portion of an undergraduate dormitory at Johns Hopkins University, is named in his honor. Several professorships there are named in his honor also.
Field extension
Field extensions are fundamental in algebraic number theory, and in the study of polynomial roots through Galois theory, and are widely used in algebraic geometry.A subfield of a field L is a subset K of L that is a field with respect to the field operations inherited from L. Equivalently, a subfield is a subset that contains 1, and is closed under the operations of addition, subtraction, multiplication, and taking the inverse of a nonzero element of L.As 1 – 1 = 0, the latter definition implies K and L have the same zero element.For example, the field of rational numbers is a subfield of the real numbers, which is itself a subfield of the complex numbers. More generally, the field of rational numbers is (or is isomorphic to) a subfield of any field of characteristic 0.The characteristic of a subfield is the same as the characteristic of the larger field.If K is a subfield of L, then L is an extension field or simply extension of K, and this pair of fields is a field extension. Such a field extension is denoted L / K (read as "L over K").If L is an extension of F which is in turn an extension of K, then F is said to be an intermediate field (or intermediate extension or subextension) of L / K.Given a field extension L / K, the larger field L is a K-vector space. The dimension of this vector space is called the degree of the extension and is denoted by [L : K]. The degree of an extension is 1 if and only if the two fields are equal. In this case, the extension is a trivial extension. Extensions of degree 2 and 3 are called quadratic extensions and cubic extensions, respectively. A finite extension is an extension that has a finite degree. The degree of a finite extension L / K is denoted [L : K]Given two extensions L / K and M / L, the extension M / K is finite if and only if both L / K and M / L are finite. In this case, one hasAn extension field of the form K(S) is often said to result from the adjunction of S to K.[7][8]In characteristic 0, every finite extension is a simple extension. This is the primitive element theorem, which does not hold true for fields of non-zero characteristic.If a simple extension K(s) / K is not finite, the field K(s) is isomorphic to the field of rational fractions in s over K.The notation L / K is purely formal and does not imply the formation of a quotient ring or quotient group or any other kind of division. Instead the slash expresses the word "over". In some literature the notation L:K is used.It is often desirable to talk about field extensions in situations where the small field is not actually contained in the larger one, but is naturally embedded. For this purpose, one abstractly defines a field extension as an injective ring homomorphism between two fields.Every non-zero ring homomorphism between fields is injective because fields do not possess nontrivial proper ideals, so field extensions are precisely the morphisms in the category of fields.Henceforth, we will suppress the injective homomorphism and assume that we are dealing with actual subfields.The field The fieldBy iterating the above construction, one can construct a splitting field of  any polynomial from K[X]. This is an extension field L of K in which the given polynomial splits into a product of linear factors.Given a field K, we can consider the field K(X) of all rational functions in the variable X with coefficients in K; the elements of K(X) are fractions of two polynomials over K, and indeed K(X) is the field of fractions of the polynomial ring K[X]. This field of rational functions is an extension field of K. This extension is infinite.The set of the elements of L that are algebraic over K form a subextension, which is called the algebraic closure of K in L. This results from the preceding characterization: if s and t are algebraic, the extensions K(s) /K and K(s)(t) /K(s) are finite. Thus K(s, t) /K is also finite, as well as the sub extensions K(s ± t) /K, K(st) /K and K(1/s) /K (if s ≠ 0. It follows that s ± t, st and 1/s are all algebraic.A simple extension is algebraic if and only if it is finite. This implies that an extension is algebraic if and only if it is the union of its finite subextensions, and that every finite extension is algebraic. An algebraic extension L/K is called normal if every irreducible polynomial in K[X] that has a root in L completely factors into linear factors over L. Every algebraic extension F/K admits a normal closure L, which is an extension field of F such that L/K is normal and which is minimal with this property.An algebraic extension L/K is called separable if the minimal polynomial of every element of L over K is separable, i.e., has no repeated roots in an algebraic closure over K. A Galois extension is a field extension that is both normal and separable.A consequence of the primitive element theorem states that every finite separable extension has a primitive element (i.e. is simple).Given any field extension L/K, we can consider its automorphism group Aut(L/K), consisting of all field automorphisms α: L → L with α(x) = x for all x in K. When the extension is Galois this automorphism group is called the Galois group of the extension. Extensions whose Galois group is abelian are called abelian extensions.For a given field extension L/K, one is often interested in the intermediate fields F (subfields of L that contain K). The significance of Galois extensions and Galois groups is that they allow a complete description of the intermediate fields: there is a bijection between the intermediate fields and the subgroups of the Galois group, described by the fundamental theorem of Galois theory.Field extensions can be generalized to ring extensions which consist of a ring and one of its subrings. A closer non-commutative analog are central simple algebras (CSAs) – ring extensions over a field, which are simple algebra (no non-trivial 2-sided ideals, just as for a field) and where the center of the ring is exactly the field. For example, the only finite field extension of the real numbers is the complex numbers, while the quaternions are a central simple algebra over the reals, and all CSAs over the reals are Brauer equivalent to the reals or the quaternions. CSAs can be further generalized to Azumaya algebras, where the base field is replaced by a commutative local ring.Given a field extension, one can "extend scalars" on associated algebraic objects. For example, given a real vector space, one can produce a complex vector space via complexification. In addition to vector spaces, one can perform extension of scalars for associative algebras defined over the field, such as polynomials or group algebras and the associated group representations. Extension of scalars of polynomials is often used implicitly, by just considering the coefficients as being elements of a larger field, but may also be considered more formally. Extension of scalars has numerous applications, as discussed in extension of scalars: applications.
Euclidean vector
A vector is what is needed to "carry" the point A to the point B; the Latin word vector means "carrier".[4] It was first used by 18th century astronomers investigating planet rotation around the Sun.[5]  The magnitude of the vector is the distance between the two points and the direction refers to the direction of displacement from A to B. Many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. These operations and associated laws qualify Euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space.Vectors play an important role in physics: the velocity and acceleration of a moving object and the forces acting on it can all be described with vectors. Many other physical quantities can be usefully thought of as vectors. Although most of them do not represent distances (except, for example, position or displacement), their magnitude and direction can still be represented by the length and direction of an arrow. The mathematical representation of a physical vector depends on the coordinate system used to describe it. Other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include pseudovectors and tensors.The concept of vector, as we know it today, evolved gradually over a period of more than 200 years. About a dozen people made significant contributions.[6]Giusto Bellavitis abstracted the basic idea in 1835 when he established the concept of equipollence. Working in a Euclidean plane, he made equipollent any pair of line segments of the same length and orientation. Essentially he realized an equivalence relation on the pairs of points (bipoints) in the plane and thus erected the first space of vectors in the plane.[6]:52–4The term vector was introduced by William Rowan Hamilton as part of a quaternion, which is a sum q = s + v of a Real number s (also called scalar) and a 3-dimensional vector. Like Bellavitis, Hamilton viewed vectors as representative of classes of equipollent directed segments. As complex numbers use an imaginary unit to complement the real line, Hamilton considered the vector v to be the imaginary part of a quaternion:Several other mathematicians developed vector-like systems in the middle of the nineteenth century,  including Augustin Cauchy, Hermann Grassmann, August Möbius, Comte de Saint-Venant, and Matthew O'Brien. Grassmann's 1840 work Theorie der Ebbe und Flut (Theory of the Ebb and Flow) was the first system of spatial analysis similar to today's system and had ideas corresponding to the cross product, scalar product and vector differentiation. Grassmann's work was largely neglected until the 1870s.[6]Peter Guthrie Tait carried the quaternion standard after Hamilton. His 1867 Elementary Treatise of Quaternions included extensive treatment of the nabla or del operator ∇.In 1878 Elements of Dynamic was published by William Kingdon Clifford. Clifford simplified the quaternion study by isolating the dot product and cross product of two vectors from the complete quaternion product. This approach made vector calculations available to engineers and others working in three dimensions and skeptical of the fourth.Josiah Willard Gibbs, who was exposed to quaternions through James Clerk Maxwell's Treatise on Electricity and Magnetism, separated off their vector part for independent treatment. The first half of Gibbs's Elements of Vector Analysis, published in 1881, presents what is essentially the modern system of vector analysis.[6] In 1901 Edwin Bidwell Wilson published Vector Analysis, adapted from Gibb's lectures, which banished any mention of quaternions in the development of vector calculus.In physics and engineering, a vector is typically regarded as a geometric entity characterized by a magnitude and a direction. It is formally defined as a directed line segment, or arrow, in a Euclidean space.[8] In pure mathematics, a vector is defined more generally as any element of a vector space. In this context, vectors are abstract entities which may or may not be characterized by a magnitude and a direction. This generalized definition implies that the above-mentioned geometric entities are a special kind of vectors, as they are elements of a special kind of vector space called Euclidean space.This article is about vectors strictly defined as arrows in Euclidean space. When it becomes necessary to distinguish these special vectors from vectors as defined in pure mathematics, they are sometimes referred to as geometric, spatial, or Euclidean vectors.The term vector also has generalizations to higher dimensions and to more formal approaches with much wider applications.Since the physicist's concept of force has a direction and a magnitude, it may be seen as a vector. As an example, consider a rightward force F of 15 newtons. If the positive axis is also directed rightward, then F is represented by the vector 15 N, and if positive points leftward, then the vector for F is −15 N. In either case, the magnitude of the vector is 15 N. Likewise, the vector representation of a displacement Δs of 4 meters would be 4 m or −4 m, depending on its direction, and its magnitude would be 4 m regardless.Vectors are fundamental in the physical sciences. They can be used to represent any quantity that has magnitude, has direction, and which adheres to the rules of vector addition. An example is velocity, the magnitude of which is speed. For example, the velocity 5 meters per second upward could be represented by the vector (0, 5) (in 2 dimensions with the positive y-axis as 'up'). Another quantity represented by a vector is force, since it has a magnitude and direction and follows the rules of vector addition. Vectors also describe many other physical quantities, such as linear displacement, displacement, linear acceleration, angular acceleration, linear momentum, and angular momentum. Other physical vectors, such as the electric and magnetic field, are represented as a system of vectors at each point of a physical space; that is, a vector field. Examples of quantities that have magnitude and direction but fail to follow the rules of vector addition are angular displacement and electric current. Consequently, these are not vectors.In Cartesian coordinates a free vector may be thought of in terms of a corresponding bound vector, in this sense, whose initial point has the coordinates of the origin O = (0, 0, 0). It is then determined by the coordinates of that bound vector's terminal point. Thus the free vector represented by (1, 0, 0) is a vector of unit length pointing along the direction of the positive x-axis.This coordinate representation of free vectors allows their algebraic features to be expressed in a convenient numerical fashion. For example, the sum of the two (free) vectors (1, 2, 3) and (−2, 0, 4) is the (free) vectorIn the geometrical and physical settings, sometimes it is possible to associate, in a natural way, a length or magnitude and a direction to vectors. In addition, the notion of direction is strictly associated with the notion of an angle between two vectors. If the dot product of two vectors is defined—a scalar-valued product of two vectors—then it is also possible to define a length; the dot product gives a convenient algebraic characterization of both angle (a function of the dot product between any two non-zero vectors) and length (the square root of the dot product of a vector by itself). In three dimensions, it is further possible to define the cross product, which supplies an algebraic characterization of the area and orientation in space of the parallelogram defined by two vectors (used as sides of the parallelogram). In any dimension (and, in particular, higher dimensions), it's possible to define the exterior product, which (among other things) supplies an algebraic characterization of the area and orientation in space of the n-dimensional parallelotope defined by n vectors.However, it is not always possible or desirable to define the length of a vector in a natural way. This more general type of spatial vector is the subject of vector spaces (for free vectors) and affine spaces (for bound vectors, as each represented by an ordered pair of "points"). An important example is Minkowski space that is important to our understanding of special relativity, where there is a generalization of length that permits non-zero vectors to have zero length. Other physical examples come from thermodynamics, where many of the quantities of interest can be considered vectors in a space with no notion of length or angle.[10]In physics, as well as mathematics, a vector is often identified with a tuple of components, or list of numbers, that act as scalar coefficients for a set of basis vectors. When the basis is transformed, for example by rotation or stretching, then the components of any vector in terms of that basis also transform in an opposite sense. The vector itself has not changed, but the basis has, so the components of the vector must change to compensate. The vector is called covariant or contravariant depending on how the transformation of the vector's components is related to the transformation of the basis. In general, contravariant vectors are "regular vectors" with units of distance (such as a displacement) or distance times some other unit (such as velocity or acceleration); covariant vectors, on the other hand, have units of one-over-distance such as gradient. If you change units (a special case of a change of basis) from meters to millimeters, a scale factor of 1/1000, a displacement of 1 m becomes 1000 mm—a contravariant change in numerical value. In contrast, a gradient of 1 K/m becomes 0.001 K/mm—a covariant change in value. See covariance and contravariance of vectors. Tensors are another type of quantity that behave in this way; a vector is one type of tensor.In pure mathematics, a vector is any element of a vector space over some field and is often represented as a coordinate vector. The vectors described in this article are a very special case of this general definition because they are contravariant with respect to the ambient space. Contravariance captures the physical intuition behind the idea that a vector has "magnitude and direction".Vectors are usually shown in graphs or other diagrams as arrows (directed line segments), as illustrated in the figure. Here the point A is called the origin, tail, base, or initial point; point B is called the head, tip, endpoint, terminal point or final point. The length of the arrow is proportional to the vector's magnitude, while the direction in which the arrow points indicates the vector's direction.On a two-dimensional diagram, sometimes a vector perpendicular to the plane of the diagram is desired. These vectors are commonly shown as small circles. A circle with a dot at its centre (Unicode U+2299 ⊙) indicates a vector pointing out of the front of the diagram, toward the viewer. A circle with a cross inscribed in it (Unicode U+2297 ⊗) indicates a vector pointing into and behind the diagram. These can be thought of as viewing the tip of an arrow head on and viewing the flights of an arrow from the back.In order to calculate with vectors, the graphical representation may be too cumbersome. Vectors in an n-dimensional Euclidean space can be represented as coordinate vectors in a Cartesian coordinate system. The endpoint of a vector can be identified with an ordered list of n real numbers (n-tuple). These numbers are the coordinates of the endpoint of the vector, with respect to a given Cartesian coordinate system, and are typically called the scalar components (or scalar projections) of the vector on the axes of the coordinate system.As an example in two dimensions (see figure), the vector from the origin O = (0, 0) to the point A = (2, 3) is simply written asIn three dimensional Euclidean space (or R3), vectors are identified with triples of scalar components:This can be generalised to n-dimensional Euclidean space (or Rn).These numbers are often arranged into a column vector or row vector, particularly when dealing with matrices, as follows:Another way to represent a vector in n-dimensions is to introduce the standard basis vectors. For instance, in three dimensions, there are three of them:These have the intuitive interpretation as vectors of unit length pointing up the x-, y-, and z-axis of a Cartesian coordinate system, respectively. In terms of these, any vector a in R3 can be expressed in the form:orwhere a1, a2, a3 are called the vector components (or vector projections) of a on the basis vectors or, equivalently, on the corresponding Cartesian axes x, y, and z (see figure), while a1, a2, a3 are the respective scalar components (or scalar projections).The notation ei is compatible with the index notation and the summation convention commonly used in higher level mathematics, physics, and engineering.As explained above a vector is often described by a set of vector components that add up to form the given vector. Typically, these components are the projections of the vector on a set of mutually perpendicular reference axes (basis vectors). The vector is said to be decomposed or resolved with respect to that set.The decomposition or resolution[11] of a vector into components is not unique, because it depends on the choice of the axes on which the vector is projected.The choice of a basis does not affect the properties of a vector or its behaviour under transformations.A vector can also be broken up with respect to "non-fixed" basis vectors that change their orientation as a function of time or space. For example, a vector in three-dimensional space can be decomposed with respect to two axes, respectively normal, and tangent to a surface (see figure). Moreover, the radial and tangential components of a vector relate to the radius of rotation of an object. The former is parallel to the radius and the latter is orthogonal to it.[12]In these cases, each of the components may be in turn decomposed with respect to a fixed coordinate system or basis set (e.g., a global coordinate system, or inertial reference frame).The following section uses the Cartesian coordinate system with basis vectorsand assumes that all vectors have the origin as a common base point. A vector a will be written asTwo vectors are said to be equal if they have the same magnitude and direction. Equivalently they will be equal if their coordinates are equal. So two vectorsandare equal ifTwo vectors are opposite if they have the same magnitude but opposite direction.  So two vectorsandare opposite ifTwo vectors are parallel if they have the same direction but not necessarily the same magnitude, or antiparallel if they have opposite direction but not necessarily the same magnitude.Assume now that a and b are not necessarily equal vectors, but that they may have different magnitudes and directions. The sum of a and b isThe addition may be represented graphically by placing the tail of the arrow b at the head of the arrow a, and then drawing an arrow from the tail of a to the head of b. The new arrow drawn represents the vector a + b, as illustrated below:This addition method is sometimes called the parallelogram rule because a and b form the sides of a parallelogram and a + b is one of the diagonals. If a and b are bound vectors that have the same base point, this point will also be the base point of a + b. One can check geometrically that a + b = b + a and (a + b) + c = a + (b + c).The difference of a and b isSubtraction of two vectors can be geometrically defined as follows: to subtract b from a, place the tails of a and b at the same point, and then draw an arrow from the head of b to the head of a. This new arrow represents the vector a − b, as illustrated below:A vector may also be multiplied, or re-scaled, by a real number r. In the context of conventional vector algebra, these real numbers are often called scalars (from scale) to distinguish them from vectors. The operation of multiplying a vector by a scalar is called scalar multiplication. The resulting vector isIntuitively, multiplying by a scalar r stretches a vector out by a factor of r. Geometrically, this can be visualized (at least in the case when r is an integer) as placing r copies of the vector in a line where the endpoint of one vector is the initial point of the next vector.If r is negative, then the vector changes direction: it flips around by an angle of 180°. Two examples (r = −1 and r = 2) are given below:Scalar multiplication is distributive over vector addition in the following sense: r(a + b) = ra + rb for all vectors a and b and all scalars r. One can also show that a − b = a + (−1)b.The length or magnitude or norm of the vector a is denoted by ‖a‖ or, less commonly, |a|, which is not to be confused with the absolute value (a scalar "norm").The length of the vector a can be computed with the Euclidean normwhich is a consequence of the Pythagorean theorem since the basis vectors e1, e2, e3 are orthogonal unit vectors.This happens to be equal to the square root of the dot product, discussed below, of the vector with itself:A unit vector is any vector with a length of one; normally unit vectors are used simply to indicate direction. A vector of arbitrary length can be divided by its length to create a unit vector. This is known as normalizing a vector. A unit vector is often indicated with a hat as in â.To normalize a vector a = (a1, a2, a3), scale the vector by the reciprocal of its length ‖a‖. That is:The dot product of two vectors a and b (sometimes called the inner product, or, since its result is a scalar, the scalar product) is denoted by a ∙ b and is defined as:where θ is the measure of the angle between a and b (see trigonometric function for an explanation of cosine). Geometrically, this means that a and b are drawn with a common start point and then the length of a is multiplied with the length of the component of b that points in the same direction as a.The dot product can also be defined as the sum of the products of the components of each vector asThe cross product (also called the vector product or outer product) is only meaningful in three or seven dimensions. The cross product differs from the dot product primarily in that the result of the cross product of two vectors is a vector. The cross product, denoted a × b, is a vector perpendicular to both a and b and is defined aswhere θ is the measure of the angle between a and b, and n is a unit vector perpendicular to both a and b which completes a right-handed system. The right-handedness constraint is necessary because there exist two unit vectors that are perpendicular to both a and b, namely, n and (–n).The cross product a × b is defined so that a, b, and a × b also becomes a right-handed system (but note that a and b are not necessarily orthogonal). This is the right-hand rule.The length of a × b can be interpreted as the area of the parallelogram having a and b as sides.The cross product can be written asFor arbitrary choices of spatial orientation (that is, allowing for left-handed as well as right-handed coordinate systems) the cross product of two vectors is a pseudovector instead of a vector (see below).The scalar triple product (also called the box product or mixed triple product) is not really a new operator, but a way of applying the other two multiplication operators to three vectors. The scalar triple product is sometimes denoted by (a b c) and defined as:It has three primary uses. First, the absolute value of the box product is the volume of the parallelepiped which has edges that are defined by the three vectors. Second, the scalar triple product is zero if and only if the three vectors are linearly dependent, which can be easily proved by considering that in order for the three vectors to not make a volume, they must all lie in the same plane. Third, the box product is positive if and only if the three vectors a, b and c are right-handed.In components (with respect to a right-handed orthonormal basis), if the three vectors are thought of as rows (or columns, but in the same order), the scalar triple product is simply the determinant of the 3-by-3 matrix having the three vectors as rowsThe scalar triple product is linear in all three entries and anti-symmetric in the following sense:All examples thus far have dealt with vectors expressed in terms of the same basis, namely, the e basis {e1, e2, e3}. However, a vector can be expressed in terms of any number of different bases that are not necessarily aligned with each other, and still remain the same vector.  In the e basis, a vector a is expressed, by definition, asThe scalar components in the e basis are, by definition,In another orthnormal basis n = {n1, n2, n3} that is not necessarily aligned with e, the vector a is expressed asand the scalar components in the n basis are, by definition,The values of p, q, r, and u, v, w relate to the unit vectors in such a way that the resulting vector sum is exactly the same physical vector a in both cases.  It is common to encounter vectors known in terms of different bases (for example, one basis fixed to the Earth and a second basis fixed to a moving vehicle).  In such a case it is necessary to develop a method to convert between bases so the basic vector operations such as addition and subtraction can be performed.  One way to express u, v, w in terms of p, q, r is to use column matrices along with a direction cosine matrix containing the information that relates the two bases.  Such an expression can be formed by substitution of the above equations to formDistributing the dot-multiplication givesReplacing each dot product with a unique scalar givesand these equations can be expressed as the single matrix equationThis matrix equation relates the scalar components of a in the n basis (u,v, and w) with those in the e basis (p, q, and r).  Each matrix element cjk is the direction cosine relating nj to ek.[13] The term direction cosine refers to the cosine of the angle between two unit vectors, which is also equal to their dot product.[13] Therefore,By referring collectively to e1, e2, e3 as the e basis and to n1, n2, n3 as the n basis, the matrix containing all the cjk is known as the "transformation matrix from e to n", or the "rotation matrix from e to n" (because it can be imagined as the "rotation" of a vector from one basis to another), or the "direction cosine matrix from e to n"[13] (because it contains direction cosines).  The properties of a rotation matrix are such that its inverse is equal to its transpose. This means that the "rotation matrix from e to n" is the transpose of "rotation matrix from n to e".The properties of a direction cosine matrix, C are[14]:The advantage of this method is that a direction cosine matrix can usually be obtained independently by using Euler angles or a quaternion to relate the two vector bases, so the basis conversions can be performed directly, without having to work out all the dot products described above.By applying several matrix multiplications in succession, any vector can be expressed in any basis so long as the set of direction cosines is known relating the successive bases.[13]With the exception of the cross and triple products, the above formulae generalise to two dimensions and higher dimensions. For example, addition generalises to two dimensions asand in four dimensions asThe cross product does not readily generalise to other dimensions, though the closely related exterior product does, whose result is a bivector. In two dimensions this is simply a pseudoscalarA seven-dimensional cross product is similar to the cross product in that its result is a vector orthogonal to the two arguments; there is however no natural way of selecting one of the possible such products.Vectors have many uses in physics and other sciences.In abstract vector spaces, the length of the arrow depends on a dimensionless scale. If it represents, for example, a force, the "scale" is of physical dimension length/force. Thus there is typically consistency in scale among quantities of the same dimension, but otherwise scale ratios may vary; for example, if "1 newton" and "5 m" are both represented with an arrow of 2 cm, the scales are 1 m:50 N and 1:250 respectively. Equal length of vectors of different dimension has no particular significance unless there is some proportionality constant inherent in the system that the diagram represents. Also length of a unit vector (of dimension length, not length/force, etc.) has no coordinate-system-invariant significance.Often in areas of physics and mathematics, a vector evolves in time, meaning that it depends on a time parameter t. For instance, if r represents the position vector of a particle, then r(t) gives a parametric representation of the trajectory of the particle. Vector-valued functions can be differentiated and integrated by differentiating or integrating the components of the vector, and many of the familiar rules from calculus continue to hold for the derivative and integral of vector-valued functions.The position of a point x = (x1, x2, x3) in three-dimensional space can be represented as a position vector whose base point is the originThe position vector has dimensions of length.Given two points x = (x1, x2, x3), y = (y1, y2, y3) their displacement is a vectorwhich specifies the position of y relative to x. The length of this vector gives the straight-line distance from x to y. Displacement has the dimensions of length.The velocity v of a point or particle is a vector, its length gives the speed. For constant velocity the position at time t will bewhere x0 is the position at time t = 0. Velocity is the time derivative of position. Its dimensions are length/time.Acceleration a of a point is vector which is the time derivative of velocity. Its dimensions are length/time2.Force is a vector with dimensions of mass×length/time2 and Newton's second law is the scalar multiplicationWork is the dot product of force and displacementTherefore, any directional derivative can be identified with a corresponding vector, and any vector can be identified with a corresponding directional derivative. A vector can therefore be defined precisely asIn the language of differential geometry, the requirement that the components of a vector transform according to the same matrix of the coordinate transition is equivalent to defining a contravariant vector to be a tensor of contravariant rank one. Alternatively, a contravariant vector is defined to be a tangent vector, and the rules for transforming a contravariant vector follow from the chain rule.Some vectors transform like contravariant vectors, except that when they are reflected through a mirror, they flip and gain a minus sign. A transformation that switches right-handedness to left-handedness and vice versa like a mirror does is said to change the orientation of space. A vector which gains a minus sign when the orientation of space changes is called a pseudovector or an axial vector. Ordinary vectors are sometimes called true vectors or polar vectors to distinguish them from pseudovectors. Pseudovectors occur most frequently as the cross product of two ordinary vectors.One example of a pseudovector is angular velocity. Driving in a car, and looking forward, each of the wheels has an angular velocity vector pointing to the left. If the world is reflected in a mirror which switches the left and right side of the car, the reflection of this angular velocity vector points to the right, but the actual angular velocity vector of the wheel still points to the left, corresponding to the minus sign. Other examples of pseudovectors include magnetic field, torque, or more generally any cross product of two (true) vectors.This distinction between vectors and pseudovectors is often ignored, but it becomes important in studying symmetry properties. See parity (physics).
Linear complementarity problem
In mathematical optimization theory, the linear complementarity problem (LCP) arises frequently in computational mechanics and encompasses the well-known quadratic programming as a special case.  It was proposed by Cottle and Dantzig in 1968.[1][2][3]Given a real matrix M and vector q, the linear complementarity problem LCP(M, q) seeks vectors z and w which satisfy the following constraints:A sufficient condition for existence and uniqueness of a solution to this problem is that M be symmetric positive-definite. If M is such that LCP(M, q) have a solution for every q, then M is a Q-matrix. If M is such that LCP(M, q) have a unique solution for every q, then M is a P-matrix. Both of these characterizations are sufficient and necessary.[4]The vector w is a slack variable,[5] and so is generally discarded after z is found. As such, the problem can also be formulated as:Finding a solution to the linear complementarity problem is associated with minimizing the quadratic functionsubject to the constraintsThese constraints ensure that f is always non-negative. The minimum of f is 0 at z if and only if z solves the linear complementarity problem.If M is positive definite, any algorithm for solving (strictly) convex QPs can solve the LCP.  Specially designed basis-exchange pivoting algorithms, such as Lemke's algorithm and a variant of the simplex algorithm of Dantzig have been used for decades. Besides having polynomial time complexity, interior-point methods are also effective in practice.is the same as solving the LCP withThis is because the Karush–Kuhn–Tucker conditions of the QP problem can be written as:with v the Lagrange multipliers on the non-negativity constraints, λ the multipliers on the inequality constraints, and s the slack variables for the inequality constraints. The fourth condition derives from the complementarity of each group of variables (x, s) with its set of KKT vectors (optimal Lagrange multipliers) being (v, λ). In that case,If the non-negativity constraint on the x is relaxed, the dimensionality of the LCP problem can be reduced to the number of the inequalities, as long as Q is non-singular (which is guaranteed if it is positive definite). The multipliers v are no longer present, and the first KKT conditions can be rewritten as:or:pre-multiplying the two sides by A and subtracting b we obtain:The left side, due to the second KKT condition, is s. Substituting and reordering:Calling nowwe have an LCP, due to the relation of complementarity between the slack variables s and their Lagrange multipliers λ. Once we solve it, we may obtain the value of x from λ  through the first KKT condition.Finally, it is also possible to handle additional equality constraints:From λ we can now recover the values of both x and the Lagrange multiplier of equalities μ:In fact, most QP solvers work on the LCP formulation, including the interior point method, principal / complementarity pivoting, and active set methods.[1][2] LCP problems can be solved also by the criss-cross algorithm,[6][7][8][9] conversely, for linear complementarity problems, the criss-cross algorithm terminates finitely only if the matrix is a sufficient matrix.[8][9] A sufficient matrix is a generalization both of a positive-definite matrix and of a P-matrix, whose principal minors are each positive.[8][9][10]Such LCPs can be solved when they are formulated abstractly using oriented-matroid theory.[11][12][13]
Affine space
In mathematics, an affine space is a geometric structure that generalizes some of the properties of Euclidean spaces in such a way that these are independent of the concepts of distance and measure of angles, keeping only the properties related to parallelism and ratio of lengths for parallel line segments.In an affine space, there is no distinguished point that serves as an origin. Hence, no vector has a fixed origin and no vector can be uniquely associated to a point. In an affine space, there are instead displacement vectors, also called translation vectors or simply translations, between two points of the space.[1] Thus it makes sense to subtract two points of the space, giving a translation vector, but it does not make sense to add two points of the space. Likewise, it makes sense to add a displacement vector to a point of an affine space, resulting in a new point translated from the starting point by that vector.Any vector space may be considered as an affine space, and this amounts to forgetting the special role played by the zero vector. In this case, the elements of the vector space may be viewed either as points of the affine space or as displacement vectors or translations. When considered as a point, the zero vector is called the origin. Adding a fixed vector to the elements of a linear subspace of a vector space produces an affine subspace. One commonly says that this affine subspace has been obtained by translating (away from the origin) the linear subspace by the translation vector. In finite dimensions, such an affine subspace is the solution set of an inhomogeneous linear system. The displacement vectors for that affine space are the solutions of the corresponding homogeneous linear system, which is a linear subspace. Linear subspaces, in contrast, always contain the origin of the vector space.The dimension of an affine space is defined as the dimension of the vector space of its translations. An affine space of dimension one is an affine line. An affine space of dimension 2 is an affine plane. An affine subspace of dimension n – 1 in an affine space or a vector space of dimension n is an affine hyperplane.The following characterization may be easier to understand than the usual formal definition: an affine space is what is left of a vector space after you've forgotten which point is the origin (or, in the words of the French mathematician Marcel Berger, "An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps"[2]). Imagine that Alice knows that a certain point is the actual origin, but Bob believes that another point — call it p — is the origin. Two vectors, a and b, are to be added. Bob draws an arrow from point p to point a and another arrow from point p to point b, and completes the parallelogram to find what Bob thinks is a + b, but Alice knows that he has actually computedSimilarly, Alice and Bob may evaluate any linear combination of a and b, or of any finite set of vectors, and will generally get different answers. However, if the sum of the coefficients in a linear combination is 1, then Alice and Bob will arrive at the same answer.If Alice travels tothen Bob can similarly travel toUnder this condition, for all coefficients λ + (1 − λ) = 1, Alice and Bob describe the same point with the same linear combination, despite using different origins.While only Alice knows the "linear structure", both Alice and Bob know the "affine structure"—i.e. the values of affine combinations, defined as linear combinations in which the sum of the coefficients is 1. A set with an affine structure is an affine space.that has the following properties.[4][5][6]The first two properties are simply defining properties of a (right) group action. The third property characterizes free and transitive actions, the onto character coming from transitivity, and then the injective character follows from the action being free. There is a fourth property that follows from 1, 2, 3 above:Property 3 is often used in the following equivalent form.Another way to express the definition is that an affine space is a principal homogeneous space for the action of the additive group of a vector space. Homogeneous spaces are by definition endowed with a transitive group action, and for a principal homogeneous space such a transitive action is by definition free.Existence follows from the transitivity of the action, and uniqueness follows because the action is free.This subtraction has the two following properties, called Weyl's axioms:[7]In Euclidean geometry, the second Weyl's axiom is commonly called the parallelogram rule.The affine subspaces of A are the subsets of A of the formThe linear subspace associated with an affine subspace is often called its direction, and two subspaces that share the same direction are said to be parallel.This implies the following generalization of  Playfair's axiom: Given a direction V, for any point a of A there is one and only one affine subspace of direction V, which passes through a, namely the subspace a + V.The term parallel is also used for two affine subspaces such that the direction of one is included in the direction of the other.such thatEvery vector space V may be considered as an affine space over itself. This means that every element of V may be considered either as a point or as a vector. This affine set is sometimes denoted (V, V) for emphasizing the double role of the elements of V. When considered as a point, the zero vector is commonly denoted o (or O, when upper-case letters are used for points) and called the origin.Euclidean spaces (including the one-dimensional line, two-dimensional plane, and three-dimensional space commonly studied in elementary geometry, as well as higher-dimensional analogues) are affine spaces.Indeed, in most modern definitions, a Euclidean space is defined to be an affine space, such that the associated vector space is a real inner product space of finite dimension, that is a vector space over the reals with a positive-definite quadratic form q(x). The inner product of two vectors x and y is the value of the symmetric bilinear formThe usual Euclidean distance between two points A and B is In older definition of Euclidean spaces through synthetic geometry, vectors are defined as equivalence classes of ordered pairs of points under equipollence (the pairs (A, B) and (C, D) are equipollent if the points A, B, D, C (in this order) form a parallelogram). It is straightforward to verify that the vectors form a vector space, the square of the Euclidean distance is a quadratic form on the space of vectors, and the two definitions of Euclidean spaces are equivalent.In Euclidean geometry, the common phrase "affine property" refers to a property that can be proved in affine spaces, that is, it can be proved without using the quadratic form and its associated inner product. In other words, an affine property is a property that does not involve lengths and angles. Typical examples are parallelism, and the definition of a tangent. A non-example is the definition of a normal.Equivalently, an affine property is a property that is invariant under affine transformations of the Euclidean space.Thus this sum is independent of the choice of the origin, and the resulting vector is denotedone writesFor any subset X of an affine space A, there is a smallest affine subspace that contains it, called the affine span of X. It is the intersection of all affine subspaces containing X, and its direction is the intersection of the directions of the affine subspaces that contain X.The affine span of X is the set of all (finite) affine combinations of points of X, and its direction is the linear span of the x − y for x and y in X. If one chooses a particular point x0, the direction of the affine span of X is also the linear span of the x – x0 for x in X.One says also that the affine span of X is generated by X and that X is a generating set of its affine span.A set X of points of an affine space is said affinely independent or, simply, independent, if the affine span of any strict subset of X is a strict subset of the affine span of X. An affine basis, or barycentric frame (see § Barycentric coordinates, below) of an affine space is a generating set that is also independent (that is a minimal generating set).Recall the dimension of an affine space is the dimension of its associated vector space. The bases of an affine space of finite dimension n are the independent subsets of n + 1 elements, or, equivalently, the generating subsets of n + 1 elements. Equivalently, {x0, …, xn} is an affine basis of an affine space if and only if {x1 − x0, …, xn − x0} is a linear basis of the associated vector space.There are two strongly related kinds of coordinate systems that may be defined on affine spaces.andFor affine spaces of infinite dimension, the same definition applies, using only finite sums. This means that for each point, only a finite number of coordinates are non-zero.or equivalentlyExample: In Euclidean geometry, Cartesian coordinates are affine coordinates relative to an orthonormal frame, that is an affine frame (o, v1, …, vn) such that (v1, …, vn) is an orthonormal basis.Barycentric coordinates and affine coordinates are strongly related, and may be considered as equivalent.In fact, given a barycentric frameone deduces immediately the affine frameand, ifare the barycentric coordinates of a point over the barycentric frame, then the affine coordinates of the same point over the affine frame areConversely, ifis an affine frame, thenis a barycentric frame. Ifare the affine coordinates of a point over the affine frame, then its barycentric coordinates over the barycentric frame areTherefore, barycentric and affine coordinates are almost equivalent. In most applications, affine coordinates are preferred, as involving less coordinates that are independent. However, in the situations where the important points of the studied problem are affinity independent, barycentric coordinates may lead to simpler computation, as in the following example.The vertices of a non-flat triangle form an affine basis of the Euclidean plane. The barycentric coordinates allows easy characterization of the elements of the triangle that do not involve angles or distance:The vertices are the points of barycentric coordinates (1, 0, 0),  (0, 1, 0) and  (0, 0, 1). The lines supporting the edges are the points that have a zero coordinate. The edges themselves are the points that have a zero coordinate and two nonnegative coordinates. The interior of the triangle are the points whose all coordinates are positive. The medians are the points that have two equal coordinates, and the centroid is the point of coordinates (1/3, 1/3, 1/3).Letbe an affine homomorphism, withas associated linear map.An important example is the projection parallel to some direction onto an affine subspace. The importance of this example lies in the fact that Euclidean spaces are affine spaces, and that this kind of projections is fundamental in Euclidean geometry.for x and y in E.The image of this projection is  F, and its fibers are the subspaces of direction  D.Although kernels are not defined for affine spaces, quotient spaces are defined. This results from the fact that "belonging to the same fiber of an affine homomorphism" is an equivalence relation.Affine space is usually studied as analytic geometry using coordinates, or equivalently vector spaces. It can also be studied as synthetic geometry by writing down axioms, though this approach is much less common. There are several different systems of axioms for affine space.Coxeter (1969, p. 192) axiomatizes affine geometry (over the reals) as ordered geometry together with an affine form of Desargues's theorem and an axiom stating that in a plane there is at most one line through a given point not meeting a given line.Affine planes satisfy the following axioms (Cameron 1991, chapter 2):(in which two lines are called parallel if they are equal ordisjoint):As well as affine planes over fields (or division rings), there are also many non-Desarguesian planes satisfying these axioms. (Cameron 1991, chapter 3) gives axioms for higher-dimensional affine spaces.Affine spaces are subspaces of projective spaces: an affine plane can be obtained from any projective plane by removing a line and all the points on it, and conversely any affine plane can be used to construct a projective plane as a closure by adding a line at infinity whose points correspond to equivalence classes of parallel lines.Further, transformations of projective space that preserve affine space (equivalently, that leave the hyperplane at infinity invariant as a set) yield transformations of affine space. Conversely, any affine linear transformation extends uniquely to a projective linear transformation, so the affine group is a subgroup of the projective group. For instance, Möbius transformations (transformations of the complex projective line, or Riemann sphere) are affine (transformations of the complex plane) if and only if they fix the point at infinity.In algebraic geometry, an affine variety (or, more generally, an affine algebraic set) is defined as the subset of an affine space that is the set of the common zeros of a set of so-called polynomial functions over the affine space. For defining a polynomial function over the affine space, one has to choose an affine coordinate system. Then, a polynomial function is a function such that the image of any point is the value of some multivariate polynomial function of the coordinates of the point. As a change of affine coordinates may be expressed by linear functions (more precisely affine functions) of the coordinates, this definition is independent of a particular choice of coordinates.As the whole affine space is the set of the common zeros of the zero polynomial, affine spaces are affine algebraic varieties.Affine spaces over topological fields, such as the real or the complex numbers, have a natural topology. The Zariski topology, which is defined for affine spaces over any field, allows use of topological methods in any case. Zariski topology is the unique topology on an affine space whose closed sets are affine algebraic sets (that is sets of the common zeros of polynomials functions over the affine set). As, over a topological field, polynomial functions are continuous, every Zariski closed set is closed for the usual topology, if any. In other words, over a topological field, Zariski topology is coarser than the natural topology.The case of an algebraically closed ground field is especially important in algebraic geometry, because, in this case, the homeomorphism above is a map between the affine space and the set of all maximal ideals of the ring of functions (this is Hilbert's Nullstellensatz).This is the starting idea of scheme theory of Grothendieck, which consists, for studying algebraic varieties, of considering as "points", not only the points of the affine space, but also all the prime ideals of the spectrum. This allows gluing together algebraic varieties in a similar way as, for manifolds, charts are glued together for building a manifold.
Kempner series
The Kempner series is a modification of the harmonic series, formed by omitting all terms whose denominator expressed in base 10 contains the digit 9. That is, it is the sumwhere the prime indicates that n takes only values whose decimal expansion has no nines. The series was first studied by A. J. Kempner in 1914.[1] The series is counter-intuitive because, unlike the harmonic series, it converges. Kempner showed the sum of this series is less than 80. Baillie[2] showed that, rounded to 20 decimals, the actual sum is 22.92067 66192 64150 34816(sequence A082838 in the OEIS).Heuristically, this series converges because most large integers contain all digits. For example, a random 100-digit integer is very likely to contain at least one '9', causing it to be excluded from the above sum.Schmelzer and Baillie[3] found an efficient algorithm for the more general problem of any omitted string of digits. For example, the sum of 1/n where n has no "42" is about 228.44630 41592 30813 25415. Another example: the sum of 1/n where n has no occurrence of the digit string "314159" is about 2302582.33386 37826 07892 02376. (All values are rounded in the last decimal place).Kempner's proof of convergence[1] is repeated in many textbooks, for example Hardy and Wright[4]:120 and Apostol.[5]:212 We group the terms of the sum by the number of digits in the denominator. The number of n-digit positive integers that have no digit equal to '9' is 8(9n−1) because there are 8 choices (1 through 8) for the first digit, and 9 independent choices (0 through 8) for each of the other n−1 digits. Each of these numbers having no '9' is greater than or equal to 10n−1, so the reciprocal of each of these numbers is less than or equal to 101−n. Therefore, the contribution of this group to the sum of reciprocals is less than 8(9/10)n−1. Therefore the whole sum of reciprocals is at mostThe same argument works for any omitted non-zero digit. The number of n-digit positive integers that have no '0' is 9n, so the sum of 1/n where n has no digit '0' is at mostThe series also converge if strings of k digits are omitted, for example if we omit all denominators that have a decimal substring of 42. This can be proved in almost the same way.[3] First we observe that we can work with numbers in base 10k and omit all denominators that have the given string as a "digit". The analogous argument to the base 10 case shows that this series converges. Now switching back to base 10, we see that this series contains all denominators that omit the given string, as well as denominators that include it if it is not on a "k-digit" boundary. For example, if we are omitting 42, the base-100 series would omit 4217 and 1742, but not 1427, so it is larger than the series that omits all 42s.Farhi[6] considered generalized Kempner series, namely, the sums S(d, n) of the reciprocals of the positive integers that have exactly n instances of the digit d where 0 ≤ d ≤ 9 (so that the original Kempner series is S(9, 0)). He showed that for each d the sequence of values S(d, n) for n ≥ 1 is decreasing and converges to 10 ln 10. The sequence is not in general decreasing starting with n = 0; for example, for the original Kempner series we have S(9, 0) ≈ 22.921 < 23.026 ≈ 10 ln 10 < S(9, n) for n ≥ 1.The series converges extremely slowly. Baillie[2] remarks that after summing 1027 terms the remainder is still larger than 1.The upper bound of 80 is very crude, and Irwin showed[7] by a slightly finer analysis of the bounds that the value of the Kempner series is near 23, since refined to about 22.92067.[8]Baillie[2] developed a recursion that expresses the contribution from each k+1-digit block in terms of the contributions of the k-digit blocks for all choices of omitted digit. This permits a very accurate estimate with a small amount of computation.Most authors do not name this series. The name "Kempner series" is used in MathWorld[9] and in Havil's book Gamma on the Euler–Mascheroni constant.[10]:31–33
Kernel (linear algebra)
In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L : V → W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W.  That is, in set-builder notation,The kernel of L is a linear subspace of the domain V.[1]In the linear map L : V → W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:It follows that the image of L is isomorphic to the quotient of V by the kernel:This implies the rank–nullity theorem:where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L).  This is the generalization to linear operators of the row space, or coimage, of a matrix.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring.The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L: V → W is continuous if and only if the kernel of L is a closed subspace of V.Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K.The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,The matrix equation is equivalent to a homogeneous system of linear equations:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The kernel of an m × n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:The product Ax can be written in terms of the dot product of vectors as follows:Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank–nullity theoremThe left null space, or cokernel, of a matrix A consists of all vectors x such that xTA = 0T, where T denotes the transpose of a column vector.  The left null space of A is the same as the kernel of AT.  The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation.  The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The kernel also plays a role in the solution to a nonhomogeneous system of linear equations:If u and v are two possible solutions to the above equation, thenThus, the difference of any two solutions to the equation Ax = b lies in the kernel of A.It follows that any solution to the equation Ax = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel.  That is, the solution set to the equation Ax = b isGeometrically, this says that the solution set to Ax = b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).We give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Consider the matrixThe kernel of this matrix consists of all vectors (x, y, z) ∈ R3 for whichwhich can be expressed as a homogeneous system of linear equations involving x, y, and z:which can be written in matrix form as:Gauss–Jordan elimination reduces this to:Rewriting yields:Now we can express an element of the kernel:for c a scalar.Since c is a free variable, this can be expressed equally well as,The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (−1,−26,16)T constitutes a basis of the kernel of A.Thus, the nullity of A is 1.Note also that the following dot products are zero:which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (−1,−26,16)T.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.A basis of the kernel of a matrix may be computed by Gaussian elimination.In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.For example, suppose thatThenPutting the upper part in column echelon form by column operations on the whole matrix givesThe last three columns of B are zero columns. Therefore, the three last vectors of C,are a basis of the kernel of A.The problem of computing the kernel on a computer depends on the nature of the coefficients.If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic and Chinese remainder theorem, which reduces the problem to several similar ones over finite fields (this avoids the overhead induced by the non-linearity of the computational complexity of integer multiplication).[citation needed]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]
Pointwise
Examples includeSee pointwise product, scalar.Pointwise operations inherit such properties as associativity, commutativity and distributivity from corresponding operations on the codomain. An example of an operation on functions which is not pointwise is convolution.In order theory it is common to define a pointwise partial order on functions. With A, B posets, the set of functions A → B can be ordered by f ≤ g if and only if (∀x ∈ A) f(x) ≤ g(x). Pointwise orders also inherit some properties of the underlying posets. For instance if A and B are continuous lattices, then so is the set of functions A → B with pointwise order.[1] Using the pointwise order on functions one can concisely define other important notions, for instance:[2]An example of infinitary pointwise relation is pointwise convergence of functions — a sequence of functions withFor order theory examples:This article incorporates material from Pointwise on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
Row and column vectors
In linear algebra, a column vector or column matrix is an m × 1 matrix, that is, a matrix consisting of a single column of m elements,Similarly, a row vector or row matrix is a 1 × m matrix, that is, a matrix consisting of a single row of m elements[1]Throughout, boldface is used for the row and column vectors. The transpose (indicated by T) of a row vector is a column vectorand the transpose of a column vector is a row vectorThe set of all row vectors forms a vector space called row space, similarly the set of all column vectors forms a vector space called column space. The dimensions of the row and column spaces equals the number of entries in the row or column vector.The column space can be viewed as the dual space to the row space, since any linear functional on the space of column vectors can be represented uniquely as an inner product with a specific row vector.To simplify writing column vectors in-line with other text, sometimes they are written as row vectors with the transpose operation applied to them.orSome authors also use the convention of writing both column vectors and row vectors as rows, but separating row vector elements with commas and column vector elements with semicolons (see alternative notation 2 in the table below). Matrix multiplication involves the action of multiplying each row vector of one matrix by each column vector of another matrix. The dot product of two vectors a and b is equivalent to the matrix product of the row vector representation of a and the column vector representation of b,which is also equivalent to the matrix product of the row vector representation of b and the column vector representation of a,The matrix product of a column and a row vector gives the outer product of two vectors a and b, an example of the more general tensor product. The matrix product of the column vector representation of a and the row vector representation of b gives the components of their dyadic product,which is the transpose of the matrix product of the column vector representation of b and the row vector representation of a,Frequently a row vector presents itself for an operation within n-space expressed by an n × n matrix M,Then p is also a row vector and may present to another n × n matrix Q,Conveniently, one can write t = p Q = v MQ telling us that the matrix product transformation MQ can take v directly to t.  Continuing with row vectors, matrix transformations further reconfiguring n-space can be applied to the right of previous outputs.In contrast, when a column vector is transformed to become another column under an n × n matrix action, the operation occurs to the left,leading to the algebraic expression QM v  for the composed output from v input. The matrix transformations mount up to the left in this use of a column vector for input to matrix transformation.Nevertheless, using the transpose operation these differences between inputs of a row or column nature are resolved by an antihomomorphism between the groups arising on the two sides. The technical construction uses the dual space associated with a vector space to develop the transpose of a linear map.For an instance where this row vector input convention has been used to good effect see Raiz Usmani,[2] where on page 106 the convention allows the statement "The product mapping ST of U into W [is given] by:(The Greek letters represent row vectors).Ludwik Silberstein used row vectors for spacetime events; he applied Lorentz transformation matrices on the right in his Theory of Relativity in 1914 (see page 143).In 1963 when McGraw-Hill published Differential Geometry by Heinrich Guggenheimer of the University of Minnesota, he uses the row vector convention in chapter 5, "Introduction to transformation groups" (eqs. 7a,9b and 12 to 15). When H. S. M. Coxeter reviewed[3] Linear Geometry by Rafael Artzy, he wrote, "[Artzy] is to be congratulated on his choice of the 'left-to-right' convention, which enables him to regard a point as a row matrix instead of the clumsy column that many authors prefer." J. W. P. Hirschfeld used right multiplication of row vectors by matrices in his description of projectivities on the Galois geometry PG(1,q).[4]In the study of stochastic processes with a stochastic matrix, it is conventional to use a row vector as the stochastic vector.[5]
Multi-core processor
A multi-core processor is a single computing component with two or more independent processing units called cores, which read and execute program instructions.[1] The instructions are ordinary CPU instructions (such as add, move data, and branch) but the single processor can run multiple instructions on separate cores at the same time, increasing overall speed for programs amenable to parallel computing.[2] Manufacturers typically integrate the cores onto a single integrated circuit die (known as a chip multiprocessor or CMP) or onto multiple dies in a single chip package.  The microprocessors currently used in almost all personal computers are multi-core.A multi-core processor implements multiprocessing in a single physical package. Designers may couple cores in a multi-core device tightly or loosely. For example, cores may or may not share caches, and they may implement message passing or shared-memory inter-core communication methods. Common network topologies to interconnect cores include bus, ring, two-dimensional mesh, and crossbar. Homogeneous multi-core systems include only identical cores; heterogeneous multi-core systems have cores that are not identical (e.g. big.LITTLE have heterogeneous cores that share the same instruction set, while AMD Accelerated Processing Units have cores that don't even share the same instruction set). Just as with single-processor systems, cores in multi-core systems may implement architectures such as VLIW, superscalar, vector, or multithreading.Multi-core processors are widely used across many application domains, including general-purpose, embedded, network, digital signal processing (DSP), and graphics (GPU).The improvement in performance gained by the use of a multi-core processor depends very much on the software algorithms used and their implementation. In particular, possible gains are limited by the fraction of the software that can run in parallel simultaneously on multiple cores; this effect is described by Amdahl's law. In the best case, so-called embarrassingly parallel problems may realize speedup factors near the number of cores, or even more if the problem is split up enough to fit within each core's cache(s), avoiding use of much slower main-system memory. Most applications, however, are not accelerated so much unless programmers invest a prohibitive amount of effort in re-factoring the whole problem.[3]The parallelization of software is a significant ongoing topic of research.The terms multi-core and dual-core most commonly refer to some sort of central processing unit (CPU), but are sometimes also applied to digital signal processors (DSP) and system on a chip (SoC). The terms are generally used only to refer to multi-core microprocessors that are manufactured on the same integrated circuit die; separate microprocessor dies in the same package are generally referred to by another name, such as multi-chip module. This article uses the terms "multi-core" and "dual-core" for CPUs manufactured on the same integrated circuit, unless otherwise noted.In contrast to multi-core systems, the term multi-CPU refers to multiple physically separate processing-units (which often contain special circuitry to facilitate communication between each other).The terms many-core and massively multi-core are sometimes used to describe multi-core architectures with an especially high number of cores (tens to thousands[4]).[5]Some systems use many soft microprocessor cores placed on a single FPGA. Each "core" can be considered a "semiconductor intellectual property core" as well as a CPU core.[citation needed]While manufacturing technology improves, reducing the size of individual gates, physical limits of semiconductor-based microelectronics have become a major design concern. These physical limitations can cause significant heat dissipation and data synchronization problems. Various other methods are used to improve CPU performance. Some instruction-level parallelism (ILP) methods such as superscalar pipelining are suitable for many applications, but are inefficient for others that contain difficult-to-predict code. Many applications are better suited to thread-level parallelism (TLP) methods, and multiple independent CPUs are commonly used to increase a system's overall TLP. A combination of increased available space (due to refined manufacturing processes) and the demand for increased TLP led to the development of multi-core CPUs.Several business motives drive the development of multi-core architectures. For decades, it was possible to improve performance of a CPU by shrinking the area of the integrated circuit (IC), which reduced the cost per device on the IC. Alternatively, for the same circuit area, more transistors could be used in the design, which increased functionality, especially for complex instruction set computing (CISC) architectures. Clock rates also increased by orders of magnitude in the decades of the late 20th century, from several megahertz in the 1980s to several gigahertz in the early 2000s.As the rate of clock speed improvements slowed, increased use of parallel computing in the form of multi-core processors has been pursued to improve overall processing performance. Multiple cores were used on the same CPU chip, which could then lead to better sales of CPU chips with two or more cores. For example, Intel has produced a 48-core processor for research in cloud computing; each core has an x86 architecture.[6][7]Since computer manufacturers have long implemented symmetric multiprocessing (SMP) designs using discrete CPUs, the issues regarding implementing multi-core processor architecture and supporting it with software are well known.Additionally:In order to continue delivering regular performance improvements for general-purpose processors, manufacturers such as Intel and AMD have turned to multi-core designs, sacrificing lower manufacturing-costs for higher performance in some applications and systems. Multi-core architectures are being developed, but so are the alternatives. An especially strong contender for established markets is the further integration of peripheral functions into the chip.The proximity of multiple CPU cores on the same die allows the cache coherency circuitry to operate at a much higher clock rate than what is possible if the signals have to travel off-chip. Combining equivalent CPUs on a single die significantly improves the performance of cache snoop (alternative: Bus snooping) operations. Put simply, this means that signals between different CPUs travel shorter distances, and therefore those signals degrade less. These higher-quality signals allow more data to be sent in a given time period, since individual signals can be shorter and do not need to be repeated as often.Assuming that the die can physically fit into the package, multi-core CPU designs require much less printed circuit board (PCB) space than do multi-chip SMP designs. Also, a dual-core processor uses slightly less power than two coupled single-core processors, principally because of the decreased power required to drive signals external to the chip. Furthermore, the cores share some circuitry, like the L2 cache and the interface to the front-side bus (FSB). In terms of competing technologies for the available silicon die area, multi-core design can make use of proven CPU core library designs and produce a product with lower risk of design error than devising a new wider-core design. Also, adding more cache suffers from diminishing returns.Multi-core chips also allow higher performance at lower energy. This can be a big factor in mobile devices that operate on batteries. Since each core in a multi-core CPU is generally more energy-efficient, the chip becomes more efficient than having a single large monolithic core. This allows higher performance with less energy. A challenge in this, however, is the additional overhead of writing parallel code.[9]Maximizing the usage of the computing resources provided by multi-core processors requires adjustments both to the operating system (OS) support and to existing application software. Also, the ability of multi-core processors to increase application performance depends on the use of multiple threads within applications.Integration of a multi-core chip can lower the chip production yields. They are also more difficult to manage thermally than lower-density single-core designs. Intel has partially countered this first problem by creating its quad-core designs by combining two dual-core ones on a single die with a unified cache, hence any two working dual-core dies can be used, as opposed to producing four cores on a single die and requiring all four to work to produce a quad-core CPU. From an architectural point of view, ultimately, single CPU designs may make better use of the silicon surface area than multiprocessing cores, so a development commitment to this architecture may carry the risk of obsolescence. Finally, raw processing power is not the only constraint on system performance. Two processing cores sharing the same system bus and memory bandwidth limits the real-world performance advantage. In a 2009 report, Dr Jun Ni showed that if a single core is close to being memory-bandwidth limited, then going to dual-core might give 30% to 70% improvement; if memory bandwidth is not a problem, then a 90% improvement can be expected; however, Amdahl's law makes this claim dubious.[10] It would be possible for an application that used two CPUs to end up running faster on a single-core one if communication between the CPUs was the limiting factor, which would count as more than 100% improvement.The trend in processor development has been towards an ever-increasing number of cores, as processors with hundreds or even thousands of cores become theoretically possible.[11] In addition, multi-core chips mixed with simultaneous multithreading, memory-on-chip, and special-purpose "heterogeneous" (or asymmetric) cores promise further performance and efficiency gains,[12] especially in processing multimedia, recognition and networking applications. For example, a big.LITTLE core includes a high-performance core (called 'big') and a low-power core (called 'LITTLE'). There is also a trend towards improving energy-efficiency by focusing on performance-per-watt with advanced fine-grain or ultra fine-grain power management and dynamic voltage and frequency scaling (i.e. laptop computers and portable media players).Chips designed from the outset for a large number of cores (rather than having evolved from single core designs) are sometimes referred to as manycore designs, emphasising qualitative differences.The composition and balance of the cores in multi-core architecture show great variety. Some architectures use one core design repeated consistently ("homogeneous"), while others use a mixture of different cores, each optimized for a different, "heterogeneous" role.The article "CPU designers debate multi-core future" by Rick Merritt, EE Times 2008,[13] includes these comments:Chuck Moore [...] suggested computers should be like cellphones, using a variety of specialty cores to run modular software scheduled by a high-level applications programming interface.[...] Atsushi Hasegawa, a senior chief engineer at Renesas, generally agreed. He suggested the cellphone's use of many specialty cores working in concert is a good model for future multi-core designs.[...] Anant Agarwal, founder and chief executive of startup Tilera, took the opposing view. He said multi-core chips need to be homogeneous collections of general-purpose cores to keep the software model simple.An outdated version of an anti-virus application may create a new thread for a scan process, while its GUI thread waits for commands from the user (e.g. cancel the scan). In such cases, a multi-core architecture is of little benefit for the application itself due to the single thread doing all the heavy lifting and the inability to balance the work evenly across multiple cores. Programming truly multithreaded code often requires complex co-ordination of threads and can easily introduce subtle and difficult-to-find bugs due to the interweaving of processing on data shared between threads (see thread-safety). Consequently, such code is much more difficult to debug than single-threaded code when it breaks. There has been a perceived lack of motivation for writing consumer-level threaded applications because of the relative rarity of consumer-level demand for maximum use of computer hardware. Although threaded applications incur little additional performance penalty on single-processor machines, the extra overhead of development has been difficult to justify due to the preponderance of single-processor machines. Also, serial tasks like decoding the entropy encoding algorithms used in video codecs are impossible to parallelize because each result generated is used to help create the next result of the entropy decoding algorithm.Given the increasing emphasis on multi-core chip design, stemming from the grave thermal and power consumption problems posed by any further significant increase in processor clock speeds, the extent to which software can be multithreaded to take advantage of these new chips is likely to be the single greatest constraint on computer performance in the future. If developers are unable to design software to fully exploit the resources provided by multiple cores, then they will ultimately reach an insurmountable performance ceiling.The telecommunications market had been one of the first that needed a new design of parallel datapath packet processing because there was a very quick adoption of these multiple-core processors for the datapath and the control plane. These MPUs are going to replace[14] the traditional Network Processors that were based on proprietary microcode or picocode.Parallel programming techniques can benefit from multiple cores directly. Some existing parallel programming models such as Cilk Plus, OpenMP, OpenHMPP, FastFlow, Skandium, MPI, and Erlang can be used on multi-core platforms. Intel introduced a new abstraction for C++ parallelism called TBB. Other research efforts include the Codeplay Sieve System, Cray's Chapel, Sun's Fortress, and IBM's X10.Multi-core processing has also affected the ability of modern computational software development. Developers programming in newer languages might find that their modern languages do not support multi-core functionality. This then requires the use of numerical libraries to access code written in languages like C and Fortran, which perform math computations faster than newer languages like C#. Intel's MKL and AMD's ACML are written in these native languages and take advantage of multi-core processing. Balancing the application workload across processors can be problematic, especially if they have different performance characteristics. There are different conceptual models to deal with the problem, for example using a coordination language and program building blocks (programming libraries or higher-order functions). Each block can have a different native implementation for each processor type. Users simply program using these abstractions and an intelligent compiler chooses the best implementation based on the context.[15]Managing concurrency acquires a central role in developing parallel applications. The basic steps in designing parallel applications are:On the other hand, on the server side, multi-core processors are ideal because they allow many users to connect to a site simultaneously and have independent threads of execution. This allows for Web servers and application servers that have much better throughput.Vendors may license some software "per processor". This can give rise to ambiguity, because a "processor" may consist either of a single core or of a combination of cores.Embedded computing operates in an area of processor technology distinct from that of "mainstream" PCs. The same technological drives towards multi-core apply here too. Indeed, in many cases the application is a "natural" fit for multi-core technologies, if the task can easily be partitioned between the different processors.In addition, embedded software is typically developed for a specific hardware release, making issues of software portability, legacy code or supporting independent developers less critical than is the case for PC or enterprise computing. As a result, it is easier for developers to adopt new technologies and as a result there is a greater variety of multi-core processing architectures and suppliers.As of  2010[update], multi-core network processing devices have become mainstream, with companies such as Freescale Semiconductor, Cavium Networks, Wintegra and Broadcom all manufacturing products with eight processors. For the system developer, a key challenge is how to exploit all the cores in these devices to achieve maximum networking performance at the system level, despite the performance limitations inherent in an SMP operating system. To address this issue, companies such as 6WIND provide portable packet processing software designed so that the networking data plane runs in a fast path environment outside the OS.[18]In digital signal processing the same trend applies: Texas Instruments has the three-core TMS320C6488 and four-core TMS320C5441, Freescale the four-core MSC8144 and six-core MSC8156 (and both have stated they are working on eight-core successors). Newer entries include the Storm-1 family from Stream Processors, Inc with 40 and 80 general purpose ALUs per chip, all programmable in C as a SIMD engine and Picochip with three-hundred processors on a single die, focused on communication applications.As of  2016[update] heterogeneous multi-core solutions are becoming more common: Xilinx Zynq UltraScale+ MPSoC has Quad-core ARM Cortex-A53 and Dual-core ARM Cortex-R5. Software solutions such as OpenAMP are being used to help with inter processor communication.The research and development of multicore processors often compares many options, and benchmarks are developed to help such evaluations. Existing benchmarks include SPLASH-2, PARSEC, and COSMIC for heterogeneous systems.[26]
Row and column spaces
In linear algebra, the column space (also called the range or image) of a matrix A is the span (set of all possible linear combinations) of its column vectors. The column space of a matrix is the image or range of the corresponding matrix transformation.The row space is defined similarly.This article considers matrices of real numbers.  The row and column spaces are subspaces of the real spaces Rn and Rm respectively.[2]Let A be an m-by-n matrix. ThenIf one considers the matrix as a linear transformation from Rn to Rm, then the column space of the matrix equals the image of this linear transformation.The column space of a matrix A is the set of all linear combinations of the columns in A.  If A = [a1, ...., an], then colsp(A) = span {a1, ...., an}.The concept of row space generalizes to matrices over C, the field of complex numbers, or over any field.Intuitively, given a matrix A, the action of the matrix A on a vector x will return a linear combination of the columns of A weighted by the coordinates of x as coefficients. Another way to look at this is that it will (1) first project x into the row space of A, (2) perform an invertible transformation, and (3) place the resulting vector y in the column space of A.  Thus the result y = A x must reside in the column space of A. See singular value decomposition for more details on this second interpretation.[clarification needed]Given a matrix J:the rows arer1 = (2,4,1,3,2),r2 = (−1,−2,1,0,5),r3 = (1,6,2,2,2),r4 = (3,6,2,5,1).Consequently, the row space of J is the subspace of R5 spanned by { r1, r2, r3, r4 }.   Since these four row vectors are linearly independent, the row space is 4-dimensional.  Moreover, in this case it can be seen that they are all orthogonal to the vector n = (6,−1,4,−4,0), so it can be deduced that the row space consists of all vectors in R5 that are orthogonal to n.Let K be a field of scalars. Let A be an m × n matrix, with column vectors v1, v2, ..., vn.  A linear combination of these vectors is any vector of the formwhere c1, c2, ..., cn are scalars.  The set of all possible linear combinations of v1, ... ,vn is called the column space of A.  That is, the column space of A is the span of the vectors v1, ... , vn.Any linear combination of the column vectors of a matrix A can be written as the product of A with a column vector:Therefore, the column space of A consists of all possible products Ax, for x ∈ Cn.  This is the same as the image (or range) of the corresponding matrix transformation.The columns of A span the column space, but they may not form a basis if the column vectors are not linearly independent.  Fortunately, elementary row operations do not affect the dependence relations between the column vectors.  This makes it possible to use row reduction to find a basis for the column space.For example, consider the matrixThe columns of this matrix span the column space, but they may not be linearly independent, in which case some subset of them will form a basis.  To find this basis, we reduce A to reduced row echelon form:At this point, it is clear that the first, second, and fourth columns are linearly independent, while the third column is a linear combination of the first two.  (Specifically, v3 = –2v1 + v2.)  Therefore, the first, second, and fourth columns of the original matrix are a basis for the column space:Note that the independent columns of the reduced row echelon form are precisely the columns with pivots.  This makes it possible to determine which columns are linearly independent by reducing only to echelon form.The above algorithm can be used in general to find the dependence relations between any set of vectors, and to pick out a basis from any spanning set.  A different algorithm for finding a basis from a spanning set is given in the row space article;  finding a basis for the column space of A is equivalent to finding a basis for the row space of the transpose matrix AT.To find the basis in a practical setting (e.g., for large matrices), the singular-value decomposition is typically used.The dimension of the column space is called the rank of the matrix.  The rank is equal to the number of pivots in the reduced row echelon form, and is the maximum number of linearly independent columns that can be chosen from the matrix.  For example, the 4 × 4 matrix in the example above has rank three.Because the column space is the image of the corresponding matrix transformation, the rank of a matrix is the same as the dimension of the image.  For example, the transformation R4 → R4 described by the matrix above maps all of R4 to some three-dimensional subspace.The nullity of a matrix is the dimension of the null space, and is equal to the number of columns in the reduced row echelon form that do not have pivots.[6]  The rank and nullity of a matrix A with n columns are related by the equation:This is known as the rank–nullity theorem.The left null space of A is the set of all vectors x such that xTA = 0T.  It is the same as the null space of the transpose of A. The product of the matrix AT and the vector x can be written in terms of the dot product of vectors:because row vectors of AT are transposes of column vectors vk of A.  Thus ATx = 0 if and only if x is orthogonal (perpendicular) to each of the column vectors of A.It follows that the left null space (the null space of AT) is the orthogonal complement to the column space of A.For a matrix A, the column space, row space, null space, and left null space are sometimes referred to as the four fundamental subspaces.Similarly the column space (sometimes disambiguated as right column space) can be defined for matrices over a ring K asfor any c1, ..., cn, with replacement of the vector m-space with "right free module", which changes the order of scalar multiplication of the vector vk to the scalar ck such that it is written in an unusual order vector–scalar.[7]Let K be a field of scalars. Let A be an m × n matrix, with row vectors r1, r2, ... , rm.  A linear combination of these vectors is any vector of the formwhere c1, c2, ... , cm are scalars.  The set of all possible linear combinations of r1, ... , rm is called the row space of A.  That is, the row space of A is the span of the vectors r1, ... , rm.For example, ifthen the row vectors are r1 = (1, 0, 2) and r2 = (0, 1, 0).  A linear combination of r1 and r2 is any vector of the formThe set of all such vectors is the row space of A.  In this case, the row space is precisely the set of vectors (x, y, z) ∈ K3 satisfying the equation z = 2x (using Cartesian coordinates, this set is a plane through the origin in three-dimensional space).For a matrix that represents a homogeneous system of linear equations, the row space consists of all linear equations that follow from those in the system.The column space of A is equal to the row space of AT.The row space is not affected by elementary row operations.  This makes it possible to use row reduction to find a basis for the row space.For example, consider the matrixThe rows of this matrix span the row space, but they may not be linearly independent, in which case the rows will not be a basis.  To find a basis, we reduce A to row echelon form:r1, r2, r3 represents the rows.Once the matrix is in echelon form, the nonzero rows are a basis for the row space.  In this case, the basis is { (1, 3, 2), (0, 1, 0) }. Another possible basis { (1, 0, 2), (0, 1, 0) } comes from a further reduction.[8]This algorithm can be used in general to find a basis for the span of a set of vectors.  If the matrix is further simplified to reduced row echelon form, then the resulting basis is uniquely determined by the row space.It is sometimes convenient to find a basis for the row space from among the rows of the original matrix instead (for example, this result is useful in giving an elementary proof that the determinantal rank of a matrix is equal to its rank). Since row operations can affect linear dependence relations of the row vectors, such a basis is instead found indirectly using the fact that the column space of AT is equal to the row space of A. Using the example matrix A above, find AT and reduce it to row echelon form:The pivots indicate that the first two columns of AT form a basis of the column space of  AT. Therefore, the first two rows of A (before any row reductions) also form a basis of the row space of A.The dimension of the row space is called the rank of the matrix.  This is the same as the maximum number of linearly independent rows that can be chosen from the matrix, or equivalently the number of pivots.  For example, the 3 × 3 matrix in the example above has rank two.[8]The rank of a matrix is also equal to the dimension of the column space.  The dimension of the null space is called the nullity of the matrix, and is related to the rank by the following equation:where n is the number of columns of the matrix A.  The equation above is known as the rank–nullity theorem.The null space of matrix A is the set of all vectors x for which Ax = 0.  The product of the matrix A and the vector x can be written in terms of the dot product of vectors:where r1, ... , rm are the row vectors of A.  Thus Ax = 0 if and only if x is orthogonal (perpendicular) to each of the row vectors of A.It follows that the null space of A is the orthogonal complement to the row space.  For example, if the row space is a plane through the origin in three dimensions, then the null space will be the perpendicular line through the origin.  This provides a proof of the rank–nullity theorem (see dimension above).The row space and null space are two of the four fundamental subspaces associated with a matrix A (the other two being the column space and left null space).If V and W are vector spaces, then the kernel of a linear transformation T: V → W is the set of vectors v ∈ V for which T(v) = 0.  The kernel of a linear transformation is analogous to the null space of a matrix.If V is an inner product space, then the orthogonal complement to the kernel can be thought of as a generalization of the row space.  This is sometimes called the coimage of T.  The transformation T is one-to-one on its coimage, and the coimage maps isomorphically onto the image of T.When V is not an inner product space, the coimage of T can be defined as the quotient space V / ker(T).
Invertible matrix
In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such thatwhere In denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A−1.A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.Non-square matrices (m-by-n matrices for which m ≠ n) do not have an inverse.  However, in some cases such a matrix may have a left inverse or right inverse.  If A is m-by-n and the rank of A is equal to n, then A has a left inverse: an n-by-m matrix B such that BA = In.  If A has rank m, then it has a right inverse: an n-by-m matrix B such that AB = Im.Matrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.While the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. However, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. For a noncommutative ring, the usual determinant is not defined.  The conditions for existence of left-inverse or right-inverse are more complicated since a notion of rank does not exist over rings.The set of n × n invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree n.Let A be a square n by n matrix over a field K (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:Furthermore, the following properties hold for an invertible matrix A:A matrix that is its own inverse, i.e. such that A = A−1 and A2 = I, is called an involutory matrix.It follows from the associativity of matrix multiplication that iffor finite square matrices A and B, then alsoOver the field of real numbers, the set of singular n-by-n matrices, considered as a subset of Rn×n, is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all n-by-n matrices are invertible.Furthermore, the n-by-n invertible matrices are a dense open set in the topological space of all n-by-n matrices.  Equivalently, the set of singular matrices is closed and nowhere dense in the space of n-by-n matrices.In practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.Consider the following 2-by-2 matrix:As an example of a non-invertible, or singular, matrix, consider the matrixGauss-Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.A generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:Victor Pan and John Reif have done work that includes ways of generating a starting seed.[2][3] Byte magazine summarised one of their approaches.[4]Newton's method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton's method is also useful for "touch up" corrections to the Gauss–Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given byIf matrix A is positive definite, then its inverse can be obtained aswhere L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.Writing the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of small matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:so thatwhere |A| is the determinant of A, C is the matrix of cofactors, and CT represents the matrix transpose.The cofactor equation listed above yields the following result for 2 × 2 matrices. Inversion of these matrices can be done as follows:[6]This is possible because 1/(ad − bc) is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.The Cayley–Hamilton method givesA computationally efficient 3 × 3 matrix inversion is given by(where the scalar A is not to be confused with the matrix A).If the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given byThe determinant of A can be computed by applying the rule of Sarrus as follows:The Cayley–Hamilton decomposition givesWith increasing dimension, expressions for the inverse of A get complicated. For n = 4, the Cayley–Hamilton method leads to an expression that is still tractable:Matrices can also be inverted blockwise by using the following analytic inversion formula:    ( 1)where A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and D − CA−1B must be nonsingular.[7]) This strategy is particularly advantageous if A is diagonal and D − CA−1B (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion.This technique was reinvented several times and is due to Hans Boltz (1923),[citation needed] who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.The nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.The inversion procedure that led to Equation (1) performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and A − BD−1C are nonsingular,[8] the result is    ( 2)Equating Equations (1) and (2) leads to    ( 3)where Equation (3) is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.Since a blockwise inversion of an n × n matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.[9] There exist matrix multiplication algorithms with a complexity of O(n2.3727) operations, while the best proven lower bound is Ω(n2 log n).[10]If a matrix A has the property thatthen A is nonsingular and its inverse may be expressed by a Neumann series:[11]Truncating the sum results in an "approximate" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfies More generally, if A is "near" the invertible matrix X in the sense thatthen A is nonsingular and its inverse isIf it is also the case that A − X has rank 1 then this simplifies toSuppose that the invertible matrix A depends on a parameter t. Then the derivative of the inverse of A with respect to t is given byMore generally, ifthen,Therefore,Some of the properties of inverse matrices are shared by generalized inverses (e.g., the Moore–Penrose inverse), which can be defined for any m-by-n matrix.For most practical applications, it is not necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it is necessary that the matrix involved be invertible.Decomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.Although an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.Matrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.Matrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.
Dimension theorem for vector spaces
In mathematics, the dimension theorem for vector spaces states that all bases of a vector space have equally many elements. This number of elements may be finite or infinite (in the latter case, it is a cardinal number), and defines the dimension of the vector space.Formally, the dimension theorem for vector spaces states thatAs a basis is a generating set that is linearly independent, the theorem is a consequence of the following theorem, which is also useful:In particular if V is finitely generated, then all its bases are finite and have the  same number of elements.While the proof of the existence of a basis for any vector space in the general case requires Zorn's lemma and is in fact equivalent to the axiom of choice, the uniqueness of the cardinality of the basis requires only the ultrafilter lemma,[1] which is strictly weaker (the proof given below, however, assumes trichotomy, i.e., that all cardinal numbers are comparable, a statement which is also equivalent to the axiom of choice). The theorem can be generalized to arbitrary R-modules for rings R having invariant basis number.In the finitely generated case the proof uses only elementary arguments of algebra, and does not require the axiom of choice nor its weaker variants.Let V be a vector space, {ai: i ∈ I } be a linearly independent set of elements of V, and {bj: j ∈ J } be a generating set. One has to prove that the cardinality of I is not larger than that of J.If J is finite, this results from the Steinitz exchange lemma. (Indeed, the Steinitz exchange lemma implies every finite subset of I has cardinality not larger than that of J, hence I is finite with cardinality not larger than that of J.) If J is finite, a proof based on matrix theory is also possible.[2] Assume that J is infinite. If I is finite, there is nothing to prove. Thus, we may assume that I is also infinite. Let us suppose that the cardinality of I is larger than that of J.[note 1] We have to prove that this leads to a contradiction. By Zorn's lemma, every linearly independent set is contained in a maximal linearly independent set K. This maximality implies that K spans V and is therefore a basis (the maximality implies that every element of V is linearly dependent from the elements of K, and therefore is a linear combination of elements of K. As the cardinality of K is greater or equal with the cardinality of I, one may replace I with K, that is, one may suppose, without loss of generality, that I is a basis. Thus, every bj can be written as a finite sumThis application of the dimension theorem is sometimes itself called the dimension theorem. Letbe a linear transformation. Thenthat is, the dimension of U is equal to the dimension of the transformation's range plus the dimension of the kernel. See rank–nullity theorem for a fuller discussion.
Benjamin Peirce
Benjamin Peirce (/ˈpɜːrs/;[1]) FRSFor HFRSE  April 4, 1809 – October 6, 1880) was an American mathematician who taught at Harvard University for approximately 50 years. He made contributions to celestial mechanics, statistics, number theory, algebra, and the philosophy of mathematics.He was born in Salem, Massachusetts, the son of Benjamin Peirce (1778–1831), later librarian of Harvard, and Lydia Ropes Nichols Peirce (1781–1868).[2]After graduating from Harvard University in 1829, he remained as a tutor, and was subsequently appointed professor of mathematics in 1831. He added astronomy to his portfolio in 1842, and remained as Harvard professor until his death. In addition, he was instrumental in the development of Harvard's science curriculum, served as the college  librarian, and was director of the U.S. Coast Survey from 1867 to 1874.He was elected a Foreign Member of the Royal Society of London in 1852.[3]Benjamin Peirce is often regarded as the earliest American scientist whose research was recognized as world class.[4] He was an apologist for slavery, opining that it should be condoned if it was used to allow an elite to pursue scientific enquiry.[5]In number theory, he proved there is no odd perfect number with fewer than four prime factors.In algebra, he was notable for the study of associative algebras. He first introduced the terms idempotent and nilpotent in 1870 to describe elements of these algebras, and he also introduced the Peirce decomposition.In the philosophy of mathematics, he became known for the statement that "Mathematics is the science that draws necessary conclusions".[6] Peirce's definition of mathematics was credited by his son, Charles Sanders Peirce, as helping to initiate the consequence-oriented philosophy of pragmatism.  Like George Boole, Peirce believed that mathematics could be used to study logic. These ideas were further developed by son Charles , who noted that logic also includes the study of faulty reasoning.  In contrast, the later logicist program of Gottlob Frege and Bertrand Russell attempted to base mathematics on logic.Peirce proposed what came to be known as Peirce's Criterion for the statistical treatment of outliers, that is, of apparently extreme observations. His ideas were further developed by his son Charles.[7]Peirce was an expert witness in the Howland will forgery trial, where he was assisted by his son Charles. Their analysis of the questioned signature showed that it resembled another particular handwriting example so closely that the chances of such a match were statistically extremely remote.[citation needed]He was devoutly religious, though he seldom published his theological thoughts.[8] Peirce credited God as shaping nature in ways that account for the efficacy of pure mathematics in describing empirical phenomena.[9] Peirce viewed "mathematics as study of God's work by God's creatures", according to an encyclopedia.[8]He married Sarah Hunt Mills, the daughter of U.S. Senator Elijah Hunt Mills.[10] Peirce and his wife had four sons and one daughter:[11]The lunar crater Peirce is named for Peirce.Post-doctoral positions in Harvard University's mathematics department are named in his honor as Benjamin Peirce Fellows and Lecturers.The United States Coast Survey ship USCS Benjamin Peirce, in commission from 1855 to 1868, was named for him.[12]
Generalizations of Pauli matrices
In mathematics and physics, in particular quantum information, the term generalized Pauli matrices refers to families of matrices which generalize the (linear algebraic) properties of the Pauli matrices. Here,  a few classes of such matrices are summarized.Let Ejk be the matrix with 1 in the jk-th entry and 0 elsewhere. Consider the space of d×d  complex matrices,   ℂd×d,  for a fixed d.Define the following matrices,The collection of matrices defined above without the identity matrix are called the generalized Gell-Mann matrices, in dimension d.[1]The symbol ⊕  (utilized in the Cartan subalgebra above) means matrix direct sum.In dimensions d=2 and 3, the above construction recovers the Pauli and Gell-Mann matrices, respectively.The so-called Walsh–Hadamard conjugation matrix isThe goal now is to extend the above to higher dimensions, d, a problem solved by J. J. Sylvester (1882).Fix the dimension d as before. Let  ω = exp(2πi/d),  a root of unity. Since  ωd = 1  and  ω  ≠ 1,  the sum  of all roots annuls:Integer indices may then be cyclically identified mod d.Now define, with Sylvester, the shift matrix[2]and the clock matrix, These matrices generalize σ1 and σ3, respectively.Note that the unitarity and tracelessness of the two Pauli matrices  is preserved, but not Hermiticity in dimensions higher than two. Since Pauli matrices describe Quaternions, Sylvester dubbed the higher-dimensional analogs "nonions", "sedenions", etc.These two matrices are also the cornerstone of quantum mechanical dynamics in finite-dimensional vector spaces[3][4][5]  as formulated by Hermann Weyl, and find routine applications in numerous areas of mathematical physics.[6]  The clock matrix amounts to the exponential of position in a "clock" of d hours, and the shift matrix is just the translation operator in that cyclic vector space, so the exponential of the momentum. They are (finite-dimensional)  representations of the corresponding elements of the Weyl-Heisenberg on a d-dimensional Hilbert space.The following relations echo and generalize those of the Pauli matrices:and the braiding relation,the Weyl formulation of the CCR, and can be rewritten as On the other hand, to generalize the Walsh–Hadamard matrix W, noteDefine, again with Sylvester,  the following analog matrix,[7] still denoted by W in a slight abuse of notation,It is evident that  W is no longer Hermitian, but is still unitary. Direct calculation  yieldswhich is the desired analog result. Thus, W , a Vandermonde matrix, arrays the eigenvectors of   Σ1, which has the same eigenvalues as   Σ3.When d = 2k,   W * is precisely the matrix of the discrete Fourier transform,converting position coordinates to momentum coordinates and vice versa.The complete family of d2 unitary (but non-Hermitian) independent matrices
Perspectivity
In geometry and in its applications to drawing, a perspectivity is the formation of an image in a picture plane of a scene viewed from a fixed point.The science of graphical perspective uses perspectivities to make realistic images in proper proportion. According to Kirsti Andersen, the first author to describe perspectivity was Leon Alberti in his De Pictura (1435).[1] In English, Brook Taylor presented his Linear Perspective in 1715, where he explained "Perspective is the Art of drawing on a Plane the Appearances of any Figures, by the Rules of Geometry".[2] In a second book, New Principles of Linear Perspective (1719), Taylor wroteIn projective geometry the points of a line are called a projective range, and the set of lines in a plane on a point is called a pencil.The existence of a perspectivity means that corresponding points are in perspective. The dual concept, axial perspectivity, is the correspondence between the lines of two pencils determined by a projective range.The composition of two perspectivities is, in general, not a perspectivity. A perspectivity or a composition of two or more perspectivities is called a projectivity (projective transformation, projective collineation and homography are synonyms).There are several results concerning projectivities and perspectivities which hold in any pappian projective plane:[6]Theorem: Any projectivity between two distinct projective ranges can be written as the composition of no more than two perspectivities.Theorem: Any projectivity from a projective range to itself can be written as the composition of three perspectivities.Theorem: A projectivity between two distinct projective ranges which fixes a point is a perspectivity.The bijective correspondence between points on two lines in a plane determined by a point of that plane not on either line has higher-dimensional analogues which will also be called perspectivities.Let Sm and Tm be two distinct m-dimensional projective spaces contained in an n-dimensional projective space Rn. Let Pn−m−1 be an (n − m − 1)-dimensional subspace of Rn with no points in common with either Sm or Tm. For each point X of Sm, the space L spanned by X and Pn-m-1 meets Tm in a point Y = fP(X). This correspondence fP is also called a perspectivity.[7] The central perspectivity described above is the case with n = 2 and m = 1.Let S2 and T2 be two distinct projective planes in a projective 3-space R3. With O and O* being points of R3 in neither plane, use the construction of the last section to project S2 onto T2 by the perspectivity with center O followed by the projection of T2 back onto S2 with the perspectivity with center O*. This composition is a bijective map of the points of S2 onto itself which preserves collinear points and is called a perspective collineation (central collineation in more modern terminology).[8] Let φ be a perspective collineation of S2. Each point of the line of intersection of S2 and T2 will be fixed by φ and this line is called the axis of φ. Let point P be the intersection of line OO* with the plane S2. P is also fixed by φ and every line of S2 that passes through P is stabilized by φ (fixed, but not necessarily pointwise fixed). P is called the center of φ. The restriction of φ to any line of S2 not passing through P is the central perspectivity in S2 with center P between that line and the line which is its image under φ.
Rank (linear algebra)
In linear algebra, the rank of a matrix A is the dimension of the vector space generated (or spanned) by its columns.[1] This corresponds to the maximal number of linearly independent columns of A. This, in turn, is identical to the dimension of the space spanned by its rows.[2] Rank is thus a measure of the "nondegenerateness" of the system of linear equations and linear transformation encoded by A.  There are multiple equivalent definitions of rank. A matrix's rank is one of its most fundamental characteristics.The rank is commonly denoted rank(A) or rk(A); sometimes the parentheses are not written, as in rank A.In this section we give some definitions of the rank of a matrix.  Many definitions are possible; see Alternative definitions for several of these.The column rank of A is the dimension of the column space of A, while the row rank of A is the dimension of the row space of A.A fundamental result in linear algebra is that the column rank and the row rank are always equal. (Two proofs of this result are given in Proofs that column rank = row rank below.)  This number (i.e., the number of linearly independent rows or columns) is simply called the rank of A.A matrix is said to have full rank if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns.  A matrix is said to be rank deficient if it does not have full rank.The rank is also the dimension of the image of the linear transformation that is given by multiplication by A.  More generally, if a linear operator on a vector space (possibly infinite-dimensional) has finite-dimensional image (e.g., a finite-rank operator), then the rank of the operator is defined as the dimension of the image.The matrixhas rank 2: the first two rows are linearly independent, so the rank is at least 2, but since the third is a linear combination of the first two (the second subtracted from the first), the three rows are linearly dependent so the rank must be less than 3.The matrixhas rank 1: there are nonzero columns, so the rank is positive, but any pair of columns is linearly dependent.  Similarly, the transposeof A has rank 1.  Indeed, since the column vectors of A are the row vectors of the transpose of A, the statement that the column rank of a matrix equals its row rank is equivalent to the statement that the rank of a matrix is equal to the rank of its transpose, i.e., rk(A) = rk(AT).A common approach to finding the rank of a matrix is to reduce it to a simpler form, generally row echelon form, by elementary row operations. Row operations do not change the row space (hence do not change the row rank), and, being invertible, map the column space to an isomorphic space (hence do not change the column rank). Once in row echelon form, the rank is clearly the same for both row rank and column rank, and equals the number of pivots (or basic columns) and also the number of non-zero rows.For example, the matrix A given bycan be put in reduced row-echelon form by using the following elementary row operations:The final matrix (in row echelon form) has two non-zero rows and thus the rank of matrix A is 2.When applied to floating point computations on computers, basic Gaussian elimination (LU decomposition) can be unreliable, and a rank-revealing decomposition should be used instead. An effective alternative is the singular value decomposition (SVD), but there are other less expensive choices, such as QR decomposition with pivoting (so-called rank-revealing QR factorization), which are still more numerically robust than Gaussian elimination. Numerical determination of rank requires a criterion for deciding when a value, such as a singular value from the SVD, should be treated as zero, a practical choice which depends on both the matrix and the application.The fact that the column and row ranks of any matrix are equal forms an important part of the fundamental theorem of linear algebra. We present two proofs of this result. The first is short, uses only basic properties of linear combinations of vectors, and is valid over any field. The proof is based upon Wardlaw (2005).[3] The second is an elegant argument using orthogonality and is valid for matrices over the real numbers; it is based upon Mackiw (1995).[2] Both proofs can be found in the book by Banerjee and Roy (2014) [4]Let A be a matrix of size m × n (with m rows and n columns). Let the column rank of A be r and let c1,...,cr be any basis for the column space of A. Place these as the columns of an m × r matrix C. Every column of A can be expressed as a linear combination of the r columns in C. This means that there is an r × n matrix R such that A = CR. R is the matrix whose i-th column is formed from the coefficients giving the i-th column of A as a linear combination of the r columns of C. Now, each row of A is given by a linear combination of the r rows of R. Therefore, the rows of R form a spanning set of the row space of A and, by the Steinitz exchange lemma, the row rank of A cannot exceed r. This proves that the row rank of A is less than or equal to the column rank of A. This result can be applied to any matrix, so apply the result to the transpose of A. Since the row rank of the transpose of A is the column rank of A and the column rank of the transpose of A is the row rank of A, this establishes the reverse inequality and we obtain the equality of the row rank and the column rank of A. (Also see rank factorization.)In all the definitions in this section, the matrix A is taken to be an m × n matrix over an arbitrary field F.Given the matrix A, there is an associated linear mapping defined byThe rank of A is the dimension of the image of f.  This definition has the advantage that it can be applied to any linear map without need for a specific matrix.Given the same linear mapping f as above, the rank is n minus the dimension of the kernel of f.  The rank–nullity theorem states that this definition is equivalent to the preceding one.The rank of A is the maximal number of linearly independent rows of A; this is the dimension of the row space of A.As in the case of the "dimension of image" characterization, this can be generalized to a definition of the rank of any linear map: the rank of a linear map f : V → W is the minimal dimension k of an intermediate space X such that f can be written as the composition of a map V → X and a map X → W. Unfortunately, this definition does not suggest an efficient manner to compute the rank (for which it is better to use one of the alternative definitions). See rank factorization for details.The rank of A is the largest order of any non-zero minor in A.  (The order of a minor is the side-length of the square sub-matrix of which it is the determinant.) Like the decomposition rank characterization, this does not give an efficient way of computing the rank, but it is useful theoretically: a single non-zero minor witnesses a lower bound (namely its order) for the rank of the matrix, which can be useful (for example) to prove that certain operations do not lower the rank of a matrix.A non-vanishing p-minor (p × p submatrix with non-zero determinant) shows that the rows and columns of that submatrix are linearly independent, and thus those rows and columns of the full matrix are linearly independent (in the full matrix), so the row and column rank are at least as large as the determinantal rank; however, the converse is less straightforward.  The equivalence of determinantal rank and column rank is a strengthening of the statement that if the span of n vectors has dimension p, then p of those vectors span the space (equivalently, that one can choose a spanning set that is a subset of the vectors): the equivalence implies that a subset of the rows and a subset of the columns simultaneously define an invertible submatrix (equivalently, if the span of n vectors has dimension p, then p of these vectors span the space and there is a set of p coordinates on which they are linearly independent).We assume that A is an m × n matrix, and we define the linear map f by f(x) = Ax as above.One useful application of calculating the rank of a matrix is the computation of the number of solutions of a system of linear equations. According to the Rouché–Capelli theorem, the system is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, then the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank.  In this case (and assuming the system of equations is in the real or complex numbers) the system of equations has infinitely many solutions.In control theory, the rank of a matrix can be used to determine whether a linear system is controllable, or observable.In the field of communication complexity, the rank of the communication matrix of a function gives bounds on the amount of communication needed for two parties to compute the function.There are different generalizations of the concept of rank to matrices over arbitrary rings, where column rank, row rank, dimension of column space, and dimension of row space of a matrix may be different from the others or may not exist.Thinking of matrices as tensors, the tensor rank generalizes to arbitrary tensors; note that for tensors of order greater than 2 (matrices are order 2 tensors), rank is very hard to compute, unlike for matrices.There is a notion of rank for smooth maps between smooth manifolds. It is equal to the linear rank of the derivative.Matrix rank should not be confused with tensor order, which is called tensor rank. Tensor order is the number of indices required to write a tensor, and thus matrices all have tensor order 2. More precisely, matrices are tensors of type (1,1), having one row index and one column index, also called covariant order 1 and contravariant order 1; see Tensor (intrinsic definition) for details.Note that the tensor rank of a matrix can also mean the minimum number of simple tensors necessary to express the matrix as a linear combination, and that this definition does agree with matrix rank as here discussed.
Relative change and difference
In any quantitative science, the terms relative change and relative difference are used to compare two quantities while taking into account the "sizes" of the things being compared. The comparison is expressed as a ratio and is a unitless number. By multiplying these ratios by 100 they can be expressed as percentages so the terms percentage change, percent(age) difference, or relative percentage difference are also commonly used. The distinction between "change" and "difference" depends on whether or not one of the quantities being compared is considered a standard or reference or starting value. When this occurs, the term relative change (with respect to the reference value) is used and otherwise the term relative difference is preferred. Relative difference is often used as a quantitative indicator of quality assurance and quality control for repeated measurements where the outcomes are expected to be the same. A special case of percent change (relative change expressed as a percentage) called percent error occurs in measuring situations where the reference value is the accepted or actual value (perhaps theoretically determined) and the value being compared to it is experimentally determined (by measurement).Given two numerical quantities, x and y, their difference, Δ = x − y, can be called their actual difference. When y is a reference value (a theoretical/actual/correct/accepted/optimal/starting, etc. value; the value that x is being compared to) then Δ is called their actual change. When there is no reference value, the sign of Δ has little meaning in the comparison of the two values since it doesn't matter which of the two values is written first, so one often works with |Δ| = |x − y|, the absolute difference instead of Δ, in these situations. Even when there is a reference value, if it doesn't matter whether the compared value is larger or smaller than the reference value, the absolute difference can be considered in place of the actual change.The absolute difference between two values is not always a good way to compare the numbers. For instance, the absolute difference of 1 between 6 and 5 is more significant than the same absolute difference between 100,000,001 and 100,000,000. We can adjust the comparison to take into account the "size" of the quantities involved, by defining, for positive values of xreference:The relative change is not defined if the reference value (xreference) is zero.For values greater than the reference value, the relative change should be a positive number and for values that are smaller, the relative change should be negative. The formula given above behaves in this way only if xreference is positive, and reverses this behavior if xreference is negative. For example, if we are calibrating a thermometer which reads −6 °C when it should read −10 °C, this formula for relative change (which would be called relative error in this application) gives ((−6) − (−10)) / (−10) = 4/−10 = −0.4, yet the reading is too high. To fix this problem we alter the definition of relative change so that it works correctly for all nonzero values of xreference:If the relationship of the value with respect to the reference value (that is, larger or smaller) does not matter in a particular application, the absolute difference may be used in place of the actual change in the above formula to produce a value for the relative change which is always non-negative.Defining relative difference is not as easy as defining relative change since there is no "correct" value to scale the absolute difference with. As a result, there are many options for how to define relative difference and which one is used depends on what the comparison is being used for. In general we can say that the absolute difference |Δ| is being scaled by some function of the values x and y, say f(x, y).[1]As with relative change, the relative difference is undefined if f(x, y) is zero.Several common choices for the function f(x, y) would be:Measures of relative difference are unitless numbers expressed as a fraction. Corresponding values of percent difference would be obtained by multiplying these values by 100 (and appending the % sign to indicate that the value is a percentage).One way to define the relative difference of two numbers is to take their absolute difference divided by the maximum absolute value of the two numbers.if at least one of the values does not equal zero. This approach is especially useful when comparing floating point values in programming languages for equality with a certain tolerance.[2] Another application is in the computation of approximation errors when the relative error of a measurement is required.Another way to define the relative difference of two numbers is to take their absolute difference divided by some functional value of the two numbers, for example, the absolute value of their arithmetic mean:This approach is often used when the two numbers reflect a change in some single underlying entity.[citation needed] A problem with the above approach arises when the functional value is zero. In this example, if x and y have the same magnitude but opposite sign, thenwhich causes division by 0.  So it may be better to replace the denominator with the average of the absolute values of x and y:[citation needed]Percent Error is a special case of the percentage form of relative change calculated from the absolute change between the experimental (measured) and theoretical (accepted) values, and dividing by the theoretical (accepted) value.The terms "Experimental" and "Theoretical" used in the equation above are commonly replaced with similar terms. Other terms used for experimental could be "measured," "calculated," or "actual" and another term used for theoretical could be "accepted."  Experimental value is what has been derived by use of calculation and/or measurement and is having its accuracy tested against the theoretical value, a value that is accepted by the scientific community or a value that could be seen as a goal for a successful result.Although it is common practice to use the absolute value version of relative change when discussing percent error, in some situations, it can be beneficial to remove the absolute values to provide more information about the result. Thus, if an experimental value is less than the theoretical value, the percent error will be negative. This negative result provides additional information about the experimental result. For example, experimentally calculating the speed of light and coming up with a negative percent error says that the experimental value is a velocity that is less than the speed of light. This is a big difference from getting a positive percent error, which means the experimental value is a velocity that is greater than the speed of light (violating the theory of relativity) and is a newsworthy result.The percent error equation, when rewritten by removing the absolute values, becomes:It is important to note that the two values in the numerator do not commute. Therefore, it is vital to preserve the order as above: subtract the theoretical value from the experimental value and not vice versa.A percentage change is a way to express a change in a variable. It represents the relative change between the old value and the new one.For example, if a house is worth $100,000 today and the year after its value goes up to $110,000, the percentage change of its value can be expressed asIt can then be said that the worth of the house went up by 10%.More generally, if V1 represents the old value and V2 the new one,Some calculators directly support this via a %CH or Δ% function.When the variable in question is a percentage itself, it is better to talk about its change by using percentage points, to avoid confusion between relative difference and absolute difference.If a bank were to raise the interest rate on a savings account from 3% to 4%, the statement that "the interest rate was increased by 1%" is ambiguous and should be avoided. The absolute change in this situation is 1 percentage point (4% − 3%), but the relative change in the interest rate is:In general, the term "percentage point(s)" indicates an absolute change or difference of percentages, while the percent sign or the word "percentage" refers to the relative change or difference.[3]Car M costs $50,000 and car L costs $40,000. We wish to compare these costs.[4] With respect to car L, the absolute difference is $10,000 = $50,000 − $40,000. That is, car M costs $10,000 more than car L. The relative difference is,and we say that car M costs 25% more than car L. It is also common to express the comparison as a ratio, which in this example is,and we say that car M costs 125% of the cost of car L.In this example the cost of car L was considered the reference value, but we could have made the choice the other way and considered the cost of car M as the reference value. The absolute difference is now −$10,000 = $40,000 − $50,000 since car L costs $10,000 less than car M. The relative difference,is also negative since car L costs 20% less than car M. The ratio form of the comparison,says that car L costs 80% of what car M costs.It is the use of the words "of" and "less/more than" that distinguish between ratios and relative differences.[5]  Change in a quantity can also be expressed logarithmically using the unit of logarithmic change: the decibel and the neper (Np).  Normalization with a factor of 100, as done for percent, yields the derived unit centineper (cNp), which aligns with the definition for percentage change for very small changes:The second advantage is that the total change after a series of cNp changes equals the sum of the changes.  With percent, summing the changes is only an approximation, with larger error for larger changes.  For example:
Dual basis
In linear algebra, given a vector space V with a basis B of vectors indexed by an index set I (the cardinality of I is the dimensionality of V), its dual set is a set B∗ of vectors in the dual space V∗ with the same index set I such that B and B∗ form a biorthogonal system. The dual set is always linearly independent but does not necessarily span V∗. If it does span V∗, then B∗ is called the dual basis or reciprocal basis for the basis B.The dual set always exists and gives an injection from V into V∗, namely the mapping that sends vi to vi. This says, in particular, that the dual space has dimension greater or equal to that of V.The dual of an infinite-dimensional space has greater dimensionality (this being a greater infinite cardinality) than the original space has, and thus these cannot have a basis with the same indexing set. However, a dual set of vectors exists, which defines a subspace of the dual isomorphic to the original space. Further, for topological vector spaces, a continuous dual space can be defined, in which case a dual basis may exist.In the case of finite-dimensional vector spaces, the dual set is always a dual basis and it is unique. These bases are denoted by B = { e1, …, en } and B∗ = { e1, …, en }. If one denotes the evaluation of a covector on a vector as a pairing, the biorthogonality condition becomes:The association of a dual basis with a basis gives a map from the space of bases of V to the space of bases of V∗, and this is also an isomorphism. For topological fields such as the real numbers, the space of duals is a topological space, and this gives a homeomorphism between the Stiefel manifolds of bases of these spaces.To perform operations with a vector, we must have a straightforward method of calculating its components. In a Cartesian frame the necessary operation is the dot product of the vector and the base vector.[1] E.g.,In a non-Cartesian frame, we do not necessarily have ei · ej = 0 for all i ≠ j. However, it is always possible to find a vector ei such thatthe equality holds when ei is the dual base of eiFor example, the standard basis vectors of R2 (the Cartesian plane) areand the standard basis vectors of its dual space R2* areIn 3-dimensional Euclidean space, for a given basis {e1, e2, e3}, you can find the biorthogonal (dual) basis {e1, e2, e3} by formulas below:where T denotes the transpose and
Spectral theorem
In mathematics, particularly linear algebra and functional analysis, a spectral theorem is a result about when a linear operator or matrix can be diagonalized (that is, represented as a diagonal matrix in some basis). This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix. The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces. In general, the spectral theorem identifies a class of linear operators that can be modeled by multiplication operators, which are as simple as one can hope to find. In more abstract language, the spectral theorem is a statement about commutative C*-algebras. See also spectral theory for a historical perspective.Examples of operators to which the spectral theorem applies are self-adjoint operators or more generally normal operators on Hilbert spaces.The spectral theorem also provides a canonical decomposition, called the spectral decomposition, eigenvalue decomposition, or eigendecomposition, of the underlying vector space on which the operator acts.Augustin-Louis Cauchy proved the spectral theorem for self-adjoint matrices, i.e., that every real, symmetric matrix is diagonalizable. In addition, Cauchy was the first to be systematic about determinants.[1][2] The spectral theorem as generalized by John von Neumann is today perhaps the most important result of operator theory.This article mainly focuses on the simplest kind of spectral theorem, that for a self-adjoint operator on a Hilbert space. However, as noted above, the spectral theorem also holds for normal operators on a Hilbert space.(An equivalent condition is that A∗ = A, where A∗ is the hermitian conjugate of A.) In the case that A is identified with a Hermitian matrix, the matrix of A∗ can be identified with its conjugate transpose. (If A is a real matrix, this is equivalent to AT = A, that is, A is a symmetric matrix.)This condition implies that all eigenvalues of a Hermitian map are real: it is enough to apply it to the case when x = y is an eigenvector. (Recall that an eigenvector of a linear map A is a (non-zero) vector x such that Ax = λx for some scalar λ. The value λ is the corresponding eigenvalue. Moreover, the eigenvalues are solutions to the characteristic polynomial.)Theorem. If A is Hermitian, there exists an orthonormal basis of V consisting of eigenvectors of A. Each eigenvalue is real.We provide a sketch of a proof for the case where the underlying field of scalars is the complex numbers.By the fundamental theorem of algebra, applied to the characteristic polynomial of A, there is at least one eigenvalue λ1 and eigenvector e1. Then since we find that λ1 is real. Now consider the space K = span{e1}⊥, the orthogonal complement of e1. By Hermiticity, K is an invariant subspace of A. Applying the same argument to K shows that A has an eigenvector e2 ∈ K. Finite induction then finishes the proof.The spectral theorem holds also for symmetric maps on finite-dimensional real inner product spaces, but the existence of an eigenvector does not follow immediately from the  fundamental theorem of algebra. To prove this, consider A as a Hermitian matrix and use the fact that all eigenvalues of a Hermitian matrix are real.If one chooses the eigenvectors of A as an orthonormal basis, the matrix representation of A in this basis is diagonal. Equivalently, A can be written as a linear combination of pairwise orthogonal projections, called its spectral decomposition. Letbe the eigenspace corresponding to an eigenvalue λ. Note that the definition does not depend on any choice of specific eigenvectors. V is the orthogonal direct sum of the spaces Vλ where the index ranges over eigenvalues. In other words, if Pλ denotes the orthogonal projection onto Vλ, and λ1, ..., λm are the eigenvalues of A, then the spectral decomposition may be written asThe spectral decomposition is a special case of both the Schur decomposition and the singular value decomposition.The spectral theorem extends to a more general class of matrices. Let A be an operator on a finite-dimensional inner product space. A is said to be normal  if A∗A = AA∗. One can show that A is normal if and only if it is unitarily diagonalizable. Proof: By the Schur decomposition, we can write any matrix as A = UTU∗, where U is unitary and T is upper-triangular.If A is normal, one sees that TT∗ = T*T. Therefore, T must be diagonal since a normal upper triangular matrix is diagonal (see normal matrix). The converse is obvious.In other words, A is normal if and only if there exists a unitary matrix U such thatwhere D is a diagonal matrix. Then, the entries of the diagonal of D are the eigenvalues of A. The column vectors of U are the eigenvectors of A and they are orthonormal. Unlike the Hermitian case, the entries of D need not be real.In the more general setting of Hilbert spaces, which may have an infinite dimension, the statement of the spectral theorem for compact self-adjoint operators is virtually the same as in the finite-dimensional case.Theorem. Suppose A is a compact self-adjoint operator on a (real or complex) Hilbert space V. Then there is an orthonormal basis of V consisting of eigenvectors of A. Each eigenvalue is real.As for Hermitian matrices, the key point is to prove the existence of at least one nonzero eigenvector. One cannot rely on determinants to show existence of eigenvalues, but one can use a maximization argument analogous to the variational characterization of eigenvalues. If the compactness assumption is removed, it is not true that every self-adjoint operator has eigenvectors.The next generalization we consider is that of bounded self-adjoint operators on a Hilbert space. Such operators may have no eigenvalues: for instance let A be the operator of multiplication by t on L2[0, 1], that is,[3]One formulation of the spectral theorem expresses the operator A as an integral of the coordinate function over the operator's spectrum with respect to a projection-valued measure.[5]When the self-adjoint operator in question is compact, this version of the spectral theorem reduces to something similar to the finite-dimensional spectral theorem above, except that the operator is expressed as a finite or countably infinite linear combination of projections, that is, the measure consists only of atoms.An alternative formulation of the spectral theorem says that every bounded self-adjoint operator is unitarily equivalent to a multiplication operator. The significance of this result is that multiplication operators are in many ways easy to understand.Theorem.[6] Let A  be a bounded self-adjoint operator on a Hilbert space H.  Then there is a measure space (X, Σ, μ) and a real-valued essentially bounded measurable function f on X and a unitary operator U:H → L2μ(X) such thatThe spectral theorem is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.There is also an analogous spectral theorem for bounded normal operators on Hilbert spaces.  The only difference in the conclusion is that now f may be complex-valued.There is also a formulation of the spectral theorem in terms of direct integrals. It is similar to the multiplication-operator formulation, but more canonical.Many important linear operators which occur in analysis, such as differential operators, are unbounded. There is also a spectral theorem for self-adjoint operators that applies in these cases.  To give an example, every constant-coefficient differential operator is unitarily equivalent to a multiplication operator. Indeed, the unitary operator that implements this equivalence is the Fourier transform; the multiplication operator is a type of Fourier multiplier.In general, spectral theorem for self-adjoint operators may take several equivalent forms.[10] Notably, all of the formulations given in the previous section for bounded self-adjoint operators—the projection-valued measure version, the multiplication-operator version, and the direct-integral version—continue to hold for unbounded self-adjoint operators, with small technical modifications to deal with domain issues.
Manifold
In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point.  More precisely, each point of an n-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension n. In this more precise terminology, a manifold is referred to as an n-manifold.One-dimensional manifolds include lines and circles, but not figure eights (because they have crossing points that are not locally homeomorphic to Euclidean 1-space).  Two-dimensional manifolds are also called surfaces.  Examples include the plane, the sphere, and the torus, which can all be embedded (formed without self-intersections) in three dimensional real space, but also the Klein bottle and real projective plane, which will always self-intersect when immersed in three-dimensional real space.Although a manifold locally resembles Euclidean space, meaning that every point has a neighborhood homeomorphic to an open subset of Euclidean space, globally it may not: manifolds in general are not homeomorphic to Euclidean space.  For example, the surface of the sphere is not homeomorphic to the Euclidean plane, because (among other properties) it has the global topological property of compactness that Euclidean space lacks, but in a region it can be charted by means of map projections of the region into the Euclidean plane (in the context of manifolds they are called charts).  When a region appears in two neighbouring charts, the two representations do not coincide exactly and a transformation is needed to pass from one to the other, called a transition map.The concept of a manifold is central to many parts of geometry and modern mathematical physics because it allows complicated structures to be described and understood in terms of the simpler local topological properties of Euclidean space.  Manifolds naturally arise as solution sets of systems of equations and as graphs of functions.Manifolds can be equipped with additional structure.  One important class of manifolds is the class of differentiable manifolds; this differentiable structure allows calculus to be done on manifolds.  A Riemannian metric on a manifold allows distances and angles to be measured.  Symplectic manifolds serve as the phase spaces in the Hamiltonian formalism of classical mechanics, while four-dimensional Lorentzian manifolds model spacetime in general relativity.A surface is a two dimensional manifold, meaning that it locally resembles the Euclidean plane near each point.  For example, the surface of a globe can be described by a collection of maps (called charts), which together form an atlas of the globe.  Although no individual map is sufficient to cover the entire surface of the globe, any place in the globe will be in at least one of the charts.  Many places will appear in more than one chart.  For example, a map of North America will likely include parts of South America and the Arctic circle.  These regions of the globe will be described in full in separate charts, which in turn will contain parts of North America.  There is a relation between adjacent charts, called a transition map that allows them to be consistently patched together to cover the whole of the globe.Describing the coordinate charts on surfaces explicitly requires knowledge of functions of two variables, because these patching functions must map a region in the plane to another region of the plane.  However, one-dimensional examples of manifolds (or curves) can be described with functions of a single variable only.Manifolds have applications in computer-graphics and augmented-reality given the need to associate pictures(texture) to coordinates (i.e CT scans ).In an augmented reality setting, a picture (tangent plane) can be seen as something associated with a coordinate  and by using sensors for detecting movements and rotation one can have knowledge of how the picture is oriented and placed in space. After a line, the circle is the simplest example of a topological manifold. Topology ignores bending, so a small piece of a circle is treated exactly the same as a small piece of a line. Consider, for instance, the top part of the unit circle, x2 + y2 = 1, where the y-coordinate is positive (indicated by the yellow circular arc in Figure 1). Any point of this arc can be uniquely described by its x-coordinate. So, projection onto the first coordinate is a continuous, and invertible, mapping from the upper arc to the open interval (−1, 1):Such functions along with the open regions they map are called charts. Similarly, there are charts for the bottom (red), left (blue), and right (green) parts of the circle:Together, these parts cover the whole circle and the four charts form an atlas for the circle.Such a function is called a transition map.The top, bottom, left, and right charts show that the circle is a manifold, but they do not form the only possible atlas. Charts need not be geometric projections, and the number of charts is a matter of choice. Consider the chartsandHere s is the slope of the line through the point at coordinates (x,y) and the fixed pivot point (−1, 0); t follows similarly, but with pivot point (+1, 0). The inverse mapping from s to (x, y) is given byIt can easily be confirmed that x2 + y2 = 1 for all values of the slope s. These two charts provide a second atlas for the circle, withEach chart omits a single point, either (−1, 0) for s or (+1, 0) for t, so neither chart alone is sufficient to cover the whole circle. It can be proved that it is not possible to cover the full circle with a single chart. For example, although it is possible to construct a circle from a single line interval by overlapping and "gluing" the ends, this does not produce a chart; a portion of the circle will be mapped to both ends at once, losing invertibility.The sphere is an example of a surface. The unit sphere of implicit equationmay be covered by an atlas of six charts: the plane z = 0 divides the sphere into two half spheres (z > 0 and z < 0), which may both be mapped on the disc x2 + y2 < 1 by the projection on the xy plane of coordinates. This provides two charts; the four other charts are provided by a similar construction with the two other coordinate planes.As for the circle, one may define one chart that covers the whole sphere excluding one point. Thus two charts are sufficient, but the sphere cannot be covered by a single chart.This example is historically significant, as it has motivated the terminology; it became apparent that the whole surface of the Earth cannot have a plane representation consisting of a single map (also called "chart", see nautical chart), and therefore one needs atlases for covering the whole Earth surface.Viewed using calculus, the circle transition function T is simply a function between open intervals, which gives a meaning to the statement that T is differentiable. The transition map T, and all the others, are differentiable on (0, 1); therefore, with this atlas the circle is a differentiable manifold. It is also smooth and analytic because the transition functions have these properties as well.Other circle properties allow it to meet the requirements of more specialized types of manifold. For example, the circle has a notion of distance between two points, the arc-length between the points; hence it is a Riemannian manifold.Manifolds need not be connected (all in "one piece"); an example is a pair of separate circles.Manifolds need not be closed; thus a line segment without its end points is a manifold. And they are never countable, unless the dimension of the manifold is 0. Putting these freedoms together, other examples of manifolds are a parabola, a hyperbola (two open, infinite pieces), and the locus of points on a cubic curve y2 = x3 − x (a closed loop piece and an open, infinite piece).However, excluded are examples like two touching circles that share a point to form a figure-8; at the shared point a satisfactory chart cannot be created. Even with the bending allowed by topology, the vicinity of the shared point looks like a "+", not a line. A "+" is not homeomorphic to a closed interval (line segment), since deleting the center point from the "+" gives a space with four components (i.e. pieces), whereas deleting a point from a closed interval gives a space with at most two pieces; topological operations always preserve the number of pieces.Informally, a manifold is a space that is "modeled on" Euclidean space.There are many different kinds of manifolds, depending on the context.  In geometry and topology, all manifolds are topological manifolds, possibly with additional structure, such as a differentiable structure. A manifold can be constructed by giving a collection of coordinate charts, that is a covering by open sets with homeomorphisms to a Euclidean space, and patching functions: homeomorphisms from one region of Euclidean space to another region if they correspond to the same part of the manifold in two different coordinate charts.  A manifold can be given additional structure if the patching functions satisfy axioms beyond continuity. For instance, differentiable manifolds have homeomorphisms on overlapping neighborhoods diffeomorphic with each other, so that the manifold has a well-defined set of functions which are differentiable in each neighborhood, and so differentiable on the manifold as a whole.Formally, a (topological) manifold is a second countable Hausdorff space that is locally homeomorphic to Euclidean space.Second countable and Hausdorff are point-set conditions; second countable excludes spaces which are in some sense 'too large' such as the long line, while Hausdorff excludes spaces such as "the line with two origins" (these generalizations of manifolds are discussed in non-Hausdorff manifolds).Locally homeomorphic to Euclidean space means that every point has a neighborhood homeomorphic to an open Euclidean n-ball,Generally manifolds are taken to have a fixed dimension (the space must be locally homeomorphic to a fixed n-ball), and such a space is called an n-manifold; however, some authors admit manifolds where different points can have different dimensions.[1] If a manifold has a fixed dimension, it is called a pure manifold. For example, the sphere has a constant dimension of 2 and is therefore a pure manifold whereas the disjoint union of a sphere and a line in three-dimensional space is not a pure manifold. Since dimension is a local invariant (i.e. the map sending each point to the dimension of its neighbourhood over which a chart is defined, is locally constant), each connected component has a fixed dimension.Scheme-theoretically, a manifold is a locally ringed space, whose structure sheaf is locally isomorphic to the sheaf of continuous (or differentiable, or complex-analytic, etc.) functions on Euclidean space. This definition is mostly used when discussing analytic manifolds in algebraic geometry.The spherical Earth is navigated using flat maps or charts, collected in an atlas. Similarly, a differentiable manifold can be described using mathematical maps, called coordinate charts, collected in a mathematical atlas. It is not generally possible to describe a manifold with just one chart, because the global structure of the manifold is different from the simple structure of the charts.  For example, no single flat map can represent the entire Earth without separation of adjacent features across the map's boundaries or duplication of coverage. When a manifold is constructed from multiple overlapping charts, the regions where they overlap carry information essential to understanding the global structure.A coordinate map, a coordinate chart, or simply a chart, of a manifold is an invertible map between a subset of the manifold and a simple space such that both the map and its inverse preserve the desired structure.[2] For a topological manifold, the simple space is a subset of some Euclidean space Rn and interest focuses on the topological structure. This structure is preserved by homeomorphisms, invertible maps that are continuous in both directions.In the case of a differentiable manifold, a set of charts called an atlas allows us to do calculus on manifolds. Polar coordinates, for example, form a chart for the plane R2 minus the positive x-axis and the origin.  Another example of a chart is the map χtop mentioned in the section above, a chart for the circle.The description of most manifolds requires more than one chart (a single chart is adequate for only the simplest manifolds). A specific collection of charts which covers a manifold is called an atlas. An atlas is not unique as all manifolds can be covered multiple ways using different combinations of charts. Two atlases are said to be equivalent if their union is also an atlas.The atlas containing all possible charts consistent with a given atlas is called the maximal atlas (i.e. an equivalence class containing that given atlas (under the already defined equivalence relation given in the previous paragraph)). Unlike an ordinary atlas, the maximal atlas of a given manifold is unique. Though it is useful for definitions, it is an abstract object and not used directly (e.g. in calculations).Charts in an atlas may overlap and a single point of a manifold may be represented in several charts. If two charts overlap, parts of them represent the same region of the manifold, just as a map of Europe and a map of Asia may both contain Moscow.  Given two overlapping charts, a transition function can be defined which goes from an open ball in Rn to the manifold and then back to another (or perhaps the same) open ball in Rn.  The resultant map, like the map T in the circle example above, is called a change of coordinates, a coordinate transformation, a transition function, or a transition map.An atlas can also be used to define additional structure on the manifold. The structure is first defined on each chart separately. If all the transition maps are compatible with this structure, the structure transfers to the manifold.This is the standard way differentiable manifolds are defined. If the transition functions of an atlas for a topological manifold preserve the natural differential structure of Rn (that is, if they are diffeomorphisms), the differential structure transfers to the manifold and turns it into a differentiable manifold.  Complex manifolds are introduced in an analogous way by requiring that the transition functions of an atlas are holomorphic functions.  For symplectic manifolds, the transition functions must be symplectomorphisms.The structure on the manifold depends on the atlas, but sometimes different atlases can be said to give rise to the same structure. Such atlases are called compatible.These notions are made precise in general through the use of pseudogroups.A manifold with boundary is a manifold with an edge. For example, a sheet of paper is a 2-manifold with a 1-dimensional boundary. The boundary of an n-manifold with boundary is an (n − 1)-manifold.  A disk (circle plus interior) is a 2-manifold with boundary.  Its boundary is a circle, a 1-manifold. A square with interior is also a 2-manifold with boundary. A ball (sphere plus interior) is a 3-manifold with boundary.  Its boundary is a sphere, a 2-manifold.  (See also Boundary (topology)).In technical language, a manifold with boundary is a space containing both interior points and boundary points.  Every interior point has a neighborhood homeomorphic to the open n-ball {(x1, x2, …, xn) | Σxi2 < 1} .  Every boundary point has a neighborhood homeomorphic to the "half" n-ball  {(x1, x2, …, xn) | Σxi2 < 1 and x1 ≥ 0} .  The homeomorphism must send each boundary point to a point with x1 = 0.Let M be a manifold with boundary. The interior of M, denoted Int M, is the set of points in M which have neighborhoods homeomorphic to an open subset of Rn. The boundary of M, denoted ∂M, is the complement of Int M in M. The boundary points can be characterized as those points which land on the boundary hyperplane (xn = 0) of Rn+ under some coordinate chart.If M is a manifold with boundary of dimension n, then Int M is a manifold (without boundary) of dimension n and ∂M is a manifold (without boundary) of dimension n − 1.A single manifold can be constructed in different ways, each stressing a different aspect of the manifold, thereby  leading to a slightly different viewpoint.Perhaps the simplest way to construct a manifold is the one used in the example above of the circle. First, a subset of R2 is identified, and then an atlas covering this subset is constructed. The concept of manifold grew historically from constructions like this. Here is another example, applying this method to the construction of a sphere:A sphere can be treated in almost the same way as the circle. In mathematics a sphere is just the surface (not the solid interior), which can be defined as a subset of R3:The sphere is two-dimensional, so each chart will map part of the sphere to an open subset of R2. Consider the northern hemisphere, which is the part with positive z coordinate (coloured red in the picture on the right). The function χ defined bymaps the northern hemisphere to the open unit disc by projecting it on the (x, y) plane. A similar chart exists for the southern hemisphere. Together with two charts projecting on the (x, z) plane and two charts projecting on the (y, z) plane, an atlas of six charts is obtained which covers the entire sphere.This can be easily generalized to higher-dimensional spheres.A manifold can be constructed by gluing together pieces in a consistent manner, making them into overlapping charts. This construction is possible for any manifold and hence it is often used as a characterisation, especially for differentiable and Riemannian manifolds. It focuses on an atlas, as the patches naturally provide charts, and since there is no exterior space involved it leads to an intrinsic view of the manifold.The manifold is constructed by specifying an atlas, which is itself defined by transition maps. A point of the manifold is therefore an equivalence class of points which are mapped to each other by transition maps. Charts map equivalence classes to points of a single patch. There are usually strong demands on the consistency of the transition maps. For topological manifolds they are required to be homeomorphisms; if they are also diffeomorphisms, the resulting manifold is a differentiable manifold.This can be illustrated with the transition map t = 1⁄s from the second half of the circle example. Start with two copies of the line. Use the coordinate s for the first copy, and t for the second copy. Now, glue both copies together by identifying the point t on the second copy with the point s = 1⁄t on the first copy (the points t = 0 and s = 0 are not identified with any point on the first and second copy, respectively). This gives a circle.The first construction and this construction are very similar, but they represent rather different points of view. In the first construction, the manifold is seen as embedded in some Euclidean space. This is the extrinsic view. When a manifold is viewed in this way, it is easy to use intuition from Euclidean spaces to define additional structure. For example, in a Euclidean space it is always clear whether a vector at some point is tangential or normal to some surface through that point.The patchwork construction does not use any embedding, but simply views the manifold as a topological space by itself. This abstract point of view is called the intrinsic view. It can make it harder to imagine what a tangent vector might be, and there is no intrinsic notion of a normal bundle, but instead there is an intrinsic stable normal bundle.The n-sphere Sn is a generalisation of the idea of a circle (1-sphere) and sphere (2-sphere) to higher dimensions. An n-sphere Sn can be constructed by gluing together two copies of Rn. The transition map between them is defined asThis function is its own inverse and thus can be used in both directions. As the transition map is a smooth function, this atlas defines a smooth manifold.In the case n = 1, the example simplifies to the circle example given earlier.It is possible to define different points of a manifold to be same. This can be visualized as gluing these points together in a single point, forming a quotient space. There is, however, no reason to expect such quotient spaces to be manifolds. Among the possible quotient spaces that are not necessarily manifolds, orbifolds and CW complexes are considered to be relatively well-behaved. An example of a quotient space of a manifold that is also a manifold is the real projective space identified as a quotient space of the corresponding sphere.One method of identifying points (gluing them together) is through a right (or left) action of a group, which acts on the manifold. Two points are identified if one is moved onto the other by some group element. If M is the manifold and G is the group, the resulting quotient space is denoted by M / G (or G \ M).Manifolds which can be constructed by identifying points include tori and real projective spaces (starting with a plane and a sphere, respectively).Two manifolds with boundaries can be glued together along a boundary. If this is done the right way, the result is also a manifold. Similarly, two boundaries of a single manifold can be glued together.Formally, the gluing is defined by a bijection between the two boundaries[dubious  – discuss]. Two points are identified when they are mapped onto each other. For a topological manifold this bijection should be a homeomorphism, otherwise the result will not be a topological manifold. Similarly for a differentiable manifold it has to be a diffeomorphism. For other manifolds other structures should be preserved.A finite cylinder may be constructed as a manifold by starting with a strip [0, 1] × [0, 1] and gluing a pair of opposite edges on the boundary by a suitable diffeomorphism. A projective plane may be obtained by gluing a sphere with a hole in it to a Möbius strip along their respective circular boundaries.The Cartesian product of manifolds is also a manifold.The dimension of the product manifold is the sum of the dimensions of its factors. Its topology is the product topology, and a Cartesian product of charts is a chart for the product manifold. Thus, an atlas for the product manifold can be constructed using atlases for its factors. If these atlases define a differential structure on the factors, the corresponding atlas defines a differential structure on the product manifold. The same is true for any other structure defined on the factors. If one of the factors has a boundary, the product manifold also has a boundary. Cartesian products may be used to construct tori and finite cylinders, for example, as S1 × S1 and S1 × [0, 1], respectively.The study of manifolds combines many important areas of mathematics: it generalizes concepts such as curves and surfaces as well as ideas from linear algebra and topology.Before the modern concept of a manifold there were several important results.Non-Euclidean geometry considers spaces where Euclid's parallel postulate fails. Saccheri first studied such geometries in 1733 but sought only to disprove them. Gauss, Bolyai and Lobachevsky independently discovered them 100 years later. Their research uncovered two types of spaces whose geometric structures differ from that of classical Euclidean space; these gave rise to hyperbolic geometry and elliptic geometry. In the modern theory of manifolds, these notions correspond to Riemannian manifolds with constant negative and positive curvature, respectively.Carl Friedrich Gauss may have been the first to consider abstract spaces as mathematical objects in their own right.  His theorema egregium gives a method for computing the curvature of a surface without considering the ambient space in which the surface lies.  Such a surface would, in modern terminology, be called a manifold; and in modern terms, the theorem proved that the curvature of the surface is an intrinsic property. Manifold theory has come to focus exclusively on these intrinsic properties (or invariants), while largely ignoring the extrinsic properties of the ambient space.Another, more topological example of an intrinsic property of a manifold is its Euler characteristic. Leonhard Euler showed that for a convex polytope in the three-dimensional Euclidean space with V vertices (or corners), E edges, and F faces,The same formula will hold if we project the vertices and edges of the polytope onto a sphere, creating a topological map with V vertices, E edges, and F faces, and in fact, will remain true for any spherical map, even if it does not arise from any convex polytope.[3] Thus 2 is a topological invariant of the sphere, called its Euler characteristic.  On the other hand, a torus can be sliced open by its 'parallel' and 'meridian' circles, creating a map with V = 1 vertex, E = 2 edges, and F = 1 face. Thus the Euler characteristic of the torus is 1 − 2 + 1 = 0. The Euler characteristic of other surfaces is a useful topological invariant, which can be extended to higher dimensions using Betti numbers. In the mid nineteenth century, the Gauss–Bonnet theorem linked the Euler characteristic to the Gaussian curvature.Investigations of Niels Henrik Abel and Carl Gustav Jacobi on inversion of elliptic integrals in the first half of 19th century led them to consider special types of complex manifolds, now known as Jacobians. Bernhard Riemann further contributed to their theory, clarifying the geometric meaning of the process of analytic continuation of functions of complex variables.Another important source of manifolds in 19th century mathematics was analytical mechanics, as developed by Siméon Poisson, Jacobi, and William Rowan Hamilton. The possible states of a mechanical system are thought to be points of an abstract space, phase space in Lagrangian and Hamiltonian formalisms of classical mechanics. This space is, in fact, a high-dimensional manifold, whose dimension corresponds to the degrees of freedom of the system and where the points are specified by their generalized coordinates. For an unconstrained movement of free particles the manifold is equivalent to the Euclidean space, but various conservation laws constrain it to more complicated formations, e.g. Liouville tori. The theory of a rotating solid body, developed in the 18th century by Leonhard Euler and Joseph-Louis Lagrange, gives another example where the manifold is nontrivial. Geometrical and topological aspects of classical mechanics were emphasized by Henri Poincaré, one of the founders of topology.Riemann was the first one to do extensive work generalizing the idea of a surface to higher dimensions. The name manifold comes from Riemann's original German term, Mannigfaltigkeit, which William Kingdon Clifford translated as "manifoldness". In his Göttingen inaugural lecture, Riemann described the set of all possible values of a variable with certain constraints as a Mannigfaltigkeit, because the variable can have many values. He distinguishes between stetige Mannigfaltigkeit and diskrete Mannigfaltigkeit (continuous manifoldness and discontinuous manifoldness), depending on whether the value changes continuously or not. As continuous examples, Riemann refers to not only colors and the locations of objects in space, but also the possible shapes of a spatial figure. Using induction, Riemann constructs an n-fach ausgedehnte Mannigfaltigkeit (n times extended manifoldness or n-dimensional manifoldness) as a continuous stack of (n−1) dimensional manifoldnesses. Riemann's intuitive notion of a Mannigfaltigkeit evolved into what is today formalized as a manifold. Riemannian manifolds and Riemann surfaces are named after Riemann.In his very influential paper, Analysis Situs,[4] Henri Poincaré gave a definition of a (differentiable) manifold (variété) which served as a precursor to the modern concept of a manifold.[5]In the first section of Analysis Situs, Poincaré defines a manifold as the level set of a continuously differentiable function between Euclidean spaces that satisfies the nondegeneracy hypothesis of the implicit function theorem. In the third section, he begins by remarking that the graph of a continuously differentiable function is a manifold in the latter sense. He then proposes a new, more general, definition of manifold based on a 'chain of manifolds' (une chaîne des variétés).Hermann Weyl gave an intrinsic definition for differentiable manifolds in his lecture course on Riemann surfaces in 1911–1912, opening the road to the general concept of a topological space that followed shortly. During the 1930s Hassler Whitney and others clarified the foundational aspects of the subject, and thus intuitions dating back to the latter half of the 19th century became precise, and developed through differential geometry and Lie group theory. Notably, the Whitney embedding theorem[6] showed that the intrinsic definition in terms of charts was equivalent to Poincaré's definition in terms of subsets of Euclidean space.Two-dimensional manifolds, also known as a 2D surfaces embedded in our common 3D space, were considered by Riemann under the guise of Riemann surfaces, and rigorously classified in the beginning of the 20th century by Poul Heegaard and Max Dehn. Henri Poincaré pioneered the study of three-dimensional manifolds and raised a fundamental question about them, today known as the Poincaré conjecture.  After nearly a century of effort by many mathematicians, starting with Poincaré himself, Grigori Perelman proved the Poincaré conjecture (see the Solution of the Poincaré conjecture). William Thurston's geometrization program, formulated in the 1970s, provided a far-reaching extension of the Poincaré conjecture to the general three-dimensional manifolds. Four-dimensional manifolds were brought to the forefront of mathematical research in the 1980s by Michael Freedman and in a different setting, by Simon Donaldson, who was motivated by the then recent progress in theoretical physics (Yang–Mills theory), where they serve as a substitute for ordinary 'flat' spacetime.  Andrey Markov Jr. showed in 1960 that no algorithm exists for classifying four-dimensional manifolds. Important work on higher-dimensional manifolds, including analogues of the Poincaré conjecture, had been done earlier by René Thom, John Milnor, Stephen Smale and Sergei Novikov. One of the most pervasive and flexible techniques underlying much work on the topology of manifolds is Morse theory.The simplest kind of manifold to define is the topological manifold, which looks locally like some "ordinary" Euclidean space Rn. By definition, all manifolds are topological manifolds, so the phrase "topological manifold" is usually used to emphasize that a manifold lacks additional structure, or that only its topological properties are being considered. Formally, a topological manifold is a topological space locally homeomorphic to  a  Euclidean space. This means that every point has a neighbourhood for which there exists a homeomorphism (a bijective continuous function whose inverse is also continuous) mapping that neighbourhood to Rn. These homeomorphisms are the charts of the manifold.It is to be noted that a topological manifold looks locally like a Euclidean space in a rather weak manner: while for each individual chart it is possible to distinguish differentiable functions or measure distances and angles, merely by virtue of being a topological manifold a space does not have any particular and consistent choice of such concepts. In order to discuss such properties for a manifold, one needs to specify further structure and consider differentiable manifolds and Riemannian manifolds discussed below. In particular, the same underlying topological manifold can have several mutually incompatible classes of differentiable functions and an infinite number of ways to specify distances and angles.Usually additional technical assumptions on the topological space are made to exclude pathological cases. It is customary to require that the space be Hausdorff and second countable.The dimension of the manifold at a certain point is the dimension of the Euclidean space that the charts at that point map to (number n in the definition). All points in a connected manifold have the same dimension. Some authors require that all charts of a topological manifold map to Euclidean spaces of same dimension. In that case every topological manifold has a topological invariant, its dimension. Other authors allow disjoint unions of topological manifolds with differing dimensions to be called manifolds.For most applications a special kind of topological manifold, namely a differentiable manifold, is used. If the local charts on a manifold are compatible in a certain sense, one can define directions, tangent spaces, and differentiable functions on that manifold. In particular it is possible to use calculus on a differentiable manifold. Each point of an n-dimensional differentiable manifold has a tangent space. This is an n-dimensional Euclidean space consisting of the tangent vectors of the curves through the point.Two important classes of differentiable manifolds are smooth and analytic manifolds. For smooth manifolds the transition maps are smooth, that is infinitely differentiable. Analytic manifolds are smooth manifolds with the additional condition that the transition maps are analytic (they can be expressed as power series). The sphere can be given analytic structure, as can most familiar curves and surfaces.There are also topological manifolds, i.e., locally Euclidean spaces, which possess no differentiable structures at all.[7]A rectifiable set generalizes the idea of a piecewise smooth or rectifiable curve to higher dimensions; however, rectifiable sets are not in general manifolds.To measure distances and angles on manifolds, the manifold must be Riemannian. A 'Riemannian manifold' is a differentiable manifold in which each tangent space is equipped with an inner product ⟨⋅,⋅⟩ in a manner which varies smoothly from point to point. Given two tangent vectors u and v, the inner product ⟨u,v⟩ gives a real number. The dot (or scalar) product is a typical example of an inner product. This allows one to define various notions such as length, angles, areas (or volumes), curvature and divergence of vector fields.All differentiable manifolds (of constant dimension) can be given the structure of a Riemannian manifold. The Euclidean space itself carries a natural structure of Riemannian manifold (the tangent spaces are naturally identified with the Euclidean space itself and carry the standard scalar product of the space). Many familiar curves and surfaces, including for example all n-spheres, are specified as subspaces of a Euclidean space and inherit a metric from their embedding in it.A Finsler manifold allows the definition of distance but does not require the concept of angle; it is an analytic manifold in which each tangent space is equipped with a norm, ||·||, in a manner which varies smoothly from point to point. This norm can be extended to a metric, defining the length of a curve; but it cannot in general be used to define an inner product.Any Riemannian manifold is a Finsler manifold.Lie groups, named after Sophus Lie, are differentiable manifolds that carry also the structure of a group which is such that the group operations are defined by smooth maps.A Euclidean vector space with the group operation of vector addition is an example of a non-compact Lie group. A simple example of a compact Lie group is the circle: the group operation is simply rotation. This group, known as U(1), can be also characterised as the group of complex numbers of modulus 1 with multiplication as the group operation.Other examples of Lie groups include special groups of matrices, which are all subgroups of the general linear group, the group of n by n matrices with non-zero determinant. If the matrix entries are real numbers, this will be an n2-dimensional disconnected manifold. The orthogonal groups, the symmetry groups of the sphere and hyperspheres, are n(n−1)/2 dimensional manifolds, where n−1 is the dimension of the sphere. Further examples can be found in the table of Lie groups.Different notions of manifolds have different notions of classification and invariant; in this section we focus on smooth closed manifolds.The classification of smooth closed manifolds is well understood in principle, except in dimension 4: in low dimensions (2 and 3) it is geometric, via the uniformization theorem and the solution of the Poincaré conjecture, and in high dimension (5 and above) it is algebraic, via surgery theory. This is a classification in principle: the general question of whether two smooth manifolds are diffeomorphic is not computable in general. Further, specific computations remain difficult, and there are many open questions.Orientable surfaces can be visualized, and their diffeomorphism classes enumerated, by genus. Given two orientable surfaces, one can determine if they are diffeomorphic by computing their respective genera and comparing: they are diffeomorphic if and only if the genera are equal, so the genus forms a complete set of invariants.This is much harder in higher dimensions: higher-dimensional manifolds cannot be directly visualized (though visual intuition is useful in understanding them), nor can their diffeomorphism classes be enumerated, nor can one in general determine if two different descriptions of a higher-dimensional manifold refer to the same object.However, one can determine if two manifolds are different if there is some intrinsic characteristic that differentiates them. Such criteria are commonly referred to as invariants, because, while they may be defined in terms of some presentation (such as the genus in terms of a triangulation), they are the same relative to all possible descriptions of a particular manifold: they are invariant under different descriptions.Naively, one could hope to develop an arsenal of invariant criteria that would definitively classify all manifolds up to isomorphism.Unfortunately, it is known that for manifolds of dimension 4 and higher, no program exists that can decide whether two manifolds are diffeomorphic.Smooth manifolds have a rich set of invariants, coming from point-set topology,classic algebraic topology, and geometric topology. The most familiar invariants, which are visible for surfaces, are orientability (a normal invariant, also detected by homology) and genus (a homological invariant).Smooth closed manifolds have no local invariants (other than dimension), though geometric manifolds have local invariants, notably the curvature of a Riemannian manifold and the torsion of a manifold equipped with an affine connection.This distinction between local invariants and no local invariants is a common way to distinguish between geometry and topology. All invariants of a smooth closed manifold are thus global.Algebraic topology is a source of a number of important global invariant properties.  Some key criteria include the simply connected property and orientability (see below).  Indeed, several branches of mathematics, such as homology and homotopy theory, and the theory of characteristic classes were founded in order to study invariant properties of manifolds.In dimensions two and higher, a simple but important invariant criterion is the question of whether a manifold admits a meaningful orientation. Consider a topological manifold with charts mapping to Rn. Given an ordered basis for Rn, a chart causes its piece of the manifold to itself acquire a sense of ordering, which in 3-dimensions can be viewed as either right-handed or left-handed. Overlapping charts are not required to agree in their sense of ordering, which gives manifolds an important freedom. For some manifolds, like the sphere, charts can be chosen so that overlapping regions agree on their "handedness"; these are orientable manifolds. For others, this is impossible. The latter possibility is easy to overlook, because any closed surface embedded (without self-intersection) in three-dimensional space is orientable.Some illustrative examples of non-orientable manifolds include: (1) the Möbius strip, which is a manifold with boundary, (2) the Klein bottle, which must intersect itself in its 3-space representation, and (3) the real projective plane, which arises naturally in geometry. Begin with an infinite circular cylinder standing vertically, a manifold without boundary. Slice across it high and low to produce two circular boundaries, and the cylindrical strip between them. This is an orientable manifold with boundary, upon which "surgery" will be performed. Slice the strip open, so that it could unroll to become a rectangle, but keep a grasp on the cut ends. Twist one end 180°, making the inner surface face out, and glue the ends back together seamlessly. This results in a strip with a permanent half-twist: the Möbius strip. Its boundary is no longer a pair of circles, but (topologically) a single circle; and what was once its "inside" has merged with its "outside", so that it now has only a single side.Take two Möbius strips; each has a single loop as a boundary. Straighten out those loops into circles, and let the strips distort into cross-caps.  Gluing the circles together will produce a new, closed manifold without boundary, the Klein bottle. Closing the surface does nothing to improve the lack of orientability, it merely removes the boundary. Thus, the Klein bottle is a closed surface with no distinction between inside and outside. Note that in three-dimensional space, a Klein bottle's surface must pass through itself. Building a Klein bottle which is not self-intersecting requires four or more dimensions of space.Begin with a sphere centered on the origin. Every line through the origin pierces the sphere in two opposite points called antipodes. Although there is no way to do so physically, it is possible (by considering a quotient space) to mathematically merge each antipode pair into a single point. The closed surface so produced is the real projective plane, yet another non-orientable surface. It has a number of equivalent descriptions and constructions, but this route explains its name: all the points on any given line through the origin project to the same "point" on this "plane".For two dimensional manifolds a key invariant property is the genus, or the "number of handles" present in a surface. A torus is a sphere with one handle, a double torus is a sphere with two handles, and so on.  Indeed, it is possible to fully characterize compact, two-dimensional manifolds on the basis of genus and orientability.  In higher-dimensional manifolds genus is replaced by the notion of Euler characteristic, and more generally Betti numbers and homology and cohomology.Just as there are various types of manifolds, there are various types of maps of manifolds. In addition to continuous functions and smooth functions generally, there are maps with special properties. In geometric topology a basic type are embeddings, of which knot theory is a central example, and generalizations such as immersions, submersions, covering spaces, and ramified covering spaces.Basic results include the Whitney embedding theorem and Whitney immersion theorem.In Riemannian geometry, one may ask for maps to preserve the Riemannian metric, leading to notions of isometric embeddings, isometric immersions, and Riemannian submersions; a basic result is the Nash embedding theorem.A basic example of maps between manifolds are scalar-valued functions on a manifold,sometimes called regular functions or functionals, by analogy with algebraic geometry or linear algebra. These are of interest both in their own right, and to study the underlying manifold.In geometric topology, most commonly studied are Morse functions, which yield handlebody decompositions, while in mathematical analysis, one often studies solution to partial differential equations, an important example of which is harmonic analysis, where one studies harmonic functions: the kernel of the Laplace operator. This leads to such functions as the spherical harmonics, and to heat kernel methods of studying manifolds, such as hearing the shape of a drum and some proofs of the Atiyah–Singer index theorem.
Remez algorithm
The Remez algorithm  or Remez exchange algorithm, published by Evgeny Yakovlevich Remez in 1934,[1] is an iterative algorithm used to find simple approximations to functions, specifically, approximations by functions in a Chebyshev space that are the best in the uniform norm L∞ sense.A typical example of a Chebyshev space is the subspace of Chebyshev polynomials of order n in the space of real continuous functions on an interval, C[a, b].The polynomial of best approximation within a given subspace is defined to be the one that minimizes the maximum absolute difference between the polynomial and the function. In this case, the form of the solution is precised by the equioscillation theorem.The result is called the polynomial of best approximation or the minimax approximation algorithm.A review of technicalities in implementing the Remez algorithm is given by W. Fraser.[2]The Chebyshev nodes are a common choice for the initial approximation because of their role in the theory of polynomial interpolation. For the initialization of the optimization problem for function f by the Lagrange interpolant Ln(f), it can be shown that this initial approximation is bounded bywith the norm or Lebesgue constant of the Lagrange interpolation operator Ln of the nodes (t1, ..., tn + 1) beingT being the zeros of the Chebyshev polynomials, and the Lebesgue functions beingFor Chebyshev nodes, which provides a suboptimal, but analytically explicit choice, the asymptotic behavior is known as[5](γ being the Euler-Mascheroni constant) withand upper bound[6]This section provides more information on the steps outlined above. In this section, the index i runs from 0 to n+1.The error at the given n+2 ordered nodes is positive and negative in turn becauseSometimes more than one sample point is replaced at the same time with the locations of nearby maximum absolute differences.Sometimes relative error is used to measure the difference between the approximation and the function, especially if the approximation will be used to compute the function on a computer which uses floating point arithmetic.
Shear matrix
In mathematics, a shear matrix or transvection is an elementary matrix that represents the addition of a multiple of one row or column to another. Such a matrix may be derived by taking the identity matrix and replacing one of the zero elements with a non-zero value.A typical shear matrix is shown below:The name shear reflects the fact that the matrix represents a shear transformation. Geometrically, such a transformation takes pairs of points in a linear space, that are purely axially separated along the axis whose row in the matrix contains the shear element, and effectively replaces those pairs by pairs whose separation is no longer purely axial but has two vector components. Thus, the shear axis is always an eigenvector of S.If S is an n×n shear matrix, then:
Free module
In mathematics, a free module is a module that has a basis – that is, a generating set consisting of linearly independent elements. Every vector space is a free module,[1] but, if the ring of the coefficients is not a division ring (not a field in the commutative case), then there exist non-free modules.Given any set S and ring R, there is a free R-module with basis S, which is called free module on S or module of formal linear combinations of the elements of S.A free abelian group is precisely a free module over the ring Z of integers.A free module is a module with a basis.[2]An immediate consequence of the second half of the definition is that the coefficients in the first half are unique for each element of M.Let R be a ring.Given a set E and ring R, there is a free R-module that has E as a basis: namely, the direct sum of copies of R indexed by EA similar argument shows that every free left (resp. right) R-module is isomorphic to a direct sum of copies of R as left (resp. right) module.The free module R(E) may also be constructed in the following equivalent way.Given a ring R and a set E, first as a set we letWe equip it with a structure of a left module such that the addition is defined by: for x in E,and the scalar multiplication by: for r in R and x in E,Many statements about free modules, which are wrong for general modules over rings, are still true for certain generalisations of free modules. Projective modules are direct summands of free modules, so one can choose an injection in a free module and use the basis of this one to prove something for the projective module. Even weaker generalisations are flat modules, which still have the property that tensoring with them preserves exact sequences, and torsion-free modules. If the ring has special properties, this hierarchy may collapse, e.g., for any perfect local Dedekind ring, every torsion-free module is flat, projective and free as well. A finitely generated torsion-free module of a commutative PID is free. A finitely generated Z-module is free if and only if it is flat.See local ring, perfect ring and Dedekind ring.This article incorporates material from free vector space over a set on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
Permanent (mathematics)
In linear algebra, the permanent of a square matrix is a function of the matrix similar to the determinant. The permanent, as well as the determinant, is a polynomial in the entries of the matrix.[1] Both are special cases of a more general function of a matrix called the immanant.The permanent of an n-by-n matrix A = (ai,j) is defined asThe sum here extends over all elements σ of the symmetric group Sn; i.e. over all permutations of the numbers 1, 2, ..., n.For example,andThe definition of the permanent of A differs from that of the determinant of A in that the signatures of the permutations are not taken into account.The word, permanent, originated with Cauchy in 1812 as “fonctions symétriques permanentes” for a related type of function,[2] and was used by Muir (1882) in the modern, more specific, sense.[3]On the other hand, the basic multiplicative property of determinants is not valid for permanents.[6] A simple example shows that this is so.A formula similar to Laplace's for the development of a determinant along a row, column or diagonal is also valid for the permanent;[7] all signs have to be ignored for the permanent. For example, expanding along the first column,while expanding along the last row gives,Unlike the determinant, the permanent has no easy geometrical interpretation; it is mainly used in combinatorics, in treating boson Green's functions in quantum field theory, and in determining state probabilities of boson sampling systems[8]. However, it has two graph-theoretic interpretations: as the sum of weights of cycle covers of a directed graph, and as the sum of weights of perfect matchings in a bipartite graph.If the weight of a cycle-cover is defined to be the product of the weights of the arcs in each cycle, thenThus the permanent of A is equal to the sum of the weights of all perfect matchings of the graph.The answers to many counting questions can be computed as permanents of matrices that only have 0 and 1 as entries. Let Ω(n,k) be the class of all (0, 1)-matrices of order n with each row and column sum equal to k. Every matrix A in this class has perm(A) > 0.[10] The incidence matrices of projective planes are in the class Ω(n2 + n + 1, n + 1) for n an integer > 1. The permanents corresponding to the smallest projective planes have been calculated. For n = 2, 3, and 4 the values are 24, 3852 and 18,534,400 respectively.[10] Let Z be the incidence matrix of the projective plane with n = 2, the Fano plane. Remarkably, perm(Z) = 24 = |det (Z)|, the absolute value of the determinant of Z. This is a consequence of Z being a circulant matrix and the theorem:[11]where J is the n×n all 1's matrix and I is the identity matrix, and the ménage numbers are given bywhere I' is the (0, 1)-matrix with nonzero entries in positions (i, i + 1) and (n, 1).The Bregman–Minc inequality, conjectured by H. Minc in 1963[12] and proved by L. M. Brégman in 1973,[13] gives an upper bound for the permanent of an n × n (0, 1)-matrix.  If A has ri ones in row i for each 1 ≤ i ≤ n, the inequality states thatIn 1926 Van der Waerden conjectured that the minimum permanent among all n × n doubly stochastic matrices is n!/nn, achieved by the matrix for which all entries are equal to 1/n.[14] Proofs of this conjecture were published in 1980 by B. Gyires[15] and in 1981 by G. P. Egorychev[16] and D. I. Falikman;[17] Egorychev's proof is an application of the Alexandrov–Fenchel inequality.[18] For this work, Egorychev and Falikman won the Fulkerson Prize in 1982.[19]It may be rewritten in terms of the matrix entries as follows:MacMahon's Master Theorem relating permanents and determinants is:[24]where P(n,m) is the set of all m-permutations of the n-set {1,2,...,n}.[26]The generalization of the definition of a permanent to non-square matrices allows the concept to be used in a more natural way in some applications. For instance:Let S1, S2, ..., Sm be subsets (not necessarily distinct) of an n-set with m ≤ n. The incidence matrix of this collection of subsets is an m × n (0,1)-matrix A. The number of systems of distinct representatives (SDR's) of this collection is perm(A).[27]
Normal matrix
In mathematics, a complex square matrix A is normal if it commutes with its conjugate transpose A*:The concept of normal matrices can be extended to normal operators on infinite dimensional normed spaces and to normal elements in C*-algebras. As in the matrix case, normality means commutativity is preserved, to the extent possible, in the noncommutative setting. This makes normal operators, and normal elements of C*-algebras, more amenable to analysis.Spectral theorem states that a matrix is normal if and only if it is unitarily similar to a diagonal matrix, and therefore any matrix A satisfying the equation A∗A = AA∗ is diagonalizable.Among complex matrices, all unitary, Hermitian, and skew-Hermitian matrices are normal. Likewise, among real matrices, all orthogonal, symmetric, and skew-symmetric matrices are normal. However, it is not the case that all normal matrices are either unitary or (skew-)Hermitian. For example,is neither unitary, Hermitian, nor skew-Hermitian, yet it is normal because Let A be a normal upper triangular matrix. Since (A∗A)ii = (AA∗)ii, one has ⟨ei, A*Aei⟩ = ⟨ei, AA*ei⟩; i.e., the first row must have the same norm as the first column:The first entry of row 1 and column 1 are the same, and the rest of column 1 is zero. This implies the first row must be zero for entries 2 through n. Continuing this argument for row–column pairs 2 through n shows A is diagonal.The concept of normality is important because normal matrices are precisely those to which the spectral theorem applies: The diagonal entries of Λ are the eigenvalues of A, and the columns of U are the eigenvectors of A. The matching eigenvalues in Λ come in the same order as the eigenvectors are ordered as columns of U.Another way of stating the spectral theorem is to say that normal matrices are precisely those matrices that can be represented by a diagonal matrix with respect to a properly chosen orthonormal basis of Cn. Phrased differently: a matrix is normal if and only if its eigenspaces span Cn and are pairwise orthogonal with respect to the standard inner product of Cn.The spectral theorem for normal matrices is a special case of the more general Schur decomposition which holds for all square matrices. Let A be a square matrix. Then by Schur decomposition it is unitary similar to an upper-triangular matrix, say, B. If A is normal, so is B. But then B must be diagonal, for, as noted above, a normal upper-triangular matrix is diagonal.The spectral theorem permits the classification of normal matrices in terms of their spectra, for example: In general, the sum or product of two normal matrices need not be normal. However, the following holds: In this special case, the columns of U∗ are eigenvectors of both A and B and form an orthonormal basis in Cn. This follows by combining the theorems that, over an algebraically closed field, commuting matrices are simultaneously triangularizable and a normal matrix is diagonalizable – the added result is that these can both be done simultaneously.It is possible to give a fairly long list of equivalent definitions of a normal matrix. Let A be a n × n complex matrix. Then the following are equivalent:Some but not all of the above generalize to normal operators on infinite-dimensional Hilbert spaces. For example, a bounded operator satisfying (9) is only quasinormal.It is occasionally useful (but sometimes misleading) to think of the relationships of different kinds of normal matrices as analogous to the relationships between different kinds of complex numbers:As a special case, the complex numbers may be embedded in the normal 2 × 2 real matrices by the mapping which preserves addition and multiplication. It is easy to check that this embedding respects all of the above analogies.
Non-negative matrix factorization
Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.NMF finds applications in such fields as astronomy,[3][4][5] computer vision, document clustering,[1] chemometrics, audio signal processing, recommender systems,[6][7] and bioinformatics.[8]In chemometrics non-negative matrix factorization has a long history under the name "self modeling curve resolution".[9]In this framework the vectors in the right matrix are continuous curves rather than discrete vectors.Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name positive matrix factorization.[10][11]It became more widely known as non-negative matrix factorization after Lee and Seung investigatedthe properties of the algorithm and published some simple and usefulalgorithms for two types of factorizations.[12][13]Let matrix V be the product of the matrices W and H,Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H.  That is, each column of V can be computed as follows:where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m × n matrix, W is an m × p matrix, and H is a p × n matrix then p can be significantly less than both m and n.Here is an example based on a text-mining application: This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H.When the error function to be used is Kullback–Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.[15]Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V.  The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization.[17][18][19] The problem of finding the NRF of V, if it exists, is known to be NP-hard.[20]There are different types of non-negative matrix factorizations.The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.[1]Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback–Leibler divergence to positive matrices (the original Kullback–Leibler divergence is defined on probability distributions).Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.Another type of NMF for images is based on the total variation norm.[21]When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,[22][23]although it may also still be referred to as NMF.[24]Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.[25][26][27]There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule[13] has been a popular method due to the simplicity of implementation.  This algorithm is:Note that the updates are done on an element by element basis not matrix multiplication.We note that W and H multiplicative factor is identity matrix when V = W H.More recently other algorithms have been developed.Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[28] or different, as some NMF variants regularize one of W and H.[22] Specific approaches include the projected gradient descent methods,[28][29] the active set method,[6][30] the optimal gradient method,[31] and the block principal pivoting method[32] among several others.[33]Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete.[34] However, as in many other data mining applications, a local minimum may still prove to be useful.The contribution of the sequential NMF components can be compared with the Karhunen–Loève theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the "elbow" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting.[36][37] For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA,[5] which is the indication of less over-fitting of sequential NMF.Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981.[38] Kalofolias and Gallopoulos (2012)[39] solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.[40]In Learning the parts of objects by non-negative matrix factorization Lee and Seung[41] proposed NMF mainly for parts-based decomposition of images.  It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.It was later shown that some types of NMF are an instance of a more general probabilistic model called "multinomial PCA".[42]When NMF is obtained by minimizing the Kullback–Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,[43]trained by maximum likelihood estimation.That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.[14][44]  This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with "semi-NMF".[16]NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.[45]NMF extends beyond matrices to tensors of arbitrary order.[46][47][48] This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.Other extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.[49]NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.[50]The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,[51]More control over the non-uniqueness of NMF is obtained with sparsity constraints.[52]In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations [3][4] and the direct imaging observations [5] as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) [4] takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) [35] where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) [5] to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.Ren et al. (2018) [5] are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10⁵ to 10¹⁰, various statistical methods have been adopted,[53][54][36] however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux.[55][37] Forward modeling is currently optimized for point sources,[37] however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors,[5] rather than a computationally intensive data re-reduction on generated models.NMF can be used for text mining applications.In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents.This matrix is factored into a term-feature and a feature-document matrix.The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.[56]Another research group clustered parts of the Enron email dataset[57]with 65,033 messages and 91,133 terms into 50 clusters.[58]NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.[59]Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.[40]NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.[60]Speech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al.[63] use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters.[23][64][65][66] In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.[67]NMF, also referred in this field as factor analysis, has been used since the 1980s[68] to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.[69]Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,
Levi-Civita symbol
In mathematics, particularly in linear algebra, tensor analysis, and differential geometry, the Levi-Civita symbol represents a collection of numbers; defined from the sign of a permutation of the natural numbers 1, 2, …, n, for some positive integer n. It is named after the Italian mathematician and physicist Tullio Levi-Civita. Other names include the permutation symbol, antisymmetric symbol, or alternating symbol, which refer to its antisymmetric property and definition in terms of permutations.The standard letters to denote the Levi-Civita symbol are the Greek lower case epsilon ε or ϵ, or less commonly the Latin lower case e. Index notation allows one to display permutations in a way compatible with tensor analysis:where each index i1, i2, ..., in takes values 1, 2, ..., n. There are nn indexed values of εi1i2…in, which can be arranged into an n-dimensional array. The key defining property of the symbol is total antisymmetry in all the indices. When any two indices are interchanged, equal or not, the symbol is negated:If any two indices are equal, the symbol is zero. When all indices are unequal, we have:where p (called the parity of the permutation) is the number of pairwise interchanges of indices necessary to unscramble i1, i2, ..., in into the order 1, 2, ..., n, and the factor (−1)p is called the sign or signature of the permutation. The value ε1 2 ... n must be defined, else the particular values of the symbol for all permutations are indeterminate. Most authors choose ε1 2 ... n = +1, which means the Levi-Civita symbol equals the sign of a permutation when the indices are all unequal. This choice is used throughout this article.The term "n-dimensional Levi-Civita symbol" refers to the fact that the number of indices on the symbol n matches the dimensionality of the vector space in question, which may be Euclidean or non-Euclidean, for example, ℝ3 or Minkowski space. The values of the Levi-Civita symbol are independent of any metric tensor and coordinate system. Also, the specific term "symbol" emphasizes that it is not a tensor because of how it transforms between coordinate systems; however it can be interpreted as a tensor density.The Levi-Civita symbol allows the determinant of a square matrix, and the cross product of two vectors in three-dimensional Euclidean space, to be expressed in index notation.The Levi-Civita symbol is most often used in three and four dimensions, and to some extent in two dimensions, so these are given here before defining the general case.In two dimensions, Levi-Civita symbol is defined by:The values can be arranged into a 2 × 2 antisymmetric matrix:Use of the two-dimensional symbol is relatively uncommon, although in certain specialized topics like supersymmetry[1] and twistor theory[2] it appears in the context of 2-spinors. The three- and higher-dimensional Levi-Civita symbols are used more commonly.In three dimensions, the Levi-Civita symbol is defined by:[3]That is, εijk is 1 if (i, j, k) is an even permutation of (1, 2, 3), −1 if it is an odd permutation, and 0 if any index is repeated. In three dimensions only, the cyclic permutations of (1, 2, 3) are all even permutations, similarly the anticyclic permutations are all odd permutations. This means in 3d it is sufficient to take cyclic or anticyclic permutations of (1, 2, 3) and easily obtain all the even or odd permutations.Analogous to 2-dimensional matrices, the values of the 3-dimensional Levi-Civita symbol can be arranged into a 3 × 3 × 3 array:where i is the depth (blue: i = 1; red: i = 2; green: i = 3), j is the row and k is the column.Some examples:In four dimensions, the Levi-Civita symbol is defined by:These values can be arranged into a 4 × 4 × 4 × 4 array, although in 4 dimensions and higher this is difficult to draw.Some examples:More generally, in n dimensions, the Levi-Civita symbol is defined by:[4]Thus, it is the sign of the permutation in the case of a permutation, and zero otherwise.Using the capital pi notation ∏ for ordinary multiplication of numbers, an explicit expression for the symbol is:where the signum function (denoted sgn) returns the sign of its argument while discarding the absolute value if nonzero. The formula is valid for all index values, and for any n (when n = 0 or n = 1, this is the empty product). However, computing the formula above naively has a time complexity of O(n2), whereas the sign can be computed from the parity of the permutation from its disjoint cycles in only O(n log(n)) cost.A tensor whose components in an orthonormal basis are given by the Levi-Civita symbol (a tensor of covariant rank n) is sometimes called a permutation tensor.Under the ordinary transformation rules for tensors the Levi-Civita symbol is unchanged under pure rotations, consistent with that it is (by definition) the same in all coordinate systems related by orthogonal transformations. However, the Levi-Civita symbol is a pseudotensor because under an orthogonal transformation of Jacobian determinant −1, for example, a reflection in an odd number of dimensions, it should acquire a minus sign if it were a tensor. As it does not change at all, the Levi-Civita symbol is, by definition, a pseudotensor. As the Levi-Civita symbol is a pseudotensor, the result of taking a cross product is a pseudovector, not a vector.[5]Under a general coordinate change, the components of the permutation tensor are multiplied by the Jacobian of the transformation matrix. This implies that in coordinate frames different from the one in which the tensor was defined, its components can differ from those of the Levi-Civita symbol by an overall factor. If the frame is orthonormal, the factor will be ±1 depending on whether the orientation of the frame is the same or not.[5]In index-free tensor notation, the Levi-Civita symbol is replaced by the concept of the Hodge dual.In a context where tensor index notation is used to manipulate tensor components, the Levi-Civita symbol may be written with its indices as either subscripts or superscripts with no change in meaning, as might be convenient. Thus, one could writeIn these examples, superscripts should be considered equivalent with subscripts.Summation symbols can be eliminated by using Einstein notation, where an index repeated between two or more terms indicates summation over that index.  For example,In the following examples, Einstein notation is used.In two dimensions, when all i, j, m, n each take the values 1 and 2,[3]    ( 1)    ( 2)    ( 3)In three dimensions, when all i, j, k, m, n each take values 1, 2, and 3:[3]    ( 4)    ( 5)    ( 6)The Levi-Civita symbol is related to the Kronecker delta. In three dimensions, the relationship is given by the following equations (vertical lines denote the determinant):[4]A special case of this result is (4):sometimes called the "contracted epsilon identity".In Einstein notation, the duplication of the i index implies the sum on i. The previous is then denoted εijkεimn = δjmδkn − δjnδkm.In n dimensions, when all i1, …,in, j1, ..., jn take values 1, 2, ..., n:    ( 7)    ( 8)    ( 9)where the exclamation mark (!) denotes the factorial, and δα…β… is the generalized Kronecker delta. For any n, the propertyfollows from the facts that In general, for n dimensions, one can write the product of two Levi-Civita symbols as:For (1), both sides are antisymmetric with respect of ij and mn. We therefore only need to consider the case i ≠ j and m ≠ n. By substitution, we see that the equation holds for ε12ε12, that is, for i = m = 1 and j = n = 2. (Both sides are then one). Since the equation is antisymmetric in ij and mn, any set of values for these can be reduced to the above case (which holds). The equation thus holds for all values of ij and mn.Using (1), we have for (2)Here we used the Einstein summation convention with i going from 1 to 2. Next, (3) follows similarly from (2).To establish (5), notice that both sides vanish when i ≠ j. Indeed, if i ≠ j, then one can not choose m and n such that both permutation symbols on the left are nonzero. Then, with i = j fixed, there are only two ways to choose m and n from the remaining two indices. For any such indices, we have(no summation), and the result follows.Then (6) follows since 3! = 6 and for any distinct indices i, j, k taking values 1, 2, 3, we have In linear algebra, the determinant of a 3 × 3 square matrix A = [aij] can be written[6]Similarly the determinant of an n × n matrix A = [aij] can be written as[5]where each ir should be summed over 1, …, n, or equivalently:where now each ir and each jr should be summed over 1, …, n. More generally, we have the identity[5]If a = (a1, a2, a3) and b = (b1, b2, b3) are vectors in ℝ3 (represented in some right-handed coordinate system using an orthonormal basis), their cross product can be written as a determinant:[5]hence also using the Levi-Civita symbol, and more simply:In Einstein notation, the summation symbols may be omitted, and the ith component of their cross product equals[4]The first component isthen by cyclic permutations of 1, 2, 3 the others can be derived immediately, without explicitly calculating them from the above formulae:From the above expression for the cross product, we have:If c = (c1, c2, c3) is a third vector, then the triple scalar product equalsFrom this expression, it can be seen that the triple scalar product is antisymmetric when exchanging any pair of arguments. For example,If F = (F1, F2, F3) is a vector field defined on some open set of ℝ3 as a function of position x = (x1, x2, x3) (using Cartesian coordinates). Then the ith component of the curl of F equals[4]which follows from the cross product expression above, substituting components of the gradient vector operator (nabla).In any arbitrary curvilinear coordinate system and even in the absence of a metric on the manifold, the Levi-Civita symbol as defined above may be considered to be a tensor density field in two different ways. It may be regarded as a contravariant tensor density of weight +1 or as a covariant tensor density of weight −1. In n dimensions using the generalized Kronecker delta,[7][8]Notice that these are numerically identical. In particular, the sign is the same.On a pseudo-Riemannian manifold, one may define a coordinate-invariant covariant tensor field whose coordinate representation agrees with the Levi-Civita symbol wherever the coordinate system is such that the basis of the tangent space is orthonormal with respect to the metric and matches a selected orientation. This tensor should not be confused with the tensor density field mentioned above. The presentation in this section closely follows Carroll 2004.The covariant Levi-Civita tensor (also known as the Riemannian volume form) in any coordinate system that matches the selected orientation iswhere gab is the representation of the metric in that coordinate system. We can similarly consider a contravariant Levi-Civita tensor by raising the indices with the metric as usual,but notice that if the metric signature contains an odd number of negatives q, then the sign of the components of this tensor differ from the standard Levi-Civita symbol:From this we can infer the identity,whereis the generalized Kronecker delta.In Minkowski space (the four-dimensional spacetime of special relativity), the covariant Levi-Civita tensor iswhere the sign depends on the orientation of the basis.  The contravariant Levi-Civita tensor isThe following are examples of the general identity above specialized to Minkowski space (with the negative sign arising from the odd number of negatives in the signature of the metric tensor in either sign convention):This article incorporates material from Levi-Civita permutation symbol on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
Monte Carlo method
Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. Their essential idea is using randomness to solve problems that might be deterministic in principle. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three problem classes:[1] optimization, numerical integration, and generating draws from a probability distribution.In physics-related problems, Monte Carlo methods are useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean-Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals  with complicated boundary conditions.  In application to systems engineering problems (space, oil exploration, aircraft design, etc.) problems, Monte Carlo–based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative "soft" methods.[2]In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is parametrized, mathematicians often use a Markov chain Monte Carlo (MCMC) sampler.[3][4][5][6] The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. That is, in the limit, the samples being generated by the MCMC method will be samples from the desired (target) distribution.[7] By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler.In other problems, the objective is generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depend on the distributions of the current random states (see McKean-Vlasov processes, nonlinear filtering equation).[8][9] In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain.[9][10]  A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and MCMC methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.Monte Carlo methods vary, but tend to follow a particular pattern:For example, consider a quadrant inscribed in a unit square. Given that the ratio of their areas is π/4, the value of π can be approximated using a Monte Carlo method:[11]In this procedure the domain of inputs is the square that circumscribes the quadrant.  We generate random inputs by scattering grains over the square then perform a computation on each input (test whether it falls within the quadrant). Aggregating the results yields our final result, the approximation of π.There are two important points:Uses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of pseudorandom number generators, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling.Before the Monte Carlo method was developed, simulations tested a previously understood deterministic problem, and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a probabilistic analog (see Simulated annealing).An early variant of the Monte Carlo method can be seen in the Buffon's needle experiment, in which π can be estimated by dropping needles on a floor made of parallel and equidistant strips. In the 1930s, Enrico Fermi first experimented with the Monte Carlo method while studying neutron diffusion, but did not publish anything on it.[12]The modern version of the Markov Chain Monte Carlo method was invented in the late 1940s by Stanislaw Ulam, while he was working on nuclear weapons projects at the Los Alamos National Laboratory. Immediately after Ulam's breakthrough, John von Neumann understood its importance and programmed the ENIAC computer to carry out Monte Carlo calculations. In 1946, physicists at Los Alamos Scientific Laboratory were investigating radiation shielding and the distance that neutrons would likely travel through various materials. Despite having most of the necessary data, such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus, and how much energy the neutron was likely to give off following a collision, the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods. Ulam had the idea of using random experiments. He recounts his inspiration as follows:Being secret, the work of von Neumann and Ulam required a code name.[14] A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name Monte Carlo, which refers to the Monte Carlo Casino in Monaco where Ulam's uncle would borrow money from relatives to gamble.[12] Using lists of "truly random" random numbers was extremely slow, but von Neumann developed a way to calculate pseudorandom numbers, using the middle-square method. Though this method has been criticized as crude, von Neumann was aware of this: he justified it as being faster than any other method at his disposal, and also noted that when it went awry it did so obviously, unlike methods that could be subtly incorrect.Monte Carlo methods were central to the simulations required for the Manhattan Project, though severely limited by the computational tools at the time. In the 1950s they were used at Los Alamos for early work relating to the development of the hydrogen bomb, and became popularized in the fields of physics, physical chemistry, and operations research. The Rand Corporation and the U.S. Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.The theory of more sophisticated mean field type particle Monte Carlo methods had certainly started by the mid-1960s, with the work of Henry P. McKean Jr. on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics.[15][16] We also quote an earlier pioneering article by Theodore E. Harris and Herman Kahn, published in 1951, using mean field genetic-type Monte Carlo methods for estimating particle transmission energies.[17] Mean field genetic type Monte Carlo methodologies are also used as heuristic natural search algorithms (a.k.a. Metaheuristic) in evolutionary computing. The origins of these mean field computational techniques can be traced to 1950 and 1954 with the work of Alan Turing on genetic type mutation-selection learning machines[18] and the articles by Nils Aall Barricelli at the Institute for Advanced Study in Princeton, New Jersey.[19][20]Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods can also be interpreted as a mean field particle Monte Carlo approximation of Feynman-Kac path integrals.[21][22][23][24][25][26][27] The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions,[28] but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984[27] In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth.[29]The use of Sequential Monte Carlo in advanced signal processing and Bayesian inference is more recent. It was in 1993, that Gordon et al., published in their seminal work[30] the first application of a Monte Carlo resampling algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. We also quote another pioneering article in this field of Genshiro Kitagawa on a related "Monte Carlo filter",[31] and the ones by Pierre Del Moral[32] and Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut[33] on particle filters published in the mid-1990s. Particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the LAAS-CNRS (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems.[34][35][36][37][38][39] These Sequential Monte Carlo methodologies can be interpreted as an acceptance-rejection sampler equipped with an interacting recycling mechanism.From 1950 to 1996, all the publications on Sequential Monte Carlo methodologies including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these particle algorithms are due to Pierre Del Moral[32][40] in 1996. Branching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons,[41][42][43] and by Dan Crisan, Pierre Del Moral and Terry Lyons.[44] Further developments in this field were developed in 2000 by P. Del Moral, A. Guionnet and L. Miclo.[22][45][46]There is no consensus on how Monte Carlo should be defined. For example, Ripley[47] defines most probabilistic modeling as stochastic simulation, with Monte Carlo being reserved for Monte Carlo integration and Monte Carlo statistical tests. Sawilowsky[48] distinguishes between a simulation, a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to obtain the statistical properties of some phenomenon (or behavior). Examples:Kalos and Whitlock[11] point out that such distinctions are not always easy to maintain. For example, the emission of radiation from atoms is a natural stochastic process. It can be simulated directly, or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods. "Indeed, the same computer code can be viewed simultaneously as a 'natural simulation' or as a solution of the equations by natural sampling."The main idea behind this method is that the results are computed based on repeated random sampling and statistical analysis. The Monte Carlo simulation is in fact random experimentations, in the case that, the results of these experiments are not well known.Monte Carlo simulations are typically characterized by a large number of unknown parameters, many of which are difficult to obtain experimentally.[49] Monte Carlo simulation methods do not always require truly random numbers to be useful (although, for some applications such as primality testing, unpredictability is vital).[50] Many of the most useful techniques use deterministic, pseudorandom sequences, making it easy to test and re-run simulations. The only quality usually necessary to make good simulations is for the pseudo-random sequence to appear "random enough" in a certain sense.What this means depends on the application, but typically they should pass a series of statistical tests. Testing that the numbers are uniformly distributed or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest, and most common ones. Weak correlations between successive samples is also often desirable/necessary.Sawilowsky lists the characteristics of a high quality Monte Carlo simulation:[48]Pseudo-random number sampling algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given probability distribution.Low-discrepancy sequences are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences. Methods based on their use are called quasi-Monte Carlo methods.In an effort to assess the impact of random number quality on Monte Carlo simulation outcomes, astrophysical researchers tested cryptographically-secure pseudorandom numbers generated via Intel's RdRand instruction set, as compared to those derived from algorithms, like the Mersenne Twister, in Monte Carlo simulations of radio flares from brown dwarfs.  RdRand is the closest pseudorandom number generator to a true random number generator.  No statistically-significant difference was found between models generated with typical pseudorandom number generators and RdRand for trials consisting of the generation of 107 random numbers.[51]There are ways of using probabilities that are definitely not Monte Carlo simulations — for example, deterministic modeling using single-point estimates. Each uncertain variable within a model is assigned a “best guess” estimate.  Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded.[52]By contrast, Monte Carlo simulations sample from a probability distribution for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring.[53] For example, a comparison of a spreadsheet cost construction model run using traditional “what if” scenarios, and then running the comparison again with Monte Carlo simulation and triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the “what if” analysis.[example  needed]  This is because the “what if” analysis gives equal weight to all scenarios (see quantifying uncertainty in corporate finance), while the Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called "rare events".Monte Carlo methods are especially useful for simulating phenomena with significant uncertainty in inputs and systems with a large number of coupled degrees of freedom. Areas of application include:Numerical analysis · SimulationFinite element · Boundary element Lattice Boltzmann · Riemann solverDissipative particle dynamicsSmoothed particle hydrodynamicsMonte Carlo methods are very important in computational physics, physical chemistry, and related applied fields, and have diverse applications from complicated quantum chromodynamics calculations to designing heat shields and aerodynamic forms as well as in modeling radiation transport for radiation dosimetry calculations.[54][55][56]  In statistical physics Monte Carlo molecular modeling is an alternative to computational molecular dynamics, and Monte Carlo methods are used to compute statistical field theories of simple particle and polymer systems.[29][57]  Quantum Monte Carlo methods solve the many-body problem for quantum systems.[8][9][21] In radiation materials science, the binary collision approximation for simulating ion implantation is usually based on a Monte Carlo approach to select the next colliding atom.[58] In experimental particle physics, Monte Carlo methods are used for designing detectors, understanding their behavior and comparing experimental data to theory. In astrophysics, they are used in such diverse manners as to model both galaxy evolution[59] and microwave radiation transmission through a rough planetary surface.[60] Monte Carlo methods are also used in the ensemble models that form the basis of modern weather forecasting.Monte Carlo methods are widely used in engineering for sensitivity analysis and quantitative probabilistic analysis in process design. The need arises from the interactive, co-linear and non-linear behavior of typical process simulations. For example,The Intergovernmental Panel on Climate Change relies on Monte Carlo methods in probability density function analysis of radiative forcing.Probability density function (PDF) of ERF due to total GHG, aerosol forcing and total anthropogenic forcing. The GHG consists of WMGHG, ozone and stratospheric water vapour. The PDFs are generated based on uncertainties provided in Table 8.6. The combination of the individual RF agents to derive total forcing over the Industrial Era are done by Monte Carlo simulations and based on the method in Boucher and Haywood (2001). PDF of the ERF from surface albedo changes and combined contrails and contrail-induced cirrus are included in the total anthropogenic forcing, but not shown as a separate PDF. We currently do not have ERF estimates for some forcing mechanisms: ozone, land use, solar, etc.[68]Monte Carlo methods are used in various fields of computational biology, for example for Bayesian inference in phylogeny, or for studying biological systems such as genomes, proteins,[69] or membranes.[70]The systems can be studied in the coarse-grained or ab initio frameworks depending on the desired accuracy. Computer simulations allow us to monitor the local environment of a particular molecule to see if some chemical reaction is happening for instance. In cases where it is not feasible to conduct a physical experiment, thought experiments can be conducted (for instance: breaking bonds, introducing impurities at specific sites, changing the local/global structure, or introducing external fields).Path tracing, occasionally referred to as Monte Carlo ray tracing, renders a 3D scene by randomly tracing samples of possible light paths. Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the rendering equation, making it one of the most physically accurate 3D graphics rendering methods in existence.The standards for Monte Carlo experiments in statistics were set by Sawilowsky.[71][72] In applied statistics, Monte Carlo methods are generally used for three purposes:Monte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate randomization test is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice—or more frequently—for the efficiency of not having to track which permutations have already been selected).Monte Carlo methods have been developed into a technique called Monte-Carlo tree search that is useful for searching for the best move in a game.  Possible moves are organized in a search tree and a large number of random simulations are used to estimate the long-term potential of each move. A black box simulator represents the opponent's moves.[74]The Monte Carlo tree search (MCTS) method has four steps:[75]The net effect, over the course of many simulated games, is that the value of a node representing a move will go up or down, hopefully corresponding to whether or not that node represents a good move.Monte Carlo Tree Search has been used successfully to play games such as Go,[76] Tantrix,[77] Battleship,[78] Havannah,[79] and Arimaa.[80]Monte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport, and thus these methods have been used in global illumination computations that produce photo-realistic images of virtual 3D models, with applications in video games, architecture, design, computer generated films, and cinematic special effects.[81]The US Coast Guard utilizes Monte Carlo methods within its computer modeling software SAROPS in order to calculate the probable locations of vessels during search and rescue operations. Each simulation can generate as many as ten thousand data points that are randomly distributed based upon provided variables.[82] Search patterns are then generated based upon extrapolations of these data in order to optimize the probability of containment (POC) and the probability of detection (POD), which together will equal an overall probability of success (POS). Ultimately this serves as a practical application of probability distribution in order to provide the swiftest and most expedient method of rescue, saving both lives and resources.[83]Monte Carlo simulation is commonly used to evaluate the risk and uncertainty that would affect the outcome of different decision options. Monte Carlo simulation allows the business risk analyst to incorporate the total effects of uncertainty in variables like sales volume, commodity and labour prices, interest and exchange rates, as well as the effect of distinct risk events like the cancellation of a contract or the change of a tax law.Monte Carlo methods in finance are often used to evaluate investments in projects at a business unit or corporate level, or to evaluate financial derivatives. They can be used to model project schedules, where simulations aggregate estimates for worst-case, best-case, and most likely durations for each task to determine outcomes for the overall project. Monte Carlo methods are also used in option pricing, default risk analysis.[84][85][86]A Monte Carlo approach was used for evaluating the potential value of a proposed program to help female petitioners in Wisconsin be successful in their applications for harassment and domestic abuse restraining orders.  It was proposed to help women succeed in their petitions by providing them with greater advocacy thereby potentially reducing the risk of rape and physical assault.  However, there were many variables in play that could not be estimated perfectly, including the effectiveness of restraining orders, the success rate of petitioners both with and without advocacy, and many others.  The study ran trials that varied these variables to come up with an overall estimate of the success level of the proposed program as a whole.[87]In general, the Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers (see also Random number generation) and observing that fraction of the numbers that obeys some property or properties. The method is useful for obtaining numerical solutions to problems too complicated to solve analytically.  The most common application of the Monte Carlo method is Monte Carlo integration.Deterministic numerical integration algorithms work well in a small number of dimensions, but encounter two problems when the functions have many variables. First, the number of function evaluations needed increases rapidly with the number of dimensions. For example, if 10 evaluations provide adequate accuracy in one dimension, then 10100 points are needed for 100 dimensions—far too many to be computed. This is called the curse of dimensionality. Second, the boundary of a multidimensional region may be very complicated, so it may not be feasible to reduce the problem to an iterated integral.[88] 100 dimensions is by no means unusual, since in many physical problems, a "dimension" is equivalent to a degree of freedom.A refinement of this method, known as importance sampling in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as stratified sampling, recursive stratified sampling, adaptive umbrella sampling[89][90] or the VEGAS algorithm.A similar approach, the quasi-Monte Carlo method, uses low-discrepancy sequences. These sequences "fill" the area better and sample the most important points more frequently, so quasi-Monte Carlo methods can often converge on the integral more quickly.Another class of methods for sampling points in a volume is to simulate random walks over it (Markov chain Monte Carlo). Such methods include the Metropolis-Hastings algorithm, Gibbs sampling, Wang and Landau algorithm, and interacting type MCMC methodologies such as the sequential Monte Carlo samplers.[91]Another powerful and very popular application for random numbers in numerical simulation is in numerical optimization. The problem is to minimize (or maximize) functions of some vector that often has a large number of dimensions. Many problems can be phrased in this way: for example, a computer chess program could be seen as trying to find the set of, say, 10 moves that produces the best evaluation function at the end. In the traveling salesman problem the goal is to minimize distance traveled. There are also applications to engineering design, such as multidisciplinary design optimization. It has been applied with quasi-one-dimensional models to solve particle dynamics problems by efficiently exploring large configuration space. Reference [92] is a comprehensive review of many issues related to simulation and optimization.The traveling salesman problem is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines prior information with new information obtained by measuring some observable parameters (data).As, in the general case, the theory linking data with model parameters is nonlinear, the posterior probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.).When analyzing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available.The best-known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.[93][94]
Seven-dimensional cross product
In mathematics, the seven-dimensional cross product is a bilinear operation on vectors in seven-dimensional Euclidean space. It assigns to any two vectors a, b in R7 a vector a × b also in R7.[1] Like the cross product in three dimensions, the seven-dimensional product is anticommutative and a × b is orthogonal both to a and to b. Unlike in three dimensions, it does not satisfy the Jacobi identity, and while the three-dimensional cross product is unique up to a sign, there are many seven-dimensional cross products. The seven-dimensional cross product has the same relationship to the octonions as the three-dimensional product does to the quaternions.The seven-dimensional cross product is one way of generalising the cross product to other than three dimensions, and it is the only other non-trivial bilinear product of two vectors that is vector-valued, anticommutative and orthogonal.[2] In other dimensions there are vector-valued products of three or more vectors that satisfy these conditions, and binary products with bivector results.The product can be given by a multiplication table, such as the one here. This table, due to Cayley,[3][4] gives the product of basis vectors ei and ej for each i, j from 1 to 7. For example, from the tableThe table can be used to calculate the product of any two vectors. For example, to calculate the e1 component of x × y the basis vectors that multiply to produce e1 can be picked out to giveThis can be repeated for the other six components.There are 480 such tables, one for each of the products satisfying the definition.[5] This table can be summarized by the relation[4]The top left 3 × 3 corner of this table gives the cross product in three dimensions.The cross product on a Euclidean space V is a bilinear map  from V × V to V, mapping vectors x and y in V to another vector x × y also in V, where x × y has the properties[1][6]where (x·y) is the Euclidean dot product and |x| is the Euclidean norm. The first property states that the product is perpendicular to its arguments, while the second property gives the magnitude of the product. An equivalent expression in terms of the angle θ between the vectors[7] is[8]which is the area of the parallelogram in the plane of x and y with the two vectors as sides.[9] A third statement of the magnitude condition isif x × x = 0 is assumed as a separate axiom.[10]Given the properties of bilinearity, orthogonality and magnitude, a nonzero cross product exists only in three and seven dimensions.[2][8][10]  This can be shown by postulating the properties required for the cross product, then deducing an equation which is only satisfied when the dimension is 0, 1, 3 or 7. In zero dimensions there is only the zero vector, while in one dimension all vectors are parallel, so in both these cases the product must be identically zero.The restriction to 0, 1, 3 and 7 dimensions is related to Hurwitz's theorem, that normed division algebras are only possible in 1, 2, 4 and 8 dimensions. The cross product is formed from the product of the normed division algebra by restricting it to the 0, 1, 3, or 7 imaginary dimensions of the algebra, giving nonzero products in only three and seven dimensions.[11]In contrast the three-dimensional cross product, which is unique (apart from sign), there are many possible binary cross products in seven dimensions. One way to see this is to note that given any pair of vectors x and y ∈ ℝ7 and any vector v of magnitude |v| = |x||y| sin θ in the five-dimensional space perpendicular to the plane spanned by x and y, it is possible to find a cross product with a multiplication table (and an associated set of basis vectors) such that x × y = v. Unlike in three dimensions, x × y = a × b does not imply that a and b lie in the same plane as x and y.[8]Further properties follow from the definition, including the following identities:Other properties follow only in the three-dimensional case, and are not satisfied by the seven-dimensional cross product, notably,To define a particular cross product, an orthonormal basis {ej} may be selected and a multiplication table provided that determines all the products {ei × ej}. One possible multiplication table is described in the Example section, but it is not unique.[5] Unlike three dimensions, there are many tables because every pair of unit vectors is perpendicular to five other unit vectors, allowing many choices for each cross product.Once we have established a multiplication table, it is then applied to general vectors x and y by expressing x and y in terms of the basis and expanding x × y through bilinearity.  Using e1  to e7 for the basis vectors a different multiplication table from the one in the Introduction, leading to a different cross product, is given with anticommutativity by[8]More compactly this rule can be written aswith i = 1...7 modulo 7 and the indices i, i + 1 and i + 3 allowed to permute evenly. Together with anticommutativity this generates the product. This rule directly produces the two diagonals immediately adjacent to the diagonal of zeros in the table. Also, from an identity in the subsection on consequences,which produces diagonals further out, and so on.The ej component of cross product x × y is given by selecting all occurrences of ej in the table and collecting the corresponding components of x from the left column and of y from the top row. The result is:As the cross product is bilinear the operator x×– can be written as a matrix, which takes the form[citation needed]The cross product is then given byTwo different multiplication tables have been used in this article, and there are more.[5][12] These multiplication tables are characterized by the Fano plane,[13][14] and these are shown in the figure for the two tables used here: at top, the one described by Sabinin, Sbitneva, and Shestakov, and at bottom that described by Lounesto. The numbers under the Fano diagrams (the set of lines in the diagram) indicate a set of indices for seven independent products in each case, interpreted as ijk → ei × ej = ek. The multiplication table is recovered from the Fano diagram by following either the straight line connecting any three points, or the circle in the center, with a sign as given by the arrows. For example, the first row of multiplications resulting in e1 in the above listing is obtained by following the three paths connected to e1 in the lower Fano diagram: the circular path e2 × e4, the diagonal path e3 × e7, and the edge path e6 × e1 = e5 rearranged using one of the above identities as:oralso obtained directly from the diagram with the rule that any two unit vectors on a straight line are connected by multiplication to the third unit vector on that straight line with signs according to the arrows (sign of the permutation that orders the unit vectors).It can be seen that both multiplication rules follow from the same Fano diagram by simply renaming the unit vectors, and changing the sense of the center unit vector. Considering all possible permutations of the basis there are 480 multiplication tables and so 480 cross products like this.[14]The product can also be calculated using geometric algebra. The product starts with the exterior product, a bivector valued product of two vectors:This is bilinear, alternate, has the desired magnitude, but is not vector valued. The vector, and so the cross product, comes from the product of this bivector with a trivector. In three dimensions up to a scale factor there is only one trivector, the pseudoscalar of the space, and a product of the above bivector and one of the two unit trivectors gives the vector result, the dual of the bivector.A similar calculation is done is seven dimensions, except as trivectors form a 35-dimensional space there are many trivectors that could be used, though not just any trivector will do. The trivector that gives the same product as the above coordinate transform isThis is combined with the exterior product to give the cross productJust as the 3-dimensional cross product can be expressed in terms of the quaternions, the 7-dimensional cross product can be expressed in terms of the octonions. After identifying ℝ7 with the imaginary octonions (the orthogonal complement of the real line in O), the cross product is given in terms of octonion multiplication byConversely, suppose V is a 7-dimensional Euclidean space with a given cross product. Then one can define a bilinear multiplication on ℝ⊕V as follows:The space ℝ⊕V with this multiplication is then isomorphic to the octonions.[16]The cross product only exists in three and seven dimensions as one can always define a multiplication on a space of one higher dimension as above, and this space can be shown to be a normed division algebra. By Hurwitz's theorem such algebras only exist in one, two, four, and eight dimensions, so the cross product must be in zero, one, three or seven dimensions. The products in zero and one dimensions are trivial, so non-trivial cross products only exist in three and seven dimensions.[17][18]The failure of the 7-dimension cross product to satisfy the Jacobi identity is due to the nonassociativity of the octonions. In fact,where [x, y, z] is the associator.In three dimensions the cross product is invariant under the action of the rotation group, SO(3), so the cross product of x and y after they are rotated is the image of x × y under the rotation. But this invariance is not true in seven dimensions; that is, the cross product is not invariant under the group of rotations in seven dimensions, SO(7). Instead it is invariant under the exceptional Lie group G2, a subgroup of SO(7).[8][16]Nonzero binary cross products exist only in three and seven dimensions. Further products are possible when lifting the restriction that it must be a binary product.[19][20] We require the product to be multi-linear, alternating, vector-valued, and orthogonal to each of the input vectors ai. The orthogonality requirement implies that in n dimensions, no more than n − 1 vectors can be used. The magnitude of the product should equal the volume of the parallelotope with the vectors as edges, which can be calculated using the Gram determinant. The conditions areThe Gram determinant is the squared volume of the parallelotope with a1, ..., ak as edges.With these conditions a non-trivial cross product only exists:One version of the product of three vectors in eight dimensions is given byThere are also trivial products. As noted already, a binary product only exists in 7, 3, 1 and 0 dimensions, the last two being identically zero. A further trivial 'product' arises in even dimensions, which takes a single vector and produces a vector of the same magnitude orthogonal to it through the left contraction with a suitable bivector. In two dimensions this is a rotation through a right angle.
Ring (mathematics)
In mathematics, a ring is one of the fundamental algebraic structures used in abstract algebra. It consists of a set equipped with two binary operations that generalize the arithmetic operations of addition and multiplication. Through this generalization, theorems from arithmetic are extended to non-numerical objects such as polynomials, series, matrices and functions.A ring is an abelian group with a second binary operation that is associative, is distributive over the abelian group operation, and has an identity element (this last property is not required by some authors, see § Notes on the definition). By extension from the integers, the abelian group operation is called addition and the second binary operation is called multiplication.Whether a ring is commutative or not (i.e., whether the order in which two elements are multiplied changes the result or not) has profound implications on its behavior as an abstract object. As a result, commutative ring theory, commonly known as commutative algebra, is a key topic in ring theory. Its development has been greatly influenced by problems and ideas occurring naturally in algebraic number theory and algebraic geometry. Examples of commutative rings include the set of integers equipped with the addition and multiplication operations, the set of polynomials equipped with their addition and multiplication, the coordinate ring of an affine algebraic variety, and the ring of integers of a number field. Examples of noncommutative rings include the ring of n × n real square matrices with n ≥ 2, group rings in representation theory, operator algebras in functional analysis, rings of differential operators in the theory of differential operators, and the cohomology ring of a topological space in topology.The conceptualization of rings began in the 1870s and was completed in the 1920s. Key contributors include Dedekind, Hilbert, Fraenkel, and Noether. Rings were first formalized as a generalization of Dedekind domains that occur in number theory, and of polynomial rings and rings of invariants that occur in algebraic geometry and invariant theory. Afterward, they also proved to be useful in other branches of mathematics such as geometry and mathematical analysis.The familiar properties for addition and multiplication of integers serve as a model for the axioms for rings.A ring is a set R equipped with two binary operations[1] + and · satisfying the following three sets of axioms, called the ring axioms[2][3][4]1. R is an abelian group under addition, meaning that:2. R is a monoid under multiplication, meaning that:3. Multiplication is distributive with respect to addition:As explained in § History below, many authors follow an alternative convention in which a ring is not defined to have a multiplicative identity. This article adopts the convention that, unless otherwise stated, a ring is assumed to have such an identity.  A structure satisfying all the axioms except the requirement that there exists a multiplicative identity element is called a rng (commonly pronounced "rung", and sometimes called a pseudo-ring). For example, the set of even integers with the usual + and ⋅ is a rng, but not a ring.The operations + and ⋅ are called addition and multiplication, respectively. The multiplication symbol ⋅ is often omitted, so the juxtaposition of ring elements is interpreted as multiplication. For example, xy means x ⋅ y.Although ring addition is commutative, ring multiplication is not required to be commutative: ab need not necessarily equal ba. Rings that also satisfy commutativity for multiplication (such as the ring of integers) are called commutative rings. Books on commutative algebra or algebraic geometry often adopt the convention that "ring" means "commutative ring", to simplify terminology.In a ring, multiplication does not have to have an inverse. A (non-trivial) commutative ring such that every nonzero element has a multiplicative inverse is called a field.The additive group of a ring is the ring equipped just with the structure of addition. Although the definition assumes that the additive group is abelian, this can be inferred from the other ring axioms.[6]Some basic properties of a ring follow immediately from the axioms:The set of 2-by-2 matrices with real number entries is writtenMore generally, for any ring R, commutative or not, and any nonnegative integer n, one may form the ring of n-by-n matrices with entries in R: see Matrix ring.The study of rings originated from the theory of polynomial rings and the theory of algebraic integers.[7] In 1871, Richard Dedekind defined the concept of the ring of integers of a number field.[8] In this context, he introduced the terms "ideal" (inspired by Ernst Kummer's notion of ideal number) and "module" and studied their properties. But Dedekind did not use the term "ring" and did not define the concept of a ring in a general setting.The term "Zahlring" (number ring) was coined by David Hilbert in 1892 and published in 1897.[9] In 19th century German, the word "Ring" could mean "association", which is still used today in English in a limited sense (e.g., spy ring),[10] so if that were the etymology then it would be similar to the way "group" entered mathematics by being a non-technical word for "collection of related things". According to Harvey Cohn, Hilbert used the term for a ring that had the property of "circling directly back" to an element of itself.[11] Specifically, in a ring of algebraic integers, all high powers of an algebraic integer can be written as an integral combination of a fixed set of lower powers, and thus the powers "cycle back". For instance, if a3 − 4a + 1 = 0 then a3 = 4a − 1, a4 = 4a2 − a, a5 = −a2 + 16a − 4, a6 = 16a2 − 8a + 1, a7 = −8a2 + 65a − 16, and so on; in general, an is going to be an integral linear combination of 1, a, and a2.The first axiomatic definition of a ring was given by Adolf Fraenkel in 1914,[12][13] but his axioms were stricter than those in the modern definition. For instance, he required every non-zero-divisor to have a multiplicative inverse.[14] In 1921, Emmy Noether gave the modern axiomatic definition of (commutative) ring and developed the foundations of commutative ring theory in her paper Idealtheorie in Ringbereichen.[15]Fraenkel required a ring to have a multiplicative identity 1,[16] whereas Noether did not.[15]Most or all books on algebra[17][18] up to around 1960 followed Noether's convention of not requiring a 1. Starting in the 1960s, it became increasingly common to see books including the existence of 1 in the definition of ring, especially in advanced books by notable authors such as Artin,[19] Atiyah and MacDonald,[20] Bourbaki,[21] Eisenbud,[22] and Lang.[23] But even today, there remain many books that do not require a 1.Faced with this terminological ambiguity, some authors have tried to impose their views, while others have tried to adopt more precise terms.In the first category, we find for instance Gardner and Wiegandt, who argue that if one requires all rings to have a 1, then some consequences include the lack of existence of infinite direct sums of rings, and the fact that proper direct summands of rings are not subrings. They conclude that "in many, maybe most, branches of ring theory the requirement of the existence of a unity element is not sensible, and therefore unacceptable."[24]In the second category, we find authors who use the following terms:[25][26]Commutative rings:Noncommutative rings:Non-rings:For example, the ring Z of integers is a subring of the field of real numbers and also a subring of the ring of polynomials Z[X] (in both cases, Z contains 1, which is the multiplicative identity of the larger rings). On the other hand, the subset of even integers 2Z does not contain the identity element 1 and thus does not qualify as a subring of Z.An intersection of subrings is a subring. The smallest subring containing a given subset E of R is called a subring generated by E. Such a subring exists since it is the intersection of all subrings containing E.The definition of an ideal in a ring is analogous to that of normal subgroup in a group. But, in actuality, it plays a role of an idealized generalization of an element in a ring; hence, the name "ideal". Like elements of rings, the study of ideals is central to structural understanding of a ring.Like a group, a ring is said to be simple if it is nonzero and it has no proper nonzero two-sided ideals. A commutative simple ring is precisely a field.Rings are often studied with special conditions set upon their ideals. For example, a ring in which there is no strictly increasing infinite chain of left ideals is called a left Noetherian ring. A ring in which there is no strictly decreasing infinite chain of left ideals is called a left Artinian ring. It is a somewhat surprising fact that a left Artinian ring is left Noetherian (the Hopkins–Levitzki theorem). The integers, however, form a Noetherian ring which is not Artinian.A homomorphism from a ring (R, +, ·) to a ring (S, ‡, *) is a function f from R to S that preserves the ring operations; namely, such that, for all a, b in R the following identities hold:If one is working with not necessarily unital rings, then the third condition is dropped.Examples:To give a ring homomorphism from a commutative ring R to a ring A with image contained in the center of A is the same as to give a structure of an algebra over R to A (in particular gives a structure of A-module).The quotient ring of a ring, is analogous to the notion of a quotient group of a group. More formally, given a ring (R, +, · ) and a two-sided ideal I of (R, +, · ), the quotient ring (or factor ring) R/I is the set of cosets of I (with respect to the additive group of (R, +, · ); i.e. cosets with respect to (R, +)) together with the operations:for every a, b in R.The concept of a module over a ring generalizes the concept of a vector space (over a field) by generalizing from multiplication of vectors with elements of a field (scalar multiplication) to multiplication with elements of a ring. More precisely, given a ring R with 1, an R-module M is an abelian group equipped with an operation R × M → M (associating an element of M to every pair of an element of R and an element of M) that satisfies certain axioms. This operation is commonly denoted multiplicatively and called multiplication. The axioms of modules are the following: for all a, b in R and all x, y in M, we have:When the ring is noncommutative these axioms define left modules; right modules are defined similarly by writing xa instead of ax. This is not only a change of notation, as the last axiom of right modules (that is x(ab) = (xa)b) becomes (ab)x = b(ax), if left multiplication (by ring elements) is used for a right module.Basic examples of modules are ideals, including the ring itself.Although similarly defined, the theory of modules is much more complicated than that of vector space, mainly, because, unlike vector spaces, modules are not characterized (up to an isomorphism) by a single invariant (the dimension of a vector space). In particular, not all modules have a basis.The axioms of modules imply that (−1)x = −x, where the first minus denotes the additive inverse in the ring and the second minus the additive inverse in the module. Using this and denoting repeated addition by a multiplication by a positive integer allows identifying abelian groups with modules over the ring of integers.Any ring homomorphism induces a structure of a module: if f : R → S is a ring homomorphism, then S is a left module over R by the multiplication: rs = f(r)s. If R is commutative or if f(R) is contained in the center of S, the ring S is called a R-algebra. In particular, every ring is an algebra over the integers.Let R and S be rings. Then the product R × S can be equipped with the following natural ring structure:as a direct sum of abelian groups (because for abelian groups finite products are the same as direct sums). Clearly the direct sum of such ideals also defines a product of rings that is isomorphic to R. Equivalently, the above can be done through central idempotents. Assume R has the above decomposition. Then we can writeAn important application of an infinite direct product is the construction of a projective limit of rings (see below). Another application is a restricted product of a family of rings (cf. adele ring).Given a symbol t (called a variable) and a commutative ring R, the set of polynomialsAny ring homomorphism R → S induces Mn(R) → Mn(S); in fact, any ring homomorphism between matrix rings arises in this way.[33]The Artin–Wedderburn theorem states any semisimple ring (cf. below) is of this form.A ring R and the matrix ring Mn(R) over it are Morita equivalent: the category of right modules of R is equivalent to the category of right modules over Mn(R).[33] In particular, two-sided ideals in R correspond in one-to-one to two-sided ideals in Mn(R).Examples:Examples of colimits:Any commutative ring is the colimit of finitely generated subrings.For an example of a projective limit, see § Completion.The most important properties of localization are the following: when R is a commutative ring and S a multiplicatively closed subsetA complete ring has much simpler structure than a commutative ring. This owns to the Cohen structure theorem, which says, roughly, that a complete local ring tends to look like a formal power series ring or a quotient of it. On the other hand, the interaction between the integral closure and completion has been among the most important aspects that distinguish modern commutative ring theory from the classical one developed by the likes of Noether. Pathological examples found by Nagata led to the reexamination of the roles of Noetherian rings and motivated, among other things, the definition of excellent ring.A nonzero ring with no nonzero zero-divisors is called a domain. A commutative domain is called an integral domain. The most important integral domains are principal ideals domains, PID for short, and fields. A principal ideal domain is an integral domain in which every ideal is principal. An important class of integral domains that contain a PID is a unique factorization domain (UFD), an integral domain in which every nonunit element is a product of prime elements (an element is prime if it generates a prime ideal.) The fundamental question in algebraic number theory is on the extent to which the ring of (generalized) integers in a number field, where an "ideal" admits prime factorization, fails to be a PID.In algebraic geometry, UFDs arise because of smoothness. More precisely, a point in a variety (over a perfect field) is smooth if the local ring at the point is a regular local ring. A regular local ring is a UFD.[42]The following is a chain of class inclusions that describes the relationship between rings, domains and fields:A division ring is a ring such that every non-zero element is a unit. A commutative division ring is a field. A prominent example of a division ring that is not a field is the ring of quaternions. Any centralizer in a division ring is also a division ring. In particular, the center of a division ring is a field. It turned out that every finite domain (in particular finite division ring) is a field; in particular commutative (the Wedderburn's little theorem).Every module over a division ring is a free module (has a basis); consequently, much of linear algebra can be carried out over a division ring instead of a field.The study of conjugacy classes figures prominently in the classical theory of division rings. Cartan famously asked the following question: given a division ring D and a proper sub-division-ring S that is not contained in the center, does each inner automorphism of D restrict to an automorphism of S? The answer is negative: this is the Cartan–Brauer–Hua theorem.A cyclic algebra, introduced by L. E. Dickson, is a generalization of a quaternion algebra.A ring is called a semisimple ring if it is semisimple as a left module (or right module) over itself; i.e., a direct sum of simple modules. A ring is called a semiprimitive ring if its Jacobson radical is zero. (The Jacobson radical is the intersection of all maximal left ideals.) A ring is semisimple if and only if it is artinian and is semiprimitive.An algebra over a field k is artinian if and only if it has finite dimension. Thus, a semisimple algebra over a field is necessarily finite-dimensional, while a simple algebra may have infinite dimension; e.g., the ring of differential operators.Any module over a semisimple ring is semisimple. (Proof: any free module over a semisimple ring is clearly semisimple and any module is a quotient of a free module.)Examples of semisimple rings:The Skolem–Noether theorem states any automorphism of a central simple algebra is inner.Azumaya algebras generalize the notion of central simple algebras to a commutative local ring.If K is a field, a valuation v is a group homomorphism from the multiplicative group K* to a totally ordered abelian group G such that, for any f, g in K with f + g nonzero, v(f + g) ≥ min{v(f), v(g)}. The valuation ring of v is the subring of K consisting of zero and all nonzero f such that v(f) ≥ 0.Examples:See also: Novikov ring and uniserial ring.A ring may be viewed as an abelian group (by using the addition operation), with extra structure: namely, ring multiplication. In the same way, there are other mathematical objects which may be considered as rings with extra structure. For example:Many different kinds of mathematical objects can be fruitfully analyzed in terms of some associated ring.To any topological space X one can associate its integral cohomology ringThe ring structure in cohomology provides the foundation for characteristic classes of fiber bundles, intersection theory on manifolds and algebraic varieties, Schubert calculus and much more.To any group is associated its Burnside ring which uses a ring to describe the various ways the group can act on a finite set. The Burnside ring's additive group is the free abelian group whose basis are the transitive actions of the group and whose addition is the disjoint union of the action. Expressing an action in terms of the basis is decomposing an action into its transitive constituents. The multiplication is easily expressed in terms of the representation ring: the multiplication in the Burnside ring is formed by writing the tensor product of two permutation modules as a permutation module. The ring structure allows a formal way of subtracting one action from another. Since the Burnside ring is contained as a finite index subring of the representation ring, one can pass easily from one to the other by extending the coefficients from integers to the rational numbers.To any group ring or Hopf algebra is associated its representation ring or "Green ring". The representation ring's additive group is the free abelian group whose basis are the indecomposable modules and whose addition corresponds to the direct sum. Expressing a module in terms of the basis is finding an indecomposable decomposition of the module. The multiplication is the tensor product. When the algebra is semisimple, the representation ring is just the character ring from character theory, which is more or less the Grothendieck group given a ring structure.To any irreducible algebraic variety is associated its function field. The points of an algebraic variety correspond to valuation rings contained in the function field and containing the coordinate ring. The study of algebraic geometry makes heavy use of commutative algebra to study geometric concepts in terms of ring-theoretic properties. Birational geometry studies maps between the subrings of the function field.Every simplicial complex has an associated face ring, also called its Stanley–Reisner ring. This ring reflects many of the combinatorial properties of the simplicial complex, so it is of particular interest in algebraic combinatorics. In particular, the algebraic geometry of the Stanley–Reisner ring was used to characterize the numbers of faces in each dimension of simplicial polytopes.Let (A, +) be an abelian group and let End(A) be its endomorphism ring (see above). Note that, essentially, End(A) is the set of all morphisms of A, where if f is in End(A), and g is in End(A), the following rules may be used to compute f + g and f · g:where + as in f(x) + g(x) is addition in A, and function composition is denoted from right to left. Therefore, associated to any abelian group, is a ring. Conversely, given any ring, (R, +, · ), (R, +) is an abelian group. Furthermore, for every r in R, right (or left) multiplication by r gives rise to a morphism of (R, +), by right (or left) distributivity. Let A = (R, +). Consider those endomorphisms of A, that "factor through" right (or left) multiplication of R. In other words, let EndR(A) be the set of all morphisms m of A, having the property that m(r · x) = r · m(x). It was seen that every r in R gives rise to a morphism of A: right multiplication by r. It is in fact true that this association of any element of R, to a morphism of A, as a function from R to EndR(A), is an isomorphism of rings. In this sense, therefore, any ring can be viewed as the endomorphism ring of some abelian X-group (by X-group, it is meant a group with X being its set of operators).[45] In essence, the most general form of a ring, is the endomorphism group of some abelian X-group.Any ring can be seen as a preadditive category with a single object. It is therefore natural to consider arbitrary preadditive categories to be generalizations of rings. And indeed, many definitions and theorems originally given for rings can be translated to this more general context. Additive functors between preadditive categories generalize the concept of ring homomorphism, and ideals in additive categories can be defined as sets of morphisms closed under addition and under composition with arbitrary morphisms.Algebraists have defined structures more general than rings by weakening or dropping some of ring axioms.A rng is the same as a ring, except that the existence of a multiplicative identity is not assumed.[46]A nonassociative ring is an algebraic structure that satisfies all of the ring axioms except the associative property and the existence of a multiplicative identity. A notable example is a Lie algebra. There exists some structure theory for such algebras that generalizes the analogous results for Lie algebras and associative algebras.[citation needed]A semiring is obtained by weakening the assumption that (R,+) is an abelian group to the assumption that (R,+) is a commutative monoid, and adding the axiom that 0 · a = a · 0 = 0 for all a in R (since it no longer follows from the other axioms).Example: a tropical semiring.In algebraic geometry, a ring scheme over a base scheme S is a ring object in the category of S-schemes. One example is the ring scheme Wn over Spec Z, which for any commutative ring A returns the ring Wn(A) of p-isotypic Witt vectors of length n over A.[47]Special types of rings:^ a: Some authors only require that a ring be a semigroup under multiplication; that is, do not require that there be a multiplicative identity (1). See the section Notes on the definition for more details.^ b: Elements which do have multiplicative inverses are called units, see Lang 2002, §II.1, p. 84.^ c: The closure axiom is already implied by the condition that +/• be a binary operation. Some authors therefore omit this axiom. Lang 2002^ d: The transition from the integers to the rationals by adding fractions is generalized by the quotient field.^ e: Many authors include commutativity of rings in the set of ring axioms (see above) and therefore refer to "commutative rings" as just "rings".
Signal-flow graph
A signal-flow graph or signal-flowgraph (SFG), invented by Claude Shannon,[1] but often called a Mason graph after Samuel Jefferson Mason who coined the term,[2] is a specialized flow graph, a directed graph in which nodes represent system variables, and branches (edges, arcs, or arrows) represent functional connections between pairs of nodes. Thus, signal-flow graph theory builds on that of directed graphs (also called digraphs), which includes as well that of oriented graphs. This mathematical theory of digraphs exists, of course, quite apart from its applications.[3][4]SFGs are most commonly used to represent signal flow in a physical system and its controller(s), forming a cyber-physical system.  Among their other uses are the representation of signal flow in various electronic networks and amplifiers, digital filters, state-variable filters and some other types of analog filters.  In nearly all literature, a signal-flow graph is associated with a set of linear equations.Wai-Kai Chen wrote: "The concept of a signal-flow graph was originally worked out by Shannon [1942][1] in dealing with analog computers. The greatest credit for the formulation of signal-flow graphs is normally extended to Mason [1953],[2] [1956].[5] He showed how to use the signal-flow graph technique to solve some difficult electronic problems in a relatively simple manner. The term signal flow graph was used because of its original application to electronic problems and the association with electronic signals and flowcharts of the systems under study."[6]Lorens wrote: "Previous to Mason's work, C. E. Shannon[1]  worked out a number of the properties of what are now known as flow graphs. Unfortunately, the paper originally had a restricted classification and very few people had access to the material."[7]"The rules for the evaluation of the graph determinant of a Mason Graph were first given and proven by Shannon [1942] using mathematical induction. His work remained essentially unknown even after Mason published his classical work in 1953. Three years later, Mason [1956] rediscovered the rules and proved them by considering the value of a determinant and how it changes as variables are added to the graph. [...]"[8]Robichaud et al. identify the domain of application of SFGs as follows:[9]The following illustration and its meaning were introduced by Mason to illustrate basic concepts:[2]In the simple flow graphs of the figure, a functional dependence of a node is indicated by an incoming arrow, the node originating this influence is the beginning of this arrow, and in its most general form the signal flow graph indicates by incoming arrows only those nodes that influence the processing at the receiving node, and at each node, i, the incoming variables are processed according to a function associated with that node, say Fi. The flowgraph in (a) represents a set of explicit relationships:Node x1 is an isolated node because no arrow is incoming; the equations for x2 and x3 have the graphs shown in parts (b) and (c) of the figure.These relationships define for every node a function that processes the input signals it receives. Each non-source node combines the input signals in some manner, and broadcasts a resulting signal along each outgoing branch. "A flow graph, as defined originally by Mason, implies a set of functional relations, linear or not."[9]However, the commonly used Mason graph is more restricted, assuming that each node simply sums its incoming arrows, and that each branch involves only the initiating node involved. Thus, in this more restrictive approach, the node x1 is unaffected while:and now the functions fij can be associated with the signal-flow branches ij joining the pair of nodes xi, xj, rather than having general relationships associated with each node. A contribution by a node to itself like f33 for x3  is called a self-loop. Frequently these functions are simply multiplicative factors (often called transmittances or gains), for example, fij(xj)=cijxj, where c is a scalar, but possibly a function of some parameter like the Laplace transform variable s. Signal-flow graphs are very often used with Laplace-transformed signals, and in this case the transmittance, c(s), often is called a transfer function.In general, there are several ways of choosing the variables in a complex system. Corresponding to each choice, a system of equations can be written and each system of equations can be represented in a graph. This formulation of the equations becomes direct and automatic if one has at his disposal techniques which permit the drawing of a graph directly from the schematic diagram of the system under study. The structure of the graphs thus obtained is related in a simple manner to the topology of the schematic diagram, and it becomes unnecessary to consider the equations, even implicitly, to obtain the graph. In some cases, one has simply to imagine the flow graph in the schematic diagram and the desired answers can be obtained without even drawing the flow graph.Robichaud et al. wrote: "The signal flow graph contains the same information as the equations from which it is derived; but there does not exist a one-to-one correspondence between the graph and the system of equations. One system will give different graphs according to the order in which the equations are used to define the variable written on the left-hand side."[9] If all equations relate all dependent variables, then there are n! possible SFGs to choose from.[12]Linear signal-flow graph methods only apply to linear time-invariant systems, as studied by their associated theory.  When modeling a system of interest, the first step is often to determine the equations representing the system's operation without assigning causes and effects (this is called acausal modeling).[13] A SFG is then derived from this system of equations.A linear SFG consists of nodes indicated by dots and weighted directional branches indicated by arrows. The nodes are the variables of the equations and the branch weights are the coefficients. Signals may only traverse a branch in the direction indicated by its arrow.  The elements of a SFG can only represent the operations of multiplication by a coefficient and addition, which are sufficient to represent the constrained equations. When a signal traverses a branch in its indicated direction, the signal is multiplied the weight of the branch. When two or more branches direct into the same node, their outputs are added.For systems described by linear algebraic or differential equations, the signal-flow graph is mathematically equivalent to the system of equations describing the system, and the equations governing the nodes are discovered for each node by summing incoming branches to that node. These incoming branches convey the contributions of the other nodes, expressed as the connected node value multiplied by the weight of the connecting branch, usually a real number or function of some parameter (for example a Laplace transform variable s).For linear active networks, Choma writes:[14] "By a 'signal flow representation' [or 'graph', as it is commonly referred to] we mean a diagram that, by displaying the algebraic relationships among relevant branch variables of network, paints an unambiguous picture of the way an applied input signal ‘flows’ from input-to-output ... ports."A motivation for a SFG analysis is described by Chen:[15]A linear signal flow graph is related to a system of linear equations[16] of the following form:The figure to the right depicts various elements and constructs of a signal flow graph (SFG).[17]Terms used in linear SFG theory also include:[17]A signal-flow graph may be simplified by graph transformation rules.[19][20][21]  These simplification rules are also referred to as signal-flow graph algebra.[22]The purpose of this reduction is to relate the dependent variables of interest (residual nodes, sinks) to its independent variables (sources).The systematic reduction of a linear signal-flow graph is a graphical method equivalent to the Gauss-Jordan elimination method for solving linear equations.[23]The rules presented below may be applied over and over until the signal flow graph is reduced to its "minimal residual form".  Further reduction can require loop elimination or the use of a "reduction formula" with the goal to directly connect sink nodes representing the dependent variables to the source nodes representing the independent variables. By these means, any signal-flow graph can be simplified by successively removing internal nodes until only the input and output and index nodes remain.[24][25] Robichaud described this process of systematic flow-graph reduction:The reduction of a graph proceeds by the elimination of certain nodes to obtain a residual graph showing only the variables of interest. This elimination of nodes is called "node absorption". This method is close to the familiar process of successive eliminations of undesired variables in a system of equations. One can eliminate a variable by removing the corresponding node in the graph. If one reduces the graph sufficiently, it is possible to obtain the solution for any variable and this is the objective which will be kept in mind in this description of the different methods of reduction of the graph. In practice, however, the techniques of reduction will be used solely to transform the graph to a residual graph expressing some fundamental relationships. Complete solutions will be more easily obtained by application of Mason's rule.[26]The graph itself programs the reduction process. Indeed a simple inspection of the graph readily suggests the different steps of the reduction which are carried out by elementary transformations, by loop elimination, or by the use of a reduction formula.[26]For digitally reducing a flow graph using an algorithm, Robichaud extends the notion of a simple flow graph to a generalized flow graph:Before describing the process of reduction...the correspondence between the graph and a system of linear equations ... must be generalized...The generalized graphs will represent some operational relationships between groups of variables...To each branch of the generalized graph is associated a matrix giving the relationships between the variables represented by the nodes at the extremities of that branch...[27]The elementary transformations [defined by Robichaud in his Figure 7.2, p. 184] and the loop reduction permit the elimination of any node j of the graph by the reduction formula:[described in Robichaud's Equation 7-1]. With the reduction formula, it is always possible to reduce a graph of any order... [After reduction] the final graph will be a cascade graph in which the variables of the sink nodes are explicitly expressed as functions of the sources. This is the only method for reducing the generalized graph since Mason's rule is obviously inapplicable.[28]The definition of an elementary transformation varies from author to author:Parallel edges. Replace parallel edges with a single edge having a gain equal to the sum of original gains.The graph on the left has parallel edges between nodes. On the right, these parallel edges have been replaced with a single edge having a gain equal to the sum of the gains on each original edge.The equations corresponding to the reduction between N and node I1 are:Outflowing edges. Replace outflowing edges with edges directly flowing from the node's sources.The graph on the left has an intermediate node N between nodes from which it has inflows, and nodes to which it flows out.The graph on the right shows direct flows between these node sets, without transiting via N.For the sake of simplicity, N and its inflows are not represented. The outflows from N are eliminated.The equations corresponding to the reduction directly relating N's input signals to its output signals are:Zero-signal nodes.Eliminate outflowing edges from a node determined to have a value of zero.If the value of a node is zero, its outflowing edges can be eliminated.Nodes without outflows.Eliminate a node without outflows.In this case, N is not a variable of interest, and it has no outgoing edges; therefore, N, and its inflowing edges, can be eliminated.Self-looping edge. Replace looping edges by adjusting the gains on the incoming edges.The graph on the left has a looping edge at node N, with a gain of  g. On the right, the looping edge has been eliminated, and all inflowing edges have their gain divided by (1-g).The equations corresponding to the reduction between N and all its input signals are:The above procedure for building the SFG from an acausal system of equations and for solving the SFG's gains have been implemented[31] as an add-on to MATHLAB 68,[32] an on-line system providing machine aid for the mechanical symbolic processes encountered in analysis.Signal flow graphs can be used to solve sets of simultaneous linear equations.[33] The set of equations must be consistent and all equations must be linearly independent.For M equations with N unknowns where each yj is a known value and each xj is an unknown value, there is equation for each known of the following form.Although it is feasible, particularly for simple cases, to establish a signal flow graph using the equations in this form, some rearrangement allows a general procedure that works easily for any set of equations, as now is presented. To proceed, first the equations are rewritten asand further rewritten asand finally rewritten as The signal-flow graph is now arranged by selecting one of these equations and addressing the node on the right-hand side. This is the node for which the node connects to itself with the branch of weight including a '+1', making a self-loop in the flow graph. The other terms in that equation connect this node first to the source in this equation and then to all the other branches incident on this node. Every equation is treated this way, and then each incident branch is joined to its respective emanating node. For example, the case of three variables is shown in the figure, and the first equation is:where the right side of this equation is the sum of the weighted arrows incident on node x1.As there is a basic symmetry in the treatment of every node, a simple starting point is an arrangement of nodes with each node at one vertex of a regular polygon. When expressed using the general coefficients {cin}, the environment of each node is then just like all the rest apart from a permutation of indices. Such an implementation for a set of three simultaneous equations is seen in the figure.[34]Often the known values, yj are taken as the primary causes and the unknowns values, xj to be effects, but regardless of this interpretation, the last form for the set of equations can be represented as a signal-flow graph. This point is discussed further in the subsection Interpreting 'causality'.In the most general case, the values for all the xk variables can be calculated by computing Mason's gain formula for the path from each yj to each xk and using superposition.In general, there are N-1 paths from yj to variable xk so the computational effort to calculated Gkj is proportional to N-1.Since there are M values of yj, Gkj must be computed M times for a single value of xk.  The computational effort to calculate a single xk variable is proportional to (N-1)(M).  The effort to compute all the xk variables is proportional to (N)(N-1)(M).  If there are N equations and N unknowns, then the computation effort is on the order of N3.For some authors, a linear signal-flow graph is more constrained than a block diagram,[35] in that the SFG rigorously describes linear algebraic equations represented by a directed graph.For other authors, linear block diagrams and linear signal-flow graphs are equivalent ways of depicting a system, and either can be used to solve the gain.[36]A tabulation of the comparison between block diagrams and signal-flow graphs is provided by Bakshi & Bakshi,[37] and another tabulation by Kumar.[38]  According to Barker et al.:[39]In the figure, a simple block diagram for a feedback system is shown with two possible interpretations as a signal-flow graph. The input R(s) is the Laplace-transformed input signal; it is shown as a source node in the signal-flow graph (a source node has no input edges). The output signal C(s) is the Laplace-transformed output variable. It is represented as a sink node in the flow diagram (a sink has no output edges). G(s) and H(s) are transfer functions, with H(s) serving to feed back a modified version of the output to the input, B(s). The two flow graph representations are equivalent.The term "cause and effect" was applied by Mason to SFGs:[2] and has been repeated by many later authors:[40]However, Mason's paper is concerned to show in great detail how a set of equations is connected to an SFG, an emphasis unrelated to intuitive notions of "cause and effect". Intuitions can be helpful for arriving at an SFG or for gaining insight from an SFG, but are inessential to the SFG. The essential connection of the SFG is to its own set of equations, as described, for example, by Ogata:[41]There is no reference to "cause and effect" here, and as said by Barutsky:[42]The term "cause and effect" may be misinterpreted as it applies to the SFG, and taken incorrectly to suggest a system view of causality,[43] rather than a computationally based meaning. To keep discussion clear, it may be advisable to use the term "computational causality",  as is suggested for bond graphs:[44]The term "computational causality" is explained using the example of current and voltage in a resistor:[45]A computer program or algorithm can be arranged to solve a set of equations using various strategies. They differ in how they prioritize finding some of the variables in terms of the others, and these algorithmic decisions, which are simply about solution strategy, then set up the variables expressed as dependent variables earlier in the solution to be "effects", determined by the remaining variables that now are "causes", in the sense of "computational causality".Using this terminology, it is computational causality, not system causality, that is relevant to the SFG. There exists a wide-ranging philosophical debate, not concerned specifically with the SFG, over connections between computational causality and system causality.[46]Signal-flow graphs can be used for analysis, that is for understanding a model of an existing system, or for synthesis, that is for determining the properties of a design alternative.When building a model of a dynamic system, a list of steps is provided by Dorf & Bishop:[47]In this workflow, equations of the physical system's mathematical model are used to derive the signal-flow graph equations.Signal-flow graphs have been used in Design Space Exploration (DSE), as an intermediate representation towards a physical implementation.  The DSE process seeks a suitable solution among different alternatives. In contrast with the typical analysis workflow, where a system of interest is first modeled with the physical equations of its components, the specification for synthesizing a design could be a desired transfer function. For example, different strategies would create different signal-flow graphs, from which implementations are derived.[48]Another example uses an annotated SFG as an expression of the continuous-time behavior, as input to an architecture generator[49]Shannon's formula is an analytic expression for calculating the gain of an interconnected set of amplifiers in an analog computer.  During World War II, while investigating the functional operation of an analog computer, Claude Shannon developed his formula.  Because of wartime restrictions, Shannon's work was not published at that time, and, in 1952, Mason rediscovered the same formula.Happ generalized the Shannon formula for topologically closed systems.[50] The Shannon-Happ formula can be used for deriving transfer functions, sensitivities, and error functions.[51]For a consistent set of linear unilateral relations, the Shannon-Happ formula expresses the solution using direct substitution (non-iterative).[51][52]NASA's electrical circuit software NASAP is based on the Shannon-Happ formula.[51][52]The amplification of a signal V1 by an amplifier with gain a12 is described mathematically byThis relationship represented by the signal-flow graph of Figure 1. is that V2 is dependent on V1 but it implies no dependency of V1 on V2.  See Kou page 57.[53]A possible SFG for the asymptotic gain model for a negative feedback amplifier is shown in Figure 3, and leads to the equation for the gain of this amplifier asThe interpretation of the parameters is as follows: T = return ratio, G∞ = direct amplifier gain, G0 = feedforward (indicating the possible bilateral nature of the feedback, possibly deliberate as in the case of feedforward compensation). Figure 3 has the interesting aspect that it resembles Figure 2 for the two-port network with the addition of the extra feedback relation x2 =  T y1.From this gain expression an interpretation of the parameters G0 and G∞ is evident, namely:There are many possible SFG's associated with any particular gain relation. Figure 4 shows another SFG for the asymptotic gain model that can be easier to interpret in terms of a circuit. In this graph, parameter  β is interpreted as a feedback factor and A as a "control parameter", possibly related to a dependent source in the circuit. Using this graph, the gain isTo connect to the asymptotic gain model, parameters A and β cannot be arbitrary circuit parameters, but must relate to the return ratio T by:and to the asymptotic gain as:Substituting these results into the gain expression,which is the formula of the asymptotic gain model.The figure to the right depicts a circuit that contains a y-parameter two-port network. Vin is the input of the circuit and V2 is the output.  The two-port equations impose a set of linear constraints between its port voltages and currents.  The terminal equations impose other constraints.  All these constraints are represented in the SFG (Signal Flow Graph) below the circuit.  There is only one path from input to output which is shown in a different color and has a (voltage) gain of -RLy21.  There are also three loops: -Riny11, -RLy22, Riny21RLy12. Sometimes a loop indicates intentional feedback but it can also indicate a constraint on the relationship of two variables.  For example, the equation that describes a resistor says that the ratio of the voltage across the resistor to the current through the resistor is a constant which is called the resistance.  This can be interpreted as the voltage is the input and the current is the output, or the current is the input and the voltage is the output, or merely that the voltage and current have a linear relationship.  Virtually all passive two terminal devices in a circuit will show up in the SFG as a loop.The SFG and the schematic depict the same circuit, but the schematic also suggests the circuit's purpose.  Compared to the schematic, the SFG is awkward but it does have the advantage that the input to output gain can be written down by inspection using Mason's rule.This example is representative of a SFG (signal-flow graph) used to represent a servo control system and illustrates several features of SFGs.  Some of the loops (loop 3, loop 4 and loop 5) are extrinsic intentionally designed feedback loops.  These are shown with dotted lines.  There are also intrinsic loops (loop 0, loop1, loop2) that are not intentional feedback loops, although they can be analyzed as though they were.  These loops are shown with solid lines.  Loop 3 and loop 4 are also known as minor loops because they are inside a larger loop.See Mason's rule for development of Mason's Gain Formula for this example.There is some confusion in literature about what a signal-flow graph is; Henry Paynter, inventor of bond graphs, writes: "But much of the decline of signal-flow graphs [...] is due in part to the mistaken notion that the branches must be linear and the nodes must be summative. Neither assumption was embraced by Mason, himself !"[55]A state transition SFG or state diagram is a simulation diagram for a system of equations, including the initial conditions of the states.[56] Closed flowgraphs describe closed systems and have been utilized to provide a rigorous theoretical basis for topological techniques of circuit analysis.[50]Mason introduced both nonlinear and linear flow graphs.  To clarify this point, Mason wrote : "A linear flow graph is one whose associated equations are linear."[2]It we denote by xj the signal at node j, the following are examples of node functions that do not pertain to a linear time-invariant system:
Material point method
The material point method (MPM) is a numerical technique used to simulate the behavior of solids, liquids, gases, and any other continuum material. Especially, it is a robust spatial discretization method for simulating multi-phase (solid-fluid-gas) interactions. In the MPM, a continuum body is described by a number of small Lagrangian elements referred to as 'material points'. These material points are surrounded by a background mesh/grid that is used only to calculate gradient terms such as the deformation gradient. Unlike other mesh-based methods like the finite element method, finite volume method or finite difference method, the MPM is not a mesh based method and is instead categorized as a meshless/meshfree or continuum-based particle method, examples of which are smoothed particle hydrodynamics and peridynamics. Despite the presence of a background mesh, the MPM does not encounter the drawbacks of mesh-based methods (high deformation tangling, advection errors etc.) which makes it a promising and powerful tool in computational mechanics.The MPM was originally proposed, as an extension of a similar method known as FLIP (a further extension of a method called PIC) to computational solid dynamics, in the early 1990 by Professors Deborah L. Sulsky, Zhen Chen and Howard L. Schreyer at University of New Mexico. After this initial development, the MPM has been further developed both in the national labs as well as the University of New Mexico, Oregon State University, University of Utah and more across the US and the world. Recently the number of institutions researching the MPM has been growing with added popularity and awareness coming from various sources such as the MPM's use in the Disney film Frozen.An MPM simulation consists of the following stages:(Prior to the time integration phase)(During the time integration phase - explicit formulation)2. Material point quantities are extrapolated to grid nodes.3. Equations of motion are solved on the grid.5.Resetting of grid.The PIC was originally conceived to solve problems in fluid dynamics, and developed by Harlow at Los Alamos National Laboratory in 1957.[1] One of the first PIC codes was the Fluid-Implicit Particle (FLIP) program, which was created by Brackbill in 1986[2] and has been constantly in development ever since. Until the 1990s, the PIC method was used principally in fluid dynamics.Motivated by the need for better simulating penetration problems in solid dynamics, Sulsky, Chen and Schreyer started in 1993 to reformulate the PIC and develop the MPM, with funding from Sandia National Laboratories.[3] The original MPM was then further extended by Bardenhagen et al.. to include frictional contact,[4] which enabled the simulation of granular flow,[5] and by Nairn to include explicit cracks[6] and crack propagation (known as CRAMP).Recently, an MPM implementation based on a micro-polar Cosserat continuum [7] has been used to simulate high-shear granular flow, such as silo discharge. MPM's uses were further extended into Geotechnical engineering with the recent development of a quasi-static, implicit MPM solver which provides numerically stable analyses of large-deformation problems in Soil mechanics.[8]Annual workshops on the use of MPM are held at various locations in the United States. The Fifth MPM Workshop was held at Oregon State University, in Corvallis, OR, on April 2 and 3, 2009.The uses of the PIC or MPM method can be divided into two broad categories: firstly, there are many applications involving fluid dynamics, plasma physics, magnetohydrodynamics, and multiphase applications. The second category of applications comprises problems in solid mechanics.The PIC method has been used to simulate a wide range of fluid-solid interactions, including sea ice dynamics,[9] penetration of biological soft tissues,[10] fragmentation of gas-filled canisters,[11] dispersion of atmospheric pollutants,[12] multiscale simulations coupling molecular dynamics with MPM,[13][14] and fluid-membrane interactions.[15] In addition, the PIC-based FLIP code has been applied in magnetohydrodynamics and plasma processing tools, and simulations in astrophysics and free-surface flow.[16]As a result of a joint effort between UCLA's mathematics department and Walt Disney Animation Studios, MPM was successfully used to simulate snow in the 2013 computer-animated film Frozen.[17][18][19]MPM has also been used extensively in solid mechanics, to simulate impact, penetration, collision and rebound, as well as crack propagation.[20][21] MPM has also become a widely used method within the field of soil mechanics: it has been used to simulate granular flow, silo discharge, pile driving, bucket filling, and material failure; and to model soil stress distribution, compaction, and hardening. It is now being used in wood mechanics problems such as simulations of transverse compression on the cellular level including cell wall contact [22] (this work received the George Marra Award for paper of the year from the Society of Wood Science and Technology [1])One subset of numerical methods are Meshfree methods, which are defined as methods for which "a predefined mesh is not necessary, at least in field variable interpolation". Ideally, a meshfree method does not make use of a mesh "throughout the process of solving the problem governed by partial differential equations, on a given arbitrary domain, subject to all kinds of boundary conditions," although existing methods are not ideal and fail in at least one of these respects. Meshless methods, which are also sometimes called particle methods, share a "common feature that the history of state variables is traced at points (particles) which are not connected with any element mesh, the distortion of which is a source of numerical difficulties." As can be seen by these varying interpretations, some scientists consider MPM to be a meshless method, while others do not. All agree, however, that MPM is a particle method.The Arbitrary Lagrangian Eulerian (ALE) methods form another subset of numerical methods which includes MPM. Purely Lagrangian methods employ a framework in which a space is discretised into initial subvolumes, whose flowpaths are then charted over time. Purely Eulerian methods, on the other hand, employ a framework in which the motion of material is described relative to a mesh that remains fixed in space throughout the calculation. As the name indicates, ALE methods combine Lagrangian and Eulerian frames of reference.PIC methods may be based on either the strong form collocation or a weak form discretisation of the underlying partial differential equation (PDE). Those based on the strong form are properly referred to as finite-volume PIC methods. Those based on the weak form discretisation of PDEs may be called either PIC or MPM.MPM solvers can model problems in one, two, or three spatial dimensions, and can also model axisymmetric problems. MPM can be implemented to solve either quasi-static or dynamic equations of motion, depending on the type of problem that is to be modeled.The time-integration used for MPM may be either explicit or implicit. The advantage to implicit integration is guaranteed stability, even for large timesteps. On the other hand, explicit integration runs much faster and is easier to implement.Unlike FEM, MPM does not require periodical remeshing steps and remapping of state variables, and is therefore better suited to the modeling of large material deformations. In MPM, particles and not the mesh points store all the information on the state of the calculation. Therefore, no numerical error results from the mesh returning to its original position after each calculation cycle, and no remeshing algorithm is required.The particle basis of MPM allows it to treat crack propagation and other discontinuities better than FEM, which is known to impose the mesh orientation on crack propagation in a material. Also, particle methods are better at handling history-dependent constitutive models.Because in MPM nodes remain fixed on a regular grid, the calculation of gradients is trivial.In simulations with two or more phases it is rather easy to detect contact between entities, as particles can interact via the grid with other particles in the same body, with other solid bodies, and with fluids.MPM is more expensive in terms of storage than other methods, as MPM makes use of mesh as well as particle data. MPM is more computationally expensive than FEM, as the grid must be reset at the end of each MPM calculation step and reinitialised at the beginning of the following step. Spurious oscillation may occur as particles cross the boundaries of the mesh in MPM, although this effect can be minimized by using generalized interpolation methods (GIMP). In MPM as in FEM, the size and orientation of the mesh can impact the results of a calculation: for example, in MPM, strain localisation is known to be particularly sensitive to mesh refinement.A commercial package based on a meshless method is MPMsim.
Matrix norm
In mathematics, a matrix norm is a vector norm in a vector space whose elements (vectors) are matrices (of given dimensions).Additionally, in the case of square matrices (thus, m = n), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors:There are three types of matrix norms which will be discussed below:Moreover, any induced norm satisfies the inequality    (1)which is simply the maximum absolute column sum of the matrix;which is simply the maximum absolute row sum of the matrix;then we haveandFor example, using the p-norm for vectors, p ≥ 1, we get:This is a different norm from the induced p-norm (see above) and the Schatten p-norm (see below), but the notation is the same.The special case p = 2 is the Frobenius norm, and p = ∞ yields the maximum norm.andIt also satisfies and The max norm is the elementwise norm with p = q = ∞:This norm is not sub-multiplicative.The Schatten p-norms arise when applying the p-norm to the vector of singular values of a matrix. If the singular values are denoted by σi, then the Schatten p-norm is defined byThese norms again share the notation with the induced and entrywise p-norms, but they are different.The most familiar cases are p = 1, 2, ∞. The case p = 2 yields the Frobenius norm, introduced before. The case p = ∞ yields the spectral norm, which is the operator norm induced by the vector 2-norm (see above). Finally, p = 1 yields the nuclear norm (also known as the trace norm, or the Ky Fan 'n'-norm[3]), defined asAnother useful inequality between matrix norms iswhich is a special case of Hölder's inequality.
Quantum mechanics
Quantum mechanics (QM; also known as quantum physics, quantum theory, the wave mechanical model, or matrix mechanics), including quantum field theory, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles.[2]Classical physics, the physics existing before quantum mechanics, describes nature at ordinary (macroscopic) scale. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3] Quantum mechanics differs from classical physics in that energy, momentum, angular momentum and other quantities of a bound system are restricted to discrete values (quantization); objects have characteristics of both particles and waves (wave-particle duality); and there are limits to the precision with which quantities can be measured (uncertainty principle).[note 1]Quantum mechanics gradually arose from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and from the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schrödinger, Werner Heisenberg, Max Born and others.  The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.Important applications of quantum theory[5] include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy.  Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.[6]Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[7] In 1803, Thomas Young, an English polymath, performed the famous double-slit experiment that he later described in a paper titled On the nature of light and colours. This experiment played a major role in the general acceptance of the wave theory of light.In 1838, Michael Faraday discovered cathode rays. These studies were followed by the 1859 statement of the black-body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system can be discrete, and the 1900 quantum hypothesis of Max Planck.[8] Planck's hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets) precisely matched the observed patterns of black-body radiation.In 1896, Wilhelm Wien empirically determined a distribution law of black-body radiation,[9] known as Wien's law in his honor. Ludwig Boltzmann independently arrived at this result by considerations of Maxwell's equations. However, it was valid only at high frequencies and underestimated the radiance at low frequencies. Later, Planck corrected this model using Boltzmann's statistical interpretation of thermodynamics and proposed what is now called Planck's law, which led to the development of quantum mechanics.Following Max Planck's solution in 1900 to the black-body radiation problem (reported 1859), Albert Einstein offered a quantum-based theory to explain the photoelectric effect (1905, reported 1887).  Around 1900–1910, the atomic theory and the corpuscular theory of light[10] first came to be widely accepted as scientific fact; these latter theories can be viewed as quantum theories of matter and electromagnetic radiation, respectively.Among the first to study quantum phenomena in nature were Arthur Compton, C. V. Raman, and Pieter Zeeman, each of whom has a quantum effect named after him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Ernest Rutherford experimentally discovered the nuclear model of the atom, for which Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld.[11] This phase is known as old quantum theory.According to Planck, each energy element (E) is proportional to its frequency (ν):where h is Planck's constant.Planck cautiously insisted that this was simply an aspect of the processes of absorption and emission of radiation and had nothing to do with the physical reality of the radiation itself.[12] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[13] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. He won the 1921 Nobel Prize in Physics for this work.Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete quantum of energy that was dependent on its frequency.[14]The foundations of quantum mechanics were established during the first half of the 20th century by Max Planck, Niels Bohr, Werner Heisenberg, Louis de Broglie, Arthur Compton, Albert Einstein, Erwin Schrödinger, Max Born, John von Neumann, Paul Dirac, Enrico Fermi, Wolfgang Pauli, Max von Laue, Freeman Dyson, David Hilbert, Wilhelm Wien, Satyendra Nath Bose, Arnold Sommerfeld, and others. The Copenhagen interpretation of Niels Bohr became widely accepted.In the mid-1920s, developments in quantum mechanics led to its becoming the standard formulation for atomic physics. In the summer of 1925, Bohr and Heisenberg published results that closed the old quantum theory. Out of deference to their particle-like behavior in certain processes and measurements, light quanta came to be called photons (1926). In 1926 Erwin Schrödinger suggested a partial differential equation for the wave functions of particles like electrons. And when effectively restricted to a finite region, this equation allowed only certain modes, corresponding to discrete quantum states—whose properties turned out to be exactly the same as implied by matrix mechanics.[15] From Einstein's simple postulation was born a flurry of debating, theorizing, and testing. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[citation needed]It was found that subatomic particles and electromagnetic waves are neither simply particle nor wave but have certain properties of each. This originated the concept of wave–particle duality.[citation needed]By 1930, quantum mechanics had been further unified and formalized by the work of David Hilbert, Paul Dirac and John von Neumann[16] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry, quantum electronics, quantum optics, and quantum information science. Its speculative modern developments include string theory and quantum gravity theories. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies.[citation needed]While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors,[17]  and superfluids.[18]The word quantum derives from the Latin, meaning "how great" or "how much".[19] In quantum mechanics, it refers to a discrete unit assigned to certain physical quantities such as the energy of an atom at rest (see Figure 1). The discovery that particles are discrete packets of energy with wave-like properties led to the branch of physics dealing with atomic and subatomic systems which is today called quantum mechanics. It underlies the mathematical framework of many fields of physics and chemistry, including condensed matter physics, solid-state physics, atomic physics, molecular physics, computational physics, computational chemistry, quantum chemistry, particle physics, nuclear chemistry, and nuclear physics.[20][better source needed] Some fundamental aspects of the theory are still actively studied.[21]Quantum mechanics is essential to understanding the behavior of systems at atomic length scales and smaller. If the physical nature of an atom were solely described by classical mechanics, electrons would not orbit the nucleus, since orbiting electrons emit radiation (due to circular motion) and would quickly collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, non-deterministic, smeared, probabilistic wave–particle orbital about the nucleus, defying the traditional assumptions of classical mechanics and electromagnetism.[22]Quantum mechanics was initially developed to provide a better explanation and description of the atom, especially the differences in the spectra of light emitted by different isotopes of the same chemical element, as well as subatomic particles. In short, the quantum-mechanical atomic model has succeeded spectacularly in the realm where classical mechanics and electromagnetism falter.Broadly speaking, quantum mechanics incorporates four classes of phenomena for which classical physics cannot account:However, later, in October 2018, physicists reported that quantum behavior can be explained with classical physics for a single particle, but not for multiple particles as in quantum entanglement and related nonlocality phenomena.[23][24]In the mathematically rigorous formulation of quantum mechanics developed by Paul Dirac,[25] David Hilbert,[26] John von Neumann,[27] and Hermann Weyl,[28] the possible states of a quantum mechanical system are symbolized[29] as unit vectors (called state vectors). Formally, these reside in a complex separable Hilbert space—variously called the state space or the associated Hilbert space of the system—that is well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system—for example, the state space for position and momentum states is the space of square-integrable functions, while the state space for the spin of a single proton is just the product of two complex planes. Each observable is represented by a maximally Hermitian (precisely: by a self-adjoint) linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. If the operator's spectrum is discrete, the observable can attain only those discrete eigenvalues.In the formalism of quantum mechanics, the state of a system at a given time is described by a complex wave function, also referred to as state vector in a complex vector space.[30] This abstract mathematical object allows for the calculation of probabilities of outcomes of concrete experiments. For example, it allows one to compute the probability of finding an electron in a particular region around the nucleus at a particular time. Contrary to classical mechanics, one can never make simultaneous predictions of conjugate variables, such as position and momentum, to arbitrary precision. For instance, electrons may be considered (to a certain probability) to be located somewhere within a given region of space, but with their exact positions unknown. Contours of constant probability density, often referred to as "clouds", may be drawn around the nucleus of an atom to conceptualize where the electron might be located with the most probability. Heisenberg's uncertainty principle quantifies the inability to precisely locate the particle given its conjugate momentum.[31]According to one interpretation, as the result of a measurement, the wave function containing the probability information for a system collapses from a given initial state to a particular eigenstate. The possible results of a measurement are the eigenvalues of the operator representing the observable—which explains the choice of Hermitian operators, for which all the eigenvalues are real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator. Heisenberg's uncertainty principle is represented by the statement that the operators corresponding to certain observables do not commute.The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of "wave function collapse" (see, for example, the relative state interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled, so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.[32]Generally, quantum mechanics does not assign definite values. Instead, it makes a prediction using a probability distribution; that is, it describes the probability of obtaining the possible outcomes from measuring an observable. Often these results are skewed by many causes, such as dense probability clouds. Probability clouds are approximate (but better than the Bohr model) whereby electron location is given by a probability function, the wave function eigenvalue, such that the probability is the squared modulus of the complex amplitude, or quantum state nuclear attraction.[33][34] Naturally, these probabilities will depend on the quantum state at the "instant" of the measurement. Hence, uncertainty is involved in the value. There are, however, certain states that are associated with a definite value of a particular observable. These are known as eigenstates of the observable ("eigen" can be translated from German as meaning "inherent" or "characteristic").[35]In the everyday world, it is natural and intuitive to think of everything (every observable) as being in an eigenstate. Everything appears to have a definite position, a definite momentum, a definite energy, and a definite time of occurrence. However, quantum mechanics does not pinpoint the exact values of a particle's position and momentum (since they are conjugate pairs) or its energy and time (since they too are conjugate pairs).  Rather, it provides only a range of probabilities in which that particle might be given its momentum and momentum probability. Therefore, it is helpful to use different words to describe states having uncertain values and states having definite values (eigenstates).Usually, a system will not be in an eigenstate of the observable (particle) we are interested in. However, if one measures the observable, the wave function will instantaneously be an eigenstate (or "generalized" eigenstate) of that observable. This process is known as wave function collapse, a controversial and much-debated process[36] that involves expanding the system under study to include the measurement device. If one knows the corresponding wave function at the instant before the measurement, one will be able to compute the probability of the wave function collapsing into each of the possible eigenstates.For example, the free particle in the previous example will usually have a wave function that is a wave packet centered around some mean position x0 (neither an eigenstate of position nor of momentum). When one measures the position of the particle, it is impossible to predict with certainty the result.[32] It is probable, but not certain, that it will be near x0, where the amplitude of the wave function is large. After the measurement is performed, having obtained some result x, the wave function collapses into a position eigenstate centered at x.[37]The time evolution of a quantum state is described by the Schrödinger equation, in which the Hamiltonian (the operator corresponding to the total energy of the system) generates the time evolution. The time evolution of wave functions is deterministic in the sense that—given a wave function at an initial time—it makes a definite prediction of what the wave function will be at any later time.[38]During a measurement, on the other hand, the change of the initial wave function into another, later wave function is not deterministic, it is unpredictable (i.e., random). A time-evolution simulation can be seen here.[39][40]Wave functions change as time progresses. The Schrödinger equation describes how wave functions change in time, playing a role similar to Newton's second law in classical mechanics. The Schrödinger equation, applied to the aforementioned example of the free particle, predicts that the center of a wave packet will move through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more uncertain with time. This also has the effect of turning a position eigenstate (which can be thought of as an infinitely sharp wave packet) into a broadened wave packet that no longer represents a (definite, certain) position eigenstate.[41]Some wave functions produce probability distributions that are constant, or independent of time—such as when in a stationary state of constant energy, time vanishes in the absolute square of the wave function. Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics it is described by a static, spherically symmetric wave function surrounding the nucleus (Fig. 1) (note, however, that only the lowest angular momentum states, labeled s, are spherically symmetric).[42]The Schrödinger equation acts on the entire probability amplitude, not merely its absolute value. Whereas the absolute value of the probability amplitude encodes information about probabilities, its phase encodes information about the interference between quantum states. This gives rise to the "wave-like" behavior of quantum states. As it turns out, analytic solutions of the Schrödinger equation are available for only a very small number of relatively simple model Hamiltonians, of which the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom are the most important representatives. Even the helium atom—which contains just one more electron than does the hydrogen atom—has defied all attempts at a fully analytic treatment.There exist several techniques for generating approximate solutions, however. In the important method known as perturbation theory, one uses the analytic result for a simple quantum mechanical model to generate a result for a more complicated model that is related to the simpler model by (for one example) the addition of a weak potential energy. Another method is the "semi-classical equation of motion" approach, which applies to systems for which quantum mechanics produces only weak (small) deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.There are numerous mathematically equivalent formulations of quantum mechanics. One of the oldest and most commonly used formulations is the "transformation theory" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics—matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrödinger).[43]Especially since Werner Heisenberg was awarded the Nobel Prize in Physics in 1932 for the creation of quantum mechanics, the role of Max Born in the development of QM was overlooked until the 1954 Nobel award. The role is noted in a 2005 biography of Born, which recounts his role in the matrix formulation of quantum mechanics, and the use of probability amplitudes. Heisenberg himself acknowledges having learned matrices from Born, as published in a 1940 festschrift honoring Max Planck.[44]  In the matrix formulation, the instantaneous state of a quantum system encodes the probabilities of its measurable properties, or "observables". Examples of observables include energy, position, momentum, and angular momentum. Observables can be either continuous (e.g., the position of a particle) or discrete (e.g., the energy of an electron bound to a hydrogen atom).[45] An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.The rules of quantum mechanics are fundamental. They assert that the state space of a system is a Hilbert space (crucially, that the space has an inner product) and that observables of that system are Hermitian operators acting on vectors in that space—although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system. An important guide for making these choices is the correspondence principle, which states that the predictions of quantum mechanics reduce to those of classical mechanics when a system moves to higher energies or, equivalently, larger quantum numbers, i.e. whereas a single particle exhibits a degree of randomness, in systems incorporating millions of particles averaging takes over and, at the high energy limit, the statistical probability of random behaviour approaches zero. In other words, classical mechanics is simply a quantum mechanics of large systems. This "high energy" limit is known as the classical or correspondence limit. One can even start from an established classical model of a particular system, then attempt to guess the underlying quantum model that would give rise to the classical model in the correspondence limit.When quantum mechanics was originally formulated, it was applied to models whosecorrespondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg. These three men shared the Nobel Prize in Physics in 1979 for this work.[46]It has proven difficult to construct quantum models of gravity, the remaining fundamental force. Semi-classical approximations are workable, and have led to predictions such as Hawking radiation. However, the formulation of a complete theory of quantum gravity is hindered by apparent incompatibilities between general relativity (the most accurate theory of gravity currently known) and some of the fundamental assumptions of quantum theory. The resolution of these incompatibilities is an area of active research, and theories such as string theory are among the possible candidates for a future theory of quantum gravity.Classical mechanics has also been extended into the complex domain, with complex classical mechanics exhibiting behaviors similar to quantum mechanics.[47]Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy.[48] According to the correspondence principle between classical and quantum mechanics, all objects obey the laws of quantum mechanics, and classical mechanics is just an approximation for large systems of objects (or a statistical quantum mechanics of a large collection of particles).[49] The laws of classical mechanics thus follow from the laws of quantum mechanics as a statistical average at the limit of large systems or large quantum numbers.[50] However, chaotic systems do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.Quantum coherence is an essential difference between classical and quantum theories as illustrated by the Einstein–Podolsky–Rosen (EPR) paradox — an attack on a certain philosophical interpretation of quantum mechanics by an appeal to local realism.[51] Quantum interference involves adding together probability amplitudes, whereas classical "waves" infer that there is an adding together of intensities. For microscopic bodies, the extension of the system is much smaller than the coherence length, which gives rise to long-range entanglement and other nonlocal phenomena characteristic of quantum systems.[52] Quantum coherence is not typically evident at macroscopic scales, though an exception to this rule may occur at extremely low temperatures (i.e. approaching absolute zero) at which quantum behavior may manifest itself macroscopically.[53] This is in accordance with the following observations:A big difference between classical and quantum mechanics is that they use very different kinematic descriptions.[56]In Niels Bohr's mature view, quantum mechanical phenomena are required to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics.[57][58][59][60] The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, in terms of both position and momentum, of an initial condition or "state" (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition.[61][62] In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous "state" in the classical sense of that word.[63][64] Thus there are two kinds of processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined.[61] Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schrödinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.[65][66]For many experiments, it is possible to think of the initial and final conditions of the system as being a particle. In some cases it appears that there are potentially several spatially distinct pathways or trajectories by which a particle might pass from initial to final condition. It is an important feature of the quantum kinematic description that it does not permit a unique definite statement of which of those pathways is actually followed. Only the initial and final conditions are definite, and, as stated in the foregoing paragraph, they are defined only as precisely as allowed by the configuration space description or its equivalent. In every case for which a quantum kinematic description is needed, there is always a compelling reason for this restriction of kinematic precision. An example of such a reason is that for a particle to be experimentally found in a definite position, it must be held motionless; for it to be experimentally found to have a definite momentum, it must have free motion; these two are logically incompatible.[67][68]Classical kinematics does not primarily demand experimental description of its phenomena. It allows completely precise description of an instantaneous state by a value in phase space, the Cartesian product of configuration and momentum spaces. This description simply assumes or imagines a state as a physically existing entity without concern about its experimental measurability. Such a description of an initial condition, together with Newton's laws of motion, allows a precise deterministic and causal prediction of a final condition, with a definite trajectory of passage. Hamiltonian dynamics can be used for this. Classical kinematics also allows the description of a process analogous to the initial and final condition description used by quantum mechanics. Lagrangian mechanics applies to this.[69] For processes that need account to be taken of actions of a small number of Planck constants, classical kinematics is not adequate; quantum mechanics is needed.Even with the defining postulates of both Einstein's theory of general relativity and quantum theory being indisputably supported by rigorous and repeated empirical evidence, and while they do not directly contradict each other theoretically (at least with regard to their primary claims), they have proven extremely difficult to incorporate into one consistent, cohesive model.[70]Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. Many prominent physicists, including Stephen Hawking, have labored for many years in the attempt to discover a theory underlying everything. This TOE would combine not only the different models of subatomic physics, but also derive the four fundamental forces of nature - the strong force, electromagnetism, the weak force, and gravity - from a single force or phenomenon. While Stephen Hawking was initially a believer in the Theory of Everything, after considering Gödel's Incompleteness Theorem, he has concluded that one is not obtainable, and has stated so publicly in his lecture "Gödel and the End of Physics" (2002).[71]The quest to unify the fundamental forces through quantum mechanics is still ongoing. Quantum electrodynamics (or "quantum electromagnetism"), which is currently (in the perturbative regime at least) the most accurately tested physical theory in competition with general relativity,[72][73] has been successfully merged with the weak nuclear force into the electroweak force and work is currently being done to merge the electroweak and strong force into the electrostrong force. Current predictions state that at around 1014 GeV the three aforementioned forces are fused into a single unified field.[74] Beyond this "grand unification", it is speculated that it may be possible to merge gravity with the other three gauge symmetries, expected to occur at roughly 1019 GeV. However — and while special relativity is parsimoniously incorporated into quantum electrodynamics — the expanded general relativity, currently the best theory describing the gravitation force, has not been fully incorporated into quantum theory. One of those searching for a coherent TOE is Edward Witten, a theoretical physicist who formulated the M-theory, which is an attempt at describing the supersymmetrical based string theory. M-theory posits that our apparent 4-dimensional spacetime is, in reality, actually an 11-dimensional spacetime containing 10 spatial dimensions and 1 time dimension, although 7 of the spatial dimensions are - at lower energies - completely "compactified" (or infinitely curved) and not readily amenable to measurement or probing.Another popular theory is Loop quantum gravity (LQG), a theory first proposed by Carlo Rovelli that describes the quantum properties of gravity. It is also a theory of quantum space and quantum time, because in general relativity the geometry of spacetime is a manifestation of gravity. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature of the granularity of the photons in the quantum theory of electromagnetism or the discrete levels of the energy of the atoms. But here it is space itself which is discrete.More precisely, space can be viewed as an extremely fine fabric or network "woven" of finite loops. These networks of loops are called spin networks. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the Planck length, which is approximately 1.616×10−35 m. According to theory, there is no meaning to length shorter than this (cf. Planck scale energy). Therefore, LQG predicts that not just matter, but also space itself, has an atomic structure.Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. Even fundamental issues, such as Max Born's basic rules concerning probability amplitudes and probability distributions, took decades to be appreciated by society and many leading scientists. Richard Feynman once said, "I think I can safely say that nobody understands quantum mechanics."[75]  According to Steven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."[76]The Copenhagen interpretation—due largely to Niels Bohr and Werner Heisenberg—remains most widely accepted amongst physicists, some 75 years after its enunciation. According to this interpretation, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but instead must be considered a final renunciation of the classical idea of "causality." It is also believed therein that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the conjugate nature of evidence obtained under different experimental situations.Albert Einstein, himself one of the founders of quantum theory, did not accept some of the more philosophical or metaphysical interpretations of quantum mechanics, such as rejection of determinism and of causality. He is famously quoted as saying, in response to this aspect, "God does not play with dice".[77] He rejected the concept that the state of a physical system depends on the experimental arrangement for its measurement. He held that a state of nature occurs in its own right, regardless of whether or how it might be observed. In that view, he is supported by the currently accepted definition of a quantum state, which remains invariant under arbitrary choice of configuration space for its representation, that is to say, manner of observation. He also held that underlying quantum mechanics there should be a theory that thoroughly and directly expresses the rule against action at a distance; in other words, he insisted on the principle of locality. He considered, but rejected on theoretical grounds, a particular proposal for hidden variables to obviate the indeterminism or acausality of quantum mechanical measurement. He considered that quantum mechanics was a currently valid but not a permanently definitive theory for quantum phenomena. He thought its future replacement would require profound conceptual advances, and would not come quickly or easily. The Bohr-Einstein debates provide a vibrant critique of the Copenhagen Interpretation from an epistemological point of view. In arguing for his views, he produced a series of objections, the most famous of which has become known as the Einstein–Podolsky–Rosen paradox.John Bell showed that this EPR paradox led to experimentally testable differences between quantum mechanics and theories that rely on added hidden variables. Experiments have been performed confirming the accuracy of quantum mechanics, thereby demonstrating that quantum mechanics cannot be improved upon by addition of hidden variables.[78] Alain Aspect's initial experiments in 1982, and many subsequent experiments since, have definitively verified quantum entanglement. By the early 1980s, experiments had shown that such inequalities were indeed violated in practice—so that there were in fact correlations of the kind suggested by quantum mechanics. At first these just seemed like isolated esoteric effects, but by the mid-1990s, they were being codified in the field of quantum information theory, and led to constructions with names like quantum cryptography and quantum teleportation.[79] Entanglement, as demonstrated in Bell-type experiments, does not, however, violate causality, since no transfer of information happens. Quantum entanglement forms the basis of quantum cryptography, which is proposed for use in high-security commercial applications in banking and government.The Everett many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[80] This is not accomplished by introducing some "new axiom" to quantum mechanics, but on the contrary, by removing the axiom of the collapse of the wave packet. All of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a real physical—not just formally mathematical, as in other interpretations—quantum superposition. Such a superposition of consistent state combinations of different systems is called an entangled state. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett's interpretation is perfectly consistent with John Bell's experiments and makes them intuitively understandable. However, according to the theory of quantum decoherence, these "parallel universes" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes entangled with both the physicist who measured it and a huge number of other particles, some of which are photons flying away at the speed of light towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring all these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one could theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist's memory). In light of these Bell tests, Cramer (1986) formulated his transactional interpretation[81] which is unique in providing a physical explanation for the Born rule.[82] Relational quantum mechanics appeared in the late 1990s as the modern derivative of the Copenhagen Interpretation.Quantum mechanics has had enormous[83] success in explaining many of the features of our universe. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Quantum mechanics has strongly influenced string theories, candidates for a Theory of Everything (see reductionism).Quantum mechanics is also critically important for understanding how individual atoms are joined by covalent bond to form molecules. The application of quantum mechanics to chemistry is known as quantum chemistry.  Quantum mechanics can also provide quantitative insight into ionic and covalent bonding processes by explicitly showing which molecules are energetically favorable to which others and the magnitudes of the energies involved.[84] Furthermore, most of the calculations performed in modern computational chemistry rely on quantum mechanics.In many aspects modern technology operates at a scale where quantum effects are significant.Many modern electronic devices are designed using quantum mechanics. Examples include the laser, the transistor (and thus the microchip), the electron microscope, and magnetic resonance imaging (MRI). The study of semiconductors led to the invention of the diode and the transistor, which are indispensable parts of modern electronics systems, computer and telecommunication devices. Another application is for making laser diode and light emitting diode which are a high-efficiency source of light.Many electronic devices operate under effect of quantum tunneling. It even exists in the simple light switch. The switch would not work if electrons could not quantum tunnel through the layer of oxidation on the metal contact surfaces. Flash memory chips found in USB drives use quantum tunneling to erase their memory cells. Some negative differential resistance devices also utilize quantum tunneling effect, such as resonant tunneling diode. Unlike classical diodes, its current is carried by resonant tunneling through two or more potential barriers (see right figure). Its negative resistance behavior can only be understood with quantum mechanics: As the confined state moves close to Fermi level, tunnel current increases. As it moves away, current decreases. Quantum  mechanics is necessary to understanding and designing such electronic devices.Researchers are currently seeking robust methods of directly manipulating quantum states. Efforts are being made to more fully develop quantum cryptography, which will theoretically allow guaranteed secure transmission of information.An inherent advantage yielded by quantum cryptography when compared to classical cryptography is the detection of passive eavesdropping. This is a natural result of the behavior of quantum bits; due to the observer effect, if a bit in a superposition state were to be observed, the superposition state would collapse into an eigenstate. Because the intended recipient was expecting to receive the bit in a superposition state, the intended recipient would know there was an attack, because the bit's state would no longer be in a superposition.[85]Another goal is the development of quantum computers, which are expected to perform certain computational tasks exponentially faster than classical computers. Instead of using classical bits, quantum computers use qubits, which can be in superpositions of states. Quantum programmers are able to  manipulate the superposition of qubits in order to solve problems that classical computing cannot do effectively, such as searching unsorted databases or integer factorization. IBM claims that the advent of quantum computing may progress the fields of medicine, logistics, financial services, artificial intelligence and cloud security.[86]Another active research topic is quantum teleportation, which deals with techniques to transmit quantum information over arbitrary distances.While quantum mechanics primarily applies to the smaller atomic regimes of matter and energy, some systems exhibit quantum mechanical effects on a large scale. Superfluidity, the frictionless flow of a liquid at temperatures near absolute zero, is one well-known example. So is the closely related phenomenon of superconductivity, the frictionless flow of an electron gas in a conducting material (an electric current) at sufficiently low temperatures. The fractional quantum Hall effect is a topological ordered state which corresponds to patterns of long-range quantum entanglement.[87] States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.Quantum theory also provides accurate descriptions for many previously unexplained phenomena, such as black-body radiation and the stability of the orbitals of electrons in atoms. It has also given insight into the workings of many different biological systems, including smell receptors and protein structures.[88] Recent work on photosynthesis has provided evidence that quantum correlations play an essential role in this fundamental process of plants and many other organisms.[89] Even so, classical physics can often provide good approximations to results otherwise obtained by quantum physics, typically in circumstances with large numbers of particles or large quantum numbers. Since classical formulas are much simpler and easier to compute than quantum formulas, classical approximations are used and preferred when the system is large enough to render the effects of quantum mechanics insignificant.For example, consider a free particle. In quantum mechanics, a free matter is described by a wave function.  The particle properties of the matter become apparent when we measure its position and velocity.  The wave properties of the matter become apparent when we measure its wave properties like interference.  The wave–particle duality feature is incorporated in the relations of coordinates and operators in the formulation of quantum mechanics.  Since the matter is free (not subject to any interactions), its quantum state can be represented as a wave of arbitrary shape and extending over space as a wave function. The position and momentum of the particle are observables. The Uncertainty Principle states that both the position and the momentum cannot simultaneously be measured with complete precision. However, one can measure the position (alone) of a moving free particle, creating an eigenstate of position with a wave function that is very large (a Dirac delta) at a particular position x, and zero everywhere else. If one performs a position measurement on such a wave function, the resultant x will be obtained with 100% probability (i.e., with full certainty, or complete precision). This is called an eigenstate of position—or, stated in mathematical terms, a generalized position eigenstate (eigendistribution). If the particle is in an eigenstate of position, then its momentum is completely unknown. On the other hand, if the particle is in an eigenstate of momentum, then its position is completely unknown.[90]In an eigenstate of momentum having a plane wave form, it can be shown that the wavelength is equal to h/p, where h is Planck's constant and p is the momentum of the eigenstate.[91]With the differential operator defined bythe previous equation is evocative of the classic kinetic energy analogue,The general solutions of the Schrödinger equation for the particle in a box areor, from Euler's formula,The infinite potential walls of the box determine the values of C, D, and k at x = 0 and x = L where ψ must be zero. Thus, at x = 0,and D = 0. At x = L,in which C cannot be zero as this would conflict with the Born interpretation. Therefore, since sin(kL) = 0, kL must be an integer multiple of π,The quantization of energy levels follows from this constraint on k, sinceA finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth.The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well.This is a model for the quantum tunneling effect which plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy. Quantum tunneling is central to physical phenomena involved in superlattices.As in the classical case, the potential for the quantum harmonic oscillator is given byThis problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. The eigenstates are given bywhere Hn are the Hermite polynomialsand the corresponding energy levels areThis is another example illustrating the quantification of energy for bound states.The potential in this case is given by:The solutions are superpositions of left- and right-moving waves:andwith coefficients A and B determined from the boundary conditions and by imposing a continuous derivative on the solution, and where the wave vectors are related to the energy viaandEach term of the solution can be interpreted as an incident, reflected, or transmitted component of the wave, allowing the calculation of transmission and reflection coefficients. Notably, in contrast to classical mechanics, incident particles with energies greater than the potential step are partially reflected.The following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus.More technical:On Wikibooks
Linear equation
In mathematics, a linear equation is an equation that may be put in the formIn the words of algebra, a linear equation is obtained by equating to zero a linear polynomial over some field, where the coefficients are taken from, and that does not contain the symbols for the indeterminates.The solutions of such an equation are the values that, when substituted to the unknowns, make the equality true.The case of just one variable is of particular importance, and it is frequent that the term linear equation refers implicitly to this particular case, in which the name unknown for the variable is sensibly used.All the pairs of numbers that are solutions of a linear equation in two variables form a line in the Euclidean plane, and every line may be defined as the solutions of a linear equation. This is the origin of the term linear for qualifying this type of equations. More generally, the solutions of a linear equation in n variables form a hyperplane (of dimension n – 1) in the Euclidean space of dimension n.Linear equations occur frequently in all mathematics and their applications in physics and engineering, partly because non-linear systems are often well approximated by linear equations.This article considers the case of a single equation with coefficients from the field of real numbers, for which one studies the real solutions. All its content applies for complex solutions and, more generally, for linear equations with coefficient and solutions in any field. For the case of several simultaneous linear equations, see System of linear equations.Frequently the term linear equation refers implicitly to the case of just one variable. This case, in which the name unknown for the variable is sensibly used, is of particular importance, since it offers a unique value as solution to the equation. According to the above definition such an equation has the formand, for a ≠ 0, a unique value as solution The above equation may always be rewritten toand the solution is of course the same in both cases: These equivalent variants are sometimes given generic names, like general form or standard form,[1] but contribute no new concepts.which is identical to the above form. The intercept form also works conveniently in higher dimensions for specifying (hyper)planes, when their intersections with all coordinate axes exist and are known.Expanding, regrouping, and appropriately factoring the products leads toThe products in the above equation result also from the evaluation of a 2-rowed determinant, inducing this form of the linear equation:The products on the left hand side of the expanded version can be reproduced by evaluating the 3-rowed determinant, designed for easy memorability:Equating the exterior product of these two vectors, as specified above, to zero, yields a linear equationwhich is identical to the determinant form above.Writing a linear equation in two unknowns in the formThis notation can easily expanded to more linear equations in more than two variables. For example, a system of two equations in two variablesA linear equation, written in the form y = f(x) whose  graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.An everyday example of the use of different forms of linear equations is computation of tax with tax brackets. This is commonly done with a progressive tax computation, using either point–slope form or slope–intercept form.For an equation to have meaningful solutions, at least one coefficient must be non-zero. This can be formulated as 
Mathematics
Mathematics (from Greek μάθημα máthēma, "knowledge, study, learning") includes the study of such topics as quantity,[1] structure,[2] space,[1] and change.[3][4][5]Mathematicians seek and use patterns[6][7] to formulate new conjectures; they resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.[8]Mathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics, or mathematics for its own sake, without having any application in mind. Practical applications for what began as pure mathematics are often discovered.[9][10]The history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals,[11] was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.[12]Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[13] The most ancient mathematical texts from Mesopotamia and Egypt are from 2000–1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication and division) first appear in the archaeological record. The Babylonians also possessed a place-value system, and used a sexagesimal numeral system, still in use today for measuring angles and time.[14]Beginning in the 6th century BC with the Pythagoreans, the Ancient Greeks began a systematic study of mathematics as a subject in its own right with Greek mathematics.[15] Around 300 BC, Euclid introduced the axiomatic method still used in mathematics today, consisting of definition, axiom, theorem, and proof. His textbook Elements is widely considered the most successful and influential textbook of all time.[16] The greatest mathematician of antiquity is often held to be Archimedes (c. 287–212 BC) of Syracuse.[17] He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[18] Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC),[19] trigonometry (Hipparchus of Nicaea (2nd century BC),[20] and the beginnings of algebra (Diophantus, 3rd century AD).[21]The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition of sine and cosine, and an early form of infinite series.During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other notable achievements of the Islamic period are advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī. During the early modern period, mathematics began to develop at an accelerating pace in Western Europe. The development of calculus by Newton and Leibniz in the 17th century revolutionized mathematics. Leonhard Euler was the most notable mathematician of the 18th century, contributing numerous theorems and discoveries. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Friedrich Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory,number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show that any axiomatic system that is consistent will contain unprovable propositions.Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."[22]The word mathematics comes from Ancient Greek μάθημα (máthēma), meaning "that which is learnt",[23] "what one gets to know", hence also "study" and "science". The word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times.[24] Its adjective is μαθηματικός (mathēmatikós), meaning "related to learning" or "studious", which likewise further came to mean "mathematical". In particular, μαθηματικὴ τέχνη (mathēmatikḗ tékhnē), Latin: ars mathematica, meant "the mathematical art".Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant "teachers" rather than "mathematicians" in the modern sense.In Latin, and in English until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of mathematici, meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.[25]The apparent plural form in English, like the French plural form les mathématiques (and the less commonly used singular derivative la mathématique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural τὰ μαθηματικά (ta mathēmatiká), used by Aristotle (384–322 BC), and meaning roughly "all things mathematical"; although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from Greek.[26] In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in North America, math.[27]Mathematics has no generally accepted definition.[28][29] Aristotle defined mathematics as "the science of quantity", and this definition prevailed until the 18th century.[30]  Galileo Galilei (1564–1642) said, "The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth."[31] Carl Friedrich Gauss (1777–1855) referred to mathematics as "the Queen of the Sciences".[32] Benjamin Peirce (1809–1880) called mathematics "the science that draws necessary conclusions".[33] David Hilbert said of mathematics: "We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise."[34] Albert Einstein (1879–1955) stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[35]Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions.[36] Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals.[28] There is not even consensus on whether mathematics is an art or a science.[29] A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable.[28] Some just say, "Mathematics is what mathematicians do."[28]Three leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought.[37] All have severe problems, none has widespread acceptance, and no reconciliation seems possible.[37]An early definition of mathematics in terms of logic was Benjamin Peirce's "the science that draws necessary conclusions" (1870).[38] In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's "All Mathematics is Symbolic Logic" (1903).[39]Intuitionist definitions, developing from the philosophy of mathematician L. E. J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other."[37] A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems".[40] A formal system is a set of symbols, or tokens, and some rules telling how the tokens may be combined into formulas. In formal systems, the word axiom has a special meaning, different from the ordinary meaning of "a self-evident truth". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.The German mathematician Carl Friedrich Gauss referred to mathematics as "the Queen of the Sciences".[32] More recently, Marcus du Sautoy has called mathematics "the Queen of Science ... the main driving force behind scientific discovery".[41] In the original Latin Regina Scientiarum, as well as in German Königin der Wissenschaften, the word corresponding to science means a "field of knowledge", and this was the original meaning of "science" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of "science" to natural science follows the rise of Baconian science, which contrasted "natural science" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that "as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."[35]Many philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper.[42] However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians[who?] that mathematics cannot be reduced to logic alone, and Karl Popper concluded that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."[43] Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.[44][45]An alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.The opinions of mathematicians on this matter are varied. Many mathematicians[46] feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others[who?] feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). It is common to see universities divided into sections that include a division of Science and Mathematics, indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.[citation needed]Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.[47]Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what Eugene Wigner has called "the unreasonable effectiveness of mathematics".[10] As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages.[48] Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G. H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic.[49] Mathematicians often strive to find proofs that are particularly elegant, proofs from "The Book" of God according to Paul Erdős.[50][51] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.Most of the mathematical notation in use today was not invented until the 16th century.[52] Before that, mathematics was written out in words, limiting mathematical discovery.[53] Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more abstract and more encrypted than those of natural language.[54] Unlike natural language, where people can often equate a word (such as cow) with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog.[55] Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.[56]Mathematical language can be difficult to understand for beginners because even common terms, such as or and only, have a more precise meaning than they have in everyday speech, and other terms such as open and field refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as homeomorphism and integrable that have no meaning outside of mathematics. Additionally, shorthand phrases such as iff for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject.[b] The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.[57]Axioms in traditional thought were "self-evident truths", but that conception is problematic.[58] At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.[59]Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase "crisis of foundations" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930.[60] Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proved are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science,[citation needed] as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the "P = NP?" problem, one of the Millennium Prize Problems.[61] Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.The study of quantity starts with numbers, first the familiar natural numbers and integers ("whole numbers") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.As the number system is further developed, the integers are recognized as a subset of the rational numbers ("fractions"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of "infinity". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.The study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments;[62] the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.[c]Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[63] Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.[64]Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.Arguably the most prestigious award in mathematics is the Fields Medal,[65][66] established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a $1 million reward.
Endomorphism
In mathematics, an endomorphism is a morphism (or homomorphism) from a mathematical object to itself.  For example, an endomorphism of a vector space V is a linear map f: V → V, and an endomorphism of a group G is a group homomorphism f: G → G. In general, we can talk about endomorphisms in any category. In the category of sets, endomorphisms are functions from a set S to itself.In any category, the composition of any two endomorphisms of X is again an endomorphism of X.  It follows that the set of all endomorphisms of X forms a monoid, denoted End(X) (or EndC(X) to emphasize the category C).An invertible endomorphism of X is called an automorphism.  The set of all automorphisms is a subset of End(X) with a group structure, called the automorphism group of X and denoted Aut(X).  In the following diagram, the arrows denote implication:Any two endomorphisms of an abelian group, A, can be added together by the rule (f + g)(a) = f(a) + g(a).  Under this addition, and with multiplication being defined as function composition, the endomorphisms of an abelian group form a ring (the endomorphism ring).  For example, the set of endomorphisms of ℤn is the ring of all n × n matrices with integer entries.  The endomorphisms of a vector space or module also form a ring, as do the endomorphisms of any object in a preadditive category.  The endomorphisms of a nonabelian group generate an algebraic structure known as a near-ring. Every ring with one is the endomorphism ring of its regular module, and so is a subring of an endomorphism ring of an abelian group;[1] however there are rings that are not the endomorphism ring of any abelian group.In any concrete category, especially for vector spaces, endomorphisms are maps from a set into itself, and may be interpreted as unary operators on that set, acting on the elements, and allowing to define the notion of orbits of elements, etc.Depending on the additional structure defined for the category at hand (topology, metric, ...), such operators can have properties like continuity, boundedness, and so on.  More details should be found in the article about operator theory.An endofunction is a function whose domain is equal to its codomain. A homomorphic endofunction is an endomorphism.Let S be an arbitrary set. Among endofunctions on S one finds permutations of S and constant functions associating to every x ∈ S the same c ∈ S.  Every permutation of S has the codomain equal to its domain and is bijective and invertible. A constant function on S, if S has more than 1 element, has an image that is a proper subset of its codomain, is not bijective (and non invertible). The function associating to each natural integer n the floor of n/2 has its image equal to its codomain and is not invertible.Finite endofunctions are equivalent to directed pseudoforests. For sets of size n there are nn endofunctions on the set.Particular examples of bijective endofunctions are the involutions; i.e., the functions coinciding with their inverses.
Bernstein polynomial
In the mathematical field of numerical analysis, a Bernstein polynomial, named after Sergei Natanovich Bernstein, is a polynomial in the Bernstein form, that is a linear combination of Bernstein basis polynomials.A numerically stable way to evaluate polynomials in Bernstein form is de Casteljau's algorithm.Polynomials in Bernstein form were first used by Bernstein in a constructive proof for the Stone–Weierstrass approximation theorem. With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bézier curves.The n + 1 Bernstein basis polynomials of degree n are defined asThe Bernstein basis polynomials of degree n form a basis for the vector space Πn of polynomials of degree at most n.A linear combination of Bernstein basis polynomialsis called a Bernstein polynomial or polynomial in Bernstein form of degree n.[1]The first few Bernstein basis polynomials are:The Bernstein basis polynomials have the following properties:and by the inverse binomial transformation the reverse transformation is[2]Let ƒ be a continuous function on the interval [0, 1]. Consider the Bernstein polynomialIt can be shown thatuniformly on the interval [0, 1].[3]  This is a stronger statement than the proposition that the limit holds for each value of x separately; that would be pointwise convergence rather than uniform convergence. Specifically, the word uniformly signifies thatBernstein polynomials thus afford one way to prove the Weierstrass approximation theorem that every real-valued continuous function on a real interval [a, b] can be uniformly approximated by polynomial functions over R.[4]A more general statement for a function with continuous kth derivative iswhere additionallyis an eigenvalue of Bn; the corresponding eigenfunction is a polynomial of degree k.Suppose K is a random variable distributed as the number of successes in n independent Bernoulli trials with probability x of success on each trial; in other words, K has a binomial distribution with parameters n and x.  Then we have the expected value E(K/n) = x.By the weak law of large numbers of probability theory,for every δ > 0. Moreover, this relation holds uniformly in x, which can be seen from its proof via Chebyshev's inequality, taking into account that the variance of K/n, equal to x(1-x)/n, is bounded from above by 1/(4n) irrespective of x.Because ƒ, being continuous on a closed bounded interval, must be uniformly continuous on that interval, one infers a statement of the formuniformly in x. Taking into account that ƒ is bounded (on the given interval) one gets for the expectationuniformly in x. To this end one splits the sum for the expectation in two parts. On one part the difference does not exceed ε; this part cannot contribute more than ε.On the other part the difference exceeds ε, but does not exceed 2M, where M is an upper bound for |ƒ(x)|; this part cannot contribute more than 2M times the small probability that the difference exceeds ε.Finally, one observes that the absolute value of the difference between expectations never exceeds the expectation of the absolute value of the difference, and that E(ƒ(K/n)) is just the Bernstein polynomial Bn(ƒ, x).See for instance.[5]
Matrix multiplication
In mathematics, matrix multiplication or matrix product is a binary operation that produces a matrix from two matrices with entries in a field, or, more generally, in a ring or even a semiring. The matrix product is designed for representing the composition of linear maps that are represented by matrices. Matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering.[1][2] In more detail, if A is an n × m matrix and B is an m × p matrix, their matrix product AB is an n × p matrix, in which the m entries across a row of A are multiplied with the m entries down a column of B and summed to produce an entry of AB.  When two linear maps are represented by matrices, then the matrix product represents the composition of the two maps.The definition of matrix product requires that the entries belong to a semiring, and does not require multiplication of elements of the semiring to be commutative. In many applications, the matrix elements belong to a field, although the tropical semiring is also a common choice for graph shortest path problems.[3] Even in the case of matrices over fields, the product is not commutative in general, although it is associative and is distributive over matrix addition. The identity matrices (which are the square matrices whose entries are zero outside of the main diagonal and 1 on the main diagonal) are identity elements of the matrix product. It follows that the n × n matrices over a ring form a ring, which is noncommutative except if n = 1 and the ground ring is commutative.A square matrix may have a multiplicative inverse, called an inverse matrix. In the common case where the entries belong to a commutative ring r, a matrix has an inverse if and only if its determinant has a multiplicative inverse in r. The determinant of a product of square matrices is the product of the determinants of the factors. The n × n matrices that have an inverse form a group under matrix multiplication, the subgroups of which are called matrix groups. Many classical groups (including all finite groups) are isomorphic to matrix groups; this is the starting point of the theory of group representations.This article will use the following notational conventions: matrices are represented by capital letters in bold, e.g. A, vectors in lowercase bold, e.g. a, and entries of vectors and matrices are italic (since they are numbers from a field), e.g. A and a. Index notation is often the clearest way to express definitions, and is used as standard in the literature. The i, j entry of matrix A is indicated by (A)ij, Aij or aij, whereas a numerical label (not matrix entries) on a collection of matrices is subscripted only, e.g. A1, A2, etc.If A is an n × m matrix and B is an m × p matrix,the matrix product C = AB  (denoted without multiplication signs or dots) is defined to be the n × p matrix[5][6][7][8]such that for i = 1, ..., n and j = 1, ..., p.Thus the product AB is defined if and only if the number of columns in A equals the number of rows in B, in this case m.Usually the entries are numbers, but they may be any kind mathematical objects for which an addition and a multiplication are defined, that are associative, and such that the addition is commutative, and the multiplication is distributive with respect to the addition. In particular, the entries may be matrices themselves (see block matrix).The figure to the right illustrates diagrammatically the product of two matrices A and B, showing how each intersection in the product matrix corresponds to a row of A and a column of B.The values at the intersections marked with circles are:Historically, matrix multiplication has been introduced for making easier and clarifying computations in linear algebra. This strong relationship between matrix multiplication and linear algebra remains fundamental in all mathematics, as well as in physics, engineering and computer science.If a vector space has a finite basis, its elements (vectors) are uniquely represented by a finite sequence, called coordinate vector, or scalars, which are the coordinates of the vector on the basis. These coordinates are commonly organized as a column matrix (also called column vector), that is a matrix with only one column.A linear map A from a vector space of dimension n into a vector space of dimension m maps a column vectoronto the column vectorThe linear map A is thus defined by the matrix The general form of a system of linear equations isUsing same notation as above, such a system is equivalent with the single matrix equationThe dot product of two column vectors is the matrix product More generally, any bilinear form over a vector space of finite dimension may be expressed as a matrix productand any inner product may be expressed as Matrix multiplication shares some properties with usual multiplication. However, matrix multiplication is not defined if the number of columns of the first factor differs from the number of rows of the second factor, and it is non-commutative, even when the product remains definite after changing the order of the factors.[9][10]For exampleandThe matrix product is distributive with respect of matrix addition. That is, if A, B, C, D are matrices of respective sizes m × n, n × p,  n × p, and p × q, one has (left distributivity)and (right distributivity)This results from the distributivity for coefficients by If the scalars have the commutative property, then all four matrices are equal. More generally, all four are equal if c belongs to the center of a ring containing the entries of the matrices, because in this case cX = Xc for all matrices X.These properties result from the bilinearity of the product of scalars:If the scalars have the commutative property, the transpose of a product of matrices is the product, in the reverse order, of the transposes of the factors. That is where T denotes the transpose, that is the interchange of rows and columns.This identity does not hold for noncommutative entries, since the order between the entries of A and B is reversed, when one expands the definition of the matrix product.If A and B have complex entries, thenwhere * denotes the entry-wise complex conjugate of a matrix.This results of applying to the definition of matrix product the fact that the conjugate of a sum is the sum of the conjugates of the summands and the conjugate of a product is the product of the conjugates of the factors.Transposition acts on the indices of the entries, while conjugation acts independently on the entries themselves. It results that, if A and B have complex entries, one haswhere † denotes the conjugate transpose (conjugate of the transpose, or equivalently transpose of the conjugate).Given three matrices A, B and C, the products (AB)C and A(BC) are defined if and only if the number of columns of A equals the number of rows of B  and the number of columns of B equals the number of rows of C (in particular, if one of the product is defined, the other is also defined). In this case, one has the associative propertyThis extends naturally to the product of any number of matrix provided that the dimension match. That is, if A1, A2, ..., An are matrices such that the number of columns of Ai equals the number of rows of Ai + 1 for i = 1, ..., n – 1, then the product is defined and does not depend on the order of the multiplications, if the order of the matrices is kept fixed.These properties may be proved by straightforward but complicate summation manipulations. This result also from the fact that matrices represent linear maps. Therefore, the associative property of matrices is simply a specific case of the associative property of function composition.Although the result of a sequence of matrix product does not depend on the order of operation (provided that the order of the matrices is not changed), the computational complexity may depend dramatically on this order.For example, if A, B and C are matrices of respective sizes 10×30, 30×5, 5×60, computing (AB)C needs 10×30×5 + 10×5×60 = 4,500 multiplications, while computing A(BC) needs 30×5×60 + 10×30×60 = 27,000 multiplications.Similarity transformations map product to products, that is In fact, one has If n > 1, many matrices do not have a multiplicative inverse. For example, a matrix such that all entries of a row (or a column) are 0 does not have an inverse. If it exists, the inverse of a matrix A is denoted A−1, and, thus verifiesA matrix that has an inverse is an invertible matrix. Otherwise, it is a singular matrix.A product of matrices is invertible if and only if each factor is invertible. In this case, one hasWhen R is commutative, and, in particular, when it is a field, the determinant of a product is the product of the determinants. As determinants are scalars, and scalars commute, one has thus One may raise a square matrix to any nonnegative integer power multiplying it by itself repeatedly in the same way as for ordinary numbers. That is,Computing the kth power of a matrix needs k – 1 times the time of a single matrix multiplication, if it is done with the trivial algorithm (repeated multiplication). As this may be very time consuming, one generally prefers using exponentiation by squaring, which requires less than 2 log2 k matrix multiplications, and is therefore much more efficient.An easy case for exponentiation is that of a diagonal matrix. Since the product of diagonal matrices amounts to simply multiplying corresponding diagonal elements together, the kth power of a diagonal matrix is obtained by raising the entries to the power k:The starting point of Strassen's proof is using block matrix multiplication. Specifically, a matrix of even dimension 2n×2n may be partitioned in four n×n blocksUnder this form, its inverse is Thus, the inverse of a 2n×2n matrix may be computed with two inversions, six multiplications and four additions or additive inverses of n×n matrices. It follows that, denoting respectively by I(n), M(n) and A(n) = n2 the number of operations needed for multiplying, inverting and adding n×n matrices, one has for some constant d.For matrices whose dimension is not a power of two, the same complexity is reached by increasing the dimension of the matrix to a power of two, by padding the matrix with rows and columns whose entries are 1 on the diagonal and 0 elsewhere.This proves the asserted complexity for matrices such that all submatrices that have to be inverted are indeed invertible. This complexity is thus proved for almost all matrices, as a matrix with randomly chosen entries is invertible with probability one.The same argument applies to LU decomposition, as, if the matrix A is invertible, the equalityThe argument applies also for the determinant, since it results from the block LU decomposition that The term "matrix multiplication" is most commonly reserved for the definition given in this article. It could be more loosely applied to other operations on matrices.
Möbius transformation
In geometry and complex analysis, a Möbius transformation of the complex plane is a rational function of the formof one complex variable z; here the coefficients a, b, c, d are complex numbers satisfying ad − bc ≠ 0.Geometrically, a Möbius transformation can be obtained by first performing stereographic projection from the plane to the unit two-sphere, rotating and moving the sphere to a new location and orientation in space, and then performing stereographic projection (from the new position of the sphere) to the plane.[1]These transformations preserve angles, map every straight line to a line or circle, and map every circle to a line or circle.The Möbius transformations are the projective transformations of the complex projective line. They form a group called the Möbius group, which is the projective linear group PGL(2,C). Together with its subgroups, it has numerous applications in mathematics and physics.Möbius transformations are named in honor of August Ferdinand Möbius; they are also variously named homographies, homographic transformations, linear fractional transformations, bilinear transformations, or fractional linear transformations.The Möbius group is isomorphic to the group of orientation-preserving isometries of hyperbolic 3-space and therefore plays an important role when studying hyperbolic 3-manifolds.In physics, the identity component of the Lorentz group acts on the celestial sphere in the same way that the Möbius group acts on the Riemann sphere. In fact, these two groups are isomorphic. An observer who accelerates to relativistic velocities will see the pattern of constellations as seen near the Earth continuously transform according to infinitesimal Möbius transformations. This observation is often taken as the starting point of twistor theory.Certain subgroups of the Möbius group form the automorphism groups of the other simply-connected Riemann surfaces (the complex plane and the hyperbolic plane). As such, Möbius transformations play an important role in the theory of Riemann surfaces. The fundamental group of every Riemann surface is a discrete subgroup of the Möbius group (see Fuchsian group and Kleinian group).A particularly important discrete subgroup of the Möbius group is the modular group; it is central to the theory of many fractals, modular forms, elliptic curves and Pellian equations.Möbius transformations can be more generally defined in spaces of dimension n>2 as the bijective conformal orientation-preserving maps from the n-sphere to the n-sphere. Such a transformation is the most general form of conformal mapping of a domain. According to Liouville's theorem a Möbius transformation can be expressed as a composition of translations, similarities, orthogonal transformations and inversions.The general form of a Möbius transformation is given bywhere a, b, c, d are any complex numbers satisfying ad − bc ≠ 0. If ad = bc, the rational function defined above is a constant sinceand is thus not considered a Möbius transformation.In case c ≠ 0, this definition is extended to the whole Riemann sphere by definingIf c = 0, we defineThus a Möbius transformation is always a bijective holomorphic function from the Riemann sphere to the Riemann sphere.The fixed points of the transformationare obtained by solving the fixed point equation f(γ) = γ. For c ≠ 0, this has two roots obtained by expanding this equation toand applying the quadratic formula. The roots arewith discriminantParabolic transforms have coincidental fixed points due to zero discriminant. For c nonzero and nonzero discriminant the transform is elliptic or hyperbolic.When c = 0, the quadratic equation degenerates into a linear equation and the transform is linear. This corresponds to the situation that one of the fixed points is the point at infinity. When a ≠ d the second fixed point is finite and is given byIn this case the transformation will be a simple transformation composed of translations, rotations, and dilations:If c = 0 and a = d, then both fixed points are at infinity, and the Möbius transformation corresponds to a pure translation:Topologically, the fact that (non-identity) Möbius transformations fix 2 points (with multiplicity) corresponds to the Euler characteristic of the sphere being 2:Firstly, the projective linear group PGL(2,K) is sharply 3-transitive – for any two ordered triples of distinct points, there is a unique map that takes one triple to the other, just as for Möbius transforms, and by the same algebraic proof (essentially dimension counting, as the group is 3-dimensional). Thus any map that fixes at least 3 points is the identity.Möbius transformations are also sometimes written in terms of their fixed points in so-called normal form. We first treat the non-parabolic case, for which there are two distinct fixed points.Non-parabolic case:Every non-parabolic transformation is conjugate to a dilation/rotation, i.e. a transformation of the form(k ∈ C) with fixed points at 0 and ∞. To see this define a mapwhich sends the points (γ1, γ2) to (0, ∞). Here we assume that γ1 and γ2 are distinct and finite. If one of them is already at infinity then g can be modified so as to fix infinity and send the other point to 0.Solving for f gives (in matrix form):or, if one of the fixed points is at infinity:From the above expressions one can calculate the derivatives of f at the fixed points:Observe that, given an ordering of the fixed points, we can distinguish one of the multipliers (k) of f as the characteristic constant of f. Reversing the order of the fixed points is equivalent to taking the inverse multiplier for the characteristic constant:For loxodromic transformations, whenever |k| > 1, one says that γ1 is the repulsive fixed point, and γ2 is the attractive fixed point. For |k| < 1, the roles are reversed.Parabolic case:In the parabolic case there is only one fixed point γ. The transformation sending that point to ∞ isHere, β is called the translation length. The fixed point formula for a parabolic transformation is thenSolving for f (in matrix form) givesor, if γ = ∞:Note that β is not the characteristic constant of f, which is always 1 for a parabolic transformation. From the above expressions one can calculate:These four points are the vertices of a parallelogram which is sometimes called the characteristic parallelogram of the transformation.which reduces down torepresenting the transform (compare the discussion in the preceding section about the characteristic constant of a transformation). Its characteristic polynomial is equal towhich has rootsA Möbius transformation can be composed as a sequence of simple transformations.The following simple transformations are also Möbius transformations:Let:Then these functions can be composed, givingThis decomposition makes many properties of the Möbius transformation obvious.A Möbius transformation is equivalent to a sequence of simpler transformations.The composition  makes many properties of the Möbius transformation obvious.The existence of the inverse Möbius transformation and its explicit formula are easily derived by the composition of the inverse functions of the simpler transformations. That is, define functions g1, g2, g3, g4 such that each gi is the inverse of fi. Then the compositionFrom this decomposition, we see that Möbius transformations carry over all non-trivial properties of circle inversion. For example, the preservation of angles is reduced to proving that circle inversion preserves angles since the other types of transformations are dilation and isometries (translation, reflection, rotation), which trivially preserve angles.Furthermore, Möbius transformations map generalized circles to generalized circles since circle inversion has this property. A generalized circle is either a circle or a line, the latter being considered as a circle through the point at infinity. Note that a Möbius transformation does not necessarily map circles to circles and lines to lines: it can mix the two. Even if it maps a circle to another circle, it does not necessarily map the first circle's center to the second circle's center.The cross ratio of four different points is real if and only if there is a line or a circle passing through them. This is another way to show that Möbius transformations preserve generalized circles.Two points z1 and z2 are conjugate with respect to a generalized circle C, if, given a generalized circle D passing through z1 and z2 and cutting C in two points a and b, (z1, z2; a, b) are in harmonic cross-ratio (i.e. their cross ratio is −1). This property does not depend on the choice of the circle D. This property is also sometimes referred to as being symmetric with respect to a line or circle.[2][3]Two points z, z∗ are conjugate with respect to a line, if they are symmetric with respect to the line. Two points are conjugate with respect to a circle if they are exchanged by the inversion with respect to this circle.The point z∗ conjugate to z when L is the line determined by the vector based eiθ at the point z0 can be explicitly given asThe point z∗ conjugate to z when C is the circle of radius r centered z0 can be explicitly given asSince Möbius transformations preserve generalized circles and cross-ratios, they preserve also the conjugation.With every invertible complex 2-by-2 matrixwe can associate the Möbius transformationThe condition ad − bc ≠ 0 is equivalent to the condition that the determinant of above matrix be nonzero, i.e. that the matrix be invertible.It is straightforward to check that then the product of two matrices will be associated with the composition of the two corresponding Möbius transformations. In other words, the mapThe same identification of PGL(2,K) with the group of fractional linear transformations and with the group of projective linear automorphisms of the projective line holds over any field K, a fact of algebraic interest, particularly for finite fields, though the case of the complex numbers has the greatest geometric interest.The natural action of PGL(2,C) on the complex projective line CP1 is exactly the natural action of the Möbius group on the Riemann sphere, where the projective line CP1 and the Riemann sphere are identified as follows:Here [z1:z2] are homogeneous coordinates on CP1; the point [1:0] corresponds to the point ∞ of the Riemann sphere.By using homogeneous coordinates, many concrete calculations involving Möbius transformations can be simplified, since no case distinctions dealing with ∞ are required.From this we see that the Möbius group is a 3-dimensional complex Lie group (or a 6-dimensional real Lie group). It is a semisimple non-compact Lie group.Note that there are precisely two matrices with unit determinant which can be used to represent any given Möbius transformation. That is, SL(2,C) is a double cover of PSL(2,C). Since SL(2,C) is simply-connected it is the universal cover of the Möbius group. Therefore, the fundamental group of the Möbius group is Z2.Given a set of three distinct points z1, z2, z3 on the Riemann sphere and a second set of distinct points w1, w2, w3, there exists precisely one Möbius transformation f(z)  with f(zi) = wi for i = 1,2,3. (In other words: the action of the Möbius group on the Riemann sphere is sharply 3-transitive.) There are several ways to determine f(z) from the given sets of points.It is easy to check that the Möbius transformationwith matrixThe stabilizer of {0, 1, ∞} (as an unordered set) is a subgroup known as the anharmonic group.The equationis equivalent to the equation of a standard hyperbolaby means of a Laplace expansion along the first row. This results in the determinant formulaeIf we require the coefficients a, b, c, d of a Möbius transformation to be real numbers with ad − bc = 1, we obtain a subgroup of theMöbius group denoted as PSL(2,R). This is the group of those Möbius transformations that map the upper half-plane H = x + iy : y > 0 to itself, and is equal to the group of all biholomorphic (or equivalently: bijective, conformal and orientation-preserving) maps H → H. If a proper metric is introduced, the upper half-plane becomes a model of the hyperbolic plane H 2, the Poincaré half-plane model, and PSL(2,R) is the group of all orientation-preserving isometries of H 2 in this model.The subgroup of all Möbius transformations that map the open disk D = z : |z| < 1 to itself consists of all transformations of the formSince both of the above subgroups serve as isometry groups of H 2, they are isomorphic. A concrete isomorphism is given by conjugation with the transformationwhich bijectively maps the open unit disk to the upper half plane.Alternatively, consider an open disk with radius r, centered at ri. The Poincaré disk model in this disk becomes identical to the upper-half-plane model as r approaches ∞.Icosahedral groups of Möbius transformations were used by Felix Klein to give an analytic solution to the quintic equation in (Klein 1888); a modern exposition is given in.[5]If we require the coefficients a, b, c, d of a Möbius transformation to be integers with ad − bc = 1, we obtain the modular group PSL(2,Z), a discrete subgroup of PSL(2,R) important in the study of lattices in the complex plane, elliptic functions and elliptic curves. The discrete subgroups of PSL(2,R) are known as Fuchsian groups; they are important in the study of Riemann surfaces.Non-identity Möbius transformations are commonly classified into four types, parabolic, elliptic, hyperbolic and loxodromic, with the hyperbolic ones being a subclass of the loxodromic ones. The classification has both algebraic and geometric significance. Geometrically, the different types result in different transformations of the complex plane, as the figures below illustrate.which describes a translation in the complex plane.this is an example of the unipotent radical of a Borel subgroup (of the Möbius group, or of SL(2,C) for the matrix group; the notion is defined for any reductive Lie group).All non-parabolic transformations have two fixed points and are defined by a matrix conjugate towith the complex number λ not equal to 0, 1 or −1, corresponding to a dilation/rotation through multiplication by the complex number k = λ2, called the characteristic constant or multiplier of the transformation.with α real.A transform is hyperbolic if and only if λ is real and positive.Historically, navigation by loxodrome or rhumb line refers to a path of constant bearing; the resulting path is a logarithmic spiral, similar in shape to the transformations of the complex plane that a loxodromic Möbius transformation makes. See the geometric figures below.Over the real numbers (if the coefficients must be real), there are no non-hyperbolic loxodromic transformations, and the classification is into elliptic, parabolic, and hyperbolic, as for real conics. The terminology is due to considering half the absolute value of the trace, |tr|/2, as the eccentricity of the transformation – division by 2 corrects for the dimension, so the identity has eccentricity 1 (tr/n is sometimes used as an alternative for the trace for this reason), and absolute value corrects for the trace only being defined up to a factor of ±1 due to working in PSL. Alternatively one may use half the trace squared as a proxy for the eccentricity squared, as was done above; these classifications (but not the exact eccentricity values, since squaring and absolute values are different) agree for real traces but not complex traces. The same terminology is used for the classification of elements of SL(2, R) (the 2-fold cover), and analogous classifications are used elsewhere. Loxodromic transformations are an essentially complex phenomenon, and correspond to complex eccentricities.The following picture depicts (after stereographic transformation from the sphere to the plane) the two fixed points of a Möbius transformation in the non-parabolic case:The characteristic constant can be expressed in terms of its logarithm:When expressed in this way, the real number ρ becomes an expansion factor. It indicates how repulsive the fixed point γ1 is, and how attractive γ2 is. The real number α is a rotation factor, indicating to what extent the transform rotates the plane anti-clockwise about γ1 and clockwise about γ2.If ρ = 0, then the fixed points are neither attractive nor repulsive but indifferent, and the transformation is said to be elliptic. These transformations tend to move all points in circles around the two fixed points. If one of the fixed points is at infinity, this is equivalent to doing an affine rotation around a point.If we take the one-parameter subgroup generated by any elliptic Möbius transformation, we obtain a continuous transformation, such that every transformation in the subgroup fixes the same two points. All other points flow along a family of circles which is nested between the two fixed points on the Riemann sphere. In general, the two fixed points can be any two distinct points.This has an important physical interpretation.Imagine that some observer rotates with constant angular velocity about some axis. Then we can take the two fixed points to be the North and South poles of the celestial sphere. The appearance of the night sky is now transformed continuously in exactly the manner described by the one-parameter subgroup of elliptic transformations sharing the fixed points 0, ∞, and with the number α corresponding to the constant angular velocity of our observer.Here are some figures illustrating the effect of an elliptic Möbius transformation on the Riemann sphere (after stereographic projection to the plane):These pictures illustrate the effect of a single Möbius transformation. The one-parameter subgroup which it generates continuously moves points along the family of circular arcs suggested by the pictures.If α is zero (or a multiple of 2π), then the transformation is said to be hyperbolic. These transformations tend to move points along circular paths from one fixed point toward the other.If we take the one-parameter subgroup generated by any hyperbolic Möbius transformation, we obtain a continuous transformation, such that every transformation in the subgroup fixes the same two points. All other points flow along a certain family of circular arcs away from the first fixed point and toward the second fixed point. In general, the two fixed points may be any two distinct points on the Riemann sphere.This too has an important physical interpretation. Imagine that an observer accelerates (with constant magnitude of acceleration) in the direction of the North pole on his celestial sphere. Then the appearance of the night sky is transformed in exactly the manner described by the one-parameter subgroup of hyperbolic transformations sharing the fixed points 0, ∞, with the real number ρ corresponding to the magnitude of his acceleration vector. The stars seem to move along longitudes, away from the South pole toward the North pole. (The longitudes appear as circular arcs under stereographic projection from the sphere to the plane.)Here are some figures illustrating the effect of a hyperbolic Möbius transformation on the Riemann sphere (after stereographic projection to the plane):These pictures resemble the field lines of a positive and a negative electrical charge located at the fixed points, because the circular flow lines subtend a constant angle between the two fixed points.If both ρ and α are nonzero, then the transformation is said to be loxodromic. These transformations tend to move all points in S-shaped paths from one fixed point to the other.The word "loxodrome" is from the Greek: "λοξος (loxos), slanting + δρόμος (dromos), course". When sailing on a constant bearing – if you maintain a heading of (say) north-east, you will eventually wind up sailing around the north pole in a logarithmic spiral. On the mercator projection such a course is a straight line, as the north and south poles project to infinity. The angle that the loxodrome subtends relative to the lines of longitude (i.e. its slope, the "tightness" of the spiral) is the argument of k. Of course, Möbius transformations may have their two fixed points anywhere, not just at the north and south poles. But any loxodromic transformation will be conjugate to a transform that moves all points along such loxodromes.If we take the one-parameter subgroup generated by any loxodromic Möbius transformation, we obtain a continuous transformation, such that every transformation in the subgroup fixes the same two points. All other points flow along a certain family of curves, away from the first fixed point and toward the second fixed point. Unlike the hyperbolic case, these curves are not circular arcs, but certain curves which under stereographic projection from the sphere to the plane appear as spiral curves which twist counterclockwise infinitely often around one fixed point and twist clockwise infinitely often around the other fixed point. In general, the two fixed points may be any two distinct points on the Riemann sphere.You can probably guess the physical interpretation in the case when the two fixed points are 0, ∞: an observer who is both rotating (with constant angular velocity) about some axis and moving along the same axis, will see the appearance of the night sky transform according to the one-parameter subgroup of loxodromic transformations with fixed points 0, ∞, and with ρ, α determined respectively by the magnitude of the actual linear and angular velocities.These images show Möbius transformations stereographically projected onto the Riemann sphere. Note in particular that when projected onto a sphere, the special case of a fixed point at infinity looks no different from having the fixed points in an arbitrary location.This can be used to iterate a transformation, or to animate one by breaking it up into steps.These images show three points (red, blue and black) continuously iterated under transformations with various characteristic constants.And these images demonstrate what happens when you transform a circle under Hyperbolic, Elliptical, and Loxodromic transforms. Note that in the elliptical and loxodromic images, the α value is 1/10 .The pointThe inverse poleis that point to which the point at infinity is transformed.The point midway between the two poles is always the same as the point midway between the two fixed points:These four points are the vertices of a parallelogram which is sometimes called the characteristic parallelogram of the transformation.which reduces down torepresenting the transform (compare the discussion in the preceding section about the characteristic constant of a transformation). Its characteristic polynomial is equal towhich has rootsThe orientation-preserving Möbius transformations form the connected component of the identity in the Möbius group.  In dimension n = 2, the orientation-preserving Möbius transformations are exactly the maps of the Riemann sphere covered here.  The orientation-reversing ones are obtained from these by complex conjugation.[8]An isomorphism of the Möbius group with the Lorentz group was noted by several authors: Based on previous work of Felix Klein (1893, 1897)[10] on automorphic functions related to hyperbolic geometry and Möbius geometry, Gustav Herglotz (1909)[11] showed that hyperbolic motions (i.e. isometric automorphisms of a hyperbolic space) transforming the unit sphere into itself correspond to Lorentz transformations, by which Herglotz was able to classify the one-parameter Lorentz transformations into loxodromic, elliptic, hyperbolic, and parabolic groups. Other authors include Emil Artin (1957),[12] H. S. M. Coxeter (1965),[13] and Roger Penrose and Wolfgang Rindler (1984).[14]Minkowski space consists of the four-dimensional real coordinate space R4 consisting of the space of ordered quadruples (x0,x1,x2,x3) of real numbers, together with a quadratic formBorrowing terminology from special relativity, points with Q > 0 are considered timelike; in addition, if x0 > 0, then the point is called future-pointing. Points with Q < 0 are called spacelike. The null cone S consists of those points where Q = 0; the future null cone N+ are those points on the null cone with x0 > 0. The celestial sphere is then identified with the collection of rays in N+ whose initial point is the origin of R4. The collection of linear transformations on R4 with positive determinant preserving the quadratic form Q and preserving the time direction form the restricted Lorentz group SO+(1,3).In connection with the geometry of the celestial sphere, the group of transformations SO+(1,3) is identified with the group PSL(2,C) of Möbius transformations of the sphere. To each (x0,x1,x2,x3) ∈ R4, associate the hermitian matrixThe determinant of the matrix X is equal to Q(x0,x1,x2,x3). The special linear group acts on the space of such matrices via    (1)for each A ∈ SL(2,C), and this action of SL(2,C) preserves the determinant of X because det A = 1. Since the determinant of X is identified with the quadratic form Q, SL(2,C) acts by Lorentz transformations. On dimensional grounds, SL(2,C) covers a neighborhood of the identity of SO(1,3). Since SL(2,C) is connected, it covers the entire restricted Lorentz group SO+(1,3). Furthermore, since the kernel of the action (1) is the subgroup {±I}, then passing to the quotient group gives the group isomorphism    (2)Focusing now attention on the case when (x0,x1,x2,x3) is null, the matrix X has zero determinant, and therefore splits as the outer product of a complex two-vector ξ with its complex conjugate:    (3)The two-component vector ξ is acted upon by SL(2,C) in a manner compatible with (1). It is now clear that the kernel of the representation of SL(2,C) on hermitian matrices is {±I}.The action of PSL(2,C) on the celestial sphere may also be described geometrically using stereographic projection. Consider first the hyperplane in R4 given by x0 = 1. The celestial sphere may be identified with the sphere S+ of intersection of the hyperplane with the future null cone N+. The stereographic projection from the north pole (1,0,0,1) of this sphere onto the plane x3 = 0 takes a point with coordinates (1,x1,x2,x3) withto the pointIntroducing the complex coordinatethe inverse stereographic projection gives the following formula for a point (x1, x2, x3) on S+:    (4)The action of SO+(1,3) on the points of N+ does not preserve the hyperplane S+, but acting on points in S+ and then rescaling so that the result is again in S+ gives an action of SO+(1,3) on the sphere which goes over to an action on the complex variable ζ. In fact, this action is by fractional linear transformations, although this is not easily seen from this representation of the celestial sphere. Conversely, for any fractional linear transformation of ζ variable goes over to a unique Lorentz transformation on N+, possibly after a suitable (uniquely determined) rescaling.A more invariant description of the stereographic projection which allows the action to be more clearly seen is to consider the variable ζ = z:w as a ratio of a pair of homogeneous coordinates for the complex projective line CP1. The stereographic projection goes over to a transformation from C2 − {0} to N+ which is homogeneous of degree two with respect to real scalings    (5)In summary, the action of the restricted Lorentz group SO+(1,3) agrees with that of the Möbius group PSL(2,C). This motivates the following definition. In dimension n ≥ 2, the Möbius group Möb(n) is the group of all orientation-preserving conformal isometries of the round sphere Sn to itself. By realizing the conformal sphere as the space of future-pointing rays of the null cone in the Minkowski space R1,n+1, there is an isomorphism of Möb(n) with the restricted Lorentz group SO+(1,n+1) of Lorentz transformations with positive determinant, preserving the direction of time.He identified the Lorentz group with transformations for which {x : Q(x) = -1} is stable. Then he interpreted the x's as homogeneous coordinates  and {x : Q(x) = 0}, the null cone, as the Cayley absolute for a hyperbolic space of points {x : Q(x) < 0}. Next, Coxeter introduced the variablesAs seen above, the Möbius group PSL(2,C) acts on Minkowski space as the group of those isometries that preserve the origin, the orientation of space and the direction of time. Restricting to the points where Q=1 in the positive light cone, which form a model of hyperbolic 3-space H 3, we see that the Möbius group acts on H 3 as a group of orientation-preserving isometries. In fact, the Möbius group is equal to the group of orientation-preserving isometries of hyperbolic 3-space.If we use the Poincaré ball model, identifying the unit ball in R3 with H 3, then we can think of the Riemann sphere as the "conformal boundary" of H 3. Every orientation-preserving isometry of H 3 gives rise to a Möbius transformation on the Riemann sphere and vice versa; this is the very first observation leading to the AdS/CFT correspondence conjectures in physics.SpecificGeneral
Bra–ket notation
In quantum mechanics, bra–ket notation is a standard notation for describing quantum states. It can also be used to denote abstract vectors and linear functionals in mathematics. The notation uses angle brackets (the ⟨ and ⟩ symbols) and a vertical bar (the | symbol), to denote the scalar product of vectors or the action of a linear functional on a vector in a complex vector space.  The scalar product or action is written asThe right part is called the ket /kɛt/; it is a vector, typically represented as a column vector and written The left part is called the bra, /brɑː/; it is the Hermitian conjugate of the ket with the same label, typically represented as a row vector and is writtenA combination of bras, kets, and operators is interpreted using matrix multiplication. A bra and a ket with the same label are Hermitian conjugates of each other.Bra-ket notation was introduced in 1939 by Paul Dirac[1][2] and is also known as the Dirac notation.  Bra–ket notation is a notation for linear algebra, particularly focused on vectors, inner products, linear operators, Hermitian conjugation, and the dual space, for both finite-dimensional and infinite-dimensional complex vector spaces. It is specifically designed to ease the types of calculations that frequently come up in quantum mechanics.Its use in quantum mechanics is quite widespread. Many phenomena that are explained using quantum mechanics are usually explained using bra–ket notation.In simple cases, a ket |m⟩ can be described as a column vector, a bra with the same label ⟨m| is its conjugate transpose (which is a row vector), and writing bras, kets, and linear operators next to each other implies matrix multiplication.[4] However, kets may also exist in uncountably-infinite-dimensional vector spaces, such that they cannot be literally written as a column vector. Also, writing a column vector as a list of numbers requires picking a basis, whereas one can write "|m⟩" without committing to any particular basis. This is helpful because quantum mechanics calculations involve frequently switching between different bases (e.g. position basis, momentum basis, energy eigenbasis, etc.), so it is better to have the basis vectors (if any) written out explicitly.  In some situations involving two important basis vectors they will be referred to simply as "|-⟩" and "|+⟩".The standard mathematical notation for the inner product, preferred as well by some physicists, expresses exactly the same thing as the bra–ket notation,Bras and kets can also be configured in other ways, such as the outer productwhich can also be represented as a matrix multiplication (i.e., a column vector times a row vector equals a matrix).If the ket is an element of a vector space, the bra is technically an element of its dual space—see Riesz representation theorem.In mathematics, the term "vector" is used to refer generally to any element of any vector space. In physics, however, the term "vector" is much more specific: "Vector" refers almost exclusively to quantities like displacement or velocity, which have three components that relate directly to the three dimensions of the real world. Such vectors are typically denoted with over arrows (r→) or boldface (r).In quantum mechanics, a quantum state is typically represented as an element of an abstract complex vector space—for example the infinite-dimensional vector space of all possible wavefunctions (functions mapping each point of 3D space to a complex number). Since the term "vector" is already used for something else (see previous paragraph), it is very common to refer to these elements of abstract complex vector spaces as "kets", and to write them using ket notation.Ket notation, invented by Dirac, uses vertical bars and angular brackets: |A⟩. When this notation is used, these quantities are called "kets", and |A⟩ is read as "ket-A".[5] These kets can be manipulated using the usual rules of linear algebra, for example:Note how any symbols, letters, numbers, or even words—whatever serves as a convenient label—can be used as the label inside a ket. For example, the last line above involves infinitely many different kets, one for each real number x. In other words, the symbol "|A⟩" has a specific and universal mathematical meaning, while just the "A" by itself does not. For example, |1⟩ + |2⟩ might or might not be equal to |3⟩. Nevertheless, for convenience, there is usually some logical scheme behind the labels inside kets, such as the common practice of labeling energy eigenkets in quantum mechanics through a listing of their quantum numbers.An inner product is a generalization of the dot product. The inner product of two vectors is a scalar. In neutral notation (notation dedicated to the inner product only), this might be written (A, B), where A and B are elements of the abstract vector space, i.e. both are kets.Bra–ket notation uses a specific notation for inner products:Bra–ket notation splits this inner product (also called a "bracket") into two pieces, the "bra" and the "ket":where ⟨A| is called a bra, read as "bra-A", and |B⟩ is a ket as above.The purpose of "splitting" the inner product into a bra and a ket is that both the bra ⟨A| and the ket |B⟩ are meaningful on their own, and can be used in other contexts besides within an inner product. There are two main ways to think about the meanings of separate bras and kets. Accordingly, the interpretation of the expression ⟨A|B⟩ has a second interpretation, namely that of the action of a linear functional per below.For a finite-dimensional vector space, using a fixed orthonormal basis, the inner product can be written as a matrix multiplication of a row vector with a column vector:Based on this, the bras and kets can be defined as:and then it is understood that a bra next to a ket implies matrix multiplication.The conjugate transpose (also called Hermitian conjugate) of a bra is the corresponding ket and vice versa:because if one starts with the brathen performs a complex conjugation, and then a matrix transpose, one ends up with the ketA more abstract definition, which is equivalent but more easily generalized to infinite-dimensional spaces, is to say that bras are linear functionals on the space of kets, i.e. linear transformations that input a ket and output a complex number. The bra linear functionals are defined to be consistent with the inner product. Thus, if ⟨A| is the linear functional corresponding to |A⟩ under the Riesz representation theorem, theni.e. it produces the same complex number as the inner product does. The terminology for the right hand side is though not inner product, which always involves two kets. Confusing this is harmless, since the same number is produced in the end.In mathematics terminology, the vector space of bras is the dual space to the vector space of kets, and corresponding bras and kets are related by the Riesz representation theorem.Bra–ket notation can be used even if the vector space is not a Hilbert space.In quantum mechanics, it is common practice to write down kets which have infinite norm, i.e. non-normalizable wavefunctions. Examples include states whose wavefunctions are Dirac delta functions or infinite plane waves. These do not, technically, belong to the Hilbert space itself. However, the definition of "Hilbert space" can be broadened to accommodate these states (see the Gelfand–Naimark–Segal construction or rigged Hilbert spaces). The bra–ket notation continues to work in an analogous way in this broader context.Banach spaces are a different generalization of Hilbert spaces. In a Banach space B, the vectors may be notated by kets and the continuous linear functionals by bras. Over any vector space without topology, we may also notate the vectors by kets and the linear functionals by bras. In these more general contexts, the bracket does not have the meaning of an inner product, because the Riesz representation theorem does not apply.The mathematical structure of quantum mechanics is based in large part on linear algebra:Since virtually every calculation in quantum mechanics involves vectors and linear operators, it can involve, and often does involve, bra–ket notation. A few examples follow:Starting from any ket |Ψ⟩ in this Hilbert space,  one may define a complex scalar function of r, known as a wavefunction,On the left-hand side, Ψ(r) is a function mapping any point in space to a complex number; on the right-hand side, |Ψ⟩ = ∫ d3r Ψ(r) |r⟩ is a ket consisting of a superposition of kets with relative coefficients specified by that function.It is then customary to define linear operators acting on wavefunctions in terms of linear operators acting on kets, byFor instance, the momentum operator p has the following form,One occasionally encounters an expression such asthough this is something of an abuse of notation. The differential operator must be understood to be an abstract operator, acting on kets, that has the effect of differentiating wavefunctions once the expression is projected into the position basis,even though, in the momentum basis, the operator amounts to a mere multiplication operator (by iħp).In quantum mechanics the expression ⟨φ|ψ⟩ is typically interpreted as the probability amplitude for the state ψ to collapse into the state φ. Mathematically, this means the coefficient for the projection of ψ onto φ.  It is also described as the projection of state ψ onto state φ.A stationary spin-1/2 particle has a two-dimensional Hilbert space. One orthonormal basis is:where |↑z⟩ is the state with a definite value of the spin operator Sz equal to +1/2 and  |↓z⟩ is the state with a definite value of the spin operator Sz equal to −1/2.Since these are a basis, any quantum state of the particle can be expressed as a linear combination (i.e., quantum superposition) of these two states:where aψ and bψ are complex numbers.A different basis for the same Hilbert space is:defined in terms of Sx rather than Sz.Again, any state of the particle can be expressed as a linear combination of these two:In vector form, you might writedepending on which basis you are using. In other words, the "coordinates" of a vector depend on the basis used.There is a mathematical relationship between aψ, bψ, cψ and dψ; see change of basis.There are a few conventions and abuses of notation that are generally accepted by the physics community, but which might confuse the non-initiated.It is common to use the same symbol for labels and constants in the same equation. For example, α̂ |α⟩ = α |α⟩, where the symbol α is used simultaneously as the name of the operator α̂, its eigenvector |α⟩ and the associated eigenvalue α.Something similar occurs in component notation of vectors. While Ψ (uppercase) is traditionally associated with wavefunctions, ψ (lowercase) may be used to denote a label, a wave function or complex constant in the same context, usually differentiated only by a subscript.The main abuses are including operations inside the vector labels. This is done for a fast notation of scaling vectors. E.g. if the vector |α⟩ is scaled by √2, it might be denoted by |α/√2⟩, which makes no sense since α is a label, not a function or a number, so you can't perform operations on it.This is especially common when denoting vectors as tensor products, where part of the labels are moved outside the designed slot, e.g. |α⟩ = |α/√21⟩ ⊗ |α/√22⟩. Here part of the labeling that should state that all three vectors are different was moved outside the kets, as subscripts 1 and 2. And a further abuse occurs, since α is meant to refer to the norm of the first vector—which is a label denoting a value.A linear operator is a map that inputs a ket and outputs a ket. (In order to be called "linear", it is required to have certain properties.) In other words, if A is a linear operator and |ψ⟩ is a ket, then A|ψ⟩ is another ket.In an N-dimensional Hilbert space, |ψ⟩ can be written as an N × 1 column vector, and then A is an N × N matrix with complex entries. The ket A|ψ⟩ can be computed by normal matrix multiplication.Linear operators are ubiquitous in the theory of quantum mechanics. For example, observable physical quantities are represented by self-adjoint operators, such as energy or momentum, whereas transformative processes are represented by unitary linear operators such as rotation or the progression of time.Operators can also be viewed as acting on bras from the right hand side. Specifically, if A is a linear operator and ⟨φ| is a bra, then ⟨φ|A is another bra defined by the rule(in other words, a function composition). This expression is commonly written as (cf. energy inner product)In an N-dimensional Hilbert space, ⟨φ| can be written as a 1 × N row vector, and A (as in the previous section) is an N × N matrix. Then the bra ⟨φ|A can be computed by normal matrix multiplication.If the same state vector appears on both bra and ket side,then this expression gives the expectation value, or mean or average value, of the observable represented by operator A for the physical system in the state |ψ⟩.A convenient way to define linear operators on a Hilbert space H is given by the outer product: if ⟨ϕ| is a bra and |ψ⟩ is a ket, the outer productdenotes the rank-one operator with the rule For a finite-dimensional vector space, the outer product can be understood as simple matrix multiplication:The outer product is an N × N matrix, as expected for a linear operator.One of the uses of the outer product is to construct projection operators. Given a ket |ψ⟩ of norm 1, the orthogonal projection onto the subspace spanned by |ψ⟩ isJust as kets and bras can be transformed into each other (making |ψ⟩ into ⟨ψ|), the element from the dual space corresponding to A|ψ⟩ is ⟨ψ|A†,  where A† denotes the Hermitian conjugate (or adjoint) of the operator A. In other words,If A is expressed as an N × N matrix, then A† is its conjugate transpose.Self-adjoint operators, where A = A†, play an important role in quantum mechanics; for example, an observable is always described by a self-adjoint operator. If A is a self-adjoint operator, then ⟨ψ|A|ψ⟩ is always a real number (not complex). This implies that expectation values of observables are real.Bra–ket notation was designed to facilitate the formal manipulation of linear-algebraic expressions. Some of the properties that allow this manipulation are listed herein. In what follows, c1 and c2 denote arbitrary complex numbers, c* denotes the complex conjugate of c, A and B denote arbitrary linear operators, and these properties are to hold for any choice of bras and kets.Given any expression involving complex numbers, bras, kets, inner products, outer products, and/or linear operators (but not addition), written in bra–ket notation, the parenthetical groupings do not matter (i.e., the associative property holds). For example:and so forth. The expressions on the right (with no parentheses whatsoever) are allowed to be written unambiguously because of the equalities on the left. Note that the associative property does not hold for expressions that include nonlinear operators, such as the antilinear time reversal operator in physics.Bra–ket notation makes it particularly easy to compute the Hermitian conjugate (also called dagger, and denoted †) of expressions. The formal rules are:These rules are sufficient to formally write the Hermitian conjugate of any such expression; some examples are as follows:Two Hilbert spaces V and W may form a third space V ⊗ W by a tensor product. In quantum mechanics, this is used for describing composite systems. If a system is composed of two subsystems described in V and W respectively, then the Hilbert space of the entire system is the tensor product of the two spaces. (The exception to this is if the subsystems are actually identical particles. In that case, the situation is a little more complicated.)If |ψ⟩ is a ket in V and |φ⟩ is a ket in W, the direct product of the two kets is a ket in V ⊗ W. This is written in various notations:See quantum entanglement and the EPR paradox for applications of this product.Consider a complete orthonormal system (basis),for a Hilbert space H, with respect to the norm from an inner product ⟨·,·⟩. From basic functional analysis, it is  known that any ket |ψ⟩ can also be written aswith ⟨·|·⟩ the inner product on the Hilbert space.From the commutativity of kets with (complex) scalars, it follows thatmust be the identity operator, which sends each vector to itself. This, then,  can be inserted in any expression without affecting its value; for examplewhere, in the last identity, the Einstein summation convention has been used.In quantum mechanics, it often occurs that little or no information about the inner product ⟨ψ|φ⟩ of two arbitrary (state) kets is present, while it is still possible to say something about the expansion coefficients ⟨ψ|ei⟩ = ⟨ei|ψ⟩* and ⟨ei|φ⟩ of those vectors with respect to a specific (orthonormalized) basis. In this case, it is particularly useful to insert the unit operator into the bracket one time or more.For more information, see Resolution of the identity, Since ⟨x′|x⟩ = δ(x − x′), plane waves follow, ⟨x|p⟩ = eixp/ħ/√2πħ.[7]Typically, when all matrix elements of an operator such as are available,this resolution serves to reconstitute the full operator,The object physicists are considering when using bra–ket notation is a Hilbert space (a complete inner product space).Let H be a Hilbert space and h ∈ H a vector in H. What physicists would denote by |h⟩ is the vector itself. That is,Let H* be the dual space of H. This is the space of linear functionals on H. The isomorphism Φ : H → H* is defined by Φ(h) = φh, where for every g ∈ H we definewhere IP(·,·), (·,·), ⟨·,·⟩ and ⟨·|·⟩ are just different notations for expressing an inner product between two elements in a Hilbert space (or for the first three, in any inner product space). Notational confusion arises when identifying φh and g with ⟨h| and |g⟩ respectively. This is because of literal symbolic substitutions. Let φh = H = ⟨h| and let g = G = |g⟩. This givesOne ignores the parentheses and removes the double bars. Some properties of this notation are convenient since we are dealing with linear operators and composition acts like a ring multiplication.Moreover, mathematicians usually write the dual entity not at the first place, as the physicists do, but at the second one, and they usually use not an asterisk but an overline (which the physicists reserve for averages and the Dirac spinor adjoint) to denote complex conjugate numbers; i.e., for scalar products mathematicians usually writewhereas physicists would write for the same quantity
Field (mathematics)
In mathematics, a field  is a set on which addition, subtraction, multiplication, and division are defined, and behave as the corresponding operations on  rational and real numbers do. A field is thus a fundamental algebraic structure, which is widely used in algebra, number theory and many other areas of mathematics.The best known fields are the field of rational numbers, the field of real numbers and the field of complex numbers. Many other fields, such as fields of rational functions, algebraic function fields, algebraic number fields, and p-adic fields are commonly used and studied in mathematics, particularly in number theory and algebraic geometry. Most cryptographic protocols rely on finite fields, i.e., fields with finitely many elements.The relation of two fields is expressed by the notion of a field extension. Galois theory, initiated by Évariste Galois in the 1830s, is devoted to understanding the symmetries of field extensions. Among other results, this theory shows that angle trisection and squaring the circle can not be done with a compass and straightedge. Moreover, it shows that quintic equations are algebraically unsolvable.Fields serve as foundational notions in several mathematical domains. This includes different branches of analysis, which are based on fields with additional structure. Basic theorems in analysis hinge on the structural properties of the field of real numbers. Most importantly for algebraic purposes, any field may be used as the scalars for a vector space, which is the standard general context for linear algebra. Number fields, the siblings of the field of rational numbers, are studied in depth in number theory. Function fields can help describe properties of geometric objects.Informally, a field is a set, along with two operations defined on that set: an addition operation written as a + b, and a multiplication operation written as a ⋅ b, both of which behave similarly as they behave for rational numbers and real numbers, including the existence of an additive inverse −a for all elements a, and of a multiplicative inverse b−1 for every nonzero element b. This allows us to consider also the so-called inverse operations of subtraction a − b, and division a / b, via defining:Formally, a field is a set F together with two operations called addition and multiplication.[1] An operation is a mapping that associates an element of the set to every pair of its elements. The result of the addition of a and b is called the sum of a and b and denoted a + b. Similarly, the result of the multiplication of a and b is called the product of a and b, and denoted ab or a⋅b. These operations are required to satisfy the following properties, referred to as field axioms. In these axioms,  a, b and c are arbitrary elements of the field F.This may be summarized by saying: a field has two operations, called addition and multiplication; it is an abelian group under the addition, with 0 as additive identity; the nonzero elements are an abelian group under the multiplication, with 1 as multiplicative identity; the multiplication is distributive over the addition.Fields can also be defined in different, but equivalent ways. One can alternatively define a field by four binary operations (add, subtract, multiply, divide), and their required properties. Division by zero is, by definition, excluded.[2] In order to avoid existential quantifiers, fields can be defined by two binary operations (addition and multiplication), two unary operations (yielding the additive and multiplicative inverses, respectively), and two nullary operations (the constants 0 and 1). These operations are then subject to the conditions above. Avoiding existential quantifiers is important in constructive mathematics and computing.[3] One may equivalently define a field by the same two binary operations, one unary operation, (the multiplicative inverse} and two constants 1 and –1, since 0 = 1 + (–1) and –a = (–1) a.Rational numbers have been widely used a long time before the elaboration of the concept of field.They are numbers that can be written as fractionsa/b, where a and b are integers, and b ≠ 0. The additive inverse of such a fraction is −a/b, and the multiplicative inverse (provided that a ≠ 0) is b/a, which can be seen as follows:The abstractly required field axioms reduce to standard properties of rational numbers. For example, the law of distributivity can be proven as follows:[4]The real numbers R, with the usual operations of addition and multiplication, also form a field. The complex numbers C consist of expressionswhere i is the imaginary unit, i.e., a (non-real) number satisfying i2 = −1.Addition and multiplication of real numbers are defined in such a way that expressions of this type satisfy all field axioms and thus hold for C. For example, the distributive law enforcesIt is immediate that this is again an expression of the above type, and so the complex numbers form a field. Complex numbers can be geometrically represented as points in the plane, with Cartesian coordinates given by the real numbers of their describing expression, or as the arrows from the origin to these points, specified by their length and an angle enclosed with some distinct direction. Addition then corresponds to combining the arrows to the intuitive parallelogram (adding the Cartesian coordinates), and the multiplication is –less intuitively– combining rotating and scaling of the arrows (adding the angles and multiplying the lengths). The fields of real and complex numbers are used throughout mathematics, physics, engineering, statistics, and many other scientific disciplines.In addition to familiar number systems such as the rationals, there are other, less immediate examples of fields. The following example is a field consisting of four elements called O, I, A, and B. The notation is chosen such that O plays the role of the additive identity element (denoted 0 in the axioms above), and I is the multiplicative identity (denoted 1 in the axioms above). The field axioms can be verified by using some more field theory, or by direct computation. For example,This field is called a finite field with four elements, and is denoted F4 or GF(4).[6] The subset consisting of O and I (highlighted in red in the tables at the right) is also a field, known as the binary field F2 or GF(2). In the context of computer science and Boolean algebra, O and I are often denoted respectively by false and true, the addition is then denoted XOR (exclusive or), and the multiplication is denoted AND. In other words, the structure of the binary field is the basic structure that allows computing with bits.In this section, F denotes an arbitrary field and a and b are arbitrary elements of F.One has a · 0 = 0 and −a = (−1) · a.[7] In particular, one may deduce the additive inverse of every element as soon as one knows –1.If ab = 0 then a or b must be 0. Indeed, if a ≠ 0, then 0 = a–1⋅0 = a–1(ab) = (a–1a)b = b. This means that every field is an integral domain.The axioms of a field F imply that it is an abelian group under addition. This group is called the additive group of the field, and is sometimes denoted by (F, +) when denoting it simply as F could be confusing.Similarly, the nonzero elements of F form an abelian group under multiplication, called the multiplicative group, and denoted by (F \ {0}, ·) or just F \ {0} or F*. A field may thus be defined as set F equipped with two operations denoted as an addition and a multiplication such that F is an abelian group under addition, F \ {0} is an abelian group under multiplication (where 0 is the identity element of the addition), and multiplication is distributive over addition.[nb 1] Some elementary statements about fields can therefore be obtained by applying general facts of groups. For example, the additive and multiplicative inverses −a and a−1 are uniquely determined by a.The requirement 1 ≠ 0 follows, because 1 is the identity element of a group that does not contain 0.[8] Thus, the trivial ring, consisting of a single element, is not a field.Every finite subgroup of the multiplicative group of a field is cyclic (see Root of unity § Cyclic groups).In addition to the multiplication of two elements of F, it is possible to define the product n ⋅ a of an arbitrary element a of F by a positive integer n to be the n-fold sumIf there is no positive integer such thatthen F is said to have characteristic 0.[9] For example, Q has characteristic 0 since no positive integer n is zero. Otherwise, if there is a positive integer n satisfying this equation, the smallest such positive integer can be shown to be a prime number. It is usually denoted by p and the field is said to have characteristic p then.For example, the field F4 has characteristic 2 since (in the notation of the above addition table) I + I = O.If F has characteristic p, then p ⋅ a = 0 for all a in F. This implies that since all other binomial coefficients appearing in the binomial formula are divisible by p. Here, ap := a ⋅ a ⋅ ... ⋅ a (p factors) is the p-th power, i.e., the p-fold product of the element a. Therefore, the Frobenius mapis compatible with the addition in F (and also with the multiplication), and is therefore a field homomorphism.[10] The existence of this homomorphism makes fields in characteristic p quite different from fields of characteristic 0.A subfield E of a field F is a subset of F that is a field with respect to the field operations of F. Equivalently E is a subset of F that contains 1, and is closed under addition, multiplication, additive inverse and multiplicative inverse of a nonzero element. This means that 1 ∊ E, that for all a, b ∊ E both a + b and a · b are in E, and that for all a ≠ 0 in E, both –a and 1/a are in E.Field homomorphisms are maps f: E → F between two fields such that f(e1 + e2) = f(e1) + f(e2), f(e1e2) = f(e1)f(e2), and f(1E) = 1F, where e1 and e2 are arbitrary elements of E. All field homomorphisms are injective.[11] If f is also surjective, it is called an isomorphism (or the fields E and F are called isomorphic).A field is called a prime field if it has no proper (i.e., strictly smaller) subfields. Any field F contains a prime field. If the characteristic of F is p (a prime number), the prime field is isomorphic to the finite field Fp introduced below. Otherwise the prime field is isomorphic to Q.[12]Finite fields (also called Galois fields) are fields with finitely many elements, whose number is also referred to as the order of the field. The above introductory example F4 is a field with four elements. Its subfield F2 is the smallest field, because by definition a field has at least two distinct elements 1 ≠ 0.The simplest finite fields, with prime order, are most directly accessible using modular arithmetic. For a fixed positive integer n, arithmetic "modulo n" means to work with the numbersThe addition and multiplication on this set are done by performing the operation in question in the set Z of integers, dividing by n and taking the remainder as result. This construction yields a field precisely if n is a prime number. For example, taking the prime n = 2 results in the above-mentioned field F2. For n = 4 and more generally, for any composite number (i.e., any number n which can be expressed as a product n = r⋅s of two strictly smaller natural numbers), Z/nZ is not a field: the product of two non-zero elements is zero since r⋅s = 0 in Z/nZ, which, as was explained above, prevents Z/nZ from being a field. The field Z/pZ with p elements (p being prime) constructed in this way is usually denoted by  Fp.Every finite field F has q = pn elements, where p is prime and n ≥ 1. This statement holds since F may be viewed as a vector space over its prime field. The dimension of this vector space is necessarily finite, say n, which implies the asserted statement.[13]A field with q = pn elements can be constructed as the splitting field of the polynomial Such a splitting field is an extension of Fp in which the polynomial f has q zeros. This means f has as many zeros as possible since the degree of f is q. For q = 22 = 4, it can be checked case by case using the above multiplication table that all four elements of F4 satisfy the equation x4 = x, so they are zeros of f. By contrast, in F2, f has only two zeros (namely 0 and 1), so f does not split into linear factors in this smaller field. Elaborating further on basic field-theoretic notions, it can be shown that two finite fields with the same order are isomorphic.[14] It is thus customary to speak of the finite field with q elements, denoted by Fq or GF(q).Historically, three algebraic disciplines led to the concept of a field: the question of solving polynomial equations, algebraic number theory, and algebraic geometry.[15] A first step towards the notion of a field was made in 1770 by Joseph-Louis Lagrange, who observed that permuting the zeros x1, x2, x3 of a cubic polynomial in the expression(with ω being a third root of unity) only yields two values. This way, Lagrange conceptually explained the classical solution method of Scipione del Ferro and François Viète, which proceeds by reducing a cubic equation for an unknown x to an quadratic equation for x3.[16] Together with a similar observation for equations of degree 4, Lagrange thus linked what eventually became the concept of fields and the concept of groups.[17] Vandermonde, also in 1770, and to a fuller extent, Carl Friedrich Gauss, in his Disquisitiones Arithmeticae (1801), studied the equationfor a prime p and, again using modern language, the resulting cyclic Galois group. Gauss deduced that a regular p-gon can be constructed if p = 22k + 1. Building on Lagrange's work, Paolo Ruffini claimed (1799) that quintic equations (polynomial equations of degree 5) cannot be solved algebraically, however his arguments were flawed. These gaps were filled by Niels Henrik Abel in 1824.[18] Évariste Galois, in 1832, devised necessary and sufficient criteria for a polynomial equation to be algebraically solvable, thus establishing in effect what is known as Galois theory today. Both Abel and Galois worked with what is today called an algebraic number field, but conceived neither an explicit notion of a field, nor of a group.In 1871 Richard Dedekind introduced, for a set of real or complex numbers that is closed under the four arithmetic operations, the German word Körper, which means "body" or "corpus" (to suggest an organically closed entity). The English term "field" was introduced by Moore (1893).[19]By a field we will mean every infinite system of real or complex numbers so closed in itself and perfect that addition, subtraction, multiplication, and division of any two of these numbers again yields a number of the system.In 1881 Leopold Kronecker defined what he called a domain of rationality, which is a field of rational fractions in modern terms. Kronecker's notion did not cover the field of all algebraic numbers (which is a field in Dedekind's sense), but on the other hand was more abstract than Dedekind's in that it made no specific assumption on the nature of the elements of a field. Kronecker interpreted a field such as Q(π) abstractly as the rational function field Q(X). Prior to this, examples of transcendental numbers were known since Joseph Liouville's work in 1844, until Charles Hermite (1873) and Ferdinand von Lindemann (1882) proved the transcendence of e and π, respectively.[21]The first clear definition of an abstract field is due to Weber (1893).[22] In particular, Heinrich Martin Weber's notion included the field Fp. Giuseppe Veronese (1891) studied the field of formal power series, which led Hensel (1904) to introduce the field of p-adic numbers. Steinitz (1910) synthesized the knowledge of abstract field theory accumulated so far. He axiomatically studied the properties of fields and defined many important field-theoretic concepts. The majority of the theorems mentioned in the sections Galois theory, Constructing fields and Elementary notions can be found in Steinitz's work. Artin & Schreier (1927) linked the notion of orderings in a field, and thus the area of analysis, to purely algebraic properties.[23] Emil Artin redeveloped Galois theory from 1928 through 1942, eliminating the dependency on the primitive element theorem.A commutative ring is a set, equipped with an addition and multiplication operation, satisfying all the axioms of a field, except for the existence of multiplicative inverses a−1.[24] For example, the integers Z form a commutative ring, but not a field: the reciprocal of an integer n is not itself an integer, unless n = ±1.In the hierarchy of algebraic structures fields can be characterized as the commutative rings R in which every nonzero element is a unit (which means every element is invertible). Similarly, fields are the commutative rings with precisely two distinct ideals, (0) and R. Fields are also precisely the commutative rings in which (0) is the only prime ideal.Given a commutative ring R, there are two ways to construct a field related to R, i.e., two ways of modifying R such that all nonzero elements become invertible: forming the field of fractions, and forming residue fields. The field of fractions of Z is Q, the rationals, while the residue fields of Z are the finite fields Fp.Given an integral domain R, its field of fractions Q(R) is built with the fractions of two elements of R exactly as Q is constructed from the integers. More precisely, the elements of Q(R) are the fractions a/b where a and b  are in R, and  b ≠ 0. Two fractions a/b and  c/d are equal if and only if ad = bc. The operation on the fractions work exactly as for rational numbers. For example, It is straightforward to show that, if the ring is an integral domain, the set of the fractions form a field.[25]The field F(x) of the rational fractions over a field (or an integral domain) F is the field of fractions of the polynomial ring F[x]. The field F((x)) of Laurent seriesover a field F is the field of fractions of the ring F[[x]] of formal power series (in which k ≥ 0). Since any Laurent series is a fraction of a power series divided by a power of x (as opposed to an arbitrary power series), the representation of fractions is less important in this situation, though.In addition the field of fractions, which embeds R injectively into a field, a field can be obtained from a commutative ring R by means of a surjective map onto a field F. Any field obtained in this way is a quotient R / m, where m is a maximal ideal of R. If R has only one maximal ideal m, this field is called the residue field of R.[26]The ideal generated by a single polynomial f in the polynomial ring R = E[X] (over a field E) is maximal if and only if f is irreducible in E, i.e., if f can not be expressed as the product of two polynomials in E[X] of smaller degree. This yields a fieldThis field F contains an element x (namely the residue class of X) which satisfies the equationFor example, C is obtained from R by adjoining the imaginary unit symbol i, which satisfies f(i) = 0, where f(X) = X2 + 1. Moreover, f is irreducible over R, which implies that the map that sends a polynomial f(X) ∊ R[X] to f(i) yields an isomorphismFields can be constructed inside a given bigger container field. Suppose given a field E, and a field F containing E as a subfield. For any element x of F, there is a smallest subfield of F containing E and x, called the subfield of F generated by x and denoted E(x).[27] The passage from E to E(x) is referred to by adjoining an element to E. More generally, for a subset S ⊂ F, there is a minimal subfield of F containing E and S, denoted by E(S).The compositum of two subfields E and E'  of some field F is the smallest subfield of F containing both E and E'. The compositum can be used to construct the biggest subfield of F satisfying a certain property, for example the biggest subfield of F, which is, in the language introduced below, algebraic over E.[nb 2]The notion of a subfield E ⊂ F can also be regarded from the opposite point of view, by referring to F being a field extension (or just extension) of E, denoted byand read "F over E".A basic datum of a field extension is its degree [F : E], i.e., the dimension of F as an E-vector space. It satisfies the formula[28]Extensions whose degree is finite are referred to as finite extensions. The extensions C / R and F4 / F2 are of degree 2, whereas R / Q is an infinite extension.with en, ..., e0 in E, and en ≠ 0. For example, the imaginary unit i in  C is algebraic over R, and even over Q, since it satisfies the equationA field extension in which every element of F is algebraic over E is called an algebraic extension. Any finite extension is necessarily algebraic, as can be deduced from the above multiplicativity formula.[29]The subfield E(x) generated by an element x, as above, is an algebraic extension of E if and only if x is an algebraic element. That is to say, if x is algebraic, all other elements of E(x) are necessarily algebraic as well. Moreover, the degree of the extension E(x) / E, i.e., the dimension of E(x) as an E-vector space, equals the minimal degree n such that there is a polynomial equation involving x, as above. If this degree is n, then the elements of E(x) have the form For example, the field Q(i) of Gaussian rationals is the subfield of C consisting of all numbers of the form a + bi where both a and b are rational numbers: summands of the form i2 (and similarly for higher exponents) don't have to be considered here, since a + bi + ci2 can be simplified to a − c + bi.The above-mentioned field of rational fractions E(X), where X is an indeterminate, is not an algebraic extension of E since there is no polynomial equation with coefficients in E whose zero is X. Elements, such as X, which are not algebraic are called transcendental. Informally speaking, the indeterminate X and its powers do not interact with elements of E. A similar construction can be carried out with a set of indeterminates, instead of just one.Once again, the field extension E(x) / E discussed above is a key example: if x is not algebraic (i.e., x is not a root of a polynomial with coefficients in E), then E(x) is isomorphic to E(X). This isomorphism is obtained by substituting x to X in rational fractions.A subset S of a field F is a transcendence basis if it is algebraically independent (don't satisfy any polynomial relations) over E and if F is an algebraic extension of E(S). Any field extension F / E has a transcendence basis.[30] Thus, field extensions can be split into ones of the form E(S) / E (purely transcendental extensions) and algebraic extensions.A field is algebraically closed if it does not have any strictly bigger algebraic extensions or, equivalently, if any polynomial equationhas a solution x ∊ F.[31] By the fundamental theorem of algebra, C is algebraically closed, i.e., any polynomial equation with complex coefficients has a complex solution. The rational and the real numbers are not algebraically closed since the equationdoes not have any rational or real solution. A field containing F is called an algebraic closure of F if it is algebraic over F (roughly speaking, not too big compared to F) and is algebraically closed (big enough to contain solutions of all polynomial equations).By the above, C is an algebraic closure of R. The situation that the algebraic closure is a finite extension of the field F is quite special: by the Artin-Schreier theorem, the degree of this extension is necessarily 2, and F is elementarily equivalent to R. Such fields are also known as real closed fields.Any field F has an algebraic closure, which is moreover unique up to (non-unique) isomorphism. It is commonly referred to as the algebraic closure and denoted F. For example, the algebraic closure Q of Q is called the field of algebraic numbers. The field F is usually rather implicit since its construction requires the ultrafilter lemma, a set-theoretic axiom that is weaker than the axiom of choice.[32] In this regard, the algebraic closure of Fq, is exceptionally simple. It is the union of the finite fields containing Fq (the ones of order qn). For any algebraically closed field F of characteristic 0, the algebraic closure of the field F((t)) of Laurent series is the field of Puiseux series, obtained by adjoining roots of t.[33]Since fields are ubiquitous in mathematics and beyond, several refinements of the concept have been adapted to the needs of particular mathematical areas.A field F is called an ordered field if any two elements can be compared, so that x + y ≥ 0 and xy ≥ 0 whenever x ≥ 0 and y ≥ 0. For example, the reals form an ordered field, with the usual ordering ≥. The Artin-Schreier theorem states that a field can be ordered if and only if it is a formally real field, which means that any quadratic equationonly has the solution x1 = x2 = ... = xn = 0.[34] The set of all possible orders on a fixed field F is isomorphic to the set of ring homomorphisms from the Witt ring W(F) of quadratic forms over F, to Z.[35]An Archimedean field is an ordered field such that for each element there exists a finite expressionwhose value is greater than that element, that is, there are no infinite elements.  Equivalently, the field contains no infinitesimals (elements smaller than all rational numbers); or, yet equivalent, the field is isomorphic to a subfield of R.An ordered field is Dedekind-complete if all upper bounds, lower bounds (see Dedekind cut) and limits, which should exist, do exist. More formally, each bounded subset of F is required to have a least upper bound. Any complete field is necessarily Archimedean,[36] since in any non-Archimedean field there is neither a greatest infinitesimal nor a least positive rational, whence the sequence 1/2, 1/3, 1/4, …, every element of which is greater than every infinitesimal, has no limit.Since every proper subfield of the reals also contains such gaps, R is the unique complete ordered field, up to isomorphism.[37] Several foundational results in calculus follow directly from this characterization of the reals.The hyperreals R* form an ordered field that is not Archimedean. It is an extension of the reals obtained by including infinite and infinitesimal numbers. These are larger, respectively smaller than any real number. The hyperreals form the foundational basis of non-standard analysis.Another refinement of the notion of a field is a topological field, in which the set F is a topological space, such that all operations of the field (addition, multiplication, the maps a ↦ −a and a ↦ a−1) are continuous maps with respect to the topology of the space.[38]The topology of all the fields discussed below is induced from a metric, i.e., a functionthat measures a distance between any two elements of F.The completion of F is another field in which, informally speaking, the "gaps" in the original field F are filled, if there are any. For example, any irrational number x, such as x = √2, is a "gap" in the rationals Q in the sense that it is a real number that can be approximated arbitrarily closely by rational numbers p/q, in the sense that distance of x and p/q given by the absolute value |x − p/q| is as small as desired.The following table lists some examples of this construction. The fourth column shows an example of a zero sequence, i.e., a sequence whose limit (for n → ∞) is zero.The field Qp is used in number theory and p-adic analysis. The algebraic closure Qp carries a unique norm extending the one on Qp, but is not complete. The completion of this algebraic closure, however, is algebraically closed. Because of its rough analogy to the complex numbers, it is called the field of complex p-adic numbers and is denoted by Cp.[39]The following topological fields are called local fields:[40][nb 3]These two types of local fields share some fundamental similarities. In this relation, the elements p ∈ Qp and t ∈ Fp((t)) (referred to as uniformizer) correspond to each other. The first manifestation of this is at an elementary level: the elements of both fields can be expressed as power series in the uniformizer, with coefficients in Fp. (However, since the addition in Qp is done using carrying, which is not the case in Fp((t)), these fields are not isomorphic.) The following facts show that this superficial similarity goes much deeper:Differential fields are fields equipped with a derivation, i.e., allow to take derivatives of elements in the field.[42] For example, the field R(X), together with the standard derivative of polynomials forms a differential field. These fields are central to differential Galois theory, a variant of Galois theory dealing with linear differential equations.Galois theory studies algebraic extensions of a field by studying the symmetry in the arithmetic operations of addition and multiplication. An important notion in this area are finite Galois extensions F / E, which are, by definition, those that are separable and normal. The primitive element theorem shows that finite separable extensions are necessarily simple, i.e., of the formwhere f is an irreducible polynomial (as above).[43] For such an extension, being normal and separable means that all zeros of f are contained in F and that f has only simple zeros. The latter condition is always satisfied if E has characteristic 0.The tensor product of fields is not usually a field. For example, a finite extension F / E of degree n is a Galois extension if and only if there is an isomorphism of F-algebrasThis fact is the beginning of Grothendieck's Galois theory, a far-reaching extension of Galois theory applicable to algebro-geometric objects.[46]Basic invariants of a field F include the characteristic and the transcendence degree of F over its prime field. The latter is defined as the maximal number of elements in F that are algebraically independent over the prime field. Two algebraically closed fields E and F are isomorphic precisely if these two data agree.[47] This implies that any two uncountable algebraically closed fields of the same cardinality and the same characteristic are isomorphic. For example, Qp, Cp and C are isomorphic (but not isomorphic as topological fields).In model theory, a branch of mathematical logic, two fields E and F are called elementarily equivalent if every mathematical statement that is true for E is also true for F and conversely. The mathematical statements in question are required to be first-order sentences (involving 0, 1, the addition and multiplication). A typical example isThe Lefschetz principle states that C is elementarily equivalent to any algebraically closed field F of characteristic zero. Moreover, any fixed statement φ holds in C if and only if it holds in any algebraically closed field of sufficiently high characteristic.[48]If U is an ultrafilter on a set I, and Fi is a field for every i in I, the ultraproduct of the Fi with respect to U is a field.[49] It is denoted bysince it behaves in several ways as a limit of the fields Fi: Łoś's theorem states that any first order statement that holds for all but finitely many Fi, also holds for the ultraproduct. Applied to the above sentence φ, this shows that there is an isomorphism[nb 4]The Ax–Kochen theorem mentioned above also follows from this and an isomorphism of the ultraproducts (in both cases over all primes p)In addition, model theory also studies the logical properties of various other types of fields, such as real closed fields or exponential fields (which are equipped with an exponential function  exp : F → Fx).[50]For fields that are not algebraically closed (or not separably closed), the absolute Galois group Gal(F) is fundamentally important: extending the case of finite Galois extensions outlined above, this group governs all finite separable extensions of F. By elementary means, the group Gal(Fq) can be shown to be the Prüfer group, the profinite completion of Z. This statement subsumes the fact that the only algebraic extensions of Gal(Fq) are the fields Gal(Fqn) for n > 0, and that the Galois groups of these finite extensions are given byA description in terms of generators and relations is also known for the Galois groups of p-adic number fields (finite extensions of Qp).[51]Representations of Galois groups and of related groups such as the Weil group are fundamental in many branches of arithmetic, such as the Langlands program. The cohomological study of such representations is done using Galois cohomology.[52] For example, the Brauer group, which is classically defined as the group of central simple F-algebras, can be reinterpreted as a Galois cohomology group, namelyMilnor K-theory is defined asThe norm residue isomorphism theorem, proved around 2000 by Vladimir Voevodsky, relates this to Galois cohomology by means of an isomorphismAlgebraic K-theory is related to the group of invertible matrices with coefficients the given field. For example, the process of taking the determinant of an invertible matrix leads to an isomorphism K1(F) = F×. Matsumoto's theorem shows that K2(F) agrees with K2M(F). In higher degrees, K-theory diverges from Milnor K-theory and remains hard to compute in general.If a ≠ 0, then the equation has a unique solution x in F, namely x = b/a. This observation, which is an immediate consequence of the definition of a field, is the essential ingredient used to show that any vector space has a basis.[53] Roughly speaking, this allows choosing a coordinate system in any vector space, which is of central importance in linear algebra both from a theoretical point of view, and also for practical applications.Modules (the analogue of vector spaces) over most rings, including the ring Z of integers, have a more complicated structure. A particular situation arises when a ring R is a vector space over a field F in its own right. Such rings are called F-algebras and are studied in depth in the area of commutative algebra. For example, Noether normalization asserts that any finitely generated F-algebra is closely related to (more precisely, finitely generated as a module over) a polynomial ring F[x1, ..., xn].[54]A widely applied cryptographic routine uses the fact that discrete exponentiation, i.e., computingin a (large) finite field Fq can be performed much more efficiently than the discrete logarithm, which is the inverse operation, i.e., determining the solution n to an equationIn elliptic curve cryptography, the multiplication in a finite field is replaced by the operation of adding points on an elliptic curve, i.e., the solutions of an equation of the formFinite fields are also used in coding theory and combinatorics.Functions on a suitable topological space X into a field k can be added and multiplied pointwise, e.g., the product of two functions is defined by the product of their values within the domain:This makes these functions a k-commutative algebra.For having a field of functions, one must consider algebras of functions that are integral domains. In this case the ratios of two functions, i.e., expressions of the formform a field, called field of functions.This occurs in two main cases. When X is a complex manifold X. In this case, one considers the algebra of holomorphic functions, i.e., complex differentiable functions. Their ratios form the field of  meromorphic functions on X.The function field of an algebraic variety X (a geometric object defined as the common zeros of polynomial equations) consists of ratios of regular functions, i.e., ratios of polynomial functions on the variety. The function field of the n-dimensional space over a field k is k(x1, ..., xn), i.e., the field consisting of ratios of polynomials in n indeterminates. The function field of X is the same as the one of any open dense subvariety. In other words, the function field is insensitive to replacing X by a (slightly) smaller subvariety.The function field is invariant under isomorphism and birational equivalence of varieties. It is therefore an important tool for the study of abstract algebraic varieties and for the classification of algebraic varieties. For example, the dimension, which equals the transcendence degree of k(X), is invariant under birational equivalence.[55] For curves (i.e., the dimension is one), the function field k(X) is very close to X: if X is smooth and proper (the analogue of being compact), X can be reconstructed, up to isomorphism, from its field of functions.[nb 5] In higher dimension the function field remembers less, but still decisive information about X. The study of function fields and their geometric meaning in higher dimensions is referred to as birational geometry. The minimal model program attempts to identify the simplest (in a certain precise sense) algebraic varieties with a prescribed function field.Global fields are in the limelight in algebraic number theory and arithmetic geometry.They are, by definition, number fields (finite extensions of Q) or function fields over Fq (finite extensions of Fq(t)). As for local fields, these two types of fields share several similar features, even though they are of characteristic 0 and positive characteristic, respectively. This function field analogy can help to shape mathematical expectations, often first by understanding questions about function fields, and later treating the number field case. The latter is often more difficult. For example, the Riemann hypothesis concerning the zeros of the Riemann zeta function (open as of 2017) can be regarded as being parallel to the Weil conjectures (proven in 1974 by Pierre Deligne).Cyclotomic fields are among the most intensely studied number fields. They are of the form Q(ζn), where ζn is a primitive n-th root of unity, i.e., a complex number satisfying ζn = 1 and ζm ≠ 1 for all m < n.[56] For n being a regular prime, Kummer used cyclotomic fields to prove Fermat's last theorem, which asserts the non-existence of rational nonzero solutions to the equationLocal fields are completions of global fields. Ostrowski's theorem asserts that the only completions of Q, a global field, are the local fields Qp and R. Studying arithmetic questions in global fields may sometimes be done by looking at the corresponding questions locally. This technique is called the local-global principle. For example, the Hasse–Minkowski theorem reduces the problem of finding rational solutions of quadratic equations to solving these equations in R and Qp, whose solutions can easily be described.[57]Unlike for local fields, the Galois groups of global fields are not known. Inverse Galois theory studies the (unsolved) problem whether any finite group is the Galois group Gal(F/Q) for some number field F.[58] Class field theory describes the abelian extensions, i.e., ones with abelian Galois group, or equivalently the abelianized Galois groups of global fields. A classical statement, the Kronecker–Weber theorem, describes the maximal abelian Qab extension of Q: it is the fieldIn addition to the additional structure that fields may enjoy, fields admit various other related notions. Since in any field 0 ≠ 1, any field has at least two elements. Nonetheless, there is a concept of field with one element, which is suggested to be a limit of the finite fields Fp, as p tends to 1.[59] In addition to division rings, there are various other weaker algebraic structures related to fields such as quasifields, near-fields and semifields.There are also proper classes with field structure, which are sometimes called Fields, with a capital F. The surreal numbers form a Field containing the reals, and would be a field except for the fact that they are a proper class, not a set. The nimbers, a concept from game theory form a Field.[60]Dropping one or several axioms in the definition of a field leads to other algebraic structures. As was mentioned above, commutative rings satisfy all axioms of fields, except for multiplicative inverses. Dropping instead the condition that multiplication is commutative leads to the concept of a division ring or skew field.[nb 6] The only division rings that are finite-dimensional R-vector spaces are R itself, C (which is a field), the quaternions H (in which multiplication is non-commutative), and the octonions O (in which multiplication is neither commutative nor associative). This fact was proved using methods of algebraic topology in 1958 by Michel Kervaire, Raoul Bott, and John Milnor.[61] The non-existence of an odd-dimensional division algebra is more classical. It can be deduced from the hairy ball theorem illustrated at the right.[citation needed]
Flat (geometry)
In geometry, a flat is a subset of a Euclidean space that is congruent to a Euclidean space of lower dimension.  The flats in two-dimensional space are points and lines, and the flats in three-dimensional space are points, lines, and planes.In a n-dimensional space, there are flats of every dimension from 0 to n − 1.[1] Flats of dimension n − 1 are called hyperplanes.Flats are the affine subspaces of Euclidean spaces, which means that they are similar to linear subspaces, except that they need not pass through the origin. Flats occurs in linear algebra, as geometric realizations of solution sets of systems of linear equations.A flat is manifold and an algebraic variety, and is sometimes called linear manifold or linear variety for distinguishing it from other manifolds or varieties.A flat can be described by a system of linear equations.  For example, a line in two-dimensional space can be described by a single linear equation involving x and y:In three-dimensional space, a single linear equation involving x, y, and z defines a plane, while a pair of linear equations can be used to describe a line.  In general, a linear equation in n variables describes a hyperplane, and a system of linear equations describes the intersection of those hyperplanes.  Assuming the equations are consistent and linearly independent, a system of k equations describes a flat of dimension n − k.A flat can also be described by a system of linear parametric equations.  A line can be described by equations involving one parameter:while the description of a plane would require two parameters:In general, a parameterization of a flat of dimension k would require parameters t1, … , tk.An intersection of flats is either a flat or the empty set.[2]If every line from the first flat is parallel to some line from the second flat, then these flats are parallel. Two parallel flats of the same dimension either coincide or do not intersect; they can be described by two systems of linear equations which differ only in their right-hand sides.If flats do not intersect, and no line from the first flat is parallel to a line from the second flat, then these are skew flats. It is possible only if sum of their dimensions is less than dimension of the ambient space.For two flats of dimensions k1 and k2 there exists the minimal flat which contains them, of dimension at most k1 + k2 + 1. If two flats intersect, then the dimension of the containing flat equals to k1 + k2 minus the dimension of the intersection.These two operations (referred to as meet and join) make the set of all flats in the Euclidean n-space a lattice and can build systematic coordinates for flats in any dimension, leading to Grassmann coordinates or dual Grassmann coordinates. For example, a line in three-dimensional space is determined by two distinct points or by two distinct planes.However, the lattice of all flats is not a distributive lattice.If two lines ℓ1 and ℓ2 intersect, then ℓ1 ∩ ℓ2 is a point. If p is a point not lying on the same plane, then (ℓ1 ∩ ℓ2) + p = (ℓ1 + p) ∩ (ℓ2 + p), both representing a line. But when ℓ1 and ℓ2 are parallel, this distributivity fails, giving p on the left-hand side and a third parallel line on the right-hand side.The aforementioned facts do not depend on the structure being that of Euclidean space (namely, involving  Euclidean distance) and are correct in any affine space. In a Euclidean space:
Numerical analysis
Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. As an aspect of mathematics and computer science that generates, analyzes, and implements algorithms, the growth in power and the revolution in computing has raised the use of realistic mathematical models in science and engineering, and complex numerical analysis is required to provide solutions to these more involved models of the world. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.Before the advent of modern computers, numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.One of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.[2]Numerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of the square root of 2, modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.The overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following:The rest of this section outlines several important themes of numerical analysis.The field of numerical analysis predates the invention of modern computers by many centuries. Linear interpolation was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like Newton's method, Lagrange interpolation polynomial, Gaussian elimination, or Euler's method.To facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients.  Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions.  The canonical work in the field is the NIST publication edited by Abramowitz and Stegun, a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points.  The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.The mechanical calculator was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.Direct vs iterative methodsConsider the problem of solvingfor the unknown quantity x.For the iterative method, apply the bisection method to f(x) = 3x3 − 24. The initial values are a = 0, b = 3, f(a) = −24, f(b) = 57.We conclude from this table that the solution is between 1.875 and 2.0625. The algorithm might return any number in that range with an error less than 0.2.In a two-hour race, we have measured the speed of the car at three instants and recorded them in the following table.A discretization would be to say that the speed of the car was constant from 0:00 to 0:40, then from 0:40 to 1:20 and finally from 1:20 to 2:00. For instance, the total distance traveled in the first 40 minutes is approximately (7003720000000000000♠2/3 h × 7001388888888888888♠140 km/h) = 7004933000000000000♠93.3 km. This would allow us to estimate the total distance traveled as 7004933000000000000♠93.3 km + 7005100000000000000♠100 km + 7005120000000000000♠120 km = 7005313300000000000♠313.3 km, which is an example of numerical integration (see below) using a Riemann sum, because displacement is the integral of velocity.Ill-conditioned problem: Take the function f(x) = 1/(x − 1). Note that f(1.1) = 10 and f(1.001) = 1000: a change in x of less than 0.1 turns into a change in f(x) of nearly 1000. Evaluating f(x) near x = 1 is an ill-conditioned problem.Well-conditioned problem: By contrast, evaluating the same function f(x) = 1/(x − 1) near x = 10 is a well-conditioned problem. For instance, f(10) = 1/9 ≈ 0.111 and f(11) = 0.1: a modest change in x leads to a modest change in f(x).Direct methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the simplex method of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).In contrast to direct methods, iterative methods are not expected to terminate in a finite number of steps. Starting from an initial guess, iterative methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the bisection method, and Jacobi iteration. In computational matrix algebra, iterative methods are generally needed for large problems.Iterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.Furthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called discretization. For example, the solution of a differential equation is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.The study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.Round-off errors arise because it is impossible to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).What does it mean when we say that the truncation error is created when we approximate a mathematical procedure?  We know that to integrate a function exactly requires one to find the sum of infinite trapezoids.  But numerically one can find the sum of only finite trapezoids, and hence the approximation of the mathematical procedure.  Similarly, to differentiate a function, the differential element approaches zero but numerically we can only choose a finite value of the differential element.Numerical stability is an important notion in numerical analysis. An algorithm is called numerically stable if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is well-conditioned, meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is ill-conditioned, then any small error in the data will grow to be a large error.Both the original problem and the algorithm used to solve that problem can be well-conditioned and/or ill-conditioned, and any combination is possible.Observe that the Babylonian method converges quickly regardless of the initial guess, whereas Method X converges extremely slowly with initial guess x0 = 1.4 and diverges for initial guess x0 = 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable.The field of numerical analysis includes many sub-disciplines. Some of the major ones are:Interpolation: We have observed the temperature to vary from 20 degrees Celsius at 1:00 to 14 degrees at 3:00. A linear interpolation of this data would conclude that it was 17 degrees at 2:00 and 18.5 degrees at 1:30pm.Extrapolation: If the gross domestic product of a country has been growing an average of 5% per year and was 100 billion dollars last year, we might extrapolate that it will be 105 billion dollars this year.Regression: In linear regression, given n points, we compute a line that passes as close as possible to those n points.Optimization: Say you sell lemonade at a lemonade stand, and notice that at $1, you can sell 197 glasses of lemonade per day, and that for each increase of $0.01, you will sell one glass of lemonade less per day. If you could charge $1.485, you would maximize your profit, but due to the constraint of having to charge a whole cent amount, charging $1.48 or $1.49 per glass will both yield the maximum income of $220.52 per day.Differential equation: If you set up 100 fans to blow air from one end of the room to the other and then you drop a feather into the wind, what happens? The feather will follow the air currents, which may be very complex. One approximation is to measure the speed at which the air is blowing near the feather every second, and advance the simulated feather as if it were moving in a straight line at that same speed for one second, before measuring the wind speed again. This is called the Euler method for solving an ordinary differential equation.One of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control round-off errors arising from the use of floating point arithmetic.Interpolation solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?Extrapolation is very similar to interpolation, except that now we want to find the value of the unknown function at a point which is outside the given points.Regression is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), we want to determine the unknown function. The least squares-method is one popular way to achieve this.Much effort has been put in the development of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.Root-finding algorithms are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is differentiable and the derivative is known, then Newton's method is a popular choice. Linearization is another technique for solving nonlinear equations.Several important problems can be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm[4] is based on the singular value decomposition. The corresponding tool in statistics is called principal component analysis.Optimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some constraints.The field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.The method of Lagrange multipliers can be used to reduce optimization problems with constraints to unconstrained optimization problems.Numerical integration, in some instances also known as numerical quadrature, asks for the value of a definite integral. Popular methods use one of the Newton–Cotes formulas (like the midpoint rule or Simpson's rule) or Gaussian quadrature. These methods rely on a "divide and conquer" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use Monte Carlo or quasi-Monte Carlo methods (see Monte Carlo integration), or, in modestly large dimensions, the method of sparse grids.Numerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.Partial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a finite element method, a finite difference method, or (particularly in engineering) a finite volume method. The theoretical justification of these methods often involves theorems from functional analysis. This reduces the problem to the solution of an algebraic equation.Since the late twentieth century, most algorithms are implemented in a variety of programming languages. The Netlib repository contains various collections of software routines for numerical problems, mostly in Fortran and C. Commercial products implementing many different numerical algorithms include the IMSL and NAG libraries; a free-software alternative is the GNU Scientific Library.There are several popular numerical computing applications such as MATLAB, TK Solver, S-PLUS, and IDL as well as free and open source alternatives such as FreeMat, Scilab, GNU Octave (similar to Matlab), and IT++ (a C++ library). There are also programming languages such as R (similar to S-PLUS) and Python with libraries such as NumPy, SciPy and SymPy. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.[5][6]Many computer algebra systems such as Mathematica also benefit from the availability of arbitrary precision arithmetic which can provide more accurate results.Also, any spreadsheet software can be used to solve simple problems relating to numerical analysis.JournalsOnline textsOnline course material
Riemann solver
Numerical analysis · SimulationFinite element · Boundary element Lattice Boltzmann · Riemann solverDissipative particle dynamicsSmoothed particle hydrodynamicsA Riemann solver is a numerical method used to solve a Riemann problem. They are heavily used in computational fluid dynamics and computational magnetohydrodynamics.Godunov is credited with introducing the first exact Riemann solver for the Euler equations,[1] by extending the previous CIR (Courant-Isaacson-Rees) method to non-linear systems of hyperbolic conservation laws. Modern solvers are able to simulate relativistic effects and magnetic fields.For the hydrodynamic case latest research results showed the possibility to avoid the iterations to calculate the exact solution for theEuler equations.[2]As iterative solutions are too costly, especially in Magnetohydrodynamics, some approximations have to be made. The most popular solvers are:Roe used the linearisation of the Jacobian, which he then solves exactly.[3]The HLLE[4] (Harten, Lax, van Leer and  Einfeldt) solver is an approximate solution to the Riemann problem,  which is only based on the integral form of the conservation laws and the largest and smallest signal velocities at the interface.  The stability and robustness of the HLLE solver is closely related to the signal velocities and a single central average state, as proposed by Einfeldt in the original paper. The description of the HLLE scheme in the book mentioned below is incomplete and partially wrong[citation needed]. The reader is referred to the original paper. Actually, the HLLE scheme is based on a new stability theory for discontinuities in fluids, which was never published.[5]The HLLC (Harten-Lax-van Leer-Contact) solver was introduced by Toro.[6] It restores the missing Rarefaction wave by some estimates, like linearisations, these can be simple but also more advanced exists like using the Roe average velocity for the middle wave speed. They are quite robust and efficient but somewhat more diffusive.[7]These solvers were introduced by Nishikawa and Kitamura,[8] in order to overcome the carbuncle problems of the Roe solver and the excessive diffusion of the HLLE solver at the same time. They developed robust and accurate Riemann solvers by combining the Roe solver and the HLLE/Rusanov solvers: they show that being applied in two orthogonal directions the two Riemann solvers can be combined into a single Roe-type solver (the Roe solver with modified wave speeds). In particular, the one derived from the Roe and HLLE solvers, called Rotated-RHLL solver, is extremely robust (carbuncle-free for all possible test cases on both structured and unstructured grids) and accurate (as accurate as the Roe solver for the boundary layer calculation).
General linear group
In mathematics, the general linear group of degree n is the set of n×n invertible matrices, together with the operation of ordinary matrix multiplication. This forms a group, because the product of two invertible matrices is again invertible, and the inverse of an invertible matrix is invertible. The group is so named because the columns of an invertible matrix are linearly independent, hence the vectors/points they define are in general linear position, and matrices in the general linear group take points in general linear position to points in general linear position.To be more precise, it is necessary to specify what kind of objects may appear in the entries of the matrix. For example, the general linear group over R (the set of real numbers) is the group of n×n invertible matrices of real numbers, and is denoted by GLn(R) or GL(n, R).More generally, the general linear group of degree n over any field F (such as the complex numbers), or a ring R (such as the ring of integers), is the set of n×n invertible matrices with entries from F (or R), again with matrix multiplication as the group operation.[1] Typical notation is GLn(F) or GL(n, F), or simply GL(n) if the field is understood.More generally still, the general linear group of a vector space GL(V) is the abstract automorphism group, not necessarily written as matrices.The special linear group, written SL(n, F) or SLn(F), is the subgroup of GL(n, F) consisting of matrices with a determinant of 1.The group GL(n, F) and its subgroups are often called linear groups or matrix groups (the abstract group GL(V) is a linear group but not a matrix group). These groups are important in the theory of group representations, and also arise in the study of spatial symmetries and symmetries of vector spaces in general, as well as the study of polynomials. The modular group may be realised as a quotient of the special linear group SL(2, Z).If n ≥ 2, then the group GL(n, F) is not abelian.If V is a vector space over the field F, the general linear group of V, written GL(V) or Aut(V), is the group of all automorphisms of V, i.e. the set of all bijective linear transformations V → V, together with functional composition as group operation. If V has finite dimension n, then GL(V) and GL(n, F) are isomorphic. The isomorphism is not canonical; it depends on a choice of basis in V. Given a basis (e1, ..., en) of V and an automorphism T in GL(V), we have then for every basis vector ei thatfor some constants aij in F; the matrix corresponding to T is then just the matrix with entries given by the aij.In a similar way, for a commutative ring R the group GL(n, R) may be interpreted as the group of automorphisms of a free R-module M of rank n. One can also define GL(M) for any R-module, but in general this is not isomorphic to GL(n, R) (for any n).Over a field F, a matrix is invertible if and only if its determinant is nonzero. Therefore, an alternative definition of GL(n, F) is as the group of matrices with nonzero determinant.Over a commutative ring R, more care is needed: a matrix over R is invertible if and only if its determinant is a unit in R, that is, if its determinant is invertible in R. Therefore, GL(n, R) may be defined as the group of matrices whose determinants are units.Over a non-commutative ring R, determinants are not at all well behaved. In this case, GL(n, R) may be defined as the unit group of the matrix ring M(n, R).The general linear group GL(n, R) over the field of real numbers is a real Lie group of dimension n2. To see this, note that the set of all n×n real matrices, Mn(R), forms a real vector space of dimension n2. The subset GL(n, R) consists of those matrices whose determinant is non-zero. The determinant is a polynomial map, and hence GL(n, R) is an open affine subvariety of Mn(R) (a non-empty open subset of Mn(R) in the Zariski topology), and therefore[2]a smooth manifold of the same dimension.As a manifold, GL(n, R) is not connected but rather has two connected components: the matrices with positive determinant and the ones with negative determinant. The identity component, denoted by GL+(n, R), consists of the real n×n matrices with positive determinant. This is also a Lie group of dimension n2; it has the same Lie algebra as GL(n, R).The group GL(n, R) is also noncompact. "The"[3] maximal compact subgroup of GL(n, R) is the orthogonal group O(n), while "the" maximal compact subgroup of GL+(n, R) is the special orthogonal group SO(n). As for SO(n), the group GL+(n, R) is not simply connected (except when n = 1), but rather has a fundamental group isomorphic to Z for n = 2 or Z2 for n > 2.The general linear group over the field of complex numbers, GL(n, C), is a complex Lie group of complex dimension n2. As a real Lie group (through realification) it has dimension 2n2. The set of all real matrices forms a real Lie subgroup. These correspond to the inclusionswhich have real dimensions n2, 2n2, and 4n2 = (2n)2. Complex n-dimensional matrices can be characterized as real 2n-dimensional matrices that preserve a linear complex structure — concretely, that commute with a matrix J such that J2 = −I, where J corresponds to multiplying by the imaginary unit i.The Lie algebra corresponding to GL(n, C) consists of all n×n complex matrices with the commutator serving as the Lie bracket.Unlike the real case, GL(n, C) is connected. This follows, in part, since the multiplicative group of complex numbers C∗ is connected. The group manifold GL(n, C) is not compact; rather its maximal compact subgroup is the unitary group U(n). As for U(n), the group manifold GL(n, C) is not simply connected but has a fundamental group isomorphic to Z.If F is a finite field with q elements, then we sometimes write GL(n, q) instead of GL(n, F). When p is prime, GL(n, p) is the outer automorphism group  of the group Zpn, and also the automorphism group, because Zpn is abelian, so the inner automorphism group is trivial.The order of GL(n, q) is: For example, GL(3, 2) has order (8 − 1)(8 − 2)(8 − 4) = 168. It is the automorphism group of the Fano plane and of the group Z23, and is also known as PSL(2, 7).More generally, one can count points of Grassmannian over F: in other words the number of subspaces of a given dimension k. This requires only finding the order of the stabilizer subgroup of one such subspace and dividing into the formula just given, by the orbit-stabilizer theorem.These formulas are connected to the Schubert decomposition of the Grassmannian, and are q-analogs of the Betti numbers of complex Grassmannians. This was one of the clues leading to the Weil conjectures.Note that in the limit q ↦ 1 the order of GL(n, q) goes to 0! – but under the correct procedure (dividing by (q − 1)n) we see that it is the order of the symmetric group (See Lorscheid's article) – in the philosophy of the field with one element, one thus interprets the symmetric group as the general linear group over the field with one element: Sn ≅ GL(n, 1).The general linear group over a prime field, GL(ν, p), was constructed and its order computed by Évariste Galois in 1832, in his last letter (to Chevalier) and second (of three) attached manuscripts, which he used in the context of studying the Galois group of the general equation of order pν.[4]The special linear group, SL(n, F), is the group of all matrices with determinant 1. They are special in that they lie on a subvariety – they satisfy a polynomial equation (as the determinant is a polynomial in the entries). Matrices of this type form a group as the determinant of the product of two matrices is the product of the determinants of each matrix. SL(n, F) is a normal subgroup of GL(n, F).If we write F× for the multiplicative group of F (excluding 0), then the determinant is a group homomorphismthat is surjective and its kernel is the special linear group. Therefore, by the first isomorphism theorem, GL(n, F)/SL(n, F) is isomorphic to F×. In fact, GL(n, F) can be written as a semidirect product:When F is R or C, SL(n, F) is a Lie subgroup of GL(n, F) of dimension n2 − 1. The Lie algebra of SL(n, F) consists of all n×n matrices over F with vanishing trace. The Lie bracket is given by the commutator.The special linear group SL(n, R) can be characterized as the group of volume and orientation preserving linear transformations of Rn.The group SL(n, C) is simply connected, while SL(n, R) is not. SL(n, R) has the same fundamental group as GL+(n, R), that is, Z for n = 2 and Z2 for n > 2.The set of all invertible diagonal matrices forms a subgroup of GL(n, F) isomorphic to (F×)n. In fields like R and C, these correspond to rescaling the space; the so-called dilations and contractions.A scalar matrix is a diagonal matrix which is a constant times the identity matrix. The set of all nonzero scalar matrices forms a subgroup of GL(n, F) isomorphic to F× . This group is the center of GL(n, F). In particular, it is a normal, abelian subgroup.The center of SL(n, F) is simply the set of all scalar matrices with unit determinant, and is isomorphic to the group of nth roots of unity in the field F.The so-called classical groups are subgroups of GL(V) which preserve some sort of bilinear form on a vector space V. These include theThese groups provide important examples of Lie groups.The projective linear group PGL(n, F) and the projective special linear group PSL(n, F) are the quotients of GL(n, F) and SL(n, F) by their centers (which consist of the multiples of the identity matrix therein); they are the induced action on the associated projective space.The affine group Aff(n, F) is an extension of GL(n, F) by the group of translations in Fn. It can be written as a semidirect product:where GL(n, F) acts on Fn in the natural manner. The affine group can be viewed as the group of all affine transformations of the affine space underlying the vector space Fn.One has analogous constructions for other subgroups of the general linear group: for instance, the special affine group is the subgroup defined by the semidirect product, SL(n, F) ⋉ Fn, and the Poincaré group is the affine group associated to the Lorentz group, O(1, 3, F) ⋉ Fn.The general semilinear group ΓL(n, F) is the group of all invertible semilinear transformations, and contains GL. A semilinear transformation is a transformation which is linear "up to a twist", meaning "up to a field automorphism under scalar multiplication". It can be written as a semidirect product:where Gal(F) is the Galois group of F (over its prime field), which acts on GL(n, F) by the Galois action on the entries.The main interest of ΓL(n, F) is that the associated projective semilinear group PΓL(n, F) (which contains PGL(n, F)) is the collineation group of projective space, for n > 2, and thus semilinear maps are of interest in projective geometry.If one removes the restriction of the determinant being non-zero, the resulting algebraic structure is a monoid, usually called the full linear monoid,[6][7][8] but occasionally also full linear semigroup,[9] general linear monoid[10][11] etc. It is actually a regular semigroup.[7]The infinite general linear group or stable general linear group is the direct limit of the inclusions GL(n, F) → GL(n + 1, F) as the upper left block matrix. It is denoted by either GL(F) or GL(∞, F), and can also be interpreted as invertible infinite matrices which differ from the identity matrix in only finitely many places.[12]It is used in algebraic K-theory to define K1, and over the reals has a well-understood topology, thanks to Bott periodicity.It should not be confused with the space of (bounded) invertible operators on a Hilbert space, which is a larger group, and topologically much simpler, namely contractible – see Kuiper's theorem.
Geometry
Geometry (from the Ancient Greek: γεωμετρία; geo- "earth", -metron "measurement") is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. A mathematician who works in the field of geometry is called a geometer.Geometry arose independently in a number of early cultures as a practical way for dealing with lengths, areas, and volumes. Geometry began to see elements of formal mathematical science emerging in the West as early as the 6th century BC.[1] By the 3rd century BC, geometry was put into an axiomatic form by Euclid, whose treatment, Euclid's Elements, set a standard for many centuries to follow.[2] Geometry arose independently in India, with texts providing rules for geometric constructions appearing as early as the 3rd century BC.[3] Islamic scientists preserved Greek ideas and expanded on them during the Middle Ages.[4] By the early 17th century, geometry had been put on a solid analytic footing by mathematicians such as René Descartes and Pierre de Fermat. Since then, and into modern times, geometry has expanded into non-Euclidean geometry and manifolds, describing spaces that lie beyond the normal range of human experience.[5]While geometry has evolved significantly throughout the years, there are some general concepts that are more or less fundamental to geometry. These include the concepts of points, lines, planes, surfaces, angles, and curves, as well as the more advanced notions of manifolds and topology or metric.[6]Geometry has applications to many fields, including art, architecture, physics, as well as to other branches of mathematics.Contemporary geometry has many subfields:The earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC.[8][9] Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000–1800 BC) and Moscow Papyrus (c. 1890 BC), the Babylonian clay tablets such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum.[10] Later clay tablets (350–50 BC) demonstrate that Babylonian astronomers implemented trapezoid procedures for computing Jupiter's position and motion within time-velocity space. These geometric procedures anticipated the Oxford Calculators, including the mean speed theorem, by 14 centuries.[11] South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.[12][13]In the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore.  He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.[1]  Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem,[14]  though the statement of the theorem has a long history.[15][16] Eudoxus (408–c. 355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures,[17] as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time,[18] introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[19] The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today.[20] Archimedes (c. 287–212 BC) of Syracuse used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of Pi.[21] He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.Indian mathematicians also made many important contributions in geometry. The Satapatha Brahmana (3rd century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras.[3] According to (Hayashi 2005, p. 363), the Śulba Sūtras contain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples,[22] which are particular cases of Diophantine equations.[23]In the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero."[24] Aryabhata's Aryabhatiya (499) includes the computation of areas and volumes.Brahmagupta wrote his astronomical work Brāhma Sphuṭa Siddhānta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain).[25] In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).[25]In the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry.[26][27] Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra.[28] Thābit ibn Qurra (known as Thebit in Latin) (836–901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry.[4] Omar Khayyám (1048–1131) found geometric solutions to cubic equations.[29]  The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were early results in hyperbolic geometry, and along with their alternative postulates, such as Playfair's axiom, these works had a considerable influence on the development of non-Euclidean geometry among later European geometers, including Witelo (c. 1230–c. 1314), Gersonides (1288–1344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri.[30]In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661). Projective geometry is a geometry without measurement or parallel lines, just the study of how points are related to each other.Two developments in geometry in the 19th century changed the way it had been studied previously. These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, János Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen Programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826–1866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincaré, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of "space" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.The following are some of the most important concepts in geometry.[6][7]Euclid took an abstract approach to geometry in his Elements, one of the most influential books ever written. Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes. He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry. At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792–1856), János Bolyai (1802–1860), Carl Friedrich Gauss (1777–1855) and others led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862–1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.Points are considered fundamental objects in Euclidean geometry. They have been defined in a variety of ways, including Euclid's definition as 'that which has no part'[31] and through the use of algebra or nested sets.[32] In many areas of geometry, such as analytic geometry, differential geometry, and topology, all objects are considered to be built up from points. However, there has been some study of geometry without reference to points.[33]Euclid described a line as "breadthless length" which "lies equally with respect to the points on itself".[31] In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation,[34] but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.[35] In differential geometry, a  geodesic is a generalization of the notion of a line  to curved spaces.[36]A plane is a flat, two-dimensional surface that extends infinitely far.[31] Planes are used in every area of geometry. For instance, planes can be studied as a topological surface without reference to distances or angles;[37] it can be studied as an affine space, where collinearity and ratios can be studied but not distances;[38] it can be studied as the complex plane using techniques of complex analysis;[39] and so on.Euclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other.[31] In modern terms, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.[40]In Euclidean geometry, angles are used to study polygons and triangles, as well as forming an object of study in their own right.[31] The study of the angles of a triangle or of angles in a unit circle forms the basis of trigonometry.[41]In differential geometry and calculus, the angles between plane curves or space curves or surfaces can be calculated using the derivative.[42][43]A curve is a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are called plane curves and those in 3-dimensional space are called space curves.[44]In topology, a curve is defined by a function from an interval of the real numbers to another space.[37] In differential geometry, the same definition is used, but the defining function is required to be differentiable [45] Algebraic geometry studies algebraic curves, which are defined as algebraic varieties of dimension one.[46]A surface is a two-dimensional object, such as a sphere or paraboloid.[47] In differential geometry[45] and topology,[37] surfaces are described by two-dimensional 'patches' (or neighborhoods) that are assembled by diffeomorphisms or homeomorphisms, respectively. In algebraic geometry, surfaces are described by polynomial equations.[46]A manifold is a generalization of the concepts of curve and surface. In topology, a manifold is a topological space where every point has a neighborhood that is homeomorphic to Euclidean space.[37] In differential geometry, a differentiable manifold is a space where each neighborhood is diffeomorphic to Euclidean space.[45]Manifolds are used extensively in physics, including in general relativity and string theory[48]A topology is a mathematical structure on a set that tells how elements of the set relate spatially to each other.[37] The best-known examples of topologies come from metrics, which are ways of measuring distances between points.[49] For instance, the Euclidean metric measures the distance between points in the Euclidean plane, while the hyperbolic metric measures the distance in the hyperbolic plane. Other important examples of metrics include the Lorentz metric of special relativity and the semi-Riemannian metrics of general relativity.[50]Classical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments allowed in geometric constructions are the compass and straightedge. Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using parabolas and other curves, as well as mechanical devices, were found.Where the traditional geometry allowed dimensions 1 (a line), 2 (a plane) and 3 (our ambient world conceived of as three-dimensional space), mathematicians have used higher dimensions for nearly two centuries. The concept of dimension has gone through stages of being any natural number n, to being possibly infinite with the introduction of Hilbert space, to being any positive real number in fractal geometry. Dimension theory is a technical area, initially within general topology, that discusses definitions; in common with most mathematical ideas, dimension is now defined rather than an intuition. Connected topological manifolds have a well-defined dimension; this is a theorem (invariance of domain) rather than anything a priori.The issue of dimension still matters to geometry as many classic questions still lack complete answers. For instance, many open problems in topology depend on the dimension of an object for the result. In physics, dimensions 3 of space and 4 of space-time are special cases in geometric topology, and dimensions 10 and 11 are key ideas in string theory. Currently, the existence of the theoretical dimensions is purely defined by technical reasons; it is likely that further research may result in a geometric reason for the significance of 10 or 11 dimensions in the theory, lending credibility or possibly disproving string theory.The theme of symmetry in geometry is nearly as old as the science of geometry itself. Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers and were investigated in detail before the time of Euclid. Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of M.C. Escher. Nonetheless, it was not until the second half of 19th century that the unifying role of symmetry in foundations of geometry was recognized. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is. Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines. However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' proved most influential. Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory, the latter in Lie theory and Riemannian geometry.A different type of symmetry is the principle of duality in projective geometry (see Duality (projective geometry)) among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and you will get an equally true theorem. A similar and closely related form of duality exists between a vector space and its dual space.In the nearly two thousand years since Euclid, while the range of geometrical questions asked and answered inevitably expanded, the basic understanding of space remained essentially the same. Immanuel Kant argued that there is only one, absolute, geometry, which is known to be true a priori by an inner faculty of mind: Euclidean geometry was synthetic a priori.[51]  This dominant view was overturned by the revolutionary discovery of non-Euclidean geometry in the works of Bolyai, Lobachevsky, and Gauss (who never published his theory). They demonstrated that ordinary Euclidean space is only one possibility for development of geometry. A broad vision of the subject of geometry was then expressed  by Riemann in his 1867 inauguration lecture Über die Hypothesen, welche der Geometrie zu Grunde liegen (On the hypotheses on which geometry is based),[52] published only after his death. Riemann's new idea of space proved crucial in Einstein's general relativity theory, and Riemannian geometry, that considers very general spaces in which the notion of length is defined, is a mainstay of modern geometry.Euclidean geometry has become closely connected with computational geometry, computer graphics, convex geometry, incidence geometry, finite geometry, discrete geometry, and some areas of combinatorics. Attention was given to further work on Euclidean geometry and the Euclidean groups by crystallography and the work of H. S. M. Coxeter, and can be seen in theories of Coxeter groups and polytopes. Geometric group theory is an expanding area of the theory of more general discrete groups, drawing on geometric models and algebraic techniques.Differential geometry has been of increasing importance to mathematical physics due to Einstein's general relativity postulation that the universe is curved. Contemporary differential geometry is intrinsic, meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point, and not a priori parts of some ambient flat Euclidean space.The field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms. This has often been expressed in the form of the dictum 'topology is rubber-sheet geometry'. Contemporary geometric topology and differential topology, and particular subfields such as Morse theory, would be counted by most mathematicians as part of geometry. Algebraic topology and general topology have gone their own ways.[citation needed][dubious  – discuss]The field of algebraic geometry is the modern incarnation of the Cartesian geometry of co-ordinates. From late 1950s through mid-1970s it had undergone major foundational development, largely due to work of Jean-Pierre Serre and Alexander Grothendieck. This led to the introduction of schemes and greater emphasis on topological methods, including various cohomology theories. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.The study of low-dimensional algebraic varieties, algebraic curves, algebraic surfaces and algebraic varieties of dimension 3 ("algebraic threefolds"), has been far advanced. Gröbner basis theory and real algebraic geometry are among more applied subfields of modern algebraic geometry. Arithmetic geometry is an active field combining algebraic geometry and number theory. Other directions of research involve moduli spaces and complex geometry. Algebro-geometric methods are commonly applied in string and brane  theory.Geometry has found applications in many fields, some of which are described below.Mathematics and art are related in a variety of ways. For instance, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry.Mathematics and architecture are related, since, as with other arts, architects use mathematics for several reasons. Apart from the mathematics needed when engineering buildings, architects use geometry: to define the spatial form of a building; from the Pythagoreans of the sixth century BC onwards, to create forms considered harmonious, and thus to lay out buildings and their surroundings according to mathematical, aesthetic and sometimes religious principles; to decorate buildings with mathematical objects such as tessellations; and to meet environmental goals, such as to minimise wind speeds around the bases of tall buildings.The field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.Modern geometry has many ties to physics as is exemplified by the links between pseudo-Riemannian geometry and general relativity. One of the youngest physical theories, string theory, is also very geometric in flavour.Geometry has also had a large effect on other areas of mathematics. For instance, the introduction of coordinates by René Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century.  The subject of geometry was further enriched by the study of the intrinsic structure of geometric objects that originated with Euler and Gauss and led to the creation of topology and differential geometry.An important area of application is number theory. In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths, which contradicted their philosophical views, made them abandon abstract numbers in favor of concrete geometric quantities, such as length and area of figures. Since the 19th century, geometry has been used for solving problems in number theory, for example through the geometry of numbers or, more recently, scheme theory, which is used in Wiles's proof of Fermat's Last Theorem.While the visual nature of geometry makes it initially more accessible than other mathematical areas such as algebra or number theory, geometric language is also used in contexts far removed from its traditional, Euclidean provenance (for example, in fractal geometry and algebraic geometry).[53]Analytic geometry applies methods of algebra to geometric questions, typically by relating geometric curves to algebraic equations. These ideas played a key role in the development of calculus in the 17th century and led to the discovery of many new properties of plane curves. Modern algebraic geometry considers similar questions on a vastly more abstract level.Leonhard Euler, in studying problems like the Seven Bridges of Königsberg, considered the most fundamental properties of geometric figures based solely on shape, independent of their metric properties. Euler called this new branch of geometry geometria situs (geometry of place), but it is now known as topology. Topology grew out of geometry, but turned into a large independent discipline. It does not differentiate between objects that can be continuously deformed into each other. The objects may nevertheless retain some geometry, as in the case of hyperbolic knots."Three scientists, Ibn al-Haytham, Khayyam, and al-Tusi, had made the most considerable contribution to this branch of geometry whose importance came to be completely recognized only in the 19th century. In essence, their propositions concerning the properties of quadrangles which they considered, assuming that some of the angles of these figures were acute of obtuse, embodied the first few theorems of the hyperbolic and the elliptic geometries. Their other proposals showed that various geometric statements were equivalent to the Euclidean postulate V. It is extremely important that these scholars established the mutual connection between this postulate and the sum of the angles of a triangle and a quadrangle. By their works on the theory of parallel lines Arab mathematicians directly influenced the relevant investigations of their European counterparts. The first European attempt to prove the postulate on parallel lines – made by Witelo, the Polish scientists of the 13th century, while revising Ibn al-Haytham's Book of Optics (Kitab al-Manazir) – was undoubtedly prompted by Arabic sources. The proofs put forward in the 14th century by the Jewish scholar Levi ben Gerson, who lived in southern France, and by the above-mentioned Alfonso from Spain directly border on Ibn al-Haytham's demonstration. Above, we have demonstrated that Pseudo-Tusi's Exposition of Euclid had stimulated both J. Wallis's and G. Saccheri's studies of the theory of parallel lines."
Affine space
In mathematics, an affine space is a geometric structure that generalizes some of the properties of Euclidean spaces in such a way that these are independent of the concepts of distance and measure of angles, keeping only the properties related to parallelism and ratio of lengths for parallel line segments.In an affine space, there is no distinguished point that serves as an origin. Hence, no vector has a fixed origin and no vector can be uniquely associated to a point. In an affine space, there are instead displacement vectors, also called translation vectors or simply translations, between two points of the space.[1] Thus it makes sense to subtract two points of the space, giving a translation vector, but it does not make sense to add two points of the space. Likewise, it makes sense to add a displacement vector to a point of an affine space, resulting in a new point translated from the starting point by that vector.Any vector space may be considered as an affine space, and this amounts to forgetting the special role played by the zero vector. In this case, the elements of the vector space may be viewed either as points of the affine space or as displacement vectors or translations. When considered as a point, the zero vector is called the origin. Adding a fixed vector to the elements of a linear subspace of a vector space produces an affine subspace. One commonly says that this affine subspace has been obtained by translating (away from the origin) the linear subspace by the translation vector. In finite dimensions, such an affine subspace is the solution set of an inhomogeneous linear system. The displacement vectors for that affine space are the solutions of the corresponding homogeneous linear system, which is a linear subspace. Linear subspaces, in contrast, always contain the origin of the vector space.The dimension of an affine space is defined as the dimension of the vector space of its translations. An affine space of dimension one is an affine line. An affine space of dimension 2 is an affine plane. An affine subspace of dimension n – 1 in an affine space or a vector space of dimension n is an affine hyperplane.The following characterization may be easier to understand than the usual formal definition: an affine space is what is left of a vector space after you've forgotten which point is the origin (or, in the words of the French mathematician Marcel Berger, "An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps"[2]). Imagine that Alice knows that a certain point is the actual origin, but Bob believes that another point — call it p — is the origin. Two vectors, a and b, are to be added. Bob draws an arrow from point p to point a and another arrow from point p to point b, and completes the parallelogram to find what Bob thinks is a + b, but Alice knows that he has actually computedSimilarly, Alice and Bob may evaluate any linear combination of a and b, or of any finite set of vectors, and will generally get different answers. However, if the sum of the coefficients in a linear combination is 1, then Alice and Bob will arrive at the same answer.If Alice travels tothen Bob can similarly travel toUnder this condition, for all coefficients λ + (1 − λ) = 1, Alice and Bob describe the same point with the same linear combination, despite using different origins.While only Alice knows the "linear structure", both Alice and Bob know the "affine structure"—i.e. the values of affine combinations, defined as linear combinations in which the sum of the coefficients is 1. A set with an affine structure is an affine space.that has the following properties.[4][5][6]The first two properties are simply defining properties of a (right) group action. The third property characterizes free and transitive actions, the onto character coming from transitivity, and then the injective character follows from the action being free. There is a fourth property that follows from 1, 2, 3 above:Property 3 is often used in the following equivalent form.Another way to express the definition is that an affine space is a principal homogeneous space for the action of the additive group of a vector space. Homogeneous spaces are by definition endowed with a transitive group action, and for a principal homogeneous space such a transitive action is by definition free.Existence follows from the transitivity of the action, and uniqueness follows because the action is free.This subtraction has the two following properties, called Weyl's axioms:[7]In Euclidean geometry, the second Weyl's axiom is commonly called the parallelogram rule.The affine subspaces of A are the subsets of A of the formThe linear subspace associated with an affine subspace is often called its direction, and two subspaces that share the same direction are said to be parallel.This implies the following generalization of  Playfair's axiom: Given a direction V, for any point a of A there is one and only one affine subspace of direction V, which passes through a, namely the subspace a + V.The term parallel is also used for two affine subspaces such that the direction of one is included in the direction of the other.such thatEvery vector space V may be considered as an affine space over itself. This means that every element of V may be considered either as a point or as a vector. This affine set is sometimes denoted (V, V) for emphasizing the double role of the elements of V. When considered as a point, the zero vector is commonly denoted o (or O, when upper-case letters are used for points) and called the origin.Euclidean spaces (including the one-dimensional line, two-dimensional plane, and three-dimensional space commonly studied in elementary geometry, as well as higher-dimensional analogues) are affine spaces.Indeed, in most modern definitions, a Euclidean space is defined to be an affine space, such that the associated vector space is a real inner product space of finite dimension, that is a vector space over the reals with a positive-definite quadratic form q(x). The inner product of two vectors x and y is the value of the symmetric bilinear formThe usual Euclidean distance between two points A and B is In older definition of Euclidean spaces through synthetic geometry, vectors are defined as equivalence classes of ordered pairs of points under equipollence (the pairs (A, B) and (C, D) are equipollent if the points A, B, D, C (in this order) form a parallelogram). It is straightforward to verify that the vectors form a vector space, the square of the Euclidean distance is a quadratic form on the space of vectors, and the two definitions of Euclidean spaces are equivalent.In Euclidean geometry, the common phrase "affine property" refers to a property that can be proved in affine spaces, that is, it can be proved without using the quadratic form and its associated inner product. In other words, an affine property is a property that does not involve lengths and angles. Typical examples are parallelism, and the definition of a tangent. A non-example is the definition of a normal.Equivalently, an affine property is a property that is invariant under affine transformations of the Euclidean space.Thus this sum is independent of the choice of the origin, and the resulting vector is denotedone writesFor any subset X of an affine space A, there is a smallest affine subspace that contains it, called the affine span of X. It is the intersection of all affine subspaces containing X, and its direction is the intersection of the directions of the affine subspaces that contain X.The affine span of X is the set of all (finite) affine combinations of points of X, and its direction is the linear span of the x − y for x and y in X. If one chooses a particular point x0, the direction of the affine span of X is also the linear span of the x – x0 for x in X.One says also that the affine span of X is generated by X and that X is a generating set of its affine span.A set X of points of an affine space is said affinely independent or, simply, independent, if the affine span of any strict subset of X is a strict subset of the affine span of X. An affine basis, or barycentric frame (see § Barycentric coordinates, below) of an affine space is a generating set that is also independent (that is a minimal generating set).Recall the dimension of an affine space is the dimension of its associated vector space. The bases of an affine space of finite dimension n are the independent subsets of n + 1 elements, or, equivalently, the generating subsets of n + 1 elements. Equivalently, {x0, …, xn} is an affine basis of an affine space if and only if {x1 − x0, …, xn − x0} is a linear basis of the associated vector space.There are two strongly related kinds of coordinate systems that may be defined on affine spaces.andFor affine spaces of infinite dimension, the same definition applies, using only finite sums. This means that for each point, only a finite number of coordinates are non-zero.or equivalentlyExample: In Euclidean geometry, Cartesian coordinates are affine coordinates relative to an orthonormal frame, that is an affine frame (o, v1, …, vn) such that (v1, …, vn) is an orthonormal basis.Barycentric coordinates and affine coordinates are strongly related, and may be considered as equivalent.In fact, given a barycentric frameone deduces immediately the affine frameand, ifare the barycentric coordinates of a point over the barycentric frame, then the affine coordinates of the same point over the affine frame areConversely, ifis an affine frame, thenis a barycentric frame. Ifare the affine coordinates of a point over the affine frame, then its barycentric coordinates over the barycentric frame areTherefore, barycentric and affine coordinates are almost equivalent. In most applications, affine coordinates are preferred, as involving less coordinates that are independent. However, in the situations where the important points of the studied problem are affinity independent, barycentric coordinates may lead to simpler computation, as in the following example.The vertices of a non-flat triangle form an affine basis of the Euclidean plane. The barycentric coordinates allows easy characterization of the elements of the triangle that do not involve angles or distance:The vertices are the points of barycentric coordinates (1, 0, 0),  (0, 1, 0) and  (0, 0, 1). The lines supporting the edges are the points that have a zero coordinate. The edges themselves are the points that have a zero coordinate and two nonnegative coordinates. The interior of the triangle are the points whose all coordinates are positive. The medians are the points that have two equal coordinates, and the centroid is the point of coordinates (1/3, 1/3, 1/3).Letbe an affine homomorphism, withas associated linear map.An important example is the projection parallel to some direction onto an affine subspace. The importance of this example lies in the fact that Euclidean spaces are affine spaces, and that this kind of projections is fundamental in Euclidean geometry.for x and y in E.The image of this projection is  F, and its fibers are the subspaces of direction  D.Although kernels are not defined for affine spaces, quotient spaces are defined. This results from the fact that "belonging to the same fiber of an affine homomorphism" is an equivalence relation.Affine space is usually studied as analytic geometry using coordinates, or equivalently vector spaces. It can also be studied as synthetic geometry by writing down axioms, though this approach is much less common. There are several different systems of axioms for affine space.Coxeter (1969, p. 192) axiomatizes affine geometry (over the reals) as ordered geometry together with an affine form of Desargues's theorem and an axiom stating that in a plane there is at most one line through a given point not meeting a given line.Affine planes satisfy the following axioms (Cameron 1991, chapter 2):(in which two lines are called parallel if they are equal ordisjoint):As well as affine planes over fields (or division rings), there are also many non-Desarguesian planes satisfying these axioms. (Cameron 1991, chapter 3) gives axioms for higher-dimensional affine spaces.Affine spaces are subspaces of projective spaces: an affine plane can be obtained from any projective plane by removing a line and all the points on it, and conversely any affine plane can be used to construct a projective plane as a closure by adding a line at infinity whose points correspond to equivalence classes of parallel lines.Further, transformations of projective space that preserve affine space (equivalently, that leave the hyperplane at infinity invariant as a set) yield transformations of affine space. Conversely, any affine linear transformation extends uniquely to a projective linear transformation, so the affine group is a subgroup of the projective group. For instance, Möbius transformations (transformations of the complex projective line, or Riemann sphere) are affine (transformations of the complex plane) if and only if they fix the point at infinity.In algebraic geometry, an affine variety (or, more generally, an affine algebraic set) is defined as the subset of an affine space that is the set of the common zeros of a set of so-called polynomial functions over the affine space. For defining a polynomial function over the affine space, one has to choose an affine coordinate system. Then, a polynomial function is a function such that the image of any point is the value of some multivariate polynomial function of the coordinates of the point. As a change of affine coordinates may be expressed by linear functions (more precisely affine functions) of the coordinates, this definition is independent of a particular choice of coordinates.As the whole affine space is the set of the common zeros of the zero polynomial, affine spaces are affine algebraic varieties.Affine spaces over topological fields, such as the real or the complex numbers, have a natural topology. The Zariski topology, which is defined for affine spaces over any field, allows use of topological methods in any case. Zariski topology is the unique topology on an affine space whose closed sets are affine algebraic sets (that is sets of the common zeros of polynomials functions over the affine set). As, over a topological field, polynomial functions are continuous, every Zariski closed set is closed for the usual topology, if any. In other words, over a topological field, Zariski topology is coarser than the natural topology.The case of an algebraically closed ground field is especially important in algebraic geometry, because, in this case, the homeomorphism above is a map between the affine space and the set of all maximal ideals of the ring of functions (this is Hilbert's Nullstellensatz).This is the starting idea of scheme theory of Grothendieck, which consists, for studying algebraic varieties, of considering as "points", not only the points of the affine space, but also all the prime ideals of the spectrum. This allows gluing together algebraic varieties in a similar way as, for manifolds, charts are glued together for building a manifold.
Vector space
A vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called axioms, listed below.Euclidean vectors are an example of a vector space.  They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.Vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.Historically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.Today, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.The concept of vector space will first be explained by describing two particular examples:The first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the sum of the two arrows and is denoted v + w. In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. It is denoted av. When a is negative, av is defined as the arrow pointing in the opposite direction, instead.The following shows a few examples: if a = 2, the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). Equivalently, 2w is the sum w + w. Moreover, (−1)v = −v has the opposite direction and the same length as v (blue vector pointing down in the right image).A second key example of a vector space is provided by pairs of real numbers x and y. (The order of the components x and y is significant, so such a pair is also called an ordered pair.) Such a pair is written as (x, y). The sum of two such pairs and multiplication of a pair with a number is defined as follows:and The first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.In this article, vectors are represented in boldface to distinguish them from scalars.[nb 1]A vector space over a field F is a set V together with two operations that satisfy the eight axioms listed below.Elements of V are commonly called vectors. Elements of F are commonly called scalars.In the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.To qualify as a vector space, the set V and the operations of addition and multiplication must adhere to a number of requirements called axioms.[1] In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.These axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:Likewise, in the geometric example of vectors as arrows, v + w = w + v since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.Subtraction of two vectors and division by a (non-zero) scalar can be defined asWhen the scalar field F is the real numbers R, the vector space is called a real vector space. When the scalar field is the complex numbers C, the vector space is called a complex vector space. These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector spaces or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.[nb 3] For example, rational numbers form a field.In contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.Vector addition and scalar multiplication are operations, satisfying the closure property:  u + v and av are in V for all a in F, and u, v in V. Some older sources mention these properties as separate axioms.[2]In the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an F-module structure. In other words, there is a ring homomorphism f from the field F into the endomorphism ring of the group of vectors. Then scalar multiplication av is defined as (f(a))(v).[3]There are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example, the zero vector 0 of V and the additive inverse −v of any vector v are unique. Further properties follow by employing also the distributive law for the scalar multiplication, for example av equals 0 if and only if a equals 0 or v equals 0.Vector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve.[4] In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors.[5] His work was then used in the conception of barycentric coordinates by Möbius in 1827.[6] In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.[7]The definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter.[8] They are elements in R2, R4, and R8; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.An important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920.[11] At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of p-integrable functions and Hilbert spaces.[12] Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.The simplest example of a vector space over a field F is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed ofn-tuples (sequences of length n) of elements of F, such asA vector space composed of all the n-tuples of a field F is known as a coordinate space, usually denoted Fn. The case n = 1 is the above-mentioned simplest example, in which the field F is also regarded as a vector space over itself. The case F = R and n = 2 was discussed in the introduction above.The set of complex numbers C, i.e., numbers that can be written in the form x + iy for real numbers x and y where i is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: (x + iy) + (a + ib) = (x + a) + i(y + b) and c ⋅ (x + iy) = (c ⋅ x) + i(c ⋅ y) for real numbers x, y, a, b and c.  The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.In fact, the example of complex numbers is essentially the same (i.e., it is isomorphic) to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number x + i y as representing the ordered pair (x, y) in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.Functions from any fixed set Ω to a field F also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions f and g is the function (f + g) given byand similarly for multiplication. Such function spaces occur in many geometric situations, when Ω is the real line or an interval, or other subsets of R. Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property.[15]  Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space F[x] is given by polynomial functions:Systems of homogeneous linear equations are closely tied to vector spaces.[17] For example, the solutions of are given by triples with arbitrary a, b = a/2, and c = −5a/2. They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namelyyields f(x) = a e−x + bx e−x, where a and b are arbitrary constants, and ex is the natural exponential function.Bases allow one to represent vectors by a sequence of scalars called coordinates or components. A basis is a (finite or infinite) set B = {bi}i ∈ I of vectors bi, for convenience often indexed by some index set I, that spans the whole space and is linearly independent. "Spanning the whole space" means that any vector v can be expressed as a finite sum (called a linear combination) of the basis elements:    (1)where the ak are scalars, called the coordinates (or the components) of the vector v with respect to the basis B, and bik (k = 1, ..., n) elements of B. Linear independence means that the coordinates ak are uniquely determined for any vector in the vector space.For example, the coordinate vectors e1 = (1, 0, ..., 0), e2 = (0, 1, 0, ..., 0), to en = (0, 0, ..., 0, 1), form a basis of Fn, called the standard basis, since any vector (x1, x2, ..., xn) can be uniquely expressed as a linear combination of these vectors:The corresponding coordinates x1, x2, ..., xn are just the Cartesian coordinates of the vector.Every vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice.[18] Given the other axioms of Zermelo–Fraenkel set theory, the existence of bases is equivalent to the axiom of choice.[19] The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. Dimension theorem for vector spaces).[20] It is called the dimension of the vector space, denoted by dim V. If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.[21]The dimension of the coordinate space Fn is n, by the basis exhibited above. The dimension of the polynomial ring F[x] introduced above is countably infinite, a basis is given by 1, x, x2, ... A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite.[nb 4] Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation.[22] For example, the solution space for the above equation is generated by e−x and xe−x. These two functions are linearly independent over R, so the dimension of this space is two, as is the degree of the equation.A field extension over the rationals Q can be thought of as a vector space over Q (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of Q, and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension Q(α) over Q depends on α. If α satisfies some polynomial equationThe relation of two vector spaces can be expressed by linear map or linear transformation. They are functions that reflect the vector space structure—i.e., they preserve sums and scalar multiplication:An isomorphism is a linear map f : V → W such that there exists an inverse map g : W → V, which is a map such that the two possible compositions f ∘ g : W → W and g ∘ f : V → V are identity maps. Equivalently, f is both one-to-one (injective) and onto (surjective).[26]  If there exists an isomorphism between V and W, the two spaces are said to be isomorphic; they are then essentially identical as vector spaces, since all identities holding in V are, via f, transported to similar ones in W, and vice versa via g.For example, the "arrows in the plane" and "ordered pairs of numbers" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the x- and y-component of the arrow, as shown in the image at the right. Conversely, given a pair (x, y), the arrow going by x to the right (or to the left, if x is negative), and y up (down, if y is negative) turns back the arrow v.Linear maps V → W between two vector spaces form a vector space HomF(V, W), also denoted L(V, W).[27] The space of linear maps from V to F is called the dual vector space, denoted V∗.[28] Via the injective natural map V → V∗∗, any vector space can be embedded into its bidual; the map is an isomorphism if and only if the space is finite-dimensional.[29]Once a basis of V is chosen, linear maps f : V → W are completely determined by specifying the images of the basis vectors, because any element of V is expressed uniquely as a linear combination of them.[30] If dim V = dim W, a 1-to-1 correspondence between fixed bases of V and W gives rise to a linear map that maps any basis element of V to the corresponding basis element of W. It is an isomorphism, by its very definition.[31] Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is completely classified (up to isomorphism) by its dimension, a single number. In particular, any n-dimensional F-vector space V is isomorphic to Fn. There is, however, no "canonical" or preferred isomorphism; actually an isomorphism φ : Fn → V is equivalent to the choice of a basis of V, by mapping the standard basis of Fn to V, via φ. The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context; see below.Matrices are a useful notion to encode linear maps.[32]  They are written as a rectangular array of scalars as in the image at the right. Any m-by-n matrix A gives rise to a linear map from Fn to Fm, by the followingor, using the matrix multiplication of the matrix A with the coordinate vector x:Moreover, after choosing bases of V and W, any linear map f : V → W is uniquely represented by a matrix via this assignment.[33]The determinant det (A) of a square matrix A is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero.[34] The linear transformation of Rn corresponding to a real n-by-n matrix is orientation preserving if and only if its determinant is positive.Endomorphisms, linear maps f : V → V, are particularly important since in this case vectors v can be compared with their image under f, f(v). Any nonzero vector v satisfying λv = f(v), where λ is a scalar, is called an eigenvector of f with eigenvalue λ.[nb 5][35] Equivalently, v is an element of the kernel of the difference f − λ · Id (where Id is the identity map V → V). If V is finite-dimensional, this can be rephrased using determinants: f having eigenvalue λ is equivalent toBy spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in λ, called the characteristic polynomial of f.[36]  If the field F is large enough to contain a zero of this polynomial (which automatically happens for F algebraically closed, such as F = C) any linear map has at least one eigenvector. The vector space V may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map.[37][nb 6] The set of all eigenvectors corresponding to a particular eigenvalue of f forms a vector space known as the eigenspace corresponding to the eigenvalue (and f) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.In addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object X by specifying the linear maps from X to any other vector space.A nonempty subset W of a vector space V that is closed under addition and scalar multiplication (and therefore contains the 0-vector of V) is called a linear subspace of V, or simply a subspace of V, when the ambient space is unambiguously a vector space.[38][nb 7] Subspaces of V are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set S of vectors is called its span, and it is the smallest subspace of V containing the set S. Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of S.[39]A linear subspace of dimension 1 is a vector line.  A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension n, a vector hyperplane is thus a subspace of dimension n – 1.The counterpart to subspaces are quotient vector spaces.[40] Given any subspace W ⊂ V, the quotient space V/W ("V modulo W") is defined as follows: as a set, it consists of v + W = {v + w : w ∈ W}, where v is an arbitrary vector in V. The sum of two such elements v1 + W and v2 + W is (v1 + v2) + W, and scalar multiplication is given by a · (v + W) = (a · v) + W. The key point in this definition is that v1 + W = v2 + W if and only if the difference of v1 and v2 lies in W.[nb 8] This way, the quotient space "forgets" information that is contained in the subspace W.The kernel ker(f) of a linear map f : V → W consists of vectors v that are mapped to 0 in W.[41] Both kernel and image im(f) = {f(v) : v ∈ V} are subspaces of V and W, respectively.[42] The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field F) is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups.[43] Because of this, many statements such as the first isomorphism theorem (also called rank–nullity theorem in matrix-related terms)and the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.An important example is the kernel of a linear map x ↦ Ax for some fixed matrix A, as above. The kernel of this map is the subspace of vectors x such that Ax = 0, which is precisely the set of solutions to the system of homogeneous linear equations belonging to A. This concept also extends to linear differential equationsIn the corresponding mapthe derivatives of the function f appear linearly (as opposed to f′′(x)2, for example). Since differentiation is a linear procedure (i.e., (f + g)′ = f′ + g ′ and (c·f)′ = c·f′ for a constant c) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation D(f) = 0 form a vector space (over R or C).The direct product of vector spaces and the direct sum of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.The tensor product V ⊗F W, or simply V ⊗ W, of two vector spaces V and W is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map g : V × W → X is called bilinear if g is linear in both variables v and w.  That is to say, for fixed w the map v ↦ g(v, w) is linear in the sense above and likewise for fixed v.The tensor product is a particular vector space that is a universal recipient of bilinear maps g, as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensorssubject to the rules These rules ensure that the map f from the V × W to V ⊗ W that maps a tuple (v, w) to v ⊗ w is bilinear. The universality states that given any vector space X and any bilinear map g : V × W → X, there exists a unique map u, shown in the diagram with a dotted arrow, whose composition with f equals g: u(v ⊗ w) = g(v, w).[46] This is called the universal property of the tensor product, an instance of the method—much used in advanced abstract algebra—to indirectly define objects by specifying maps from or to this object.From the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces per se do not offer a framework to deal with the question—crucial to analysis—whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.A vector space may be given a partial order ≤, under which some vectors can be compared.[47] For example, n-dimensional real space Rn can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functionswhere f+ denotes the positive part of f and f− the negative part.[48]Coordinate space Fn can be equipped with the standard dot product:In R2, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:Convergence questions are treated by considering vector spaces V carrying a compatible topology, a structure that allows one to talk about elements being close to each other.[51][52] Compatible here means that addition and scalar multiplication have to be continuous maps.  Roughly, if x and y in V, and a in F vary by a bounded amount, then so do x + y and ax.[nb 9] To make sense of specifying the amount a scalar changes, the field F also has to carry a topology in this context; a common choice are the reals or the complex numbers.In such topological vector spaces one can consider series of vectors.  The infinite sumdenotes the limit of the corresponding finite partial sums of the sequence (fi)i∈N of elements of V. For example, the fi could be (real or complex) functions belonging to some function space V, in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.A way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem.[53]  In contrast, the space of all continuous functions on [0,1] with the same topology is complete.[54] A norm gives rise to a topology by defining that a sequence of vectors vn converges to v if and only ifBanach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study—a key piece of functional analysis—focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence.[55] The image at the right shows the equivalence of the 1-norm and ∞-norm on R2: as the unit "balls" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.From a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) V → W, maps between topological vector spaces are required to be continuous.[56] In particular, the (topological) dual space V∗ consists of continuous functionals V → R (or to C). The fundamental Hahn–Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.[57]Banach spaces, introduced by Stefan Banach, are complete normed vector spaces.[58] Imposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.[60]Complete inner product spaces are known as Hilbert spaces, in honor of David Hilbert.[61]The Hilbert space L2(Ω), with inner product given byBy definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions fn with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions f by polynomials.[63] By the Stone–Weierstrass theorem, every continuous function on [a, b] can be approximated as closely as desired by a polynomial.[64] A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what "basic functions", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space H, in the sense that the closure of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a basis of H, its cardinality is known as the Hilbert space dimension.[nb 13] Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram–Schmidt process, it enables one to construct a basis of orthogonal vectors.[65] Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.The solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal.[66] As an example from physics, the time-dependent Schrödinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions.[67] Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.[68]General vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an algebra over a field.[69] Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone–Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.Commutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.[70]Another crucial example are Lie algebras, which are neither commutative nor associative, but the failure to be so is limited by the constraints ([x, y] denotes the product of x and y):Examples include the vector space of n-by-n matrices, with [x, y] = xy − yx, the commutator of two matrices, and R3, endowed with the cross product.The tensor algebra T(V) is a formal way of adding products to any vector space V to obtain an algebra.[72] As a vector space, it is spanned by symbols, called simple tensorsThe multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product ⊗, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between v1 ⊗ v2 and v2 ⊗ v1. Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing v1 ⊗ v2 = − v2 ⊗ v1 yields the exterior algebra.[73]When a field, F is explicitly stated, a common term used is F-algebra.Vector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods.[74] Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.[75]A distribution (or generalized function) is a linear map assigning a number to each "test" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space.[76] The latter space is endowed with a topology that takes into account not only f itself, but also all its higher derivatives. A standard example is the result of integrating a test function f over some domain Ω:When Ω = {p}, the set consisting of a single point, this reduces to the Dirac distribution, denoted by δ, which associates to a test function f its value at the p: δ(f) = f(p). Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax–Milgram theorem, a consequence of the Riesz representation theorem).[77]Resolving a periodic function into a sum of trigonometric functions forms a Fourier series, a technique much used in physics and engineering.[nb 14][78] The underlying vector space is usually the Hilbert space L2(0, 2π), for which the functions sin mx and cos mx (m an integer) form an orthogonal basis.[79] The Fourier expansion of an L2 function f isThe coefficients am and bm are called Fourier coefficients of f, and are calculated by the formulas[80]In physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum.[81] A complex-number form of Fourier series is also commonly used.[80] The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality.[82] Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.[83]Fourier series are used to solve boundary value problems in partial differential equations.[84] In 1822, Fourier first used this technique to solve the heat equation.[85] A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points.[86] The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression.[87] The JPEG image format is an application of the closely related discrete cosine transform.[88]The fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform.[89] It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences.[90] They in turn are applied in digital filters[91] and as a rapid multiplication algorithm for polynomials and large integers (Schönhage–Strassen algorithm).[92][93]The tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact.  The tangent plane is the best linear approximation, or linearization, of a surface at a point.[nb 15] Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The tangent space is the generalization to higher-dimensional differentiable manifolds.[94]Riemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product.[95] Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time.[96][97] The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.[98]A vector bundle is a family of vector spaces parametrized continuously by a topological space X.[94] More precisely, a vector bundle over X is a topological space E equipped with a continuous map such that for every x in X, the fiber π−1(x) is a vector space. The case dim V = 1 is called a line bundle. For any vector space V, the projection X × V → X makes the product X × V into a "trivial" vector bundle. Vector bundles over X are required to be locally a product of X and some (fixed) vector space V: for every x in X, there is a neighborhood U of x such that the restriction of π to π−1(U) is isomorphic[nb 16] to the trivial bundle U × V → U. Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space X) be "twisted" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle X × V). For example, the Möbius strip can be seen as a line bundle over the circle S1 (by identifying open intervals with the real line). It is, however, different from the cylinder S1 × R, because the latter is orientable whereas the former is not.[99]Properties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle S1 is globally isomorphic to S1 × R, since there is a global nonzero vector field on S1.[nb 17] In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere S2 which is everywhere nonzero.[100] K-theory studies the isomorphism classes of all vector bundles over some topological space.[101] In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.The cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.Modules are to rings what vector spaces are to fields: the same axioms, applied to a ring R instead of a field F, yield modules.[102] The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors.  Some authors use the term vector space to mean modules over a division ring.[103]  The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.Roughly, affine spaces are vector spaces whose origins are not specified.[104] More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the mapIf W is a vector space, then an affine subspace is a subset of W obtained by translating a linear subspace V by a fixed vector x ∈ W; this space is denoted by x + V (it is a coset of V in W) and consists of all vectors of the form x + v for v ∈ V. An important example is the space of solutions of a system of inhomogeneous linear equationsgeneralizing the homogeneous case b = 0 above.[105] The space of solutions is the affine subspace x + V where x is a particular solution of the equation, and V is the space of solutions of the homogeneous equation (the nullspace of A).The set of one-dimensional subspaces of a fixed finite-dimensional vector space V is known as projective space; it may be used to formalize the idea of parallel lines intersecting at infinity.[106] Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension k and flags of subspaces, respectively.   
Complex conjugate
In mathematics, the complex conjugate of a complex number is the number with an equal real part and an imaginary part equal in magnitude but opposite in sign.[1][2] For example, the complex conjugate of 3 + 4i is 3 − 4i.Complex conjugates are important for finding roots of polynomials. According to the complex conjugate root theorem, if a complex number is a root to a polynomial in one variable with real coefficients (such as the quadratic equation or the cubic equation), so is its conjugate.The following properties apply for all complex numbers z and w, unless stated otherwise, and can be proved by writing z and w in the form a + ib.A significant property of the complex conjugate is that a complex number is equal to its complex conjugate if its imaginary part is zero, that is, if the complex number is real.For any two complex numbers w,z:The penultimate relation is involution; i.e., the conjugate of the conjugate of a complex number z is z. The ultimate relation is the method of choice to compute the inverse of a complex number if it is given in rectangular coordinates.These uses of the conjugate of z as a variable are illustrated in Frank Morley's book Inversive Geometry (1933), written with his son Frank Vigor Morley.The other planar real algebras, dual numbers, and split-complex numbers are also analyzed using complex conjugation.Taking the conjugate transpose (or adjoint) of complex matrices generalizes complex conjugation. Even more general is the concept of adjoint operator for operators on (possibly infinite-dimensional) complex Hilbert spaces. All this is subsumed by the *-operations of C*-algebras.Note that all these generalizations are multiplicative only if the factors are reversed:Since the multiplication of planar real algebras is commutative, this reversal is not needed there.One example of this notion is the conjugate transpose operation of complex matrices defined above. It should be remarked that on generic complex vector spaces there is no canonical notion of complex conjugation.
Material point method
The material point method (MPM) is a numerical technique used to simulate the behavior of solids, liquids, gases, and any other continuum material. Especially, it is a robust spatial discretization method for simulating multi-phase (solid-fluid-gas) interactions. In the MPM, a continuum body is described by a number of small Lagrangian elements referred to as 'material points'. These material points are surrounded by a background mesh/grid that is used only to calculate gradient terms such as the deformation gradient. Unlike other mesh-based methods like the finite element method, finite volume method or finite difference method, the MPM is not a mesh based method and is instead categorized as a meshless/meshfree or continuum-based particle method, examples of which are smoothed particle hydrodynamics and peridynamics. Despite the presence of a background mesh, the MPM does not encounter the drawbacks of mesh-based methods (high deformation tangling, advection errors etc.) which makes it a promising and powerful tool in computational mechanics.The MPM was originally proposed, as an extension of a similar method known as FLIP (a further extension of a method called PIC) to computational solid dynamics, in the early 1990 by Professors Deborah L. Sulsky, Zhen Chen and Howard L. Schreyer at University of New Mexico. After this initial development, the MPM has been further developed both in the national labs as well as the University of New Mexico, Oregon State University, University of Utah and more across the US and the world. Recently the number of institutions researching the MPM has been growing with added popularity and awareness coming from various sources such as the MPM's use in the Disney film Frozen.An MPM simulation consists of the following stages:(Prior to the time integration phase)(During the time integration phase - explicit formulation)2. Material point quantities are extrapolated to grid nodes.3. Equations of motion are solved on the grid.5.Resetting of grid.The PIC was originally conceived to solve problems in fluid dynamics, and developed by Harlow at Los Alamos National Laboratory in 1957.[1] One of the first PIC codes was the Fluid-Implicit Particle (FLIP) program, which was created by Brackbill in 1986[2] and has been constantly in development ever since. Until the 1990s, the PIC method was used principally in fluid dynamics.Motivated by the need for better simulating penetration problems in solid dynamics, Sulsky, Chen and Schreyer started in 1993 to reformulate the PIC and develop the MPM, with funding from Sandia National Laboratories.[3] The original MPM was then further extended by Bardenhagen et al.. to include frictional contact,[4] which enabled the simulation of granular flow,[5] and by Nairn to include explicit cracks[6] and crack propagation (known as CRAMP).Recently, an MPM implementation based on a micro-polar Cosserat continuum [7] has been used to simulate high-shear granular flow, such as silo discharge. MPM's uses were further extended into Geotechnical engineering with the recent development of a quasi-static, implicit MPM solver which provides numerically stable analyses of large-deformation problems in Soil mechanics.[8]Annual workshops on the use of MPM are held at various locations in the United States. The Fifth MPM Workshop was held at Oregon State University, in Corvallis, OR, on April 2 and 3, 2009.The uses of the PIC or MPM method can be divided into two broad categories: firstly, there are many applications involving fluid dynamics, plasma physics, magnetohydrodynamics, and multiphase applications. The second category of applications comprises problems in solid mechanics.The PIC method has been used to simulate a wide range of fluid-solid interactions, including sea ice dynamics,[9] penetration of biological soft tissues,[10] fragmentation of gas-filled canisters,[11] dispersion of atmospheric pollutants,[12] multiscale simulations coupling molecular dynamics with MPM,[13][14] and fluid-membrane interactions.[15] In addition, the PIC-based FLIP code has been applied in magnetohydrodynamics and plasma processing tools, and simulations in astrophysics and free-surface flow.[16]As a result of a joint effort between UCLA's mathematics department and Walt Disney Animation Studios, MPM was successfully used to simulate snow in the 2013 computer-animated film Frozen.[17][18][19]MPM has also been used extensively in solid mechanics, to simulate impact, penetration, collision and rebound, as well as crack propagation.[20][21] MPM has also become a widely used method within the field of soil mechanics: it has been used to simulate granular flow, silo discharge, pile driving, bucket filling, and material failure; and to model soil stress distribution, compaction, and hardening. It is now being used in wood mechanics problems such as simulations of transverse compression on the cellular level including cell wall contact [22] (this work received the George Marra Award for paper of the year from the Society of Wood Science and Technology [1])One subset of numerical methods are Meshfree methods, which are defined as methods for which "a predefined mesh is not necessary, at least in field variable interpolation". Ideally, a meshfree method does not make use of a mesh "throughout the process of solving the problem governed by partial differential equations, on a given arbitrary domain, subject to all kinds of boundary conditions," although existing methods are not ideal and fail in at least one of these respects. Meshless methods, which are also sometimes called particle methods, share a "common feature that the history of state variables is traced at points (particles) which are not connected with any element mesh, the distortion of which is a source of numerical difficulties." As can be seen by these varying interpretations, some scientists consider MPM to be a meshless method, while others do not. All agree, however, that MPM is a particle method.The Arbitrary Lagrangian Eulerian (ALE) methods form another subset of numerical methods which includes MPM. Purely Lagrangian methods employ a framework in which a space is discretised into initial subvolumes, whose flowpaths are then charted over time. Purely Eulerian methods, on the other hand, employ a framework in which the motion of material is described relative to a mesh that remains fixed in space throughout the calculation. As the name indicates, ALE methods combine Lagrangian and Eulerian frames of reference.PIC methods may be based on either the strong form collocation or a weak form discretisation of the underlying partial differential equation (PDE). Those based on the strong form are properly referred to as finite-volume PIC methods. Those based on the weak form discretisation of PDEs may be called either PIC or MPM.MPM solvers can model problems in one, two, or three spatial dimensions, and can also model axisymmetric problems. MPM can be implemented to solve either quasi-static or dynamic equations of motion, depending on the type of problem that is to be modeled.The time-integration used for MPM may be either explicit or implicit. The advantage to implicit integration is guaranteed stability, even for large timesteps. On the other hand, explicit integration runs much faster and is easier to implement.Unlike FEM, MPM does not require periodical remeshing steps and remapping of state variables, and is therefore better suited to the modeling of large material deformations. In MPM, particles and not the mesh points store all the information on the state of the calculation. Therefore, no numerical error results from the mesh returning to its original position after each calculation cycle, and no remeshing algorithm is required.The particle basis of MPM allows it to treat crack propagation and other discontinuities better than FEM, which is known to impose the mesh orientation on crack propagation in a material. Also, particle methods are better at handling history-dependent constitutive models.Because in MPM nodes remain fixed on a regular grid, the calculation of gradients is trivial.In simulations with two or more phases it is rather easy to detect contact between entities, as particles can interact via the grid with other particles in the same body, with other solid bodies, and with fluids.MPM is more expensive in terms of storage than other methods, as MPM makes use of mesh as well as particle data. MPM is more computationally expensive than FEM, as the grid must be reset at the end of each MPM calculation step and reinitialised at the beginning of the following step. Spurious oscillation may occur as particles cross the boundaries of the mesh in MPM, although this effect can be minimized by using generalized interpolation methods (GIMP). In MPM as in FEM, the size and orientation of the mesh can impact the results of a calculation: for example, in MPM, strain localisation is known to be particularly sensitive to mesh refinement.A commercial package based on a meshless method is MPMsim.
Symmetric matrix
In linear algebra, a symmetric matrix is a square matrix that is equal to its transpose. Formally, matrix A is symmetric ifBecause equal matrices have equal dimensions, only square matrices can be symmetric.The entries of a symmetric matrix are symmetric with respect to the main diagonal. So if the entries are written as A = (aij), then aij = aji, for all indices i and j.The following 3 × 3 matrix is symmetric:Every square diagonal matrix is symmetric, since all off-diagonal elements are zero. Similarly in characteristic different from 2, each diagonal element of a skew-symmetric matrix must be zero, since each is its own negative.In linear algebra, a real symmetric matrix represents a self-adjoint operator[1] over a real inner product space. The corresponding object for a complex inner product space is a Hermitian matrix with complex-valued entries, which is equal to its conjugate transpose. Therefore, in linear algebra over the complex numbers, it is often assumed that a symmetric matrix refers to one which has real-valued entries.  Symmetric matrices appear naturally in a variety of applications, and typical numerical linear algebra software makes special accommodations for them.The sum and difference of two symmetric matrices is again symmetric, but this is not always true for the product: given symmetric matrices A and B, then AB is symmetric if and only if A and B commute, i.e., if AB = BA. So for integer n, An is symmetric if A is symmetric.  If A−1 exists, it is symmetric if and only if A is symmetric.Let Matn denote the space of n × n matrices. A symmetric n × n matrix is determined by n(n + 1)/2 scalars (the number of entries on or above the main diagonal). Similarly, a skew-symmetric matrix is determined by n(n − 1)/2 scalars (the number of entries above the main diagonal). If Symn denotes the space of n × n symmetric matrices and Skewn the space of n × n skew-symmetric matrices then Matn = Symn + Skewn and Symn ∩ Skewn = {0}, i.e.where ⊕ denotes the direct sum. Let X ∈ Matn thenNotice that 1/2(X + XT) ∈ Symn and 1/2(X − XT) ∈ Skewn. This is true for every square matrix X with entries from any field whose characteristic is different from 2.Any matrix congruent to a symmetric matrix is again symmetric: if X is a symmetric matrix then so is AXAT for any matrix A.  A symmetric matrix is necessarily a normal matrix.Since this definition is independent of the choice of basis, symmetry is a property that depends only on the linear operator A and a choice of inner product. This characterization of symmetry is useful, for example, in differential geometry, for each tangent space to a manifold may be endowed with an inner product, giving rise to what is called a Riemannian manifold. Another area where this formulation is used is in Hilbert spaces.The finite-dimensional spectral theorem says that any symmetric matrix whose entries are real can be diagonalized by an orthogonal matrix. More explicitly: For every symmetric real matrix A there exists a real orthogonal matrix Q such that D = QTAQ is a diagonal matrix. Every symmetric matrix is thus, up to choice of an orthonormal basis, a diagonal matrix.Every real symmetric matrix is Hermitian, and therefore all its eigenvalues are real. (In fact, the eigenvalues are the entries in the diagonal matrix D (above), and therefore D is uniquely determined by A up to the order of its entries.) Essentially, the property of being symmetric for real matrices corresponds to the property of being Hermitian for complex matrices.A complex symmetric matrix can be 'diagonalized' using a unitary matrix: thus if A is a complex symmetric matrix, there is a unitary matrix U such that  U A U T is a real diagonal matrix. This result is referred to as the Autonne–Takagi factorization. It was originally proved by Léon Autonne (1915) and Teiji Takagi (1925) and rediscovered with different proofs by several other mathematicians.[2][3] In fact, the matrix B = A†A is Hermitian and non-negative, so there is a unitary matrix V such that V†BV is diagonal with non-negative real entries. Thus C = VTAV is complex symmetric with C†C real. Writing C = X + iY with X and Y real symmetric matrices,  C†C = X2 + Y2 + i(XY − YX). Thus XY = YX. Since X and Y commute, there is a real orthogonal matrix W such that both WXWT and WYWT are diagonal. Setting U = WVT, the matrix UAUT is complex diagonal. Post-multiplying U by another diagonal matrix the diagonal entries can be made to be real and non-negative. Since their squares are the eigenvalues of A†A, they coincide with the singular values of A. (Note, about the eigen-decomposition of a complex symmetric matrix A, the Jordan normal form of A may not be diagonal, therefore A may not be diagonalized by any similarity transformation.)Using the Jordan normal form, one can prove that every square real matrix can be written as a product of two real symmetric matrices, and every square complex matrix can be written as a product of two complex symmetric matrices.[4]Every real non-singular matrix can be uniquely factored as the product of an orthogonal matrix and a symmetric positive definite matrix, which is called a polar decomposition. Singular matrices can also be factored, but not uniquely.A complex symmetric matrix may not be diagonalizable by similarity; every real symmetric matrix is diagonalizable by a real orthogonal similarity.Every complex symmetric matrix A can be diagonalized by unitary congruenceSymmetric n-by-n matrices of real functions appear as the Hessians of twice continuously differentiable functions of n real variables.Every quadratic form q on Rn can be uniquely written in the form q(x) = xTAx with a symmetric n-by-n matrix A. Because of the above spectral theorem, one can then say that every quadratic form, up to the choice of an orthonormal basis of Rn, "looks like"with real numbers λi. This considerably simplifies the study of quadratic forms, as well as the study of the level sets {x : q(x) = 1} which are generalizations of conic sections.This is important partly because the second-order behavior of every smooth multi-variable function is described by the quadratic form belonging to the function's Hessian; this is a consequence of Taylor's theorem.An n-by-n matrix A is said to be symmetrizable if there exists an invertible diagonal matrix D and symmetric matrix S such that A = DS.The transpose of a symmetrizable matrix is symmetrizable, since AT = (DS)T = SD = D−1 (DSD) and DSD is symmetric. A matrix A = (aij) is symmetrizable if and only if the following conditions are met:Other types of symmetry or pattern in square matrices have special names; see for example:See also symmetry in mathematics.
Field extension
Field extensions are fundamental in algebraic number theory, and in the study of polynomial roots through Galois theory, and are widely used in algebraic geometry.A subfield of a field L is a subset K of L that is a field with respect to the field operations inherited from L. Equivalently, a subfield is a subset that contains 1, and is closed under the operations of addition, subtraction, multiplication, and taking the inverse of a nonzero element of L.As 1 – 1 = 0, the latter definition implies K and L have the same zero element.For example, the field of rational numbers is a subfield of the real numbers, which is itself a subfield of the complex numbers. More generally, the field of rational numbers is (or is isomorphic to) a subfield of any field of characteristic 0.The characteristic of a subfield is the same as the characteristic of the larger field.If K is a subfield of L, then L is an extension field or simply extension of K, and this pair of fields is a field extension. Such a field extension is denoted L / K (read as "L over K").If L is an extension of F which is in turn an extension of K, then F is said to be an intermediate field (or intermediate extension or subextension) of L / K.Given a field extension L / K, the larger field L is a K-vector space. The dimension of this vector space is called the degree of the extension and is denoted by [L : K]. The degree of an extension is 1 if and only if the two fields are equal. In this case, the extension is a trivial extension. Extensions of degree 2 and 3 are called quadratic extensions and cubic extensions, respectively. A finite extension is an extension that has a finite degree. The degree of a finite extension L / K is denoted [L : K]Given two extensions L / K and M / L, the extension M / K is finite if and only if both L / K and M / L are finite. In this case, one hasAn extension field of the form K(S) is often said to result from the adjunction of S to K.[7][8]In characteristic 0, every finite extension is a simple extension. This is the primitive element theorem, which does not hold true for fields of non-zero characteristic.If a simple extension K(s) / K is not finite, the field K(s) is isomorphic to the field of rational fractions in s over K.The notation L / K is purely formal and does not imply the formation of a quotient ring or quotient group or any other kind of division. Instead the slash expresses the word "over". In some literature the notation L:K is used.It is often desirable to talk about field extensions in situations where the small field is not actually contained in the larger one, but is naturally embedded. For this purpose, one abstractly defines a field extension as an injective ring homomorphism between two fields.Every non-zero ring homomorphism between fields is injective because fields do not possess nontrivial proper ideals, so field extensions are precisely the morphisms in the category of fields.Henceforth, we will suppress the injective homomorphism and assume that we are dealing with actual subfields.The field The fieldBy iterating the above construction, one can construct a splitting field of  any polynomial from K[X]. This is an extension field L of K in which the given polynomial splits into a product of linear factors.Given a field K, we can consider the field K(X) of all rational functions in the variable X with coefficients in K; the elements of K(X) are fractions of two polynomials over K, and indeed K(X) is the field of fractions of the polynomial ring K[X]. This field of rational functions is an extension field of K. This extension is infinite.The set of the elements of L that are algebraic over K form a subextension, which is called the algebraic closure of K in L. This results from the preceding characterization: if s and t are algebraic, the extensions K(s) /K and K(s)(t) /K(s) are finite. Thus K(s, t) /K is also finite, as well as the sub extensions K(s ± t) /K, K(st) /K and K(1/s) /K (if s ≠ 0. It follows that s ± t, st and 1/s are all algebraic.A simple extension is algebraic if and only if it is finite. This implies that an extension is algebraic if and only if it is the union of its finite subextensions, and that every finite extension is algebraic. An algebraic extension L/K is called normal if every irreducible polynomial in K[X] that has a root in L completely factors into linear factors over L. Every algebraic extension F/K admits a normal closure L, which is an extension field of F such that L/K is normal and which is minimal with this property.An algebraic extension L/K is called separable if the minimal polynomial of every element of L over K is separable, i.e., has no repeated roots in an algebraic closure over K. A Galois extension is a field extension that is both normal and separable.A consequence of the primitive element theorem states that every finite separable extension has a primitive element (i.e. is simple).Given any field extension L/K, we can consider its automorphism group Aut(L/K), consisting of all field automorphisms α: L → L with α(x) = x for all x in K. When the extension is Galois this automorphism group is called the Galois group of the extension. Extensions whose Galois group is abelian are called abelian extensions.For a given field extension L/K, one is often interested in the intermediate fields F (subfields of L that contain K). The significance of Galois extensions and Galois groups is that they allow a complete description of the intermediate fields: there is a bijection between the intermediate fields and the subgroups of the Galois group, described by the fundamental theorem of Galois theory.Field extensions can be generalized to ring extensions which consist of a ring and one of its subrings. A closer non-commutative analog are central simple algebras (CSAs) – ring extensions over a field, which are simple algebra (no non-trivial 2-sided ideals, just as for a field) and where the center of the ring is exactly the field. For example, the only finite field extension of the real numbers is the complex numbers, while the quaternions are a central simple algebra over the reals, and all CSAs over the reals are Brauer equivalent to the reals or the quaternions. CSAs can be further generalized to Azumaya algebras, where the base field is replaced by a commutative local ring.Given a field extension, one can "extend scalars" on associated algebraic objects. For example, given a real vector space, one can produce a complex vector space via complexification. In addition to vector spaces, one can perform extension of scalars for associative algebras defined over the field, such as polynomials or group algebras and the associated group representations. Extension of scalars of polynomials is often used implicitly, by just considering the coefficients as being elements of a larger field, but may also be considered more formally. Extension of scalars has numerous applications, as discussed in extension of scalars: applications.
Kernel (linear algebra)
In mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map L : V → W between two vector spaces V and W, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W.  That is, in set-builder notation,The kernel of L is a linear subspace of the domain V.[1]In the linear map L : V → W, two elements of V have the same image in W if and only if their difference lies in the kernel of L:It follows that the image of L is isomorphic to the quotient of V by the kernel:This implies the rank–nullity theorem:where, by rank we mean the dimension of the image of L, and by nullity that of the kernel of L.When V is an inner product space, the quotient V / ker(L) can be identified with the orthogonal complement in V of ker(L).  This is the generalization to linear operators of the row space, or coimage, of a matrix.The notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring.The domain of the mapping is a module, and the kernel constitutes a "submodule". Here, the concepts of rank and nullity do not necessarily apply.If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L: V → W is continuous if and only if the kernel of L is a closed subspace of V.Consider a linear map represented as a m × n matrix A with coefficients in a field K (typically the field of the real numbers or of the complex numbers) and operating on column vectors x with n components over K.The kernel of this linear map is the set of solutions to the equation A x = 0, where 0 is understood as the zero vector. The dimension of the kernel of A is called the nullity of A. In set-builder notation,The matrix equation is equivalent to a homogeneous system of linear equations:Thus the kernel of A is the same as the solution set to the above homogeneous equations.The kernel of an m × n matrix A over a field K is a linear subspace of Kn. That is, the kernel of A, the set Null(A), has the following three properties:The product Ax can be written in terms of the dot product of vectors as follows:Here a1, ... , am denote the rows of the matrix A. It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).The row space, or coimage, of a matrix A is the span of the row vectors of A. By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A.The dimension of the row space of A is called the rank of A, and the dimension of the kernel of A is called the nullity of A. These quantities are related by the rank–nullity theoremThe left null space, or cokernel, of a matrix A consists of all vectors x such that xTA = 0T, where T denotes the transpose of a column vector.  The left null space of A is the same as the kernel of AT.  The left null space of A is the orthogonal complement to the column space of A, and is dual to the cokernel of the associated linear transformation.  The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A.The kernel also plays a role in the solution to a nonhomogeneous system of linear equations:If u and v are two possible solutions to the above equation, thenThus, the difference of any two solutions to the equation Ax = b lies in the kernel of A.It follows that any solution to the equation Ax = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel.  That is, the solution set to the equation Ax = b isGeometrically, this says that the solution set to Ax = b is the translation of the kernel of A by the vector v. See also Fredholm alternative and flat (geometry).We give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.Consider the matrixThe kernel of this matrix consists of all vectors (x, y, z) ∈ R3 for whichwhich can be expressed as a homogeneous system of linear equations involving x, y, and z:which can be written in matrix form as:Gauss–Jordan elimination reduces this to:Rewriting yields:Now we can express an element of the kernel:for c a scalar.Since c is a free variable, this can be expressed equally well as,The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R3); the vector (−1,−26,16)T constitutes a basis of the kernel of A.Thus, the nullity of A is 1.Note also that the following dot products are zero:which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.These two (linearly independent) row vectors span the row space of A, a plane orthogonal to the vector (−1,−26,16)T.With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.A basis of the kernel of a matrix may be computed by Gaussian elimination.In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.For example, suppose thatThenPutting the upper part in column echelon form by column operations on the whole matrix givesThe last three columns of B are zero columns. Therefore, the three last vectors of C,are a basis of the kernel of A.The problem of computing the kernel on a computer depends on the nature of the coefficients.If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic and Chinese remainder theorem, which reduces the problem to several similar ones over finite fields (this avoids the overhead induced by the non-linearity of the computational complexity of integer multiplication).[citation needed]For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.[citation needed]For matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.[2]Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.[citation needed]
Invertible matrix
In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such thatwhere In denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A−1.A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.Non-square matrices (m-by-n matrices for which m ≠ n) do not have an inverse.  However, in some cases such a matrix may have a left inverse or right inverse.  If A is m-by-n and the rank of A is equal to n, then A has a left inverse: an n-by-m matrix B such that BA = In.  If A has rank m, then it has a right inverse: an n-by-m matrix B such that AB = Im.Matrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.While the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. However, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. For a noncommutative ring, the usual determinant is not defined.  The conditions for existence of left-inverse or right-inverse are more complicated since a notion of rank does not exist over rings.The set of n × n invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree n.Let A be a square n by n matrix over a field K (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:Furthermore, the following properties hold for an invertible matrix A:A matrix that is its own inverse, i.e. such that A = A−1 and A2 = I, is called an involutory matrix.It follows from the associativity of matrix multiplication that iffor finite square matrices A and B, then alsoOver the field of real numbers, the set of singular n-by-n matrices, considered as a subset of Rn×n, is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all n-by-n matrices are invertible.Furthermore, the n-by-n invertible matrices are a dense open set in the topological space of all n-by-n matrices.  Equivalently, the set of singular matrices is closed and nowhere dense in the space of n-by-n matrices.In practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.Consider the following 2-by-2 matrix:As an example of a non-invertible, or singular, matrix, consider the matrixGauss-Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.A generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:Victor Pan and John Reif have done work that includes ways of generating a starting seed.[2][3] Byte magazine summarised one of their approaches.[4]Newton's method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton's method is also useful for "touch up" corrections to the Gauss–Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given byIf matrix A is positive definite, then its inverse can be obtained aswhere L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.Writing the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of small matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:so thatwhere |A| is the determinant of A, C is the matrix of cofactors, and CT represents the matrix transpose.The cofactor equation listed above yields the following result for 2 × 2 matrices. Inversion of these matrices can be done as follows:[6]This is possible because 1/(ad − bc) is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.The Cayley–Hamilton method givesA computationally efficient 3 × 3 matrix inversion is given by(where the scalar A is not to be confused with the matrix A).If the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given byThe determinant of A can be computed by applying the rule of Sarrus as follows:The Cayley–Hamilton decomposition givesWith increasing dimension, expressions for the inverse of A get complicated. For n = 4, the Cayley–Hamilton method leads to an expression that is still tractable:Matrices can also be inverted blockwise by using the following analytic inversion formula:    ( 1)where A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and D − CA−1B must be nonsingular.[7]) This strategy is particularly advantageous if A is diagonal and D − CA−1B (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion.This technique was reinvented several times and is due to Hans Boltz (1923),[citation needed] who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.The nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.The inversion procedure that led to Equation (1) performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and A − BD−1C are nonsingular,[8] the result is    ( 2)Equating Equations (1) and (2) leads to    ( 3)where Equation (3) is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.Since a blockwise inversion of an n × n matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.[9] There exist matrix multiplication algorithms with a complexity of O(n2.3727) operations, while the best proven lower bound is Ω(n2 log n).[10]If a matrix A has the property thatthen A is nonsingular and its inverse may be expressed by a Neumann series:[11]Truncating the sum results in an "approximate" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfies More generally, if A is "near" the invertible matrix X in the sense thatthen A is nonsingular and its inverse isIf it is also the case that A − X has rank 1 then this simplifies toSuppose that the invertible matrix A depends on a parameter t. Then the derivative of the inverse of A with respect to t is given byMore generally, ifthen,Therefore,Some of the properties of inverse matrices are shared by generalized inverses (e.g., the Moore–Penrose inverse), which can be defined for any m-by-n matrix.For most practical applications, it is not necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it is necessary that the matrix involved be invertible.Decomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.Although an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.Matrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.Matrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.
Relative change and difference
In any quantitative science, the terms relative change and relative difference are used to compare two quantities while taking into account the "sizes" of the things being compared. The comparison is expressed as a ratio and is a unitless number. By multiplying these ratios by 100 they can be expressed as percentages so the terms percentage change, percent(age) difference, or relative percentage difference are also commonly used. The distinction between "change" and "difference" depends on whether or not one of the quantities being compared is considered a standard or reference or starting value. When this occurs, the term relative change (with respect to the reference value) is used and otherwise the term relative difference is preferred. Relative difference is often used as a quantitative indicator of quality assurance and quality control for repeated measurements where the outcomes are expected to be the same. A special case of percent change (relative change expressed as a percentage) called percent error occurs in measuring situations where the reference value is the accepted or actual value (perhaps theoretically determined) and the value being compared to it is experimentally determined (by measurement).Given two numerical quantities, x and y, their difference, Δ = x − y, can be called their actual difference. When y is a reference value (a theoretical/actual/correct/accepted/optimal/starting, etc. value; the value that x is being compared to) then Δ is called their actual change. When there is no reference value, the sign of Δ has little meaning in the comparison of the two values since it doesn't matter which of the two values is written first, so one often works with |Δ| = |x − y|, the absolute difference instead of Δ, in these situations. Even when there is a reference value, if it doesn't matter whether the compared value is larger or smaller than the reference value, the absolute difference can be considered in place of the actual change.The absolute difference between two values is not always a good way to compare the numbers. For instance, the absolute difference of 1 between 6 and 5 is more significant than the same absolute difference between 100,000,001 and 100,000,000. We can adjust the comparison to take into account the "size" of the quantities involved, by defining, for positive values of xreference:The relative change is not defined if the reference value (xreference) is zero.For values greater than the reference value, the relative change should be a positive number and for values that are smaller, the relative change should be negative. The formula given above behaves in this way only if xreference is positive, and reverses this behavior if xreference is negative. For example, if we are calibrating a thermometer which reads −6 °C when it should read −10 °C, this formula for relative change (which would be called relative error in this application) gives ((−6) − (−10)) / (−10) = 4/−10 = −0.4, yet the reading is too high. To fix this problem we alter the definition of relative change so that it works correctly for all nonzero values of xreference:If the relationship of the value with respect to the reference value (that is, larger or smaller) does not matter in a particular application, the absolute difference may be used in place of the actual change in the above formula to produce a value for the relative change which is always non-negative.Defining relative difference is not as easy as defining relative change since there is no "correct" value to scale the absolute difference with. As a result, there are many options for how to define relative difference and which one is used depends on what the comparison is being used for. In general we can say that the absolute difference |Δ| is being scaled by some function of the values x and y, say f(x, y).[1]As with relative change, the relative difference is undefined if f(x, y) is zero.Several common choices for the function f(x, y) would be:Measures of relative difference are unitless numbers expressed as a fraction. Corresponding values of percent difference would be obtained by multiplying these values by 100 (and appending the % sign to indicate that the value is a percentage).One way to define the relative difference of two numbers is to take their absolute difference divided by the maximum absolute value of the two numbers.if at least one of the values does not equal zero. This approach is especially useful when comparing floating point values in programming languages for equality with a certain tolerance.[2] Another application is in the computation of approximation errors when the relative error of a measurement is required.Another way to define the relative difference of two numbers is to take their absolute difference divided by some functional value of the two numbers, for example, the absolute value of their arithmetic mean:This approach is often used when the two numbers reflect a change in some single underlying entity.[citation needed] A problem with the above approach arises when the functional value is zero. In this example, if x and y have the same magnitude but opposite sign, thenwhich causes division by 0.  So it may be better to replace the denominator with the average of the absolute values of x and y:[citation needed]Percent Error is a special case of the percentage form of relative change calculated from the absolute change between the experimental (measured) and theoretical (accepted) values, and dividing by the theoretical (accepted) value.The terms "Experimental" and "Theoretical" used in the equation above are commonly replaced with similar terms. Other terms used for experimental could be "measured," "calculated," or "actual" and another term used for theoretical could be "accepted."  Experimental value is what has been derived by use of calculation and/or measurement and is having its accuracy tested against the theoretical value, a value that is accepted by the scientific community or a value that could be seen as a goal for a successful result.Although it is common practice to use the absolute value version of relative change when discussing percent error, in some situations, it can be beneficial to remove the absolute values to provide more information about the result. Thus, if an experimental value is less than the theoretical value, the percent error will be negative. This negative result provides additional information about the experimental result. For example, experimentally calculating the speed of light and coming up with a negative percent error says that the experimental value is a velocity that is less than the speed of light. This is a big difference from getting a positive percent error, which means the experimental value is a velocity that is greater than the speed of light (violating the theory of relativity) and is a newsworthy result.The percent error equation, when rewritten by removing the absolute values, becomes:It is important to note that the two values in the numerator do not commute. Therefore, it is vital to preserve the order as above: subtract the theoretical value from the experimental value and not vice versa.A percentage change is a way to express a change in a variable. It represents the relative change between the old value and the new one.For example, if a house is worth $100,000 today and the year after its value goes up to $110,000, the percentage change of its value can be expressed asIt can then be said that the worth of the house went up by 10%.More generally, if V1 represents the old value and V2 the new one,Some calculators directly support this via a %CH or Δ% function.When the variable in question is a percentage itself, it is better to talk about its change by using percentage points, to avoid confusion between relative difference and absolute difference.If a bank were to raise the interest rate on a savings account from 3% to 4%, the statement that "the interest rate was increased by 1%" is ambiguous and should be avoided. The absolute change in this situation is 1 percentage point (4% − 3%), but the relative change in the interest rate is:In general, the term "percentage point(s)" indicates an absolute change or difference of percentages, while the percent sign or the word "percentage" refers to the relative change or difference.[3]Car M costs $50,000 and car L costs $40,000. We wish to compare these costs.[4] With respect to car L, the absolute difference is $10,000 = $50,000 − $40,000. That is, car M costs $10,000 more than car L. The relative difference is,and we say that car M costs 25% more than car L. It is also common to express the comparison as a ratio, which in this example is,and we say that car M costs 125% of the cost of car L.In this example the cost of car L was considered the reference value, but we could have made the choice the other way and considered the cost of car M as the reference value. The absolute difference is now −$10,000 = $40,000 − $50,000 since car L costs $10,000 less than car M. The relative difference,is also negative since car L costs 20% less than car M. The ratio form of the comparison,says that car L costs 80% of what car M costs.It is the use of the words "of" and "less/more than" that distinguish between ratios and relative differences.[5]  Change in a quantity can also be expressed logarithmically using the unit of logarithmic change: the decibel and the neper (Np).  Normalization with a factor of 100, as done for percent, yields the derived unit centineper (cNp), which aligns with the definition for percentage change for very small changes:The second advantage is that the total change after a series of cNp changes equals the sum of the changes.  With percent, summing the changes is only an approximation, with larger error for larger changes.  For example:
Binary operation
In mathematics, a binary operation on a set is a calculation that combines two elements of the set (called operands) to produce another element of the set. More formally, a binary operation is an operation of arity of two whose two domains and one codomain are the same set.  Examples include the familiar elementary arithmetic operations of addition, subtraction, multiplication and division.  Other examples are readily found in different areas of mathematics, such as vector addition, matrix multiplication and conjugation in groups.More precisely, a binary operation on a set S is a map which sends elements of the Cartesian product S × S to S:[1][2][3]Because the result of performing the operation on a pair of elements of S is again an element of S, the operation is called a closed binary operation on S (or sometimes expressed as having the property of closure).[4]  If f is not a function, but is instead a partial function, it is called a partial binary operation.  For instance, division of real numbers is a partial binary operation, because one can't divide by zero: a/0 is not defined for any real a.  Note however that both in algebra and model theory the binary operations considered are defined on all of S × S.Sometimes, especially in computer science, the term is used for any binary function.Binary operations are the keystone of algebraic structures studied in abstract algebra: they are essential in the definitions of groups, monoids, semigroups, rings, and more.  Most generally, a magma is a set together with some binary operation defined on it.Typical examples of binary operations are the addition (+) and multiplication (×) of numbers and matrices as well as composition of functions on a single set.For instance,Many binary operations of interest in both algebra and formal logic are commutative, satisfying f(a, b) = f(b, a) for all elements a and b in S, or associative, satisfying f(f(a, b), c) = f(a, f(b, c)) for all a, b and c in S.  Many also have identity elements and inverse elements.The first three examples above are commutative and all of the above examples are associative.On the set of real numbers R, subtraction, that is, f(a, b) = a − b, is a binary operation which is not commutative since, in general, a − b ≠ b − a.  It is also not associative, since, in general, a − (b − c) ≠ (a − b) − c; for instance, 1 − (2 − 3) = 2 but (1 − 2) − 3 = −4.On the set of natural numbers N, the binary operation exponentiation, f(a,b) = ab, is not commutative since, in general, ab ≠ ba and is also not associative since f(f(a, b), c) ≠ f(a, f(b, c)).  For instance, with a = 2, b = 3 and c = 2, f(23,2) = f(8,2) = 82 = 64, but f(2,32) = f(2,9) = 29 = 512.  By changing the set N to the set of integers Z, this binary operation becomes a partial binary operation since it is now undefined when a = 0 and b is any negative integer.  For either set, this operation has a right identity (which is 1) since f(a, 1) = a for all a in the set, which is not an identity (two sided identity) since f(1, b) ≠ b in general.Division (/), a partial binary operation on the set of real or rational numbers, is not commutative or associative.  Tetration (↑↑), as a binary operation on the natural numbers, is not commutative or associative and has no identity element.Binary operations are often written using infix notation such as a ∗ b, a + b, a · b or (by juxtaposition with no symbol) ab rather than by functional notation of the form f(a, b).  Powers are usually also written without operator, but with the second argument as superscript.Binary operations sometimes use prefix or (probably more often) postfix notation, both of which dispense with parentheses.  They are also called, respectively, Polish notation and reverse Polish notation.A binary operation, ab, depends on the ordered pair (a, b) and so (ab)c (where the parentheses here mean first operate on the ordered pair (a, b) and then operate on the result of that using the ordered pair ((ab), c)) depends in general on the ordered pair ((a, b), c).  Thus, for the general, non-associative case, binary operations can be represented with binary trees.However:A binary operation f on a set S may be viewed as a ternary relation on S, that is, the set of triples (a, b, f(a,b)) in S × S × S for all a and b in S.An external binary operation is a binary function from K × S to S.  This differs from a binary operation in the strict sense in that K need not be S; its elements come from outside.An example of an external binary operation is scalar multiplication in linear algebra.  Here K is a field and S is a vector space over that field.An external binary operation may alternatively be viewed as an action; K is acting on S.Note that the dot product of two vectors is not a binary operation, external or otherwise, as it maps from S × S to K, where K is a field and S is a vector space over K.
